@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {software architecture, variability management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3442391.3442408,
author = {Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Uta, Mathias and Tran, Thi Ngoc Trang and Atas, M\"{u}sl\"{u}m},
title = {An Overview of Recommender Systems and Machine Learning in Feature Modeling and Configuration},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442408},
doi = {10.1145/3442391.3442408},
abstract = {Recommender systems support decisions in various domains ranging from simple items such as books and movies to more complex items such as financial services, telecommunication equipment, and software systems. In this context, recommendations are determined, for example, on the basis of analyzing the preferences of similar users. In contrast to simple items which can be enumerated in an item catalog, complex items have to be represented on the basis of variability models (e.g., feature models) since a complete enumeration of all possible configurations is infeasible and would trigger significant performance issues. In this paper, we give an overview of a potential new line of research which is related to the application of recommender systems and machine learning techniques in feature modeling and configuration. In this context, we give examples of the application of recommender systems and machine learning and discuss future research issues.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {8},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@article{10.1016/j.rcim.2019.101851,
author = {Li, Lei and Zheng, Yufan and Yang, Maolin and Leng, Jiewu and Cheng, Zhengrong and Xie, Yanan and Jiang, Pingyu and Ma, Yongsheng},
title = {A survey of feature modeling methods: Historical evolution and new development},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2019.101851},
doi = {10.1016/j.rcim.2019.101851},
journal = {Robot. Comput.-Integr. Manuf.},
month = feb,
numpages = {16},
keywords = {Feature modeling, Feature ontology, Feature interoperability, Engineering informatics, Socio-cyber-physical system}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@article{10.1109/TCBB.2020.2979430,
author = {Bankapur, Sanjay and Patil, Nagamma},
title = {Enhanced Protein Structural Class Prediction Using Effective Feature Modeling and Ensemble of Classifiers},
year = {2020},
issue_date = {Nov.-Dec. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.2979430},
doi = {10.1109/TCBB.2020.2979430},
abstract = {Protein Secondary Structural Class (PSSC) information is important in investigating further challenges of protein sequences like protein fold recognition, protein tertiary structure prediction, and analysis of protein functions for drug discovery. Identification of PSSC using biological methods is time-consuming and cost-intensive. Several computational models have been developed to predict the structural class; however, they lack in generalization of the model. Hence, predicting PSSC based on protein sequences is still proving to be an uphill task. In this article, we proposed an effective, novel and generalized prediction model consisting of a feature modeling and an ensemble of classifiers. The proposed feature modeling extracts discriminating information (features) by leveraging three techniques: (i) Embedding – features are extracted on the basis of spatial residue arrangements of the sequences using word embedding approaches; (ii) SkipXGram Bi-gram – various sets of skipped bi-gram features are extracted from the sequences; and (iii) General Statistical (GS) based features are extracted which covers the global information of structural sequences. The combined effective sets of features are trained and classified using an ensemble of three classifiers: Support Vector Machine (SVM), Random Forest (RF), and Gradient Boosting Machines (GBM). The proposed model when assessed on five benchmark datasets (high and low sequence similarity), viz. z277, z498, 25PDB, 1189, and FC699, reported an overall accuracy of 93.55, 97.58, 81.82, 81.11, and 93.93 percent respectively. The proposed model is further validated on a large-scale updated low similarity (&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$leq$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="bankapur-ieq1-2979430.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;25%) dataset, where it achieved an overall accuracy of 81.11 percent. The proposed generalized model is robust and consistently outperformed several state-of-the-art models on all the five benchmark datasets.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {2409–2419},
numpages = {11}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/978-3-030-59716-0_71,
author = {Gammulle, Harshala and Denman, Simon and Sridharan, Sridha and Fookes, Clinton},
title = {Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis},
year = {2020},
isbn = {978-3-030-59715-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59716-0_71},
doi = {10.1007/978-3-030-59716-0_71},
abstract = {Automating the analysis of imagery of the Gastrointestinal (GI) tract captured during endoscopy procedures has substantial potential benefits for patients, as it can provide diagnostic support to medical practitioners and reduce mistakes via human error. To further the development of such methods, we propose a two-stream model for endoscopic image analysis. Our model fuses two streams of deep feature inputs by mapping their inherent relations through a novel relational network model, to better model symptoms and classify the image. In contrast to handcrafted feature-based models, our proposed network is able to learn features automatically and outperforms existing state-of-the-art methods on two public datasets: KVASIR and Nerthus. Our extensive evaluations illustrate the importance of having two streams of inputs instead of a single stream and also demonstrates the merits of the proposed relational network architecture to combine those streams.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part III},
pages = {742–751},
numpages = {10},
keywords = {Endoscopy image analysis, Deep networks, Relational networks.},
location = {Lima, Peru}
}

@inproceedings{10.1007/978-3-030-50423-6_24,
author = {Krishnan, Gokul S. and Kamath, S. Sowmya},
title = {Hybrid Text Feature Modeling for Disease Group Prediction Using Unstructured Physician Notes},
year = {2020},
isbn = {978-3-030-50422-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-50423-6_24},
doi = {10.1007/978-3-030-50423-6_24},
abstract = {Existing Clinical Decision Support Systems (CDSSs) largely depend on the availability of structured patient data and Electronic Health Records (EHRs) to aid caregivers. However, in case of hospitals in developing countries, structured patient data formats are not widely adopted, where medical professionals still rely on clinical notes in the form of unstructured text. Such unstructured clinical notes recorded by medical personnel can also be a potential source of rich patient-specific information which can be leveraged to build CDSSs, even for hospitals in developing countries. If such unstructured clinical text can be used, the manual and time-consuming process of EHR generation will no longer be required, with huge person-hours and cost savings. In this article, we propose a generic ICD9 disease group prediction CDSS built on unstructured physician notes modeled using hybrid word embeddings. These word embeddings are used to train a deep neural network for effectively predicting ICD9 disease groups. Experimental evaluation showed that the proposed approach outperformed the state-of-the-art disease group prediction model built on structured EHRs by 15% in terms of AUROC and 40% in terms of AUPRC, thus proving our hypothesis and eliminating dependency on availability of structured patient data.},
booktitle = {Computational Science – ICCS 2020: 20th International Conference, Amsterdam, The Netherlands, June 3–5, 2020, Proceedings, Part IV},
pages = {321–333},
numpages = {13},
keywords = {Topic modeling, Word embedding, Natural language processing, Machine learning, Healthcare informatics, Mortality prediction},
location = {Amsterdam, The Netherlands}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@inproceedings{10.1109/MILCOM47813.2019.9020931,
author = {Jiang, Jianguo and Chen, Jiuming and Gu, Tianbo and Choo, Kim-Kwang Raymond and Liu, Chao and Yu, Min and Huang, Weiqing and Mohapatra, Prasant},
title = {Warder: Online Insider Threat Detection System Using Multi-Feature Modeling and Graph-Based Correlation},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MILCOM47813.2019.9020931},
doi = {10.1109/MILCOM47813.2019.9020931},
abstract = {Existing insider threat detection models and frameworks generally focus on characterizing and detecting malicious insiders, for example by fusing behavioral analysis, machine learning, psychological characters, management measures, etc. However, it remains challenging to design a practical insider threat detection scheme that can be efficiently implemented and deployed in a real-world system. For example, existing approaches focus on extracting features from user behavioral activities but they lack in-depth correlation and decision making for suspected alerts; thus, resulting in high false positives and low detection accuracy. In this work, we propose a novel online insider threat detection system, Warder, which leverages diverse feature dimensions (using neural language processing) and fuses content and behavior features to create a user's daily profile to facilitate threat detection. Besides, hypergraph-based threat scenario feature tree is designed to correlate suspicious users' activities with threat scenarios to further screen the users. In practice, Warder can also be constantly updated using newly discovered features and threat scenarios. We evaluate the performance of Warder using the public CMU CERT dataset, as well as that of approaches from the Oxford group and CMU group. Findings from the evaluation demonstrate that Warder outperforms the other two competing approaches.},
booktitle = {MILCOM 2019 - 2019 IEEE Military Communications Conference (MILCOM)},
pages = {1–6},
numpages = {6},
location = {Norfolk, VA, USA}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.csl.2019.04.004,
author = {Campos, Victor de Abreu and Pedronette, Daniel Carlos Guimar\~{a}es},
title = {A framework for speaker retrieval and identification through unsupervised learning},
year = {2019},
issue_date = {Nov 2019},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {58},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2019.04.004},
doi = {10.1016/j.csl.2019.04.004},
journal = {Comput. Speech Lang.},
month = nov,
pages = {153–174},
numpages = {22},
keywords = {Speaker recognition, Speaker retrieval, Unsupervised learning, Vector quantization, Gaussian mixture model, i-vector}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1145/3214284,
author = {Salekin, Asif and Eberle, Jeremy W. and Glenn, Jeffrey J. and Teachman, Bethany A. and Stankovic, John A.},
title = {A Weakly Supervised Learning Framework for Detecting Social Anxiety and Depression},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3214284},
doi = {10.1145/3214284},
abstract = {Although social anxiety and depression are common, they are often underdiagnosed and undertreated, in part due to difficulties identifying and accessing individuals in need of services. Current assessments rely on client self-report and clinician judgment, which are vulnerable to social desirability and other subjective biases. Identifying objective, nonburdensome markers of these mental health problems, such as features of speech, could help advance assessment, prevention, and treatment approaches. Prior research examining speech detection methods has focused on fully supervised learning approaches employing strongly labeled data. However, strong labeling of individuals high in symptoms or state affect in speech audio data is impractical, in part because it is not possible to identify with high confidence which regions of a long speech indicate the person's symptoms or affective state. We propose a weakly supervised learning framework for detecting social anxiety and depression from long audio clips. Specifically, we present a novel feature modeling technique named NN2Vec that identifies and exploits the inherent relationship between speakers' vocal states and symptoms/affective states. Detecting speakers high in social anxiety or depression symptoms using NN2Vec features achieves F-1 scores 17% and 13% higher than those of the best available baselines. In addition, we present a new multiple instance learning adaptation of a BLSTM classifier, named BLSTM-MIL. Our novel framework of using NN2Vec features with the BLSTM-MIL classifier achieves F-1 scores of 90.1% and 85.44% in detecting speakers high in social anxiety and depression symptoms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jul,
articleno = {81},
numpages = {26},
keywords = {Social anxiety, anxiety, assessment, audio word, depression, embedding, feature modeling, mental disorder, multiple instance learning, speech, weakly labeled, weakly supervised learning}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@article{10.1504/ijcvr.2021.115165,
author = {Sharma, Vipul and Mir, Roohie Naaz},
title = {Maximum entropy-based semi-supervised learning for automatic detection and recognition of objects using deep ConvNets},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {3},
issn = {1752-9131},
url = {https://doi.org/10.1504/ijcvr.2021.115165},
doi = {10.1504/ijcvr.2021.115165},
abstract = {Object detection and localisation is one of the major research areas in computer vision that is growing very rapidly. Currently, there is a plethora of pre-trained models for object detection including YOLO, mask RCNN, RCNN, fast RCNN, multi-box, etc. In this paper, we proposed a new framework for object detection called 'maximum entropy-based semi-supervised learning for automatic detection and recognition of objects'. The main objective of this paper is to recognise objects from a number of visual object classes in a realistic scene simultaneously. The major operations of our proposed approach are preprocessing, localisation, segmentation and object detection. In the preprocessing, three processes, noise reduction, intensity normalisation, and morphology are considered. Then localisation and object segmentation is performed using maximum entropy in which optimal threshold is detected and in the end, object detection is performed using deep ConvNet. The performance of the proposed framework is evaluated using MATLAB-R2018b and it is compared with some previous state of the art techniques in terms of localisation error, detection and segmentation accuracy along with computation time.},
journal = {Int. J. Comput. Vision Robot.},
month = jan,
pages = {328–356},
numpages = {28},
keywords = {maximum entropy, object detection, weakly supervised learning, deep convolutional neural networks, segmentation and localisation}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@article{10.1016/j.asoc.2020.107050,
author = {Chora\'{s}, Micha\l{} and Demestichas, Konstantinos and Gie\l{}czyk, Agata and Herrero, \'{A}lvaro and Ksieniewicz, Pawe\l{} and Remoundou, Konstantina and Urda, Daniel and Wo\'{z}niak, Micha\l{}},
title = {Advanced Machine Learning techniques for fake news (online disinformation) detection: A systematic mapping study},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.107050},
doi = {10.1016/j.asoc.2020.107050},
journal = {Appl. Soft Comput.},
month = mar,
numpages = {15},
keywords = {Fake news, Machine Learning, Social media, Media content manipulation, Disinformation detection}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@article{10.1145/3398069,
author = {Thieme, Anja and Belgrave, Danielle and Doherty, Gavin},
title = {Machine Learning in Mental Health: A Systematic Review of the HCI Literature to Support the Development of Effective and Implementable ML Systems},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3398069},
doi = {10.1145/3398069},
abstract = {High prevalence of mental illness and the need for effective mental health care, combined with recent advances in AI, has led to an increase in explorations of how the field of machine learning (ML) can assist in the detection, diagnosis and treatment of mental health problems. ML techniques can potentially offer new routes for learning patterns of human behavior; identifying mental health symptoms and risk factors; developing predictions about disease progression; and personalizing and optimizing therapies. Despite the potential opportunities for using ML within mental health, this is an emerging research area, and the development of effective ML-enabled applications that are implementable in practice is bound up with an array of complex, interwoven challenges. Aiming to guide future research and identify new directions for advancing development in this important domain, this article presents an introduction to, and a systematic review of, current ML work regarding psycho-socially based mental health conditions from the computing and HCI literature. A quantitative synthesis and qualitative narrative review of 54 papers that were included in the analysis surfaced common trends, gaps, and challenges in this space. Discussing our findings, we (i) reflect on the current state-of-the-art of ML work for mental health, (ii) provide concrete suggestions for a stronger integration of human-centered and multi-disciplinary approaches in research and development, and (iii) invite more consideration of the potentially far-reaching personal, social, and ethical implications that ML models and interventions can have, if they are to find widespread, successful adoption in real-world mental health contexts.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {34},
numpages = {53},
keywords = {AI applications, Mental health, ethics, health care, interaction design, interpretability, machine learning, mental illness, real-world interventions, society + AI, systematic review}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@article{10.3233/JIFS-189568,
author = {Wang, Linuo and Paul, Anand and Cheung, Simon K.S. and Ho, Chiung Ching and Din, Sadia},
title = {Simulation of athlete gait recognition based on spectral features and machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189568},
doi = {10.3233/JIFS-189568},
abstract = {The current technology related to athlete gait recognition has shortcomings such as complicated equipment and high cost, and there are also certain problems in recognition accuracy and recognition efficiency. In order to improve the efficiency of athletes’ gait recognition, this paper studies the different recognition technologies of athletes based on machine learning and spectral feature technology and applies computer vision technology to sports. Moreover, according to the calf angular velocity signal, the occurrence of leg movement is detected in real time, and the gait cycle is accurately divided to reduce the influence of the signal unrelated to the behavior on the recognition process. In addition, this study proposes a gait behavior recognition method based on event-driven strategies. This method uses a gyroscope as the main sensor and uses a wearable sensor node to collect the angular velocity signals of the legs and waist. In addition, this study analyzes the performance of the algorithm proposed by this paper through experimental research. The comparison results show that the method proposed by this paper has improved the number of recognition action types and accuracy and has certain advantages from the perspective of computation and scalability.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7459–7470},
numpages = {12},
keywords = {Spectral features, machine learning, athletes, gait recognition, improved algorithm}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@article{10.1007/s10994-018-5747-8,
author = {Berrar, Daniel and Lopes, Philippe and Dubitzky, Werner},
title = {Incorporating domain knowledge in machine learning for soccer outcome prediction},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-018-5747-8},
doi = {10.1007/s10994-018-5747-8},
abstract = {The task of the 2017 Soccer Prediction Challenge was to use machine learning to predict the outcome of future soccer matches based on a data set describing the match outcomes of 216,743 past soccer matches. One of the goals of the Challenge was to gauge where the limits of predictability lie with this type of commonly available data. Another goal was to pose a real-world machine learning challenge with a fixed time line, involving the prediction of real future events. Here, we present two novel ideas for integrating soccer domain knowledge into the modeling process. Based on these ideas, we developed two new feature engineering methods for match outcome prediction, which we denote as recency feature extraction and rating feature learning. Using these methods, we constructed two learning sets from the Challenge data. The top-ranking model of the 2017 Soccer Prediction Challenge was our k-nearest neighbor model trained on the rating feature learning set. In further experiments, we could slightly improve on this performance with an ensemble of extreme gradient boosted trees (XGBoost). Our study suggests that a key factor in soccer match outcome prediction lies in the successful incorporation of domain knowledge into the machine learning modeling process.},
journal = {Mach. Learn.},
month = jan,
pages = {97–126},
numpages = {30},
keywords = {2017 Soccer Prediction Challenge, Feature engineering, k-NN, Knowledge representation, Open International Soccer Database, Rating feature learning, Recency feature extraction, Soccer analytics, XGBoost}
}

@article{10.1007/s10291-021-01115-0,
author = {Huang, Bohua and Ji, Zengxi and Zhai, Renjian and Xiao, Changfu and Yang, Fan and Yang, Bohang and Wang, Yupu},
title = {Clock bias prediction algorithm for navigation satellites based on a supervised learning long short-term memory neural network},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {1080-5370},
url = {https://doi.org/10.1007/s10291-021-01115-0},
doi = {10.1007/s10291-021-01115-0},
abstract = {In a satellite navigation system, high-precision prediction of satellite clock bias directly determines the accuracy of navigation, positioning, and time synchronization and is the key to realizing autonomous navigation. To further improve satellite clock bias prediction accuracy, we establish a satellite clock bias prediction model by using long short-term memory (LSTM) that can accurately express the nonlinear characteristics of the navigation satellite clock bias. Outliers in the original clock bias should be preprocessed before using the clock bias for prediction. By analyzing the working principle of the traditional median absolute deviations method, the ambiguity of the mathematical model of that method was improved. Experimental results show that the improved method is better than the traditional method at detecting gross errors. The single difference sequence of the preprocessed satellite clock bias was taken as the research object. First, a quadratic polynomial model was fit to the trend term of the single difference sequence. Second, based on the LSTM neural network model and the basic principles of supervised learning, a supervised learning LSTM network model (SL-LSTM) was proposed that models cyclic and random terms. Finally, the prediction function of the satellite clock bias was realized by extrapolating the model by adding a trend term. We adopt the GPS precision satellite clock bias of International GNSS Service data forecast&nbsp;experiments and apply wavelet neural network (WNN), autoregressive integrated moving average (ARIMA), and quadratic polynomial (QP) models to compare their prediction effects. The average prediction RMSE for 3&nbsp;h, 6&nbsp;h, 12&nbsp;h, 1 d, and 3 d based on the SL-LSTM improved by approximately −21.80, −1.85, 8.57, 2.27, and 40.79%, respectively, compared with the results of the WNN. The average prediction RMSE based on the SL-LSTM improved by approximately 38.23, 65.48, 80.22, 85.18, and 94.51% compared with the ARIMA results. The average prediction RMSE based on the SL-LSTM improved by approximately 82.37, 75.88, 67.24, 45.71, and 58.22% compared with the QP results. Compared with the WNN, the SL-LSTM method has no obvious advantages in the prediction accuracy and stability in short-term prediction but achieves a better long-term prediction accuracy and stability. With an increased prediction duration, the SL-LSTM method is clearly better than the other three methods in terms of the prediction accuracy and stability. The results indicated that the quality of satellite clock bias prediction by the SL-LSTM method is better than that of the above three methods and is more suitable for the middle- and long-term prediction of satellite clock bias.},
journal = {GPS Solut.},
month = apr,
numpages = {16},
keywords = {Navigation satellite clock bias, Gross error, International GNSS Service, Prediction model, Long short-term memory, Supervised learning}
}

@article{10.1016/j.neucom.2009.11.045,
author = {Shanableh, T. and Assaleh, K.},
title = {Feature modeling using polynomial classifiers and stepwise regression},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {73},
number = {10–12},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2009.11.045},
doi = {10.1016/j.neucom.2009.11.045},
abstract = {In polynomial networks, feature vectors are mapped to a higher dimensional space through a polynomial function. The expanded vectors are then passed to a single layer network to compute the model parameters. However, as the dimensionality of the feature vectors grows with polynomial expansion, polynomial training and classification become impractical due to the prohibitive number of expanded variables. This problem is more prominent in vision-based systems where high dimensionality feature vectors are extracted from digital images and/or video. In this paper we propose to reduce the dimensionality of the expanded vector through the use of stepwise regression. We compare our work to the reduced-model multinomial networks where the dimensionality of the expanded feature vectors grows linearly whilst preserving the classification ability. We also compare the proposed work to standard polynomial classifiers and to established techniques of polynomial classifiers with dimensionality reduction. Two application scenarios are used to test the proposed solution, namely; image-based hand recognition and video-based recognition of isolated sign language gestures. Various datasets from the UCI machine learning repository are also used for testing. Experimental results illustrate the effectiveness of the proposed dimensionality reduction technique in comparison to published methods.},
journal = {Neurocomput.},
month = jun,
pages = {1752–1759},
numpages = {8},
keywords = {Image/video processing, Pattern classification, Polynomial classifier, Vision-based intelligent systems}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@article{10.1016/j.infsof.2020.106273,
author = {Singh, Jagsir and Singh, Jaswinder},
title = {Detection of malicious software by analyzing the behavioral artifacts using machine learning algorithms},
year = {2020},
issue_date = {May 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {121},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106273},
doi = {10.1016/j.infsof.2020.106273},
journal = {Inf. Softw. Technol.},
month = may,
numpages = {13},
keywords = {Dynamic analysis, Malware, Machine learning algorithms, Random Forest, Static analysis}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1985441.1985458,
author = {Krishnan, Sandeep and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Empirical evaluation of reliability improvement in an evolving software product line},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985458},
doi = {10.1145/1985441.1985458},
abstract = {Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), highreuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {103–112},
numpages = {10},
keywords = {change, failures, reliability, reuse, software product lines},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.5555/2004685.2005507,
author = {Engstr\"{o}m, Emelie and Runeson, Per},
title = {Decision Support for Test Management and Scope Selection in a Software Product Line Context},
year = {2011},
isbn = {9780769543451},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In large software organizations with a product line development approach, system test planning and scope selection is a complex tasks for which tool support is needed. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of double testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. This paper discusses the need and challenges of providing decision support for test planning and test selection in a product line context, and highlights possible paths towards a pragmatic implementation of context-specific decision support of various levels of automation. With existing regression testing approaches it is possible to provide automated decision support in a few specific cases, while test management in general may be supported through visualization of test execution coverage, the testing space and the delta between the sufficiently tested system and the system under test. A better understanding of the real world context and how to map research results to the same is needed.},
booktitle = {Proceedings of the 2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
pages = {262–265},
numpages = {4},
keywords = {decision support, regression testing, software product line testing, test coverage, test selection, visualization},
series = {ICSTW '11}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Graph Laplacian, Nearest neighborhood graph, Self-paced learning, Semi-supervised learning}
}

@article{10.1016/j.cmpb.2021.106040,
author = {Jiang, Zhengyu and Bo, Lulong and Xu, Zhenhua and Song, Yubing and Wang, Jiafeng and Wen, Pingshan and Wan, Xiaojian and Yang, Tao and Deng, Xiaoming and Bian, Jinjun},
title = {An explainable machine learning algorithm for risk factor analysis of in-hospital mortality in sepsis survivors with ICU readmission},
year = {2021},
issue_date = {Jun 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {204},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106040},
doi = {10.1016/j.cmpb.2021.106040},
journal = {Comput. Methods Prog. Biomed.},
month = jun,
numpages = {7},
keywords = {Sepsis, Sepsis survivor, Readmission, Machine learning algorithm, Predictive modeling, Critical care}
}

@article{10.1155/2021/5533884,
author = {Zhang, Xueliang and Yang, Fu-Qiang and Wang, Wei},
title = {Machine Learning-Based Multitarget Tracking of Motion in Sports Video},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1076-2787},
url = {https://doi.org/10.1155/2021/5533884},
doi = {10.1155/2021/5533884},
abstract = {In this paper, we track the motion of multiple targets in sports videos by a machine learning algorithm and study its tracking technique in depth. In terms of moving target detection, the traditional detection algorithms are analysed theoretically as well as implemented algorithmically, based on which a fusion algorithm of four interframe difference method and background averaging method is proposed for the shortcomings of interframe difference method and background difference method. The fusion algorithm uses the learning rate to update the background in real time and combines morphological processing to correct the foreground, which can effectively cope with the slow change of the background. According to the requirements of real time, accuracy, and occupying less video memory space in intelligent video surveillance systems, this paper improves the streamlined version of the algorithm. The experimental results show that the improved multitarget tracking algorithm effectively improves the Kalman filter-based algorithm to meet the real-time and accuracy requirements in intelligent video surveillance scenarios.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {complex software product line, relational concept analysis, reverse engineering, variability extraction},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Class activation mapping, Convolutional neural network, Sentiment analysis, Weakly supervised learning, Word localization}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Feature Extraction, Feature Terms Identification, Requirement Documents, Software Product Lines},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.1016/j.compbiomed.2021.104409,
author = {Amiri, Sepideh and Akbarabadi, Mina and Abdolali, Fatemeh and Nikoofar, Alireza and Esfahani, Azam Janati and Cheraghi, Susan},
title = {Radiomics analysis on CT images for prediction of radiation-induced kidney damage by machine learning models},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104409},
doi = {10.1016/j.compbiomed.2021.104409},
journal = {Comput. Biol. Med.},
month = jun,
numpages = {8},
keywords = {Chronic kidney disease, Computed tomography, Machine learning, Radiation therapy, Radiomics}
}

@article{10.1016/j.future.2019.06.022,
author = {Raza, Muhammad and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Zhao, Ming and Rehman, Zia ur},
title = {A comparative analysis of machine learning models for quality pillar assessment of SaaS services by multi-class text classification of users’ reviews},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.06.022},
doi = {10.1016/j.future.2019.06.022},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {341–371},
numpages = {31},
keywords = {SaaS, Quality pillars, User reviews, Text classification, Machine learning approaches}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1155/2019/4368036,
author = {Deli\'{c}, Vlado and Peri\'{c}, Zoran and Se\v{c}ujski, Milan and Jakovljevi\'{c}, Nik\v{s}a and Nikoli\'{c}, Jelena and Mi\v{s}kovi\'{c}, Dragi\v{s}a and Simi\'{c}, Nikola and Suzi\'{c}, Sini\v{s}a and Deli\'{c}, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.5555/2999611.2999768,
author = {Pan, Xinghao and Gonzalez, Joseph and Jegelka, Stefanie and Broderick, Tamara and Jordan, Michael I.},
title = {Optimistic concurrency control for distributed unsupervised learning},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this "optimistic concurrency control" paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1403–1411},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1007/978-3-540-88582-5_50,
author = {Jiang, Michael and Zhang, Jing and Zhao, Hong and Zhou, Yuanyuan},
title = {Enhancing Software Product Line Maintenance with Source Code Mining},
year = {2008},
isbn = {9783540885818},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-88582-5_50},
doi = {10.1007/978-3-540-88582-5_50},
abstract = {Large-scale reuse and accelerated software development have been some of the key attractions behind software product lines. Various strategies and processes have been developed to facilitate product line development, maintenance, and evolution. However, experiences with software product lines also showed that it is a rather challenging task to maintain software product lines and families over a long period of time. The time and effort needed to manage and maintain product lines increase and quality degrades as product lines evolve. Without proper methods and tools to support the evolution, the cost can outweigh the benefits.This paper describes an approach to simplifying the maintenance of software product lines and improving software quality by integrating traditional software maintenance practices with pattern-based source code mining for defect detection and correction. Our case studies were performed in an industrial setting where the evolution of multiple mobile phone models of a product line was investigated.},
booktitle = {Proceedings of the Third International Conference on Wireless Algorithms, Systems, and Applications},
pages = {538–547},
numpages = {10},
keywords = {Product Line, Reuse, Software Maintenance},
location = {Dallas, Texas},
series = {WASA '08}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1007/s10618-011-0234-x,
author = {Noto, Keith and Brodley, Carla and Slonim, Donna},
title = {FRaC: a feature-modeling approach for semi-supervised and unsupervised anomaly detection},
year = {2012},
issue_date = {July      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-011-0234-x},
doi = {10.1007/s10618-011-0234-x},
abstract = {Anomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called "normal" instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection. The unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them. Many real-world machine learning tasks, including many fraud and intrusion detection tasks, are unsupervised because it is impractical (or impossible) to verify all of the training data. We recently presented FRaC, a new approach for semi-supervised anomaly detection. FRaC is based on using normal instances to build an ensemble of feature models, and then identifying instances that disagree with those models as anomalous. In this paper, we investigate the behavior of FRaC experimentally and explain why FRaC is so successful. We also show that FRaC is a superior approach for the unsupervised as well as the semi-supervised anomaly detection task, compared to well-known state-of-the-art anomaly detection methods, LOF and one-class support vector machines, and to an existing feature-modeling approach.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {109–133},
numpages = {25},
keywords = {Anomaly detection, Unsupervised learning}
}

@inproceedings{10.1145/2020390.2020397,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Are change metrics good predictors for an evolving software product line?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020397},
doi = {10.1145/2020390.2020397},
abstract = {Background: Previous research on three years of early data for an Eclipse product identified some predictors of failure-prone files that work well for that data set. Additionally, Eclipse has been used to explore characteristics of product line software in previous research.Aims: To assess whether change metrics are good predictors of failure-prone files over time for the family of products in the evolving Eclipse product line.Method: We repeat, to the extent possible, the decision tree portion of the prior study to assess our ability to replicate the method, and then extend it by including four more recent years of data. We compare the most prominent predictors with the previous study's results. We then look at the data for three additional Eclipse products as they evolved over time. We explore whether the set of good predictors change over time for one product and whether the set differs among products.Results: We find that change metrics are consistently good and incrementally better predictors across the evolving products in Eclipse. There is also some consistency regarding which change metrics are the best predictors.Conclusion: Change metrics are good predictors for failure-prone files for the Eclipse product line. A small subset of these change metrics is fairly stable and consistent across products and releases.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {change metrics, failure-prone files, post-release defects, prediction, reuse, software product lines},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/3461001.3472729,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard Paul},
title = {Requirements-driven reuse recommendation},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472729},
doi = {10.1145/3461001.3472729},
abstract = {This tutorial explores requirements-based reuse recommendation for product line assets in the context of clone-and-own product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210},
numpages = {1},
keywords = {SPL adoption, similarity, software reuse},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {configuration, evaluation, feature models, recommender systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1007/978-3-319-91947-8_13,
author = {Krishnan, Gokul S. and Sowmya Kamath, S.},
title = {A Supervised Learning Approach for ICU Mortality Prediction Based on Unstructured Electrocardiogram Text Reports},
year = {2018},
isbn = {978-3-319-91946-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91947-8_13},
doi = {10.1007/978-3-319-91947-8_13},
abstract = {Extracting patient data documented in text-based clinical records into a structured form is a predominantly manual process, both time and cost-intensive. Moreover, structured patient records often fail to effectively capture the nuances of patient-specific observations noted in doctors’ unstructured clinical notes and diagnostic reports. Automated techniques that utilize such unstructured text reports for modeling useful clinical information for supporting predictive analytics applications can thus be highly beneficial. In this paper, we propose a neural network based method for predicting mortality risk of ICU patients using unstructured Electrocardiogram (ECG) text reports. Word2Vec word embedding models were adopted for vectorizing and modeling textual features extracted from the patients’ reports. An unsupervised data cleansing technique for identification and removal of anomalous data/special cases was designed for optimizing the patient data representation. Further, a neural network model based on Extreme Learning Machine architecture was proposed for mortality prediction. ECG text reports available in the MIMIC-III dataset were used for experimental validation. The proposed model when benchmarked against four standard ICU severity scoring methods, outperformed all by 10–13%, in terms of prediction accuracy.},
booktitle = {Natural Language Processing and Information Systems: 23rd International Conference on Applications of Natural Language to Information Systems, NLDB 2018, Paris, France, June 13-15, 2018, Proceedings},
pages = {126–134},
numpages = {9},
keywords = {Unstructured text analysis, Healthcare analytics, Clinical Decision Support Systems, Word2Vec, NLP, Machine Learning},
location = {Paris, France}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {feature extraction, requirement documents, reverse engineering, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {ArgoUML SPL benchmark, dynamic feature localization, spectrum-based localization},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {SAT, benchmark, configurable systems, sampling, software product lines, variability model},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {active automata learning, featured transition systems, model learning, software product lines, variability mining},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1109/AVSS.2006.50,
author = {McCool, Chris and Cook, Jamie and Chandran, Vinod and Sridharan, Sridha},
title = {Feature Modelling of PCA Difference Vectors for 2D and 3D Face Recognition},
year = {2006},
isbn = {0769526888},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/AVSS.2006.50},
doi = {10.1109/AVSS.2006.50},
abstract = {This paper examines the the effectiveness of feature modelling to conduct 2D and 3D face recognition. In particular, PCA difference vectors are modelled using Gaussian Mixture Models (GMMs) which describe Intra-Personal (IP) and Extra-Personal (EP) variations. Two classifiers, an IP and IPEP classifier, are formed using these GMMs and their performance is compared to that of the Mahalanobis cosine metric (MahCosine). The best results for the 2D and 3D face modalities are obtained with the IP and IPEP classifiers respectively. The multi-modal fusion of these two systems provided consistent performance improvement across the FRGC database v2.0.},
booktitle = {Proceedings of the IEEE International Conference on Video and Signal Based Surveillance},
pages = {57},
series = {AVSS '06}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {AutoML, NAS, configuration search, feature model, neural architecture search},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1155/2021/5165115,
author = {Tang, Yin and Liao, Dongxue and Huang, Shuqiang and Fan, Qing and Liu, Liang and Ding, Bai Yuan},
title = {Construction of Machine Learning Model Based on Text Mining and Ranking of Meituan Merchants},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/5165115},
doi = {10.1155/2021/5165115},
abstract = {In the Web 2.0 era, the problem of uneven quality and overload of online reviews is very serious, and the cognitive cost of obtaining valuable content from them is getting higher and higher. This paper explores an effective solution to address comment overload by means of information recommendation in order to improve the utilization of online information and information service quality. This paper proposes a review ranking recommendation scheme that focuses on the information quality of reviews and places more emphasis on satisfying users’ personal information need. The paper’s approach is used to extract and rank low-frequency keywords that appear only once in the comment set. The more useful the extracted phrases are, the more useful this review will be and the higher the usefulness votes will be, which can reflect the actual situation of this product more objectively and accurately and facilitate better consumption decisions for consumers. The experimental results show that users’ satisfaction with the perceived usefulness of the reviews is jointly influenced by the information quality of Meituan’s reviews and users’ individual information needs; the recommendation strategy achieves the organic integration of the two, and the evaluation results under three different recommendation modes show that compared with “interest recommendation” and “utility recommendation,” the satisfaction score of “fusion recommendation” is the highest},
journal = {Sci. Program.},
month = jan,
numpages = {9}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/1147249.1147254,
author = {Fischbein, Dario and Uchitel, Sebastian and Braberman, Victor},
title = {A foundation for behavioural conformance in software product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147254},
doi = {10.1145/1147249.1147254},
abstract = {Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {39–48},
numpages = {10},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/1775256.1775271,
author = {Toews, Matthew and Arbel, Tal},
title = {Detecting, localizing and classifying visual traits from arbitrary viewpoints using probabilistic local feature modeling},
year = {2007},
isbn = {3540756892},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present the first framework for detecting, localizing and classifying visual traits of object classes, e.g. gender or age of human faces, from arbitrary viewpoints. We embed all three tasks in a viewpoint-invariant model derived from local scale-invariant features (e.g. SIFT), where features are probabilistically quantified in terms of their occurrence, appearance, geometry and relationship to visual traits of interest. An appearance model is first learned for the object class, after which a Bayesian classifier is trained to identify the model features indicative of visual traits. The advantage of our framework is that it can be applied and evaluated in realistic scenarios, unlike other trait classification techniques that assume data that is single-viewpoint, pre-aligned and cropped from background distraction. Experimentation on the standard color FERET database shows our approach can automatically identify the visual cues in face images linked to the trait of gender. Combined detection, localization and gender classification error rates are a) 15% over a 180-degree range of face viewpoint and b) 13% in frontal faces, lower than other reported results.},
booktitle = {Proceedings of the 3rd International Conference on Analysis and Modeling of Faces and Gestures},
pages = {154–167},
numpages = {14},
location = {Rio de Janeiro, Brazil},
series = {AMFG'07}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {artificial neural networks, empirical study, reusability, survey, systematic reuse},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@article{10.1016/j.patcog.2006.12.027,
author = {Tsap, Leonid V. and Duchaineau, Mark and Goldgof, Dmitry B. and Shin, Min C.},
title = {Data-driven feature modeling, recognition and analysis in a discovery of supersonic cracks in multimillion-atom simulations},
year = {2007},
issue_date = {September, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {40},
number = {9},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2006.12.027},
doi = {10.1016/j.patcog.2006.12.027},
abstract = {This paper presents a new, automated image- and feature-based pipeline of analysis algorithms to elucidate and quantify a discovery regarding supersonic cracks made using large-scale molecular dynamics computations. The first computational confirmation of supersonic cracks was made in recent years, along with the discovery of a two-step, discrete process of increasing crack velocity through the nucleation of a transonic daughter crack and the later nucleation of a supersonic granddaughter crack. This discovery was facilitated by the work presented here. The algorithm pipeline includes modeling, recognition and motion analysis of both the front most crack tips and the more subtle secondary wavefronts from the slower ancestor cracks. The algorithms employed include line extraction from Canny edge maps, feature modeling based on physical properties, and subsequent tracking of primary and secondary wavefronts. The model embeds anticipated propagation properties (physics-based framework) and adapts to changes in the data for unexpected aspects (data-driven modeling). This process is completely automated; it runs in real time on three different 834-frame sequences using 40 250MHz processors. Results supporting the discovery of the two-step transition to supersonic crack propagation in bilayer materials are presented in terms of both feature tracking and velocity analysis.},
journal = {Pattern Recogn.},
month = sep,
pages = {2400–2407},
numpages = {8},
keywords = {Atomic simulation, Data-driven, Feature modeling (analysis, Image motion analysis, Molecular dynamics, Nanoscale analysis, Physics-based, Supersonic cracks, extraction, recognition)}
}

@inproceedings{10.5555/998673.999311,
title = {Using Feature Modeling for Program Comprehension and Software Architecture Recovery},
year = {2004},
isbn = {0769521258},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The available evidence in a legacy software system,which can help in its understanding and recovery of itsarchitecture are not always sufficient. Very often thesystem's documentation is poor and outdated. One mayargue that the most reliable resource of information isthe system's source code. Nevertheless a significantknowledge about the problem domain is required inorder to facilitate the extraction of the system's usefularchitectural information.In this approach feature modeling is introduced as anadditional step in a system's architectural recoveryprocess. Feature modeling structures the system'sfunctionality and supports reverse engineering bydetecting the relations between source code elementsand requirements. Tracing these relations may lead to abetter understanding of the program's behavior and therecovery of various architectural elements. In this way,by providing a mapping between source code andfeatures, the system's feature model supports programcomprehension and architectural recovery.The approach is developed as first part of a migrationmethodology towards a component-based architecture oflegacy systems. Recovered information about featuresand architecture is collected in a repository to enable arefactoring as next step. The approach is currentlyapplied in a large project for reengineering of anindustrial Image Processing System.},
booktitle = {Proceedings of the 11th IEEE International Conference and Workshop on Engineering of Computer-Based Systems},
pages = {406},
series = {ECBS '04}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {constraints, dynamic product line models, product line engineering, simulation, tool, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@inproceedings{10.5555/647366.725826,
author = {Bihler, Monika},
title = {Feature-Modelling - Design by Feature},
year = {1999},
isbn = {3519027461},
publisher = {Teubner},
booktitle = {Effiziente Methoden Der Geometrischen Modellierung Und Der Wissenschaftlichen Visualisierung},
pages = {32–54},
numpages = {23}
}

@inproceedings{10.1145/1178677.1178684,
author = {Xie, Hua and Andreu, Victor and Ortega, Antonio},
title = {Quantization-based probabilistic feature modeling for kernel design in content-based image retrieval},
year = {2006},
isbn = {1595934952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1178677.1178684},
doi = {10.1145/1178677.1178684},
abstract = {In this paper, a quantization-based probabilistic feature modeling approach is proposed for relevance feedback in content-based image retrieval. We demonstrate its performance by using the resulting models within a support vector machine (SVM) based technique. Each feature component is quantized and mapped to probabilistic quantities representing the likelihood of the image being relevant (and irrelevant). These probabilistic quantities are then used to derive an information divergence-based kernel function for SVM classification which we introduced in earlier work. We show that the proposed method leads to the optimal maximum likelihood solution as the knowledge of the actual underlying probability model improves (i.e.,as the feature space is partitioned into arbitrarily small "regions "and accurate models are known for all regions). vWe investigate several practical quantization designs for feature modeling specifically in relevance feedback applications,where the scarcity of the data and high dimensionality prevent usage of vector quantization and parametric modeling approaches.Our proposed framework naturally takes into account the statistics of the data that is available during relevance feedback for the purpose of discriminating between relevant and irrelevant images.Experiments with the Corel dataset show that quantizers specifically designed for this application achieve gains over simple uniform quantizers (e.g.,5% to 10% in retrieval accuracy) when combined with our information divergence kernel. This kernel achieves gains (e.g.,17% in retrieval accuracy after first relevance feedback)as compared to the standard radial basis function (RBF) kernel used for SVM-based relevance feedback.},
booktitle = {Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval},
pages = {23–32},
numpages = {10},
keywords = {kernel, probabilistic modeling, quantization, relevance feedback, support vector machines},
location = {Santa Barbara, California, USA},
series = {MIR '06}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@article{10.1007/s11042-017-5257-x,
author = {Makula, Pooja and Kumar, Akshay and Mukherjee, Snehasis},
title = {Measuring level of cuteness of baby images: a supervised learning scheme},
year = {2018},
issue_date = {July      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5257-x},
doi = {10.1007/s11042-017-5257-x},
abstract = {The attractiveness of a baby face image depends on the perception of the perceiver. However, several recent studies advocate the idea that human perceptual analysis can be approximated by statistical models. We believe that the cuteness of baby faces depends on the low level facial features extracted from different parts (e.g., mouth, eyes, nose) of the faces. In this paper, we introduce a new problem of classifying baby face images based on their cuteness level using supervised learning techniques. The proposed learning model finds the potential of a deep learning technique in measuring the level of cuteness of baby faces. Since no datasets are available to validate the proposed technique, we construct a dataset of images of baby faces, downloaded from the internet. The dataset consists of several challenges like different view-point, orientation, lighting condition, contrast and background. We annotate the data using some well-known statistical tools inherited from Reliability theory. The experiments are conducted with some well-known image features like Speeded Up Robust Feature (SURF), Histogram of Oriented Gradient (HOG), Convolutional Neural Network (CNN) on Gradient and CNN on Laplacian, and the results are presented and discussed.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {16867–16885},
numpages = {19},
keywords = {CNN, HOG, Multi layer perceptron, Perception, SURF}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {150% model, family model, model learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@inproceedings{10.1007/978-3-030-61609-0_43,
author = {Koo, Jaehoon and Klabjan, Diego},
title = {Improved Classification Based on Deep Belief Networks},
year = {2020},
isbn = {978-3-030-61608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61609-0_43},
doi = {10.1007/978-3-030-61609-0_43},
abstract = {For better classification, generative models are used to initialize the model and extract features before training a classifier. Typically, separate unsupervised and supervised learning problems are solved. Generative restricted Boltzmann machines and deep belief networks are widely used for unsupervised learning. We developed several supervised models based on deep belief networks in order to improve this two-phase strategy. Modifying the loss function to account for expectation with respect to the underlying generative model, introducing weight bounds, and multi-level programming are all applied in model development. The proposed models capture both unsupervised and supervised objectives effectively. The computational study verifies that our models perform better than the two-phase training approach. In addition, we conduct an ablation study to examine how a different part of our model and a different mix of training samples affect the performance of our models.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part I},
pages = {541–552},
numpages = {12},
keywords = {Classification, Deep belief networks, Deep learning.},
location = {Bratislava, Slovakia}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {Family-based model checking, Software Product Lines, mCRL2},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3362789.3362923,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto},
title = {Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362923},
doi = {10.1145/3362789.3362923},
abstract = {Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1007–1011},
numpages = {5},
keywords = {Automatic generation, Domain engineering, High-level requirements, Information Dashboards, Meta-modeling},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-540-75690-3_12,
author = {Toews, Matthew and Arbel, Tal},
title = {Detecting, Localizing and Classifying Visual Traits from Arbitrary Viewpoints Using Probabilistic Local Feature Modeling},
year = {2007},
isbn = {978-3-540-75689-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-75690-3_12},
doi = {10.1007/978-3-540-75690-3_12},
abstract = {We present the first framework for detecting, localizing and classifying visual traits of object classes, e.g. gender or age of human faces, from arbitrary viewpoints. We embed all three tasks in a viewpoint-invariant model derived from local scale-invariant features (e.g. SIFT), where features are probabilistically quantified in terms of their occurrence, appearance, geometry and relationship to visual traits of interest. An appearance model is first learned for the object class, after which a Bayesian classifier is trained to identify the model features indicative of visual traits. The advantage of our framework is that it can be applied and evaluated in realistic scenarios, unlike other trait classification techniques that assume data that is single-viewpoint, pre-aligned and cropped from background distraction. Experimentation on the standard color FERET database shows our approach can automatically identify the visual cues in face images linked to the trait of gender. Combined detection, localization and gender classification error rates are a) 15% over a 180-degree range of face viewpoint and b) 13% in frontal faces, lower than other reported results.},
booktitle = {Analysis and Modeling of Faces and Gestures: Third International Workshop, AMFG 2007 Rio de Janeiro, Brazil, October 20, 2007 Proceedings},
pages = {154–167},
numpages = {14},
keywords = {Training Image, Object Class, Face Database, Viewpoint Change, Combine Detection},
location = {Rio de Janeiro, Brazil}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/267734.267765,
author = {de Kraker, Klaas Jan and Dohmen, Maurice and Bronsvoort, Willem F.},
title = {Maintaining multiple views in feature modeling},
year = {1997},
isbn = {0897919467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/267734.267765},
doi = {10.1145/267734.267765},
booktitle = {Proceedings of the Fourth ACM Symposium on Solid Modeling and Applications},
pages = {123–130},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {SMA '97}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Feature Identification, Natural Language Documents, Reverse Engineering, Software Product Lines, Systematic Literature Review, Variability Extraction},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {case-based reasoning, commonality and variability, feature-based representation, product lines, similarity metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/0166-3615(93)90045-3,
author = {Bronsvoort, Willem F. and Jansen, Frederik W.},
title = {Feature modelling and conversion: key concepts to concurrent engineering},
year = {1993},
issue_date = {Jan. 1993},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {21},
number = {1},
issn = {0166-3615},
url = {https://doi.org/10.1016/0166-3615(93)90045-3},
doi = {10.1016/0166-3615(93)90045-3},
journal = {Comput. Ind.},
month = jan,
pages = {61–86},
numpages = {26},
keywords = {concurrent engineering, constraint-based modeling, design by features, feature conversion, feature modeling, feature recognition, parametric modeling, solid modeling}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {configurations, program analysis, testing, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.comcom.2019.02.007,
author = {Schwenk, G. and Pabst, R. and M\"{u}ller, K.R.},
title = {Classification of structured validation data using stateless and stateful features},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {138},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2019.02.007},
doi = {10.1016/j.comcom.2019.02.007},
journal = {Comput. Commun.},
month = apr,
pages = {54–66},
numpages = {13},
keywords = {Mobile communication, Quality of service, Feature modeling, Supervised learning, Structured data, Interpretable learning, Stateless and stateful features}
}

@article{10.1007/s10044-017-0657-0,
author = {Gensler, Andr\'{e} and Sick, Bernhard},
title = {Performing event detection in time series with SwiftEvent: an algorithm with supervised learning of detection criteria},
year = {2018},
issue_date = {May 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {2},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-017-0657-0},
doi = {10.1007/s10044-017-0657-0},
abstract = {The automated detection of points in a time series with a special meaning to a user, commonly referred to as the detection of events, is an important aspect of temporal data mining. These events often are points in a time series that can be peaks, level changes, sudden changes of spectral characteristics, etc. Fast algorithms are needed for event detection for online applications or applications with huge time series data sets.
 In this article, we present a very fast algorithm for event detection that learns detection criteria from labeled sample time series (i.e., time series where events are marked). This algorithm is based on fast transformations of time series into low-dimensional feature spaces and probabilistic modeling techniques to identify criteria in a supervised manner. Events are then found in one, single fast pass over the signal (therefore, the algorithm is called SwiftEvent) by evaluating learned thresholds on Mahalanobis distances in the feature space. We analyze the run-time complexity of SwiftEvent and demonstrate its application in some use cases with artificial and real-world data sets in comparison with other state-of-the-art techniques.},
journal = {Pattern Anal. Appl.},
month = may,
pages = {543–562},
numpages = {20},
keywords = {Change point detection, Event detection, Polynomial approximation, Segmentation, Supervised learning, Temporal data mining, Time series classification, User-defined points}
}

@article{10.5555/3546258.3546382,
author = {Li, Jingyi Jessica and Chen, Yiling Elaine and Tong, Xin},
title = {A flexible model-free prediction-based framework for feature ranking},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Despite the availability of numerous statistical and machine learning tools for joint feature modeling, many scientists investigate features marginally, i.e., one feature at a time. This is partly due to training and convention but also roots in scientists' strong interests in simple visualization and interpretability. As such, marginal feature ranking for some predictive tasks, e.g., prediction of cancer driver genes, is widely practiced in the process of scientific discoveries. In this work, we focus on marginal ranking for binary classification, one of the most common predictive tasks. We argue that the most widely used marginal ranking criteria, including the Pearson correlation, the two-sample t test, and two-sample Wilcoxon rank-sum test, do not fully take feature distributions and prediction objectives into account. To address this gap in practice, we propose two ranking criteria corresponding to two prediction objectives: the classical criterion (CC) and the Neyman-Pearson criterion (NPC), both of which use model-free nonparametric implementations to accommodate diverse feature distributions. Theoretically, we show that under regularity conditions, both criteria achieve sample-level ranking that is consistent with their population-level counterpart with high probability. Moreover, NPC is robust to sampling bias when the two class proportions in a sample deviate from those in the population. This property endows NPC good potential in biomedical research where sampling biases are ubiquitous. We demonstrate the use and relative advantages of CC and NPC in simulation and real data studies. Our model-free objective-based ranking idea is extendable to ranking feature subsets and generalizable to other prediction tasks and learning objectives.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {124},
numpages = {54},
keywords = {model-free, marginal feature ranking, binary classification, classical and Neyman-Pearson paradigms, sampling bias}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1016/j.jvcir.2007.02.005,
author = {Bouguila, Nizar and Ziou, Djemel},
title = {Unsupervised learning of a finite discrete mixture: Applications to texture modeling and image databases summarization},
year = {2007},
issue_date = {August, 2007},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {18},
number = {4},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2007.02.005},
doi = {10.1016/j.jvcir.2007.02.005},
abstract = {This paper presents an unsupervised learning algorithm for fitting a finite mixture model based on the Multinomial Dirichlet distribution (MDD). This mixture is particularly useful for modeling discrete data (vectors of counts). The algorithm proposed is based on the expectation maximization (EM) approach. This mixture is used to improve image databases categorization by integrating semantic features and to produce a new texture model. For the texture modeling problem, the results are reported on the Vistex texture image database from the MIT Media Lab.},
journal = {J. Vis. Comun. Image Represent.},
month = aug,
pages = {295–309},
numpages = {15},
keywords = {Cooccurrence matrix, EM, Finite mixture models, Image retrieval, Maximum likelihood, Multinomial Dirichlet, Semantic features, Vistex}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {clone and own, families of software products, software reuse},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {GCC, feature models, software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s10270-020-00791-9,
author = {Westfechtel, Bernhard and Greiner, Sandra},
title = {Extending single- to multi-variant model transformations by trace-based propagation of variability annotations},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00791-9},
doi = {10.1007/s10270-020-00791-9},
abstract = {Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {853–888},
numpages = {36},
keywords = {Model transformation, Software product line, Annotative variability}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {e-discovery, electronic discovery, predictive coding, technology-assisted review},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Cyber-Physical Systems, Fault Detection, Highly-Configurable Systems, Product Line Testing, Search-Based Software Engineering, Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {boolean functions, feature interactions, fourier transform, performance},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1007/978-3-030-29551-6_3,
author = {Yu, Jing and Yang, Chenghao and Qin, Zengchang and Yang, Zhuoqian and Hu, Yue and Shi, Zhiguo},
title = {Semantic Modeling of Textual Relationships in Cross-modal Retrieval},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_3},
doi = {10.1007/978-3-030-29551-6_3},
abstract = {Feature modeling of different modalities is a basic problem in current research of cross-modal information retrieval. Existing models typically project texts and images into one embedding space, in which semantically similar information will have a shorter distance. Semantic modeling of textural relationships is notoriously difficult. In this paper, we propose an approach to model texts using a featured graph by integrating multi-view textual relationships including semantic relationships, statistical co-occurrence, and prior relationships in knowledge base. A dual-path neural network is adopted to learn multi-modal representations of information and cross-modal similarity measure jointly. We use a Graph Convolutional Network (GCN) for generating relation-aware text representations, and use a Convolutional Neural Network (CNN) with non-linearities for image representations. The cross-modal similarity measure is learned by distance metric learning. Experimental results show that, by leveraging the rich relational semantics in texts, our model can outperform the state-of-the-art models by 3.4% on 6.3% in accuracy on two benchmark datasets.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {24–32},
numpages = {9},
keywords = {Textual relationships, Relationship integration, Cross-modal retrieval, Knowledge graph, Graph Convolutional Network},
location = {Athens, Greece}
}

@inproceedings{10.1007/978-3-030-21290-2_42,
author = {Reinhartz-Berger, Iris and Shimshoni, Ilan and Abdal, Aviva},
title = {Behavior-Derived Variability Analysis: Mining Views for Comparison and Evaluation},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_42},
doi = {10.1007/978-3-030-21290-2_42},
abstract = {The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {675–690},
numpages = {16},
keywords = {Software Product Line Engineering, Variability analysis, Requirements specifications, Feature diagrams},
location = {Rome, Italy}
}

@inproceedings{10.1145/2499777.2500714,
author = {Huang, Changyun and Kamei, Yasutaka and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
title = {Using alloy to support feature-based DSL construction for mining software repositories},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500714},
doi = {10.1145/2499777.2500714},
abstract = {The Mining Software Repositories (MSR) field reveals knowledge for software development by analyzing data stored in repositories such as source control and bug trace systems. In order to reveal the knowledge, MSR researchers need to perform complicated procedures iteratively. To help the complex work of MSR practitioners, we study the construction of domain specific languages (DSLs) for MSR. We have conducted feature-oriented domain analysis (FODA) on MSR and developed a DSL based on the feature model. In this paper, we expand our previous work and propose to construct not a single DSL but a DSL family. A DSL family consists of a series of DSLs with commonality in their domain but suitable to specific applications of MSR. To readily construct these DSLs, we use Alloy to encode the feature model. Our encoding includes not only the DSL features and their relations but also some composition rules that can be used to generate the syntax of DSLs. Based on this, we can automatically derive the language elements to construct DSLs suitable to specific purposes of MSR.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {86–89},
numpages = {4},
keywords = {DSL, FODA, SPL, mining software repositories},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1007/s10664-021-09940-0,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {An empirical investigation of organic software product lines},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09940-0},
doi = {10.1007/s10664-021-09940-0},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems that is increasingly being applied to novel and emerging domains. In this work we investigate the use of software product line engineering in one of these new domains, synthetic biology. In synthetic biology living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse DNA parts. In this paper we perform an investigation of domain engineering that leverages an open-source repository of more than 45,000 reusable DNA parts. We show the feasibility of these new types of product line models by identifying features and related artifacts in up to 93.5% of products, and that there is indeed both commonality and variability. We then construct feature models for four commonly engineered functions leading to product lines ranging from 10 to 7.5 \texttimes{} 1020 products. In a case study we demonstrate how we can use the feature models to help guide new experimentation in aspects of application engineering. Finally, in an empirical study we demonstrate the effectiveness and efficiency of automated reverse engineering on both complete and incomplete sets of products. In the process of these studies, we highlight key challenges and uncovered limitations of existing SPL techniques and tools which provide a roadmap for making SPL engineering applicable to new and emerging domains.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {43},
keywords = {Software product lines, Synthetic biology, Reverse engineering, BioBricks}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Commonality decision, Product platform scope, Software product line engineering}
}

@inproceedings{10.1007/978-3-030-87589-3_19,
author = {Yan, Yutong and Conze, Pierre-Henri and Lamard, Mathieu and Zhang, Heng and Quellec, Gwenol\'{e} and Cochener, B\'{e}atrice and Coatrieux, Gouenou},
title = {Deep Active Learning for Dual-View Mammogram Analysis},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_19},
doi = {10.1007/978-3-030-87589-3_19},
abstract = {Supervised deep learning on medical imaging requires massive manual annotations, which are expertise-needed and time-consuming to perform. Active learning aims at reducing annotation efforts by adaptively selecting the most informative samples for labeling. We propose in this paper a novel deep active learning approach for dual-view mammogram analysis, especially for breast mass segmentation and detection, where the necessity of labeling is estimated by exploiting the consistency of predictions arising from craniocaudal (CC) and mediolateral-oblique (MLO) views. Intuitively, if mass segmentation or detection is robustly performed, prediction results achieved on CC and MLO views should be consistent. Exploiting the inter-view consistency is hence a good way to guide the sampling mechanism which iteratively selects the next image pairs to be labeled by an oracle. Experiments on public DDSM-CBIS and INbreast datasets demonstrate that comparable performance with respect to fully-supervised models can be reached using only 6.83% (9.56%) of labeled data for segmentation (detection). This suggests that combining dual-view mammogram analysis and active learning can strongly contribute to the development of computer-aided diagnosis systems.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {180–189},
numpages = {10},
keywords = {Breast cancer, Mass segmentation, Mass detection, Dual-view mammogram analysis, Active learning, Computer-aided diagnosis},
location = {Strasbourg, France}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.5555/1753235.1753266,
author = {Hubaux, Arnaud and Classen, Andreas and Heymans, Patrick},
title = {Formal modelling of feature configuration workflows},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {In software product line engineering, the configuration process can be a long and complex undertaking that involves many participants. When configuration is supported by feature diagrams, two challenges are to modularise the feature diagram into related chunks, and to schedule them as part of the configuration process. Existing work has only focused on the first of these challenges and, for the rest, assumes that feature diagram modules are configured sequentially. This paper addresses the second challenge. It suggests using YAWL, a state-of-the-art workflow language, to represent the configuration workflow while feature diagrams model the available configuration options. The principal contribution of the paper is a new combined formalism: feature configuration workflows. A formal semantics is provided so as to pave the way for unambiguous tool specification and safer reasoning about of the configuration process. The work is motivated and illustrated through a configuration scenario taken from the space industry.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {221–230},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@article{10.1007/s10270-011-0220-1,
author = {Hubaux, Arnaud and Heymans, Patrick and Schobbens, Pierre-Yves and Deridder, Dirk and Abbasi, Ebrahim Khalil},
title = {Supporting multiple perspectives in feature-based configuration},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0220-1},
doi = {10.1007/s10270-011-0220-1},
abstract = {Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {641–663},
numpages = {23},
keywords = {Feature diagram, Feature-based configuration, Multi-view, Separation of concerns, Software product line engineering}
}

@article{10.5555/3220754.3220861,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Qin, Yongrui and Taylor, Kerry and Yao, Lina},
title = {Learning-based SPARQL query performance modeling and prediction},
year = {2018},
issue_date = {Jul 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1386-145X},
abstract = {One of the challenges of managing an RDF database is predicting performance of SPARQL queries before they are executed. Performance characteristics, such as the execution time and memory usage, can help data consumers identify unexpected long-running queries before they start and estimate the system workload for query scheduling. Extensive works address such performance prediction problem in traditional SQL queries but they are not directly applicable to SPARQL queries. In this paper, we adopt machine learning techniques to predict the performance of SPARQL queries. Our work focuses on modeling features of a SPARQL query to a vector representation. Our feature modeling method does not depend on the knowledge of underlying systems and the structure of the underlying data, but only on the nature of SPARQL queries. Then we use these features to train prediction models. We propose a two-step prediction process and consider performances in both cold and warm stages. Evaluations are performed on real world SPRAQL queries, whose execution time ranges from milliseconds to hours. The results demonstrate that the proposed approach can effectively predict SPARQL query performance and outperforms state-of-the-art approaches.},
journal = {World Wide Web},
month = jul,
pages = {1015–1035},
numpages = {21},
keywords = {Feature modeling, Prediction, Query performance, SPARQL}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Dynamic Software Product Line, Feature Model, Modeling},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Semi-supervised learning, Speech emotion recognition, Domain adaptation, Deep neural networks}
}

@inproceedings{10.1145/3459637.3482246,
author = {Chen, Zekai and Zhong, Fangtian and Chen, Zhumin and Zhang, Xiao and Pless, Robert and Cheng, Xiuzhen},
title = {DCAP: Deep Cross Attentional Product Network for User Response Prediction},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482246},
doi = {10.1145/3459637.3482246},
abstract = {User response prediction, which aims to predict the probability that a user will provide a predefined positive response in a given context such as clicking on an ad or purchasing an item, is crucial to many industrial applications such as online advertising, recommender systems, and search ranking. For these tasks and many other machine learning tasks, an indispensable part of success is feature engineering, where cross features are a significant type of feature transformations. However, due to the high dimensionality and super sparsity of the data collected in these tasks, handcrafting cross features is inevitably time expensive. Prior studies in predicting user response leveraged the feature interactions by enhancing feature vectors with products of features to model second-order or high-order cross features, either explicitly or implicitly. However, these existing methods can be hindered by not learning sufficient cross features due to model architecture limitations or modeling all high-order feature interactions with equal weights. Different features should contribute differently to the prediction, and not all cross features are with the same prediction power.This work aims to fill this gap by proposing a novel architecture Deep Cross Attentional Product Network (DCAP), which keeps cross network's benefits in modeling high-order feature interactions explicitly at the vector-wise level. By computing the inner product or outer product between attentional feature embeddings and original input embeddings as each layer's output, we can model cross features with a higher degree of order as the network's depth increases. We concatenate all the outputs from each layer, which further helps the model capture much information on cross features of different orders. Beyond that, it can differentiate the importance of different cross features in each network layer inspired by the multi-head attention mechanism and Product Neural Network (PNN), allowing practitioners to perform a more in-depth analysis of user behaviors. Additionally, our proposed model can be easily implemented and train in parallel. We conduct comprehensive experiments on three real-world datasets. The results have robustly demonstrated that our proposed model DCAP achieves superior prediction performance compared with the state-of-the-art models. Public codes are available at https://github.com/zachstarkk/DCAP.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {221–230},
numpages = {10},
keywords = {user response prediction, self-attention, recommender system, cross feature modeling},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.3103/S1060992X19020048,
author = {Yakovenko, A. A.},
title = {A Hybrid Learning Approach for Adaptive Classification of Acoustic Signals Using the Simulated Responses of Auditory Nerve Fibers},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X19020048},
doi = {10.3103/S1060992X19020048},
journal = {Opt. Mem. Neural Netw.},
month = apr,
pages = {118–128},
numpages = {11},
keywords = {unsupervised learning, self-organizing maps, radial basis functions, neural responses, machine perception, auditory periphery model, adaptive pattern classification}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Quality, Product line model, Defect, Software product line, Feature model}
}

@inproceedings{10.1007/978-3-030-64694-3_17,
author = {Benmerzoug, Amine and Yessad, Lamia and Ziadi, Tewfik},
title = {Analyzing the Impact of Refactoring Variants on Feature Location},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_17},
doi = {10.1007/978-3-030-64694-3_17},
abstract = {Due to the increasing importance of feature location process, several studies evaluate the performance of different techniques based on IR strategies and a set of software variants as input artifacts. The proposed techniques attempt to improve the results obtained but it is often a difficult task. None of the existing feature location techniques considers the changing nature of the input artifacts, which may undergo series of refactoring changes. In this paper, we investigate the impact of refactoring variants on the feature location techniques. We first evaluate the performance of two techniques through the ArgoUML SPL benchmark when the variants are refactored. We then discuss the degraded results and the possibility of restoring them. Finally, we outline a process of variant alignment that aims to preserve the performance of the feature location.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {279–291},
numpages = {13},
keywords = {Refactoring, Feature location, Software Product Line},
location = {Hammamet, Tunisia}
}

@article{10.3233/IDA-163101,
author = {Chikamai, Keith and Viriri, Serestina and Tapamo, Jules-Raymond},
title = {Mammogram content-based image retrieval based on malignancy classification},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {5},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-163101},
doi = {10.3233/IDA-163101},
abstract = {Content-based image retrieval (CBIR) technique is increasingly gaining research attention as a Computer Aided Diagnosis (CAD) approach for breast cancer diagnosis. This work discusses a novel feature modeling technique for CBIR systems based on classifier scores and standard statistical calculations on the same. Established textural and geometric features are initially used to represent medical characteristics, before being used to generate secondary features through classifier scoring using the Support Vector Machine and Quadratic Discriminant Analysis classifiers. The model is validated through a range of benchmarks, and is shown to perform competitively in comparison to similar works.},
journal = {Intell. Data Anal.},
month = jan,
pages = {1193–1212},
numpages = {20},
keywords = {Mammography, microcalcifications, image processing, CBIR, machine learning, computer aided diagnosis}
}

@article{10.1016/j.asoc.2016.07.048,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
title = {Extracting features from online software reviews to aid requirements reuse},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.048},
doi = {10.1016/j.asoc.2016.07.048},
abstract = {Display Omitted The extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have the access.Online reviews for software products can be used as input for features extraction to assist requirements reuse.Techniques from unsupervised learning and Natural Language Processing is employed as a propose solutions to Requirements Reuse problem.The approach obtained a precision of 87% (62% average) and a recall of 86% (82% average), when evaluated against the truth data set created manually. Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1297–1315},
numpages = {19},
keywords = {Unsupervised learning, Software engineering, Requirements reuse, Natural language processing, Latent semantic analysis}
}

@article{10.1016/j.patcog.2021.108164,
author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
title = {Training object detectors from few weakly-labeled and many unlabeled images},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108164},
doi = {10.1016/j.patcog.2021.108164},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Unlabelled set, Semi-supervised learning, Weakly-supervised learning, Object detection}
}

@inproceedings{10.1007/978-3-030-87196-3_28,
author = {Wu, Yicheng and Xu, Minfeng and Ge, Zongyuan and Cai, Jianfei and Zhang, Lei},
title = {Semi-supervised Left Atrium Segmentation with Mutual Consistency&nbsp;Training},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_28},
doi = {10.1007/978-3-030-87196-3_28},
abstract = {Semi-supervised learning has attracted great attention in the field of machine learning, especially for medical image segmentation tasks, since it alleviates the heavy burden of collecting abundant densely annotated data for training. However, most of existing methods underestimate the importance of challenging regions (e.g. small branches or blurred edges) during training. We believe that these unlabeled regions may contain more crucial information to minimize the uncertainty prediction for the model and should be emphasized in the training process. Therefore, in this paper, we propose a novel Mutual Consistency Network (MC-Net) for semi-supervised left atrium segmentation from 3D MR images. Particularly, our MC-Net consists of one encoder and two slightly different decoders, and the prediction discrepancies of two decoders are transformed as an unsupervised loss by our designed cycled pseudo label scheme to encourage mutual consistency. Such mutual consistency encourages the two decoders to have consistent and low-entropy predictions and enables the model to gradually capture generalized features from these unlabeled challenging regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it obtains impressive performance gains by exploiting the unlabeled data effectively. Our MC-Net outperforms six recent semi-supervised methods for left atrium segmentation, and sets the new state-of-the-art performance on the LA database.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {297–306},
numpages = {10},
keywords = {Cycled pseudo label, Mutual consistency, Semi-supervised learning},
location = {Strasbourg, France}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {rl-as-inference, tempered inference, self-paced learning, reinforcement learning, curriculum learning}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {self-supervised learning, once and for all, multi-modal, co-training},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1016/j.jnca.2021.103210,
author = {Jiang, Yuning and Atif, Yacine},
title = {A selective ensemble model for cognitive cybersecurity analysis},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {193},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2021.103210},
doi = {10.1016/j.jnca.2021.103210},
journal = {J. Netw. Comput. Appl.},
month = nov,
numpages = {16},
keywords = {Database management, Data mining, Ensemble, Machine learning, Data correlation, Vulnerability analysis, Information security}
}

@article{10.1007/s10664-020-09853-4,
author = {Hajri, Ines and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
title = {Automating system test case classification and prioritization for use case-driven testing in product lines},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09853-4},
doi = {10.1007/s10664-020-09853-4},
abstract = {Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation was performed in the context of an industrial product family in the automotive domain. Results provide empirical evidence that we propose a practical and beneficial way to classify and prioritize system test cases for industrial product lines.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3711–3769},
numpages = {59},
keywords = {Requirements engineering, Automotive, Test case selection and prioritization, Regression testing, Use case driven development, Product Line Engineering}
}

@inproceedings{10.1007/978-3-030-87193-2_11,
author = {Wang, Wenxuan and Chen, Chen and Ding, Meng and Yu, Hong and Zha, Sen and Li, Jiangyun},
title = {TransBTS: Multimodal Brain Tumor Segmentation Using Transformer},
year = {2021},
isbn = {978-3-030-87192-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87193-2_11},
doi = {10.1007/978-3-030-87193-2_11},
abstract = {Transformer, which can benefit from global (long-range) information modeling using self-attention mechanisms, has been successful in natural language processing and 2D image classification recently. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we for the first time exploit Transformer in 3D CNN for MRI Brain Tumor Segmentation and propose a novel network named TransBTS based on the encoder-decoder structure. To capture the local 3D context information, the encoder first utilizes 3D CNN to extract the volumetric spatial feature maps. Meanwhile, the feature maps are reformed elaborately for tokens that are fed into Transformer for global feature modeling. The decoder leverages the features embedded by Transformer and performs progressive upsampling to predict the detailed segmentation map. Extensive experimental results on both BraTS 2019 and 2020 datasets show that TransBTS achieves comparable or higher results than previous state-of-the-art 3D methods for brain tumor segmentation on 3D MRI scans. The source code is available at .},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I},
pages = {109–119},
numpages = {11},
keywords = {Segmentation, Brain tumor, MRI, Transformer, 3D CNN},
location = {Strasbourg, France}
}

@article{10.1016/j.infsof.2009.11.001,
author = {Rabiser, Rick and Gr\"{u}nbacher, Paul and Dhungana, Deepak},
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
year = {2010},
issue_date = {March, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.001},
doi = {10.1016/j.infsof.2009.11.001},
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {324–346},
numpages = {23},
keywords = {Systematic literature review, Software product line, Product line engineering, Product derivation}
}

@article{10.5555/3455716.3455816,
author = {Valera, Isabel and Pradier, Melanie F. and Lomeli, Maria and Ghahramani, Zoubin},
title = {General latent feature models for heterogeneous datasets},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Latent variable models allow capturing the hidden structure underlying the data. In particular, feature allocation models represent each observation by a linear combination of latent variables. These models are often used to make predictions either for new observations or for missing information in the original data, as well as to perform exploratory data analysis. Although there is an extensive literature on latent feature allocation models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) type, there is no general framework for practical latent feature modeling for heterogeneous datasets. In this paper, we introduce a general Bayesian nonparametric latent feature allocation model suitable for heterogeneous datasets, where the attributes describing each object can be arbitrary combinations of real-valued, positive real-valued, categorical, ordinal and count variables. The proposed model presents several important properties. First, it is suitable for heterogeneous data while keeping the properties of conjugate models, which enables us to develop an inference algorithm that presents linear complexity with respect to the number of objects and attributes per MCMC iteration. Second, the Bayesian nonparametric component allows us to place a prior distribution on the number of features required to capture the latent structure in the data. Third, the latent features in the model are binary-valued, which facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a software package, called GLFM toolbox, is made publicly available for other researchers to use and extend. It is available at https://ivaleram.github.io/GLFM/. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {100},
numpages = {49}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST '20}
}

@article{10.1007/s11265-018-1406-3,
author = {Wang, Qiangyu and Zhang, Guoying and Yu, Shu},
title = {2D Hand Detection Using Multi-Feature Skin Model Supervised Cascaded CNN},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {91},
number = {10},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-018-1406-3},
doi = {10.1007/s11265-018-1406-3},
abstract = {Hand gesture recognition is one of the most popular Human Computer Interface. The first step in most vision-based gesture recognition system is the hand detection and segmentation. Since hands are involved in a variety of daily tasks, the detection work suffers from both extreme illumination changes and the intrinsic variability of hand appearance. To overcome these problems, we propose a new method for 2D hand detection which can be considered as a combination of Multi-Feature based hand proposal generation and cascaded convolutional neural network (CCNN) classification. Considered various luminance, we choose color, Gabor, HOG and SIFT feature to discriminate skin region and generate hand proposal. Also, we propose a cascaded CNN that keeps the deep context information to detect hand among the proposals. The proposed Multi-Feature Supervised Cascaded CNN (MFS-CCNN) method is tested on a combination of several datasets including Oxford Hands Dataset, VIVA hand detection and Egohands Dataset as positive sample and ImageNet 2012, FDDB dataset as negative sample. The proposed method achieves competitive results.},
journal = {J. Signal Process. Syst.},
month = oct,
pages = {1105–1113},
numpages = {9},
keywords = {Hand detection, Feature modeling, Convolutional neural networks}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Community detection, Information retrieval, Feature extraction, Software product line}
}

@article{10.1007/s11280-017-0498-1,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Qin, Yongrui and Taylor, Kerry and Yao, Lina},
title = {Learning-based SPARQL query performance modeling and prediction},
year = {2018},
issue_date = {Jul 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-017-0498-1},
doi = {10.1007/s11280-017-0498-1},
abstract = {One of the challenges of managing an RDF database is predicting performance of SPARQL queries before they are executed. Performance characteristics, such as the execution time and memory usage, can help data consumers identify unexpected long-running queries before they start and estimate the system workload for query scheduling. Extensive works address such performance prediction problem in traditional SQL queries but they are not directly applicable to SPARQL queries. In this paper, we adopt machine learning techniques to predict the performance of SPARQL queries. Our work focuses on modeling features of a SPARQL query to a vector representation. Our feature modeling method does not depend on the knowledge of underlying systems and the structure of the underlying data, but only on the nature of SPARQL queries. Then we use these features to train prediction models. We propose a two-step prediction process and consider performances in both cold and warm stages. Evaluations are performed on real world SPRAQL queries, whose execution time ranges from milliseconds to hours. The results demonstrate that the proposed approach can effectively predict SPARQL query performance and outperforms state-of-the-art approaches.},
journal = {World Wide Web},
month = jul,
pages = {1015–1035},
numpages = {21},
keywords = {Feature modeling, Prediction, Query performance, SPARQL}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Real-world case studies, Variability modeling, Constraint classification, Multi-stage and multi-step configuration process, Automated configuration, Product line engineering, Cyber-physical systems}
}

@inproceedings{10.1145/3488662.3498658,
author = {Lima, Dimas S. and Oliveira, Bruno Guimar\~{a}es and Mendes, Paulo Renato C. and Costa, Lucas and Coelho, Yago},
title = {An ML-Based Approach for Near Real-Time Content Caching},
year = {2021},
isbn = {9781450391375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488662.3498658},
doi = {10.1145/3488662.3498658},
abstract = {Content caching is a well-known promising solution to address large demands for streaming companies. This paper presents an ongoing work towards improving CDN network traffic focusing on users' quality of experience (QoE) by anticipating which videos will be popular on Globo's platform. To do so, a deep neural network approach was chosen to model video's popularity based on its metadata and a near real-time framework is presented describing how to make content caching in a preemptive way. Additionally, a threshold selection approach is presented defining whether a video should be cached or not. The presented approach allows making content cache without any user interaction, aiming to decide about the admission of the content before it starts to receive requests. This approach is important to most of the daily published videos at Globo, especially for breaking news. Using Globo's real-world data, we demonstrate the popularity predictor results and conclude with some directions for future works.},
booktitle = {Proceedings of the Workshop on Design, Deployment, and Evaluation of Network-Assisted Video Streaming},
pages = {8–14},
numpages = {7},
keywords = {Popularity Prediction, Natural Language Processing, Machine Learning, Deep Neural Network, Content Delivery Network, Content Caching},
location = {Virtual Event, Germany},
series = {VisNEXT'21}
}

@inproceedings{10.1109/SPLC.2011.47,
author = {Chen, Sheng and Erwig, Martin},
title = {Optimizing the Product Derivation Process},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.47},
doi = {10.1109/SPLC.2011.47},
abstract = {Feature modeling is widely used in software product-line engineering to capture the commonalities and variabilities within an application domain. As feature models evolve, they can become very complex with respect to the number of features and the dependencies among them, which can cause the product derivation based on feature selection to become quite time consuming and error prone. We address this problem by presenting techniques to find good feature selection sequences that are based on the number of products that contain a particular feature and the impact of a selected feature on the selection of other features. Specifically, we identify a feature selection strategy, which brings up highly selective features early for selection. By prioritizing feature selection based on the selectivity of features our technique makes the feature selection process more efficient. Moreover, our approach helps with the problem of unexpected side effects of feature selection in later stages of the selection process, which is commonly considered a difficult problem. We have run our algorithm on the e-Shop and Berkeley DB feature models and also on some automatically generated feature models. The evaluation results demonstrate that our techniques can shorten the product derivation processes significantly.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {35–44},
numpages = {10},
keywords = {Feature Selection, Feature Model, Decision Sequence},
series = {SPLC '11}
}

@article{10.1016/j.patrec.2020.09.028,
author = {Sharma, Vipal Kumar and Mir, Roohie Naaz},
title = {SSFNET-VOS: Semantic segmentation and fusion network for video object segmentation},
year = {2020},
issue_date = {Dec 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {140},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2020.09.028},
doi = {10.1016/j.patrec.2020.09.028},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {49–58},
numpages = {10},
keywords = {Multimedia processing, Video object segmentation, Unsupervised learning, Computer vision, Semantic segmentation}
}

@article{10.1016/j.eswa.2018.04.033,
author = {Sreevani and Murthy, C.A. and Chanda, Bhabatosh},
title = {Generation of compound features based on feature interaction for classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.04.033},
doi = {10.1016/j.eswa.2018.04.033},
journal = {Expert Syst. Appl.},
month = oct,
pages = {61–73},
numpages = {13},
keywords = {Feature extraction, Feature selection, Compound features, Semi-features, Information theory, Feature interaction, Mutual information}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {sub-Gaussian biclustering, stochastic block model, spectral clustering, regularization of random graphs, graphon clustering, community detection, bipartite networks}
}

@article{10.1016/j.artmed.2021.102162,
author = {Naranjo, Lizbeth and P\'{e}rez, Carlos J. and Campos-Roca, Yolanda and Madruga, Mario},
title = {Replication-based regularization approaches to diagnose Reinke's edema by using voice recordings},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102162},
doi = {10.1016/j.artmed.2021.102162},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {10},
keywords = {Variable selection, Replicated measurements, Regularization, Reinke's edema, Classification, Acoustic features}
}

@article{10.1016/j.future.2018.12.025,
author = {Cao, Yang and Lung, Chung-Horng and Ajila, Samuel A. and Li, Xiaolin},
title = {Support mechanisms for cloud configuration using XML filtering techniques: A case study in SaaS},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {95},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.12.025},
doi = {10.1016/j.future.2018.12.025},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {52–67},
numpages = {16},
keywords = {Yfilter, XML Filtering, Feature Modeling, Multi-Tenancy, Software-as-a-Service, Cloud Computing}
}

@article{10.1007/s11063-020-10286-9,
author = {Li, Li and Zhao, Kaiyi and Li, Sicong and Sun, Ruizhi and Cai, Saihua},
title = {Extreme Learning Machine for Supervised Classification with Self-paced Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10286-9},
doi = {10.1007/s11063-020-10286-9},
abstract = {The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1723–1744},
numpages = {22},
keywords = {Accuracy, Self-paced learning, Extreme learning machine, Classification}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Multi-view learning, Semi-supervised feature selection, Self-paced learning, Graph-based semi-supervised learning}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@article{10.1007/s11042-020-10470-y,
author = {Choudhury, Ananya and Sarma, Kandarpa Kumar},
title = {A CNN-LSTM based ensemble framework for in-air handwritten Assamese character recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {28–29},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10470-y},
doi = {10.1007/s11042-020-10470-y},
abstract = {In-air handwriting is a contemporary human computer interaction (HCI) technique which enables users to write and communicate in free space in a simple and intuitive manner. Air-written characters exhibit wide variations depending upon different writing styles of users and their speed of articulation, which presents a great challenge towards effective recognition of linguistic characters. So, in this paper we have proposed an ensemble model for in-air handwriting recognition which is based on convolutional neural network (CNN) and a long short-term memory neural network (LSTM-NN). The method collaborates overall character trajectory appearance modeling and temporal trajectory feature modeling for efficient recognition of varied types of air-written characters. In contrast to two-dimensional handwriting, in-air handwriting generally involves writing of characters interlinked by a continuous stroke, which makes segregation of intended writing activity from insignificant connecting motions an intricate task. So, a two-stage statistical framework is incorporated in the system for automatic detection and extraction of relevant writing segments from air-written characters. Identification of writing events from a continuous stream of air-written data is accomplished by formulating a Markov Random Field (MRF) model, while the segmentation of writing events into meaningful handwriting segments and redundant parts is performed by implementation of a Mahalanobis distance (MD) classifier. The proposed approach is assessed on an air-written character dataset comprising of Assamese vowels, consonants and numerals. The experimental results connote that our hybrid network can assimilate more information from the air-writing patterns and hence offer better recognition performance than the state-of-the-art approaches.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {35649–35684},
numpages = {36},
keywords = {In-air handwriting recognition, Human computer interaction, Ligature, Markov random field, Convolutional neural network, Long short-term memory}
}

@article{10.1016/j.ijar.2007.03.006,
author = {Peterson, Leif E. and Coleman, Matthew A.},
title = {Machine learning-based receiver operating characteristic (ROC) curves for crisp and fuzzy classification of DNA microarrays in cancer research},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2007.03.006},
doi = {10.1016/j.ijar.2007.03.006},
abstract = {Receiver operating characteristic (ROC) curves were generated to obtain classification area under the curve (AUC) as a function of feature standardization, fuzzification, and sample size from nine large sets of cancer-related DNA microarrays. Classifiers used included k-nearest neighbor (kNN), naive Bayes classifier (NBC), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), learning vector quantization (LVQ1), logistic regression (LOG), polytomous logistic regression (PLOG), artificial neural networks (ANN), particle swarm optimization (PSO), constricted particle swarm optimization (CPSO), kernel regression (RBF), radial basis function networks (RBFN), gradient descent support vector machines (SVMGD), and least squares support vector machines (SVMLS). For each data set, AUC was determined for a number of combinations of sample size, total sum[-log(p)] of feature t-tests, with and without feature standardization and with (fuzzy) and without (crisp) fuzzification of features. Altogether, a total of 2,123,530 classification runs were made. At the greatest level of sample size, ANN resulted in a fitted AUC of 90%, while PSO resulted in the lowest fitted AUC of 72.1%. AUC values derived from 4NN were the most dependent on sample size, while PSO was the least. ANN depended the most on total statistical significance of features used based on sum[-log(p)], whereas PSO was the least dependent. Standardization of features increased AUC by 8.1% for PSO and -0.2% for QDA, while fuzzification increased AUC by 9.4% for PSO and reduced AUC by 3.8% for QDA. AUC determination in planned microarray experiments without standardization and fuzzification of features will benefit the most if CPSO is used for lower levels of feature significance (i.e., sum[-log(p)]~50) and ANN is used for greater levels of significance (i.e., sum[-log(p)]~500). When only standardization of features is performed, studies are likely to benefit most by using CPSO for low levels of feature statistical significance and LVQ1 for greater levels of significance. Studies involving only fuzzification of features should employ LVQ1 because of the substantial gain in AUC observed and low expense of LVQ1. Lastly, PSO resulted in significantly greater levels of AUC (89.5% average) when feature standardization and fuzzification were performed. In consideration of the data sets used and factors influencing AUC which were investigated, if low-expense computation is desired then LVQ1 is recommended. However, if computational expense is of less concern, then PSO or CPSO is recommended.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {17–36},
numpages = {20},
keywords = {Soft computing, Receiver operator characteristic (ROC) curve, Gene expression, Fuzzy classification, DNA microarrays, Area under the curve (AUC)}
}

@article{10.1007/s00521-020-05233-7,
author = {Nagasubramanian, Gayathri and Sankayya, Muthuramalingam},
title = {Multi-Variate vocal data analysis for Detection of Parkinson disease using Deep Learning},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05233-7},
doi = {10.1007/s00521-020-05233-7},
abstract = {Machine learning (ML) and Deep learning (DL) methods are differently implemented with various decision-making abilities. Particularly, the usage of ML and DL techniques in disease detection is inevitable in the near future. This work uses the ability of acoustic-based DL techniques for detecting Parkinson disease symptoms. This disease can be identified by many DL techniques such as deep knowledge creation networks and recurrent networks. The proposed Deep Multi-Variate Vocal Data Analysis (DMVDA) System has been designed and implemented using Acoustic Deep Neural Network (ADNN), Acoustic Deep Recurrent Neural Network (ADRNN), and Acoustic Deep Convolutional Neural Network (ADCNN). Further, DMVDA has been specially developed with absolute multi-variate speech attribute processing algorithm for effective value creation. In order to improve the benefits of this speech-processing algorithm, the DMVDA has acoustic data sampling procedures. The DL techniques introduced in this work helps to identify Parkinson symptoms by analyzing the heterogeneous dataset. The integration of these techniques produces nominal 3% increase in the performance than the existing techniques.},
journal = {Neural Comput. Appl.},
month = may,
pages = {4849–4864},
numpages = {16},
keywords = {Parkinson, Disease detection, Acoustic data, Machine learning, Deep learning}
}

@article{10.14778/3457390.3457394,
author = {Fang, Ziquan and Pan, Lu and Chen, Lu and Du, Yuntao and Gao, Yunjun},
title = {MDTP: a multi-source deep traffic prediction framework over spatio-temporal trajectory data},
year = {2021},
issue_date = {April 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3457390.3457394},
doi = {10.14778/3457390.3457394},
abstract = {Traffic prediction has drawn increasing attention for its ubiquitous real-life applications in traffic management, urban computing, public safety, and so on. Recently, the availability of massive trajectory data and the success of deep learning motivate a plethora of deep traffic prediction studies. However, the existing neural-network-based approaches tend to ignore the correlations between multiple types of moving objects located in the same spatio-temporal traffic area, which is suboptimal for traffic prediction analytics.In this paper, we propose a multi-source deep traffic prediction framework over spatio-temporal trajectory data, termed as MDTP. The framework includes two phases: spatio-temporal feature modeling and multi-source bridging. We present an enhanced graph convolutional network (GCN) model combined with long short-term memory network (LSTM) to capture the spatial dependencies and temporal dynamics of traffic in the feature modeling phase. In the multi-source bridging phase, we propose two methods, Sum and Concat, to connect the learned features from different trajectory data sources. Extensive experiments on two real-life datasets show that MDTP i) has superior efficiency, compared with classical time-series methods, machine learning methods, and state-of-the-art neural-network-based approaches; ii) offers a significant performance improvement over the single-source traffic prediction approach; and iii) performs traffic predictions in seconds even on tens of millions of trajectory data. we develop MDTP+, a user-friendly interactive system to demonstrate traffic prediction analysis.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {1289–1297},
numpages = {9}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, Andr\'{e} and Batista, Thais and Cacho, N\'{e}lio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {software product lines, services, health watcher system, cloud platforms, cloud computing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1109/SPLC.2008.28,
author = {Chae, Wonseok and Blume, Matthias},
title = {Building a Family of Compilers},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.28},
doi = {10.1109/SPLC.2008.28},
abstract = {We have developed and maintained a set of closely related compilers. Although much of their code is duplicated and shared, they have been maintained separately because they are treated as different compilers. Even if they were merged together, the combined code would become too complicated to serve as the base for another extension. We describe our experience to address this problem by adopting the product line engineering paradigm to build a family of compilers. This paradigm encourages developers to focus on developing a set of compilers rather than on developing one particular compiler. We show engineering activities for a family of compilers from product line analysis through product line architecture design to product line component design. Then, we present how to build particular compilers from core assets resulting from the previous activities and how to take advantage of modern programming language technology to organize this task. Our experience demonstrates that the product line engineering as a developing paradigm can ease the construction of a family of compilers.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {307–316},
numpages = {10},
keywords = {standard ml, product line engineering, module system, feature-oriented, compilers},
series = {SPLC '08}
}

@article{10.1145/3312739,
author = {Taha, Ayman and Hadi, Ali S.},
title = {Anomaly Detection Methods for Categorical Data: A Review},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3312739},
doi = {10.1145/3312739},
abstract = {Anomaly detection has numerous applications in diverse fields. For example, it has been widely used for discovering network intrusions and malicious events. It has also been used in numerous other applications such as identifying medical malpractice or credit fraud. Detection of anomalies in quantitative data has received a considerable attention in the literature and has a venerable history. By contrast, and despite the widespread availability use of categorical data in practice, anomaly detection in categorical data has received relatively little attention as compared to quantitative data. This is because detection of anomalies in categorical data is a challenging problem. Some anomaly detection techniques depend on identifying a representative pattern then measuring distances between objects and this pattern. Objects that are far from this pattern are declared as anomalies. However, identifying patterns and measuring distances are not easy in categorical data compared with quantitative data. Fortunately, several papers focussing on the detection of anomalies in categorical data have been published in the recent literature. In this article, we provide a comprehensive review of the research on the anomaly detection problem in categorical data. Previous review articles focus on either the statistics literature or the machine learning and computer science literature. This review article combines both literatures. We review 36 methods for the detection of anomalies in categorical data in both literatures and classify them into 12 different categories based on the conceptual definition of anomalies they use. For each approach, we survey anomaly detection methods, and then show the similarities and differences among them. We emphasize two important issues, the number of parameters each method requires and its time complexity. The first issue is critical, because the performance of these methods are sensitive to the choice of these parameters. The time complexity is also very important in real applications especially in big data applications. We report the time complexity if it is reported by the authors of the methods. If it is not, then we derive it ourselves and report it in this article. In addition, we discuss the common problems and the future directions of the anomaly detection in categorical data.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {38},
numpages = {35},
keywords = {unsupervised learning, supervised learning, semi-supervised learning, outliers detection, novelty detection, nominal data, mixed data, intrusion detection systems, holo entropy, data mining, Shannon entropy, Computational complexity}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@article{10.1504/IJGUC.2018.090225,
title = {An improved image classification based on K-means clustering and BoW model},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {1},
issn = {1741-847X},
url = {https://doi.org/10.1504/IJGUC.2018.090225},
doi = {10.1504/IJGUC.2018.090225},
abstract = {Image classification constitutes an important issue in large-scale image data process systems based on cluster. In this context, a significant number of relying BoW models and SVM methods have been proposed for image fusion systems. Some works classified these methods into Generative Mode and Discriminative Mode. Very few works deal with a classifier based on the fusion of these modes when building an image classification system. In this paper, we propose a revised algorithm based on weighted visual dictionary of K-means cluster. First, it uses SIFT and Laplace spectrum features to cluster object respectively to get local characteristics of low dimension images sub-visual dictionary; then clusters low-dimension characteristics to get the super visual dictionaries of two features; finally, we get the visual dictionary although most of these features have been proposed for a balance role through weighting of the parent visual dictionaries. Experimental result shows that the algorithm and this model are efficient in descript image information and can provide image classification performance. It is widely used in unmanned-navigation and the machine-vision and other fields.},
journal = {Int. J. Grid Util. Comput.},
month = jan,
pages = {37–42},
numpages = {6}
}

@article{10.5555/3546258.3546455,
author = {Bouchard-C\^{o}t\'{e}, Alexandre and Roth, Andrew},
title = {Particle-Gibbs sampling for Bayesian feature allocation models},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Bayesian feature allocation models are a popular tool for modelling data with a combinatorial latent structure. Exact inference in these models is generally intractable and so practitioners typically apply Markov Chain Monte Carlo (MCMC) methods for posterior inference. The most widely used MCMC strategies rely on a single variable Gibbs update of the feature allocation matrix. These updates can be inefficient as features are typically strongly correlated. To overcome this problem we have developed a block sampler that can update an entire row of the feature allocation matrix in a single move. In the context of feature allocation models, naive block Gibbs sampling is impractical for models with a large number of features as the computational complexity scales exponentially in the number of features. We develop a Particle Gibbs (PG) sampler that targets the same distribution as the row wise Gibbs updates, but has computational complexity that only grows linearly in the number of features. We compare the performance of our proposed methods to the standard Gibbs sampler using synthetic and real data from a range of feature allocation models. Our results suggest that row wise updates using the PG methodology can significantly improve the performance of samplers for feature allocation models.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {197},
numpages = {105},
keywords = {Bayesian feature allocation, Indian buffet process, Gibbs sampler, particle Gibbs sampler, sequential monte Carlo}
}

@inproceedings{10.1007/978-3-030-32692-0_27,
author = {Liu, Jiali and Wang, Wenxuan and Guan, Tianyao and Zhao, Ningbo and Han, Xiaoguang and Li, Zhen},
title = {Ultrasound Liver Fibrosis Diagnosis Using Multi-indicator Guided Deep Neural Networks},
year = {2019},
isbn = {978-3-030-32691-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32692-0_27},
doi = {10.1007/978-3-030-32692-0_27},
abstract = {Accurate analysis of the fibrosis stage plays very important roles in follow-up of patients with chronic hepatitis B infection. In this paper, a deep learning framework is presented for automatically liver fibrosis prediction. On contrary of previous works, our approach can take use of the information provided by multiple ultrasound images. An indicator-guided learning mechanism is further proposed to ease the training of the proposed model. This follows the workflow of clinical diagnosis and make the prediction procedure interpretable. To support the training, a dataset is well-collected which contains the ultrasound videos/images, indicators and labels of 229 patients. As demonstrated in the experimental results, our proposed model shows its effectiveness by achieving the state-of-the-art performance, specifically, the accuracy is 65.6% (20% higher than previous best).},
booktitle = {Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {230–237},
numpages = {8},
keywords = {Multi-indicator, Liver fibrosis diagnosis, Ultrasound},
location = {Shenzhen, China}
}

@inproceedings{10.1007/978-3-030-62463-7_50,
author = {Zhang, Qinghui and Xu, Hongbin and Li, Zhengyu and Liu, Xiaobin and Li, Yuxi and Jiao, Yingjie},
title = {Research on Automatic Target Detection and Recognition System Based on Deep Learning Algorithm},
year = {2020},
isbn = {978-3-030-62462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62463-7_50},
doi = {10.1007/978-3-030-62463-7_50},
abstract = {Automatic target detection and recognition is the cornerstone of the intelligent unmanned systems to realize higher-level tasks. In this paper, the deep learning algorithm of Faster R-CNN was studied in depth, and the target detection model is designed combining the RPN network and the fast R-CNN. The target detection and recognition device with the ability of image acquisition and intelligent processing was also designed. Combining the device with the Faster R-CNN model, the automatic target detection and recognition system was developed. At last, the VGG-16 model was adopted for training the detection model, and the system was used for target detection experiments. The results show that the recognition accuracies of the system for the visible light images of trucks and tanks are 89.7% and 90.3%, respectively, and that for infrared images of tanks is 63.7%. Therefore, a good recognition effect has been achieved. This work provides a reference for the application of deep learning algorithms in the field of automatic target detection and recognition.},
booktitle = {Machine Learning for Cyber Security: Third International Conference, ML4CS 2020, Guangzhou, China, October 8–10, 2020, Proceedings, Part III},
pages = {538–548},
numpages = {11},
keywords = {Automatic target detection and recognition, Deep learning algorithm, Faster R-CNN},
location = {Guangzhou, China}
}

@article{10.1016/j.neunet.2019.09.014,
author = {Meng, Lei and Tan, Ah-Hwee and Miao, Chunyan},
title = {Salience-aware adaptive resonance theory for large-scale sparse data clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.09.014},
doi = {10.1016/j.neunet.2019.09.014},
journal = {Neural Netw.},
month = dec,
pages = {143–157},
numpages = {15},
keywords = {Adaptive resonance theory, Clustering, Sparse data, Subspace learning, Feature weighting, Parameter adaptation}
}

@inproceedings{10.1145/3340531.3411912,
author = {Khawar, Farhan and Hang, Xu and Tang, Ruiming and Liu, Bin and Li, Zhenguo and He, Xiuqiang},
title = {AutoFeature: Searching for Feature Interactions and Their Architectures for Click-through Rate Prediction},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411912},
doi = {10.1145/3340531.3411912},
abstract = {Click-through rate prediction is an important task in commercial recommender systems and it aims to predict the probability of a user clicking on an item. The event of a user clicking on an item is accompanied by several user and item features. As modelling the feature interactions effectively can lead to better predictions, it has been the focus of many recent approaches including deep learning-based models. However, the existing approaches either (i) model all possible feature interactions for a given order, or (ii) manually select which feature interactions to model. Besides, they use the same network structure or function to model all the feature interactions while ignoring the difference of complexity among them. To address these issues, we propose a neural architecture search based approach called AutoFeature that automatically finds essential feature interactions and selects an appropriate structure to model each of these interactions. Specifically, we first define a flexible architecture search space for the CTR prediction task which covers many popular designs such as PIN, PNN and DeepFM, etc., and enables higher-order interactions. Then we propose an efficient neural architecture search algorithm that recursively refines the search space by partitioning it into several subspaces and samples from higher quality ones. Extensive experiments on multiple CTR prediction benchmarks show the superiority of our AutoFeature over the state-of-the-art baselines. In addition, our experiments show that the learned architectures use fewer flops/parameters and hence can efficiently incorporate higher-order feature interactions. This further boosts the performance. Finally, we show that AutoFeature can find meaningful feature interactions.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {625–634},
numpages = {10},
keywords = {recommender systems, neural architecture search, feature modeling, feature generation, click-through rate prediction, CTR prediction},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.5555/2946645.3053434,
author = {Szab\'{o}, Zolt\'{a}n and Sriperumbudur, Bharath K. and P\'{o}czos, Barnab\'{a}s and Gretton, Arthur},
title = {Learning theory for distribution regression},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; G\"{a}rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5272–5311},
numpages = {40},
keywords = {two-Stage sampled distribution regression, multi-instance learning, minimax optimality, mean embedding, Kernel ridge regression}
}

@article{10.1016/j.asoc.2021.107104,
author = {Liu, Sisi and Lee, Ickjai},
title = {Sequence encoding incorporated CNN model for Email document sentiment classification},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107104},
doi = {10.1016/j.asoc.2021.107104},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {14},
keywords = {Sentiment analysis, CNN model, Sequence encoding, Graph-based position encoding}
}

@article{10.5555/3455716.3455897,
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
title = {Curriculum learning for reinforcement learning domains: a framework and survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {181},
numpages = {50},
keywords = {transfer learning, reinforcement learning, curriculum learning}
}

@inproceedings{10.1007/978-3-030-97774-0_16,
author = {Zhang, Pengyu and Wang, Chunmei and Hao, Baocong and Hu, Wenhui and Liu, Xueyang and Sun, Lizhuang},
title = {An SG-CIM Model Table Classification Method Based on Multi Feature Semantic Recognition Technology},
year = {2022},
isbn = {978-3-030-97773-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-97774-0_16},
doi = {10.1007/978-3-030-97774-0_16},
abstract = {In this paper, based on the SG-CIM model Knowledge Graph, we introduce the semantic recognition technology in Natural Language Processing to model the information contained in the graph and mine the semantic information with multiple features according to the characteristics of the data itself. For the problem of graph domain name-related attribute complementation, this paper adopts a multi-feature semantic recognition approach to classify the given SG-CIM model table by domain name. We propose an ATT-ALE-TextRNN model for the descriptive features of the table, adding N times second-level domain name embeddings to the basic TextRNN and calculating the attention score together to capture the tendency of different contextual information for a given category. In this paper, with reference to the multidimensional discrete feature classification problem of the recommender system, an improved DeepFM model is proposed for the table discrete, class-forming features. It facilitates the discovery of semantic dependencies between class features, makes the feature distribution more diverse, and avoids the problems of low repetition between multidimensional features and low performance of combined computation. By combining the above two models, this paper achieves more accurate mining of multi-feature semantics and accurate classification of topic domains.},
booktitle = {Smart Computing and Communication: 6th International Conference, SmartCom 2021, New York City, NY, USA, December 29–31, 2021, Proceedings},
pages = {175–185},
numpages = {11},
keywords = {Semantic recognition, Classification, Attention network, Deep learning},
location = {New York, NY, USA}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.knosys.2019.105185,
author = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Xie, Shengli and Su, Chun-Yi},
title = {Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105185},
doi = {10.1016/j.knosys.2019.105185},
journal = {Know.-Based Syst.},
month = feb,
numpages = {10},
keywords = {Non-negative matrix factorization, Multi-view clustering, Semi-supervised learning, Graph-regularization}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Rough Set, Naive Bayes, K-NN, DNA, Apriori Algorithm}
}

@inproceedings{10.1145/3136014.3136038,
author = {Schauss, Simon and L\"{a}mmel, Ralf and H\"{a}rtel, Johannes and Heinz, Marcel and Klein, Kevin and H\"{a}rtel, Lukas and Berger, Thorsten},
title = {A chrestomathy of DSL implementations},
year = {2017},
isbn = {9781450355254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136014.3136038},
doi = {10.1145/3136014.3136038},
abstract = {Selecting and properly using approaches for DSL implementation can be challenging, given their variety and complexity. To support developers, we present the software chrestomathy MetaLib, a well-organized and well-documented collection of DSL implementations useful for learning. We focus on basic metaprogramming techniques for implementing DSL syntax and semantics. The DSL implementations are organized and enhanced by feature modeling, semantic annotation, and model-based documentation. The chrestomathy enables side-by-side exploration of different implementation approaches for DSLs. Source code, feature model, feature configurations, semantic annotations, and documentation are publicly available online, explorable through a web application, and maintained by a collaborative process.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {103–114},
numpages = {12},
keywords = {DSL implementation. Metaprogramming. Software chrestomathy. Learning. MetaLib. Feature modeling. Model-based documentation},
location = {Vancouver, BC, Canada},
series = {SLE 2017}
}

@article{10.5555/2946645.2946709,
author = {Adi, Yossi and Keshet, Joseph},
title = {StructED: risk minimization in structured prediction},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents STRUCTED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from http://adiyoss.github.io/StructED/.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2282–2286},
numpages = {5},
keywords = {structured prediction, structural SVM, direct loss minimization, CRF}
}

@inproceedings{10.5555/3524938.3525259,
author = {Geng, Sinong and Nassif, Houssam and Manzanares, Carlos A. and Reppen, A. Max and Sircar, Ronnie},
title = {Deep PQR: solving inverse reinforcement learning using anchor actions},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the Q- function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {321},
numpages = {11},
series = {ICML'20}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {hinge loss minimization, linear regression, curriculum learning}
}

@article{10.1016/j.cose.2021.102320,
author = {Nguyen, Quoc-Th\^{o}ng and Mai, An and Chagas, Lionel and Reverdy-Bruas, Nad\`{e}ge},
title = {Microscopic printing analysis and application for classification of source printer},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {108},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102320},
doi = {10.1016/j.cose.2021.102320},
journal = {Comput. Secur.},
month = sep,
numpages = {12},
keywords = {Microscopic printing, Source printer identification, Printer forensics, Documentauthentication, Support vector machine}
}

@inproceedings{10.1007/978-3-030-55789-8_59,
author = {Abeyrathna, Kuruge Darshana and Granmo, Ole-Christoffer and Goodwin, Morten},
title = {Integer Weighted Regression Tsetlin Machines},
year = {2020},
isbn = {978-3-030-55788-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55789-8_59},
doi = {10.1007/978-3-030-55789-8_59},
abstract = {The Regression Tsetlin Machine (RTM) addresses the lack of interpretability impeding state-of-the-art nonlinear regression models. It does this by using conjunctive clauses in propositional logic to capture the underlying non-linear frequent patterns in the data. These, in turn, are combined into a continuous output through summation, akin to a linear regression function, however, with non-linear components and binary weights. However, the resolution of the RTM output is proportional to the number of clauses employed. This means that computation cost increases with resolution. To address this problem, we here introduce integer weighted RTM clauses. Our integer weighted clause is a compact representation of multiple clauses that capture the same sub-pattern—w repeating clauses are turned into one, with an integer weight w. This reduces computation cost w times, and increases interpretability through a sparser representation. We introduce a novel learning scheme, based on so-called stochastic searching on the line. We evaluate the potential of the integer weighted RTM empirically using two artificial datasets. The results show that the integer weighted RTM is able to acquire on par or better accuracy using significantly less computational resources compared to regular RTM and an RTM with real-valued weights.},
booktitle = {Trends in Artificial Intelligence Theory and Applications. Artificial Intelligence Practices: 33rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2020, Kitakyushu, Japan, September 22-25, 2020, Proceedings},
pages = {686–694},
numpages = {9},
keywords = {Stochastic searching on the line, Interpretable machine learning, Weighted tsetlin machines, Regression tsetlin machines, Tsetlin machines},
location = {Kitakyushu, Japan}
}

@inproceedings{10.1145/3411763.3451651,
author = {Wei, Datong and Yang, Chaofan and Zhang, Xiaolong (Luke) and Yuan, Xiaoru},
title = {Predicting Mouse Click Position Using Long Short-Term Memory Model Trained by Joint Loss Function},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451651},
doi = {10.1145/3411763.3451651},
abstract = {Knowing where users might click in advance can potentially improve the efficiency of user interaction in desktop user interfaces. In this paper, we propose a machine learning approach to predict mouse click location. Our model, which is LSTM (long short-term memory)-based and trained by joint supervision, can predict the rectangular region of mouse click with feeding mouse trajectories on the fly. Experiment results show that our model can achieve a result of a predicted rectangle area of 58 \texttimes{} 79 pixels with 92% accuracy, and reduce prediction error when compared with other state-of-the-art prediction methods using a multi-user dataset.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {214},
numpages = {6},
keywords = {Machine Learning, Mouse Interaction, Mouse Prediction, User Intention},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1007/978-3-030-80599-9_23,
author = {Krishnan, Gokul S. and Sowmya Kamath, S. and Sugumaran, Vijayan},
title = {Predicting Vaccine Hesitancy and Vaccine Sentiment Using Topic Modeling and&nbsp;Evolutionary Optimization},
year = {2021},
isbn = {978-3-030-80598-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-80599-9_23},
doi = {10.1007/978-3-030-80599-9_23},
abstract = {The ongoing COVID-19 pandemic has posed serious threats to the world population, affecting over 219 countries with a staggering impact of over 162 million cases and 3.36 million casualties. With the availability of multiple vaccines across the globe, framing vaccination policies for effectively inoculating a country’s population against such diseases is currently a crucial task for public health agencies. Social network users post their views and opinions on vaccines publicly and these posts can be put to good use in identifying vaccine hesitancy. In this paper, a vaccine hesitancy identification approach is proposed, built on novel text feature modeling based on evolutionary computation and topic modeling. The proposed approach was experimentally validated on two standard tweet datasets – the flu vaccine dataset and UK COVID-19 vaccine tweets. On the first dataset, the proposed approach outperformed the state-of-the-art in terms of standard metrics. The proposed model was also evaluated on the UKCOVID dataset and the results are presented in this paper, as our work is the first to benchmark a vaccine hesitancy model on this dataset.},
booktitle = {Natural Language Processing and Information Systems: 26th International Conference on Applications of Natural Language to Information Systems, NLDB 2021, Saarbr\"{u}cken, Germany, June 23–25, 2021, Proceedings},
pages = {255–263},
numpages = {9},
keywords = {Evolutionary computation, Machine learning, Natural language processing, Population health analytics, Topic modeling},
location = {Saarbr\"{u}cken, Germany}
}

@inproceedings{10.5555/3524938.3524952,
author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
title = {Learning what to defer for maximum independent sets},
year = {2020},
publisher = {JMLR.org},
abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {14},
numpages = {11},
series = {ICML'20}
}

@article{10.1155/2019/8127869,
author = {Zhu, Qi and Yuan, Ning and Guan, Donghai and Deng, Ke},
title = {Cognitive Driven Multilayer Self-Paced Learning with Misclassified Samples},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1076-2787},
url = {https://doi.org/10.1155/2019/8127869},
doi = {10.1155/2019/8127869},
abstract = {In recent years, self-paced learning (SPL) has attracted much attention due to its improvement to nonconvex optimization based machine learning algorithms. As a methodology introduced from human learning, SPL dynamically evaluates the learning difficulty of each sample and provides the weighted learning model against the negative effects from hard-learning samples. In this study, we proposed a cognitive driven SPL method, i.e., retrospective robust self-paced learning (R2SPL), which is inspired by the following two issues in human learning process: the misclassified samples are more impressive in upcoming learning, and the model of the follow-up learning process based on large number of samples can be used to reduce the risk of poor generalization in initial learning phase. We simultaneously estimated the degrees of learning-difficulty and misclassified in each step of SPL and proposed a framework to construct multilevel SPL for improving the robustness of the initial learning phase of SPL. The proposed method can be viewed as a multilayer model and the output of the previous layer can guide constructing robust initialization model of the next layer. The experimental results show that the R2SPL outperforms the conventional self-paced learning models in classification task.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {reference architecture, multi product line, cross-domain reference architecture, actor, Software product line},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Multi-objective optimization, Evolutionary algorithm, Software product line, Optimal feature selection}
}

@inproceedings{10.5555/3524938.3525116,
author = {Choo, Davin and Grunau, Christoph and Portmann, Julian and Rozho\v{n}, V\'{a}clav},
title = {k-means++: few more steps yield constant approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k)-approximation in expectation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant ε &gt; 0, with only εk additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {178},
numpages = {9},
series = {ICML'20}
}

@inproceedings{10.1609/aaai.v33i01.33015725,
author = {Zhang, Biqiao and Kong, Yuqing and Essl, Georg and Provost, Emily Mower},
title = {undefined-similarity preservation loss for soft labels: a demonstration on cross-corpus speech emotion recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015725},
doi = {10.1609/aaai.v33i01.33015725},
abstract = {In this paper, we propose a Deep Metric Learning (DML) approach that supports soft labels. DML seeks to learn representations that encode the similarity between examples through deep neural networks. DML generally presupposes that data can be divided into discrete classes using hard labels. However, some tasks, such as our exemplary domain of speech emotion recognition (SER), work with inherently subjective data, data for which it may not be possible to identify a single hard label. We propose a family of loss functions, undefined-Similarity Preservation Loss (undefined-SPL), based on the dual form of undefined-divergence for DML with soft labels. We show that the minimizer of undefined-SPL preserves the pairwise label similarities in the learned feature embeddings. We demonstrate the efficacy of the proposed loss function on the task of cross-corpus SER with soft labels. Our approach, which combines undefined-SPL and classification loss, significantly outperforms a baseline SER system with the same structure but trained with only classification loss in most experiments. We show that the presented techniques are more robust to over-training and can learn an embedding space in which the similarity between examples is meaningful.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {702},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.ins.2021.04.056,
author = {Meng, Xuying and Wang, Suhang and Liang, Zhimin and Yao, Di and Zhou, Jihua and Zhang, Yujun},
title = {Semi-supervised anomaly detection in dynamic communication networks},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {571},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.04.056},
doi = {10.1016/j.ins.2021.04.056},
journal = {Inf. Sci.},
month = sep,
pages = {527–542},
numpages = {16},
keywords = {Anomaly detection, Semi-supervised learning, Generative adversarial networks, Self-learning}
}

@inproceedings{10.1007/978-3-319-24888-2_3,
author = {Ma, Guangkai and Gao, Yaozong and Wang, Li and Wu, Ligang and Shen, Dinggang},
title = {Soft-Split Random Forest for Anatomy Labeling},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_3},
doi = {10.1007/978-3-319-24888-2_3},
abstract = {Random Forest (RF) has been widely used in the learning-based labeling. In RF, each sample is directed from the root to each leaf based on the decisions made in the interior nodes, also called splitting nodes. The splitting nodes assign a testing sample to either left or right child based on the learned splitting function. The final prediction is determined as the average of label probability distributions stored in all arrived leaf nodes. For ambiguous testing samples, which often lie near the splitting boundaries, the conventional splitting function, also referred to as hard split function, tends to make wrong assignments, hence leading to wrong predictions. To overcome this limitation, we propose a novel soft-split random forest (SSRF) framework to improve the reliability of node splitting and finally the accuracy of classification. Specifically, a soft split function is employed to assign a testing sample into both left and right child nodes with their certain probabilities, which can effectively reduce influence of the wrong node assignment on the prediction accuracy. As a result, each testing sample can arrive at multiple leaf nodes, and their respective results can be fused to obtain the final prediction according to the weights accumulated along the path from the root node to each leaf node. Besides, considering the importance of context information, we also adopt a Haar-features based context model to iteratively refine the classification map. We have comprehensively evaluated our method on two public datasets, respectively, for labeling hippocampus in MR images and also labeling three organs in Head &amp; Neck CT images. Compared with the hard-split RF (HSRF), our method achieved a notable improvement in labeling accuracy.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {17–25},
numpages = {9},
location = {Munich, Germany}
}

@article{10.1007/s00034-021-01657-1,
author = {Pravin, Sheena Christabel and Palanivelan, M.},
title = {A Hybrid Deep Ensemble for Speech Disfluency Classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01657-1},
doi = {10.1007/s00034-021-01657-1},
abstract = {In this paper, a novel Hybrid Deep Ensemble (HDE) is proposed for automatic speech disfluency classification on a sparse speech dataset. Categorizations of speech disfluencies for diagnosis of speech disorders have so long relied on sophisticated deep learning models. Such a task can be accomplished by a straightforward approach with high accuracy by the proposed model which is an optimal combination of diverse machine learning and deep learning algorithms in a hierarchical arrangement which includes a deep autoencoder that yields the compressed latent features. The proposed model has shown considerable improvement in downgrading processing time overcoming the issues of cumbersome hyper-parameter tuning and huge data demand of the deep learning algorithms with high classification accuracy. Experimental results show that the proposed Hybrid Deep Ensemble has superior performance compared to the individual base learners, and the deep neural network as well. The proposed model and the baseline models were evaluated in terms of Cohen’s kappa coefficient, Hamming loss, Jaccard score, F-score and classification accuracy.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3968–3995},
numpages = {28},
keywords = {Latent features, Deep autoencoder, Sparse speech dataset, Speech disfluency classification, Hybrid Deep Ensemble}
}

@inproceedings{10.1145/2110147.2110161,
author = {Lienhardt, Michael and Clarke, Dave},
title = {Row types for delta-oriented programming},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110161},
doi = {10.1145/2110147.2110161},
abstract = {Delta-oriented programming (DOP) provides a technique for implementing Software Product Lines based on modifications (add, remove, modify) to a core program. Unfortunately, such modifications can introduce errors into a program, especially when type signatures of classes are modified in a non-monotonic fashion. To deal with this problem we present a type system for delta-oriented programs based on row polymorphism. This exercise elucidates the close correspondence between delta-oriented programs and row polymorphism.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {121–128},
numpages = {8},
keywords = {structural typing, software product line engineering, delta-oriented programming},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Clustering, Process discovery, Process mining, Configuration workflow, Variability}
}

@article{10.1007/s00521-018-3478-1,
author = {Gu, Nannan and Fan, Pengying and Fan, Mingyu and Wang, Di},
title = {Structure regularized self-paced learning for robust semi-supervised pattern classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3478-1},
doi = {10.1007/s00521-018-3478-1},
abstract = {Semi-supervised classification is a hot topic in pattern recognition and machine learning. However, in presence of heavy noise and outliers, the unlabeled training data could be very challenging or even misleading for the semi-supervised classifier. In this paper, we propose a novel structure regularized self-paced learning method for semi-supervised classification problems, which can efficiently learn partially labeled training data sequentially from the simple to the complex ones. The proposed formulation consists of three components: a cost function defined by a mixture of losses, a functional complexity regularizer, and a self-paced regularizer; and the corresponding optimization algorithm involves three iterative steps: classifier updating, sample importance calculating, and pseudo-labeling. In the proposed method, the cost function for classifier updating and sample importance calculating is defined as a combination of the label fitting loss and manifold smoothness loss. Then, the importance of the pseudo-labeled and unlabeled samples is adaptively calculated by the novel cost. Unlabeled samples with high importance values are pseudo-labeled with their current predictions. In this way, labels are efficiently propagated from the labeled samples to the unlabeled ones in the robust self-paced manner. Experimental results on several benchmark data sets are provided to show the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6559–6574},
numpages = {16},
keywords = {Locally linear coding, Manifold learning, Self-paced learning, Pattern classification, Semi-supervised classification}
}

@article{10.1016/j.ins.2019.09.055,
author = {Bu, Seok-Jun and Cho, Sung-Bae},
title = {A convolutional neural-based learning classifier system for detecting database intrusion via insider attack},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {512},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.09.055},
doi = {10.1016/j.ins.2019.09.055},
journal = {Inf. Sci.},
month = feb,
pages = {123–136},
numpages = {14},
keywords = {Deep learning, Convolutional neural network, Learning classifier system, Database intrusion detection}
}

@article{10.1016/j.neunet.2021.03.022,
author = {Zhong, Yongjian and Du, Bo and Xu, Chang},
title = {Learning to reweight examples in multi-label classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.03.022},
doi = {10.1016/j.neunet.2021.03.022},
journal = {Neural Netw.},
month = oct,
pages = {428–436},
numpages = {9},
keywords = {Reweight instance, Self-paced learning, Multi-label classification}
}

@article{10.1016/j.artmed.2021.102165,
author = {de Siqueira, Vilson Soares and Borges, Mois\'{e}s Marcos and Furtado, Rog\'{e}rio Gomes and Dourado, Colandy Nunes and da Costa, Ronaldo Martins},
title = {Artificial intelligence applied to support medical decisions for the automatic analysis of echocardiogram images: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102165},
doi = {10.1016/j.artmed.2021.102165},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {19},
keywords = {Deep Learning, Machine Learning, Echocardiography, Echocardiogram}
}

@article{10.1007/s11042-020-10443-1,
author = {Rao, Champakamala Sundar and Karunakara, K.},
title = {A comprehensive review on brain tumor segmentation and classification of MRI images},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10443-1},
doi = {10.1007/s11042-020-10443-1},
abstract = {In the analysis of medical images, one of the challenging tasks is the recognition of brain tumours via medical resonance images (MRIs). The diagnosis process is still tedious due to its complexity and considerable variety in tissues of tumor perception. Therefore, the necessities of tumor identification techniques are improving nowadays for medical applications. In the past decades, different approaches in the segmentation of various precisions and complexity degree have been accomplished, which depends on the simplicity and the benchmark of the technique. An overview of this analysis is to give out the summary of the semi-automatic techniques for brain tumor segmentation and classification utilizing MRI. An enormous amount of MRI based image data is accomplished using deep learning approaches. There are several works, dealing on the conventional approaches for MRI-based segmentation of brain tumor. Alternatively, in this review, we revealed the latest trends in the methods of deep learning. Initially, we explain the several threads in MRI pre-processing, including registration of image, rectification of bias field, and non-brain tissue dismissal. And terminally, the present state evaluation of algorithm is offered and forecasting the growths to systematise the MRI-based brain tumor into a regular cyclic routine in the clinical field are focussed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17611–17643},
numpages = {33},
keywords = {Image processing, Tissue, Bias field, Segmentation, Brain tumor, MRI}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Valence, Speech elicitation, Machine learning, Paralinguistics, Digital medicine, Digital phenotyping}
}

@inproceedings{10.1007/978-3-030-32692-0_49,
author = {Peng, Shiqi and Lai, Bolin and Yao, Guangyu and Zhang, Xiaoyun and Zhang, Ya and Wang, Yan-Feng and Zhao, Hui},
title = {Learning-Based Bone Quality Classification Method for Spinal Metastasis},
year = {2019},
isbn = {978-3-030-32691-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32692-0_49},
doi = {10.1007/978-3-030-32692-0_49},
abstract = {Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by +12.54%, +7.23% and +29.06% for blastic, mixed and lytic lesions, respectively, meanwhile +12.33%, +23.21% and +34.25% at vertebrae level.},
booktitle = {Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {426–434},
numpages = {9},
keywords = {Spinal metastasis, Bone quality classification, Multi-task learning, Self-paced learning},
location = {Shenzhen, China}
}

@article{10.1155/2021/4327896,
author = {Xie, Shu-Tong and He, Zong-Bao and Chen, Qiong and Chen, Rong-Xin and Kong, Qing-Zhao and Song, Cun-Ying and Huang, Jiwei},
title = {Predicting Learning Behavior Using Log Data in Blended Teaching},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4327896},
doi = {10.1155/2021/4327896},
abstract = {Online and offline blended teaching mode, the future trend of higher education, has recently been widely used in colleges around the globe. In the article, we conducted a study on students’ learning behavior analysis and student performance prediction based on the data about students’ behavior logs in three consecutive years of blended teaching in a college’s “Java Language Programming” course. Firstly, the data from diverse platforms such as MOOC, Rain Classroom, PTA, and cnBlog are integrated and preprocessed. Secondly, a novel multiclass classification framework, combining the genetic algorithm (GA) and the error correcting output codes (ECOC) method, is developed to predict the grade levels of students. In the framework, GA is designed to realize both the feature selection and binary classifier selection to fit the ECOC models. Finally, key factors affecting grades are identified in line with the optimal subset of features selected by GA, which can be analyzed for teaching significance. The results show that the multiclass classification algorithm designed in this article can effectively predict grades compared with other algorithms. In addition, the selected subset of features corresponding to learning behaviors is pedagogically instructive.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-86380-7_23,
author = {Krysi\'{n}ska, Izabela and Morzy, Miko\l{}aj and Kajdanowicz, Tomasz},
title = {Curriculum Learning Revisited: Incremental Batch Learning with Instance Typicality Ranking},
year = {2021},
isbn = {978-3-030-86379-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86380-7_23},
doi = {10.1007/978-3-030-86380-7_23},
abstract = {The technique of curriculum learning mimics cognitive mechanisms observed in human learning, where simpler concepts are presented prior to gradual introduction of more difficult concepts. Until now, the major obstacle for curriculum methods was the lack of a reliable method for estimating the difficulty of training instances. In this paper we show that, instead of trying to assess the difficulty of learning instances, a simple graph-based method of computing the typicality of instances can be used in conjunction with curriculum methods. We design new batch schedulers which organize ordered instances into batches of varying size and learning difficulty. Our method does not require any changes to the architecture of trained models, we improve the training merely by manipulating the order and frequency of instance presentation to the model.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part IV},
pages = {279–291},
numpages = {13},
keywords = {Batch training, Typicality, Curriculum learning},
location = {Bratislava, Slovakia}
}

@inproceedings{10.1007/978-3-030-90439-5_26,
author = {Lakshya},
title = {Behaviour of Sample Selection Techniques Under Explicit Regularization},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_26},
doi = {10.1007/978-3-030-90439-5_26},
abstract = {There is a multitude of sample selection-based learning strategies that have been developed for learning with noisy labels. However, It has also been indicated in the literature that perhaps early stopping is better than fully training the model for getting better performance. It leads us to wonder about the behavior of the sample selection strategies under explicit regularization. To this end, we considered four of the most fundamental sample selection-based models MentorNet, Coteaching, Coteaching-plus and JoCor. We provide empirical results of applying explicit L2 regularization to the above-mentioned approaches. We also compared the results with a baseline - a vanilla CNN model trained with just regularization. We show that under explicit regularization, the pre-conceived ranking of the approaches might change. We also show several instances where the baseline was able to outperform some or all of the existing approaches. Moreover, we show that under explicit regularization, the performance gap between the approaches can also reduce.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {331–340},
numpages = {10}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Network Intrusion Detection, Multiple Kernel Learning, Multiclass Classification, Machine Learning, Kernel Selection, KDD Cup 1999, Fractional Polynomial Kernels, Extreme Learning Machine, Ensemble Learning, Cyber security, Adaptive Boosting}
}

@article{10.1007/s10845-020-01550-9,
author = {Ning, Fangwei and Shi, Yan and Cai, Maolin and Xu, Weiqing},
title = {Various realization methods of machine-part classification based on deep learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {8},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01550-9},
doi = {10.1007/s10845-020-01550-9},
abstract = {Parts classification can improve the efficacy of the manufacturing process in a computer-aided process planning system. In this study, we investigate various methodologies to assist with parts classification based on deep learning technologies, including a two-dimensional convolutional neural network (2D-CNN) trained using both picture data and CSV files; and a three-dimensional convolutional neural network (3D-CNN) trained using voxel data. Additionally, two novel methods are proposed: (1) feature recognition for the processing parts based on syntactic patterns, where their feature quantities are computed and saved to comma-separated variable (CSV) files that are subsequently employed to train the 2D-CNN model; and (2) voxelization of parts, wherein the voxel data of the parts is obtained for training the 3D-CNN model. The two methods are compared with a 2D-CNN model trained with the images of parts to classify. Results indicated that the 2D-CNN model trained with CSV data yielded the best performance and highest accuracy, followed by the 3D-CNN model, which was simpler and easier to implement and utilized better learning ability for the parts’ details. The 2D-CNN model trained with picture files evinced the lowest accuracy and a complex training network.},
journal = {J. Intell. Manuf.},
month = dec,
pages = {2019–2032},
numpages = {14},
keywords = {Artificial intelligence, Artificial neural networks, Feature extraction, Industrial informatics, Optimized production technology, Classification algorithms, Image classification}
}

@article{10.1007/s10462-020-09904-8,
author = {Pareek, Preksha and Thakkar, Ankit},
title = {A survey on video-based Human Action Recognition: recent updates, datasets, challenges, and applications},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {3},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09904-8},
doi = {10.1007/s10462-020-09904-8},
abstract = {Human Action Recognition (HAR) involves human activity monitoring task in different areas of medical, education, entertainment, visual surveillance, video retrieval, as well as abnormal activity identification, to name a few. Due to an increase in the usage of cameras, automated systems are in demand for the classification of such activities using computationally intelligent techniques such as Machine Learning (ML) and Deep Learning (DL). In this survey, we have discussed various ML and DL techniques for HAR for the years 2011–2019. The paper discusses the characteristics of public datasets used for HAR. It also presents a survey of various action recognition techniques along with the HAR applications namely, content-based video summarization, human–computer interaction, education, healthcare, video surveillance, abnormal activity detection, sports, and entertainment. The advantages and disadvantages of action representation, dimensionality reduction, and action analysis methods are also provided. The paper discusses challenges and future directions for HAR.},
journal = {Artif. Intell. Rev.},
month = mar,
pages = {2259–2322},
numpages = {64},
keywords = {Human Action Recognition (HAR), Machine Learning (ML), Deep Learning (DL), Challenges in HAR, Public Datasets for HAR, Future directions}
}

@inproceedings{10.5555/3524938.3525166,
author = {Degenne, R\'{e}my and Shao, Han and Koolen, Wouter M.},
title = {Structure adaptive algorithms for stochastic bandits},
year = {2020},
publisher = {JMLR.org},
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the suboptimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {228},
numpages = {10},
series = {ICML'20}
}

@article{10.1007/s10579-020-09527-z,
author = {China Bhanja, Chuya and Laskar, Mohammad Azharuddin and Laskar, Rabul Hussain},
title = {Modelling multi-level prosody and spectral features using deep neural network for an automatic tonal and non-tonal pre-classification-based Indian language identification system},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {55},
number = {3},
issn = {1574-020X},
url = {https://doi.org/10.1007/s10579-020-09527-z},
doi = {10.1007/s10579-020-09527-z},
abstract = {In this paper an attempt has been made to prepare an automatic tonal and non-tonal pre-classification-based Indian language identification (LID) system using multi-level prosody and spectral features. Languages are first categorized into tonal and non-tonal groups, and then, from among the languages of the respective groups, individual languages are identified. The system uses syllable, word (tri-syllable) and phrase level (multi-word) prosody (collectively called multi-level prosody) along with spectral features, namely Mel-frequency cepstral coefficients (MFCCs), Mean Hilbert envelope coefficients (MHEC), and shifted delta cepstral coefficients of MFCCs and MHECs for the pre-classification task. Multi-level analysis of spectral features has also been proposed and the complementarity of the syllable, word and phrase level (spectral + prosody) has been examined for pre-classification-based LID task. Four different models, particularly, Gaussian Mixture Model (GMM)-Universal Background Model (UBM), Artificial Neural Network (ANN), i-vector based support vector machine (SVM) and Deep Neural Network (DNN) have been developed to identify the languages. Experiments have been carried out on National Institute of Technology Silchar language database (NITS-LD) and OGI Multi-language Telephone Speech corpus (OGI-MLTS). The experiments confirm that both prosody and (spectral + prosody) obtained from syllable-, word- and phrase-level carry complementary information for pre-classification-based LID task. At the pre-classification stage, DNN models based on multi-level (prosody + MFCC) features, coupled with score combination technique results in the lowest EER value of 9.6% for NITS-LD. For OGI-MLTS database, the lowest EER value of 10.2% is observed for multi-level (prosody + MHEC). The pre-classification module helps to improve the performance of baseline single-stage LID system by 3.2% and 4.2% for NITS-LD and OGI-MLTS database respectively.},
journal = {Lang. Resour. Eval.},
month = sep,
pages = {689–730},
numpages = {42},
keywords = {Tonal and non-tonal languages, Multi-level analysis, Prosody and spectral features, Databases, Classifiers}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1016/j.neucom.2019.04.066,
author = {Zhu, Qi and Yuan, Ning and Huang, Jiashuang and Hao, Xiaoke and Zhang, Daoqiang},
title = {Multi-modal AD classification via self-paced latent correlation analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.066},
doi = {10.1016/j.neucom.2019.04.066},
journal = {Neurocomput.},
month = aug,
pages = {143–154},
numpages = {12},
keywords = {Computer-aided diagnosis, Self-paced learning, Low-rank, Feature extraction, Multi-modal fusion}
}

@article{10.1016/j.cose.2018.12.005,
author = {Yadav, Ram Mahesh},
title = {Effective analysis of malware detection in cloud computing},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {83},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2018.12.005},
doi = {10.1016/j.cose.2018.12.005},
journal = {Comput. Secur.},
month = jun,
pages = {14–21},
numpages = {8},
keywords = {Malware detection, Weighted Fuzzy C-means clustering, Auto associative neural network, Host based antivirus, Cloud computing, Machine learning, Classifiers}
}

@inproceedings{10.1007/978-3-030-85462-1_4,
author = {Li, Xiang and Liu, Wenbing and Chen, Qun},
title = {Intelligent Extraction Method of Inertial Navigation Trajectory Behavior Features Considering Road Environment},
year = {2021},
isbn = {978-3-030-85461-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-85462-1_4},
doi = {10.1007/978-3-030-85462-1_4},
abstract = {Inertial navigation systems play an important role in areas without satellite signals. However, the inertial navigation error changes over time. Therefore, it is essential to use external data sources to correct the trajectory data, and feature extraction is required for error correction. At present, the inertial navigation trajectory consists primarily of track points and segments, with relatively simple features, no time-series data, and no spatial attributes, such as elevation and speed. In this paper, a convolutional neural network (CNN) is established based on deep learning theory and the rich behavioral features of the inertial navigation trajectory. A feature classification model based on the CNN and gated recurrent unit (GRU) is proposed to extract features from the real-time inertial navigation trajectory. First, preprocessing of the vehicle data obtained by the inertial navigation system is performed to filter redundant and invalid data. Subsequently, the trajectory features are categorized according to the motion trend and time-series information. The trend feature vector CNN and the time-series feature vector are used as inputs to the CNN and GRU, respectively, and the trajectory model with the behavioral features is established. Finally, the long short-term memory (LSTM) model, which is prone to overfitting when many parameters are used, is improved, and the two feature vectors are input into the CNN model and GRU model for data fusion to ex-tract the behavioral features. A typical inertial navigation trajectory dataset of the Miyun area in Beijing in December 2015 is used to extract the behavioral features. The experimental results are compared with traditional feature ex-traction and neural network classification methods. The results show that the proposed method outperforms the other methods, with a feature extraction ac-curacy of 91.44%. The method shows excellent performance for extracting the behavioral features of the inertial navigation trajectory in a region with variable elevation and velocity.},
booktitle = {Spatial Data and Intelligence: Second International Conference, SpatialDI 2021, Hangzhou, China, April 22–24, 2021, Proceedings},
pages = {43–56},
numpages = {14},
keywords = {Inertial navigation system, Trajectory behavior feature, Feature vector, Deep learning, Blending feature neural network, Feature extraction},
location = {Hangzhou, China}
}

@inproceedings{10.5555/3524938.3525504,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Extrapolation for large-batch training in deep learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {566},
numpages = {11},
series = {ICML'20}
}

@article{10.1016/j.aei.2021.101416,
author = {Li, Ren and Mo, Tianjin and Yang, Jianxi and Li, Dong and Jiang, Shixin and Wang, Di},
title = {Bridge inspection named entity recognition via BERT and lexicon augmented machine reading comprehension neural model},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2021.101416},
doi = {10.1016/j.aei.2021.101416},
journal = {Adv. Eng. Inform.},
month = oct,
numpages = {11},
keywords = {Bridge inspection, Named entity recognition, Machine reading comprehension, BERT}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.5555/3524938.3525278,
author = {Gopi, Sivakanth and Gulhane, Pankaj and Kulkarni, Janardhan and Shen, Judy Hanwen and Shokouhi, Milad and Yekhanin, Sergey},
title = {Differentially private set union},
year = {2020},
publisher = {JMLR.org},
abstract = {We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe U of items, possibly of infinite size, and a database D of users. Each user i contributes a subset Wi ⊆ U of items. We want an (ε,δ)-differentially private Algorithm which outputs a subset S ⊂ UiWi such that the size of S is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, n-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {340},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Learning with privileged information, Self-paced learning, Curriculum learning}
}

@article{10.1007/s10845-021-01827-7,
author = {Ning, Fangwei and Shi, Yan and Cai, Maolin and Xu, Weiqing},
title = {Part machining feature recognition based on a deep learning method},
year = {2021},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {2},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01827-7},
doi = {10.1007/s10845-021-01827-7},
abstract = {Machining feature recognition is a key step in computer-aided process planning to improve the level of design and manufacturing, production efficiency, and competitiveness. Although the traditional feature recognition method using a graph-based approach has advantages in feature logic expression, the calculation process is inefficient. Deep learning is a new technology that can automatically learn complex mapping relationships and high-level data features from a large amount of data. Therefore, this classification technology has been successfully and widely used in various fields. This study examined a three-dimensional convolutional neural network combined with a graph-based approach, taking advantage of deep learning technology and traditional feature recognition methods. First, the convex and concave machining features of a part were determined using an attributed adjacency graph. Then, the machining features were separated using the bounding box method and voxelized. Subsequently, a stretching and zooming method was proposed to obtain the training data. After training, the test and comparison results demonstrated the high accuracy rate of the proposed method and the improvement in recognition efficiency. The proposed method could also identify convex features, which further improved the recognition range.},
journal = {J. Intell. Manuf.},
month = sep,
pages = {809–821},
numpages = {13},
keywords = {Deep learning, Feature recognition, STEP, Convolution neural network, CAPP}
}

@inproceedings{10.1145/3078971.3079003,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079003},
doi = {10.1145/3078971.3079003},
abstract = {Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {32–40},
numpages = {9},
keywords = {webly-supervised learning, web label, video understanding, prior knowledge, noisy data, concept detection, big data},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@article{10.1016/j.patcog.2019.107173,
author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
title = {Unsupervised domain adaptive re-identification: Theory and practice},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107173},
doi = {10.1016/j.patcog.2019.107173},
journal = {Pattern Recogn.},
month = jun,
numpages = {11},
keywords = {Unsupervised domain adaptation, Person re-identification}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Cl\'{e}ment and Mann, Zolt\'{a}n \'{A}d\'{a}m and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Cloud service, Feature model, Reinforcement learning, Adaptation},
location = {Dubai, United Arab Emirates}
}

@article{10.1016/j.ins.2019.12.015,
author = {Xiao, Yanshan and Yang, Xiaozhou and Liu, Bo},
title = {A new self-paced method for multiple instance boosting learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {515},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.015},
doi = {10.1016/j.ins.2019.12.015},
journal = {Inf. Sci.},
month = apr,
pages = {80–90},
numpages = {11},
keywords = {Self-Paced learning, Multiple instance boost learning, Multiple instance learning}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Unlabeled data, Recurring concept drift, Ensemble learning, Data stream classification}
}

@article{10.1016/j.eswa.2021.115218,
author = {Serrano-P\'{e}rez, Jonathan and Sucar, L. Enrique},
title = {Artificial datasets for hierarchical classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115218},
doi = {10.1016/j.eswa.2021.115218},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Evaluation, Hierarchical classification, Artificial datasets}
}

@inproceedings{10.1007/978-3-030-73197-7_29,
author = {Du, Yuntao and Chen, Yinghao and Cui, Fengli and Zhang, Xiaowen and Wang, Chongjun},
title = {Cross-Domain Error Minimization for Unsupervised Domain Adaptation},
year = {2021},
isbn = {978-3-030-73196-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73197-7_29},
doi = {10.1007/978-3-030-73197-7_29},
abstract = {Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Previous methods focus on learning domain-invariant features to decrease the discrepancy between the feature distributions as well as minimizing the source error and have made remarkable progress. However, a recently proposed theory reveals that such a strategy is not sufficient for a successful domain adaptation. It shows that besides a small source error, both the discrepancy between the feature distributions and the discrepancy between the labeling functions should be small across domains. The discrepancy between the labeling functions is essentially the cross-domain errors which are ignored by existing methods. To overcome this issue, in this paper, a novel method is proposed to integrate all the objectives into a unified optimization framework. Moreover, the incorrect pseudo labels widely used in previous methods can lead to error accumulation during learning. To alleviate this problem, the pseudo labels are obtained by utilizing structural information of the target domain besides source classifier and we propose a curriculum learning based strategy to select the target samples with more accurate pseudo-labels during training. Comprehensive experiments are conducted, and the results validate that our approach outperforms state-of-the-art methods.},
booktitle = {Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II},
pages = {429–448},
numpages = {20},
keywords = {Cross-domain errors, Domain adaptation, Transfer learning},
location = {Taipei, Taiwan}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@article{10.1016/j.neucom.2019.03.062,
author = {Ren, Yazhou and Que, Xiaofan and Yao, Dezhong and Xu, Zenglin},
title = {Self-paced multi-task clustering},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {350},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.062},
doi = {10.1016/j.neucom.2019.03.062},
journal = {Neurocomput.},
month = jul,
pages = {212–220},
numpages = {9},
keywords = {Soft weighting, Non-convexity, Self-paced learning, Multi-task clustering}
}

@article{10.1016/j.cie.2019.03.029,
author = {Kang, Seokho and Kim, Dongil and Cho, Sungzoon},
title = {Approximate training of one-class support vector machines using expected margin},
year = {2019},
issue_date = {Apr 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {130},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.03.029},
doi = {10.1016/j.cie.2019.03.029},
journal = {Comput. Ind. Eng.},
month = apr,
pages = {772–778},
numpages = {7},
keywords = {One-class support vector machine, Support vector data description, Approximate training, Data selection, Expected margin}
}

@article{10.1016/j.compag.2018.07.014,
author = {Xie, Chengjun and Wang, Rujing and Zhang, Jie and Chen, Peng and Dong, Wei and Li, Rui and Chen, Tianjiao and Chen, Hongbo},
title = {Multi-level learning features for automatic classification of field crop pests},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2018.07.014},
doi = {10.1016/j.compag.2018.07.014},
journal = {Comput. Electron. Agric.},
month = sep,
pages = {233–241},
numpages = {9},
keywords = {Pest classification, Unsupervised feature learning, Dictionary learning, Feature encoding}
}

@article{10.5555/2627435.2638574,
author = {Ruiz, Francisco J. R. and Valera, Isabel and Blanco, Carlos and Perez-Cruz, Fernando},
title = {Bayesian nonparametric comorbidity analysis of psychiatric disorders},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an effcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1215–1247},
numpages = {33},
keywords = {Bayesian nonparametrics, Indian buffet process, Laplace approximation, categorical observations, multinomial-logit function, variational inference}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@inproceedings{10.5555/3060832.3060891,
author = {Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting},
title = {Self-paced boost learning for classification},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Effectiveness and robustness are two essential aspects of supervised learning studies. For effective learning, ensemble methods are developed to build a strong effective model from ensemble of weak models. For robust learning, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously enhancing the learning effectiveness and robustness, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a max-margin boosting optimization with self-paced sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1932–1938},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.1016/j.micpro.2021.103964,
author = {Gokilavani, N. and Bharathi, B.},
title = {Multi-Objective based test case selection and prioritization for distributed cloud environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103964},
doi = {10.1016/j.micpro.2021.103964},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {6},
keywords = {Cloud environment, Software testing, Similarity-based clustering, Test case prioritization, Test case selection, Particle swarm optimization, Software product line}
}

@article{10.1145/3243316,
author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
title = {Unsupervised Person Re-identification: Clustering and Fine-tuning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3243316},
doi = {10.1145/3243316},
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {83},
numpages = {18},
keywords = {unsupervised learning, convolutional neural network, clustering, Large-scale person re-identification}
}

@article{10.1016/j.neucom.2021.04.064,
author = {He, Tiantian and Ong, Yew-Soon and Hu, Pengwei},
title = {Multi-source propagation aware network clustering☆      },
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {453},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.064},
doi = {10.1016/j.neucom.2021.04.064},
journal = {Neurocomput.},
month = sep,
pages = {119–130},
numpages = {12},
keywords = {Network cluster analysis, Matrix factorization, Non-negative matrix factorization, Latent factor model}
}

@article{10.1007/s10586-019-03012-1,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Ther\'{o}n, Roberto and Amo Filv\`{a}, Daniel and Fonseca Escudero, David},
title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-03012-1},
doi = {10.1007/s10586-019-03012-1},
abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm.},
journal = {Cluster Computing},
month = sep,
pages = {1803–1816},
numpages = {14},
keywords = {Automatic configuration, Artificial intelligence, Feature model, Information dashboards, Meta-model, Domain engineering, SPL}
}

@article{10.1613/jair.1.11688,
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11688},
doi = {10.1613/jair.1.11688},
abstract = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1183–1317},
numpages = {135},
keywords = {deep learning, computer vision, machine learning, natural language}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {sensor attack, autonomous driving, adversarial machine learning},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Variability Model, Rule Generation, Model Driven Architecture, Domain-specific rules, Business Process Model}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Mar\'{\i}n, Carlos Enrique Montenegro and Crespo, Rub\'{e}n Gonz\'{a}lez},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Deep learning, Classification, Vehicle, Road traffic, Environmental noise}
}

@inproceedings{10.1007/978-3-030-58577-8_17,
author = {Pan, Lili and Ai, Shijie and Ren, Yazhou and Xu, Zenglin},
title = {Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_17},
doi = {10.1007/978-3-030-58577-8_17},
abstract = {Deep discriminative models (e.g.&nbsp;deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model—self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {271–287},
numpages = {17},
keywords = {Underrepresented examples, Self-paced learning, Entropy, Deep regression forests},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10618-019-00616-4,
author = {Clark, Jessica and Provost, Foster},
title = {Unsupervised dimensionality reduction versus supervised regularization for classification from sparse data},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-019-00616-4},
doi = {10.1007/s10618-019-00616-4},
abstract = {Unsupervised matrix-factorization-based dimensionality reduction (DR) techniques are popularly used for feature engineering with the goal of improving the generalization performance of predictive models, especially with massive, sparse feature sets. Often DR is employed for the same purpose as supervised regularization and other forms of complexity control: exploiting a bias/variance tradeoff to mitigate overfitting. Contradicting this practice, there is consensus among existing expert guidelines that supervised regularization is a superior way to improve predictive performance. However, these guidelines are not always followed for this sort of data, and it is not unusual to find DR used with no comparison to modeling with the full feature set. Further, the existing literature does not take into account that DR and supervised regularization are often used in conjunction. We experimentally compare binary classification performance using DR features versus the original features under numerous conditions: using a total of 97 binary classification tasks, 6 classifiers, 3 DR techniques, and 4 evaluation metrics. Crucially, we also experiment using varied methodologies to tune and evaluate various key hyperparameters. We find a very clear, but nuanced result. Using state-of-the-art hyperparameter-selection methods, applying DR does not add value beyond supervised regularization, and can often diminish performance. However, if regularization is not done well (e.g., one just uses the default regularization parameter), DR does have relatively better performance--but these approaches result in lower performance overall. These latter results provide an explanation for why practitioners may be continuing to use DR without undertaking the necessary comparison to using the original features. However, this practice seems generally wrongheaded in light of the main results, if the goal is to maximize generalization performance.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {871–916},
numpages = {46},
keywords = {Sparse data, Experimental comparison, Dimensionality reduction, Data mining, Binary classification}
}

@article{10.1016/j.knosys.2019.105424,
author = {Liu, Xiaoshuang and Luo, Senlin and Pan, Limin},
title = {Robust boosting via self-sampling},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105424},
doi = {10.1016/j.knosys.2019.105424},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Self-sampling, Robustness, Loss function, Boosting}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Multi-modal, Self-paced learning, Curriculum learning, Image classification}
}

@article{10.1109/TCBB.2015.2476790,
author = {Deng, Su-Ping and Zhu, Lin and Huang, De-Shuang},
title = {Predicting hub genes associated with cervical cancer through gene co-expression networks},
year = {2016},
issue_date = {January/February 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2476790},
doi = {10.1109/TCBB.2015.2476790},
abstract = {Cervical cancer is the third most common malignancy in women worldwide. It remains a leading cause of cancer-related death for women in developing countries. In order to contribute to the treatment of the cervical cancer, in our work, we try to find a few key genes resulting in the cervical cancer. Employing functions of several bioinformatics tools, we selected 143 differentially expressed genes (DEGs) associated with the cervical cancer. The results of bioinformatics analysis show that these DEGs play important roles in the development of cervical cancer. Through comparing two differential co-expression networks (DCNs) at two different states, we found a common sub-network and two differential sub-networks as well as some hub genes in three sub-networks. Moreover, some of the hub genes have been reported to be related to the cervical cancer. Those hub genes were analyzed from Gene Ontology function enrichment, pathway enrichment and protein binding three aspects. The results can help us understand the development of the cervical cancer and guide further experiments about the cervical cancer.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {27–35},
numpages = {9},
keywords = {hub genes, differentially expressed genes, co-expression network, cervical cancer}
}

@article{10.1016/j.jss.2009.02.011,
author = {White, Jules and Dougherty, Brian and Schmidt, Douglas C.},
title = {Selecting highly optimal architectural feature sets with Filtered Cartesian Flattening},
year = {2009},
issue_date = {August, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.02.011},
doi = {10.1016/j.jss.2009.02.011},
abstract = {Feature modeling is a common method used to capture the variability in a configurable application. A key challenge developers face when using a feature model is determining how to select a set of features for a variant that simultaneously satisfy a series of resource constraints. This paper presents an approximation technique for selecting highly optimal feature sets while adhering to resource limits. The paper provides the following contributions to configuring application variants from feature models: (1) we provide a polynomial time approximation algorithm for selecting a highly optimal set of features that adheres to a set of resource constraints, (2) we show how this algorithm can incorporate complex configuration constraints; and (3) we present empirical results showing that the approximation algorithm can be used to derive feature sets that are more than 90%+ optimal.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1268–1284},
numpages = {17},
keywords = {Resource constraints, Optimization, Feature modeling, Approximation algorithm}
}

@article{10.3233/THC-218026,
author = {Zhou, Zhiming and Huang, Haihui and Liang, Yong},
title = {Cancer classification and biomarker selection via a penalized logsum network-based logistic regression model},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {S1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-218026},
doi = {10.3233/THC-218026},
journal = {Technol. Health Care},
month = jan,
pages = {287–295},
numpages = {9},
keywords = {network-based knowledge, log-sum penalty, gene selection, Regularization}
}

@inbook{10.5555/3454287.3454459,
author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
title = {Meta-weight-net: learning an explicit mapping for sample weighting},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {172},
numpages = {12}
}

@article{10.1016/j.engappai.2019.08.015,
author = {Tavasoli, Hanane and Oommen, B. John and Yazidi, Anis},
title = {On utilizing weak estimators to achieve the online classification of data streams},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.08.015},
doi = {10.1016/j.engappai.2019.08.015},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {11–31},
numpages = {21},
keywords = {Classification in data streams, Non-stationary environments, Learning automata, Weak estimators}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {variability mining, software product lines, reverse engineering, feature location, benchmark},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1007/978-3-030-00308-1_33,
author = {O’Keeffe, Simon and Villing, Rudi},
title = {A Benchmark Data Set and Evaluation of Deep Learning Architectures for Ball Detection in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_33},
doi = {10.1007/978-3-030-00308-1_33},
abstract = {This paper presents a benchmark data set for evaluating ball detection algorithms in the RoboCup Soccer Standard Platform League. We created a labelled data set of images with and without ball derived from vision log files recorded by multiple NAO robots in various lighting conditions. The data set contains 5209 labelled ball image regions and 10924 non-ball regions. Non-ball image regions all contain features that had been classified as a potential ball candidate by an existing ball detector. The data set was used to train and evaluate 252 different Deep Convolutional Neural Network (CNN) architectures for ball detection. In order to control computational requirements, this evaluation focused on networks with 2–5 layers that could feasibly run in the vision and cognition cycle of a NAO robot using two cameras at full frame rate (2&nbsp;\texttimes{}&nbsp;30&nbsp;Hz). The results show that the classification performance of the networks is quite insensitive to the details of the network design including input image size, number of layers and number of outputs at each layer. In an effort to reduce the computational requirements of CNNs we evaluated XNOR-Net architectures which quantize the weights and activations of a neural network to binary values. We examined XNOR-Nets corresponding to the real-valued CNNs we had already tested in order to quantify the effect on classification metrics. The results indicate that ball classification performance degrades by 12% on average when changing from real-valued CNN to corresponding XNOR-Net.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {398–409},
numpages = {12},
keywords = {XNOR-Net, Ball detection, Deep learning, Convolution neural network},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3476100.3484461,
author = {Guan, Jiazhi and Shen, Dongyao},
title = {Transfer Spatio-Temporal Knowledge from Emotion-Related Tasks for Facial Expression Spotting},
year = {2021},
isbn = {9781450386838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3476100.3484461},
doi = {10.1145/3476100.3484461},
abstract = {Facial micro-expression computational analyses are becoming a prevalent research area, automatically micro-expression spotting as the first-in-the-pipeline problem has not been resolved yet. There are two main factors that confine the performance of current studies. 1) Subtle involuntary movements of micro-expression are hard to capture. 2) Micro-expression datasets are relatively small that can not fully support the training of deep neural networks. For the first problem, we propose modeling the expression movements from the view of consecutive frames in the wavelet space as temporal features. Combined with spatial features encoded by a convolutional neural network, temporal and spatial information can supplement each other in further analyses. For the second problem, we adopt transfer learning from other emotion-related tasks since the facial prior is homologous to our task. To train our model, we covert the spotting task to a frame-level classification task, meanwhile, weighted focal loss is used to deal with severe class imbalance. With leave-one-subject-out cross-validation, our method reports F1-score of 0.1763 and 0.1360 for CAS(ME)2 and SAMM-LV respectively. Code is available at https://github.com/guanjz20/MM21_FME_solution.},
booktitle = {Proceedings of the 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting},
pages = {19–24},
numpages = {6},
keywords = {transfer learning, micro-expression, facial expression spotting},
location = {Virtual Event, China},
series = {FME'21}
}

@phdthesis{10.5555/AAI28544034,
author = {Khoshmanesh, Seyedehzahra and Samik, Basu, and Andrew, Miner, and Hridesh, Rajan, and Karin, Dorman,},
advisor = {R, Lutz, Robyn},
title = {Learning Feature Interactions with and without Specifications},
year = {2021},
isbn = {9798544278207},
publisher = {Iowa State University},
address = {USA},
abstract = {Developers of software product lines and highly configurable systems reuse and combine features (units of functionality) to build new or customize existing products. However, features can interact in ways that are contrary to developers' intent. Predicting whether a new combination of features will produce an unwanted or even hazardous feature interaction is a continuing challenge. Current techniques to detect unwanted feature interactions are costly, slow, and inadequate. In this thesis, we investigate how to detect unwanted feature interactions early in development and that are scalable to large software product lines or highly configurable systems. First, we propose a similarity-based method to identify unwanted feature interactions much earlier in the development process for early detection. It uses knowledge of prior feature interactions stored with the software product line's feature model to help find unwanted interactions between a new feature and existing features. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.Moreover, to learn and automate the detection, we show how detecting unwanted feature interactions can be effectively represented as a link prediction problem. We investigate six link-based similarity metrics and evaluate our approach on a software product line benchmark. Results show that the best machine learning algorithms achieve an accuracy of 0.75 to 1 for classifying feature interactions.Finally, we develop a new approach based on program analysis that extracts feature-relevant learning models from the source code to obtain more semantic details of unwanted feature interactions. The method is capable of learning feature interactions whether constraints on feature combinations are specified or not. If specifications of feature constraints are unavailable, as is common in real-world systems, our approach infers the constraints using feature-related data-flow dependency information. Experimental evaluation on three software product line benchmarks and a highly configurable system shows that this approach is fast and effective.The contribution is to support developers by automatically detecting those feature combinations in a new product or version that can interact in unwanted or unrecognized ways. This enables a better understanding of hidden interactions and identifies software components that should be tested together because their features interact in some configurations.},
note = {AAI28544034}
}

@article{10.1016/j.cviu.2014.11.003,
author = {Kim, Minyoung},
title = {Multiple-concept feature generative models for multi-label image classification},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2014.11.003},
doi = {10.1016/j.cviu.2014.11.003},
abstract = {Novel generative model representing fusion of multiple concepts in feature generation.Learning algorithms based on likelihood and margin maximization without line search.Performance improvement in several multi-label image classification tasks. We consider the problem of multi-label classification where a feature vector may belong to one of more different classes or concepts at the same time. Many existing approaches are devoted for solving the difficult estimation task of uncovering the relationship between features and active concepts, solely from data without taking into account any sensible functional structure. In this paper, we propose a novel probabilistic generative model that aims to describe the core generative process of how multiple active concepts can contribute to feature generation. Within our model, each concept is associated with multiple representative base feature vectors, which shares the central idea of sparse feature modeling with the popular dictionary learning. However, by dealing with the weight coefficients as exclusive latent random variables encoding contribution levels, we effectively frame the coefficient learning task as probabilistic inference. We introduce two parameter learning algorithms for the proposed model: one based on standard maximum likelihood learning via the expectation-maximization algorithm, the other focusing on maximally separating the margin of the true concept configuration away from the class boundary. In the latter we suggest an efficient approximate optimization method where each iteration admits closed-form update with no line search. For several benchmark datasets mostly from the multi-label image classification, we demonstrate that our generative model with proposed estimators can often yield superior prediction performance to existing methods.},
journal = {Comput. Vis. Image Underst.},
month = jul,
pages = {69–78},
numpages = {10},
keywords = {Concept prediction, Image classification, Multi-label classification, Probabilistic graphical models}
}

@article{10.1007/s42979-020-00341-6,
author = {Malekmohamadi Faradonbe, Soroor and Safi-Esfahani, Faramarz and Karimian-kelishadrokhi, Morteza},
title = {A Review on Neural Turing Machine (NTM)},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {6},
url = {https://doi.org/10.1007/s42979-020-00341-6},
doi = {10.1007/s42979-020-00341-6},
abstract = {One of the significant objectives of artificial intelligence is to design learning algorithms that are executed on general-purpose computational machines inspired by the human brain. Neural Turing Machine (NTM) is a step towards realizing such a computational machine. In the literature, a variety of approaches have been presented for the NTM; however, there is no existing comprehensive survey and taxonomy for NTM methods. This article presents an overview of taxonomies characterizing the critical concepts of the NTM through a comprehensive survey on the related research activities. This in-depth analysis of taxonomies can provide researchers, designers, and application developers with a clear guideline to compare NTM methods. The taxonomy of machine learning, neural networks, and the Turing machine is introduced. The NTM is also inspected in terms of concepts, structure, implemented tasks, and related works. The article further presents research discussions and future challenges in this area.},
journal = {SN Comput. Sci.},
month = oct,
numpages = {23},
keywords = {Neural turing machine (NTM), Deep learning, Machine learning, Turing machine, Neural networks}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Case study, User trust, Business-to-business, Machine learning, Big data}
}

@article{10.1016/j.asoc.2021.107162,
author = {Liang, Tingting and Sheng, Xuan and Zhou, Li and Li, Youhuizi and Gao, Honghao and Yin, Yuyu and Chen, Liang},
title = {Mobile app recommendation via heterogeneous graph neural network in edge computing},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {103},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107162},
doi = {10.1016/j.asoc.2021.107162},
journal = {Appl. Soft Comput.},
month = may,
numpages = {9},
keywords = {Service recommendation, Heterogeneous graph, Graph neural network, Edge computing}
}

@inproceedings{10.1109/SPLC.2008.11,
author = {Niu, Nan and Easterbrook, Steve},
title = {On-Demand Cluster Analysis for Product Line Functional Requirements},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.11},
doi = {10.1109/SPLC.2008.11},
abstract = {We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {87–96},
numpages = {10},
keywords = {requirements clustering, overlapping clustering, information-theoretic clustering, functional requirements profiles},
series = {SPLC '08}
}

@article{10.1016/j.procs.2019.11.281,
author = {Xianya, Jiang and Mo, Hai and Haifeng, Li},
title = {Stock Classification Prediction Based on Spark},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {162},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.11.281},
doi = {10.1016/j.procs.2019.11.281},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {243–250},
numpages = {8},
keywords = {PR(Area under the Precision-Recall curve), AUC(Area under the receiver operating characteristic curve), Classification accuracy, Run time, Stock prediction, Logistic regression, Decision tree, Random forest, Na\"{\i}ve bayes, Classification, Spark}
}

@inproceedings{10.1145/3288599.3297118,
author = {Krishnan, Gokul S. and S, Sowmya Kamath},
title = {Evaluating the quality of word representation models for unstructured clinical Text based ICU mortality prediction},
year = {2019},
isbn = {9781450360944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288599.3297118},
doi = {10.1145/3288599.3297118},
abstract = {In modern hospitals, the role of Clinical Decision Support Systems (CDSS) in assisting care providers is well-established. Most conventional CDSS systems are built on the availability of patient data in the form of structured Electronic Health Records. However, a significant percentage of patient data is still stored in the form of unstructured clinical text notes, especially in developing countries. These contain abundant patient-specific information, which has so far remained largely under-utilized in powering CDSS applications. In this paper, we attempt to build one such CDSS system for patient mortality prediction, using unstructured clinical records. Effectiveness of such prediction models largely depends on optimally capturing latent concept features, thus, word representation quality is of utmost importance. We experiment with three popular word embedding models - Word2Vec, FastText and GloVe for generating word embeddings of unstructured nursing notes of patients from a standard, open dataset, MIMIC-III. These word representations are used as features to train machine learning classifiers to build ICU mortality prediction models, a critical CDSS in ICUs of hospitals. Experimental validation showed that a model built on Word2Vec Skipgram based Random Forest classifier was the most optimal word embedding based mortality prediction model, that outperformed traditional severity scores like SAPS-II, SOFA, APS-III and OASIS, by a significant margin of 43-52%.},
booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
pages = {480–485},
numpages = {6},
keywords = {healthcare informatics, machine learning, mortality prediction, natural language processing, word embedding},
location = {Bangalore, India},
series = {ICDCN '19}
}

@inproceedings{10.1007/978-3-030-61705-9_51,
author = {Martinez-Murcia, Francisco J. and Ortiz, Andres and Formoso, Marco A. and Lopez-Zamora, Miguel and Luque, Juan Luis and Gimenez, Almudena},
title = {A Neural Approach to Ordinal Regression for the Preventive Assessment of Developmental Dyslexia},
year = {2020},
isbn = {978-3-030-61704-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61705-9_51},
doi = {10.1007/978-3-030-61705-9_51},
abstract = {Developmental Dyslexia (DD) is a learning disability related to the acquisition of reading skills that affects about 5% of the population. DD can have an enormous impact on the intellectual and personal development of affected children, so early detection is key to implementing preventive strategies for teaching language. Research has shown that there may be biological underpinnings to DD that affect phoneme processing, and hence these symptoms may be identifiable before reading ability is acquired, allowing for early intervention.&nbsp;In this paper we propose a new methodology to assess the risk of DD before students learn to read. For this purpose, we propose a mixed neural model that calculates risk levels of dyslexia from tests that can be completed at the age of 5&nbsp;years. Our method first trains an auto-encoder, and then combines the trained encoder with an optimized ordinal regression neural network devised to ensure consistency of predictions. Our experiments show that the system is able to detect unaffected subjects two years before it can assess the risk of DD based mainly on phonological processing, giving a specificity of 0.969 and a correct rate of more than 0.92. In addition, the trained encoder can be used to transform test results into an interpretable subject spatial distribution that facilitates risk assessment and validates methodology.},
booktitle = {Hybrid Artificial Intelligent Systems: 15th International Conference, HAIS 2020, Gij\'{o}n, Spain, November 11-13, 2020, Proceedings},
pages = {620–630},
numpages = {11},
keywords = {Autoencoder, Deep learning, Dyslexia, Prevention, Ordinal regression},
location = {Gij\'{o}n, Spain}
}

@article{10.1016/j.neucom.2019.11.104,
author = {Ren, Yazhou and Huang, Shudong and Zhao, Peng and Han, Minghao and Xu, Zenglin},
title = {Self-paced and auto-weighted multi-view clustering},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.104},
doi = {10.1016/j.neucom.2019.11.104},
journal = {Neurocomput.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Soft weighting, Multi-view clustering, Self-paced learning}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {God class, Project context information, Software metrics, Machine learning, Design smell detection}
}

@inproceedings{10.1145/3434780.3436640,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Theron, Roberto},
title = {Advances in the use of domain engineering to support feature identification and generation of information visualizations},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436640},
doi = {10.1145/3434780.3436640},
abstract = {Information visualization tools are widely used to better understand large and complex datasets. However, to make the most out of them, it is necessary to rely on proper designs that consider not only the data to be displayed, but also the audience and the context. There are tools that already allow users to configure their displays without requiring programming skills, but this research project aims at exploring the automatic generation of information visualizations and dashboards in order to avoid the configuration process, and select the most suitable features of these tools taking into account their contexts. To address this problem, a domain engineering, and machine learning approach is proposed.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1053–1056},
numpages = {4},
keywords = {Meta-modeling, Machine Learning, Information Dashboards, High-level requirements, Domain engineering, Automatic generation},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {99-00, 00-01, Local structure preservation, Latent representation learning, Hypergraph learning, Unsupervised feature selection}
}

@article{10.2478/cait-2020-0040,
author = {Gumusbas, Dilara and Yildirim, Tulay},
title = {Offline Signature Identification and Verification Based on Capsule Representations},
year = {2020},
issue_date = {Dec 2020},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {20},
number = {5},
issn = {1314-4081},
url = {https://doi.org/10.2478/cait-2020-0040},
doi = {10.2478/cait-2020-0040},
abstract = {Offline signature is one of the frequently used biometric traits in daily life and yet skilled forgeries are posing a great challenge for offline signature verification. To differentiate forgeries, a variety of research has been conducted on hand-crafted feature extraction methods until now. However, these methods have recently been set aside for automatic feature extraction methods such as Convolutional Neural Networks (CNN). Although these CNN-based algorithms often achieve satisfying results, they require either many samples in training or pre-trained network weights. Recently, Capsule Network has been proposed to model with fewer data by using the advantage of convolutional layers for automatic feature extraction. Moreover, feature representations are obtained as vectors instead of scalar activation values in CNN to keep orientation information. Since signature samples per user are limited and feature orientations in signature samples are highly informative, this paper first aims to evaluate the capability of Capsule Network for signature identification tasks on three benchmark databases. Capsule Network achieves 97 96, 94 89, 95 and 91% accuracy on CEDAR, GPDS-100 and MCYT databases for 64\texttimes{}64 and 32\texttimes{}32 resolutions, which are lower than usual, respectively. The second aim of the paper is to generalize the capability of Capsule Network concerning the verification task. Capsule Network achieves average 91, 86, and 89% accuracy on CEDAR, GPDS-100 and MCYT databases for 64\texttimes{}64 resolutions, respectively. Through this evaluation, the capability of Capsule Network is shown for offline verification and identification tasks.},
journal = {Cybern. Inf. Technol.},
month = dec,
pages = {60–67},
numpages = {8},
keywords = {Capsule Network, Offline Signature Verification, Offline Signature Identification, Convolutional Neural Networks}
}

@inproceedings{10.5555/1158337.1158678,
author = {Czarnecki, Krzysztof and Peter Kim, Chang Hwan and Kalleberg, Karl Trygve},
title = {Feature Models are Views on Ontologies},
year = {2006},
isbn = {0769525997},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Feature modeling has been proposed as an approach for describing variable requirements for software product lines. In this paper, we explore the relationship between feature models and ontologies. First, we examine how previous extensions to basic feature modeling move it closer to richer formalisms for specifying ontologies such as MOF and OWL. Then, we explore the idea of feature models as views on ontologies. Based on that idea, we propose two approaches for the combined use of feature models and ontologies: view derivation and view integration. Finally, we give some ideas about tool support for these approaches.},
booktitle = {Proceedings of the 10th International on Software Product Line Conference},
pages = {41–51},
numpages = {11},
series = {SPLC '06}
}

@article{10.1145/3369393,
author = {Ding, Yuhang and Fan, Hehe and Xu, Mingliang and Yang, Yi},
title = {Adaptive Exploration for Unsupervised Person Re-identification},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3369393},
doi = {10.1145/3369393},
abstract = {Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {3},
numpages = {19},
keywords = {unsupervised learning, domain adaptation, deep learning, Person re-identification}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Text, Linear discriminant analysis, Fisher, Eigenvectors, Classification, Arabic}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Variability modeling, Variability management, Software product family, Classification}
}

@article{10.1016/j.future.2019.07.013,
author = {Shen, Rongbo and Yan, Kezhou and Tian, Kuan and Jiang, Cheng and Zhou, Ke},
title = {Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.07.013},
doi = {10.1016/j.future.2019.07.013},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {668–679},
numpages = {12},
keywords = {Self-paced learning, Deep active learning, Mass detection, Mammography, Breast cancer}
}

@article{10.1016/j.neucom.2014.12.100,
author = {Garcia, Lu\'{\i}s P.F. and Carvalho, Andr\'{e} C.P.L.F. de and Lorena, Ana C.},
title = {Noise detection in the meta-learning level},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {176},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.12.100},
doi = {10.1016/j.neucom.2014.12.100},
abstract = {The presence of noise in real data sets can harm the predictive performance of machine learning algorithms. There are several noise filtering techniques whose goal is to improve the quality of the data in classification tasks. These techniques usually scan the data for noise identification in a preprocessing step. Nonetheless, this is a non-trivial task and some noisy data can remain unidentified, while safe data can also be removed. The bias of each filtering technique influences its performance on a particular data set. Therefore, there is no single technique that can be considered the best for all domains or data distribution and choosing a particular filter is not straightforward. Meta-learning has been largely used in the last years to support the recommendation of the most suitable machine learning algorithm(s) for a new data set. This paper presents a meta-learning recommendation system able to predict the expected performance of noise filters in noisy data identification tasks. For such, a meta-base is created, containing meta-features extracted from several corrupted data sets along with the performance of some noise filters when applied to these data sets. Next, regression models are induced from this meta-base to predict the expected performance of the investigated filters in the identification of noisy data. The experimental results show that meta-learning can provide a good recommendation of the most promising filters to be applied to new classification data sets.},
journal = {Neurocomput.},
month = feb,
pages = {14–25},
numpages = {12},
keywords = {Noise identification, Meta-learning, Complexity measures, Characterization measures}
}

@article{10.1007/s00521-020-04805-x,
author = {Ni, Pin and Li, Yuming and Li, Gangmin and Chang, Victor},
title = {Natural language understanding approaches based on joint task of intent detection and slot filling for IoT voice interaction},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04805-x},
doi = {10.1007/s00521-020-04805-x},
abstract = {Internet of Things (IoT) based voice interaction system, as a new artificial intelligence application, provides a new human–computer interaction mode. The more intelligent and efficient communication approach poses greater challenges to the semantic understanding module in the system. Facing with the complex and diverse interactive scenarios in practical applications, the academia and the industry urgently need more powerful Natural Language Understanding (NLU) methods as support. Intent Detection and Slot Filling joint task, as one of the core sub-tasks in NLU, has been widely used in different human–computer interaction scenarios. In the current era of deep learning, the joint task of Intent Detection and Slot Filling has also changed from previous rule-based methods to deep learning-based methods. It is an important problem to explore how to realize the models of these tasks to be refined and targeted designed, and to make the Intent&nbsp;Detection task better serve the improvement of precision of Slot Filling task by connecting the before and after tasks. It has great significance for building a more humanized IoT voice interaction system. In this study, we designed two joint models to realize Intent&nbsp;Detection and Slot Filling joint task. For the Intent Detection type task, one is based on BiGRU-Att-CapsuleNet (hybrid-based model) and the other is based on the RCNN model. Both methods use the BiGRU-CRF model for the Slot Filling type task. The hybrid-based model can enhance the semantic capture capability of a single model. And by combining specialized models built independently for each task to achieve a complete joint task, it can be better to achieve optimal performance on each task. This study also carried out detailed comparative experiments of tasks and joint tasks on multiple datasets. Experiments show that the joint models have achieved competitive results in 7 typical datasets included in multiple scenarios in English and Chinese compared with other models.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {16149–16166},
numpages = {18},
keywords = {Capsule network, Intent detection and slot filling, Voice interaction, Natural language understanding, Artificial intelligence, Internet of Things}
}

@inproceedings{10.1145/3229607.3229610,
author = {Geyer, Fabien and Carle, Georg},
title = {Learning and Generating Distributed Routing Protocols Using Graph-Based Deep Learning},
year = {2018},
isbn = {9781450359047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229607.3229610},
doi = {10.1145/3229607.3229610},
abstract = {Automated network control and management has been a long standing target of network protocols. We address in this paper the question of automated protocol design, where distributed networked nodes have to cooperate to achieve a common goal without a priori knowledge on which information to exchange or the network topology. While reinforcement learning has often been proposed for this task, we propose here to apply recent methods from semi-supervised deep neural networks which are focused on graphs. Our main contribution is an approach for applying graph-based deep learning on distributed routing protocols via a novel neural network architecture named Graph-Query Neural Network. We apply our approach to the tasks of shortest path and max-min routing. We evaluate the learned protocols in cold-start and also in case of topology changes. Numerical results show that our approach is able to automatically develop efficient routing protocols for those two use-cases with accuracies larger than 95%. We also show that specific properties of network protocols, such as resilience to packet loss, can be explicitly included in the learned protocol.},
booktitle = {Proceedings of the 2018 Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
pages = {40–45},
numpages = {6},
keywords = {Routing, Graph Neural Network, Deep Learning},
location = {Budapest, Hungary},
series = {Big-DAMA '18}
}

@inproceedings{10.1007/978-3-030-67658-2_34,
author = {Yamaguchi, Akihiro and Maya, Shigeru and Ueno, Ken},
title = {RLTS: Robust Learning Time-Series Shapelets},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_34},
doi = {10.1007/978-3-030-67658-2_34},
abstract = {Shapelets are time-series segments effective for classifying time-series instances. Joint learning of both classifiers and shapelets has been studied in recent years because such a method provides both superior classification performance and interpretable results. For robust learning, we introduce Self-Paced Learning (SPL) and adaptive robust losses into this method. The SPL method can assign latent instance weights by considering not only classification losses but also understandable shapelet discovery. Furthermore, the adaptive robustness introduced into feature vectors is jointly learned with shapelets, a classifier, and latent instance weights. We demonstrate the superiority of AUC and the validity of our approach on UCR time-series datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {595–611},
numpages = {17},
keywords = {Time-series shapelets, Self-paced learning, Robust losses},
location = {Ghent, Belgium}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1007/s11042-019-7498-3,
author = {Kaur, Taranjit and Saini, Barjinder Singh and Gupta, Savita},
title = {An adaptive fuzzy K-nearest neighbor approach for MR brain tumor image classification using parameter free bat optimization algorithm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7498-3},
doi = {10.1007/s11042-019-7498-3},
abstract = {This paper presents an automatic diagnosis system for the tumor grade classification through magnetic resonance imaging (MRI). The diagnosis system involves a region of interest (ROI) delineation using intensity and edge magnitude based multilevel thresholding algorithm. Then the intensity and the texture attributes are extracted from the segregated ROI. Subsequently, a combined approach known as Fisher+ Parameter-Free BAT (PFreeBAT) optimization is employed to derive the optimal feature subset. Finally, a novel learning approach dubbed as PFree BAT enhanced fuzzy K-nearest neighbor (FKNN) is proposed by combining FKNN with PFree BAT for the classification of MR images into two categories: High and Low-Grade. In PFree BAT enhanced FKNN, the model parameters, i.e., neighborhood size k and the fuzzy strength parameter m are adaptively specified by the PFree BAT optimization approach. Integrating PFree BAT with FKNN enhances the classification capability of the FKNN. The diagnostic system is rigorously evaluated on four MR images datasets including images from BRATS 2012 database and the Harvard repository using classification performance metrics. The empirical results illustrate that the diagnostic system reached to ceiling level of accuracy on the test MR image dataset via 5-fold cross-validation mechanism. Additionally, the proposed PFree BAT enhanced FKNN is evaluated on the Parkinson dataset (PD) from the UCI repository having the pre-extracted feature space. The proposed PFree BAT enhanced FKNN reached to an average accuracy of 98% and 97.45%. with and without feature selection on PD dataset. Moreover, solely to contrast, the performance of the proposed PFree BAT enhanced FKNN with the existing FKNN variants the experimentations were also done on six other standard datasets from KEEL repository. The results indicate that the proposed learning strategy achieves the best value of accuracy in contrast to the existing FKNN variants.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21853–21890},
numpages = {38},
keywords = {Model parameters, Diagnosis system, PFree BAT optimization, Fuzzy K-nearest neighbor}
}

@article{10.1007/s11042-020-09457-6,
author = {Das, Dibyasundar and Nayak, Deepak Ranjan and Dash, Ratnakar and Majhi, Banshidhar},
title = {MJCN: Multi-objective Jaya Convolutional Network for handwritten optical character recognition},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {43–44},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09457-6},
doi = {10.1007/s11042-020-09457-6},
abstract = {In recent years, the non-handcrafted feature extraction methods have gained increasing popularity for solving pattern classification tasks due to their inherent ability to extract robust features and handle outliers. However, the design of such features demands a large set of training data. Meta-heuristic optimization schemes can facilitate feature learning even with a small amount of training data. This paper presents a new feature learning mechanism called multi-objective Jaya convolutional network (MJCN) that attempts to learn meaningful features directly from the images. The proposed scheme, unlike the convolutional neural networks, comprises a convolution layer, a multiplication layer, an activation layer and an optimizer known as multi-objective Jaya optimizer (MJO). The convolution layer searches meaningful patterns in an image through the local neighborhood connections and the multiplication layer projects the convolutional response to a more compact feature space. The weights used in these layers are initialized randomly and MJO is then introduced to optimize the weights. The main objective of MJO is to maximize the inter-class distance and minimize the intra-class variance. The feature vectors are finally derived using the optimized weights. The derived features are finally fed to a set of standard classifiers for recognition of characters. The performance of the proposed model is evaluated on various benchmark datasets, namely, NITR Odia handwritten character, ISI Kolkata Odia numeral, ISI Kolkata Bangla numeral, and MNIST as well as a newly developed dataset NITR Bangla numeral. The experimental results show that the proposed scheme outperforms other state-of-the-art approaches in terms of recognition accuracy.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {33023–33042},
numpages = {20},
keywords = {Convolution, Feature extraction, Handwritten character recognition, Multi-objective Jaya optimizer, Non-handcrafted feature}
}

@article{10.1016/j.jss.2019.05.026,
author = {Umer, Qasim and Liu, Hui and Sultan, Yasir},
title = {Sentiment based approval prediction for enhancement reports},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.026},
doi = {10.1016/j.jss.2019.05.026},
journal = {J. Syst. Softw.},
month = sep,
pages = {57–69},
numpages = {13},
keywords = {Enhancement reports, Machine learning algorithms, Classification}
}

@article{10.3233/HIS-2011-0140,
author = {Ahumada, Hern\'{a}n and Grinblat, Guillermo L. and Uzal, Lucas C. and Ceccatto, Alejandro and Granitto, Pablo M.},
title = {Evaluation of a new hybrid algorithm for highly imbalanced classification problems},
year = {2011},
issue_date = {October 2011},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {4},
issn = {1448-5869},
url = {https://doi.org/10.3233/HIS-2011-0140},
doi = {10.3233/HIS-2011-0140},
abstract = {Many times in classification problems, particularly in critical real world applications, one of the classes has much less samples than the others usually known as the class imbalance problem. In this work we discuss and evaluate the use of the REPMAC algorithm to solve imbalanced problems. Using a clustering method, REPMAC recursively splits the majority class in several subsets, creating a decision tree, until the resulting sub-problems are balanced or easy to solve. We use two diverse clustering methods and three different classifiers coupled with REPMAC to evaluate the new method on several benchmark datasets spanning a wide range of number of features, samples and imbalance degree. We also apply our method to a real world problem, the identification of weed seeds. We find that the good performance of REPMAC is almost independent of the classifier or the clustering method coupled to it, which suggests that its success is mostly related to the use of an appropriate strategy to cope with imbalanced problems.},
journal = {Int. J. Hybrid Intell. Syst.},
month = oct,
pages = {199–211},
numpages = {13},
keywords = {Hybrid Systems, Clustering, Classification, Class Imbalance}
}

@article{10.1007/s00521-020-05318-3,
author = {Nguyen, Teron and Nguyen-Phuoc, Duy Q. and Wong, Y. D.},
title = {Developing artificial neural networks to estimate real-time onboard bus ride comfort},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05318-3},
doi = {10.1007/s00521-020-05318-3},
abstract = {The ride comfort of bus passengers is a critical factor that is recognised to attract greater ridership towards a sustainable public transport system. However, it is challenging to estimate bus passenger comfort onboard while travelling due to the complex non-linear interaction among various factors. A practicable method to collect real-time comfort ratings by passengers is also not readily available. This study developed an artificial neural network (ANN) model with three layers to precisely estimate real-time ride comfort of bus passengers. The inputs are vehicle-related parameters (speed, acceleration and jerk), passenger-related features (posture, location, facing, gender, age, weight and height), ride comfort index in ISO 2631-1997 (vibration dose value and maximum transient vibration value), and output is passenger rating (collected from a specialised mobile application). The ANN model provided a satisfactory performance and good correlation between inputs and output with an average MSE = 0.03 and R-value = 0.83, respectively. Sensitivity analysis was also conducted to quantify the relative contribution of each variable in the ANN model, revealing similar contributions among all influencing factors in the range of 4–6%. On average, passenger-related factors contribute slightly higher than vehicle-related factors to the ride comfort estimation based on the connection weight approach. The development of ANN model which can precisely estimate bus ride comfort is important as a considerable amount of machine learning and artificial intelligence are utilised to guide autonomous bus (AB). The present findings can help AB designers and engineers in improving AB technology to achieve a higher level of passengers’ onboard comfort.},
journal = {Neural Comput. Appl.},
month = may,
pages = {5287–5299},
numpages = {13},
keywords = {Ride comfort, Bus passenger, Artificial neural network, Autonomous bus, Machine learning}
}

@inproceedings{10.1145/2884781.2884796,
author = {Cheung, Shing-Chi and Chen, Wanjun and Liu, Yepang and Xu, Chang},
title = {CUSTODES: automatic spreadsheet cell clustering and smell detection using strong and weak features},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884796},
doi = {10.1145/2884781.2884796},
abstract = {Various techniques have been proposed to detect smells in spreadsheets, which are susceptible to errors. These techniques typically detect spreadsheet smells through a mechanism based on a fixed set of patterns or metric thresholds. Unlike conventional programs, tabulation styles vary greatly across spreadsheets. Smell detection based on fixed patterns or metric thresholds, which are insensitive to the varying tabulation styles, can miss many smells in one spreadsheet while reporting many spurious smells in another. In this paper, we propose CUSTODES to effectively cluster spreadsheet cells and detect smells in these clusters. The clustering mechanism can automatically adapt to the tabulation styles of each spreadsheet using strong and weak features. These strong and weak features capture the invariant and variant parts of tabulation styles, respectively. As smelly cells in a spreadsheet normally occur in minority, they can be mechanically detected as clusters' outliers in feature spaces. We implemented and applied CUSTODES to 70 spreadsheets files randomly sampled from the EUSES corpus. These spreadsheets contain 1,610 formula cell clusters. Experimental results confirmed that CUSTODES is effective. It successfully detected harmful smells that can induce computation anomalies in spreadsheets with an F-measure of 0.72, outperforming state-of-the-art techniques.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {464–475},
numpages = {12},
keywords = {spreadsheets, smell detection, feature modeling, end-user programming, cell clustering},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.artint.2018.12.008,
author = {Wang, Leye and Geng, Xu and Ma, Xiaojuan and Zhang, Daqing and Yang, Qiang},
title = {Ridesharing car detection by transfer learning},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {273},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2018.12.008},
doi = {10.1016/j.artint.2018.12.008},
journal = {Artif. Intell.},
month = aug,
pages = {1–18},
numpages = {18},
keywords = {Trajectory mining, Co-training, Transfer learning}
}

@article{10.1145/2000791.2000794,
author = {Anvik, John and Murphy, Gail C.},
title = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000794},
doi = {10.1145/2000791.2000794},
abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {10},
numpages = {35},
keywords = {task assignment, recommendation, machine learning, configuration assistance, Bug report triage}
}

@article{10.5555/1314498.1314553,
author = {Saar-Tsechansky, Maytal and Provost, Foster},
title = {Handling Missing Values when Applying Classification Models},
year = {2007},
issue_date = {12/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper first compares several different methods---predictive value imputation, the distribution-based imputation used by C4.5, and using reduced models---for applying classification trees to instances with missing values (and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its (perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1623–1657},
numpages = {35}
}

@inproceedings{10.5555/3540261.3541537,
author = {Peng, Jizong and Wang, Ping and Desrosiers, Christian and Pedersoli, Marco},
title = {Self-paced contrastive learning for semi-supervised medical image segmentation with meta-labels},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classification. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1276},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {product line engineering, model-based testing, behavioral variability, aspect-oriented modeling, UML state machine},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/3430984.3431022,
author = {Virk, Jitender Singh and Bathula, Deepti R.},
title = {Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431022},
doi = {10.1145/3430984.3431022},
abstract = {Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {145–153},
numpages = {9},
keywords = {transfer learning, pseudo-labelling, mixed-kernels neural networks, integrated gradients, image perturbations, domain-specific, Semi-supervised learning, Neural networks, CT scans},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1007/978-3-319-94295-7_15,
author = {Zhu, Mingyi and Ye, Kejiang and Xu, Cheng-Zhong},
title = {Network Anomaly Detection and&nbsp;Identification Based on Deep Learning Methods},
year = {2018},
isbn = {978-3-319-94294-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-94295-7_15},
doi = {10.1007/978-3-319-94295-7_15},
abstract = {Network anomaly detection is the process of determining when network behavior has deviated from the normal behavior. The detection of abnormal events in large dynamic network has become increasingly important as networks grow in size and complexity. However, fast and accurate network anomaly detection is very challenging. Deep learning is a potential method for network anomaly detection due to its good feature modeling capability. This paper presents a new anomaly detection method based on deep learning models, specifically the feedforward neural network (FNN) model and convolutional neural network (CNN) model. The performance of the models is evaluated by several experiments with a popular NSL-KDD dataset. From the experimental results, we find the FNN and CNN models not only have a strong modeling ability for network anomaly detection, but also have high accuracy. Compared with several traditional machine learning methods, such as J48, Naive Bayes, NB Tree, Random Forest, Random Tree and SVM, the proposed models obtain a higher accuracy and detection rate with lower false positive rate. The deep learning models can effectively improve both the detection accuracy and the ability to identify anomaly types.},
booktitle = {Cloud Computing – CLOUD 2018: 11th International Conference, Held as Part of the Services Conference Federation, SCF 2018, Seattle, WA, USA, June 25–30, 2018, Proceedings},
pages = {219–234},
numpages = {16},
location = {Seattle, WA, USA}
}

@article{10.1145/3439729,
author = {Deldjoo, Yashar and Noia, Tommaso Di and Merra, Felice Antonio},
title = {A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3439729},
doi = {10.1145/3439729},
abstract = {Latent-factor models (LFM) based on collaborative filtering (CF), such as matrix factorization (MF) and deep CF methods, are widely used in modern recommender systems (RS) due to their excellent performance and recommendation accuracy. However, success has been accompanied with a major new arising challenge: Many applications of machine learning (ML) are adversarial in nature [146]. In recent years, it has been shown that these methods are vulnerable to adversarial examples, i.e., subtle but non-random perturbations designed to force recommendation models to produce erroneous outputs.The goal of this survey is two-fold: (i) to present recent advances on adversarial machine learning (AML) for the security of RS (i.e., attacking and defense recommendation models) and (ii) to show another successful application of AML in generative adversarial networks (GANs) for generative applications, thanks to their ability for learning (high-dimensional) data distributions. In this survey, we provide an exhaustive literature review of 76 articles published in major RS and ML journals and conferences. This review serves as a reference for the RS community working on the security of RS or on generative models using GANs to improve their quality.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {35},
numpages = {38},
keywords = {Recommender systems, adversarial machine learning, adversarial perturbation, generative adversarial network, min-max game, privacy, robustness, security}
}

@inproceedings{10.1007/978-3-319-03731-8_79,
author = {Wei, Hengyang and Liu, Qingjie and Wang, Yunhong and Zhu, Chao and Huang, Di},
title = {Texture Classification via Local Feature Representation of Multi-order Gradients},
year = {2013},
isbn = {978-3-319-03730-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03731-8_79},
doi = {10.1007/978-3-319-03731-8_79},
abstract = {This paper presents a novel method to texture classification using local feature representation of multiple order gradients. Different from the state of the art approaches in literature that make use of the widely-used first order gradient based local descriptors, e.g. LBP, HOG, DAISY, SIFT, etc., we claim that the second order gradient based ones also provide critical contribution to classification performance, and thus propose to use Histogram of Second Order Gradients (HSOG) to describe micro-texton patterns. Both the similarity measurements of first and second order gradients computed by Bag-of-Feature modeling and SVM classifier are combined for decision making. Experimental results achieved on the Outex_TC dataset not only illustrate that the second order gradient based HSOG is effective to classify texture images, but also highlight that multiple order gradient based description by fusing complementary clues of the first and second order gradients is a promising solution to improve the accuracy in texture classification.},
booktitle = {Advances in Multimedia Information Processing – PCM 2013: 14th Pacific-Rim Conference on Multimedia, Nanjing, China, December 13-16, 2013. Proceedings},
pages = {846–855},
numpages = {10},
keywords = {multi-order gradients, texture classification, Local image descriptor},
location = {Nanjing, China}
}

@article{10.3233/IDA-170887,
title = {Editorial},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {5},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-170887},
doi = {10.3233/IDA-170887},
journal = {Intell. Data Anal.},
month = jan,
pages = {1037–1039},
numpages = {3}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.1007/s11263-018-1112-4,
author = {Zhang, Dingwen and Han, Junwei and Zhao, Long and Meng, Deyu},
title = {Leveraging Prior-Knowledge for Weakly Supervised Object Detection Under a Collaborative Self-Paced Curriculum Learning Framework},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {4},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1112-4},
doi = {10.1007/s11263-018-1112-4},
abstract = {Weakly supervised object detection is an interesting yet challenging research topic in computer vision community, which aims at learning object models to localize and detect the corresponding objects of interest only under the supervision of image-level annotation. For addressing this problem, this paper establishes a novel weakly supervised learning framework to leverage both the instance-level prior-knowledge and the image-level prior-knowledge based on a novel collaborative self-paced curriculum learning (C-SPCL) regime. Under the weak supervision, C-SPCL can leverage helpful prior-knowledge throughout the whole learning process and collaborate the instance-level confidence inference with the image-level confidence inference in a robust way. Comprehensive experiments on benchmark datasets demonstrate the superior capacity of the proposed C-SPCL regime and the proposed whole framework as compared with state-of-the-art methods along this research line.},
journal = {Int. J. Comput. Vision},
month = apr,
pages = {363–380},
numpages = {18},
keywords = {Weakly supervised learning, Self-paced larning, Object detection}
}

@inproceedings{10.1007/978-3-030-33246-4_45,
author = {Gonz\'{a}lez-Rojas, Oscar and Tafurth, Juan},
title = {Multi-cloud Services Configuration Based on Risk Optimization},
year = {2019},
isbn = {978-3-030-33245-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33246-4_45},
doi = {10.1007/978-3-030-33246-4_45},
abstract = {Nowadays risk analysis becomes critical in the Cloud Computing domain due to the increasing number of threats affecting applications running on cloud infrastructures. Multi-cloud environments allow connecting and migrating services from multiple cloud providers to manage risks. This paper addresses the question of how to model and configure multi-cloud services that can adapt to changes in user preferences and threats on individual and composite services. We propose an approach that combines Product Line (PL) and Machine Learning (ML) techniques to model and timely find optimal configurations of large adaptive systems such as multi-cloud services. A three-layer variability modeling on domain, user preferences, and adaptation constraints is proposed to configure multi-cloud solutions. ML regression algorithms are used to quantify the risk of resulting configurations by analyzing how a service was affected by incremental threats over time. An experimental evaluation on a real life electronic identification and trust multi-cloud service shows the applicability of the proposed approach to predict the risk for alternative re-configurations on autonomous and decentralized services that continuously change their availability and provision attributes.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&amp;TC 2019, Rhodes, Greece, October 21–25, 2019, Proceedings},
pages = {733–749},
numpages = {17},
keywords = {Machine learning, Risk optimization, Product line configuration, Variability modeling, Multi-cloud services},
location = {Rhodes, Greece}
}

@article{10.1007/s10270-017-0641-6,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Enabling automated requirements reuse and configuration},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0641-6},
doi = {10.1007/s10270-017-0641-6},
abstract = {A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2177–2211},
numpages = {35},
keywords = {Reuse, Requirements, Product line, Feature model, Configuration}
}

@inbook{10.5555/3454287.3455282,
author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
title = {Data parameters: a new family of parameters for learning a differentiable curriculum},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFAR10, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, without any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {995},
numpages = {11}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Combinatorial optimization, Multi-armed bandits, Machine learning, A/B testing, Continuous experimentation}
}

@article{10.1016/j.engappai.2018.06.002,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Yu, Yang and Yuan, Bo-Wen and Tang, Jia-Fu},
title = {Integration of an improved dynamic ensemble selection approach to enhance one-vs-one scheme},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.002},
doi = {10.1016/j.engappai.2018.06.002},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {43–53},
numpages = {11},
keywords = {Multi-class classification, Decomposition strategy, One-vs-one, Heterogeneous ensemble, Dynamic selection}
}

@article{10.1007/s10270-018-0658-5,
author = {Zolotas, Athanasios and Matragkas, Nicholas and Devlin, Sam and Kolovos, Dimitrios S. and Paige, Richard F.},
title = {Type inference in flexible model-driven engineering using classification algorithms},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0658-5},
doi = {10.1007/s10270-018-0658-5},
abstract = {Flexible or bottom-up model-driven engineering (MDE) is an emerging approach to domain and systems modelling. Domain experts, who have detailed domain knowledge, typically lack the technical expertise to transfer this knowledge using traditional MDE tools. Flexible MDE approaches tackle this challenge by promoting the use of simple drawing tools to increase the involvement of domain experts in the language definition process. In such approaches, no metamodel is created upfront, but instead the process starts with the definition of example models that will be used to infer the metamodel. Pre-defined metamodels created by MDE experts may miss important concepts of the domain and thus restrict their expressiveness. However, the lack of a metamodel, that encodes the semantics of conforming models has some drawbacks, among others that of having models with elements that are unintentionally left untyped. In this paper, we propose the use of classification algorithms to help with the inference of such untyped elements. We evaluate the proposed approach in a number of random generated example models from various domains. The correct type prediction varies from 23 to 100% depending on the domain, the proportion of elements that were left untyped and the prediction algorithm used.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {345–366},
numpages = {22},
keywords = {Type inference, Random forests, Model-driven engineering, Flexible model-driven engineering, Classification and regression trees, Bottom-up metamodelling}
}

@article{10.1016/j.aei.2015.06.001,
author = {Sajadfar, Narges and Ma, Yongsheng},
title = {A hybrid cost estimation framework based on feature-oriented data mining approach},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {29},
number = {3},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2015.06.001},
doi = {10.1016/j.aei.2015.06.001},
abstract = {A new cost estimation method combining feature-based engineering concept with data mining algorithms.Leverages empirical linear regression and data-mining algorithms with historical data.Considers gradual implementation in phases while building up the data mining capability.A case study shows steps determining the costs of the example welding features.Aiming for a highly practical solution for applications. This paper presents an informatics framework to apply feature-based engineering concept for cost estimation supported with data mining algorithms. The purpose of this research work is to provide a practical procedure for more accurate cost estimation by using the commonly available manufacturing process data associated with ERP systems. The proposed method combines linear regression and data-mining techniques, leverages the unique strengths of the both, and creates a mechanism to discover cost features. The final estimation function takes the user's confidence level over each member technique into consideration such that the application of the method can phase in gradually in reality by building up the data mining capability. A case study demonstrates the proposed framework and compares the results from empirical cost prediction and data mining. The case study results indicate that the combined method is flexible and promising for determining the costs of the example welding features. With the result comparison between the empirical prediction and five different data mining algorithms, the ANN algorithm shows to be the most accurate for welding operations.},
journal = {Adv. Eng. Inform.},
month = aug,
pages = {633–647},
numpages = {15},
keywords = {Welding feature, Feature modeling, ERP, Data mining, Cost estimation}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@article{10.1016/j.knosys.2019.105321,
author = {Gangavarapu, Tushaar and Jayasimha, Aditya and Krishnan, Gokul S. and S., Sowmya Kamath},
title = {Predicting ICD-9 code groups with fuzzy similarity based supervised multi-label classification of unstructured clinical nursing notes},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105321},
doi = {10.1016/j.knosys.2019.105321},
journal = {Know.-Based Syst.},
month = feb,
numpages = {16},
keywords = {Natural language processing, Machine learning, ICD-9 code group prediction, Healthcare analytics, Disease prediction, Clinical decision support systems}
}

@inproceedings{10.1145/3394486.3403107,
author = {Luo, Junyu and Ye, Muchao and Xiao, Cao and Ma, Fenglong},
title = {HiTANet: Hierarchical Time-Aware Attention Networks for Risk Prediction on Electronic Health Records},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403107},
doi = {10.1145/3394486.3403107},
abstract = {Deep learning methods especially recurrent neural network based models have demonstrated early success in disease risk prediction on longitudinal patient data. Existing works follow a strong assumption to implicitly assume the stationary disease progression during each time period, and thus, take a homogeneous way to decay the information from previous time steps for all patients. However,in reality, disease progression is non-stationary. Besides, the key time steps for a target disease vary among patients. To leverage time information for risk prediction in a more reasonable way, we propose a new hierarchical time-aware attention network, named HiTANet, which imitates the decision making process of doctors inrisk prediction. Particularly, HiTANet models time information in local and global stages. The local evaluation stage has a time aware Transformer that embeds time information into visit-level embed-ding and generates local attention weight for each visit. The global synthesis stage further adopts a time-aware key-query attention mechanism to assign global weights to different time steps. Finally, the two types of attention weights are dynamically combined to generate the patient representations for further risk prediction. We evaluate HiTANet on three real-world datasets. Compared with the best results among twelve competing baselines, HiTANet achieves over 7% in terms of F1 score on all datasets, which demonstrates the effectiveness of the proposed model and the necessity of modeling time information in risk prediction task.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {647–656},
numpages = {10},
keywords = {transformer, risk prediction, healthcare informatics, attention mechanism},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1016/j.neucom.2012.11.050,
author = {Fiore, Ugo and Palmieri, Francesco and Castiglione, Aniello and De Santis, Alfredo},
title = {Network anomaly detection with the restricted Boltzmann machine},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {122},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2012.11.050},
doi = {10.1016/j.neucom.2012.11.050},
abstract = {With the rapid growth and the increasing complexity of network infrastructures and the evolution of attacks, identifying and preventing network abuses is getting more and more strategic to ensure an adequate degree of protection from both external and internal menaces. In this scenario many techniques are emerging for inspecting network traffic and discriminating between anomalous and normal behaviors to detect undesired or suspicious activities. Unfortunately, the concept of normal or abnormal network behavior depends on several factors and its recognition requires the availability of a model aiming at characterizing current behavior, based on a statistical idealization of past events. There are two main challenges when generating the training data needed for effective modeling. First, network traffic is very complex and unpredictable, and second, the model is subject to changes over time, since anomalies are continuously evolving. As attack techniques and patterns change, previously gained information about how to tell them apart from normal traffic may be no longer valid. Thus, a desirable characteristic of an effective model for network anomaly detection is its ability to adapt to change and to generalize its behavior to multiple different network environments. In other words, a self-learning system is needed. This suggests the adoption of machine learning techniques to implement semi-supervised anomaly detection systems where the classifier is trained with ''normal'' traffic data only, so that knowledge about anomalous behaviors can be constructed and evolve in a dynamic way. For this purpose we explored the effectiveness of a detection approach based on machine learning, using the Discriminative Restricted Boltzmann Machine to combine the expressive power of generative models with good classification accuracy capabilities to infer part of its knowledge from incomplete training data.},
journal = {Neurocomput.},
month = dec,
pages = {13–23},
numpages = {11},
keywords = {Semi-supervised learning, Restricted Boltzmann machine, Intrusion detection, Energy-based models, Anomaly detection}
}

@article{10.1007/s11042-018-6256-2,
author = {Mansour, Asma and Chenchah, Farah and Lachiri, Zied},
title = {Emotional speaker recognition in real life conditions using multiple descriptors and i-vector speaker modeling technique},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6256-2},
doi = {10.1007/s11042-018-6256-2},
abstract = {Emotional speaker recognition under real life conditions becomes an urgent need for several applications. This paper proposes a novel approach using multiple feature extraction methods and i-vector modeling technique in order to improve emotional speaker recognition under real conditions. The performance of the proposed approach is evaluated on real condition speech signal (IEMOCAP corpus) under clean and noisy environments using various SNR levels. We examined divers known spectral features in speaker recognition (MFCC, LPCC and RASTA-PLP) and performed combined features called MFCC-SDC coefficients. The feature vectors are then classified using the multiclass Support Vector Machines (SVM). Experimental results illustrate good robustness of the proposed system against talking conditions (emotions) and against real life environment (noise). Besides, results reveal that MFCC-SDC features outperforms the conventional MFCCs.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {6441–6458},
numpages = {18},
keywords = {Speaker recognition, SVM, Noise, MFCC-SDC, I-vector, Emotion}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1145/3155806,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Yao, Lina and Taylor, Kerry and Shemshadi, Ali and Qin, Yongrui},
title = {A Learning-Based Framework for Improving Querying on Web Interfaces of Curated Knowledge Bases},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3155806},
doi = {10.1145/3155806},
abstract = {Knowledge Bases (KBs) are widely used as one of the fundamental components in Semantic Web applications as they provide facts and relationships that can be automatically understood by machines. Curated knowledge bases usually use Resource Description Framework (RDF) as the data representation model. To query the RDF-presented knowledge in curated KBs, Web interfaces are built via SPARQL Endpoints. Currently, querying SPARQL Endpoints has problems like network instability and latency, which affect the query efficiency. To address these issues, we propose a client-side caching framework, SPARQL Endpoint Caching Framework (SECF), aiming at accelerating the overall querying speed over SPARQL Endpoints. SECF identifies the potential issued queries by leveraging the querying patterns learned from clients’ historical queries and prefecthes/caches these queries. In particular, we develop a distance function based on graph edit distance to measure the similarity of SPARQL queries. We propose a feature modelling method to transform SPARQL queries to vector representation that are fed into machine-learning algorithms. A time-aware smoothing-based method, Modified Simple Exponential Smoothing (MSES), is developed for cache replacement. Extensive experiments performed on real-world queries showcase the effectiveness of our approach, which outperforms the state-of-the-art work in terms of the overall querying speed.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {35},
numpages = {20},
keywords = {query suggestion, caching, SPARQL, Knowledge base query-answering}
}

@article{10.1007/s10772-012-9166-0,
author = {Hossan, M. Afzal and Gregory, Mark A.},
title = {Speaker recognition utilizing distributed DCT-II based Mel frequency cepstral coefficients and fuzzy vector quantization},
year = {2013},
issue_date = {March     2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-012-9166-0},
doi = {10.1007/s10772-012-9166-0},
abstract = {In this paper, a new and novel Automatic Speaker Recognition (ASR) system is presented. The new ASR system includes novel feature extraction and vector classification steps utilizing distributed Discrete Cosine Transform (DCT-II) based Mel Frequency Cepstral Coefficients (MFCC) and Fuzzy Vector Quantization (FVQ). The ASR algorithm utilizes an approach based on MFCC to identify dynamic features that are used for Speaker Recognition (SR). A series of experiments were performed utilizing three different feature extraction methods: (1) conventional MFCC; (2) Delta-Delta MFCC (DDMFCC); and (3) DCT-II based DDMFCC. The experiments were then expanded to include four classifiers: (1) FVQ; (2) K-means Vector Quantization (VQ); (3) Linde, Buzo and Gray VQ; and (4) Gaussian Mixed Model (GMM). The combination of DCT-II based MFCC, DMFCC and DDMFCC with FVQ was found to have the lowest Equal Error Rate for the VQ based classifiers. The results found were an improvement over previously reported non-GMM methods and approached the results achieved for the computationally expensive GMM based method. Speaker verification tests carried out highlighted the overall performance improvement for the new ASR system. The National Institute of Standards and Technology Speaker Recognition Evaluation corpora was used to provide speaker source data for the experiments.},
journal = {Int. J. Speech Technol.},
month = mar,
pages = {103–113},
numpages = {11},
keywords = {Speech feature extraction, Speaker recognition, Mel frequency cepstral coefficients, Linde---Buzo---Gray, K-Means, Fuzzy vector quantization, Discrete cosine transform}
}

@inproceedings{10.1145/3240323.3240383,
author = {Bharadhwaj, Homanga and Park, Homin and Lim, Brian Y.},
title = {RecGAN: recurrent generative adversarial networks for recommendation systems},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240383},
doi = {10.1145/3240323.3240383},
abstract = {Recent studies in recommendation systems emphasize the significance of modeling latent features behind temporal evolution of user preference and item state to make relevant suggestions. However, static and dynamic behaviors and trends of users and items, which highly influence the feasibility of recommendations, were not adequately addressed in previous works. In this work, we leverage the temporal and latent feature modelling capabilities of Recurrent Neural Network (RNN) and Generative Adversarial Network (GAN), respectively, to propose a Recurrent Generative Adversarial Network (RecGAN). We use customized Gated Recurrent Unit (GRU) cells to capture latent features of users and items observable from short-term and long-term temporal profiles. The modification also includes collaborative filtering mechanisms to improve the relevance of recommended items. We evaluate RecGAN using two datasets on food and movie recommendation. Results indicate that our model outperforms other baseline models irrespective of user behavior and density of training data.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {372–376},
numpages = {5},
keywords = {recurrent neural networks, recommendation systems, generative adversarial networks},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@article{10.1016/j.neucom.2014.06.096,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng and Wang, Yanjiang and Lu, Ke},
title = {Manifold regularized kernel logistic regression for web image annotation},
year = {2016},
issue_date = {Jan 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {172},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.06.096},
doi = {10.1016/j.neucom.2014.06.096},
journal = {Neurocomput.},
month = jan,
pages = {3–8},
numpages = {6},
keywords = {Manifold regularization, Kernel logistic regression, Laplacian eigenmaps, Semi-supervised learning, Image annotation}
}

@article{10.1016/j.ins.2018.06.014,
author = {Ma, Zilu and Liu, Shiqi and Meng, Deyu and Zhang, Yong and Lo, SioLong and Han, Zhi},
title = {On Convergence Properties of Implicit Self-paced Objective},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {462},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.06.014},
doi = {10.1016/j.ins.2018.06.014},
journal = {Inf. Sci.},
month = sep,
pages = {132–140},
numpages = {9},
keywords = {99-00, 00-01, Convergence, Non-convex optimization, Machine learning, Self-paced learning}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@inproceedings{10.5555/3045390.3045510,
author = {Zhang, Aonan and Paisley, John},
title = {Markov latent feature models},
year = {2016},
publisher = {JMLR.org},
abstract = {We introduce Markov latent feature models (MLFM), a sparse latent feature model that arises naturally from a simple sequential construction. The key idea is to interpret each state of a sequential process as corresponding to a latent feature, and the set of states visited between two null-state visits as picking out features for an observation. We show that, given some natural constraints, we can represent this stochastic process as a mixture of recurrent Markov chains. In this way we can perform correlated latent feature modeling for the sparse coding problem. We demonstrate two cases in which we define finite and infinite latent feature models constructed from first-order Markov chains, and derive their associated scalable inference algorithms. We show empirical results on a genome analysis task and an image denoising task.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1129–1137},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1007/978-3-031-12700-7_56,
author = {Dutta, Suparna and Das, Monidipa},
title = {PReLim: A Modeling Paradigm for&nbsp;Remote Sensing Image Scene Classification Under Limited Labeled Samples},
year = {2021},
isbn = {978-3-031-12699-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12700-7_56},
doi = {10.1007/978-3-031-12700-7_56},
abstract = {With the ongoing development of deep learning techniques in recent years, the convolutional neural networks (CNNs) have shown remarkable performance breakthrough in remote sensing image scene classification. However, the performance of these deep models largely depends on the number of available training samples or labeled images. Although the knowledge transferring and pre-training techniques can handle such situation, these may become ineffective due to domain difference. On the other side, the existing data augmentation approaches often produce training samples with too low diversity to help in performance improvement. In order to address these issues, in this work, we propose PReLim as a novel modeling paradigm for remote sensing scene classification under limited labeled samples scenario. PReLim is based on the notion of local and global filtering of scene fragment mixture, which overcomes both the sample diversity and the domain difference issue. Experimental analyses with the benchmark UCMerced and SIRI-WHU datasets demonstrate the effectiveness of PReLim in achieving the state-of-the-art accuracy using limited number of training samples.},
booktitle = {Pattern Recognition and Machine Intelligence: 9th International Conference, PReMI 2021, Kolkata, India, December 15–18, 2021, Proceedings},
pages = {545–555},
numpages = {11},
keywords = {Scene classification, Machine learning, Remote sensing},
location = {Kolkata, India}
}

@article{10.5555/3288992.3288997,
author = {Vinci, Giuseppe and Ventura, Val\'{e}rie and Smith, Matthew A. and Kass, Robert E.},
title = {Adjusted regularization of cortical covariance},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {45},
number = {2},
issn = {0929-5313},
abstract = {It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an L1$L_{1}$ penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4.},
journal = {J. Comput. Neurosci.},
month = oct,
pages = {83–101},
numpages = {19},
keywords = {Penalized maximum likelihood estimation, Macaque visual cortex, High-dimensional estimation, Graphical lasso, Gaussian graphical model, Functional connectivity, False discovery rate, Bayesian inference}
}

@article{10.1016/j.asoc.2016.05.020,
author = {Sachdeva, Jainy and Kumar, Vinod and Gupta, Indra and Khandelwal, Niranjan and Ahuja, Chirag Kamal},
title = {A package-SFERCB-"Segmentation, feature extraction, reduction and classification analysis by both SVM and ANN for brain tumors"},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.020},
doi = {10.1016/j.asoc.2016.05.020},
abstract = {An interactive computer aided dignostic (CAD) system for assisting inexperience young radiologists is developed. The difficulty in brain tumors classification is due to similar size, shape, location, hetrogeniety, presence of oedema, cystic and isointense regions has been the key feature of this research. Genetic Algorithm is employed as it is an easy concept and is well understood by radiologists without going in much depth of engineering.Display Omitted Brain tumors as segmented regions of interests (SROIs) by content based active contour model (CBAC).Feature extraction-intensity and texture based features.Feature reduction by Genetic Algorithm.Classification by Hybrid Models-GA-SVM and GA-ANN. The objective of this experimentation is to develop an interactive CAD system for assisting radiologists in multiclass brain tumor classification. The study is performed on a diversified dataset of 428 post contrast T1-weighted MR images of 55 patients and publically available dataset of 260 post contrast T1-weighted MR images of 10 patients. The first dataset includes primary brain tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), childhood tumor-Medulloblastoma (MED) and Meningioma (MEN), along with secondary tumor-Metastatic (MET). The second dataset consists of Astrocytoma (AS), Low Grade Glioma (LGL) and Meningioma (MEN). The tumor regions are marked by content based active contour (CBAC) model. The regions are than saved as segmented regions of interest (SROIs). 71 intensity and texture feature set is extracted from these SROIs. The features are specifically selected based on the pathological details of brain tumors provided by the radiologist. Genetic Algorithm (GA) selects the set of optimal features from this input set. Two hybrid machine learning models are implemented using GA with support vector machine (SVM) and artificial neural network (ANN) (GA-SVM and GA-ANN) and are tested on two different datasets. GA-SVM is proposed for finding preliminary probability in identifying tumor class and GA-ANN is used for confirmation of accuracy. Test results of the first dataset show that the GA optimization technique has enhanced the overall accuracy of SVM from 79.3% to 91.7% and of ANN from 75.6% to 94.9%. Individual class accuracies delivered by GA-SVM are: AS-89.8%, GBM-83.3%, MED-95.6%, MEN-91.8%, and MET-97.1%. Individual class accuracies delivered by GA-ANN classifier are: AS-96.6%, GBM-86.6%, MED-93.3%, MEN-96%, MET-100%. Similar results are obtained for the second dataset. The overall accuracy of SVM has increased from 80.8% to 89% and that of ANN has increased from 77.5% to 94.1%. Individual class accuracies delivered by GA-SVM are: AS-85.3%, LGL-88.8%, MEN-93%. Individual class accuracies delivered by GA-ANN classifier are: AS-92.6%, LGL-94.4%, MED-95.3%. It is observed from the experiments that GA-ANN classifier has provided better results than GA-SVM. Further, it is observed that along with providing finer results, GA-SVM provides advantage in speed whereas GA-ANN provides advantage in accuracy. The combined results from both the classifiers will benefit the radiologists in forming a better decision for classifying brain tumors.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {151–167},
numpages = {17},
keywords = {Genetic Algorithm (GA), GA-SVM, GA-ANN, Brain tumors}
}

@article{10.1016/j.patcog.2018.11.030,
author = {Wang, Xiaohong and Jiang, Xudong and Ren, Jianfeng},
title = {Blood vessel segmentation from fundus image by a cascade classification framework},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.030},
doi = {10.1016/j.patcog.2018.11.030},
journal = {Pattern Recogn.},
month = apr,
pages = {331–341},
numpages = {11},
keywords = {Dimensionality reduction, Cascade classification, Retinal vessel segmentation, Fundus image}
}

@article{10.1007/s10586-017-1108-9,
author = {Ilavarasi, A. K. and Sathiyabhama, B.},
title = {An evolutionary feature set decomposition based anonymization for classification workloads: Privacy Preserving Data Mining},
year = {2017},
issue_date = {Dec 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1108-9},
doi = {10.1007/s10586-017-1108-9},
abstract = {Privacy has become an important concern while publishing micro data about a population. The emerging area called privacy preserving data mining (PPDM) focus on individual privacy without compromising data mining results. An adversarial exploitation of published data poses a risk of information disclosure about individuals. On the other hand, imposing privacy constraints on the data results in substantial information loss and compromises the legitimate data analysis. Motivated by the increasing growth of PPDM algorithms, we first investigate the privacy implications and the crosscutting issues between privacy versus utility of data. We present a privacy model that embeds the anonymization procedure in to a learning algorithm and this has mitigated the additional overheads imposed on data mining tasks. Our primary concern about PPDM is that the utility of data should not be compromised by the transformation applied. Different data mining classification workloads are analyzed with the proposed anonymization procedure for any side effects incurred. It is shown empirically that classification accuracy obtained for most of the datasets outperforms the results obtained with original dataset.},
journal = {Cluster Computing},
month = dec,
pages = {3515–3525},
numpages = {11},
keywords = {Privacy, Evolutionary partitioning, Decomposition, Data mining, Classification, Anonymization}
}

@article{10.1007/s10489-020-01730-3,
author = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
title = {Query set centered sparse projection learning for set based image classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01730-3},
doi = {10.1007/s10489-020-01730-3},
abstract = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
journal = {Applied Intelligence},
month = oct,
pages = {3400–3411},
numpages = {12},
keywords = {Discriminate subspace learning, Set based image classification, Sparse projection learning, Query set}
}

@inproceedings{10.5555/3504035.3504869,
author = {Fan, Xin and Liu, Risheng and Huyan, Kang and Feng, Yuyao and Luo, Zhongxuan},
title = {Self-reinforced cascaded regression for face alignment},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {834},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.neucom.2019.08.002,
author = {Li, Zhenglai and Tang, Chang and Chen, Jiajia and Wan, Cheng and Yan, Weiqing and Liu, Xinwang},
title = {Diversity and consistency learning guided spectral embedding for multi-view clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.002},
doi = {10.1016/j.neucom.2019.08.002},
journal = {Neurocomput.},
month = dec,
pages = {128–139},
numpages = {12},
keywords = {99-00, 00-01, Diversity and consistency learning, Spectral embedding, Multi-view clustering}
}

@article{10.1007/s00034-018-0962-x,
author = {China Bhanja, Chuya and Laskar, Mohammad Azharuddin and Laskar, Rabul Hussain},
title = {A Pre-classification-Based Language Identification for Northeast Indian Languages Using Prosody and Spectral Features},
year = {2019},
issue_date = {May       2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {5},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-018-0962-x},
doi = {10.1007/s00034-018-0962-x},
abstract = {This paper is aimed at developing a two-stage language identification (LID) system for Northeast Indian languages. In the first stage, languages are pre-classified into tonal and non-tonal categories, and in the second stage, individual languages are identified from languages of the corresponding category. In this work, new parameters to model the prosodic characteristics of the speech signal have been proposed for pre-classification as well as individual language identification. Also, the effectiveness of spectral features, namely Mel-frequency cepstral coefficient (MFCC) and their combination with prosodic features, has been studied for pre-classification task. The usefulness of MFCC with their delta and acceleration coefficients in combination with prosodic features has been investigated for individual language identification. The performance of the system is analyzed for the features extracted of different analysis units, such as syllable, disyllable, word, and utterance. Comparative performance analysis of three different classifiers, namely artificial neural network (ANN), Gaussian mixture model---Universal background model (GMM---UBM), and i-vector based support vector machine (i-vector based SVM), has been made for pre-classification as well as individual language identification. A new database, NIT Silchar language database (NITS-LD), has been developed for seven NE Indian languages using All India Radio broadcast news. The experimental analysis suggests that the parameters proposed to represent the prosodic characteristics help to improve the performance of both the stages and show improvements over existing parameters by as much as 7.4%, 11.9%, and 9.1% for 30 s, 10 s, and 3 s test data, respectively, in the pre-classification stage. Of the baseline single-stage systems, GMM---UBM provides the highest accuracies of 80%, 76.8%, and 72% for 30 s, 10 s, and 3 s test data, respectively. In the proposed system, the combination of the ANN model in pre-classification stage and the GMM---UBM model in individual language identification stage provides the highest accuracies, and it shows the improvements over the baseline system by 7.2%, 7%, and 4.9% for 30 s, 10 s, and 3 s test data. For OGI-Multilingual (OGI-MLTS) database, improvements of 8.1%, 7.4%, and 5.7% for 30 s, 10 s, and 3 s test data, respectively, are observed over the baseline LID system.},
journal = {Circuits Syst. Signal Process.},
month = may,
pages = {2266–2296},
numpages = {31},
keywords = {Syllables, Pre-classification of tonal and non-tonal languages, Language identification, Features, Database, Classifiers}
}

@article{10.1016/j.patrec.2011.06.026,
author = {Fu, Zhouyu and Lu, Guojun and Ting, Kai Ming and Zhang, Dengsheng},
title = {Music classification via the bag-of-features approach},
year = {2011},
issue_date = {October, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {32},
number = {14},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2011.06.026},
doi = {10.1016/j.patrec.2011.06.026},
abstract = {A central problem in music information retrieval is audio-based music classification. Current music classification systems follow a frame-based analysis model. A whole song is split into frames, where a feature vector is extracted from each local frame. Each song can then be represented by a set of feature vectors. How to utilize the feature set for global song-level classification is an important problem in music classification. Previous studies have used summary features and probability models which are either overly restrictive in modeling power or numerically too difficult to solve. In this paper, we investigate the bag-of-features approach for music classification which can effectively aggregate the local features for song-level feature representation. Moreover, we have extended the standard bag-of-features approach by proposing a multiple codebook model to exploit the randomness in the generation of codebooks. Experimental results for genre classification and artist identification on benchmark data sets show that the proposed classification system is highly competitive against the standard methods.},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {1768–1777},
numpages = {10},
keywords = {Support Vector Machines, Pattern classification, Music classification, Multiple codebook model, Bag-of-features}
}

@article{10.1007/s13748-019-00203-0,
author = {Dhillon, Anamika and Verma, Gyanendra K.},
title = {Convolutional neural network: a review of models, methodologies and applications to object detection},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-019-00203-0},
doi = {10.1007/s13748-019-00203-0},
abstract = {Deep learning has developed as an effective machine learning method that takes in numerous layers of features or representation of the data and provides state-of-the-art results. The application of deep learning has shown impressive performance in various application areas, particularly in image classification, segmentation and object detection. Recent advances of deep learning techniques bring encouraging performance to fine-grained image classification which aims to distinguish subordinate-level categories. This task is extremely challenging due to high intra-class and low inter-class variance. In this paper, we provide a detailed review of various deep architectures and model highlighting characteristics of particular model. Firstly, we described the functioning of CNN architectures and its components followed by detailed description of various CNN models starting with classical LeNet model to AlexNet, ZFNet, GoogleNet, VGGNet, ResNet, ResNeXt, SENet, DenseNet, Xception, PNAS/ENAS. We mainly focus on the application of deep learning architectures to three major applications, namely (i) wild animal detection, (ii) small arm detection and (iii) human being detection. A detailed review summary including the systems, database, application and accuracy claimed is also provided for each model to serve as guidelines for future work in the above application areas.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {85–112},
numpages = {28},
keywords = {Object detection, Transfer learning, CNN architectures, Deep learning}
}

@article{10.1007/s10489-020-01951-6,
author = {Yan, Cairong and Chen, Yizhou and Wan, Yongquan and Wang, Pengwei},
title = {Modeling low- and high-order feature interactions with FM and self-attention network},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01951-6},
doi = {10.1007/s10489-020-01951-6},
abstract = {Click-Through Rate (CTR) prediction has always been a very popular topic. In many online applications, such as online advertising and product recommendation, a small increase in CTR will bring great returns. However, CTR prediction has always faced several challenges. A large number of users and items and the different sizes of the feature space of different data types lead to high-dimensional and sparse input, and high-order feature interactions rely too much on expert knowledge and are very time-consuming. In this paper, we build a novel model called multi-order interactive features aware factorization machine (MoFM) for CTR prediction. To effectively capturing both low-order and high-order interactive features, three different types of prediction models are integrated, of which logistic regression (LR) and factorization machine (FM) model the original features and 2-order interactive features respectively, and a multi-head self-attention network with residual connections is used to automatically identify high-value high-order feature combinations. There is also an embedding layer in the model to realize a unified embedding processing of different data types, avoiding diversification, sparsity, and high dimensionality of features. Since, feature engineering is not required, we can carry out end-to-end model learning. Experiments on three public datasets show the superiority of the proposed model over the state-of-the-art models, and the flexibility and scalability of the model structure have also been verified.},
journal = {Applied Intelligence},
month = jun,
pages = {3189–3201},
numpages = {13},
keywords = {CTR prediction, Multi-head self-attention, Feature interaction, Factorization machine}
}

@article{10.1007/s10618-016-0475-9,
author = {Garcia, Lu\'{\i}s P. and Lorena, Ana C. and Matwin, Stan and Carvalho, Andr\'{e} C.},
title = {Ensembles of label noise filters: a ranking approach},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-016-0475-9},
doi = {10.1007/s10618-016-0475-9},
abstract = {Label noise can be a major problem in classification tasks, since most machine learning algorithms rely on data labels in their inductive process. Thereupon, various techniques for label noise identification have been investigated in the literature. The bias of each technique defines how suitable it is for each dataset. Besides, while some techniques identify a large number of examples as noisy and have a high false positive rate, others are very restrictive and therefore not able to identify all noisy examples. This paper investigates how label noise detection can be improved by using an ensemble of noise filtering techniques. These filters, individual and ensembles, are experimentally compared. Another concern in this paper is the computational cost of ensembles, once, for a particular dataset, an individual technique can have the same predictive performance as an ensemble. In this case the individual technique should be preferred. To deal with this situation, this study also proposes the use of meta-learning to recommend, for a new dataset, the best filter. An extensive experimental evaluation of the use of individual filters, ensemble filters and meta-learning was performed using public datasets with imputed label noise. The results show that ensembles of noise filters can improve noise filtering performance and that a recommendation system based on meta-learning can successfully recommend the best filtering technique for new datasets. A case study using a real dataset from the ecological niche modeling domain is also presented and evaluated, with the results validated by an expert.},
journal = {Data Min. Knowl. Discov.},
month = sep,
pages = {1192–1216},
numpages = {25},
keywords = {Recommendation system, Noise ranking, Noise filters, Label noise, Ensemble filters}
}

@article{10.1016/j.imavis.2019.10.004,
author = {Gu, Ye and Ye, Xiaofeng and Sheng, Weihua and Ou, Yongsheng and Li, Yongqiang},
title = {Multiple stream deep learning model for human action recognition},
year = {2020},
issue_date = {Jan 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {93},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2019.10.004},
doi = {10.1016/j.imavis.2019.10.004},
journal = {Image Vision Comput.},
month = jan,
numpages = {8},
keywords = {Action recognition, Information fusion, Deep learning}
}

@inproceedings{10.1007/978-3-030-15712-8_6,
author = {Almquist, Axel and Jatowt, Adam},
title = {Towards Content Expiry Date Determination: Predicting Validity Periods of Sentences},
year = {2019},
isbn = {978-3-030-15711-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-15712-8_6},
doi = {10.1007/978-3-030-15712-8_6},
abstract = {Knowing how long text content will remain valid can be useful in many cases such as supporting the creation of documents to prolong their usefulness, improving document retrieval or enhancing credibility estimation. In this paper we introduce a novel research task of forecasting content’s validity period. Given an input sentence the task is to approximately determine until when the information stated in the content will remain valid. We propose machine learning approaches equipped with NLP and statistical features that can successfully work on a relatively small number of annotated data.},
booktitle = {Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14–18, 2019, Proceedings, Part I},
pages = {86–101},
numpages = {16},
keywords = {Content validity scope estimation, Text classification, Natural language processing, Machine learning},
location = {Cologne, Germany}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3451421.3451427,
author = {Liu, Xiaoli and Li, Jiali and Cao, Peng},
title = {SP-MTFL: A self paced multi-task feature learning method for cognitive performance predicting of Alzheimer's disease},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451427},
doi = {10.1145/3451421.3451427},
abstract = {Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Predicting cognitive performance of subjects from neuroimage measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimer's disease. Multi-task based feature learning (MTFL) have been widely studied to select a discriminative feature subset from MRI features, and improve the performance by incorporating inherent correlations among multiple clinical cognitive measures. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, we propose a self-paced multi-task feature learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task in this study. Experimental results on ADNI are provided, and the comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {23–27},
numpages = {5},
keywords = {regression, multi-task learning, Self-paced learning, Machine learning, Alzheimer's disease},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@inproceedings{10.5555/3540261.3542579,
author = {Ostapenko, Oleksiy and Rodr\'{\i}guez, Pau and Caccia, Massimo and Charlin, Laurent},
title = {Continual learning via local module composition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2318},
numpages = {15},
series = {NIPS '21}
}

@inproceedings{10.5555/3504035.3504406,
author = {Gong, Tieliang and Wang, Guangtao and Ye, Jieping and Xu, Zongben and Lin, Ming},
title = {Margin based PU learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The PU learning problem concerns about learning from positive and unlabeled data. A popular heuristic is to iteratively enlarge training set based on some margin-based criterion. However, little theoretical analysis has been conducted to support the success of these heuristic methods. In this work, we show that not all margin-based heuristic rules are able to improve the learned classifiers iteratively. We find that a so-called large positive margin oracle is necessary to guarantee the success of PU learning. Under this oracle, a provable positive-margin based PU learning algorithm is proposed for linear regression and classification under the truncated Gaussian distributions. The proposed algorithm is able to reduce the recovering error geometrically proportional to the positive margin. Extensive experiments on real-world datasets verify our theory and the state-of-the-art performance of the proposed PU learning algorithm.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {371},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.patcog.2011.09.011,
author = {Rasmussen, Peter M. and Hansen, Lars K. and Madsen, Kristoffer H. and Churchill, Nathan W. and Strother, Stephen C.},
title = {Model sparsity and brain pattern interpretation of classification models in neuroimaging},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.09.011},
doi = {10.1016/j.patcog.2011.09.011},
abstract = {Interest is increasing in applying discriminative multivariate analysis techniques to the analysis of functional neuroimaging data. Model interpretation is of great importance in the neuroimaging context, and is conventionally based on a 'brain map' derived from the classification model. In this study we focus on the relative influence of model regularization parameter choices on both the model generalization, the reliability of the spatial patterns extracted from the classification model, and the ability of the resulting model to identify relevant brain networks defining the underlying neural encoding of the experiment. For a support vector machine, logistic regression and Fisher's discriminant analysis we demonstrate that selection of model regularization parameters has a strong but consistent impact on the generalizability and both the reproducibility and interpretable sparsity of the models for both @?"2 and @?"1 regularization. Importantly, we illustrate a trade-off between model spatial reproducibility and prediction accuracy. We show that known parts of brain networks can be overlooked in pursuing maximization of classification accuracy alone with either @?"2 and/or @?"1 regularization. This supports the view that the quality of spatial patterns extracted from models cannot be assessed purely by focusing on prediction accuracy. Our results instead suggest that model regularization parameters must be carefully selected, so that the model and its visualization enhance our ability to interpret the brain.},
journal = {Pattern Recogn.},
month = jun,
pages = {2085–2100},
numpages = {16},
keywords = {Sparsity, Regularization, Pattern analysis, Neuroimaging, NPAIRS resampling, Model interpretation, Machine learning, Kernel methods, Classification}
}

@article{10.1007/s10772-017-9429-x,
author = {Phu, Vo Ngoc and Tran, Vo Thi and Chau, Vo Thi and Dat, Nguyen Duy and Duy, Khanh Ly},
title = {A decision tree using ID3 algorithm for English semantic analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9429-x},
doi = {10.1007/s10772-017-9429-x},
abstract = {Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {593–613},
numpages = {21},
keywords = {id3, Sentiment classification, ID3 algorithm, English sentiment classification, English document opinion mining, Decision tree}
}

@article{10.1145/3345314,
author = {Wang, Qingyong and Zhou, Yun and Ding, Weiping and Zhang, Zhiguo and Muhammad, Khan and Cao, Zehong},
title = {Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3345314},
doi = {10.1145/3345314},
abstract = {Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {34},
numpages = {12},
keywords = {self-paced learning, random forest, classification, bootstrap, Lung cancer}
}

@inproceedings{10.1145/3095713.3095722,
author = {Krismayer, Thomas and Schedl, Markus and Knees, Peter and Rabiser, Rick},
title = {Prediction of User Demographics from Music Listening Habits},
year = {2017},
isbn = {9781450353335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3095713.3095722},
doi = {10.1145/3095713.3095722},
abstract = {Online activities such as social networking, shopping, and consuming multi-media create digital traces often used to improve user experience and increase revenue, e.g., through better-fitting recommendations and targeted marketing. We investigate to which extent the music listening habits of users of the social music platform Last.fm can be used to predict their age, gender, and nationality. We propose a TF-IDF-like feature modeling approach for artist listening information and artist tags combined with additionally extracted features. We show that we can substantially outperform a baseline majority voting approach and can compete with existing approaches. Further, regarding prediction accuracy vs. available listening data we show that even one single listening event per user is enough to outperform the baseline in all prediction tasks. We conclude that personal information can be derived from music listening information, which indeed can help better tailoring recommendations.},
booktitle = {Proceedings of the 15th International Workshop on Content-Based Multimedia Indexing},
articleno = {8},
numpages = {7},
keywords = {Digital User Traces, Music Listening Habits, User Trait Prediction},
location = {Florence, Italy},
series = {CBMI '17}
}

@article{10.1007/s00034-019-01094-1,
author = {Xia, Xianjun and Togneri, Roberto and Sohel, Ferdous and Zhao, Yuanjun and Huang, Defeng},
title = {A Survey: Neural Network-Based Deep Learning for Acoustic Event Detection},
year = {2019},
issue_date = {August    2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-019-01094-1},
doi = {10.1007/s00034-019-01094-1},
abstract = {Recently, neural network-based deep learning methods have been popularly applied to computer vision, speech signal processing and other pattern recognition areas. Remarkable success has been demonstrated by using the deep learning approaches. The purpose of this article is to provide a comprehensive survey for the neural network-based deep learning approaches on acoustic event detection. Different deep learning-based acoustic event detection approaches are investigated with an emphasis on both strongly labeled and weakly labeled acoustic event detection systems. This paper also discusses how deep learning methods benefit the acoustic event detection task and the potential issues that need to be addressed for prospective real-world scenarios.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3433–3453},
numpages = {21},
keywords = {Weakly labeled, Strongly labeled, Deep learning, Acoustic event detection}
}

@inproceedings{10.1007/978-3-030-32047-8_26,
author = {Khoshmanesh, Seyedehzahra and Lutz, Robyn R.},
title = {Leveraging Feature Similarity for Earlier Detection of Unwanted Feature Interactions in Evolving Software Product Lines},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_26},
doi = {10.1007/978-3-030-32047-8_26},
abstract = {Software product lines enable reuse of shared software across a family of products. As new products are built in the product line, new features are added. The features are units of functionality that provide services to users. Unwanted feature interactions, wherein one feature interferes with another feature’s operation, is a significant problem, especially as large software product lines evolve. Detecting feature interactions is a time-consuming and difficult task for developers. Moreover, feature interactions are often only discovered during testing, at which point costly re-work is needed. This paper proposes a similarity-based method to identify unwanted feature interactions much earlier in the development process. It uses knowledge of prior feature interactions stored with the software product line’s feature model to help find unwanted interactions between a new feature and existing features. The paper describes the framework and algorithms used to detect the feature interactions using three path similarity measures and evaluates the approach on a real-world, evolving software product line. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {293–307},
numpages = {15},
keywords = {Feature interaction, Similarity measures, Software product lines},
location = {Newark, NJ, USA}
}

@article{10.1016/j.ins.2019.02.051,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {         Munec: a mutual neighbor-based clustering algorithm},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {486},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.051},
doi = {10.1016/j.ins.2019.02.051},
journal = {Inf. Sci.},
month = jun,
pages = {148–170},
numpages = {23},
keywords = {Clustering, Distance, Density, Single link, Mutual neighbors}
}

@article{10.1155/2015/196098,
author = {Yang, Jinfeng and Xiao, Yong and Wang, Jiabing and Ma, Qianli and Shen, Yanhua},
title = {A fast clustering algorithm for data with a few labeled instances},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/196098},
doi = {10.1155/2015/196098},
abstract = {The diameter of a cluster is the maximum intracluster distance between pairs of instances within the same cluster, and the split of a cluster is the minimum distance between instances within the cluster and instances outside the cluster. Given a few labeled instances, this paper includes two aspects. First, we present a simple and fast clustering algorithm with the following property: if the ratio of the minimum split to the maximum diameter (RSD) of the optimal solution is greater than one, the algorithm returns optimal solutions for three clustering criteria. Second, we study the metric learning problem: learn a distance metric to make the RSD as large as possible. Compared with existing metric learning algorithms, one of our metric learning algorithms is computationally efficient: it is a linear programming model rather than a semidefinite programming model used by most of existing algorithms. We demonstrate empirically that the supervision and the learned metric can improve the clustering quality.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {21},
numpages = {1}
}

@article{10.5555/3322706.3361993,
author = {Glimsdal, Sondre and Granmo, Ole-Christoffer},
title = {Thompson sampling guided stochastic searching on the line for deceptive environments with applications to root-finding problems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the "left" or to the "right" of the arm pulled, with the feedback being erroneous with probability 1 - π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1910–1933},
numpages = {24},
keywords = {thompson sampling, stochastic point location, searching on the line, probabilistic bisection search, deceptive environment}
}

@article{10.1016/j.eswa.2019.03.031,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {A hierarchical clustering algorithm and an improvement of the single linkage criterion to deal with noise},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.031},
doi = {10.1016/j.eswa.2019.03.031},
journal = {Expert Syst. Appl.},
month = aug,
pages = {96–108},
numpages = {13},
keywords = {Density, Dissimilarity, Agglomerative}
}

@inproceedings{10.1145/3184558.3186980,
author = {Wang, Yanyan and Chen, Qun and Liu, Xin and Ahmed, Murtadha and Li, Zhanhuai and Pan, Wei and Liu, Hailong},
title = {SenHint: A Joint Framework for Aspect-level Sentiment Analysis by Deep Neural Networks and Linguistic Hints},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186980},
doi = {10.1145/3184558.3186980},
abstract = {The state-of-the-art techniques for aspect-level sentiment analysis focus on feature modeling using a variety of deep neural networks (DNN). Unfortunately, their practical performance may fall short of expectations due to semantic complexity of natural languages. Motivated by the observation that linguistic hints (e.g. explicit sentiment words and shift words) can be strong indicators of sentiment, we present a joint framework, SenHint, which integrates the output of deep neural networks and the implication of linguistic hints into a coherent reasoning model based on Markov Logic Network (MLN). In SenHint, linguistic hints are used in two ways: (1) to identify easy instances, whose sentiment can be automatically determined by machine with high accuracy; (2) to capture implicit relations between aspect polarities. We also empirically evaluate the performance of SenHint on both English and Chinese benchmark datasets. Our experimental results show that SenHint can effectively improve accuracy compared with the state-of-the-art alternatives.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {207–210},
numpages = {4},
keywords = {aspect-level sentiment analysis, deep neural networks, linguistic hints},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1007/978-3-030-66665-1_18,
author = {Yadav, Shubham and Rathore, Santosh Singh and Chouhan, Satyendra Singh},
title = {Authorship Identification Using Stylometry and Document Fingerprinting},
year = {2020},
isbn = {978-3-030-66664-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66665-1_18},
doi = {10.1007/978-3-030-66665-1_18},
abstract = {Authorship identification focuses on finding the particular author of an anonymous or unknown document by extracting various predictive features related to that document. It helps to predict the most probable author of articles, messages, codes, or news. This task is generally viewed as a multi-class, single labeled categorization of text. This topic is important and among the most interesting topics in the field of Natural Language Processing. There are many applications in which this can be applied such as identifying anonymous author, supporting crime investigation and its security, also detecting plagiarism, or finding ghostwriters. Till now, most of the existing works are based on the character n-grams of fixed length and/or variable length to classify authorship. In this work, we tackle this problem at different levels with increasing feature engineering using various text-based models and machine learning algorithms. The propose work analyses various types of stylometric features and define individual features that are high in performance for better model understanding. We evaluate the proposed methodology on a part of Reuters news corpus. It consists of texts of 50 different authors on the same topic. The experimental results suggest that, using document finger printing features enhance the accuracy of classifier. Moreover, PCA (Principal Component Analysis) further improves the results. In addition, we compare the results with other works related to the authorship identification domain.},
booktitle = {Big Data Analytics: 8th International Conference, BDA 2020, Sonepat, India, December 15–18, 2020, Proceedings},
pages = {278–288},
numpages = {11},
keywords = {Machine learning classifiers, Authorship identification, Document Finger Printing, Stylometry, Natural Language Processing},
location = {Sonepat, India}
}

@inproceedings{10.1145/3377812.3381399,
author = {Abbas, Muhammad},
title = {Variability aware requirements reuse analysis},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381399},
doi = {10.1145/3377812.3381399},
abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {190–193},
numpages = {4},
keywords = {variability, software reuse, similarities, requirements, product line},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.5555/2946645.3007025,
author = {Escalera, Sergio and Athitsos, Vassilis and Guyon, Isabelle},
title = {Challenges in multimodal gesture recognition},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {This paper surveys the state of the art on multimodal gesture recognition and introduces the JMLR special topic on gesture recognition 2011-2015. We began right at the start of the Kinect™ revolution when inexpensive infrared cameras providing image depth recordings became available. We published papers using this technology and other more conventional methods, including regular video cameras, to record data, thus providing a good overview of uses of machine learning and computer vision using multimodal data in this area of application. Notably, we organized a series of challenges and made available several datasets we recorded for that purpose, including tens of thousands of videos, which are available to conduct further research. We also overview recent state of the art works on gesture recognition based on a proposed taxonomy for gesture recognition, discussing challenges and future lines of research.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2549–2602},
numpages = {54},
keywords = {wearable sensors, time series analysis, pattern recognition, multimodal data analysis, infrared cameras, gesture recognition, computer vision, Kinect™}
}

@article{10.1504/IJBRA.2018.092685,
title = {Subspace module extraction from MI-based co-expression network},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {3},
issn = {1744-5485},
url = {https://doi.org/10.1504/IJBRA.2018.092685},
doi = {10.1504/IJBRA.2018.092685},
abstract = {Most of the existing methods in literature have used proximity measures in the construction of co-expression networks CEN consisting of functional gene modules. This work describes the construction of co-expression network using mutual information MI as a proximity measure with non-linear correlation. The network modules are extracted that are defined over a subset of samples. This method has been tested on several publicly available datasets and the subspace network modules obtained have been validated in terms of both internal and external measures.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {207–234},
numpages = {28}
}

@article{10.3233/JIFS-179415,
author = {Hu, Yun and Zhou, Zuojian and Hu, Kongfa and Li, Hui and Kim, Young Ho},
title = {Detecting overlapping communities from micro blog network by additive spectral decomposition},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179415},
doi = {10.3233/JIFS-179415},
abstract = {Detecting community structure is critical in analysing social networks which are flourishing and influencing every aspect of people’s social life. Most social network systems are composed with complicated entity relations such and social interests, user relationships and their interactions. To understand how users interact with each other under the community level, its not enough to consider one kind of these relations while ignore the other. An united network model that can comprehensively integrate these relations is essential for community detection. Focusing on such kind of problem when dealing with social network with multiple relations, this paper proposes a heterogeneous network model which characterizes and constructs user similarity relations by combining both of users’ interests and their interactions attributes. Based on the heterogeneous similarity model, an additive spectral decomposition algorithm is applied to detect overlapped communities from the network. The remarkable effect of our heterogeneous model is the ability to reveal most important attributes of the blog network. And, comparing to crisp clustering method, the additive spectral decomposition algorithm proposed is effective for finding overlapped user groups which is more reasonable among social networks where users tend to join multiple social groups. Results of experimental studies on real-world and synthetic datasets demonstrate the effectiveness of the algorithm with respect to the size, the distributive structure and the high dimensionality of the datasets.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {409–416},
numpages = {8},
keywords = {user similarity modelling, heterogeneous network model, user interaction, user interest, micro blog network, Community detection}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-paced co-training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@article{10.1007/s11192-019-03307-5,
author = {Tattershall, E. and Nenadic, G. and Stevens, R. D.},
title = {Detecting bursty terms in computer science research},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {122},
number = {1},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-019-03307-5},
doi = {10.1007/s11192-019-03307-5},
abstract = {Research topics rise and fall in popularity over time, some more swiftly than others. The fastest rising topics are typically called bursts; for example “deep learning”, “internet of things” and “big data”. Being able to automatically detect and track bursty terms in the literature could give insight into how scientific thought evolves over time. In this paper, we take a trend detection algorithm from stock market analysis and apply it to over 30&nbsp;years of computer science research abstracts, treating the prevalence of each term in the dataset like the price of a stock. Unlike previous work in this domain, we use the free text of abstracts and titles, resulting in a finer-grained analysis. We report a list of bursty terms, and then use historical data to build a classifier to predict whether they will rise or fall in popularity in the future, obtaining accuracy in the region of 80%. The proposed methodology can be applied to any time-ordered collection of text to yield past and present bursty terms and predict their probable fate.},
journal = {Scientometrics},
month = jan,
pages = {681–699},
numpages = {19},
keywords = {MACD, DBLP, Machine learning, Term life cycles, Bibliometrics, Computer science}
}

@article{10.1016/j.compind.2010.03.007,
author = {Liu, Yong-Jin and Zhang, Dong-Liang and Yuen, Matthew Ming-Fai},
title = {A survey on CAD methods in 3D garment design},
year = {2010},
issue_date = {August, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {6},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2010.03.007},
doi = {10.1016/j.compind.2010.03.007},
abstract = {With the advance in virtual reality applications, garment industry has strived for new developments. This paper reviews state-of-the-art CAD methods in 3D garment design. A large range of techniques are selected and organized into several key modules which form the core of a 3D garment design technology platform. In each module, basic techniques are presented first. Then advanced developments are systematically discussed and commented. The selected key modules - digital human modeling, 3D garment design and modification, numerical integration of draping, 2D pattern generation, geometric details modeling, parallel computation and GPU acceleration - are discussed in turn. Major challenges and solutions that have been addressed over the years are discussed. Finally, some of the ensuing challenges in 3D garment CAD technologies are outlined.},
journal = {Comput. Ind.},
month = aug,
pages = {576–593},
numpages = {18},
keywords = {Garments, Feature modeling, CAD methods}
}

@article{10.1007/s11280-018-0622-x,
author = {Wen, Guoqiu and Zhu, Yonghua and Cai, Zhiguo and Zheng, Wei},
title = {Self-tuning clustering for high-dimensional data},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0622-x},
doi = {10.1007/s11280-018-0622-x},
abstract = {Spectral clustering is an important component of clustering method, via tightly relying on the affinity matrix. However, conventional spectral clustering methods 1). equally treat each data point, so that easily affected by the outliers; 2). are sensitive to the initialization; 3). need to specify the number of cluster. To conquer these problems, we have proposed a novel spectral clustering algorithm, via employing an affinity matrix learning to learn an intrinsic affinity matrix, using the local PCA to resolve the intersections; and further taking advantage of a robust clustering that is insensitive to initialization to automatically generate clusters without an input of number of cluster. Experimental results on both artificial and real high-dimensional datasets have exhibited our proposed method outperforms the clustering methods under comparison in term of four clustering metrics.},
journal = {World Wide Web},
month = nov,
pages = {1563–1573},
numpages = {11},
keywords = {Spectral clustering, Multi-manifold clustering, Local PCA, High-dimensional data}
}

@article{10.1007/s10462-017-9573-3,
author = {Sinha, Shweta and Jain, Aruna and Agrawal, Shyam S.},
title = {Empirical analysis of linguistic and paralinguistic information for automatic dialect classification},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9573-3},
doi = {10.1007/s10462-017-9573-3},
abstract = {Current research in automatic speech recognition is primarily concerned with the correct evaluation of linguistic information transmitted in the speech signal and with the identification of variations, naturally present in speech. These differences in speech may be due to the individual's age; gender; or speaking style influenced by his dialect. Undoubtedly, the focus of research in this field is to strengthen further the techniques developed thus far, regarding their reliability and accuracy. The endeavour of this research paper is to primarily concentrate on analysis and modelling of linguistic and paralinguistic information embedded in the speech signal for discovering the similarities and dissimilarities among acoustic characteristics arising out of different dialects. This paper investigates the influence of dialectal variations, by measuring and analysing certain acoustic features such as formant frequencies, pitch, pitch slope, duration and intensity of vowel sounds. For automatic identification of native dialect, these differences are further exploited, given a sample of native speaker's speech. For the classification of dialect in the spoken utterances support vector machines along with dialect-specific Gaussian mixture models were used. The system performance is compared with human perception of dialects. The proposed study focuses on various dialects of one of the world's major language; Hindi.},
journal = {Artif. Intell. Rev.},
month = apr,
pages = {647–672},
numpages = {26},
keywords = {Support vector machine, Human perception, Gaussian mixture model, Dialect identification, Acoustic feature}
}

@inproceedings{10.1007/978-3-030-86608-2_4,
author = {Yang, Xuqi and Zhang, Jia and Qin, Rong and Su, Yunyu and Qiu, Shuting and Yu, Jintian and Ge, Yongxin},
title = {Skeleton-Based Action Recognition with Improved Graph Convolution Network},
year = {2021},
isbn = {978-3-030-86607-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86608-2_4},
doi = {10.1007/978-3-030-86608-2_4},
abstract = {Most previous skeleton-based action recognition methods ignore weight information of joints and data features beyond labels, which is harmful to action recognition. In this paper, we propose a skeleton-based action recognition with improved Graph Convolution Network, which is based on Spatial Temporal Graph Convolutional Network (STGCN). And we add a predictive cluster network, weight generation networks on it. The model uses K-means algorithm to cluster and get the data information beyond the labels. Besides, each cluster traines weight generation networks independently. To find the best clusters, we propose a evaluation criterion with less computational effort. We perform extensive experiments on the Kinetics dataset and the NTU RGB+D dataset to verify the effectiveness of each network of our model. The comparison results show that our approach achieves satisfactory results.},
booktitle = {Biometric Recognition: 15th Chinese Conference, CCBR 2021, Shanghai, China, September 10–12, 2021, Proceedings},
pages = {31–38},
numpages = {8},
keywords = {Deep learning, Skeleton-based action recognition, Clustering algorithm},
location = {Shanghai, China}
}

@article{10.1007/s10044-017-0630-y,
author = {Soomro, Toufique Ahmed and Gao, Junbin and Khan, Tariq and Hani, Ahmad Fadzil and Khan, Mohammad A. and Paul, Manoranjan},
title = {Computerised approaches for the detection of diabetic retinopathy using retinal fundus images: a survey},
year = {2017},
issue_date = {November  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-017-0630-y},
doi = {10.1007/s10044-017-0630-y},
abstract = {Eye-related disease such as diabetic retinopathy (DR) is a medical ailment in which the retina of the human eye is smashed because of damage to the tiny retinal blood vessels in the retina. Ophthalmologists identify DR based on various features such as the blood vessels, textures and pathologies. With the rapid development of methods of analysis of biomedical images and advanced computing techniques, image processing-based software for the detection of eye disease has been widely used as an important tool by ophthalmologists. In particular, computer vision-based methods are growing rapidly in the field of medical images analysis and are appropriate to advance ophthalmology.
 These tools depend entirely on visual analysis to identify abnormalities in Retinal Fundus images. During the past two decades, exciting improvement in the development of DR detection computerised systems has been observed. This paper reviews the development of analysing retinal images for the detection of DR in three aspects: automatic algorithms (classification or pixel to pixel methods), detection methods of pathologies from retinal fundus images, and extraction of blood vessels of retinal fundus image algorithms for the detection of DR. The paper presents a detailed explanation of each problem with respect to retinal images. The current techniques that are used to analyse retinal images and DR detection issues are also discussed in detail and recommendations are made for some future directions.},
journal = {Pattern Anal. Appl.},
month = nov,
pages = {927–961},
numpages = {35},
keywords = {Retinal images, Retinal databases, Overview of DR detection imaging techniques}
}

@inproceedings{10.1007/978-3-030-48077-6_3,
author = {Claris\'{o}, Robert and Cabot, Jordi},
title = {Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering},
year = {2020},
isbn = {978-3-030-48076-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48077-6_3},
doi = {10.1007/978-3-030-48077-6_3},
abstract = {Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.},
booktitle = {Rigorous State-Based Methods: 7th International Conference, ABZ 2020, Ulm, Germany, May 27–29, 2020, Proceedings},
pages = {27–43},
numpages = {17},
keywords = {Diversity, Clustering, Graph kernels, Testing, Verification and validation, Model-driven engineering},
location = {Ulm, Germany}
}

@article{10.1145/3293319,
author = {Hsieh, Hsun-Ping and Li, Cheng-Te},
title = {Inferring Online Social Ties from Offline Geographical Activities},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3293319},
doi = {10.1145/3293319},
abstract = {As mobile devices are becoming ubiquitous nowadays, the geographical activities and interactions of human beings can be easily recorded and accessed. Each mobile individual can belong to an online social network. Unfortunately, the underlying online social relationships are hidden and only available to service providers. Acquiring the social network of mobile users would enrich lots of mobile applications, such as friend recommendation and energy-saving mobile database management. In this work, we propose to infer online social ties using purely offline geographical activities of users, such as check-in records and spatial meeting events. To tackle the problem, we devise a novel inference framework, O2O-Inf, which consists of two components, Feature Modeling and Link Inference. Feature modeling is to characterize both direct and indirect geographical interactions between nodes from co-location and graph features. Link inference aims to infer the social ties based on a small set of observed social links, and the idea is that pairs of nodes sharing similar geographical behaviors have the same tendency of linkage (i.e., either being friends or non-friends). Experiments conducted on a Gowalla location-based social network and a Meetup event-based social network exhibit a satisfying performance in comparison to state-of-the-art prediction methods under the settings of offline-to-online network inference and geo-link prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {17},
numpages = {21},
keywords = {offline geographical activities, network inference, location-based services, link prediction, Online social ties}
}

@article{10.1109/TCBB.2011.32,
author = {Ambert, Kyle H. and Cohen, Aaron M.},
title = {k-Information Gain Scaled Nearest Neighbors: A Novel Approach to Classifying Protein-Protein Interaction-Related Documents},
year = {2012},
issue_date = {January 2012},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {9},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2011.32},
doi = {10.1109/TCBB.2011.32},
abstract = {Although publicly accessible databases containing protein-protein interaction (PPI)-related information are important resources to bench and in silico research scientists alike, the amount of time and effort required to keep them up to date is often burdonsome. In an effort to help identify relevant PPI publications, text-mining tools, from the machine learning discipline, can be applied to help in this process. Here, we describe and evaluate two document classification algorithms that we submitted to the BioCreative II.5 PPI Classification Challenge Task. This task asked participants to design classifiers for identifying documents containing PPI-related information in the primary literature, and evaluated them against one another. One of our systems was the overall best-performing system submitted to the challenge task. It utilizes a novel approach to k-nearest neighbor classification, which we describe here, and compare its performance to those of two support vector machine-based classification systems, one of which was also evaluated in the challenge task.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {305–310},
numpages = {6},
keywords = {text classification., support vector machine, k-nearest neighbor, information gain, Protein-protein interaction}
}

@article{10.1007/s10766-016-0417-6,
author = {Allombert, V. and Gava, F. and Tesson, J.},
title = {Multi-ML: Programming Multi-BSP Algorithms in ML},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-016-0417-6},
doi = {10.1007/s10766-016-0417-6},
abstract = {bsp is a bridging model between abstract execution and concrete parallel systems. Structure and abstraction brought by bsp allow to have portable parallel programs with scalable performance predictions, without dealing with low-level details of architectures. In the past, we designed bsml for programming bsp algorithms in ml. However, the simplicity of the bsp model does not fit the complexity of today's hierarchical architectures such as clusters of machines with multiple multi-core processors. The multi-bsp model is an extension of the bsp model which brings a tree-based view of nested components of hierarchical architectures. To program multi-bsp algorithms in ml, we propose the multi-ml language as an extension of bsml where a specific kind of recursion is used to go through a hierarchy of computing nodes. We define a formal semantics of the language and present preliminary experiments which show performance improvements with respect to bsml.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {340–361},
numpages = {22},
keywords = {multi-bsp, ml, bsp, Parallel programming}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1609/aaai.v33i01.33015117,
author = {Tang, Ying-Peng and Huang, Sheng-Jun},
title = {Self-paced active learning: query the right thing at the right time},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015117},
doi = {10.1609/aaai.v33i01.33015117},
abstract = {Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {628},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.specom.2019.09.003,
author = {Shirzhiyan, Zahra and Shamsi, Elham and Jafarpisheh, Amir Salar and Jafari, Amir Homayoun},
title = {Objective classification of auditory brainstem responses to consonant-vowel syllables using local discriminant bases},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.09.003},
doi = {10.1016/j.specom.2019.09.003},
journal = {Speech Commun.},
month = nov,
pages = {36–48},
numpages = {13},
keywords = {Local discriminant bases, Speech encoding, Speech ABR}
}

@article{10.1016/j.knosys.2017.03.026,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Tang, Jia-Fu and Herrera, Francisco},
title = {Exploring the effectiveness of dynamic ensemble selection in the one-versus-one scheme},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.026},
doi = {10.1016/j.knosys.2017.03.026},
abstract = {The One-versus-One (OVO) strategy is one of the most common and effective techniques to deal with multi-class classification problems. The basic idea of an OVO scheme is to divide a multi-class classification problem into several easier-to-solve binary classification problems with considering each possible pair of classes from the original problem, which is then built into a binary classifier by an independent base learner. In this study, we propose a novel methodology which attempts to select a group of base classifiers in each pairwise dataset for each unknown pattern. To implement this, the Dynamic Ensemble Selection (DES) method based on a competence measure is employed to select the most appropriate ensemble in each binary classification problem derived from the OVO decomposition. In order to verify the validity and effectiveness of our proposed method, we carry out a thorough experimental study. We first compare our proposal with several state-of-the-art approaches. Then, we perform the comparison of several well-known aggregation strategies to combine the binary ensemble obtained by Dynamic Ensemble Selection. Finally, we explore whether further improvement can be achieved by considering the competence-based method in OVO scheme. The extracted findings drawn from the empirical analysis are supported by the proper statistical analysis and indicate that there is a positive synergy between the DES method and the Distance-based Relative Competence Weighting (DRCW) approach for the OVO scheme.},
journal = {Know.-Based Syst.},
month = jun,
pages = {53–63},
numpages = {11},
keywords = {Pairwise learning, One-versus-One, Multi-classification, Dynamic ensemble selection, Decomposition strategies}
}

@article{10.1016/j.patcog.2021.108058,
author = {Cui, Lixin and Bai, Lu and Wang, Yue and Yu, Philip S. and Hancock, Edwin R.},
title = {Fused lasso for feature selection using structural information},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108058},
doi = {10.1016/j.patcog.2021.108058},
journal = {Pattern Recogn.},
month = nov,
numpages = {14},
keywords = {Correlated feature group, Sparse learning, Graph-based feature selection, Fused lasso, Structural relationship, Feature selection}
}

@inproceedings{10.1007/978-3-030-77385-4_42,
author = {Halilaj, Lavdim and Dindorkar, Ishan and L\"{u}ttin, J\"{u}rgen and Rothermel, Susanne},
title = {A Knowledge Graph-Based Approach for Situation Comprehension in Driving Scenarios},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_42},
doi = {10.1007/978-3-030-77385-4_42},
abstract = {Making an informed and right decision poses huge challenges for drivers in day-to-day traffic situations. This task vastly depends on many subjective and objective factors, including the current driver state, her destination, personal preferences and abilities as well as surrounding environment. In this paper, we present CoSI (Context and Situation Intelligence), a Knowledge Graph (KG)-based approach for fusing and organizing heterogeneous types and sources of information. The KG serves as a coherence layer representing information in the form of entities and their inter-relationships augmented with additional semantic axioms. Harnessing the power of axiomatic rules and reasoning capabilities enables inferring additional knowledge from what is already encoded. Thus, dedicated components exploit and consume the semantically enriched information to perform tasks such as situation classification, difficulty assessment, and trajectory prediction. Further, we generated a synthetic dataset to simulate real driving scenarios with a large range of driving styles and vehicle configurations. We use KG embedding techniques based on a Graph Neural Network (GNN) architecture for a classification task of driving situations and achieve over 95% accuracy whereas vector-based approaches achieve only 75% accuracy for the same task. The results suggest that the KG-based information representation combined with GNN are well suited for situation understanding tasks as required in driver assistance and automated driving systems.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {699–716},
numpages = {18},
keywords = {Graph neural network, Knowledge graph embedding, Knowledge graph, Situation comprehension}
}

@inproceedings{10.1145/3474717.3483917,
author = {Yin, Yifang and Tran, An and Zhang, Ying and Hu, Wenmiao and Wang, Guanfeng and Varadarajan, Jagannadan and Zimmermann, Roger and Ng, See-Kiong},
title = {Multimodal Fusion of Satellite Images and Crowdsourced GPS Traces for Robust Road Attribute Detection},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474717.3483917},
doi = {10.1145/3474717.3483917},
abstract = {Automatic inference of missing road attributes (e.g., road type and speed limit) for enriching digital maps has attracted significant research attention in recent years. A number of machine learning based approaches have been proposed to detect road attributes from GPS traces, dash-cam videos, or satellite images. However, existing solutions mostly focus on a single modality without modeling the correlations among multiple data sources. To bridge the gap, we present a multimodal road attribute detection method, which improves the robustness by performing pixel-level fusion of crowdsourced GPS traces and satellite images. A GPS trace is usually given by a sequence of location, bearing, and speed. To align it with satellite imagery in the spatial domain, we render GPS traces into a sequence of multi-channel images that simultaneously capture the global distribution of the GPS points, the local distribution of vehicles' moving directions and speeds, and their temporal changes over time, at each pixel. Unlike previous GPS based road feature extraction methods, our proposed GPS rendering does not require map matching in the data preprocessing step. Moreover, our multimodal solution addresses single-modal challenges such as occlusions in satellite images and data sparsity in GPS traces by learning the pixel-wise correspondences among different data sources. Extensive experiments have been conducted on two real-world datasets in Singapore and Jakarta. Compared with previous work, our method is able to improve the detection accuracy on road attributes by a large margin.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {107–116},
numpages = {10},
keywords = {satellite images, digital maps, Road attributes, GPS trajectories},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@article{10.1016/j.neucom.2015.07.152,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng},
title = {Hessian regularization by patch alignment framework},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {204},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.07.152},
doi = {10.1016/j.neucom.2015.07.152},
abstract = {In recent years, semi-supervised learning has played a key part in large-scale image management, where usually only a few images are labeled. To address this problem, many representative works have been reported, including transductive SVM, universum SVM, co-training and graph-based methods. The prominent method is the patch alignment framework, which unifies the traditional spectral analysis methods. In this paper, we propose Hessian regression based on the patch alignment framework. In particular, we construct a Hessian using the patch alignment framework and apply it to regression problems. To the best of our knowledge, there is no report on Hessian construction from the patch alignment viewpoint. Compared with the traditional Laplacian regularization, Hessian can better match the data and then leverage the performance. To validate the effectiveness of the proposed method, we conduct human face recognition experiments on a celebrity face dataset. The experimental results demonstrate the superiority of the proposed solution in human face classification.},
journal = {Neurocomput.},
month = sep,
pages = {183–188},
numpages = {6},
keywords = {Semi-supervised learning, Patch alignment, Least squares, Hessian}
}

@article{10.1007/s00500-014-1461-z,
author = {Theodoridis, Theodoros and Hu, Huosheng},
title = {The binomial-neighbour instance-based learner on a multiclass performance measure scheme},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {10},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-014-1461-z},
doi = {10.1007/s00500-014-1461-z},
abstract = {This paper presents a novel instance-based learning methodology the Binomial-Neighbour (B-N) algorithm. Unlike to other k-Nearest Neighbour algorithms, B-N employs binomial search through vectors of statistical features and distance primitives. The binomial combinations derived from the search with best classification accuracy are distinct primitives which characterise a pattern. The statistical features employ a twofold role; initially to model the data set in a dimensionality reduction preprocessing, and finally to exploit these attributes to recognise patterns. The paper introduces as well a performance measure scheme for multiclass problems using type error statistics. We harness this scheme to evaluate the B-N model on a benchmark human action dataset of normal and aggressive activities. Classification results are being compared with the standard IBk and IB1 models achieving significantly exceptional recognition performance.},
journal = {Soft Comput.},
month = oct,
pages = {2973–2981},
numpages = {9},
keywords = {k-Nearest neighbours, Lazy learners, Instance-based learning, Action recognition}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, \"{O}nder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Vector space model, R, Model-Driven Engineering, Model comparison, Hierarchical clustering}
}

@inproceedings{10.1145/3459637.3482458,
author = {Keramati, Mahsa and Zohrevand, Zahra and Gl\"{a}sser, Uwe},
title = {Norma: A Hybrid Feature Alignment for Class-Aware Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482458},
doi = {10.1145/3459637.3482458},
abstract = {Unsupervised domain adaptation is the problem of transferring extracted knowledge from a labeled source domain to an unlabeled target domain. To achieve discriminative domain adaptation recent studies take advantage of target sample pseudo-labels to impose class-aware distribution alignment across the source and target domains. Still, they have some shortcomings such as making decisions based on inaccurate pseudo-labeled samples that mislead the adaptation process. In this paper, we propose a progressive deep feature alignment, called Norma, to tackle class-aware unsupervised domain adaptation for image classification by enforcing inter-class compactness and intra-class discrepancy through a hybrid learning process. To this end, Norma's optimization process is defined based on a novel triplet loss which not only addresses soft prototype alignment but also pushes away multiple negative centroids. Also, to extract maximum discriminative domain knowledge per iteration, we propose a joint positive and negative learning procedure along with an uncertainty-guided progressive pseudo-labeling on the basis of prototype-based clustering and conditional probability. Our experimental results on several benchmarks demonstrate that Norma outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {833–843},
numpages = {11},
keywords = {transfer learning, pseudo-labeling, negative learning, image classification, deep-metric learning, class-aware alignment, adversarial unsupervised domain adaptation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1016/j.peva.2018.12.003,
author = {Geyer, Fabien},
title = {DeepComNet: Performance evaluation of network topologies using graph-based deep learning},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {130},
number = {C},
issn = {0166-5316},
url = {https://doi.org/10.1016/j.peva.2018.12.003},
doi = {10.1016/j.peva.2018.12.003},
journal = {Perform. Eval.},
month = apr,
pages = {1–16},
numpages = {16},
keywords = {Deep learning, Graph neural network, Network performance evaluation}
}

@phdthesis{10.5555/AAI28023715,
author = {Sornapudi, Sudhir and V., Stoecker, William and Kurt, Kosbar, and J., Zawodniok, Maciej and A, Samaranayake, V.},
advisor = {Joe, Stanley, R.},
title = {Deep Learning for Digitized Histology Image Analysis},
year = {2020},
isbn = {9798426819580},
publisher = {Missouri University of Science and Technology},
abstract = {Cervical cancer is the fourth most frequent cancer that affects women worldwide. Assessment of cervical intraepithelial neoplasia (CIN) through histopathology remains as the standard for absolute determination of cancer. The examination of tissue samples under a microscope requires considerable time and effort from expert pathologists. There is a need to design an automated tool to assist pathologists for digitized histology slide analysis. Pre-cervical cancer is generally determined by examining the CIN which is the growth of atypical cells from the basement membrane (bottom) to the top of the epithelium.  It has four grades, including: Normal, CIN1, CIN2, and CIN3. In this research, different facets of an automated digitized histology epithelium assessment pipeline have been explored to mimic the pathologist diagnostic approach.  The entire pipeline from slide to epithelium CIN grade has been designed and developed using deep learning models and imaging techniques to analyze the whole slide image (WSI). The process is as follows:  1) identification of epithelium by filtering the regions extracted from a low-resolution image with a binary classifier network; 2) epithelium segmentation; 3) deep regression for pixel-wise segmentation of epithelium by patch-based image analysis; 4) attention-based CIN classification with localized sequential feature modeling. Deep learning-based nuclei detection by superpixels was performed as an extension of our research. Results from this research indicate an improved performance of CIN assessment over state-of-the-art methods for nuclei segmentation, epithelium segmentation, and CIN classification, as well as the development of a prototype WSI-level tool.},
note = {AAI28023715}
}

@article{10.1016/j.knosys.2016.05.048,
author = {Zhang, Zhongliang and Krawczyk, Bartosz and Garc\`{\i}a, Salvador and Rosales-P\'{e}rez, Alejandro and Herrera, Francisco},
title = {Empowering one-vs-one decomposition with ensemble learning for multi-class imbalanced data},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.048},
doi = {10.1016/j.knosys.2016.05.048},
abstract = {Extending binary ensemble techniques to multi-class imbalanced data.OVO scheme enhancement for multi-class imbalanced data by ensemble learning.A complete experimental study of comparison of the ensemble learning techniques with OVO.Study of the impact of base classifiers used in the proposed scenario. Multi-class imbalance classification problems occur in many real-world applications, which suffer from the quite different distribution of classes. Decomposition strategies are well-known techniques to address the classification problems involving multiple classes. Among them binary approaches using one-vs-one and one-vs-all has gained a significant attention from the research community. They allow to divide multi-class problems into several easier-to-solve two-class sub-problems. In this study we develop an exhaustive empirical analysis to explore the possibility of empowering the one-vs-one scheme for multi-class imbalance classification problems with applying binary ensemble learning approaches. We examine several state-of-the-art ensemble learning methods proposed for addressing the imbalance problems to solve the pairwise tasks derived from the multi-class data set. Then the aggregation strategy is employed to combine the binary ensemble outputs to reconstruct the original multi-class task. We present a detailed experimental study of the proposed approach, supported by the statistical analysis. The results indicate the high effectiveness of ensemble learning with one-vs-one scheme in dealing with the multi-class imbalance classification problems.},
journal = {Know.-Based Syst.},
month = aug,
pages = {251–263},
numpages = {13},
keywords = {Multi-class classification, Imbalanced data, Ensemble learning, Classifier combination, Binary decomposition}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.5555/3504035.3504690,
author = {Neill, James O' and Buitelaar, Paul},
title = {Few shot transfer learning between word relatedness and similarity tasks using a gated recurrent siamese network},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Word similarity and word relatedness are fundamental to natural language processing and more generally, understanding how humans relate concepts in semantic memory. A growing number of datasets are being proposed as evaluation benchmarks, however, the heterogeneity and focus of each respective dataset makes it difficult to draw plausible conclusions as to how a unified semantic model would perform. Additionally, we want to identify the transferability of knowledge obtained from one task to another, within the same domain and across domains. Hence, this paper first presents an evaluation and comparison of eight chosen datasets tested using the best performing regression models. As a baseline, we present regression models that incorporate both lexical features and word embeddings to produce consistent and competitive results compared to the state of the art. We present our main contribution, the best performing model across seven of the eight datasets - a Gated Recurrent Siamese Network that learns relationships between lexical word definitions. A parameter transfer learning strategy is employed for the Siamese Network. Subsequently, we present a secondary contribution which is the best performing non-sequential model: an Inductive and Transductive Transfer Learning strategy for transferring decision trees within a Random Forest to a target task that is learned from only few instances. The method involves measuring semantic distance between hidden factored matrix representations of decision tree traversal matrices.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {655},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {empirical study, expectation maximization, metric threshold}
}

@inproceedings{10.5555/3305890.3306045,
author = {Valera, Isabel and Ghahramani, Zoubin},
title = {Automatic discovery of the statistical types of variables in a dataset},
year = {2017},
publisher = {JMLR.org},
abstract = {A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known. However, as the availability of real-world data increases, this assumption becomes too restrictive. Data are often heterogeneous, complex, and improperly or incompletely documented. Surprisingly, despite their practical importance, there is still a lack of tools to automatically discover the statistical types of, as well as appropriate likelihood (noise) models for, the variables in a dataset. In this paper, we fill this gap by proposing a Bayesian method, which accurately discovers the statistical data types in both synthetic and real data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3521–3529},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1145/3469096.3469872,
author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
title = {On minimizing cost in legal document review workflows},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469872},
doi = {10.1145/3469096.3469872},
abstract = {Technology-assisted review (TAR) refers to human-in-the-loop machine learning workflows for document review in legal discovery and other high recall review tasks. Attorneys and legal technologists have debated whether review should be a single iterative process (one-phase TAR workflows) or whether model training and review should be separate (two-phase TAR workflows), with implications for the choice of active learning algorithm. The relative cost of manual labeling for different purposes (training vs. review) and of different documents (positive vs. negative examples) is a key and neglected factor in this debate. Using a novel cost dynamics analysis, we show analytically and empirically that these relative costs strongly impact whether a one-phase or two-phase workflow minimizes cost. We also show how category prevalence, classification task difficulty, and collection size impact the optimal choice not only of workflow type, but of active learning method and stopping point.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {30},
numpages = {10},
keywords = {total recall, high-recall retrieval, cost modeling, active learning},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@article{10.1016/j.imavis.2016.06.005,
author = {Leng, Mengjun and Moutafis, Panagiotis and Kakadiaris, Ioannis A.},
title = {Joint prototype and metric learning for image set classification: Application to video face identification},
year = {2017},
issue_date = {Feb 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {58},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.06.005},
doi = {10.1016/j.imavis.2016.06.005},
journal = {Image Vision Comput.},
month = feb,
pages = {204–213},
numpages = {10},
keywords = {Video face recognition, Prototype learning, Metric learning, Image set classification}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1007/978-3-030-00308-1_3,
author = {Hess, Timm and Mundt, Martin and Weis, Tobias and Ramesh, Visvanathan},
title = {Large-Scale Stochastic Scene Generation and Semantic Annotation for Deep Convolutional Neural Network Training in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_3},
doi = {10.1007/978-3-030-00308-1_3},
abstract = {Object detection and classification are essential tasks for any robotics scenario, where data-driven approaches, specifically deep learning techniques, have been widely adopted in recent years. However, in the context of the RoboCup standard platform league these methods have not yet gained comparable popularity in large part due to the lack of (publicly) available large enough data sets that involve a tedious gathering and error-prone manual annotation process. We propose a framework for stochastic scene generation, rendering and automatic creation of semantically annotated ground truth masks. Used as training data in conjunction with deep convolutional neural networks we demonstrate compelling classification accuracy on real-world data in a multi-class setting. An evaluation on multiple neural network architectures with varying depth and representational capacity, corresponding run-times on current NAO-H25 hardware, and required sampled training data is provided.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {33–44},
numpages = {12},
keywords = {Static Head Pose, Robotics, Standard Platform League (SPL), RoboCup SPL, Deep Convolutional Neural Networks},
location = {Nagoya, Japan}
}

@article{10.1016/j.asoc.2015.03.045,
author = {Fahad, Labiba Gillani and Rajarajan, Muttukrishnan},
title = {Integration of discriminative and generative models for activity recognition in smart homes},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.045},
doi = {10.1016/j.asoc.2015.03.045},
abstract = {Graphical abstractDisplay Omitted HighlightsA hybrid activity recognition approach that combines DM with PE using SVM.DM is suitable for imbalanced number of activity instances.PE has better generalization ability in activity recognition.Evaluation on five smart home datasets validates an improved performance of the approach. Activity recognition in smart homes enables the remote monitoring of elderly and patients. In healthcare systems, reliability of a recognition model is of high importance. Limited amount of training data and imbalanced number of activity instances result in over-fitting thus making recognition models inconsistent. In this paper, we propose an activity recognition approach that integrates the distance minimization (DM) and probability estimation (PE) approaches to improve the reliability of recognitions. DM uses distances of instances from the mean representation of each activity class for label assignment. DM is useful in avoiding decision biasing towards the activity class with majority instances; however, DM can result in over-fitting. PE on the other hand has good generalization abilities. PE measures the probability of correct assignments from the obtained distances, while it requires a large amount of data for training. We apply data oversampling to improve the representation of classes with less number of instances. Support vector machine (SVM) is applied to combine the outputs of both DM and PE, since SVM performs better with imbalanced data and further improves the generalization ability of the approach. The proposed approach is evaluated using five publicly available smart home datasets. The results demonstrate better performance of the proposed approach compared to the state-of-the-art activity recognition approaches.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {992–1001},
numpages = {10},
keywords = {Support vector machine, Smart homes, Probability estimation, Pervasive healthcare, Distance minimization, Activity recognition}
}

@article{10.1016/j.ins.2012.09.024,
author = {Guo, Jun and Zhou, Hui and Zhu, Changren},
title = {Cascaded classification of high resolution remote sensing images using multiple contexts},
year = {2013},
issue_date = {February, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {221},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2012.09.024},
doi = {10.1016/j.ins.2012.09.024},
abstract = {We present a novel cascaded classification approach by exploiting various contexts on different levels for high resolution remote sensing (HRRS) images. The contexts mentioned in our article are defined according to objects from a set of regions resulting from segmentation. The cascaded procedure comprises three stages: (1) initializing the classification using the object's inner context (i.e., the gray constraints of different pixels in an object), (2) correcting the classification using the object's neighbor context (i.e., the characteristic constraints of different objects adjacent to the concerned object), and (3) refining classification using the object's scene context (i.e., the distribution constraint of different objects' labels and their feature vectors in the whole scene). The proposed algorithm has the following distinctions. First, it uses an object's neighbor context to bridge the gap between its inner context and its scene context because the latter two types of contexts have inevitable drawbacks when being used for classification alone. Second, it carries on a cascaded classification procedure in which the previous stage provides a better initial classification for the following stage, and the result is gradually refined by integrating different contexts. The effectiveness and practicability of the proposed algorithm is demonstrated through a set of completely experimental results and substantiated using quantitative criteria.},
journal = {Inf. Sci.},
month = feb,
pages = {84–97},
numpages = {14},
keywords = {Object based image analysis, Multiple contexts integration, High resolution remote sensing images, Context-enabled classification}
}

@article{10.1007/s10916-015-0219-1,
author = {Ayd\i{}n, Serap and Tunga, M. Alper and Yetkin, Sinan},
title = {Mutual Information Analysis of Sleep EEG in Detecting Psycho-Physiological Insomnia},
year = {2015},
issue_date = {May       2015},
publisher = {Plenum Press},
address = {USA},
volume = {39},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-015-0219-1},
doi = {10.1007/s10916-015-0219-1},
abstract = {The primary goal of this study is to state the clear changes in functional brain connectivity during all night sleep in psycho-physiological insomnia (PPI). The secondary goal is to investigate the usefulness of Mutual Information (MI) analysis in estimating cortical sleep EEG arousals for detection of PPI. For these purposes, healthy controls and patients were compared to each other with respect to both linear (Pearson correlation coefficient and coherence) and nonlinear quantifiers (MI) in addition to phase locking quantification for six sleep stages (stage.1---4, rem, wake) by means of interhemispheric dependency between two central sleep EEG derivations. In test, each connectivity estimation calculated for each couple of epoches (C3-A2 and C4-A1) was identified by the vector norm of estimation. Then, patients and controls were classified by using 10 different types of data mining classifiers for five error criteria such as accuracy, root mean squared error, sensitivity, specificity and precision. High performance in a classification through a measure will validate high contribution of that measure to detecting PPI. The MI was found to be the best method in detecting PPI. In particular, the patients had lower MI, higher PCC for all sleep stages. In other words, the lower sleep EEG synchronization suffering from PPI was observed. These results probably stand for the loss of neurons that then contribute to less complex dynamical processing within the neural networks in sleep disorders an the functional central brain connectivity is nonlinear during night sleep. In conclusion, the level of cortical hemispheric connectivity is strongly associated with sleep disorder. Thus, cortical communication quantified in all existence sleep stages might be a potential marker for sleep disorder induced by PPI.},
journal = {J. Med. Syst.},
month = may,
pages = {1–10},
numpages = {10},
keywords = {Sleep EEG, Mutual information, Data mining, Classification, Brain connectivity}
}

@article{10.1016/j.ins.2021.06.013,
author = {Yang, Guoli and Kang, Yuanji and Zhu, Xianqiang and Zhu, Cheng and Xiao, Gaoxi},
title = {Info2vec: An aggregative representation method in multi-layer and heterogeneous networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.013},
doi = {10.1016/j.ins.2021.06.013},
journal = {Inf. Sci.},
month = oct,
pages = {444–460},
numpages = {17},
keywords = {Cyberspace, Representation learning, Multi-layer networks}
}

@article{10.1007/s10515-019-00253-7,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Change impact analysis for maintenance and evolution of variable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00253-7},
doi = {10.1007/s10515-019-00253-7},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This article presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining the possibly impacted source code elements when changing the source code of a product family. The approach also supports engineers, who are adapting the code of specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanisms, it is more precise than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. We report evaluation results on the benefit and performance of the approach using industrial product lines.},
journal = {Automated Software Engg.},
month = jun,
pages = {417–461},
numpages = {45},
keywords = {Variability, Program analysis, Maintenance, Change impact analysis}
}

@article{10.1007/s10664-020-09871-2,
author = {Nyamawe, Ally S. and Liu, Hui and Niu, Nan and Umer, Qasim and Niu, Zhendong},
title = {Feature requests-based recommendation of software refactorings},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09871-2},
doi = {10.1007/s10664-020-09871-2},
abstract = {Software requirements are ever-changing which often leads to software evolution. Consequently, throughout software lifetime, developers receive new requirements often expressed as feature requests. To implement the requested features, developers sometimes apply refactorings to make their systems adapt to the new requirements. However, deciding what refactorings to apply is often challenging and there is still lack of automated support to recommend refactorings given a feature request. To this end, we propose a learning-based approach that recommends refactorings based on the history of the previously requested features, applied refactorings, and code smells information. First, the state-of-the-art refactoring detection tools are leveraged to identify the previous refactorings applied to implement the past feature requests. Second, a machine classifier is trained with the history data of the feature requests, code smells, and refactorings applied on the respective commits. Consequently, the machine classifier is used to predict refactorings for new feature requests. The proposed approach is evaluated on the dataset of 55 open source Java projects and the results suggest that it can accurately recommend refactorings (accuracy is up to 83.19%).},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {4315–4347},
numpages = {33},
keywords = {Software refactoring, Recommendation, Machine learning, Code smells, Feature requests}
}

@inproceedings{10.1145/3356471.3365234,
author = {Yin, Yifang and Sunderrajan, Abhinav and Huang, Xiaocheng and Varadarajan, Jagannadan and Wang, Guanfeng and Sahrawat, Dhruva and Zhang, Ying and Zimmermann, Roger and Ng, See-Kiong},
title = {Multi-scale Graph Convolutional Network for Intersection Detection from GPS Trajectories},
year = {2019},
isbn = {9781450369572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356471.3365234},
doi = {10.1145/3356471.3365234},
abstract = {To facilitate the reconstruction of high-quality road networks, intersections as the key locations provide valuable information about the network topology. However, only a few efforts have been made on the data-driven automatic detection of intersections from, e.g., large-scale GPS trajectories. To bridge the gap, we propose a machine learning based intersection detection approach based on large-scale real-world GPS trajectories of drivers from the Grab ride-hailing service. Instead of representing locations with vector descriptors, we innovatively propose a graph representation that models a location together with its local surroundings to improve the descriptiveness of the location descriptors. Moreover, we present a multi-scale graph convolutional network (GCN) to generate robust graph-level descriptors, followed by logistic regression to discriminate intersections from non-intersections. The experimental results show that our proposed multi-scale graph model outperforms the conventional multi-scale vector representation by 8.5%. Appealingly, the proposed graph representation can be considered as a general location descriptor, which can be used in a variety of geo-based applications other than intersection detection for location modeling.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {36–39},
numpages = {4},
keywords = {multi-scale feature fusion, large-scale real-world GPS data, graph convolutional network, Intersection detection},
location = {Chicago, IL, USA},
series = {GeoAI '19}
}

@article{10.1016/j.compbiomed.2015.03.023,
author = {Song, Xiaomu and Yoon, Suk-Chung},
title = {Improving brain-computer interface classification using adaptive common spatial patterns},
year = {2015},
issue_date = {June 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2015.03.023},
doi = {10.1016/j.compbiomed.2015.03.023},
abstract = {Common Spatial Patterns (CSP) is a widely used spatial filtering technique for electroencephalography (EEG)-based brain-computer interface (BCI). It is a two-class supervised technique that needs subject-specific training data. Due to EEG nonstationarity, EEG signal may exhibit significant intra- and inter-subject variation. As a result, spatial filters learned from a subject may not perform well for data acquired from the same subject at a different time or from other subjects performing the same task. Studies have been performed to improve CSP s performance by adding regularization terms into the training. Most of them require target subjects training data with known class labels. In this work, an adaptive CSP (ACSP) method is proposed to analyze single trial EEG data from single and multiple subjects. The method does not estimate target data s class labels during the adaptive learning and updates spatial filters for both classes simultaneously. The proposed method was evaluated based on a comparison study with the classic CSP and several CSP-based adaptive methods using motor imagery EEG data from BCI competitions. Experimental results indicate that the proposed method can improve the classification performance as compared to the other methods. For circumstances where true class labels of target data are not instantly available, it was examined if adding classified target data to training data would improve the ACSP learning. Experimental results show that it would be better to exclude them from the training data. The proposed ACSP method can be performed in real-time and is potentially applicable to various EEG-based BCI applications. HighlightsAn adaptive common spatial patterns method is proposed for EEG spatial filtering.The method is evaluated for intra- and inter-subject classifications.The method is compared to existing techniques and shows superior performance.The effects of adding misclassified trials to training data are investigated.The method is potentially applicable to various real-time BCI tasks.},
journal = {Comput. Biol. Med.},
month = jun,
pages = {150–160},
numpages = {11},
keywords = {Nonstationarity, Electroencephalography, Common spatial patterns, Brain-computer interface, Adaptive}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {software product lines, media arts, gentic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1007/978-3-319-47157-0_11,
author = {Wang, Yan and Wu, Xi and Ma, Guangkai and Ma, Zongqing and Fu, Ying and Zhou, Jiliu},
title = {Patch-Based Hippocampus Segmentation Using a Local Subspace Learning Method},
year = {2016},
isbn = {978-3-319-47156-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47157-0_11},
doi = {10.1007/978-3-319-47157-0_11},
abstract = {Patch-based segmentation methods utilizing multiple atlases have been widely studied for alleviating some misalignments when registering atlases to the target image. However, weights assigned to the fused labels are typically computed based on predefined features (e.g. simple patch intensities), thus being not necessarily optimal. Due to lack of discriminating features for different regions of an anatomical structure, the original feature space defined by image intensities may limit the segmentation accuracy. To address these problems, we propose a novel local subspace learning based patch-wise label propagation method to estimate a voxel-wise segmentation of the target image. Specifically, multi-scale patch intensities and texture features are first extracted from the image patch in order to acquire the abundant appearance information. Then, margin fisher analysis (MFA) is applied to neighboring samples of each voxel to be segmented from the aligned atlases, in order to extract discriminant features. This process can enhance discrimination of features for different local regions in the anatomical structure. Finally, based on extracted discriminant features, the k-nearest neighbor (kNN) classifier is used to determine the final label for the target voxel. Moreover, for the patch-wise label propagation, we first translate label patches into several discrete class labels by using the k-means clustering method, and then apply MFA to ensure that samples with similar label patches achieve a higher similarity and those with dissimilar label patches achieve a lower similarity. To demonstrate segmentation performance, we comprehensively evaluated the proposed method on the ADNI dataset for hippocampus segmentation. Experimental results show that the proposed method outperforms several conventional multi-atlas based segmentation methods.},
booktitle = {Machine Learning in Medical Imaging: 7th International Workshop, MLMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 17, 2016, Proceedings},
pages = {86–94},
numpages = {9},
keywords = {Target Image, Image Patch, Label Propagation, Spatial Neighborhood, Deformable Image Registration},
location = {Athens, Greece}
}

@inproceedings{10.1145/3278721.3278782,
author = {Hundman, Kyle and Gowda, Thamme and Kejriwal, Mayank and Boecking, Benedikt},
title = {Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278782},
doi = {10.1145/3278721.3278782},
abstract = {Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {137–143},
numpages = {7},
keywords = {web crawling, text classification, human trafficking, clustering, bias mitigation},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@article{10.1016/j.specom.2012.01.002,
author = {Zelinka, Petr and Sigmund, Milan and Schimmel, Jiri},
title = {Impact of vocal effort variability on automatic speech recognition},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {6},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2012.01.002},
doi = {10.1016/j.specom.2012.01.002},
abstract = {The impact of changes in a speaker's vocal effort on the performance of automatic speech recognition has largely been overlooked by researchers and virtually no speech resources exist for the development and testing of speech recognizers at all vocal effort levels. This study deals with speech properties in the whole range of vocal modes - whispering, soft speech, normal speech, loud speech, and shouting. Fundamental acoustic and phonetic changes are documented. The impact of vocal effort variability on the performance of an isolated-word recognizer is shown and effective means of improving the system's robustness are tested. The proposed multiple model framework approach reaches a 50% relative reduction of word error rate compared to the baseline system. A new specialized speech database, BUT-VE1, is presented, which contains speech recordings of 13 speakers at 5 vocal effort levels with manual phonetic segmentation and sound pressure level calibration.},
journal = {Speech Commun.},
month = jul,
pages = {732–742},
numpages = {11},
keywords = {Vocal effort level, Robust speech recognition, Machine learning}
}

@inproceedings{10.1007/978-3-030-89029-2_8,
author = {Long, Xiaoyi and Hu, Ruimin and Xu, Xin},
title = {Variance Weight Distribution Network Based Noise Sample Learning for Robust Person Re-identification},
year = {2021},
isbn = {978-3-030-89028-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89029-2_8},
doi = {10.1007/978-3-030-89029-2_8},
abstract = {Person re-identification (re-ID) usually requires a large amount of well-labeled training data to learn generalized discriminative person feature representations. Most of current deep learning models assume that all training data are correctly labeled. However, noisy data commonly exists due to incorrect labeling and person detector errors or occlusions in large scale practical applications. Both types of noisy data can influence model training, while they are ignored by most re-ID models so far. In this paper, we propose a robust deep re-ID model, called variance weight distribution network (VWD-Net), to address this problem. Different from the traditional representations of each person image as a feature vector, the variance weight distribution network focuses on the following three aspects. 1) An improved Gaussian distribution and its variance are used to represent the uncertainty of person features. 2) A well-designed loss in the variance weight distribution network is used to delegate the distribution uncertainty with respect to the training data. 3) The noisy labels are rectified for further optimization on the model training performance. The large scale variance/uncertainty has been assigned to noisy samples and then rectifies their labels, in order to mitigate their negative impact on the training process. Extensive experiments on two benchmarks demonstrate the robustness and effectiveness of VWD-Net.},
booktitle = {Advances in Computer Graphics: 38th Computer Graphics International Conference, CGI 2021, Virtual Event, September 6–10, 2021, Proceedings},
pages = {101–112},
numpages = {12},
keywords = {Feature distribution, Noise sample learning, Person re-identification}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {deep active learning, active learning, Deep learning}
}

@article{10.1016/j.knosys.2021.107196,
author = {Wang, Xiaodi and Tang, Mingwei and Yang, Tian and Wang, Zhen},
title = {A novel network with multiple attention mechanisms for aspect-level sentiment analysis},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107196},
doi = {10.1016/j.knosys.2021.107196},
journal = {Know.-Based Syst.},
month = sep,
numpages = {12},
keywords = {Natural language processing, Pre-trained BERT, Attention mechanism, Aspect-level sentiment analysis}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {semi-supervised learning, multi-modal learning, label propagation, Curriculum learning}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {static analysis, nonvolatile memory, machine learning, code optimization, Silent stores}
}

@article{10.1006/cviu.1998.0704,
author = {H\"{a}usler, G. and Ritter, D.},
title = {Feature-Based Object Recognition and Localization in 3D-Space, Using a Single Video Image},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {73},
number = {1},
issn = {1077-3142},
url = {https://doi.org/10.1006/cviu.1998.0704},
doi = {10.1006/cviu.1998.0704},
abstract = {We introduce a robust algorithm to recognize objects in 3D space from one 2D video image and to localize the objects in all six degrees of freedom. Point-like attached features are used in the input image and additional edge information provides grouping. In an initial phase, a 3D model of all objects to be recognized is stored in the computer represented by their features. Combining the location of the detected features in the 2D input scene with the features of the 3D computer model, each single feature gives a subspace as possible solutions of the location parameters to be determined. The points of intersection of the corresponding trajectories are accumulated as possible solutions in a Hough table. The location of the highest peak in the space of hypothetical solutions delivers the desired rotation and translation parameters, even for partially hidden objects. The fully analytical algorithm is adapted to weak perspective (orthographic and scale) as well as to perspective projection. An application to range images leads to the automated feature modeling of the required 3D reference objects.},
journal = {Comput. Vis. Image Underst.},
month = jan,
pages = {64–81},
numpages = {18},
keywords = {segmentation, recognition, perspective projection, matching, localization, inspection, feature modeling, 3D}
}

@article{10.5555/3291125.3309615,
author = {Lamprier, Sylvain and Gisselbrecht, Thibault and Gallinari, Patrick},
title = {Profile-based bandit with unknown profiles},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such proffles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2060–2099},
numpages = {40},
keywords = {upper confidence bounds, stochastic linear bandits, profile-based exploration}
}

@inproceedings{10.1145/3490035.3490262,
author = {Bansal, Rahul and Biswas, Soma},
title = {CT-DANN: co-teaching meets DANN for wild unsupervised domain adaptation},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490262},
doi = {10.1145/3490035.3490262},
abstract = {Unsupervised domain adaptation aims at leveraging supervision from an annotated source domain for performing tasks like classification/segmentation on an unsupervised target domain. However, a large enough related dataset with clean annotations may not be always available in real scenarios, since annotations are usually obtained from crowd sourcing, and thus are noisy. Here, we consider a more realistic and challenging setting, wild unsupervised domain adaptation (WUDA), where the source domain annotations can be noisy. Standard domain adaptation approaches which directly use these noisy source labels and the unlabeled targets for the domain adaptation task perform poorly, due to severe negative transfer from the noisy source domain. In this work, we propose a novel end-to-end framework, termed CT-DANN (Co-teaching meets DANN), which seamlessly integrates a state-of-the-art approach for handling noisy labels (Co-teaching) with a standard domain adaptation framework (DANN). CT-DANN effectively utilizes all the source samples after accounting for both their noisy labels as well as transferability with respect to the target domain. Extensive experiments on three benchmark datasets with different types and levels of noise and comparison with state-of-the-art WUDA approach justify the effectiveness of the proposed framework.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {5},
numpages = {8},
keywords = {wild unsupervised domain adaptation, source data weighting, noisy source data, co-teaching},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inproceedings{10.5555/1308171.1308193,
author = {Gruler, Alexander and Harhurin, Alexander and Hartmann, Judith},
title = {Development and Configuration of Service-based Product Lines},
year = {2007},
isbn = {0769528880},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced during the development of multi-functional system families. Addressing this trend we present an approach combining model-based development with product line techniques aiming at a consistent description of a software product family as well as supporting the configuration of its variants. We integrate the concept of variability in our framework [7] which only supported the representation of single software systems on subsequent abstraction levels so far. For the configuration of a concrete product we extend this framework by a feature-based model which allows to configure and derive single systems from a system family model. Furthermore, we explain how the complexity due to the possibly huge amount of configuration decisions can be handled by means of a staged configuration process.},
booktitle = {Proceedings of the 11th International Software Product Line Conference},
pages = {107–116},
numpages = {10},
series = {SPLC '07}
}

@inproceedings{10.5555/3299905.3299978,
author = {Safavi, Saeid and Wang, Wenwu and Plumbley, Mark and Choobbasti, Ali Janalizadeh and Fazekas, George},
title = {Predicting the Perceived Level of Reverberation using Features from Nonlinear Auditory Model},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Perceptual measurements have typically been recognized as the most reliable measurements in assessing perceived levels of reverberation. In this paper, a combination of blind RT60 estimation method and a binaural, nonlinear auditory model is employed to derive signal-based measures (features) that are then utilized in predicting the perceived level of rever- beration. Such measures lack the excess of effort necessary for calculating perceptual measures; not to mention the variations in either stimuli or assessors that may cause such measures to be statistically insigni?cant. As a result, the automatic extraction of objective measurements that can be applied to predict the perceived level of reverberation become of vital signi?cance. Consequently, this work is aimed at discovering measurements such as clarity, reverberance, and RT60 which can automatically be derived directly from audio data. These measurements along with labels from human listening tests are then forwarded to a machine learning system seeking to build a model to estimate the perceived level of reverberation, which is labeled by an expert, autonomously. The data has been labeled by an expert human listener for a unilateral set of ?les from arbitrary audio source types. By examining the results, it can be observed that the automatically extracted features can aid in estimating the perceptual rates.},
booktitle = {Proceedings of the 23rd Conference of Open Innovations Association FRUCT},
articleno = {73},
numpages = {5},
keywords = {machine learning, Reverberation, Human experiments, Feature extraction, Audio signal processing},
location = {Bologna, Italy},
series = {FRUCT'23}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3167132.3167162,
author = {Hielscher, Tommy and V\"{o}lzke, Henry and Papapetrou, Panagiotis and Spiliopoulou, Myra},
title = {Discovering, selecting and exploiting feature sequence records of study participants for the classification of epidemiological data on hepatic steatosis},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167162},
doi = {10.1145/3167132.3167162},
abstract = {In longitudinal epidemiological studies, participants undergo repeated medical examinations and are thus represented by a potentially large number of short examination outcome sequences. Some of those sequences may contain important information in various forms, such as patterns, with respect to the disease under study, while others may be on features of little relevance to the outcome. In this work, we propose a framework for Discovery, Selection and Exploitation (DiSelEx) of longitudinal epidemiological data, aiming to identify informative patterns among these sequences. DiSelEx combines sequence clustering with supervised learning to identify sequence groups that contribute to class separation. Newly derived and old features are evaluated and selected according to their redundancy and informativeness regarding the target variable. The selected feature set is then used to learn a classification model on the study data. We evaluate DiSelEx on cohort participants for the disorder "hepatic steatosis" and report on the impact on predictive performance when using sequential data in comparison to utilizing only the basic classifier.1},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {6–13},
numpages = {8},
keywords = {time-series clustering, patient similarity, medical data mining, hepatic steatosis, feature selection, epidemiological studies, classification},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1109/ICMA.2017.8016048,
author = {Li, Kang and Zhao, Xiaoguang and Bian, Jiang and Tan, Min},
title = {Sequential learning for multimodal 3D human activity recognition with Long-Short Term Memory},
year = {2017},
isbn = {978-1-5090-6758-9},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICMA.2017.8016048},
doi = {10.1109/ICMA.2017.8016048},
abstract = {Capability of recognizing human activities is essential to human robot interaction for an intelligent robot. Traditional methods generally rely on hand-crafted features, which is not strong and accurate enough. In this paper, we present a feature self-learning mechanism for human activity recognition by using three-layer Long Short Term Memory (LSTM) to model long-term contextual information of temporal skeleton sequences for human activities which are represented by the trajectories of skeleton joints. Moreover, we add dropout mechanism and L2 regularization to the output of the three-layer Long Short Term Memory (LSTM) to avoid overfitting, and obtain better representation for feature modeling. Experimental results on a publicly available UTD multimodal human activity dataset demonstrate the effectiveness of the proposed recognition method.},
booktitle = {2017 IEEE International Conference on Mechatronics and Automation (ICMA)},
pages = {1556–1561},
numpages = {6},
location = {Takamatsu, Japan}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@article{10.1007/s42979-021-00932-x,
author = {Yuan, Meixue and Wei, Shouke and Zhao, Jindong and Sun, Ming},
title = {A Systematic Survey on Human Behavior Recognition Methods},
year = {2021},
issue_date = {Jan 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {1},
url = {https://doi.org/10.1007/s42979-021-00932-x},
doi = {10.1007/s42979-021-00932-x},
abstract = {Human behavior is an essential component of social interaction and is of great significance to identify and analyze human behaviors in a variety of fields. Due to the rapid development of computer vision and machine learning technology, machine with intelligence has started replacing human beings to observe, perceive and analyze the explosive growth of image and video data. Computer vision and machine learning-based human behavior recognition is one of these tasks, which has become a particularly hot research topic in many different fields, such as intelligent monitoring, human–computer interaction, smart home, virtual reality, and medical diagnosis. In this study, we survey systematically the popular methods, algorithms, models and well-known action datasets in human behavior analysis in the past two decades. In addition, the advantages and disadvantages of the methods are discussed and propitious future research directions are also presented. The results of this survey reveal that paradigms of human behavior analysis is being shifted from traditional RGB to RGB-D, from deep learning to more intelligent and automated deep reinforcement learning, and from fixed camera devices to portable devices and channel state information (CSI), and paradigms based on automated deep reinforcement learning and portable devices and CSI would become some hot topics for future research on human behavior analysis.},
journal = {SN Comput. Sci.},
month = oct,
numpages = {25},
keywords = {CSI, Portable devices, Deep reinforcement learning, Deep learning, Computer vision, Human behavior recognition}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Systematic literature review, Software product lines, Requirements engineering}
}

@inproceedings{10.1145/3292500.3330990,
author = {Chen, Yu-Chia and Bijral, Avleen S. and Ferres, Juan Lavista},
title = {On Dynamic Network Models and Application to Causal Impact},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330990},
doi = {10.1145/3292500.3330990},
abstract = {Dynamic extensions of Stochastic block model (SBM) are of importance in several fields that generate temporal interaction data. These models, besides producing compact and interpretable network representations, can be useful in applications such as link prediction or network forecasting. In this paper we present a conditional pseudo-likelihood based extension to dynamic SBM that can be efficiently estimated by optimizing a regularized objective. Our formulation leads to a highly scalable approach that can handle very large networks, even with millions of nodes. We also extend our formalism to causal impact for networks that allows us to quantify the impact of external events on a time dependent sequence of networks. We support our work with extensive results on both synthetic and real networks.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1194–1204},
numpages = {11},
keywords = {stochastic block model, dynamic networks, clustering, causal impact},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1016/j.eswa.2021.115466,
author = {Kim, Jin-Young and Cho, Sung-Bae},
title = {A systematic analysis and guidelines of graph neural networks for practical applications},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {184},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115466},
doi = {10.1016/j.eswa.2021.115466},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {15},
keywords = {Bioinformatics, Social network analysis, Graph classification, Deep learning, Graph embedding, Graph neural network}
}

@article{10.1007/s10489-018-01399-9,
author = {Abolpour Mofrad, Asieh and Yazidi, Anis and Lewi Hammer, Hugo},
title = {On solving the SPL problem using the concept of probability flux},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-01399-9},
doi = {10.1007/s10489-018-01399-9},
abstract = {The Stochastic Point Location (SPL) problem Oommen is a fundamental learning problem that has recently found a lot of research attention. SPL can be summarized as searching for an unknown point in an interval under faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic Environment which in turn informs it about the direction of the search. Since the Environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem, which was pioneered two decades ago by Oommen, relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution, i.e., infinite memory. However, this strategy yields rather poor accuracy for low discretization resolutions. In this paper, we present two major contributions to the SPL problem. First, we demonstrate that the estimation of the point location can significantly be improved by resorting to the concept of mutual probability flux between neighboring states along the line. Second, we are able to accurately track the position of the optimal point and simultaneously show a method by which we can estimate the error probability characterizing the Environment. Interestingly, learning this error probability of the Environment takes place in tandem with the unknown location estimation. We present and analyze several experiments discussing the weaknesses and strengths of the different methods.},
journal = {Applied Intelligence},
month = jul,
pages = {2699–2722},
numpages = {24},
keywords = {Stochastic Point Location (SPL), Stochastic Learning Weak Estimation (SLWE), Mutual probability flux, Last Transition-based Estimation Solution (LTES), Flux-based Estimation Solution (FES), Estimating environment effectiveness}
}

@article{10.5555/2598944.2599210,
author = {Maldonado, Sebasti\'{a}n and L\'{o}pez, Julio},
title = {Alternative second-order cone programming formulations for support vector classification},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {268},
issn = {0020-0255},
abstract = {This paper presents two novel second-order cone programming (SOCP) formulations that determine a linear predictor using Support Vector Machines (SVMs). Inspired by the soft-margin SVM formulation, our first approach (@x-SOCP-SVM) proposes a relaxation of the conic constraints via a slack variable, penalizing it in the objective function. The second formulation (r-SOCP-SVM) is based on the LP-SVM formulation principle: the bound of the VC dimension is loosened properly using the l"~-norm, and the margin is directly maximized. The proposed methods have several advantages: The first approach constructs a flexible classifier, extending the benefits of the soft-margin SVM formulation to second-order cones. The second method obtains comparable results to the SOCP-SVM formulation with less computational effort, since one conic restriction is eliminated. Experiments on well-known benchmark datasets from the UCI Repository demonstrate that our approach accomplishes the best classification performance compared to the traditional SOCP-SVM formulation, LP-SVM, and to standard linear SVM.},
journal = {Inf. Sci.},
month = jun,
pages = {328–341},
numpages = {14},
keywords = {Support Vector Machine, Second-order cone programming, Linear programming SVM}
}

@inproceedings{10.5555/3386691.3386706,
author = {Lu, Sidi and Luo, Bing and Patel, Tirthak and Yao, Yongtao and Tiwari, Devesh and Shi, Weisong},
title = {Making disk failure predictions SMARTer!},
year = {2020},
isbn = {9781939133120},
publisher = {USENIX Association},
address = {USA},
abstract = {Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.},
booktitle = {Proceedings of the 18th USENIX Conference on File and Storage Technologies},
pages = {151–168},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {FAST'20}
}

@article{10.1007/s10916-019-1433-z,
author = {Dong, Xiaojun and Du, Hongmei and Guan, Haichen and Zhang, Xuezhen},
title = {Multiscale Time-Sharing Elastography Algorithms and Transfer Learning of Clinicopathological Features of Uterine Cervical Cancer for Medical Intelligent Computing System},
year = {2019},
issue_date = {Oct 2019},
publisher = {Plenum Press},
address = {USA},
volume = {43},
number = {10},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-019-1433-z},
doi = {10.1007/s10916-019-1433-z},
abstract = {Intelligent medical diagnosis and computing system faces many challenges in complex object recognition, large-scale data imaging and real-time diagnosis, such as poor real-time computing, low efficiency of data storage and low recognition rate of lesions. In order to solve the above problems, this paper proposes a medical intelligent computing system and a series of algorithms for the clinical pathology of cervical cancer based on the multi-scale imaging and transfer learning framework. Firstly, based on data dimensions, imaging errors and other factors, this paper designs a multi-scale time-sharing elastic imaging algorithm based on image reconstruction time and data sample characteristics. Then, taking the burst imaging cohort and the calculation data set of new cervical cancer cases as the objects, based on the difficulties of cervical cancer feature modeling, this paper proposes the transfer learning algorithm of clinical and pathological features of cervical cancer. Finally, a medical intelligent computing system for cervical cancer pathology analysis and calculation with high efficiency and reliability is established. A series of proposed algorithms are compared with single-scale Retinex (SSR), which is based on single-scale Retinex migration learning (SSR-TL). The experimental results show that the proposed algorithm in cervical cancer pathological imaging and scoring, as well as the feature extraction and recognition of lesions, especially the efficiency of system execution, is obviously due to the comparison algorithm.},
journal = {J. Med. Syst.},
month = oct,
pages = {1–12},
numpages = {12},
keywords = {Medical intelligent computing, Transfer learning, Multiscale, Multiscale, Time-sharing}
}

@article{10.1007/s00500-014-1364-z,
author = {Baladhandapani, Arunadevi and Nachimuthu, Deepa Subramaniam},
title = {Evolutionary learning of spiking neural networks towards quantification of 3D MRI brain tumor tissues},
year = {2015},
issue_date = {July      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {7},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-014-1364-z},
doi = {10.1007/s00500-014-1364-z},
abstract = {This paper presents a new classification technique for 3D MR images, based on a third-generation network of spiking neurons. Implementation of multi-dimensional co-occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed. The results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue-specific structural patterns and achieve segmentation of tumor part. The spiking network classifier has been validated and tested for various real-time and Harvard benchmark datasets, where appreciable performance in terms of mean square error, accuracy and computational time is obtained. The spiking network employed Izhikevich neurons as nodes in a multi-layered structure. The classifier has been compared with computational power of multi-layer neural networks with sigmoidal neurons. The results on misclassified tumors are analyzed and suggestions for future work are discussed.},
journal = {Soft Comput.},
month = jul,
pages = {1803–1816},
numpages = {14},
keywords = {Spiking neural networks, Multi-dimensional co-occurrence matrices, Izhikevich neurons, Genetic algorithm, 3D Magnetic resonance imaging}
}

@article{10.1016/j.patcog.2008.09.015,
author = {Liu, Manhua and Jiang, Xudong and Kot, Alex C.},
title = {A multi-prototype clustering algorithm},
year = {2009},
issue_date = {May, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.09.015},
doi = {10.1016/j.patcog.2008.09.015},
abstract = {Clustering is an important unsupervised learning technique widely used to discover the inherent structure of a given data set. Some existing clustering algorithms uses single prototype to represent each cluster, which may not adequately model the clusters of arbitrary shape and size and hence limit the clustering performance on complex data structure. This paper proposes a clustering algorithm to represent one cluster by multiple prototypes. The squared-error clustering is used to produce a number of prototypes to locate the regions of high density because of its low computational cost and yet good performance. A separation measure is proposed to evaluate how well two prototypes are separated. Multiple prototypes with small separations are grouped into a given number of clusters in the agglomerative method. New prototypes are iteratively added to improve the poor cluster separations. As a result, the proposed algorithm can discover the clusters of complex structure with robustness to initial settings. Experimental results on both synthetic and real data sets demonstrate the effectiveness of the proposed clustering algorithm.},
journal = {Pattern Recogn.},
month = may,
pages = {689–698},
numpages = {10},
keywords = {Squared-error clustering, Separation measure, Data clustering, Cluster prototype}
}

@article{10.1007/s10664-017-9580-7,
author = {Przyby\l{}ek, Adam},
title = {An empirical study on the impact of AspectJ on software evolvability},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9580-7},
doi = {10.1007/s10664-017-9580-7},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2018–2050},
numpages = {33},
keywords = {Understandability, Separation of concerns, Maintainability, Controlled experiment, Aspect-oriented programming, AOP}
}

@inproceedings{10.5555/3495724.3496750,
author = {Zhang, Dingwen and Tian, Haibin and Han, Jungong},
title = {Few-cost salient object detection with adversarial-paced learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1026},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/3394231.3397901,
author = {Aloshban, Nujud},
title = {ACT : Automatic Fake News Classification Through Self-Attention},
year = {2020},
isbn = {9781450379892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394231.3397901},
doi = {10.1145/3394231.3397901},
abstract = {Automatic detection of fake news is an important issue given the disproportionate effect of fake news on democratic processes, individuals and institutions. Research on automated fact-checking has proposed different approaches based on traditional machine learning methods, using hand-crafted lexical features. Nevertheless, these approaches focus on analyzing the text claim without considering the facts that are not explicitly given but can be derived from it. For example, external evidence that is retrieved from the Web as a knowledge source of the claim can provide complementary context of the claim and gives convincing reasons from it to support or oppose. Recent approaches study this deficit by incorporating supportive evidence (article) corresponding to the claim. However, these methods are either requiring substantial feature modeling, not considering several supporting evidences, or even not analyzing the language of the supporting evidence deeply. To this end, we propose an end-to-end framework, named Automatic Fake News Classification Through Self-Attention (ACT), which exploits different supportive articles to a claim which mimics manual fact-checking processes. The model presents an approach that computes the claim credibility by aggregating over the prediction generated by every claim-retrieved article pair. The article input is represented by using self-attention on the top of a bidirectional LSTM neural network. By using the self-attention, the model concentrates on nuanced linguistic features and does not require any feature engineering, lexicons or any other manual intervention. Moreover, different aspects of the supporting article are extracted into multiple vector representations. Hence, different meaningful article representations can be extracted into a two-dimensional matrix to represent the article. In the end, a majority vote over the several external articles of a given claim is applied to assess the claim’s credibility. We conduct experiments on three different real-world datasets, compare them to the state-of-the-art approaches and analyze our results, which shows performance improvements.},
booktitle = {Proceedings of the 12th ACM Conference on Web Science},
pages = {115–124},
numpages = {10},
keywords = {recurrent neural networks, fake news classification, deep learning},
location = {Southampton, United Kingdom},
series = {WebSci '20}
}

@article{10.1016/j.compag.2019.105023,
author = {Moon, Taewon and Hong, Seojung and Choi, Ha Young and Jung, Dae Ho and Chang, Se Hong and Son, Jung Eek},
title = {Interpolation of greenhouse environment data using multilayer perceptron},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105023},
doi = {10.1016/j.compag.2019.105023},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {8},
keywords = {Spline, Random forest, Multivariate regression, Linear, Data loss}
}

@inproceedings{10.1145/3150928.3150941,
author = {Geyer, Fabien},
title = {Performance Evaluation of Network Topologies using Graph-Based Deep Learning},
year = {2017},
isbn = {9781450363464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3150928.3150941},
doi = {10.1145/3150928.3150941},
abstract = {Understanding the performance of network protocols and communication networks generally relies on expert knowledge and understanding of the different elements of a network, their configuration and the overall architecture and topology. Machine learning is often proposed as a tool to help modeling complex protocols. One drawback of this method is that high-level features are generally used - which require expert knowledge on the network protocols to be chosen, correctly engineered, and measured -- and the approaches are generally limited to a given network topology.In this paper, we propose a methodology to address the challenge of working with machine learning by using lower-level features, namely only a description of the network architecture. Our main contribution is an approach for applying deep learning on network topologies via the use of Graph Gated Neural Networks, a specialized recurrent neural network for graphs. Our approach enables us to make performance predictions based only on a graph-based representation of network topologies. We apply our approach to the task of predicting the throughput of TCP flows. We evaluate three different traffic models: large file transfers, small file transfers, and a combination of small and large file transfers. Numerical results show that our approach is able to learn the throughput performance of TCP flows with good accuracies larger than 90%, even on larger topologies.},
booktitle = {Proceedings of the 11th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {20–27},
numpages = {8},
keywords = {Network performance evaluation, Graph Neural Network, Deep learning},
location = {Venice, Italy},
series = {VALUETOOLS 2017}
}

@inproceedings{10.1145/2783258.2783347,
author = {Okumura, Shota and Suzuki, Yoshiki and Takeuchi, Ichiro},
title = {Quick Sensitivity Analysis for Incremental Data Modification and Its Application to Leave-one-out CV in Linear Classification Problems},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783347},
doi = {10.1145/2783258.2783347},
abstract = {We introduce a novel sensitivity analysis framework for large scale classification problems that can be used when a small number of instances are incrementally added or removed. For quickly updating the classifier in such a situation, incremental learning algorithms have been intensively studied in the literature. Although they are much more efficient than solving the optimization problem from scratch, their computational complexity yet depends on the entire training set size. It means that, if the original training set is large, completely solving an incremental learning problem might be still rather expensive. To circumvent this computational issue, we propose a novel framework that allows us to make an inference about the updated classifier without actually re-optimizing it. Specifically, the proposed framework can quickly provide a lower and an upper bounds of a quantity on the unknown updated classifier. The main advantage of the proposed framework is that the computational cost of computing these bounds depends only on the number of updated instances. This property is quite advantageous in a typical sensitivity analysis task where only a small number of instances are updated. In this paper we demonstrate that the proposed framework is applicable to various practical sensitivity analysis tasks, and the bounds provided by the framework are often sufficiently tight for making desired inferences.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {885–894},
numpages = {10},
keywords = {sensitivity analysis, leave-one-out cross- validation, incremental learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@article{10.1016/j.specom.2017.04.002,
title = {Fourier model based features for analysis and classification of out-of-breath speech},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.04.002},
doi = {10.1016/j.specom.2017.04.002},
abstract = {A new stressed speech database, named out-of-breath speech (OBS) database, is created, which contains three classes, out-of-breath speech, low out-of-breath speech and normal speech.Four features are proposed using mutual information (MI) on the Fourier parameters for analysis and classification of different classes of OBS database.For multi-class classification, support vector machine (SVM) classifier is used with binary cascade approach.Recognition results show that the proposed features have higher potential to classify the out-of-breath speech, compared to the breathiness, MFCC and TEO-CB-Auto-Env features. This paper presents a new method of feature extraction using Fourier model for analysis of out-of-breath speech. The proposed feature is evaluated using mutual information (MI) on the difference and ratio values of the Fourier parameters, amplitude and frequency. The difference and ratio are calculated between two contiguous values of the Fourier parameters. To analyze the out-of-breath speech, a new stressed speech database, named out-of-breath speech (OBS) database, is created. The database contains three classes of speech, out-of-breath speech, low out-of-breath speech and normal speech. The effectiveness of the proposed features is evaluated with the statistical analysis. The proposed features not only differentiate the normal speech and the out-of-breath speech, but also can discriminate different breath emission levels of speech. Hidden Markov model (HMM) and support vector machine (SVM) are used to evaluate the performance of the proposed features using the OBS database. For multi-class classification problem, SVM classifier is used with binary cascade approach. The performance of the proposed features is compared with the breathiness feature, the mel frequency cepstral coefficient (MFCC) feature and the Teager energy operator (TEO) based critical band TEO autocorrelation envelope (TEO-CB-Auto-Env) feature. The proposed feature outperforms the breathiness feature, the MFCC feature and the TEO-CB-Auto-Env feature.},
journal = {Speech Commun.},
month = jun,
pages = {1–14},
numpages = {14}
}

@article{10.1016/j.neucom.2015.04.087,
author = {Liu, Weifeng and Li, Yang and Tao, Dapeng and Wang, Yanjiang},
title = {A general framework for co-training and its applications},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.04.087},
doi = {10.1016/j.neucom.2015.04.087},
abstract = {Co-training is one of the major semi-supervised learning paradigms in which two classifiers are alternately trained on two distinct views and they teach each other by adding the predictions of unlabeled data to the training set of the other view. Co-training can achieve promising performance, especially when there is only a small number of labeled data. Hence, co-training has received considerable attention, and many variant co-training algorithms have been developed. It is essential and informative to provide a systematic framework for a better understanding of the common properties and differences in these algorithms. In this paper, we propose a general framework for co-training according to the diverse learners constructed in co-training. Specifically, we provide three types of co-training implementations, including co-training on multiple views, co-training on multiple classifiers, and co-training on multiple manifolds. Finally, comprehensive experiments of different methods are conducted on the UCF-iPhone dataset for human action recognition and the USAA dataset for social activity recognition. The experimental results demonstrate the effectiveness of the proposed solutions.},
journal = {Neurocomput.},
month = nov,
pages = {112–121},
numpages = {10},
keywords = {Social activity recognition, Semi-supervised learning, Multi-view, Human action recognition, Co-training}
}

@inproceedings{10.1145/3236024.3236052,
author = {Song, Liyan and Minku, Leandro L. and Yao, Xin},
title = {A novel automated approach for software effort estimation based on data augmentation},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236052},
doi = {10.1145/3236024.3236052},
abstract = {Software effort estimation (SEE) usually suffers from data scarcity problem due to the expensive or long process of data collection. As a result, companies usually have limited projects for effort estimation, causing unsatisfactory prediction performance. Few studies have investigated strategies to generate additional SEE data to aid such learning. We aim to propose a synthetic data generator to address the data scarcity problem of SEE. Our synthetic generator enlarges the SEE data set size by slightly displacing some randomly chosen training examples. It can be used with any SEE method as a data preprocessor. Its effectiveness is justified with 6 state-of-the-art SEE models across 14 SEE data sets. We also compare our data generator against the only existing approach in the SEE literature. Experimental results show that our synthetic projects can significantly improve the performance of some SEE methods especially when the training data is insufficient. When they cannot significantly improve the prediction performance, they are not detrimental either. Besides, our synthetic data generator is significantly superior or perform similarly to its competitor in the SEE literature. Therefore, our data generator plays a non-harmful if not significantly beneficial effect on the SEE methods investigated in this paper. Therefore, it is helpful in addressing the data scarcity problem of SEE.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {468–479},
numpages = {12},
keywords = {synthetic data, data scarcity, data generation, data augmentation, Software effort estimation},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3059336.3059348,
author = {Chen, Junyi and Yan, Shankai and Wong, Ka-Chun},
title = {Aggressivity Detection on Social Network Comments},
year = {2017},
isbn = {9781450347983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3059336.3059348},
doi = {10.1145/3059336.3059348},
abstract = {Verbal aggression and cyberbullying are widely concerned issues in netiquette. In this article, we introduce a text mining system that can detect whether a certain paragraph contains the aggressive sentiment, and demonstrate its performance with different classification models. In addition, it is observed that our system works well on both our manually curated dataset and the de facto dataset. Extensive experiments are conducted to validate the effectiveness of the proposed system and highlight some possible factors that contribute to the robust results.},
booktitle = {Proceedings of the 2017 International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence},
pages = {103–107},
numpages = {5},
keywords = {Sentiment Analysis, Machine Learning, Aggressivity Detection},
location = {Hong Kong, Hong Kong},
series = {ISMSI '17}
}

@article{10.1155/2021/5694975,
author = {Lu, Ruochen and Lu, Muchao and Zhang, Yuanpeng},
title = {Stock Trend Prediction Algorithm Based on Deep Recurrent Neural Network},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/5694975},
doi = {10.1155/2021/5694975},
abstract = {With the return of deep learning methods to the public eye, more and more scholars and industry researchers have tried to start exploring the possibility of neural networks to solve the problem, and some progress has been made. However, although neural networks have powerful function fitting ability, they are often criticized for their lack of explanatory power. Due to the large number of parameters and complex structure of neural network models, academics are unable to explain the predictive logic of most neural networks, test the significance of model parameters, and summarize the laws that humans can understand and use. Inspired by the technical analysis theory in the field of stock investment, this paper selects neural network models with different characteristics and extracts effective feature combinations from short-term stock price fluctuation data. In addition, on the basis of ensuring that the prediction effect of the model is not lower than that of the mainstream models, this paper uses the attention mechanism to further explore the predictive K-line patterns, which summarizes usable judgment experience for human researchers on the one hand and explains the prediction logic of the hybrid neural network on the other. Experiments show that the classification effect is better using this model, and the investor sentiment is obtained more accurately, and the accuracy rate can reach 85%, which lays the foundation for the establishment of the whole stock trend prediction model. In terms of explaining the prediction logic of the model, it is experimentally demonstrated that the K-line patterns mined using the attention mechanism have more significant predictive power than the general K-line patterns, and this result explains the prediction basis of the hybrid neural network.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {10}
}

@article{10.5555/3122009.3176858,
author = {Fenn, Shannon and Moscato, Pablo},
title = {Target curricula via selection of minimum feature sets: a case study in Boolean networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4070–4095},
numpages = {26},
keywords = {target curriculum, multi-label classification, k-feature Set, Boolean betworks}
}

@article{10.5555/3122009.3242055,
author = {Patrascu, Andrei and Necoara, Ion},
title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k1/2), where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7204–7245},
numpages = {42},
keywords = {stochastic proximal point, stochastic convex optimization, rates of convergence, nonasymptotic convergence analysis, intersection of convex constraints}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {deep learning, constraint solving, configurable systems, AutoML},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.jbi.2008.12.012,
author = {Saha, Sujan Kumar and Sarkar, Sudeshna and Mitra, Pabitra},
title = {Feature selection techniques for maximum entropy based biomedical named entity recognition},
year = {2009},
issue_date = {October, 2009},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {42},
number = {5},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2008.12.012},
doi = {10.1016/j.jbi.2008.12.012},
abstract = {Named entity recognition is an extremely important and fundamental task of biomedical text mining. Biomedical named entities include mentions of proteins, genes, DNA, RNA, etc which often have complex structures, but it is challenging to identify and classify such entities. Machine learning methods like CRF, MEMM and SVM have been widely used for learning to recognize such entities from an annotated corpus. The identification of appropriate feature templates and the selection of the important feature values play a very important role in the success of these methods. In this paper, we provide a study on word clustering and selection based feature reduction approaches for named entity recognition using a maximum entropy classifier. The identification and selection of features are largely done automatically without using domain knowledge. The performance of the system is found to be superior to existing systems which do not use domain knowledge.},
journal = {J. of Biomedical Informatics},
month = oct,
pages = {905–911},
numpages = {7},
keywords = {Maximum entropy classifier, Machine learning, Feature selection, Feature reduction, Biomedical named entity recognition}
}

@article{10.1007/s00180-012-0381-6,
author = {Schmidt, Miriam and Palm, G\"{u}nther and Schwenker, Friedhelm},
title = {Spectral graph features for the classification of graphs and graph sequences},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1–2},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-012-0381-6},
doi = {10.1007/s00180-012-0381-6},
abstract = {In this paper, the classification power of the eigenvalues of six graph-associated matrices is investigated. Each matrix contains a certain type of geometric/ spatial information, which may be important for the classification process. The performances of the different feature types is evaluated on two data sets: first a benchmark data set for optical character recognition, where the extracted eigenvalues were utilized as feature vectors for multi-class classification using support vector machines. Classification results are presented for all six feature types, as well as for classifier combinations at decision level. For the decision level combination, probabilistic output support vector machines have been applied, with a performance up to 92.4 %. To investigate the power of the spectra for time dependent tasks, too, a second data set was investigated, consisting of human activities in video streams. To model the time dependency, hidden Markov models were utilized and the classification rate reached 98.3 %.},
journal = {Comput. Stat.},
month = feb,
pages = {65–80},
numpages = {16},
keywords = {Spectrum, Optical character recognition, Human activity recognition, Graph-associated matrices, Graph classification}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1007/s11042-018-5626-0,
author = {Lei, Qing and Zhang, Hongbo and Xin, Minghai and Cai, Yiqiao},
title = {A hierarchical representation for human action recognition in realistic scenes},
year = {2018},
issue_date = {May       2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {9},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-5626-0},
doi = {10.1007/s11042-018-5626-0},
abstract = {BoF statistic-based local space-time features action representation is very popular for human action recognition due to its simplicity. However, the problem of large quantization error and weak semantic representation decrease traditional BoF model's discriminant ability when applied to human action recognition in realistic scenes. To deal with the problems, we investigate the generalization ability of BoF framework for action representation as well as more effective feature encoding about high-level semantics. Towards this end, we present two-layer hierarchical codebook learning framework for human action classification in realistic scenes. In the first-layer action modelling, superpixel GMM model is developed to filter out noise features in STIP extraction resulted from cluttered background, and class-specific learning strategy is employed on the refined STIP feature space to construct compact and descriptive in-class action codebooks. In the second-layer of action representation, LDA-Km learning algorithm is proposed for feature dimensionality reduction and for acquiring more discriminative inter-class action codebook for classification. We take advantage of hierarchical framework's representational power and the efficiency of BoF model to boost recognition performance in realistic scenes. In experiments, the performance of our proposed method is evaluated on four benchmark datasets: KTH, YouTube (UCF11), UCF Sports and Hollywood2. Experimental results show that the proposed approach achieves improved recognition accuracy than the baseline method. Comparisons with state-of-the-art works demonstrates the competitive ability both in recognition performance and time complexity.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {11403–11423},
numpages = {21},
keywords = {Video processing, Realistic scenes, Feature selection, Action recognition, Action modelling}
}

@article{10.1016/j.neucom.2018.11.060,
author = {Li, Shaoyong and Tang, Chang and Liu, Xinwang and Liu, Yaping and Chen, Jiajia},
title = {Dual graph regularized compact feature representation for unsupervised feature selection},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.060},
doi = {10.1016/j.neucom.2018.11.060},
journal = {Neurocomput.},
month = feb,
pages = {77–96},
numpages = {20},
keywords = {Feature representation, Local geometrical structure preservation, Dictionary learning, Unsupervised feature selection}
}

@article{10.1155/2021/6659430,
author = {Ye, Aoshuang and Wang, Lina and Wang, Run and Wang, Wenqi and Ke, Jianpeng and Wang, Danlei and Cherifi, Hocine},
title = {An End-to-End Rumor Detection Model Based on Feature Aggregation},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1076-2787},
url = {https://doi.org/10.1155/2021/6659430},
doi = {10.1155/2021/6659430},
abstract = {The social network has become the primary medium of rumor propagation. Moreover, manual identification of rumors is extremely time-consuming and laborious. It is crucial to identify rumors automatically. Machine learning technology is widely implemented in the identification and detection of misinformation on social networks. However, the traditional machine learning methods profoundly rely on feature engineering and domain knowledge, and the learning ability of temporal features is insufficient. Furthermore, the features used by the deep learning method based on natural language processing are heavily limited. Therefore, it is of great significance and practical value to study the rumor detection method independent of feature engineering and effectively aggregate heterogeneous features to adapt to the complex and variable social network. In this paper, a deep neural network- (DNN-) based feature aggregation modeling method is proposed, which makes full use of the knowledge of propagation pattern feature and text content feature of social network event without feature engineering and domain knowledge. The experimental results show that the feature aggregation model has achieved 94.4% of accuracy as the best performance in recent works.},
journal = {Complex.},
month = jan,
numpages = {16}
}

@inproceedings{10.1145/2970276.2975938,
author = {Babur, \"{O}nder},
title = {Statistical analysis of large sets of models},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2975938},
doi = {10.1145/2970276.2975938},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {vector space model, model comparison, clustering, Model-driven engineering},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3077136.3080777,
author = {He, Xiangnan and Chua, Tat-Seng},
title = {Neural Factorization Machines for Sparse Predictive Analytics},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080777},
doi = {10.1145/3077136.3080777},
abstract = {Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and occupations. To apply standard machine learning techniques, these categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector highly sparse. To learn from such sparse data effectively, it is crucial to account for the interactions between features.Factorization Machines (FMs) are a popular solution for efficiently using the second-order feature interactions. However, FM models feature interactions in a linear way, which can be insufficient for capturing the non-linear and complex inherent structure of real-world data. While deep neural networks have recently been applied to learn non-linear feature interactions in industry, such as the Wide&amp;Deep by Google and DeepCross by Microsoft, the deep structure meanwhile makes them difficult to train.In this paper, we propose a novel model Neural Factorization Machine (NFM) for prediction under sparse settings. NFM seamlessly combines the linearity of FM in modelling second-order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions. Conceptually, NFM is more expressive than FM since FM can be seen as a special case of NFM without hidden layers. Empirical results on two regression tasks show that with one hidden layer only, NFM significantly outperforms FM with a 7.3% relative improvement. Compared to the recent deep learning methods Wide&amp;Deep and DeepCross, our NFM uses a shallower structure but offers better performance, being much easier to train and tune in practice.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {355–364},
numpages = {10},
keywords = {sparse data, regression, recommendation, neural networks, factorization machines, deep learning},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{10.1016/j.dss.2012.10.005,
author = {Van Valkenhoef, Gert and Tervonen, Tommi and Zwinkels, Tijs and De Brock, Bert and Hillege, Hans},
title = {ADDIS: A decision support system for evidence-based medicine},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {2},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.10.005},
doi = {10.1016/j.dss.2012.10.005},
abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results.},
journal = {Decis. Support Syst.},
month = may,
pages = {459–475},
numpages = {17},
keywords = {ePRO, eLab, eCRF, caBIG, WHO, UMLS, TDM, SmPC, SPL, SNOMEDCT, SMAA, SEND, SDTM, RIM, QRD, PhRMA, PRM, PMDA, PIM, OWL, ODM, OCRe, OBX, NIHUS, NDA, NCI, MedDRA, MeSH, MCDA, LAB, JAMA, ICTRP, ICMJE, ICD, HSDB, HL7, GUI, GCP, FDAAA, FDA, Evidence-based medicine, Evidence synthesis, EPAR, EMA, EHR, EDC, EBM, EAV, Decision analysis, Data model, DSS, DOI, DIS, DED, DB, Clinical trial, CTMS, CTIS, CRO, CRF, CPOE, CHMP, CDMS, CDISC, CDASH, BRIDG, ATC, ANSI, AMIA, ADaM, ADR, ADE}
}

@inproceedings{10.5555/3495724.3496445,
author = {Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff A.},
title = {Curriculum learning by dynamic instance hardness},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A good teacher can adjust a curriculum based on students' learning history. By analogy, in this paper, we study the dynamics of a deep neural network's (DNN) performance on individual samples during its learning process. The observed properties allow us to develop an adaptive curriculum that leads to faster learning of more accurate models. We introduce dynamic instance hardness (DIH), the exponential moving average of a sample's instantaneous hardness (e.g., a loss, or a change in output) over the training history. A low DIH indicates that a model retains knowledge about a sample over time. For DNNs, we find that a sample's DIH early in training predicts its DIH in later stages. Hence, we can train a model using samples mostly with higher DIH and safely deprioritize those with lower DIH. This motivates a DIH guided curriculum learning (DIHCL) procedure. Compared to existing CL methods: (1) DIH is more stable over time than using only instantaneous hardness, which is noisy due to stochastic training and DNN's non-smoothness; (2) DIHCL is computationally inexpensive since it uses only a byproduct of back-propagation and thus does not require extra inference. On 11 datasets, DIHCL significantly outperforms random mini-batch SGD and recent CL methods in terms of efficiency and final performance.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.patcog.2007.06.033,
author = {Bouchaffra, Djamel and Amira, Abbes},
title = {Structural hidden Markov models for biometrics: Fusion of face and fingerprint},
year = {2008},
issue_date = {March, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {3},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2007.06.033},
doi = {10.1016/j.patcog.2007.06.033},
abstract = {The goal of this paper is threefold: (i) propose a novel face and fingerprint feature modeling using the structural hidden Markov models (SHMMs) paradigm, (ii) explore the use of some feature extraction techniques such as ridgelet transform, discrete wavelet transform with various classifiers for biometric identification, and (iii) determine the best method for classifier combination. The experimental results reported in both fingerprint and face recognition reveal that the SHMMs concept is promising since it has outperformed several state-of-the-arts classifiers when combined with the discrete wavelet transform. Besides, this study has shown that the ridgelet transform without principal components analysis (PCA) dimension reduction fits better with the support vector machines (SVMs) classifier than it does with the SHMMs in the fingerprint recognition task. Finally, these results also reveal a small improvement of the bimodal biometric system over unimodal systems; which suggest that a most effective fusion scheme is necessary.},
journal = {Pattern Recogn.},
month = mar,
pages = {852–867},
numpages = {16},
keywords = {Support vector machines, Structural hidden Markov models, Ridgelet transform, Multimodal biometrics, Discrete wavelet transform, Classifier combination}
}

@article{10.1007/s11265-006-0009-6,
author = {Pal, Siddharth and Miller, David J.},
title = {An Extension of Iterative Scaling for Decision and Data Aggregation in Ensemble Classification},
year = {2007},
issue_date = {August    2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {1–2},
issn = {0922-5773},
url = {https://doi.org/10.1007/s11265-006-0009-6},
doi = {10.1007/s11265-006-0009-6},
abstract = {Improved iterative scaling (IIS) is an algorithm for learning maximum entropy (ME) joint and conditional probability models, consistent with specified constraints, that has found great utility in natural language processing and related applications. In most IIS work on classification, discrete-valued "feature functions" are considered, depending on the data observations and class label, with constraints measured based on frequency counts, taken over  hard  (0---1) training set instances. Here, we consider the case where the training (and test) set consist of instances of  probability mass functions  on the features, rather than hard feature values. IIS extends in a natural way for this case. This has applications (1) to ME classification on mixed discrete-continuous feature spaces and (2) to ME aggregation of soft classifier decisions in ensemble classification. Moreover, we combine these methods, yielding a method, with proved learning convergence, that jointly performs (soft) decision-level and feature-level fusion in making ensemble decisions. We demonstrate favorable comparisons against standard Adaboost.M1, input-dependent boosting, and other supervised combining methods, on data sets from the UC Irvine Machine Learning repository.},
journal = {J. VLSI Signal Process. Syst.},
month = aug,
pages = {21–37},
numpages = {17},
keywords = {mixed continuous-discrete feature spaces, maximum entropy, improved iterative scaling, ensemble classification}
}

@article{10.5555/1756006.1859931,
author = {Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
title = {Spectral Regularization Algorithms for Learning Large Incomplete Matrices},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efficient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efficiently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semidefinite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 106 X 106 incomplete matrix with 107 observed entries, and fits a rank-95 approximation to the full Netflix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {2287–2322},
numpages = {36}
}

@inproceedings{10.1007/11908029_95,
author = {Ichihashi, Hidetomo and Honda, Katsuhiro and Notsu, Akira},
title = {Postsupervised hard c-means classifier},
year = {2006},
isbn = {3540476938},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11908029_95},
doi = {10.1007/11908029_95},
abstract = {Miyamoto et al. derived a hard clustering algorithms by defuzzifying a generalized entropy-based fuzzy c-means in which covariance matrices are introduced as decision variables. We apply the hard c-means (HCM) clustering algorithms to a postsupervised classifier to improve resubstitution error rate by choosing best clustering results from local minima of an objective function. Due to the nature of the prototype based classifier, the error rates can easily be improved by increasing the number of clusters with the cost of computer memory and CPU speed. But, with the HCM classifier, the resubstitution error rate along with the data set compression ratio is improved on several benchmark data sets by using a small number of clusters for each class.},
booktitle = {Proceedings of the 5th International Conference on Rough Sets and Current Trends in Computing},
pages = {918–927},
numpages = {10},
location = {Kobe, Japan},
series = {RSCTC'06}
}

@inproceedings{10.1109/ISDA.2006.25,
author = {Dong, Baoli and Liu, Huimei},
title = {Enterprise Website Topic Modeling and Web Resource Search},
year = {2006},
isbn = {0769525288},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISDA.2006.25},
doi = {10.1109/ISDA.2006.25},
abstract = {Domain-specific web resource discovery is a key issue for resources sharing and reuse. Considering the text semantic information inside enterprise sites reflects the website structures and textual content feature, a novel website topic feature modeling method are presented in this paper. The website classification algorithm on the basis of the hybrid vector employs the effective kind of support vector machine (SVM). Manufacturing enterprise website search from the web is implemented to verify the classification performances. Moreover, the accuracy and efficiency of enterprise sites search are improved by revised search strategies. Finally, a prototype system of web enterprise resource service is introduced. It preliminarily demonstrates that this method is of good applicability for specific web resource discovery.},
booktitle = {Proceedings of the Sixth International Conference on Intelligent Systems Design and Applications - Volume 03},
pages = {56–61},
numpages = {6},
series = {ISDA '06}
}

@article{10.1007/s00500-015-2004-y,
author = {Qin, Jindong and Liu, Xinwang and Pedrycz, Witold},
title = {A multiple attribute interval type-2 fuzzy group decision making and its application to supplier selection with extended LINMAP method},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {12},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-2004-y},
doi = {10.1007/s00500-015-2004-y},
abstract = {Supplier selection is a key issue in supply chain management, which directly impacts the manufacturer's performance. The problem can be viewed as a multiple attribute group decision making (MAGDM) that concerns many conflicting evaluation attributes, both being of qualitative and quantitative nature. Due to the increasing complexity and uncertainty of socio-economic environment, some evaluations of attributes are not adequately represented by numerical assessments and type-1 fuzzy sets. In this paper, we develop some linear programming models with the aid of multidimensional analysis of preference (LINMAP) method to solve interval type-2 fuzzy MAGDM problems, in which the information about attribute weights is incompletely known, and all pairwise comparison judgments over alternatives are represented by IT2FSs. First, we introduce a new distance measure based on the centroid interval between the IT2FSs. Then, we construct the linear programming model to determine the interval type-2 fuzzy positive ideal solution (IT2PIS) and corresponding attributes weight vector. Based on it, an extended LINMAP method to solve MAGDM problem under IT2FSs environment is developed. Finally, a supplier selection example is provided to demonstrate the usefulness of the proposed method.},
journal = {Soft Comput.},
month = jun,
pages = {3207–3226},
numpages = {20},
keywords = {Supplier selection, Multiple attribute group decision making (MAGDM), Linear programming techniques for multidimensional analysis of preference (LINMAP) method, Interval type-2 fuzzy sets (IT2FSs)}
}

@article{10.1016/j.patcog.2011.12.019,
author = {Graves, Daniel and Noppen, Joost and Pedrycz, Witold},
title = {Clustering with proximity knowledge and relational knowledge},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {7},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.12.019},
doi = {10.1016/j.patcog.2011.12.019},
abstract = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes.},
journal = {Pattern Recogn.},
month = jul,
pages = {2633–2644},
numpages = {12},
keywords = {Software requirements, Relational clustering, Proximity, Knowledge representation, Fuzzy clustering}
}

@inproceedings{10.1145/3474198.3478170,
author = {Liu, Qi},
title = {Research on Person Re-identification Algorithm based on Attention and Pose Estimation},
year = {2022},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478170},
doi = {10.1145/3474198.3478170},
abstract = {Pedestrian re-identification (re-ID) refers to the technology of re identifying the currently photographed pedestrian in other visual fields in the non-overlapping monitoring field. However, in the re-ID dataset, the pose of pedestrians in different images varies greatly. Therefore, this paper designs a re-ID algorithm based on attention and pose estimation. The attention discrimination module in the algorithm can learn the weight importance of features in channel and spatial dimension, and apply it to the global feature vector to obtain the global feature vector with good robustness and strong discrimination. Another pose enhancement module can enhance the foreground features of the image based on the position of key points, and divide the local features of pedestrian components. The two network modules start from extracting global features and local features of image respectively, and jointly optimize to form a multi granularity end-to-end network model. In addition, our algorithm uses the pretrained ResNet50 model as backbone to learn deep features of images, and uses the Adam optimization algorithm to minimize the loss function. Experimental results on Market-1501 dataset and DukeMTMC-reID dataset show that our algorithm can achieve better retrieval performance.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {119},
numpages = {8},
keywords = {re-ID, pose estimation, loss function, key point, attention},
location = {Changsha, China},
series = {ICFEICT 2021}
}

@inproceedings{10.1007/978-3-642-34166-3_71,
author = {Hidaka, Akinori and Kurita, Takio},
title = {Sparse discriminant analysis based on the bayesian posterior probability obtained by L1 regression},
year = {2012},
isbn = {9783642341656},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34166-3_71},
doi = {10.1007/978-3-642-34166-3_71},
abstract = {Recently the kernel discriminant analysis (KDA) has been successfully applied in many applications. However, kernel functions are usually defined a priori and it is not known what the optimum kernel function for nonlinear discriminant analysis is. Otsu derived the optimum nonlinear discriminant analysis (ONDA) by assuming the underlying probabilities similar with the Bayesian decision theory. Kurita derived discriminant kernels function (DKF) as the optimum kernel functions in terms of the discriminant criterion by investigating the optimum discriminant mapping constructed by the ONDA. The derived kernel function is defined by using the Bayesian posterior probabilities. We can define a family of DKFs by changing the estimation method of the Bayesian posterior probabilities. In this paper, we propose a novel discriminant kernel function based on L1-regularized regression, called L1 DKF. L1 DKF is given by using the Bayesian posterior probabilities estimated by L1 regression. Since L1 regression yields a sparse representation for given samples, we can naturally introduce the sparseness into the discriminant kernel function. To introduce the sparseness into LDA, we use L1 DKF as the kernel function of LDA. In experiments, we show sparseness and classification performance of L1 DKF.},
booktitle = {Proceedings of the 2012 Joint IAPR International Conference on Structural, Syntactic, and Statistical Pattern Recognition},
pages = {648–656},
numpages = {9},
location = {Hiroshima, Japan},
series = {SSPR'12/SPR'12}
}

@article{10.1016/j.jvcir.2020.102923,
author = {Cui, Ran and Hua, Gang and Wu, Jingran},
title = {AP-GAN: Predicting skeletal activity to improve early activity recognition},
year = {2020},
issue_date = {Nov 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {73},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2020.102923},
doi = {10.1016/j.jvcir.2020.102923},
journal = {J. Vis. Comun. Image Represent.},
month = nov,
numpages = {9},
keywords = {41A05, 41A10, 65D05, 65D17, Early activity recognition, Activity prediction, Skeleton, GAN}
}

@article{10.1504/ijsnet.2021.113842,
author = {Cao, Ning and Huo, Wei and Lin, Taihe and Wu, Gangshan},
title = {Application of convolutional neural networks and image processing algorithms based on traffic video in vehicle taillight detection},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {35},
number = {3},
issn = {1748-1279},
url = {https://doi.org/10.1504/ijsnet.2021.113842},
doi = {10.1504/ijsnet.2021.113842},
abstract = {In the process of assisted driving, accurately understanding the linguistic information transmitted by surrounding vehicles is the prerequisite for making correct driving decisions. In this paper, the neural network was partly used for vehicle detection. The recognition of the front vehicle taillights is based on the taillight recognition mechanism and image processing technology. The taillights are then positioned by using their colour and shape characteristics. The red-green-blue (RGB) and cyan-magenta-yellow (CMY) colour spaces were used to establish a taillight recognition mechanism to detect the taillight status of the front car, so as to understand the driving intention of the front car. The experimental results show that the method can accurately identify the state of the front taillights during the day.},
journal = {Int. J. Sen. Netw.},
month = jan,
pages = {181–192},
numpages = {11},
keywords = {vehicle state judgement, vehicle lights matching, faster RCNN, signal recognition, taillight detection, vehicle detection, driving assistance}
}

@inproceedings{10.1007/978-3-030-89363-7_28,
author = {Dai, Huan and Zhang, Yupei and Yun, Yue and Shang, Xuequn},
title = {An Improved Deep Model for Knowledge Tracing and Question-Difficulty Discovery},
year = {2021},
isbn = {978-3-030-89362-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89363-7_28},
doi = {10.1007/978-3-030-89363-7_28},
abstract = {Knowledge Tracing (KT) aims to analyze a student’s acquisition of skills over time by examining the student’s performance on questions of those skills. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, DKT and its variants often lead to oscillation results on a skill’s state may due to it ignoring the skill’s difficulty or the question’s difficulty. As a result, even when a student performs well on a skill, the prediction of that skill’s mastery level decreases instead, and vice versa. This is undesirable and unreasonable because student’s performance is expected to transit gradually over time. In this paper, we propose to learn the knowledge tracing model in a “simple-to-difficult” process, leading to a method of Self-paced Deep Knowledge Tracing (SPDKT). SPDKT learns the difficulty of per question from the student’s responses to optimize the question’s order and smooth the learning process. With mitigating the cause of oscillations, SPDKT has the capability of robustness to the puzzling questions. The experiments on real-world datasets show SPDKT achieves state-of-the-art performance on question response prediction and reaches interesting interpretations in education.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part II},
pages = {362–375},
numpages = {14},
keywords = {Personalized education, Deep learning, Self-paced learning, Knowledge tracing},
location = {Hanoi, Vietnam}
}

@article{10.1155/2021/5349916,
author = {Luo, Weiwei and Su, Jian},
title = {Analysis of Artistic Modeling of Opera Stage Clothing Based on Big Data Clustering Algorithm},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5349916},
doi = {10.1155/2021/5349916},
abstract = {In order to deal with the problem that the traditional stage costume artistry analysis method cannot correct the results of big data clustering, which leads to deviations in the extraction of costume artistry features, this paper proposes a clothing artistic modeling method based on big data clustering algorithm. The proposed method provides a database for big data clustering by constructing the attribute set of the big data feature sequence training set and, at the same time, constructing a second-order cone programming model to correct the big data. Aiming at the problem that traditional stage costume art analysis methods cannot correct the clustering results of big data. On this basis, the costume elements of the opera stage are segmented, initialized, and transformed into a binary function. Finally, using the convolutional neural network, combining the element segmentation results and the large data clustering space state vector, a feature extraction model of stage costume art is constructed. Experimental results show that the model has good convergence, short time-consuming, high accuracy, and ideal feature recognition capabilities.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {9}
}

@article{10.1016/j.knosys.2013.01.018,
author = {Fern\'{a}Ndez, Alberto and L\'{o}Pez, Victoria and Galar, Mikel and Del Jesus, Mar\'{\i}A Jos\'{e} and Herrera, Francisco},
title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.01.018},
doi = {10.1016/j.knosys.2013.01.018},
abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository.},
journal = {Know.-Based Syst.},
month = apr,
pages = {97–110},
numpages = {14},
keywords = {Preprocessing, Pairwise learning, Multi-classification, Imbalanced data-sets, Cost-sensitive learning}
}

@article{10.1016/j.comnet.2006.08.003,
author = {Weiss, M. and Esfandiari, B. and Luo, Y.},
title = {Towards a classification of web service feature interactions},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {51},
number = {2},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2006.08.003},
doi = {10.1016/j.comnet.2006.08.003},
abstract = {The rapid introduction of new web services into a dynamic business environment can lead to undesirable interactions that negatively affect service quality and user satisfaction. In previous work, we have demonstrated how such interactions between web services can be modeled as feature interactions. In this paper, we outline a classification of web service feature interactions. The goals of this classification are to understand the scope of the feature interaction problem in the web services domain, and to propose a benchmark against which to assess the coverage of solutions to this problem. As there is no standard set of web services that one could use as examples, we illustrate the interactions using a fictitious e-commerce scenario.},
journal = {Comput. Netw.},
month = feb,
pages = {359–381},
numpages = {23},
keywords = {Web services, Feature interaction, Classification}
}

@inproceedings{10.1145/3241403.3241426,
author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
title = {Model-based support for decision-making in architecture evolution of complex software systems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241426},
doi = {10.1145/3241403.3241426},
abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {21},
numpages = {7},
keywords = {systems-of-systems composition, software variability management, software migration, software architecture evolution, model-based decision support},
location = {Madrid, Spain},
series = {ECSA '18}
}

@article{10.1016/j.artmed.2021.102110,
author = {Hu, Yang and Wen, Guihua and Luo, Mingnan and Yang, Pei and Dai, Dan and Yu, Zhiwen and Wang, Changjun and Hall, Wendy},
title = {Fully-channel regional attention network for disease-location recognition with tongue images},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {118},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102110},
doi = {10.1016/j.artmed.2021.102110},
journal = {Artif. Intell. Med.},
month = aug,
numpages = {13},
keywords = {Attention mechanism, Regional detailed features, Convolutional networks, Disease-position recognition, Tongue image modeling}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jul,
articleno = {15},
numpages = {26},
keywords = {policy distillation, deep reinforcement learning, autonomous driving, Transfer reinforcement learning}
}

@article{10.4018/IJSI.2021070105,
author = {Jo, Jun-Hyuk and Lee, Jihyun and Jaffari, Aman and Kim, Eunmi},
title = {Fault Localization With Data Flow Information and an Artificial Neural Network},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {3},
issn = {2166-7160},
url = {https://doi.org/10.4018/IJSI.2021070105},
doi = {10.4018/IJSI.2021070105},
abstract = {Fault localization is a technique for identifying the exact source code line with faults. It typically requires a lot of time and cost because, to locate the fault, a developer must track the execution of the failed program line by line. To reduce the fault localization efforts, many methods have been proposed. However, their localized suspicious code range is wide, and their fault localization effect is not high. To cope with this limitation, this paper computes the degree of fault suspiciousness of statements by using an artificial neural network and information of the executed test case, such as statement coverage, execution result, and definition-use pair. Compared to the approach that uses only statement coverage as input data for training an artificial neural network, the experiment results show higher accuracy in 15 types of faults out of 29 real fault types in the approach that the definition-use pair included.},
journal = {Int. J. Softw. Innov.},
month = jul,
pages = {66–78},
numpages = {13},
keywords = {Software Verification, Software Testing, Fault Suspiciousness, Fault Localization, Du-Pair, Definition-Use, Data Flow Coverage, Artificial Neural Network}
}

@article{10.1007/s10618-010-0175-9,
author = {Silla, Carlos N. and Freitas, Alex A.},
title = {A survey of hierarchical classification across different application domains},
year = {2011},
issue_date = {January   2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1–2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-010-0175-9},
doi = {10.1007/s10618-010-0175-9},
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
journal = {Data Min. Knowl. Discov.},
month = jan,
pages = {31–72},
numpages = {42},
keywords = {Tree-structured class hierarchies, Hierarchical classification, DAG-structured class hierarchies}
}

@article{10.1016/j.engappai.2018.06.010,
author = {Chin, Cheng Siong and Ji, Xi},
title = {Adaptive online sequential extreme learning machine for frequency-dependent noise data on offshore oil rig},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.010},
doi = {10.1016/j.engappai.2018.06.010},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {226–241},
numpages = {16},
keywords = {Root mean square error, Training time, Noise prediction, Oil-rig, Extreme learning machine, Multiple frequency dependent data}
}

@article{10.1109/TASLP.2021.3104165,
author = {Hono, Yukiya and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
title = {Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3104165},
doi = {10.1109/TASLP.2021.3104165},
abstract = {This paper presents Sinsy, a deep neural network (DNN)-based singing voice synthesis (SVS) system. In recent years, DNNs have been utilized in statistical parametric SVS systems, and DNN-based SVS systems have demonstrated better performance than conventional hidden Markov model-based ones. SVS systems are required to synthesize a singing voice with pitch and timing that strictly follow a given musical score. Additionally, singing expressions that are not described on the musical score, such as vibrato and timing fluctuations, should be reproduced. The proposed system is composed of four modules: a time-lag model, a duration model, an acoustic model, and a vocoder, and singing voices can be synthesized taking these characteristics of singing voices into account. To better model a singing voice, the proposed system incorporates improved approaches to modeling pitch and vibrato and better training criteria into the acoustic model. In addition, we incorporated PeriodNet, a non-autoregressive neural vocoder with robustness for the pitch, into our systems to generate a high-fidelity singing voice waveform. Moreover, we propose automatic pitch correction techniques for DNN-based SVS to synthesize singing voices with correct pitch even if the training data has out-of-tune phrases. Experimental results show our system can synthesize a singing voice with better timing, more natural vibrato, and correct pitch, and it can achieve better mean opinion scores in subjective evaluation tests.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2803–2815},
numpages = {13}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Decision-making, Cloud service selection, Cloud computing}
}

@article{10.5555/2946645.3007037,
author = {Wei, Ermo and Luke, Sean},
title = {Lenient learning in independent-learner stochastic cooperative games},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional ("Distributed") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2914–2955},
numpages = {42},
keywords = {reinforcement learning, multiagent learning, lenient learning, independent learner, game theory}
}

@inproceedings{10.5555/2969033.2969059,
author = {Jiang, Lu and Meng, Deyu and Yu, Shoou-I and Lan, Zhenzhong and Shan, Shiguang and Hauptmann, Alexander G.},
title = {Self-paced learning with diversity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2078–2086},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{10.1109/TASLP.2014.2359628,
author = {Chen, Austin and Hasegawa-Johnson, Mark A.},
title = {Mixed stereo audio classification using a stereo-input mixed-to-panned level feature},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359628},
doi = {10.1109/TASLP.2014.2359628},
abstract = {Many past studies have been conducted on speech/music discrimination due to the potential applications for broadcast and other media; however, it remains possible to expand the experimental scope to include samples of speech with varying amounts of background music. This paper focuses on the development and evaluation of two measures of the ratio between speech energy and music energy: a reference measure called speech-to-music ratio (SMR), which is known objectively only prior to mixing, and a feature called the stereo-input mix-to-peripheral level feature (SIMPL), which is computed from the stereo mixed signal as an imprecise estimate of SMR. SIMPL is an objective signal measure calculated by taking advantage of broadcast mixing techniques in which vocals are typically placed at stereo center, unlike most instruments. Conversely, SMR is a hidden variable defined by the relationship between the powers of portions of audio attributed to speech and music. It is shown that SIMPL is predictive of SMR and can be combined with state-of-the-art features in order to improve performance. For evaluation, this new metric is applied in speech/music (binary) classification, speech/music/mixed (trinary) classification, and a new speech-to-music ratio estimation problem. Promising results are achieved, including 93.06% accuracy for trinary classification and 3.86 dB RMSE for estimation of the SMR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2025–2033},
numpages = {9},
keywords = {speech/music discrimination, speech processing, music processing, music information retrieval, mel-frequency cepstral coefficients, classification algorithms, audio segmentation, audio processing, audio classification, Gaussian mixture model}
}

@article{10.1016/j.cviu.2021.103255,
author = {Landi, Federico and Baraldi, Lorenzo and Cornia, Marcella and Corsini, Massimiliano and Cucchiara, Rita},
title = {Multimodal attention networks for low-level vision-and-language navigation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {210},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103255},
doi = {10.1016/j.cviu.2021.103255},
journal = {Comput. Vis. Image Underst.},
month = sep,
numpages = {9},
keywords = {Multi-modal attention, Embodied AI, Vision-and-language navigation, 68T45, 68T40, 68T01}
}

@inproceedings{10.5555/1776814.1776823,
author = {Lappas, Georgios},
title = {Estimating the size of neural networks from the number of available training data},
year = {2007},
isbn = {3540746897},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Estimating a priori the size of neural networks for achieving high classification accuracy is a hard problem. Existing studies provide theoretical upper bounds on the size of neural networks that are unrealistic to implement. This work provides a computational study for estimating the size of neural networks using as an estimation parameter the size of available training data. We will also show that the size of a neural network is problem dependent and that one only needs the number of available training data to determine the size of the required network for achieving high classification rate. We use for our experiments a threshold neural network that combines the perceptron algorithm with simulated annealing and we tested our results on datasets from the UCI Machine Learning Repository. Based on our experimental results, we propose a formula to estimate the number of perceptrons that have to be trained in order to achieve a high classification accuracy.},
booktitle = {Proceedings of the 17th International Conference on Artificial Neural Networks},
pages = {68–77},
numpages = {10},
location = {Porto, Portugal},
series = {ICANN'07}
}

@article{10.1007/s11042-021-10907-y,
author = {Almezhghwi, Khaled and Serte, Sertan and Al-Turjman, Fadi},
title = {Convolutional neural networks for the classification of chest X-rays in the IoT era},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {19},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10907-y},
doi = {10.1007/s11042-021-10907-y},
abstract = {Chest X-ray medical imaging technology allows the diagnosis of many lung diseases. It is known that this technology is frequently used in hospitals, and it is the most accurate way of detecting most thorax diseases. Radiologists examine these images to identify lung diseases; however, this process can require some time. In contrast, an automated artificial intelligence system could help radiologists detect lung diseases more accurately and faster. Therefore, we propose two artificial intelligence approaches for processing and identifying chest X-ray images to detect chest diseases from such images. We introduce two novel deep learning methods for fast and automated classification of chest X-ray images. First, we propose the use of support vector machines based on the AlexNet model. Second, we develop support vector machines based on the VGGNet16 method. Combined deep networks with a robust classifier have shown that the proposed methods outperform AlexNet and VGG16 deep learning approaches for the chest X-ray image classification tasks. The proposed AlexNet and VGGNet based SVM provide average area under the curve values of 98% and 97%, respectively, for twelve chest X-ray diseases.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {29051–29065},
numpages = {15},
keywords = {Convolutional neural networks, Deep learning}
}

@article{10.1155/2021/9916915,
author = {Ng, Anfernee Joan B. and Liu, Kun-Hong and Pe\~{n}a, Antonio J.},
title = {The Investigation of Different Loss Functions with Capsule Networks for Speech Emotion Recognition},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/9916915},
doi = {10.1155/2021/9916915},
abstract = {Speech emotion recognition (SER) is an important research topic. Image features like spectrograms are one of the common ways of extracting information from speech. In the area of image recognition, a relatively novel type of network called capsule networks has shown good and promising results. This study aims to use capsule networks to encode spatial information from spectrograms and analyse its performance when paired with different loss functions. Experiments comparing the capsule network with models from previous works show that the capsule network performs better than them.},
journal = {Sci. Program.},
month = jan,
numpages = {12}
}

@inproceedings{10.1007/978-3-642-40270-8_4,
author = {Bari, A. T. and Reaz, Mst. Rokeya and Choi, Ho-Jin and Jeong, Byeong-Soo},
title = {DNA Encoding for Splice Site Prediction in Large DNA Sequence},
year = {2013},
isbn = {9783642402692},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40270-8_4},
doi = {10.1007/978-3-642-40270-8_4},
abstract = {Splice site prediction in the pre-mRNA is a very important task for understanding gene structure and its function. To predict splice sites, SVM support vector machine based classification technique is frequently used because of its classification accuracy. High classification accuracy of SVM largely depends on DNA encoding method for feature extraction of DNA sequences. However, existing encoding approaches do not reveal the characteristics of DNA sequence very well enough to provide as much information as DNA sequences have. In this paper, we propose new effective DNA encoding method which can give more information of DNA sequence. Our encoding method can provide density information of each nucleotide along with positional information and chemical property. Extensive performance study shows that our method can provide better performance than existing encoding methods based on several performance criteria such as classification accuracy, sensitivity, specificity and area under receiver operating characteristics curve ROC.},
booktitle = {Proceedings of the 18th International Conference on Database Systems for Advanced Applications - Volume 7827},
pages = {46–58},
numpages = {13},
keywords = {support vector machine, splice site, orthogonal encoding, nucleotide density, gene prediction, ROC, DNA sequence}
}

@inproceedings{10.1007/978-3-030-93046-2_7,
author = {Guo, Jingwen and Lu, Zhisheng and Wang, Ti and Huang, Weibo and Liu, Hong},
title = {Object Goal Visual Navigation Using Semantic Spatial Relationships},
year = {2021},
isbn = {978-3-030-93045-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93046-2_7},
doi = {10.1007/978-3-030-93046-2_7},
abstract = {The target-driven visual navigation is a popular learning-based method and has been successfully applied to a wide range of applications. However, it has some disadvantages, including being ineffective at adapting to unseen environments. In this paper, a navigation method based on Semantic Spatial Relationships (SSR) is proposed and is shown to have more reliable performance when dealing with novel conditions. The construction of joint semantic hierarchical feature vector allows for learning implicit relationship between current observation and target objects, which benefits from construction of prior knowledge graph and semantic space. This differs from the traditional target driven methods, which integrate the visual input vector directly into the reinforcement learning path planning module. Moreover, the proposed method takes both local and global features of observed image into consideration and is thus less conservative and more robust in regards to random scenes. An additional analysis indicates that the proposed SSR performs well on classical metrics. The effectiveness of the proposed SSR model is demonstrated comparing with state-of-the-art methods in unknown scenes.},
booktitle = {Artificial Intelligence: First CAAI International Conference, CICAI 2021, Hangzhou, China, June 5–6, 2021, Proceedings, Part I},
pages = {77–88},
numpages = {12},
keywords = {Hierarchical relationship, Semantic graph, Visual navigation},
location = {Hangzhou, China}
}

@article{10.1016/j.neucom.2019.12.119,
author = {Ji, Luping and Chang, Mingzhe and Shen, Yulin and Zhang, Qian},
title = {Recurrent convolutions of binary-constraint Cellular Neural Network for texture recognition},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {387},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.12.119},
doi = {10.1016/j.neucom.2019.12.119},
journal = {Neurocomput.},
month = apr,
pages = {161–171},
numpages = {11},
keywords = {Fully-connected classifier, Texture recognition, Local binary pattern, Recurrent convolution, Cellular Neural Network}
}

@inproceedings{10.1109/CEC-EEE.2006.2,
author = {Fantinato, Marcelo and de Toledo, Maria Beatriz F. and Gimenes, Itana Maria de S.},
title = {A Feature-based Approach to Electronic Contracts},
year = {2006},
isbn = {0769525113},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CEC-EEE.2006.2},
doi = {10.1109/CEC-EEE.2006.2},
abstract = {E-contracts are used to describe the supply and the consumption details of e-services within a business process. The establishment of e-contracts in a given application domain usually involves a set of welldefined common and variation points. This paper proposes a feature-based approach in order to decrease the complexity in e-contract establishment and to foster inter-organizational cooperation. Feature modeling is a technique that has been widely used for capturing and managing commonalities and variabilities of product families in the software product line context. The feasibility of the approach is shown by a case study carried out within the telecom context and based on experimental software engineering concepts.},
booktitle = {Proceedings of the The 8th IEEE International Conference on E-Commerce Technology and The 3rd IEEE International Conference on Enterprise Computing, E-Commerce, and E-Services},
pages = {34},
series = {CEC-EEE '06}
}

@inproceedings{10.1145/1963564.1963580,
author = {Srihari, V. and Karthik, R. and Anitha, R. and Suganthi, S. D.},
title = {Speaker verification using combinational features and adaptive neuro-fuzzy inference systems},
year = {2011},
isbn = {9781450304085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963564.1963580},
doi = {10.1145/1963564.1963580},
abstract = {A new efficacious Speaker Verification System is proposed in this paper. Scrutinized study is made on different features and finally a combination of them is used. These combinational features have been modeled with ANFIS and SVM classifier. The performance of both the systems are evaluated with detection error trade-off curves and Bayes Risk function. Results have shown that proposed system using combinational features with ANFIS is more efficient compared to combinational features with SVM classifier.},
booktitle = {Proceedings of the First International Conference on Intelligent Interactive Technologies and Multimedia},
pages = {98–103},
numpages = {6},
keywords = {support vector machine, speaker verification system, detection error trade-off Curves, combinational features, adaptive neuro fuzzy inference system, Bayes risk function},
location = {Allahabad, India},
series = {IITM '10}
}

@article{10.1016/j.csl.2012.11.003,
author = {Vlasenko, Bogdan and Prylipko, Dmytro and B\"{o}ck, Ronald and Wendemuth, Andreas},
title = {Modeling phonetic pattern variability in favor of the creation of robust emotion classifiers for real-life applications},
year = {2014},
issue_date = {March, 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {28},
number = {2},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.11.003},
doi = {10.1016/j.csl.2012.11.003},
abstract = {The role of automatic emotion recognition from speech is growing continuously because of the accepted importance of reacting to the emotional state of the user in human-computer interaction. Most state-of-the-art emotion recognition methods are based on turn- and frame-level analysis independent from phonetic transcription. Here, we are interested in a phoneme-based classification of the level of arousal in acted and spontaneous emotions. To start, we show that our previously published classification technique which showed high-level results in the Interspeech 2009 Emotion Challenge cannot provide sufficiently good classification in cross-corpora evaluation (a condition close to real-life applications). To prove the robustness of our emotion classification techniques we use cross-corpora evaluation for a simplified two-class problem; namely high and low arousal emotions. We use emotion classes on a phoneme-level for classification. We build our speaker-independent emotion classifier with HMMs, using GMMs-based production probabilities and MFCC features. This classifier performs equally well when using a complete phoneme set, as it does in the case of a reduced set of indicative vowels (7 out of 39 phonemes in the German SAM-PA list). Afterwards we compare emotion classification performance of the technique used in the Emotion Challenge with phoneme-based classification within the same experimental setup. With phoneme-level emotion classes we increase cross-corpora classification performance by about 3.15% absolute (4.69% relative) for models trained on acted emotions (EMO-DB dataset) and evaluated on spontaneous emotions (VAM dataset); within vice versa experimental conditions (trained on VAM, tested on EMO-DB) we obtain 15.43% absolute (23.20% relative) improvement. We show that using phoneme-level emotion classes can improve classification performance even with comparably low speech recognition performance obtained with scant a priori knowledge about the language, implemented as a zero-gram for word-level modeling and a bi-gram for phoneme-level modeling. Finally we compare our results with the state-of-the-art cross-corpora evaluations on the VAM database. For training our models, we use an almost 15 times smaller training set, consisting of 456 utterances (210 low and 246 high arousal emotions) instead of 6820 utterances (4685 high and 2135 low arousal emotions). We are yet able to increase cross-corpora classification performance by about 2.25% absolute (3.22% relative) from UA=69.7% obtained by Zhang et al. to UA=71.95%.},
journal = {Comput. Speech Lang.},
month = mar,
pages = {483–500},
numpages = {18},
keywords = {Level of arousal classification, Emotion phonetic pattern, Emotion phoneme classes, Emotion classification, Cross-corpora evaluation, Affective speech}
}

@article{10.1016/j.cose.2019.03.015,
author = {Luh, Robert and Janicke, Helge and Schrittwieser, Sebastian},
title = {AIDIS: Detecting and classifying anomalous behavior in ubiquitous kernel processes},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {84},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2019.03.015},
doi = {10.1016/j.cose.2019.03.015},
journal = {Comput. Secur.},
month = jul,
pages = {120–147},
numpages = {28},
keywords = {SVM, Classification, Machine learning, Semantic gap, Security model, Star structure, Graph matching, Anomaly detection, Malware, Intrusion detection}
}

@inproceedings{10.1145/3164541.3164633,
author = {Kusumure, Shogo and Ushiama, Taketoshi},
title = {Dynamic Optimization for Web Page Based on User's Estimated Browsing Intention},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164633},
doi = {10.1145/3164541.3164633},
abstract = {When a web designer designs a web page, they decide the order and layout of the elements for the user. Multiple users who use the same web page have varying purposes. The order and layout decided by the designer is not necessarily effective for all users' intentions. The purpose of this study is to estimate the mental model of the user visiting a web page and to change the functionality of the web browser to optimize the order and layout of the web components in real time, based on the estimation result. In this paper, we propose a feature-modeling of the page elements (the basis for realizing this function) and an optimization method using a simple mental model.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {84},
numpages = {6},
keywords = {responsive web design, interface, content optimization, Browsing intention},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@inproceedings{10.1007/978-3-030-97774-0_22,
author = {Wei, Xiaohui and Guo, Tao and Yu, Hongmei and Li, Zijian and Guo, Hao and Li, Xiang},
title = {AreaTransfer: A Cross-City Crowd Flow Prediction Framework Based on&nbsp;Transfer Learning},
year = {2022},
isbn = {978-3-030-97773-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-97774-0_22},
doi = {10.1007/978-3-030-97774-0_22},
abstract = {Urban transfer learning transfers knowledge from the data-rich city to the data-scarce city, effectively solving the cold-start crowd flow prediction problem. In urban transfer learning, the selection of source cities mainly focuses on the experimental evaluation, which lacks methods for assessing the transferability of source cities. Besides, the complex regional matching relationships between source-target cities have not been fully addressed. To resolve these challenges, we propose a cross-city crowd flow prediction framework based on transfer learning, called AreaTransfer. AreaTransfer aims to select the appropriate source city from multi-source candidate cities and establish effective area matching relationships to improve crowd flow prediction accuracy. First, we design a source city selection algorithm based on the city’s layout characteristics to select the final source city. Then, we propose a modified deep residual neural network to allow area-level prediction. Finally, we optimize the pre-trained model by integrating the area matching results during the city selection process. Experimental results exhibit that AreaTransfer can improve the prediction accuracy by 15%–17% compared with other state-of-the-art models.},
booktitle = {Smart Computing and Communication: 6th International Conference, SmartCom 2021, New York City, NY, USA, December 29–31, 2021, Proceedings},
pages = {238–253},
numpages = {16},
keywords = {Source city selection, Crowd flow prediction, Cold-start problem, Urban transfer learning},
location = {New York, NY, USA}
}

@inproceedings{10.1145/3178876.3186145,
author = {Cheng, Zhiyong and Ding, Ying and Zhu, Lei and Kankanhalli, Mohan},
title = {Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186145},
doi = {10.1145/3178876.3186145},
abstract = {Although latent factor models (e.g., matrix factorization) achieve good accuracy in rating prediction, they suffer from several problems including cold-start, non-transparency, and suboptimal recommendation for local users or items. In this paper, we employ textual review information with ratings to tackle these limitations. Firstly, we apply a proposed aspect-aware topic model (ATM) on the review text to model user preferences and item features from different aspects, and estimate the aspect importance of a user towards an item. The aspect importance is then integrated into a novel aspect-aware latent factor model (ALFM), which learns user's and item's latent factors based on ratings. In particular, ALFM introduces a weighted matrix to associate those latent factors with the same set of aspects discovered by ATM, such that the latent factors could be used to estimate aspect ratings. Finally, the overall rating is computed via a linear combination of the aspect ratings, which are weighted by the corresponding aspect importance. To this end, our model could alleviate the data sparsity problem and gain good interpretability for recommendation. Besides, an aspect rating is weighted by an aspect importance, which is dependent on the targeted user's preferences and targeted item's features. Therefore, it is expected that the proposed method can model a user's preferences on an item more accurately for each user-item pair locally. Comprehensive experimental studies have been conducted on 19 datasets from Amazon and Yelp 2017 Challenge dataset. Results show that our method achieves significant improvement compared with strong baseline methods, especially for users with only few ratings. Moreover, our model could interpret the recommendation results in depth.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {639–648},
numpages = {10},
keywords = {topic model, review-aware, recommendation, matrix factorization, aspect-aware},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.5555/1887003.1887056,
author = {Fu, Zhouyu and Lu, Guojun and Ting, Kai-Ming and Zhang, Dengsheng},
title = {On feature combination for music classification},
year = {2010},
isbn = {3642149790},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We address the problem of combining different types of audio features for music classification. Several feature-level and decision-level combination methods have been studied, including kernel methods based on multiple kernel learning, decision level fusion rules and stacked generalization. Eight widely used audio features were examined in the experiments on multi-feature based music classification. Results on benchmark data set have demonstrated the effectiveness of using multiple types of features for music classification and identified the most effective combination method for improving classification performance.},
booktitle = {Proceedings of the 2010 Joint IAPR International Conference on Structural, Syntactic, and Statistical Pattern Recognition},
pages = {453–462},
numpages = {10},
location = {Cesme, Izmir, Turkey},
series = {SSPR&amp;SPR'10}
}

@article{10.1007/s11042-021-10529-4,
author = {Nahta, Ravi and Meena, Yogesh Kumar and Gopalani, Dinesh and Chauhan, Ganpat Singh},
title = {Embedding metadata using deep collaborative filtering to address the cold start problem for the rating prediction task},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10529-4},
doi = {10.1007/s11042-021-10529-4},
abstract = {In recent years, deep learning has yielded success in many research fields including machine translation, natural language processing, computer vision, and social network filtering. The area of deep learning in the recommender system is flourishing. Previous research has relied on incorporating metadata information in various application domains using deep learning techniques to achieve better recommendation accuracy. The use of metadata is desirable to address the cold start problem and better learning the user-item interaction, which is not captured by the user-item rating matrix. Existing methods rely on fixed user-item latent representation and ignore the metadata information. It restricts the model performance to correctly identify actual latent vectors, which results in high rating prediction error. To tackle these problems, we propose a generalized recommendation model named Meta Embedding Deep Collaborative Filtering (MEDCF), which inputs user demographics and item genre as metadata features together with the rating matrix. The proposed framework primarily comprises of Generalized Matrix Factorization (GMF), Multilayer Perceptron (MLP), and Neural Matrix Factorization (NeuMF) methods. GMF is applied to the rating matrix, whereas MLP is applied to metadata. Using NeuMF, the outputs for GMF and MLP are then concatenated and input to a neural network for rating prediction. To prove the effectiveness of proposed model, two metrics are used, Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). The MEDCF model is experimented on MovieLens and Amazon Movies datasets showing a significant improvement over the baseline methods.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {18553–18581},
numpages = {29},
keywords = {Metadata, Matrix factorization, Neural networks, Cold start problem, Collaborative filtering, Recommender systems}
}

@article{10.1016/j.eswa.2016.07.006,
author = {Elia, Valerio and Gnoni, Maria Grazia and Lanzilotto, Alessandra},
title = {Evaluating the application of augmented reality devices in manufacturing from a process point of view},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.07.006},
doi = {10.1016/j.eswa.2016.07.006},
abstract = {Augmented Reality (AR) systems could improve performance of manufacturing processes.AR systems are characterized by high technological complexity.Their effective application in the manufacturing requires methods integrating technological and process based metrics.A multi-criteria model is proposed to evaluate the feasibility of AR system application. Augmented Reality (AR) systems in last few years show great potentialities in the manufacturing context: recent pilot projects were developed for supporting quicker product and process design, as well as control and maintenance activities. The high technological complexity together with the wide variety of AR devices require a high technological skill; on the other hand, evaluating their actual impacts on the manufacturing process is still an open question. Few recent studies have analysed this topic by using qualitative approaches based on an "ex post" analysis - i.e. after the design and/or the adoption of the AR system - for evaluating the effectiveness of a developed AR application. The paper proposes an expert based tool for supporting production managers and researchers in effectively evaluating a preliminary ex-ante feasibility analysis for assessing quantitatively most efficient single AR devices (or combinations) to be applied in specific manufacturing processes. A multi-criteria model based on Analytic Hierarchy Process (AHP) method has been proposed to provide decision makers with quantitative knowledge for more efficiently designing AR applications in manufacturing. The model allows to integrate, in the same decision support tool, technical knowledge regarding AR devices with critical process features characterizing manufacturing processes, thus allowing to assess the contribution of the AR device in a wider prospective compared to current technological analyses. A test case study about the evaluation of AR system in on-site maintenance service is also discussed aiming to validate the model, and to outline its global applicability and potentialities. Obtained results highlighted the full efficacy of the proposed model in supporting ex-ante feasibility studies.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {187–197},
numpages = {11},
keywords = {Multi-criteria, Manufacturing process, Augmented Reality, AHP}
}

@article{10.1016/j.cviu.2015.11.001,
author = {Ribeiro, Pedro Canotilho and Audigier, Romaric and Pham, Quoc Cuong},
title = {RIMOC, a feature to discriminate unstructured motions},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2015.11.001},
doi = {10.1016/j.cviu.2015.11.001},
abstract = {A novel and compact feature discriminating structuredness of observed motions.A feature embedded in a weakly supervised learning framework.An efficient method for real-time violence detection in on-board video-surveillance.Ability of the learned model to generalize training data for varied contexts.A new dataset representative of the targeted application for extensive evaluation. In video-surveillance, violent event detection is of utmost interest. Although action recognition has been well studied in computer vision, literature for violence detection in video is far sparser, and even more for surveillance applications. As aggressive events are difficult to define due to their variability and often need high-level interpretation, we decided to first try to characterize what is frequently present in video with violent human behaviors, at a low level: jerky and unstructured motion. Thus, a novel problem-specific Rotation-Invariant feature modeling MOtion Coherence (RIMOC) was proposed, in order to capture its structure and discriminate the unstructured motions. It is based on the eigenvalues obtained from the second-order statistics of the Histograms of Optical Flow vectors from consecutive temporal instants, locally and densely computed, and further embedded into a spheric Riemannian manifold.The proposed RIMOC feature is used to learn statistical models of normal coherent motions in a weakly supervised manner. A multi-scale scheme applied on an inference-based method allows the events with erratic motion to be detected in space and time, as good candidates of aggressive events.We experimentally show that the proposed method produces results comparable to a state-of-the-art supervised approach, with added simplicity in training and computation. Thanks to the compactness of the feature, real-time computation is achieved in learning as well as in detection phase. Extensive experimental tests on more than 18 h of video are provided in different in-lab and real contexts, such as railway cars equipped with on-board cameras.},
journal = {Comput. Vis. Image Underst.},
month = mar,
pages = {121–143},
numpages = {23},
keywords = {Violent event detection, Violence detection, Video-surveillance, Unstructured motion, Aggression detection, Abnormality detection}
}

@inproceedings{10.1145/2911451.2914709,
author = {Roegiest, Adam and Cormack, Gordon V.},
title = {Impact of Review-Set Selection on Human Assessment for Text Classification},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914709},
doi = {10.1145/2911451.2914709},
abstract = {In a laboratory study, human assessors were significantly more likely to judge the same documents as relevant when they were presented for assessment within the context of documents selected using random or uncertainty sampling, as compared to relevance sampling. The effect is substantial and significant [0.54 vs. 0.42, p&lt;0.0002] across a population of documents including both relevant and non-relevant documents, for several definitions of ground truth. This result is in accord with Smucker and Jethani's SIGIR 2010 finding that documents were more likely to be judged relevant when assessed within low-precision versus high-precision ranked lists. Our study supports the notion that relevance is malleable, and that one should take care in assuming any labeling to be ground truth, whether for training, tuning, or evaluating text classifiers.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {861–864},
numpages = {4},
keywords = {assessor error, ediscovery, electronic discovery, evaluation, recall, supervised learning, user study},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1145/3404687.3404690,
author = {Fang, Zhou and Ma, Chao and Qu, Jiaxing and Song, Xue and Zhang, Chi},
title = {A Potential Value Preferences Elicitation Approach Based on SC-VPM and KNN},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404690},
doi = {10.1145/3404687.3404690},
abstract = {Nowadays the more and more customers start to select and use the composite Web service on Internet, at the same time the services with the same functional properties but the different non-functional properties are increasingly emerging on Internet, which cause the information overload. Then the customer is not able to completely understand various composite Web services, and he/she is not able to define reasonable value preferences clearly on them. Therefore, this paper presents a potential value preference elicitation approach based on SC-VPM model and KNN algorithm, so as to support the third-party brokers to recommends top-satisfying services to customers according to the value preferences of the customers. In the approach, the inference rules based on the semantic relationships in SC-VPM model are used to preliminarily supplement the initial customer-value preference matrix firstly, so as to reduce the impact of the matrix sparsity on the following prediction. And then the KNN algorithm is used to identify the value preferences of K nearest neighbors customers, and the value preference vector of the target customer can be predicted and obtained. At last, a case is used to validate the proposed approach.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {60–64},
numpages = {5},
keywords = {Value preferences, KNN algorithm, Information overload, Inference rules},
location = {Chengdu, China},
series = {ICBDC '20}
}

@article{10.1007/s10772-020-09691-1,
author = {Vekkot, Susmitha and Gupta, Deepa},
title = {Speaker-independent expressive voice synthesis using learning-based hybrid network model},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-020-09691-1},
doi = {10.1007/s10772-020-09691-1},
abstract = {Emotional voice conversion systems are used to formulate mapping functions to transform the neutral speech from output of text-to-speech systems to that of target emotion appropriate to the context. In this work, a learning-based hybrid model is proposed for speaker-independent emotional voice conversion using a combination of deep belief nets (DBN-DNN) and general regression neural net (GRNN). The main acoustic features considered for mapping are shape of the vocal tract given by line spectral frequencies (LSF), glottal excitation given by LP residual and long term prosodic features viz. pitch contour and energy. GRNN is used to attain the transformation function between source and target LSFs. Source and target LP residuals are subjected to wavelet transform before DBN-DNN training. This is helpful to remove phase-change induced distortions which may affect the performance of neural networks when transforming time-domain residual. Low-dimensional pitch (intonation) contour is subjected to feed-forward neural network mapping (ANN). Energy modification is achieved by taking average transformation scales across entire utterance. The system is tested on three different datasets viz. EmoDB (German), IITKGP (Telugu) and SAVEE (English). Relative performances of proposed model are compared with constrained variance GMM (CV-GMM) using objective and subjective metrics. The results obtained show a significant performance improvement of 41% in RMSE (Hz) and 9.72% in Pearson’s correlation coefficient for fundamental frequency (F0) (Fear) compared to CV-GMM across all 3 datasets. Subjective results indicate a maximum MOS score of 3.85 (Fear) and CMOS score of 3.9 (Happiness) across the three datasets considered.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {597–613},
numpages = {17},
keywords = {CV-GMM, DBN-DNN, GRNN, Discrete wavelet transform, Line spectral frequencies, Hybrid model, Speaker-independent, Emotion}
}

@article{10.1016/j.eswa.2019.113114,
author = {Dargan, Shaveta and Kumar, Munish},
title = {A comprehensive survey on the biometric recognition systems based on physiological and behavioral modalities},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {143},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113114},
doi = {10.1016/j.eswa.2019.113114},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {27},
keywords = {Classification, Feature extraction, Biometric traits, Applications of biometric, Biometric}
}

@article{10.1016/j.neucom.2021.04.070,
author = {Yang, Zhao and Liu, Jiehao and Liu, Tie and Zhu, Yuanxin and Wang, Li and Tao, Dapeng},
title = {Equidistant distribution loss for person re-identification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {455},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.070},
doi = {10.1016/j.neucom.2021.04.070},
journal = {Neurocomput.},
month = sep,
pages = {255–264},
numpages = {10},
keywords = {Imbalance learning, Equidistant distribution loss, Person re-identification}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Feature selection, Feature ranking, Data mining, Classification}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@inproceedings{10.5555/3172077.3172181,
author = {Li, Hao and Gong, Maoguo},
title = {Self-paced convolutional neural networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks. In order to distinguish the reliable data from the noisy and confusing data, we improve CNNs with self-paced learning (SPL) for enhancing the learning robustness of CNNs. In the proposed self-paced convolutional network (SPCN), each sample is assigned to a weight to reflect the easiness of the sample. Then a dynamic self-paced function is incorporated into the leaning objective of CNN to jointly learn the parameters of CNN and the latent weight variable. SPCN learns the samples from easy to complex and the sample weights can dynamically control the learning rates for converging to better values. To gain more insights of SPCN, theoretical studies are conducted to show that SPCN converges to a stationary solution and is robust to the noisy and confusing data. Experimental results on MNIST and  rectangles  datasets demonstrate that the proposed method outperforms baseline methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2110–2116},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1145/3464305,
author = {Sobhy, Dalia and Bahsoon, Rami and Minku, Leandro and Kazman, Rick},
title = {Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464305},
doi = {10.1145/3464305},
abstract = {Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {51},
numpages = {50},
keywords = {uncertainty, run-time software architecture evaluation, design-time software architecture evaluation, Continuous software architecture evaluation}
}

@inproceedings{10.1145/2961111.2962600,
author = {Al-Subaihin, A. A. and Sarro, F. and Black, S. and Capra, L. and Harman, M. and Jia, Y. and Zhang, Y.},
title = {Clustering Mobile Apps Based on Mined Textual Features},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962600},
doi = {10.1145/2961111.2962600},
abstract = {Context: Categorising software systems according to their functionality yields many benefits to both users and developers. Goal: In order to uncover the latent clustering of mobile apps in app stores, we propose a novel technique that measures app similarity based on claimed behaviour. Method: Features are extracted using information retrieval augmented with ontological analysis and used as attributes to characterise apps. These attributes are then used to cluster the apps using agglomerative hierarchical clustering. We empirically evaluate our approach on 17,877 apps mined from the BlackBerry and Google app stores in 2014. Results: The results show that our approach dramatically improves the existing categorisation quality for both Blackberry (from 0.02 to 0.41 on average) and Google (from 0.03 to 0.21 on average) stores. We also find a strong Spearman rank correlation (ρ= 0.96 for Google and ρ= 0.99 for BlackBerry) between the number of apps and the ideal granularity within each category, indicating that ideal granularity increases with category size, as expected. Conclusions: Current categorisation in the app stores studied do not exhibit a good classification quality in terms of the claimed feature space. However, a better quality can be achieved using a good feature extraction technique and a traditional clustering method.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {38},
numpages = {10},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1007/978-3-030-49435-3_29,
author = {Reinhartz-Berger, Iris and Abbas, Sameh},
title = {A Variability-Driven Analysis Method for Automatic Extraction of Domain Behaviors},
year = {2020},
isbn = {978-3-030-49434-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49435-3_29},
doi = {10.1007/978-3-030-49435-3_29},
abstract = {Domain engineering focuses on modeling knowledge in an application domain for supporting systematic reuse in the context of complex and constantly evolving systems. Automatically supporting this task is challenging; most existing methods assume high similarity of variants which limits reuse of the generated domain artifacts, or provide very low-level features rather than actual domain features. As a result, these methods are limited in handling common scenarios such as similarly behaving systems developed by different teams, or merging existing products. To address this gap, we propose a method for extracting domain knowledge in the form of domain behaviors, building on a previously developed framework for behavior-based variability analysis among class operations. Machine learning techniques are applied for identifying clusters of operations that can potentially form domain behaviors. The approach is evaluated on a set of open-source video games, named apo-games.},
booktitle = {Advanced Information Systems Engineering: 32nd International Conference, CAiSE 2020, Grenoble, France, June 8–12, 2020, Proceedings},
pages = {467–481},
numpages = {15},
keywords = {Domain engineering, Systematic reuse, Variability analysis},
location = {Grenoble, France}
}

@inproceedings{10.5555/3382225.3382412,
author = {Xylogiannopoulos, Konstantinos F.},
title = {From data points to data curves: a new approach on big data curves clustering},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {In the new era of IoT, enormous real-values datasets are produced daily. Time series created by smart devices, financial data, weather analysis, medical applications, traffic control etc. become more and more important in human day life. Analyzing and clustering these time series or in general any kind of curve could be critical. In the current paper, a new methodology (BD2C) is presented, which applies text mining and pattern detection techniques in order to cluster curves according to their shape. Several experiments have been conducted on artificial and real datasets in order to present the accuracy, efficiency and rapid discovery of the best possible clustering that the proposed methodology can achieve.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {881–884},
numpages = {4},
keywords = {pattern detection, multivariate data analytics, curve clustering, LERP-RSA, BD2C, ARPaD},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1007/s00521-016-2707-8,
author = {Li, Zairan and Shi, Kai and Dey, Nilanjan and Ashour, Amira S. and Wang, Dan and Balas, Valentina E. and Mccauley, Pamela and Shi, Fuqian},
title = {Rule-based back propagation neural networks for various precision rough set presented KANSEI knowledge prediction: a case study on shoe product form features extraction},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {3},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-016-2707-8},
doi = {10.1007/s00521-016-2707-8},
abstract = {Nonlinear operators for KANSEI evaluation dataset were significantly developed such as uncertainty reason techniques including rough set, fuzzy set and neural networks. In order to extract more accurate KANSEI knowledge, rule-based presentation was concluded a promising way in KANSEI engineering research. In the present work, variable precision rough set was applied in rule-based system to reduce the complexity of the knowledge database from normal item dataset to high frequent rule set. In addition, evidence theory's reliability indices, namely the support and confidence for rule-based knowledge presentation, were proposed by using back propagation neural network with Bayesian regularization algorithm. The proposed method was applied in shoes KANSEI evaluation system; for a certain KANSEI adjective, the key form features of products were predicted. Some similar algorithms such as Levenberg---Marquardt and scaled conjugate gradient were also discussed and compared to establish the effectiveness of the proposed approach. The experimental results established the effectiveness and feasibility of the proposed algorithms customized for shoe industry, where the proposed back propagation neural network/Bayesian regularization approach achieved superior performance compared to the other algorithms in terms of the performance, gradient, Mu, Effective number of parameter, and the sum square parameter in KANSEI support and confidence time series prediction.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {613–630},
numpages = {18},
keywords = {Variable precision rough set, Shoe product form design, KANSEI engineering, Fuzzy set, Bayesian regularization, Back propagation neural networks}
}

@inproceedings{10.1145/3240508.3240648,
author = {Zheng, Xiaoju and Zha, Zheng-Jun and Zhuang, Liansheng},
title = {A Feature-Adaptive Semi-Supervised Framework for Co-saliency Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240648},
doi = {10.1145/3240508.3240648},
abstract = {Co-saliency detection, which refers to the discovery of common salient foreground regions in a group of relevant images, has attracted increasing attention due to its widespread applications in many vision tasks. Existing methods assemble features from multiple views toward a comprehensive representation, however overlook the efficacy disparity among various features in detecting co-saliency. This paper proposes a novel feature-adaptive semi-supervised (FASS) framework for co-saliency detection, which seamlessly integrates multi-view feature learning, graph structure optimization and co-saliency prediction in a unified solution. In particular, the FASS exploits the efficacy disparity of multi-view features at both view and element levels by a joint formulation of view-wise feature weighting and element-wise feature selection, leading to an effective representation robust to feature noise and redundancy as well as adaptive to the task at hand. It predicts co-saliency map by optimizing co-saliency label prorogation over a graph of both labeled and unlabeled image regions. The graph structure is optimized jointly with feature learning and co-saliency prediction to precisely characterize underlying correlation among regions. The FASS is thus able to produce satisfactory co-saliency map based on the effective exploration of multi-view features as well as inter-region correlation. Extensive experiments on three benchmark datasets, i.e., iCoseg, Cosal2015 and MSRC, have demonstrated that the proposed FASS outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {959–966},
numpages = {8},
keywords = {semi-supervised learning, multi-view feature, graph optimization, co-saliency detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.5555/3495724.3496497,
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {Self-paced deep reinforcement learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {773},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1504/ijista.2021.121328,
author = {Natarajan, K.K. and Gokulachandran, J.},
title = {Feature-based modelling and artificial intelligence-based computer assisted process planning systems for three axis milling components},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {20},
number = {4},
issn = {1740-8865},
url = {https://doi.org/10.1504/ijista.2021.121328},
doi = {10.1504/ijista.2021.121328},
abstract = {Computer-assisted process planning systems help human planners to create better process plans. Feature-based modelling is the current trend in recognising part features. SolidWorks 2018 software is used for CAD modelling and storage of the part manufacturing details in STEP242 file format. This file type stores details such as material, size, stock, dimensional tolerance, and surface finish and interfaces with neural networks to figure out the required machining operation and cutting tool. Throughout this work, different prismatic features, such as a hole, slot, pocket, boss, chamfer, fillet, and face, were considered. A sample prismatic component with nine features was analysed and found to be highly effective. This research concentrates on neural networks-based manufacturing operation selection and cutting tool selection. Levenberg-Marquardt and scaled conjugate gradient neural networks have proven to be more effective in machining operation selection and cutting tool selection respectively. The process parameter selection is done using SolidCAM software followed by NC code generation.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {340–361},
numpages = {21},
keywords = {parameters selection, cutting tool selection, operation selection, STEP242, manufacturing feature extraction, ANNs, artificial neural networks, CAPP, computer-aided process planning}
}

@article{10.1016/j.knosys.2020.106660,
author = {Liu, Zhen and Feng, Xiaodong and Wang, Yecheng and Zuo, Wenbo},
title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106660},
doi = {10.1016/j.knosys.2020.106660},
journal = {Know.-Based Syst.},
month = feb,
numpages = {12},
keywords = {Self-paced learning, Instance weighting, Noisy and outlier corruption, Deep learning, Recommendation}
}

@article{10.1007/s11042-018-5980-y,
author = {Krismayer, Thomas and Schedl, Markus and Knees, Peter and Rabiser, Rick},
title = {Predicting user demographics from music listening information},
year = {2019},
issue_date = {Feb 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-5980-y},
doi = {10.1007/s11042-018-5980-y},
abstract = {Online activities such as social networking, online shopping, and consuming multi-media create digital traces, which are often analyzed and used to improve user experience and increase revenue, e. g., through better-fitting recommendations and more targeted marketing. Analyses of digital traces typically aim to find user traits such as age, gender, and nationality to derive common preferences. We investigate to which extent the music listening habits of users of the social music platform Last.fm can be used to predict their age, gender, and nationality. We propose a feature modeling approach building on Term Frequency-Inverse Document Frequency (TF-IDF) for artist listening information and artist tags combined with additionally extracted features. We show that we can substantially outperform a baseline majority voting approach and can compete with existing approaches. Further, regarding prediction accuracy vs. available listening data we show that even one single listening event per user is enough to outperform the baseline in all prediction tasks. We also compare the performance of our algorithm for different user groups and discuss possible prediction errors and how to mitigate them. We conclude that personal information can be derived from music listening information, which indeed can help better tailoring recommendations, as we illustrate with the use case of a music recommender system that can directly utilize the user attributes predicted by our algorithm to increase the quality of it's recommendations.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {2897–2920},
numpages = {24},
keywords = {User trait prediction, User demographics, Music listening habits, Digital user traces}
}

@article{10.1016/j.jbi.2013.05.005,
author = {Zhu, Qian and Freimuth, Robert R. and Pathak, Jyotishman and Durski, Matthew J. and Chute, Christopher G.},
title = {Disambiguation of PharmGKB drug-disease relations with NDF-RT and SPL},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {46},
number = {4},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2013.05.005},
doi = {10.1016/j.jbi.2013.05.005},
abstract = {We disambiguate PharmGKB drug and disease associations by NDF-RT and SPL.Detailed clinical associations are clearly represented in PharmGKB.The work helps to understand drug and disease relations in details from PharmGKB.Reveals standardized drug information will accelerate clinical drug integration. PharmGKB is a leading resource of high quality pharmacogenomics data that provides information about how genetic variations modulate an individual's response to drugs. PharmGKB contains information about genetic variations, pharmacokinetic and pharmacodynamic pathways, and the effect of variations on drug-related phenotypes. These relationships are represented using very general terms, however, and the precise semantic relationships among drugs, and diseases are not often captured. In this paper we develop a protocol to detect and disambiguate general clinical associations between drugs and diseases using more precise annotation terms from other data sources. PharmGKB provides very detailed clinical associations between genetic variants and drug response, including genotype-specific drug dosing guidelines, and this procedure will armGKB. The availability of more detailed data will help investigators to conduct more precise queries, such as finding particular diseases caused or treated by a specific drug.We first mapped drugs extracted from PharmGKB drug-disease relationships to those in the National Drug File Reference Terminology (NDF-RT) and to Structured Product Labels (SPLs). Specifically, we retrieved drug and disease role relationships describing and defining concepts according to their relationships with other concepts from NDF-RT. We also used the NCBO (National Center for Biomedical Ontology) annotator to annotate disease terms from the free text extracted from five SPL sections (indication, contraindication, ADE, precaution, and warning). Finally, we used the detailed drug and disease relationship information from NDF-RT and the SPLs to annotate and disambiguate the more general PharmGKB drug and disease associations.},
journal = {J. of Biomedical Informatics},
month = aug,
pages = {690–696},
numpages = {7},
keywords = {SPL, Pharmacogenomics, PharmGKB, NDF-RT, Clinical associations}
}

@inproceedings{10.1145/3474085.3475471,
author = {Huang, Zongmo and Ren, Yazhou and Pu, Xiaorong and He, Lifang},
title = {Non-Linear Fusion for Self-Paced Multi-View Clustering},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475471},
doi = {10.1145/3474085.3475471},
abstract = {With the advance of the multi-media and multi-modal data, multi-view clustering (MVC) has drawn increasing attentions recently. In this field, one of the most crucial challenges is that the characteristics and qualities of different views usually vary extensively. Therefore, it is essential for MVC methods to find an effective approach that handles the diversity of multiple views appropriately. To this end, a series of MVC methods focusing on how to integrate the loss from each view have been proposed in the past few years. Among these methods, the mainstream idea is assigning weights to each view and then combining them linearly. In this paper, inspired by the effectiveness of non-linear combination in instance learning and the auto-weighted approaches, we propose Non-Linear Fusion for Self-Paced Multi-View Clustering (NSMVC), which is totally different from the the conventional linear-weighting algorithms. In NSMVC, we directly assign different exponents to different views according to their qualities. By this way, the negative impact from the corrupt views can be significantly reduced. Meanwhile, to address the non-convex issue of the MVC model, we further define a novel regularizer-free modality of Self-Paced Learning (SPL), which fits the proposed non-linear model perfectly. Experimental results on various real-world data sets demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3211–3219},
numpages = {9},
keywords = {self-paced learning, non-linear fusion, multi-view clustering},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1007/978-3-030-92273-3_25,
author = {Zheng, Jinfang and Xie, Jinyang and Lyu, Chen and Lyu, Lei},
title = {SS-CCN: Scale Self-guided Crowd Counting Network},
year = {2021},
isbn = {978-3-030-92272-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92273-3_25},
doi = {10.1007/978-3-030-92273-3_25},
abstract = {With the emergence of deep learning, many CNN-based methods have achieved competitive performance in crowd counting, in which how to effectively solve the scale variation problem plays a key role. To tackle with the problem, we present an innovative scale self-guided crowd counting network (SS-CCN) by taking full advantage of scale information in a multi-level network. The proposed SS-CCN highlights crowd information by applying scale enhancement and scale-aware attention modules in multi-level features. Moreover, semantic attention module is applied on deep layers to extract semantic information. Besides, the fine-grained residual module is proposed to further refine the crowd information. Furthermore, we pioneer a scale pyramid loss with different loss functions applied to different scales. Integrating the proposed module, our method can effectively solve the scale variation problem. Extensive experimental results on several public datasets show that our proposed SS-CCN achieves satisfactory and superior performance compared to the state-of-the-art methods.},
booktitle = {Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part IV},
pages = {299–310},
numpages = {12},
keywords = {Scale pyramid loss, Scale-aware, Attention mechanism, Deep learning, Crowd counting},
location = {Sanur, Bali, Indonesia}
}

@article{10.1504/ijcvr.2020.108152,
author = {Pawar, Karishma and Attar, Vahida},
title = {Deep learning-based intelligent surveillance model for detection of anomalous activities from videos},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {4},
issn = {1752-9131},
url = {https://doi.org/10.1504/ijcvr.2020.108152},
doi = {10.1504/ijcvr.2020.108152},
abstract = {For safeguarding and monitoring purposes, public places are equipped with surveillance cameras. Timely and accurate identification of suspicious activities is paramount to securing the public places. Assigning human personnel to keep continuous watch over ongoing activities is error-prone and laborious. To alleviate the need of human personnel for monitoring such videos, automated surveillance systems are required. This paper proposes a deep learning based intelligent surveillance model for detection of anomalous activities. The problem of anomaly detection has been handled as one class classification problem. The proposed approach involves two dimensional convolutional auto-encoder for feature learning, sequence-to-sequence long short term memory model for learning temporal statistical correlation and radial basis function as activation function in fully connected network for one class classification. We experimented on real-world dataset by two variants of proposed approach and achieved significant results at frame-level anomaly detection.},
journal = {Int. J. Comput. Vision Robot.},
month = jan,
pages = {289–311},
numpages = {22},
keywords = {video surveillance, RBF, radial basis function, one class classification, deep learning, convolutional autoencoder, computer vision, anomaly detection}
}

@article{10.1016/j.neucom.2018.04.001,
author = {Lu, Quanmao and Li, Xuelong and Dong, Yongsheng},
title = {Structure preserving unsupervised feature selection},
year = {2018},
issue_date = {August 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {301},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.001},
doi = {10.1016/j.neucom.2018.04.001},
abstract = {Spectral analysis was usually used to guide unsupervised feature selection. However, the performances of these methods are not always satisfactory due to that they may generate continuous pseudo labels to approximate the discrete real labels. In this paper, a novel unsupervised feature selection method is proposed based on self-expression model. Unlike existing spectral analysis based methods, we utilize self-expression model to capture the relationships between the features without learning the cluster labels. Specifically, each feature can be reconstructed by using a linear combination of all the features in the original feature space, and a representative feature should give a large weight to reconstruct other features. Besides, a structure preserved constraint is incorporated into our model for keeping the local manifold structure of the data. Then an efficient alternative iterative algorithm is utilized to solve our proposed model with the theoretical analysis on its convergence. The experimental results on different datasets show the effectiveness of our method.},
journal = {Neurocomput.},
month = aug,
pages = {36–45},
numpages = {10},
keywords = {Unsupervised feature selection, Structure preserving, Self-expression model}
}

@article{10.1016/j.specom.2015.01.003,
author = {Sadeghian, Amir and Dajani, Hilmi R. and Chan, Adrian D.C.},
title = {Classification of speech-evoked brainstem responses to English vowels},
year = {2015},
issue_date = {April 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2015.01.003},
doi = {10.1016/j.specom.2015.01.003},
abstract = {We investigated the automatic classification of speech-evoked brainstem responses.Responses to five vowels were classified based on onset and sustained features.Combined sustained features gave a classification accuracy of 83.33%.Classification accuracy with onset response features was better than chance.Vowel-specific information in responses may be useful for fitting hearing aids. This study investigated whether speech-evoked auditory brainstem responses (speech ABRs) can be automatically separated into distinct classes. With five English synthetic vowels, the speech ABRs were classified using linear discriminant analysis based on features contained in the transient onset response, the sustained envelope following response (EFR), and the sustained frequency following response (FFR). EFR contains components mainly at frequencies well below the first formant, while the FFR has more energy around the first formant. Accuracies of 83.33% were obtained for combined EFR and FFR features and 38.33% were obtained for transient response features. The EFR features performed relatively well with a classification accuracy of 70.83% despite the belief that vowel discrimination is primarily dependent on the formants. The FFR features obtained a lower accuracy of 59.58% possibly because the second formant is not well represented in all the responses. Moreover, the classification accuracy based on the transient features exceeded chance level which indicates that the initial response transients contain vowel specific information. The results of this study will be useful in a proposed application of speech ABR to objective hearing aid fitting, if the separation of the brain's responses to different vowels is found to be correlated with perceptual discrimination.},
journal = {Speech Commun.},
month = apr,
pages = {69–84},
numpages = {16},
keywords = {Speech-evoked auditory brainstem response, Frequency following response, Fitting hearing aids, Envelope following response, Classification of evoked responses, Auditory processing of speech}
}

@inproceedings{10.1007/978-3-030-29551-6_12,
author = {Peng, Qiyao and Wang, Peiyi and Wang, Wenjun and Liu, Hongtao and Sun, Yueheng and Jiao, Pengfei},
title = {NRSA: Neural Recommendation with Summary-Aware Attention},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_12},
doi = {10.1007/978-3-030-29551-6_12},
abstract = {Reviews are widely used in recommendation systems to handle the sparsity problem of rating matrix. However, learning the representations of users and items only from reviews would be challenging since there are less meaningful words and reviews while modeling users or items. In fact, in addition to reviews there are rich off-the-shelf summaries written by users along with reviews, but existing recommendation methods ignore this useful information. The summary of a review is to describe the review with shorter sentences, and can be seen as a high-level abstraction of the review. Thus the summary can play a guidance role to indicate the important parts in a review and the informativeness of the review. Hence, we propose a neural recommendation method to learn summary-aware representations of users and items from reviews. We firstly apply a summary encoder to learn representations of text summary, which will be used as the guidance indicator. We design a summary-aware review encoder to learn representations of reviews from raw words, and another summary-aware user/item encoder to learn representations of users or items from reviews. To be specific, we propose a hierarchical attention model with summary representations as attention vectors under word- and review-level to select important words and reviews for users/items respectively. We conduct extensive experiments on real-world benchmark datasets and the results demonstrate that our approach can effectively improve the performance of neural recommendation.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {128–140},
numpages = {13},
keywords = {Text Mining, Summary Attention, Recommendation},
location = {Athens, Greece}
}

@article{10.1007/s11042-017-5172-1,
author = {Ma, Xueqi and Tao, Dapeng and Liu, Weifeng},
title = {Effective human action recognition by combining manifold regularization and pairwise constraints},
year = {2019},
issue_date = {May       2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {10},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5172-1},
doi = {10.1007/s11042-017-5172-1},
abstract = {The ever-growing popularity of mobile networks and electronics has prompted intensive research on multimedia data (e.g. text, image, video, audio, etc.) management. This leads to the researches of semi-supervised learning that can incorporate a small number of labeled and a large number of unlabeled data by exploiting the local structure of data distribution. Manifold regularization and pairwise constraints are representative semi-supervised learning methods. In this paper, we introduce a novel local structure preserving approach by considering both manifold regularization and pairwise constraints. Specifically, we construct a new graph Laplacian that takes advantage of pairwise constraints compared with the traditional Laplacian. The proposed graph Laplacian can better preserve the local geometry of data distribution and achieve the effective recognition. Upon this, we build the graph regularized classifiers including support vector machines and kernel least squares as special cases for action recognition. Experimental results on a multimodal human action database (CAS-YNU-MHAD) show that our proposed algorithms outperform the general algorithms.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {13313–13329},
numpages = {17},
keywords = {Pairwise constraints, Manifold regularization, Local structure preserving, Action recognition}
}

@article{10.5555/2541581.2541582,
author = {Sandberg, Kristian and Bahrami, Bahador and Kanai, Ryota and Barnes, Gareth Robert and Overgaard, Morten and Rees, Geraint},
title = {Early visual responses predict conscious face perception within and between subjects during binocular rivalry},
year = {2013},
issue_date = {June 2013},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {25},
number = {6},
issn = {0898-929X},
abstract = {Previous studies indicate that conscious face perception may be related to neural activity in a large time window around 170-800 msec after stimulus presentation, yet in the majority of these studies changes in conscious experience are confounded with changes in physical stimulation. Using multivariate classification on MEG data recorded when participants reported changes in conscious perception evoked by binocular rivalry between a face and a grating, we showed that only MEG signals in the 120-320 msec time range, peaking at the M170 around 180 msec and the P2m at around 260 msec, reliably predicted conscious experience. Conscious perception could not only be decoded significantly better than chance from the sensors that showed the largest average difference, as previous studies suggest, but also from patterns of activity across groups of occipital sensors that individually were unable to predict perception better than chance. In addition, source space analyses showed that sources in the early and late visual system predicted conscious perception more accurately than frontal and parietal sites, although conscious perception could also be decoded there. Finally, the patterns of neural activity associated with conscious face perception generalized from one participant to another around the times of maximum prediction accuracy. Our work thus demonstrates that the neural correlates of particular conscious contents here, faces are highly consistent in time and space within individuals and that these correlates are shared to some extent between individuals.},
journal = {J. Cognitive Neuroscience},
month = jun,
pages = {969–985},
numpages = {17}
}

@article{10.1016/j.procs.2015.02.001,
author = {Patri, Ashutosh and Patnaik, Yugesh},
title = {Random Forest and Stochastic Gradient Tree Boosting Based Approach for the Prediction of Airfoil Self-noise},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.001},
doi = {10.1016/j.procs.2015.02.001},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {109–121},
numpages = {13},
keywords = {NACA0012., regression, CART, Stochastic Gradient Boosting, Random Forest, prediction, airfoil self-noise}
}

@article{10.4018/IJOSSP.2016010102,
author = {Chahal, Kuljit Kaur and Saini, Munish},
title = {Open Source Software Evolution: A Systematic Literature Review Part 2},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016010102},
doi = {10.4018/IJOSSP.2016010102},
abstract = {This paper presents the results of a systematic literature review conducted to understand the Open Source Software OSS development process on the basis of evidence found in the empirical research studies. The study targets the OSS project evolution research papers to understand the methods and techniques employed for analysing the OSS evolution process. Our results suggest that there is lack of a uniform approach to analyse and interpret the results. The use of prediction techniques that just extrapolate the historic trends into the future should be a conscious task as it is observed that there are no long-term correlations in data of such systems. OSS evolution as a research area is still in nascent stage. Even after a number of empirical studies, the field has failed to establish a theory. There is need to formalize the field as a systematic and formal approach can produce better software.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {28–48},
numpages = {21},
keywords = {Software Reuse, Software Evolution Theory, Programming Languages, OSS Prediction, Co-Evolution, Automation Support, ARIMA Modelling}
}

@article{10.1016/j.camwa.2013.06.027,
author = {Borges, Helyane Bronoski and Silla, Carlos N. and Nievola, J\'{u}lio Cesar},
title = {An evaluation of global-model hierarchical classification algorithms for hierarchical classification problems with single path of labels},
year = {2013},
issue_date = {December, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {10},
issn = {0898-1221},
url = {https://doi.org/10.1016/j.camwa.2013.06.027},
doi = {10.1016/j.camwa.2013.06.027},
abstract = {Several classification tasks in different application domains can be seen as hierarchical classification problems. In order to deal with hierarchical classification problems, the use of existing flat classification approaches is not appropriate. For these reason, there has been a growing number of studies focusing on the development of novel algorithms able to induce classification models for hierarchical classification problems. In this paper we study the performance of a novel algorithm called Hierarchical Classification using a Competitive Neural Network (HC-CNN) and compare its performance against the Global-Model Naive Bayes (GMNB) on eight protein function prediction datasets. Interestingly enough, the comparison of two global-model hierarchical classification algorithms for single path of labels hierarchical classification problems has never been done before.},
journal = {Comput. Math. Appl.},
month = dec,
pages = {1991–2002},
numpages = {12},
keywords = {Hierarchical classification, Global approach}
}

@article{10.1007/s10772-016-9367-z,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with Parkinson's diseases using PCA and NPCA},
year = {2016},
issue_date = {December  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-016-9367-z},
doi = {10.1007/s10772-016-9367-z},
abstract = {In this study, we wanted to discriminate between two groups of people. The database used in this study contains 20 patients with Parkinson's disease and 20 healthy people. Three types of sustained vowels (/a/, /o/ and /u/) were recorded from each participant and then the analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used linear and nonlinear feature extraction techniques, principal component analysis (PCA), and nonlinear PCA. These techniques reduce the number of parameters and choose the most effective acoustic features used for classification. Support vector machine with its different kernel was used for classification. We obtained an accuracy up to 87.50 % for discrimination between PD patients and healthy people.},
journal = {Int. J. Speech Technol.},
month = dec,
pages = {743–754},
numpages = {12},
keywords = {SVM, Parkinson's disease, PCA, NPCA, Feature selection}
}

@inproceedings{10.1007/978-3-030-98682-7_13,
author = {Antonioni, Emanuele and Suriani, Vincenzo and Solimando, Filippo and Nardi, Daniele and Bloisi, Domenico D.},
title = {Learning from the Crowd: Improving the Decision Making Process in Robot Soccer Using the Audience Noise},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_13},
doi = {10.1007/978-3-030-98682-7_13},
abstract = {Fan input and support is an important component in many individual and team sports, ranging from athletics to basketball. Audience interaction provides a consistent impact on the athletes’ performance. The analysis of the crowd noise can provide a global indication on the ongoing game situation, less conditioned by subjective factors that can influence a single fan. In this work, we exploit the collective intelligence of the audience of a robot soccer match to improve the performance of the robot players. In particular, audio features extracted from the crowd noiseare used in a Reinforcement Learning process to possibly modify the game strategy. The effectiveness of the proposed approach is demonstrated by experiments on registered crowd noise samples from several past RoboCup SPL matches.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {153–164},
numpages = {12},
keywords = {Sound recognition, RoboCup SPL, Crowd noise interpretation},
location = {Sydney, NSW, Australia}
}

@article{10.1007/s11042-018-6415-5,
author = {Choe, Gwangmin and Choe, Chunhwa and Wang, Tianjiang and So, Hyoson and Nam, Cholman and Yuan, Caihong},
title = {Deep learning with particle filter for person re-identification},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6415-5},
doi = {10.1007/s11042-018-6415-5},
abstract = {Person re-identification, having attracted much attention in the multimedia community, is still challenged by the accuracy and the robustness, as the images for the verification contain such variations as light, pose, noise and ambiguity etc. Such practical challenges require relatively robust and accurate feature learning technologies. We introduced a novel deep neural network with PF-BP(Particle Filter-Back Propagation) to achieve relatively global and robust performances of person re-identification. The local optima in the deep networks themselves are still the main difficulty in the learning, in despite of several advanced approaches. A novel neural network learning, or PF-BP, was first proposed to solve the local optima problem in the non-convex objective function of the deep networks. When considering final deep network to learn using BP, the overall neural network with the particle filter will behave as the PF-BP neural network. Also, a max-min value searching was proposed by considering two assumptions about shapes of the non-convex objective function to learn on. Finally, a salience learning based on the deep neural network with PF-BP was proposed to achieve an advanced person re-identification. We test our neural network learning with particle filter aimed to the non-convex optimization problem, and then evaluate the performances of the proposed system in a person re-identification scenario. Experimental results demonstrate that the corresponding performances of the proposed deep network have promising discriminative capability in comparison with other ones.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {6607–6636},
numpages = {30},
keywords = {Person re-identification, Particle filter, Non-convex objective function, Global optimum, Deep learning, Back-propagation}
}

@article{10.1016/j.jbi.2011.11.002,
author = {Forestier, Germain and Lalys, Florent and Riffaud, Laurent and Trelhu, Brivael and Jannin, Pierre},
title = {Classification of surgical processes using dynamic time warping},
year = {2012},
issue_date = {April, 2012},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {45},
number = {2},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2011.11.002},
doi = {10.1016/j.jbi.2011.11.002},
abstract = {In the creation of new computer-assisted intervention systems, Surgical Process Models (SPMs) are an emerging concept used for analyzing and assessing surgical interventions. SPMs represent Surgical Processes (SPs) which are formalized as symbolic structured descriptions of surgical interventions using a pre-defined level of granularity and a dedicated terminology. In this context, one major challenge is the creation of new metrics for the comparison and the evaluation of SPs. Thus, correlations between these metrics and pre-operative data are used to classify surgeries and highlight specific information on the surgery itself and on the surgeon, such as his/her level of expertise. In this paper, we explore the automatic classification of a set of SPs based on the Dynamic Time Warping (DTW) algorithm. DTW is used to compute a similarity measure between two SPs that focuses on the different types of activities performed during surgery and their sequencing, by minimizing time differences. Indeed, it turns out to be a complementary approach to the classical methods that only focus on differences in the time and the number of activities. Experiments were carried out on 24 lumbar disk herniation surgeries to discriminate the surgeons level of expertise according to a prior classification of SPs. Supervised and unsupervised classification experiments have shown that this approach was able to automatically identify groups of surgeons according to their level of expertise (senior and junior), and opens many perspectives for the creation of new metrics for comparing and evaluating surgeries.},
journal = {J. of Biomedical Informatics},
month = apr,
pages = {255–264},
numpages = {10},
keywords = {Surgical process models, Surgery evaluation, Dynamic time warping, Clustering, Classification}
}

@inproceedings{10.1007/978-3-319-29339-4_24,
author = {Leottau, David L. and Ruiz-del-Solar, Javier and MacAlpine, Patrick and Stone, Peter},
title = {A Study of Layered Learning Strategies Applied to Individual Behaviors in Robot Soccer},
year = {2015},
isbn = {978-3-319-29338-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29339-4_24},
doi = {10.1007/978-3-319-29339-4_24},
abstract = {Hierarchical task decomposition strategies allow robots and agents in general to address complex decision-making tasks. Layered learning is a hierarchical machine learning paradigm where a complex behavior is learned from a series of incrementally trained sub-tasks. This paper describes how layered learning can be applied to design individual behaviors in the context of soccer robotics. Three different layered learning strategies are implemented and analyzed using a ball-dribbling behavior as a case study. Performance indices for evaluating dribbling speed and ball-control are defined and measured. Experimental results validate the usefulness of the implemented layered learning strategies showing a trade-off between performance and learning speed.},
booktitle = {RoboCup 2015: Robot World Cup XIX},
pages = {290–302},
numpages = {13},
keywords = {Reinforcement learning, Layered learning, Machine learning, Soccer robotics, Biped robot, NAO, Behavior, Dribbling, Fuzzy logic},
location = {Hefei, China}
}

@article{10.1007/s10115-019-01404-8,
author = {Chen, Tong and Yin, Hongzhi and Chen, Hongxu and Wang, Hao and Zhou, Xiaofang and Li, Xue},
title = {Online sales prediction via trend alignment-based multitask recurrent neural networks},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {62},
number = {6},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-019-01404-8},
doi = {10.1007/s10115-019-01404-8},
abstract = {While business trends are constantly evolving, the timely prediction of sales volume offers precious information for companies to achieve a healthy balance between supply and demand. In practice, sales prediction is formulated as a time series prediction problem which aims to predict the future sales volume for different products with the observation of various influential factors (e.g. brand, season, discount, etc.) and corresponding historical sales records. To perform accurate sales prediction under the offline setting, we gain insights from the encoder–decoder recurrent neural network (RNN) structure and have proposed a novel framework named TADA (Chen et al., in: ICDM, 2018) to carry out trend alignment with dual-attention, multitask RNNs for sales prediction. However, the sales data accumulates at a fast rate and is updated on a regular basis, rendering it difficult for the trained model to maintain the prediction accuracy with new data. In this light, we further extend the model into TADA+, which is enhanced by an online learning module based on our innovative similarity-based reservoir. To construct the data reservoir for model retraining, different from most existing random sampling-based reservoir, our similarity-based reservoir selects data samples that are “hard” for the model to mine apparent dynamic patterns. The experimental results on two real-world datasets comprehensively show the superiority of TADA and TADA+ in both online and offline sales prediction tasks against other state-of-the-art competitors.},
journal = {Knowl. Inf. Syst.},
month = jun,
pages = {2139–2167},
numpages = {29},
keywords = {Time series analysis, Attention mechanism, Recurrent neural networks, Online sales prediction}
}

@inproceedings{10.5555/3042094.3042398,
author = {Liotta, Giacomo and Chaudhuri, Atanu},
title = {Minimizing recall risk by collaborative digitized information sharing between OEM and suppliers: a simulation based investigation},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Many Original Equipment Manufacturers (OEMs) and their suppliers face recall and warranty risks due to complex supply chains and products. OEMs and suppliers can hardly take appropriate actions for mitigating these quality risks due to lack of product history data and understanding of their probability. In this work, the product consists of two components delivered by two Tier II suppliers. Probabilities of OEM's acceptance, rework and rejection of the assembled product by a Tier I supplier and probabilities of acceptance, warranty and recall are calculated combining Bayesian Belief Network and simulation of a digitized supply chain. Results show that sharing of incoming quality information between an OEM and Tier I supplier and decision models to estimate warranty and recall probabilities can help in assessing quality improvement benefits to minimize recall risks. Suitable quality improvement contracts between an OEM and Tier I supplier can be designed using embedded product quality data.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2454–2465},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@inbook{10.5555/3454287.3454621,
author = {Hwang, Gunpil and Kim, Seohyeon and Bae, Hyeon-Min},
title = {Bat-G net: bat-inspired high-resolution 3D image reconstruction using ultrasonic echoes},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899, and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {334},
numpages = {12}
}

@article{10.1007/s11042-017-5309-2,
author = {Wang, Tian and Qiao, Meina and Zhu, Aichun and Niu, Yida and Li, Ce and Snoussi, Hichem},
title = {Abnormal event detection via covariance matrix for optical flow based feature},
year = {2018},
issue_date = {July      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5309-2},
doi = {10.1007/s11042-017-5309-2},
abstract = {Abnormal event detection is one of the most important objectives in security surveillance for public scenes. In this paper, a new high-performance algorithm based on spatio-temporal motion information is proposed to detect global abnormal events from the video stream as well as the local abnormal event. We firstly propose a feature descriptor to represent the movement by adopting the covariance matrix coding optical flow and the corresponding partial derivatives of multiple connective frames or the patches of the frames. The covariance matrix of multi-RoI (region of interest) which consists of frames or patches can represent the movement in high accuracy. For public surveillance video, the normal samples are abundant while there are few abnormal samples. Thus the one-class classification method is suitable for handling this problem inherently. The nonlinear one-class support vector machine based on a proposed kernel for Lie group element is applied to detect abnormal events by merely training the normal samples. The computational complexity and time performance of the proposed method is analyzed. The PETS, UMN and UCSD benchmark datasets are employed to verify the advantages of the proposed method for both global abnormal and local abnormal event detection. This method can be used for event detection for a surveillance video and outperforms the state-of-the-art algorithms. Thus it can be adopted to detect the abnormal event in the monitoring video.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {17375–17395},
numpages = {21},
keywords = {Optical flow, Multi-RoI, Local abnormal event, Global abnormal event, Covariance matrix}
}

@article{10.1007/s11265-009-0382-z,
author = {Hauptmann, A. G. and Chen, M. -Y. and Christel, M. and Lin, W. -H. and Yang, J.},
title = {A Multi-Pronged Approach to Improving Semantic Extraction of News Video},
year = {2010},
issue_date = {March     2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {58},
number = {3},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-009-0382-z},
doi = {10.1007/s11265-009-0382-z},
abstract = {In this paper we describe a multi-strategy approach to improving semantic extraction from news video. Experiments show the value of careful parameter tuning, exploiting multiple feature sets and multilingual linguistic resources, applying text retrieval approaches for image features, and establishing synergy between multiple concepts through undirected graphical models. We present a discriminative learning framework called Multi-concept Discriminative Random Field (MDRF) for building probabilistic models of video semantic concept detectors by incorporating related concepts as well as the low-level observations. The model exploits the power of discriminative graphical models to simultaneously capture the associations of concept with observed data and the interactions between related concepts. Compared with previous methods, this model not only captures the co-occurrence between concepts but also incorporates the raw data observations into a unified framework. We also describe an approximate parameter estimation algorithm and present results obtained from the TRECVID 2006 data. No single approach, however, provides a consistently better result for all concept detection tasks, which suggests that extracting video semantics should exploit multiple resources and techniques rather than naively relying on a single approach},
journal = {J. Signal Process. Syst.},
month = mar,
pages = {373–385},
numpages = {13},
keywords = {Video analysis, Undirected graphical models, Semantic concept detection, Discriminative random fields}
}

@inproceedings{10.1145/3349341.3349414,
author = {Qiao, Weizheng and Bi, Xiaojun},
title = {Deep Spatial-Temporal Neural Network for Classification of EEG-Based Motor Imagery},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349414},
doi = {10.1145/3349341.3349414},
abstract = {As a challenging topic in brain-computer interface (BCI) research, motor imagery classification based on electroencephalogram (EEG) received more and more attention. Due to the low signal-noise ratio of the EEG signal, the promotion of classification result is formidable to carry on. In the current circumstances, how to extract more abstract representations and deep features becomes a research hotspot. Meanwhile, Deep learning (DL) models have been applied in various fields demonstrating great superiority in feature extraction and classification. Despite the fact that a variety of methods have been developed to the feature learning of EEG, few studies have applied deep learning to model the representations of EEG features. In this paper, we present a novel method to perform preprocessing and feature learning. By performing Morlet wavelet and cubic spline interpolation methods, we can obtain a series of spectral frames from the multi-channel EEG signals. Then we construct a hybrid model which associates modified Convolutional Neural Network (CNN) with Bidirectional Gated Recurrent Unit (BGRU). The BGRU is used to learn the contextual information from the feature maps that are extracted from the modified CNN. Finally, motor imagery classification task is performed to evaluate our method. The experimental results demonstrate the significant effectiveness of the proposed approach compared with the state-of-the-art models in classification accuracy.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {265–272},
numpages = {8},
keywords = {Motor imagery classification, Gated Recurrent Unit, EEG, Deep learning, Convolutional Neural Network},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@article{10.1155/2021/9956244,
author = {Li, Lei and Zhu, Yuquan and Cai, Tao and Niu, Dejiao and Shi, Huaji and Zou, Tingting and Huang, Chenxi},
title = {A Temporal Pool Learning Algorithm Based on Location Awareness},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/9956244},
doi = {10.1155/2021/9956244},
abstract = {Hierarchical Temporal Memory is a new type of artificial neural network model, which imitates the structure and information processing flow of the human brain. Hierarchical Temporal Memory has strong adaptability and fast learning ability and becomes a hot spot in current research. Hierarchical Temporal Memory obtains and saves the temporal characteristics of input sequences by the temporal pool learning algorithm. However, the current algorithm has some problems such as low learning efficiency and poor learning effect when learning time series data. In this paper, a temporal pool learning algorithm based on location awareness is proposed. The cell selection rules based on location awareness and the dendritic updating rules based on adjacent inputs are designed to improve the learning efficiency and effect of the algorithm. Through the algorithm prototype, three different datasets are used to test and analyze the algorithm performance. The experimental results verify that the algorithm can quickly obtain the complete characteristics of the input sequence. No matter whether there are similar segments in the sequence, the proposed algorithm has higher prediction recall and precision than the existing algorithms.},
journal = {Sci. Program.},
month = jan,
numpages = {12}
}

@inproceedings{10.1007/978-3-030-98682-7_17,
author = {Hasselbring, Arne and Baude, Andreas},
title = {Soccer Field Boundary Detection Using Convolutional Neural Networks},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_17},
doi = {10.1007/978-3-030-98682-7_17},
abstract = {Detecting the field boundary is often one of the first steps in the vision pipeline of soccer robots. Conventional methods make use of a (possibly adaptive) green classifier, selection of boundary points and possibly model fitting. We present an approach to predict the coordinates of the field boundary column-wise in the image using a convolutional neural network. This is combined with a method to let the network predict the uncertainty of its output, which allows to fit a line model in which columns are weighted according to the network’s confidence. Experiments show that the resulting models are accurate enough in different lighting conditions as well as real-time capable. Code and data are available online (, ).},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {202–213},
numpages = {12},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3302506.3310407,
author = {Shih, Oliver and Rowe, Anthony},
title = {Can a phone hear the shape of a room?},
year = {2019},
isbn = {9781450362849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302506.3310407},
doi = {10.1145/3302506.3310407},
abstract = {Understanding the location of acoustically reflective surfaces in a room is a critical component in advanced sound processing. For example, intelligent speakers can use a room's acoustic geometry to improve playback quality, source separation accuracy, and speech recognition. In this paper, we present Synesthesia, a system for capturing the acoustic properties of a room using a single fixed speaker and a mobile phone that records audio at multiple locations. Using the arrival time of echoes, the system is able to reconstruct the position of reflective surfaces like walls and then estimate properties like surface absorption.Previous work has shown how the acoustic room impulse response (RIR) of an environment can be used to analyze echoes within a space to reconstruct room geometry. The best current RIR-based approaches rely on high-end equipment and capturing an acoustic signal broadcast into space from a known fixed constellation of microphones. They also require the precise calibration and measurement of microphone positions. In addition, most approaches pose constraints on room geometries and limit the order of RIR to achieve accurate and consistent results. In this paper, we introduce a new approach that performs RIR imaging using a mobile phone that tracks its location with visual inertial odometry (VIO) to record a dense set of samples albeit with noise in their locations. We present a new approach that is able to relax several key assumptions on RIR and show through both experimentation and simulation that even with 20cm of uncertainty in the microphone locations provided by VIO, we are still able to reconstruct the room geometry with accurate shape and dimensions. We demonstrate this capability by prototyping a tool for acoustic engineers, that allows a user to view a room's estimated geometry and absorption overlaid on the actual sensed space with augmented reality.},
booktitle = {Proceedings of the 18th International Conference on Information Processing in Sensor Networks},
pages = {277–288},
numpages = {12},
keywords = {active acoustic sensing, room reconstruction and mapping},
location = {Montreal, Quebec, Canada},
series = {IPSN '19}
}

@inproceedings{10.1007/978-3-030-87199-4_50,
author = {Sedlar, Sara and Alimi, Abib and Papadopoulo, Th\'{e}odore and Deriche, Rachid and Deslauriers-Gauthier, Samuel},
title = {A Spherical Convolutional Neural Network for White Matter Structure Imaging via dMRI},
year = {2021},
isbn = {978-3-030-87198-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87199-4_50},
doi = {10.1007/978-3-030-87199-4_50},
abstract = {Diffusion Magnetic Resonance Imaging (dMRI) is a powerful non-invasive and in-vivo imaging modality for probing brain white matter structure. Convolutional neural networks (CNNs) have been shown to be a powerful tool for many computer vision problems where the signals are acquired on a regular grid and where translational invariance is important. However, as we are considering dMRI signals that are acquired on a sphere, rotational invariance, rather than translational, is desired. In this work, we propose a spherical CNN model with fully spectral domain convolutional and non-linear layers. It provides rotational invariance and is adapted to the real nature of dMRI signals and uniform random distribution of sampling points. The proposed model is positively evaluated on the problem of estimation of neurite orientation dispersion and density imaging (NODDI) parameters on the data from Human Connectome Project (HCP).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III},
pages = {529–539},
numpages = {11},
keywords = {White matter micro-structures, Diffusion MRI, Spherical CNN},
location = {Strasbourg, France}
}

@article{10.1016/j.ins.2020.10.002,
author = {Yoon, Young-Chul and Kim, Du Yong and Song, Young-Min and Yoon, Kwangjin and Jeon, Moongu},
title = {Online multiple pedestrians tracking using deep temporal appearance matching association},
year = {2021},
issue_date = {Jun 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {561},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2020.10.002},
doi = {10.1016/j.ins.2020.10.002},
journal = {Inf. Sci.},
month = jun,
pages = {326–351},
numpages = {26},
keywords = {Online appearance modeling, Feature embedding, Deep learning, Bayesian tracking, Visual multi-target tracking}
}

@article{10.1145/3446344,
author = {Wang, Yuandong and Yin, Hongzhi and Chen, Tong and Liu, Chunyang and Wang, Ben and Wo, Tianyu and Xu, Jie},
title = {Passenger Mobility Prediction via Representation Learning for Dynamic Directed and Weighted Graphs},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3446344},
doi = {10.1145/3446344},
abstract = {In recent years, ride-hailing services have been increasingly prevalent, as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. aspects in the graph-structure data. representation for DDW is the key to solve the prediction problem. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of dynamic directed and weighted graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {2},
numpages = {25},
keywords = {passenger demand prediction, representation learning, Dynamic graph}
}

@inproceedings{10.1007/978-3-030-75765-6_49,
author = {Thanthriwatta, Thilina and Rosenblum, David S.},
title = {Instance Selection for Online Updating in Dynamic Recommender Environments},
year = {2021},
isbn = {978-3-030-75764-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-75765-6_49},
doi = {10.1007/978-3-030-75765-6_49},
abstract = {Online recommender systems continuously learn from user interactions that occur in a streaming manner. A fundamental challenge of online recommendation is to select important instances (i.e., user interactions) for model updates to achieve higher prediction accuracy while omitting noisy instances. In this paper, we study&nbsp;(1) how to select the best instances and&nbsp;(2) how to effectively utilize the selected instances in dynamic recommender environments. We present two instance selection strategies based on Self-Paced Learning and rating profiles. We integrate them with Factorization Machines to perform online updates. Moreover, we study the impact of contextual information in online updating. We conducted experiments on a real-world check-in dataset, which contains temporal contextual features. Empirical results demonstrate that ox ur instance selection strategies effectively balance the trade-off between prediction accuracy and efficiency.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part II},
pages = {612–624},
numpages = {13},
keywords = {Online recommender systems, Context-aware recommender systems, Instance selection}
}

@article{10.1007/s11042-016-3976-z,
author = {Agrawal, Amrit Kumar and Singh, Yogendra Narain},
title = {An efficient approach for face recognition in uncontrolled environment},
year = {2017},
issue_date = {February  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-016-3976-z},
doi = {10.1007/s11042-016-3976-z},
abstract = {There is a great demand of automatic face recognition in the society. The methods of face recognition are performed satisfactorily in controlled environment. The challenging benchmarks demonstrate that these methods may not adequately work in unconstrained environment. In this paper, we develop a novel framework of face recognition system that outperforms in unconstrained environment. The framework works on features based method that extracts facial landmarks from images. After quality check the patch experts are generated and used to model the appearance of landmarks of interests. The effect of discriminatory features is further enhanced by assigning weights to them that are to be set to the ratio of the interclass variance to the intraclass variance. The result shows that the proposed framework achieves better recognition accuracy in comparison to other known methods on publically available challenging datasets.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {3751–3760},
numpages = {10},
keywords = {Facial features detection and unconstrained environment, Face recognition}
}

@article{10.1007/s10115-018-1298-3,
author = {Li, Cheng-Te},
title = {Mentor-spotting: recommending expert mentors to mentees for live trouble-shooting in Codementor},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {61},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-018-1298-3},
doi = {10.1007/s10115-018-1298-3},
abstract = {Live mentoring services are recent novel social media, in which mentees can input expertise requests and wait for accepting some expert mentor who is willing to tackle the requests in a live and one-by-one manner. While mentee’s satisfaction of being mentored is determined by the matched mentor, it is crucial to have an effective mentor–mentee matching. This paper aims at recommending mentors based on the requests in Codementor, which is one of the popular live mentoring services. An accurate mentor recommendation will support the mentees’ decisions in finding suitable mentors, support the mentors’ decisions in filtering out irrelevant requests, and support the mentoring services’ decisions in assigning mentors to mentees. We divide the mentor recommendation problem into two tasks, Mentor Willingness Prediction (MWP) and Mentee Acceptance Prediction (MAP). MWP is to predict whether a mentor is willing to tackle a request, while MAP is to predict whether a mentee user will accept a recommended mentor. We propose to simultaneously deal with such two tasks by recommending a ranked list of mentors such that the recommended mentors who are really willing to tackle the request are as many as possible (MWP) and the final mentor who is accepted by the mentee can be ranked as high as possible (MAP). We develop four categories of features, availability, capability, activity, and proximity, to model the willingness of a mentor dealing with the request and the potential of a mentee to accept the recommended mentor. By applying various supervised learning methods, experimental results show the effectiveness of these features and provide extensive analyses to reveal more factors that can affect the quality of mentor recommendation. In addition, we also conduct a user study on Codementor platform to exhibit the practical performance of the proposed method. The innovation of this work includes the formulation of MWP and MAP problem in online mentoring services, feature engineering for mentoring prediction tasks, and data-driven experimental studies in prediction and a practical user study.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {799–820},
numpages = {22},
keywords = {Codementor, Live mentoring services, Mentee Acceptance Prediction, Mentor Willingness Prediction, Mentoring analysis}
}

@article{10.1016/j.eswa.2014.04.046,
author = {Chin, Kwai-Sang and Fu, Chao},
title = {Integrated evidential reasoning approach in the presence of cardinal and ordinal preferences and its applications in software selection},
year = {2014},
issue_date = {November, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {15},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.04.046},
doi = {10.1016/j.eswa.2014.04.046},
abstract = {A combination of cardinal and ordinal preferences in multiple-attribute decision making (MADM) demonstrates more reliability and flexibility compared with sole cardinal or ordinal preferences derived from a decision maker. This situation occurs particularly when the knowledge and experience of the decision maker, as well as the data regarding specific alternatives on certain attributes, are insufficient or incomplete. This paper proposes an integrated evidential reasoning (IER) approach to analyze uncertain MADM problems in the presence of cardinal and ordinal preferences. The decision maker provides complete or incomplete cardinal and ordinal preferences of each alternative on each attribute. Ordinal preferences are expressed as unknown distributed assessment vectors and integrated with cardinal preferences to form aggregated preferences of alternatives. Three optimization models considering cardinal and ordinal preferences are constructed to determine the minimum and maximum minimal satisfaction of alternatives, simultaneous maximum minimal satisfaction of alternatives, and simultaneous minimum minimal satisfaction of alternatives. The minimax regret rule, the maximax rule, and the maximin rule are employed respectively in the three models to generate three kinds of value functions of alternatives, which are aggregated to find solutions. The attribute weights in the three models can be precise or imprecise (i.e., characterized by six types of constraints). The IER approach is used to select the optimum software for product lifecycle management of a famous Chinese automobile manufacturing enterprise.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6718–6727},
numpages = {10},
keywords = {Multiple-attribute decision making, Integrated decision, Evidential reasoning approach, Decision analysis, Cardinal and ordinal preferences}
}

@article{10.3233/JIFS-189010,
author = {Zhang, Xiaoxian and Zhang, Jianpei and Yang, Jing and Maseleno, Andino and Yuan, Xiaohui and Balas, Valentina E.},
title = {Large-scale dynamic social data representation for structure feature learning},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189010},
doi = {10.3233/JIFS-189010},
abstract = {The problems caused by network dimension disasters and computational complexity have become an important issue to be solved in the field of social network research. The existing methods for network feature learning are mostly based on static and small-scale assumptions, and there is no modified learning for the unique attributes of social networks. Therefore, existing learning methods cannot adapt to the dynamic and large-scale of current social networks. Even super large scale and other features. This paper mainly studies the feature representation learning of large-scale dynamic social network structure. In this paper, the positive and negative damping sampling of network nodes in different classes is carried out, and the dynamic feature learning method for newly added nodes is constructed, which makes the model feasible for the extraction of structural features of large-scale social networks in the process of dynamic change. The obtained node feature representation has better dynamic robustness. By selecting the real datasets of three large-scale dynamic social networks and the experiments of dynamic link prediction in social networks, it is found that DNPS has achieved a large performance improvement over the benchmark model in terms of prediction accuracy and time efficiency. When the α value is around 0.7, the model effect is optimal.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {5253–5262},
numpages = {10},
keywords = {neural network, representation learning, social network, Feature learning}
}

@article{10.1016/j.patrec.2021.08.011,
author = {Mehta, Nancy and Murala, Subrahmanyam},
title = {MSAR-Net: Multi-scale attention based light-weight image super-resolution},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.08.011},
doi = {10.1016/j.patrec.2021.08.011},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {215–221},
numpages = {7},
keywords = {65D17, 65D05, 41A10, 41A05, Image super-resolution, Up and down-sampling projection block, Multi-scale attention residual block}
}

@inproceedings{10.5555/1558109.1558283,
author = {Nunes, Ingrid and Kulesza, Uir\'{a} and Nunes, Camila and Lucena, Carlos J. P.},
title = {A domain engineering process for developing multi-agent systems product lines},
year = {2009},
isbn = {9780981738178},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent Systems Product Lines (MAS-PLs) have emerged to integrate two promising trends of software engineering: agent-oriented software engineering and software product lines. In this paper, we propose a domain engineering process to develop MAS-PLs, built on top of agent-oriented and software product line approaches.},
booktitle = {Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {1339–1340},
numpages = {2},
keywords = {software product lines, process, multi-agent systems, domain engineering, agent-oriented software engineering},
location = {Budapest, Hungary},
series = {AAMAS '09}
}

@inproceedings{10.5555/2888619.2888848,
author = {Rabe, Markus and Dross, Felix},
title = {A reinforcement learning approach for a decision support system for logistics networks},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {This paper presents the architecture and working principles of a Decision Support System (DSS) for logistics networks. The system relies on a data-driven discrete-event simulation model. A brief introduction to Reinforcement Learning (RL) and an explanation of the adoption of RL to the concepts of the DSS is given. An illustration of the realization is presented using a specific aspect of a logistics network. The logistics network is described in a data model which is represented by database tables. The tables are used to dynamically instantiate the simulation model. The authors describe how SQL queries can be used to model actions of an RL agent. A Data Warehouse can be used to measure Key Performance Indicators on the simulation output data of the simulation model, which can be used as a reward criterion for the RL agent. The paper presents a basis for the ongoing development of an RL agent.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {2020–2032},
numpages = {13},
location = {Huntington Beach, California},
series = {WSC '15}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {99-00, 00-01, Deep neural network, Weighted cross-entropy loss, Triplet loss, Deep feature representation, Software defect prediction}
}

@inproceedings{10.1007/978-3-030-67832-6_30,
author = {Wang, Fei and Ding, Youdong and Liang, Huan and Wen, Jing},
title = {Discriminative and Selective Pseudo-Labeling for Domain Adaptation},
year = {2021},
isbn = {978-3-030-67831-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67832-6_30},
doi = {10.1007/978-3-030-67832-6_30},
abstract = {Unsupervised domain adaptation aims to transfer the knowledge of source domain to a related but not labeled target domain. Due to the lack of label information of target domain, most existing methods train a weak classifier and directly apply to pseudo-labeling which may downgrade adaptation performance. To address this problem, in this paper, we propose a novel discriminative and selective pseudo-labeling (DSPL) method for domain adaptation. Specifically, we first match the marginal distributions of two domains and increase inter-class distance simultaneously. Then a feature transformation method is proposed to learn a low-dimensional transfer subspace which is discriminative enough. Finally, after data has formed good clusters, we introduce a structured prediction based selective pseudo-labeling strategy which is able to sufficiently exploit target data structure. We conduct extensive experiments on three popular visual datasets, demonstrating the good domian adaptation performance of our method.},
booktitle = {MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part I},
pages = {365–377},
numpages = {13},
keywords = {Discriminative learned subspace, Pseudo-labeling, Unsupervised domain adaptation},
location = {Prague, Czech Republic}
}

@inproceedings{10.1007/978-3-030-31723-2_54,
author = {Yang, Fengjia and Jiang, Xinghao and Sun, Tanfeng and Xu, Ke},
title = {Gait Recognition with Clothing and Carrying Variations Based on GEI and CAPDS Features},
year = {2019},
isbn = {978-3-030-31722-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31723-2_54},
doi = {10.1007/978-3-030-31723-2_54},
abstract = {Gait recognition is a promising technology in biometrics. The accuracy of gait recognition can be decreased by many interference variations, such as view angle, clothing and carrying. A novel method is proposed based on the Gait Energy Image (GEI) feature and Coordinate-Angle-Position-Distance Skeleton (CAPDS) feature to eliminate the interference of clothing and carrying variations. GEI is a common feature widely used in gait recognition, but it is sensitive to the change of clothing and carrying. The CAPDS proposed in this paper is robust to the clothing and carrying variations. They are fused in backward to complement each other for recognition. Two novel networks, the Paird ResNet (PRN) and the Temporal-Spatial Paired Network (TSPN), are designed to extract the deep features of GEI and CAPDS. The experiments evaluated on the dataset CASIA-B show that the proposed method based on the backward fusion strategy of GEI and CAPDS features can achieve better performance than most methods in gait recognition with clothing and carrying variations.},
booktitle = {Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11, 2019, Proceedings, Part II},
pages = {632–643},
numpages = {12},
keywords = {TSPN, PRN, CAPDS, GEI, Gait recognition},
location = {Xi'an, China}
}

@article{10.1007/s00034-021-01674-0,
author = {Naiemi, Fatemeh and Ghods, Vahid and Khalesi, Hassan},
title = {MOSTL: An Accurate Multi-Oriented Scene Text Localization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {9},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01674-0},
doi = {10.1007/s00034-021-01674-0},
abstract = {Automatic text localization in natural environments is the main element of many applications including self-driving cars, identifying vehicles, and providing scene information to visually impaired people. However, text in the natural and irregular scene has different degrees in orientations, shapes, and colors that make it difficult to detect. In this paper, an accurate multi-oriented scene text localization (MOSTL) is presented to obtain high efficiency of detecting text-based on convolutional neural networks. In the proposed method, an improved ReLU layer (i.ReLU) and an improved inception layer (i.inception) were introduced. Firstly, the proposed structure is used to extract low-level visual features. Then, an extra layer has been used to improve the feature extraction. The i.ReLU and i.inception layers have improved valuable information in text detection. The i.ReLU layers cause to extract some low-level features appropriately. The i.inception layers (specially 3 \texttimes{} 3 convolutions) can obtain broadly varying-sized text more effectively than a linear chain of convolution layer (without inception layers). The output of i.ReLU layers and i.inception layers was fed to an extra layer, which enables MOSTL to detect multi-oriented even curved and vertical texts. We conducted text detection experiments on well-known databases including ICDAR 2019, ICDAR 2017, ICDAR 2015, ICDAR 2003, and MSRA-TD500. MOSTL results yielded performance improvement remarkably.},
journal = {Circuits Syst. Signal Process.},
month = sep,
pages = {4452–4473},
numpages = {22},
keywords = {Curved text, Improved ReLU layer, Improved inception layer, Convolutional neural network, Multi-oriented, Object detection, Scene text localization}
}

@article{10.1016/j.scico.2016.03.009,
author = {\v{S}tuikys, Vytautas and Burbaite, Renata and Bespalova, Kristina and Ziberkas, Giedrius},
title = {Model-driven processes and tools to design robot-based generative learning objects for computer science education},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2016.03.009},
doi = {10.1016/j.scico.2016.03.009},
abstract = {In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes CS learning variability modelling using the feature-based approaches borrowed from the SW engineering domain. Firstly, we define the CS learning domain using the known educational framework TPACK (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the CS learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The GLO design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design GLOs manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the GLO design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of GLOs themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented GLOs as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented GLOs and their supporting tools.},
journal = {Sci. Comput. Program.},
month = nov,
pages = {48–71},
numpages = {24},
keywords = {Model transformation, Generative learning objects (GLOs), GLO design tool, Feature models, Educational robots}
}

@article{10.1016/j.cmpb.2019.105053,
author = {Ferreira, Daniel S. and Ramalho, Geraldo L. B. and Torres, D\'{e}bora and Tobias, Alessandra H. G. and Rezende, Mariana T. and Medeiros, F\'{a}tima N. S. and Bianchi, Andrea G. C. and Carneiro, Cl\'{a}udia M. and Ushizima, Daniela M.},
title = {Saliency-driven system models for cell analysis with deep learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2019.105053},
doi = {10.1016/j.cmpb.2019.105053},
journal = {Comput. Methods Prog. Biomed.},
month = dec,
numpages = {13},
keywords = {65D17, 65D05, 41A10, 41A05, Eye tracking experiment, Cell analysis, Convolutional neural network, Saliency prediction}
}

@inbook{10.5555/3454287.3454826,
author = {Zhang, Jiong and Yu, Hsiang-Fu and Dhillon, Inderjit S.},
title = {AutoAssist: a framework to accelerate training of deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep Neural Networks (DNNs) have yielded superior performance in many contemporary applications. However, the gradient computation in a deep model with millions of instances leads to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement by a stochastic gradient update varies dynamically with the choice of instances in the mini-batch. In AutoAssist, we utilize this fact and design an instance shrinking operation that is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. Specifically, we train a very lightweight Assistant model jointly with the original deep network, which we refer to as the Boss. The Assistant model is designed to gauge the importance of a given instance with respect to the current Boss model such that the shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a non-blocking and asynchronous fashion such that overhead is minimal. To demonstrate the effectiveness of AutoAssist, we conduct experiments on two contemporary applications: image classification using ResNets with varied number of layers, and neural machine translation using LSTMs, ConvS2S and Transformer models. For each application, we verify that AutoAssist leads to significant reduction in training time; in particular, 30% to 40% of the total operation count can be reduced which leads to faster convergence and a corresponding decrease in training time.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {539},
numpages = {11}
}

@article{10.1109/TPAMI.2020.2972281,
author = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
title = {Vision-Language Navigation Policy Learning and Adaptation},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Computer Society},
address = {USA},
volume = {43},
number = {12},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2020.2972281},
doi = {10.1109/TPAMI.2020.2972281},
abstract = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = dec,
pages = {4205–4216},
numpages = {12}
}

@inproceedings{10.1145/3177148.3180085,
author = {Surendranath, Ajay and Jayagopi, Dinesh Babu},
title = {Curriculum Learning for Depth Estimation with Deep Convolutional Neural Networks},
year = {2018},
isbn = {9781450352901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177148.3180085},
doi = {10.1145/3177148.3180085},
abstract = {Curriculum learning is a machine learning technique adapted from the way humans acquire knowledge and skills, initially mastering simple tasks and progressing to more complex tasks. The work explores curriculum training by creating multiple levels of dataset with increasing complexity on which the trainings are performed. The experiments demonstrated that there is an average of 12% improvement test loss when compared to a non-curriculum approach. The experiment also demonstrates the advantage of creating synthetic dataset and how it aids in the overall improvement of accuracy. An improvement of 26% is attained on the test error loss when curriculum trained model was compared to training on a limited real world dataset. The work also goes onto propose a novel learning approach, the Self Paced Learning approach with Error-Diversity (SPL-ED) An overall reduction of 32% in the test loss is observed when compared to the non-curriculum training limited to real-world dataset.},
booktitle = {Proceedings of the 2nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
pages = {95–100},
numpages = {6},
keywords = {Depth Estimation, Curriculum Learning},
location = {Rabat, Morocco},
series = {MedPRAI '18}
}

@inproceedings{10.1007/11492429_48,
author = {Lee, Kisung},
title = {Semantic feature extraction based on video abstraction and temporal modeling},
year = {2005},
isbn = {3540261532},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11492429_48},
doi = {10.1007/11492429_48},
abstract = {This paper presents a novel scheme of object-based video indexing and retrieval based on video abstraction and semantic event modeling. The proposed algorithm consists of three major steps; Video Object (VO) extraction, object-based video abstraction and statistical modeling of semantic features. Semantic feature modeling scheme is based on temporal variation of low-level features in object area between adjacent frames of video sequence. Each semantic feature is represented by a Hidden Markov Model (HMM) which characterizes the temporal nature of VO with various combinations of object features. The experimental results demonstrate the effective performance of the proposed approach.},
booktitle = {Proceedings of the Second Iberian Conference on Pattern Recognition and Image Analysis - Volume Part I},
pages = {392–400},
numpages = {9},
location = {Estoril, Portugal},
series = {IbPRIA'05}
}

@article{10.1016/j.jss.2015.12.024,
author = {Preuveneers, Davy and Heyman, Thomas and Berbers, Yolande and Joosen, Wouter},
title = {Systematic scalability assessment for feature oriented multi-tenant services},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.12.024},
doi = {10.1016/j.jss.2015.12.024},
abstract = {We present tool support and methodology for systematic scalability assessments.Scalar delivers strategic insights for multi-tenant customizable SaaS applications.It measures impact and scalability potential of feature combinations across tenants.Detection of unanticipated feature interactions is demonstrated in e-payment case.Automated scalability analysis is reusable asset in continuous integration process. Recent software engineering paradigms such as software product lines, supporting development techniques like feature modeling, and cloud provisioning models such as platform and infrastructure as a service, allow for great flexibility during both software design and deployment, resulting in potentially large cost savings. However, all this flexibility comes with a catch: as the combinatorial complexity of optional design features and deployment variability increases, the difficulty of assessing system qualities such as scalability and quality of service increases too. And if the software itself is not scalable (for instance, because of a specific set of selected features), deploying additional service instances is a futile endeavor. Clearly there is a need to systematically measure the impact of feature selection on scalability, as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand.In this work, we document our results on systematic load testing for automated quality of service and scalability analysis. The major contribution of our work is tool support and a methodology to analyze the scalability of these distributed, feature oriented multi-tenant software systems in a continuous integration process. We discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions. Additionally, we highlight how our methodology and framework for performance and scalability prediction differs from state-of-practice solutions. We take the viewpoint of both the tenant of the service and the service provider, and report on our experiences applying the approach to an industrial use case in the domain of electronic payments. We conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers, such as the ability to quantify the impact of new features in existing service compositions, and the early detection of hidden feature interactions that may negatively affect the overall performance of multi-tenant services.},
journal = {J. Syst. Softw.},
month = jun,
pages = {162–176},
numpages = {15},
keywords = {Tool support, Scalability, Distributed systems}
}

@article{10.1016/j.dss.2010.12.009,
author = {Ribeiro, Rita A. and Moreira, Ana M. and van den Broek, Pim and Pimentel, Afonso},
title = {Hybrid assessment method for software engineering decisions},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {1},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2010.12.009},
doi = {10.1016/j.dss.2010.12.009},
abstract = {During software development, many decisions need to be made to guarantee the satisfaction of the stakeholders' requirements and goals. The full satisfaction of all of these requirements and goals may not be possible, requiring decisions over conflicting human interests as well as technological alternatives, with an impact on the quality and cost of the final solution. This work aims at assessing the suitability of multi-criteria decision making (MCDM) methods to support software engineers' decisions. To fulfil this aim, a HAM (Hybrid Assessment Method) is proposed, which gives its user the ability to perceive the influence different decisions may have on the final result. HAM is a simple and efficient method that combines one single pairwise comparison decision matrix (to determine the weights of criteria) with one classical weighted decision matrix (to prioritize the alternatives). To avoid consistency problems regarding the scale and the prioritization method, HAM uses a geometric scale for assessing the criteria and the geometric mean for determining the alternative ratings.},
journal = {Decis. Support Syst.},
month = apr,
pages = {208–219},
numpages = {12},
keywords = {Software engineering, Non-functional software requirements, Multi-criteria decision making, Aggregation operators}
}

@article{10.1016/j.cviu.2019.03.006,
author = {Yan, Yichao and Ni, Bingbing and Zhang, Wendong and Tang, Jun and Yang, Xiaokang},
title = {Cross-modality motion parameterization for fine-grained video prediction},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {183},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2019.03.006},
doi = {10.1016/j.cviu.2019.03.006},
journal = {Comput. Vis. Image Underst.},
month = jun,
pages = {11–19},
numpages = {9},
keywords = {Adversarial learning, Cross-modality constraint, Video generation}
}

@article{10.1007/s10846-018-0839-z,
author = {Celemin, Carlos and Ruiz-Del-Solar, Javier},
title = {An Interactive Framework for Learning Continuous Actions Policies Based on Corrective Feedback},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {95},
number = {1},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-018-0839-z},
doi = {10.1007/s10846-018-0839-z},
abstract = {The main goal of this article is to present COACH (COrrective Advice Communicated by Humans), a new learning framework that allows non-expert humans to advise an agent while it interacts with the environment in continuous action problems. The human feedback is given in the action domain as binary corrective signals (increase/decrease the current action magnitude), and COACH is able to adjust the amount of correction that a given action receives adaptively, taking state-dependent past feedback into consideration. COACH also manages the credit assignment problem that normally arises when actions in continuous time receive delayed corrections. The proposed framework is characterized and validated extensively using four well-known learning problems. The experimental analysis includes comparisons with other interactive learning frameworks, with classical reinforcement learning approaches, and with human teleoperators trying to solve the same learning problems by themselves. In all the reported experiments COACH outperforms the other methods in terms of learning speed and final performance. It is of interest to add that COACH has been applied successfully for addressing a complex real-world learning problem: the dribbling of the ball by humanoid soccer players.},
journal = {J. Intell. Robotics Syst.},
month = jul,
pages = {77–97},
numpages = {21},
keywords = {Learning from demonstration, Interactive machine learning, Human teachers, Human feedback, Decision making systems}
}

@article{10.1016/j.neucom.2019.05.009,
author = {Song, Shaoyue and Yu, Hongkai and Miao, Zhenjiang and Guo, Dazhou and Ke, Wei and Ma, Cong and Wang, Song},
title = {An easy-to-hard learning strategy for within-image co-saliency detection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {358},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.009},
doi = {10.1016/j.neucom.2019.05.009},
journal = {Neurocomput.},
month = sep,
pages = {166–176},
numpages = {11},
keywords = {Multiple instance learning, Easy-to-hard learning, Within-image co-saliency}
}

@article{10.1016/j.patcog.2017.10.005,
author = {Zhou, Sanping and Wang, Jinjun and Meng, Deyu and Xin, Xiaomeng and Li, Yubing and Gong, Yihong and Zheng, Nanning},
title = {Deep self-paced learning for person re-identification},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.005},
doi = {10.1016/j.patcog.2017.10.005},
abstract = {We propose a novel deep self-paced learning algorithm to supervise the learning of deep neural network, in which a soft polynomial regularizer term is proposed to gradually involve the faithful samples into training process in a self-paced manner.We optimize the gradient back-propagation of relative distance metric by introducing a symmetric regularizer term, which can convert the back-propagation from the asymmetric mode to a symmetric one.We build an effective part-based deep neural network, in which features of different body parts are first discriminately learned in the convolutional layers and then fused in the fully connected layers. Person re-identification(Re-ID) usually suffers from noisy samples with background clutter and mutual occlusion, which makes it extremely difficult to distinguish different individuals across the disjoint camera views. In this paper, we propose a novel deep self-paced learning(DSPL) algorithm to alleviate this problem, in which we apply a self-paced constraint and symmetric regularization to help the relative distance metric training the deep neural network, so as to learn the stable and discriminative features for person Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive the adaptive weights to samples based on both the training loss and model age. As a result, the high-confidence fidelity samples will be emphasized and the low-confidence noisy samples will be suppressed at early stage of the whole training process. Such a learning regime is naturally implemented under a self-paced learning(SPL) framework, in which samples weights are adaptively updated based on both model age and sample loss using an alternative optimization method. Secondly, we introduce a symmetric regularizer term to revise the asymmetric gradient back-propagation derived by the relative distance metric, so as to simultaneously minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Finally, we build a part-based deep neural network, in which the features of different body parts are first discriminately learned in the lower convolutional layers and then fused in the higher fully connected layers. Experiments on several benchmark datasets have demonstrated the superior performance of our method as compared with the state-of-the-art approaches.},
journal = {Pattern Recogn.},
month = apr,
pages = {739–751},
numpages = {13},
keywords = {Self-paced learning, Person re-identification, Metric learning, Convolutional neural network}
}

@article{10.1007/s11042-019-7251-y,
author = {Mei, Jianhan and Wu, Ziming and Chen, Xiang and Qiao, Yu and Ding, Henghui and Jiang, Xudong},
title = {DeepDeblur: text image recovery from blur to sharp},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7251-y},
doi = {10.1007/s11042-019-7251-y},
abstract = {Digital images could be degraded by a variety of blur during the image acquisition (i.e. relative motion of cameras, electronic noise, capturing defocus, and so on). Blurring images can be computationally modeled as the result of a convolution process with the corresponding blur kernel and thus, image deblurring can be regarded as a deconvolution operation. In this paper, we explore to deblur images by approximating blind deconvolutions using a deep neural network. Different deep neural network structures are investigated to evaluate their deblurring capabilities, which contributes to the optimal design of a network architecture. It is found that shallow and narrow networks are not capable of handling complex motion blur. We thus, present a deep network with 20 layers to cope with text image blur. In addition, a novel network structure with Sequential Highway Connections (SHC) is leveraged to gain superior convergence. The experiment results demonstrate the state-of-the-art performance of the proposed framework with the higher visual quality of the delurred images.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {18869–18885},
numpages = {17},
keywords = {Text Deblurring, Short connection, Convolutional Neural Network (CNN), Blind deconvolution}
}

@inproceedings{10.1007/978-3-030-69532-3_4,
author = {Huang, Bowen and Zhou, Jinjia and Yan, Xiao and Jing, Ming’e and Wan, Rentao and Fan, Yibo},
title = {CS-MCNet: A Video Compressive Sensing Reconstruction Network with Interpretable Motion Compensation},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_4},
doi = {10.1007/978-3-030-69532-3_4},
abstract = {In this paper, a deep neural network with interpretable motion compensation called CS-MCNet is proposed to realize high-quality and real-time decoding of video compressive sensing. Firstly, explicit multi-hypothesis motion compensation is applied in our network to extract correlation information of adjacent frames (as shown in Fig.&nbsp;1), which improves the recover performance. And then, a residual module further narrows down the gap between reconstruction result and original signal. The overall architecture is interpretable by using algorithm unrolling, which brings the benefits of being able to transfer prior knowledge about the conventional algorithms. As a result, a PSNR of 22&nbsp;dB can be achieved at 64x compression ratio, which is about 4% to 9% better than state-of-the-art methods. In addition, due to the feed-forward architecture, the reconstruction can be processed by our network in real time and up&nbsp;to three orders of magnitude faster than traditional iterative methods.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {54–67},
numpages = {14},
location = {Kyoto, Japan}
}

@inproceedings{10.1007/978-3-030-58517-4_21,
author = {Kwon, Heeseung and Kim, Manjin and Kwak, Suha and Cho, Minsu},
title = {MotionSqueeze: Neural Motion Feature Learning for Video Understanding},
year = {2020},
isbn = {978-3-030-58516-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58517-4_21},
doi = {10.1007/978-3-030-58517-4_21},
abstract = {Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding. In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1&nbsp;&amp;&nbsp;V2 datasets.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVI},
pages = {345–362},
numpages = {18},
keywords = {Video understanding, Action recognition, Motion feature learning, Efficient video processing},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/645882.672394,
author = {Simon, Daniel and Eisenbarth, Thomas},
title = {Evolutionary Introduction of Software Product Lines},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines have proved to be a successful and efficient means for managing the development of software in industry. The significant benefits over traditional software architectures have the potential to convince software companies to adopt the product line approach for their existing products. In that case, the question arises how to best convert the existing products into a software product line. For several reasons, an evolutionary approach is desirable. But so far, there is little guidance on the evolutionary introduction of software product lines.In this paper, we propose a lightweight iterative process supporting the incremental introduction of product line concepts for existing software products. Starting with the analysis of the legacy code, we assess what parts of the software can be restructured for product line needs at reasonable costs. For the analysis of the products, we use feature analysis, a reengineering technique tailored to the specific needs of the initiation of software product lines.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {272–282},
numpages = {11},
series = {SPLC 2}
}

@article{10.1007/s11042-019-7310-4,
author = {Nandy, Anup},
title = {Statistical methods for analysis of Parkinson's disease gait pattern and classification},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {14},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7310-4},
doi = {10.1007/s11042-019-7310-4},
abstract = {Understanding the human gait and extracting intrinsic feature helps to classify walking patterns of Parkinson disease patients. The measurement of time series gait pattern is required to detect gait disturbances observed in medical gait data. An attempt is taken to compute Normalized Auto Correlation (NAC) along the temporal axis which calculates the degree of gait fluctuation in control subjects (CO) and Parkinson patient's (PD) gait. In this paper, an underlying statistical analysis is addressed to understand the statistical nature of data. Identifying the proper distribution of these data in advance discards the unwanted information which helps to preserve more informative features. Four different normality testing methods (i.e. W/S, Kolmogorov-Smirnov, Shapiro Wilk and Anderson Darling) are applied to ensure whether the acquired gait data are modelled by a normal distribution. It precludes the costly error during feature analysis to produce the accurate results. A feature selection method, Fisher Discriminant Ratio (FDR) is applied to select most discriminative feature among all the statistical features (i.e. Mean, Median, Mode, Standard Deviation, Variance, Skewness and Kurtosis) derived from both the classes. A probabilistic classifier based on Bayes' theorem demonstrates its efficiency in the classification of Parkinson gait with illustrating statistical error metrics (i.e. MAE, RMSE, MCE, MSE, SEM, SSE etc.).},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {19697–19734},
numpages = {38},
keywords = {Statistical normality testing, Probabilistic classifier, Parkinson disease, Normalized auto correlation, Human gait, Fisher discriminant ratio, Error metrics}
}

@inproceedings{10.1007/978-3-030-69532-3_29,
author = {Priisalu, Maria and Paduraru, Ciprian and Pirinen, Aleksis and Sminchisescu, Cristian},
title = {Semantic Synthesis of Pedestrian Locomotion},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_29},
doi = {10.1007/978-3-030-69532-3_29},
abstract = {We present a model for generating 3d articulated pedestrian locomotion in urban scenarios, with synthesis capabilities informed by the 3d scene semantics and geometry. We reformulate pedestrian trajectory forecasting as a structured reinforcement learning (RL) problem. This allows us to naturally combine prior knowledge on collision avoidance, 3d human motion capture and the motion of pedestrians as observed e.g. in Cityscapes, Waymo or simulation environments like Carla. Our proposed RL-based model allows pedestrians to accelerate and slow down to avoid imminent danger (e.g. cars), while obeying human dynamics learnt from in-lab motion capture datasets. Specifically, we propose a hierarchical model consisting of a semantic trajectory policy network that provides a distribution over possible movements, and a human locomotion network that generates 3d human poses in each step. The RL-formulation allows the model to learn even from states that are seldom exhibited in the dataset, utilizing all of the available prior and scene information. Extensive evaluations using both real and simulated data illustrate that the proposed model is on par with recent models such as S-GAN, ST-GAT and S-STGCNN in pedestrian forecasting, while outperforming these in collision avoidance. We also show that our model can be used to plan goal reaching trajectories in urban scenes with dynamic actors.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {470–487},
numpages = {18},
location = {Kyoto, Japan}
}

@article{10.1016/j.eswa.2009.09.003,
author = {Beg, Azam and Chandana Prasad, P. W.},
title = {Prediction of area and length complexity measures for binary decision diagrams},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.09.003},
doi = {10.1016/j.eswa.2009.09.003},
abstract = {Measuring the complexity of functions that represent digital circuits in non-uniform computation models is an important area of computer science theory. This paper presents a comprehensive set of machine learnt models for predicting the complexity properties of circuits represented by binary decision diagrams. The models are created using Monte Carlo data for a wide range of circuit inputs and number of minterms. The models predict number of nodes as representations of circuit size/area and path lengths: average path length, longest path length, and shortest path length. The models have been validated using an arbitrarily-chosen subset of ISCAS-85 and MCNC-91 benchmark circuits. The models yield reasonably low RMS errors for predictions, so they can be used to estimate complexity metrics of circuits without having to synthesize them.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2864–2873},
numpages = {10},
keywords = {Path length complexity, Neural network modeling, Machine learning, Complexity prediction, Circuit complexity, Binary decision diagrams, Area complexity}
}

@article{10.1007/s10489-020-01921-y,
author = {Wen, Peng and Yuan, Weihua and Qin, Qianqian and Sang, Sheng and Zhang, Zhijun},
title = {Neural attention model for recommendation based on factorization machines},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {4},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01921-y},
doi = {10.1007/s10489-020-01921-y},
abstract = {In recommendation systems, it is of vital importance to comprehensively consider various aspects of information to make accurate recommendations for users. When the low-order feature interactions between items are insufficient, it is necessary to mine information to learn higher-order feature interactions. In addition, to distinguish the different importance levels of feature interactions, larger weights should be assigned to features with larger contributions to predictions, and smaller weights to those with smaller contributions. Therefore, this paper proposes a neural attention model for recommendation (NAM), which deepens factorization machines (FMs) by adding an attention mechanism and fully connected layers. Through the attention mechanism, NAM can learn the different importance levels of low-order feature interactions. By adding fully connected layers on top of the attention component, NAM can model high-order feature interactions in a nonlinear way. Experiments on two real-world datasets demonstrate that NAM has excellent performance and is superior to FM and other state-of-the-art models. The results demonstrate the effectiveness of the proposed model and the potential of using neural networks for prediction under sparse data.},
journal = {Applied Intelligence},
month = apr,
pages = {1829–1844},
numpages = {16},
keywords = {Feature interactions, Attention weight, Factorization machines, Recommendation systems}
}

@inproceedings{10.1007/978-3-030-27544-0_8,
author = {Szemenyei, Marton and Estivill-Castro, Vladimir},
title = {Real-Time Scene Understanding Using Deep Neural Networks for RoboCup SPL},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_8},
doi = {10.1007/978-3-030-27544-0_8},
abstract = {Convolutional neural networks (CNNs) are the state-of-the-art method for most computer vision tasks. But, the deployment of CNNs on mobile or embedded platforms is challenging because of CNNs’ excessive computational requirements. We present an end-to-end neural network solution to scene understanding for robot soccer. We compose two key neural networks: one to perform semantic segmentation on an image, and another to propagate class labels between consecutive frames. We trained our networks on synthetic datasets and fine-tuned them on a set consisting of real images from a Nao robot. Furthermore, we investigate and evaluate several practical methods for increasing the efficiency and performance of our networks. Finally, we present RoboDNN, a C++ neural network library designed for fast inference on the Nao robots.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {96–108},
numpages = {13},
keywords = {Neural networks, Semantic segmentation, Deep learning, Computer vision},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1145/2816795.2818129,
author = {Ladick\'{y}, L'ubor and Jeong, SoHyeon and Solenthaler, Barbara and Pollefeys, Marc and Gross, Markus},
title = {Data-driven fluid simulations using regression forests},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2816795.2818129},
doi = {10.1145/2816795.2818129},
abstract = {Traditional fluid simulations require large computational resources even for an average sized scene with the main bottleneck being a very small time step size, required to guarantee the stability of the solution. Despite a large progress in parallel computing and efficient algorithms for pressure computation in the recent years, realtime fluid simulations have been possible only under very restricted conditions. In this paper we propose a novel machine learning based approach, that formulates physics-based fluid simulation as a regression problem, estimating the acceleration of every particle for each frame. We designed a feature vector, directly modelling individual forces and constraints from the Navier-Stokes equations, giving the method strong generalization properties to reliably predict positions and velocities of particles in a large time step setting on yet unseen test videos. We used a regression forest to approximate the behaviour of particles observed in the large training set of simulations obtained using a traditional solver. Our GPU implementation led to a speed-up of one to three orders of magnitude compared to the state-of-the-art position-based fluid solver and runs in real-time for systems with up to 2 million particles.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {199},
numpages = {9},
keywords = {regression forest, fluid simulation, data-driven}
}

@article{10.1016/j.csl.2012.01.008,
author = {Li, Ming and Han, Kyu J. and Narayanan, Shrikanth},
title = {Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
year = {2013},
issue_date = {January, 2013},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {27},
number = {1},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.01.008},
doi = {10.1016/j.csl.2012.01.008},
abstract = {The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively.},
journal = {Comput. Speech Lang.},
month = jan,
pages = {151–167},
numpages = {17},
keywords = {UBM weight posterior probability supervectors, Sparse representation, Score level fusion, SVM, Prosodic features, Polynomial expansion, Pitch, Maximum likelihood linear regression, Harmonic structure, Gender recognition, GMM, Formant, Age recognition}
}

@article{10.1155/2021/5089236,
author = {Chen, Yu and Tang, Zhong and Ding, Baiyuan},
title = {Research on the Construction of Intelligent Community Emergency Service Platform Based on Convolutional Neural Network},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/5089236},
doi = {10.1155/2021/5089236},
abstract = {Aiming at the shortcomings of the existing community emergency service platform, such as single function, poor scalability, and strong subjectivity, an intelligent community emergency service platform based on convolutional neural network was constructed. Firstly, the requirements analysis of the emergency service platform was carried out, and the functional demand of the emergency service platform was analyzed from the aspects of community environment, safety, infrastructure, health management, emergency response, and so on. Secondly, through logistics network, big data, cloud computing, artificial intelligence, and all kinds of applications, the intelligent community emergency service platform was designed. Finally, a semantic matching emergency question answering system based on convolutional neural network was developed to provide key technical support for the emergency preparation stage of intelligent community. The results show that the intelligent community emergency service platform plays an important role in preventing community emergency events and taking active and effective measures to ensure the health and safety of community residents.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@article{10.1016/j.patcog.2021.107925,
author = {Wu, Lifang and Yang, Zhou and Jian, Meng and Shen, Jialie and Yang, Yuchen and Lang, Xianglong},
title = {Global motion estimation with iterative optimization-based independent univariate model for action recognition},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.107925},
doi = {10.1016/j.patcog.2021.107925},
journal = {Pattern Recogn.},
month = aug,
numpages = {13},
keywords = {Action recognition, Independent univariate global motion model, Iterative optimization, Global motion estimation}
}

@article{10.1007/s11263-019-01278-x,
author = {Ramasinghe, Sameera and Khan, Salman and Barnes, Nick and Gould, Stephen},
title = {Representation Learning on Unit Ball with 3D Roto-translational Equivariance},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {6},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01278-x},
doi = {10.1007/s11263-019-01278-x},
abstract = {Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces—such as a sphere (S2) or a unit ball (B3)—entails unique challenges. In this work, we propose a novel ‘volumetric convolution’ operation that can effectively model and convolve arbitrary functions in B3. We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {1612–1634},
numpages = {23},
keywords = {Deep learning, Zernike polynomials, Volumetric convolution, 3D moments, Convolution neural networks}
}

@article{10.1016/j.compbiomed.2016.01.002,
author = {Bokov, Plamen and Mahut, Bruno and Flaud, Patrice and Delclaux, Christophe},
title = {Wheezing recognition algorithm using recordings of respiratory sounds at the mouth in a pediatric population},
year = {2016},
issue_date = {March 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {70},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.01.002},
doi = {10.1016/j.compbiomed.2016.01.002},
abstract = {BackgroundRespiratory diseases in children are a common reason for physician visits. A diagnostic difficulty arises when parents hear wheezing that is no longer present during the medical consultation. Thus, an outpatient objective tool for recognition of wheezing is of clinical value. MethodWe developed a wheezing recognition algorithm from recorded respiratory sounds with a Smartphone placed near the mouth. A total of 186 recordings were obtained in a pediatric emergency department, mostly in toddlers (mean age 20 months). After exclusion of recordings with artefacts and those with a single clinical operator auscultation, 95 recordings with the agreement of two operators on auscultation diagnosis (27 with wheezing and 68 without) were subjected to a two phase algorithm (signal analysis and pattern classifier using machine learning algorithms) to classify records. ResultsThe best performance (71.4% sensitivity and 88.9% specificity) was observed with a Support Vector Machine-based algorithm. We further tested the algorithm over a set of 39 recordings having a single operator and found a fair agreement (kappa=0.28, CI95% 0.12, 0.45) between the algorithm and the operator. ConclusionsThe main advantage of such an algorithm is its use in contact-free sound recording, thus valuable in the pediatric population. We recorded by Smartphone respiratory sounds at the mouth in pediatric population.Two clinical operators validated the presence or absence of wheezing in 97 toddlers.We used Short-Time Fourier Transform and SVM classifier for wheeze recognition.71.4% Sensitivity and 88.9% Specificity were observed for wheeze detection.An independent test found a fair agreement with a clinical operator.},
journal = {Comput. Biol. Med.},
month = mar,
pages = {40–50},
numpages = {11},
keywords = {Support vector machine, ROC analysis, Childhood asthma, Bronchiolitis, Automated wheezing detection}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@inproceedings{10.5555/3171837.3171839,
author = {Chen, Xinchi and Qiu, Xipeng and Huang, Xuanjing},
title = {A feature-enriched neural model for joint Chinese word segmentation and part-of-speech tagging},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this work, we propose a feature-enriched neural model for joint Chinese word segmentation and part-of-speech tagging task. Specifically, to simulate the feature templates of traditional discrete feature based models, we use different filters to model the complex compositional features with convolutional and pooling layer, and then utilize long distance dependency information with recurrent layer. Experimental results on five different datasets show the effectiveness of our proposed model.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {3960–3966},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1016/j.comnet.2021.108199,
author = {Arce, Pau and Salvo, David and Pi\~{n}ero, Gema and Gonzalez, Alberto},
title = {FIWARE based low-cost wireless acoustic sensor network for monitoring and classification of urban soundscape},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108199},
doi = {10.1016/j.comnet.2021.108199},
journal = {Comput. Netw.},
month = sep,
numpages = {10},
keywords = {Edge computing, FIWARE, Urban sound classification, Acoustic sensor networks}
}

@inproceedings{10.5555/2997189.2997277,
author = {Goodman, Dan F. M. and Brette, Romain},
title = {Learning to localise sounds with spiking neural networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds. We present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems - Volume 1},
pages = {784–792},
numpages = {9},
keywords = {auditory perception &amp; modeling (primary), computational neural models, neuroscience, supervised learning (secondary)},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@article{10.1016/j.image.2019.04.017,
author = {Wu, Hehe and Wang, Anhong and Liang, Jie and Li, Suyue and Li, Peihao},
title = {DCSN-Cast: Deep compressed sensing network for wireless video multicast},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2019.04.017},
doi = {10.1016/j.image.2019.04.017},
journal = {Image Commun.},
month = aug,
pages = {56–67},
numpages = {12},
keywords = {Deep residual network, Compressed sensing, Fully connected network, DCSFCN-cast, DCSRN-cast, DCSN-cast}
}

@inproceedings{10.5555/3540261.3540835,
author = {Yao, Huaxiu and Wang, Yu and Wei, Ying and Zhao, Peilin and Mahdavi, Mehrdad and Lian, Defu and Finn, Chelsea},
title = {Meta-learning with an adaptive task scheduler},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To benefit the learning of a new task, meta-learning has been proposed to transfer a well-generalized meta-model learned from various meta-training tasks. Existing meta-learning algorithms randomly sample meta-training tasks with a uniform probability, under the assumption that tasks are of equal importance. However, it is likely that tasks are detrimental with noise or imbalanced given a limited number of meta-training tasks. To prevent the meta-model from being corrupted by such detrimental tasks or dominated by tasks in the majority, in this paper, we propose an adaptive task scheduler (ATS) for the meta-training process. In ATS, for the first time, we design a neural scheduler to decide which meta-training tasks to use next by predicting the probability being sampled for each candidate task, and train the scheduler to optimize the generalization capacity of the meta-model to unseen tasks. We identify two meta-model-related factors as the input of the neural scheduler, which characterize the difficulty of a candidate task to the meta-model. Theoretically, we show that a scheduler taking the two factors into account improves the meta-training loss and also the optimization landscape. Under the setting of meta-learning with noise and limited budgets, ATS improves the performance on both miniImageNet and a real-world drug discovery benchmark by up to 13% and 18%, respectively, compared to state-of-the-art task schedulers.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {574},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1145/3474085.3475335,
author = {Zhang, Ji and Song, Jingkuan and Yao, Yazhou and Gao, Lianli},
title = {Curriculum-Based Meta-learning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475335},
doi = {10.1145/3474085.3475335},
abstract = {Meta-learning offers an effective solution to learn new concepts with scarce supervision through an episodic training scheme: a series of target-like tasks sampled from base classes are sequentially fed into a meta-learner to extract common knowledge across tasks, which can facilitate the quick acquisition of task-specific knowledge of the target task with few samples. Despite its noticeable improvements, the episodic training strategy samples tasks randomly and uniformly, without considering their hardness and quality, which may not progressively improve the meta-leaner's generalization ability. In this paper, we present a Curriculum-Based Meta-learning (CubMeta) method to train the meta-learner using tasks from easy to hard. Specifically, the framework of CubMeta is in a progressive way, and in each step, we design a module named BrotherNet to establish harder tasks and an effective learning scheme for obtaining an ensemble of stronger meta-learners. In this way, the meta-learner's generalization ability can be progressively improved, and better performance can be obtained even with fewer training tasks. We evaluate our method for few-shot classification on two benchmarks - mini-ImageNet and tiered-ImageNet, where it achieves consistent performance improvements on various meta-learning paradigms.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1838–1846},
numpages = {9},
keywords = {meta-learning, few-shot learning, curriculum learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1007/978-3-030-26250-1_32,
author = {Robin, Jacques and Mazo, Raul and Madeira, Henrique and Barbosa, Raul and Diaz, Daniel and Abreu, Salvador},
title = {A Self-certifiable Architecture for Critical Systems Powered by Probabilistic Logic Artificial Intelligence},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_32},
doi = {10.1007/978-3-030-26250-1_32},
abstract = {We present a versatile architecture for AI-powered self-adaptive self-certifiable critical systems. It aims at supporting semi-automated low-cost re-certification for self-adaptive systems after each adaptation of their behavior to a persistent change in their operational environment throughout their lifecycle.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {391–397},
numpages = {7},
keywords = {Probabilistic logic machine learning, Rule-based constraint solving, Argumentation, Autonomic architecture, AI certification},
location = {Turku, Finland}
}

@article{10.1007/s10994-016-5570-z,
author = {Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
title = {A topological insight into restricted Boltzmann machines},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {104},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-016-5570-z},
doi = {10.1007/s10994-016-5570-z},
abstract = {Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.},
journal = {Mach. Learn.},
month = sep,
pages = {243–270},
numpages = {28},
keywords = {Sparse restricted Boltzmann machines, Small-world networks, Scale-free networks, Deep learning, Complex networks}
}

@inproceedings{10.5555/794190.794609,
author = {Nayar, Shree K. and Baker, Simon and Murase, Hiroshi},
title = {Parametric Feature Detection},
year = {1996},
isbn = {0818672587},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We propose an algorithm to automatically construct feature detectors for arbitrary parametric features. To obtain a high level of robustness we advocate the use of realistic multi-parameter feature models and incorporate optical and sensing effects. Each feature is represented as a densely sampled parametric manifold in a low dimensional subspace of a Hilbert space. During detection, the brightness distribution around each image pixel is projected into the subspace. If the projection lies sufficiently close to the feature manifold, the feature is detected and the location of the closest manifold point yields the feature parameters. The concepts of parameter reduction by normalization, dimension reduction, pattern rejection, and heuristic search are all employed to achieve the required efficiency. By applying the algorithm to appropriate parametric feature models, detectors have been constructed for five features, namely, step edge, roof edge, line, corner, and circular disc. Detailed experiments are reported on the robustness of detection and the accuracy of parameter estimation.},
booktitle = {Proceedings of the 1996 Conference on Computer Vision and Pattern Recognition (CVPR '96)},
pages = {471},
keywords = {sensor effects, optical effects, normalizations, feature modeling, feature manifolds, Parametrized features},
series = {CVPR '96}
}

@article{10.1007/s00521-019-04435-y,
author = {Kolokas, Nikolaos and Drosou, Anastasios and Tzovaras, Dimitrios},
title = {Text synthesis from keywords: a comparison of recurrent-neural-network-based architectures and hybrid approaches},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04435-y},
doi = {10.1007/s00521-019-04435-y},
abstract = {This paper concerns an application of recurrent neural networks to text synthesis in the word level, with the help of keywords. First, a Parts Of Speech tagging library is employed to extract verbs and nouns from the texts used in our work, a part of which are then considered, after automatic eliminations, as the aforementioned keywords. Our ultimate aim is to train a recurrent neural network to map the keyword sequence of a text to the entire text. Successive reformulations of the keyword and full-text word sequences are performed, so that they can serve as the input and target of the network as efficiently as possible. The predicted texts are understandable enough, and the model performance depends on the problem difficulty, determined by the percentage of full-text words that are considered as keywords, that ranges from 1/3 to 1/2 approximately, the training memory cost, mainly affected by the network architecture, as well as the similarity between different texts, which determines the best architecture.},
journal = {Neural Comput. Appl.},
month = may,
pages = {4259–4274},
numpages = {16},
keywords = {Text mining, Natural language processing, Sequence modeling, Deep machine learning}
}

@inproceedings{10.5555/3172077.3172335,
author = {Xu, Zhenghua and Lukasiewicz, Thomas and Chen, Cheng and Miao, Yishu and Meng, Xiangwu},
title = {Tag-aware personalized recommendation using a hybrid deep model},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Recently, many efforts have been put into tag-aware personalized recommendation. However, due to uncontrolled vocabularies, social tags are usually redundant, sparse, and ambiguous. In this paper, we propose a deep neural network approach to solve this problem by mapping the tag-based user and item profiles to an abstract deep feature space, where the deep-semantic similarities between users and their target items (resp., irrelevant items) are maximized (resp., minimized). To ensure the scalability in practice, we further propose to improve this model's training efficiency by using hybrid deep learning and negative sampling. Experimental results show that our approach can significantly outperform the state-of-the-art baselines in tag-aware personalized recommendation (3:8 times better than the best baseline), and that using hybrid deep learning and negative sampling can dramatically enhance the model's training efficiency (hundreds of times quicker), while maintaining similar (and sometimes even better) training quality and recommendation performance.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {3196–3202},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1007/s00521-020-04982-9,
author = {Chang, Y. L. and Chan, C. S. and Remagnino, P.},
title = {Action recognition on continuous video},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04982-9},
doi = {10.1007/s00521-020-04982-9},
abstract = {Video action recognition has been a challenging task over the years. The challenge herein is not only due to the complication in increasing information in videos but also the requirement of an efficient method to retain information over a longer-term where human action would take to perform. This paper proposes a novel framework, named as long-term video action recognition (LVAR) to perform generic action classification in the continuous video.
 The idea of LVAR is introducing a partial recurrence connection to propagate information within every layer of a spatial-temporal network, such as the well-known C3D. Empirically, we show that this addition allows the C3D network to access long-term information, and subsequently improves action recognition performance with videos of different length selected from both UCF101 and miniKinetics datasets.
 Further confirmation of our approach is strengthened with experiments on untrimmed video from the Thumos14 dataset.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {1233–1243},
numpages = {11},
keywords = {Action recognition, Deep learning}
}

@inproceedings{10.1109/MODELS.2017.22,
author = {Taentzer, Gabriele and Salay, Rick and Str\"{u}ber, Daniel and Chechik, Marsha},
title = {Transformations of software product lines: a generalizing framework based on category theory},
year = {2017},
isbn = {9781538634929},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS.2017.22},
doi = {10.1109/MODELS.2017.22},
abstract = {Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.},
booktitle = {Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
pages = {101–111},
numpages = {11},
location = {Austin, Texas},
series = {MODELS '17}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1016/j.infsof.2011.06.002,
author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
title = {A systematic review of software architecture evolution research},
year = {2012},
issue_date = {January, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.002},
doi = {10.1016/j.infsof.2011.06.002},
abstract = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–40},
numpages = {25},
keywords = {Systematic review, Software evolvability, Software architecture, Evolvability analysis, Architecture evolution, Architecture analysis}
}

@article{10.1016/j.cmpb.2009.03.001,
author = {Moca, Vasile V. and Scheller, Bertram and Mure\c{s}an, Raul C. and Daunderer, Michael and Pipa, Gordon},
title = {EEG under anesthesia-Feature extraction with TESPAR},
year = {2009},
issue_date = {September, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {95},
number = {3},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2009.03.001},
doi = {10.1016/j.cmpb.2009.03.001},
abstract = {We investigated the problem of automatic depth of anesthesia (DOA) estimation from electroencephalogram (EEG) recordings. We employed Time Encoded Signal Processing And Recognition (TESPAR), a time-domain signal processing technique, in combination with multi-layer perceptrons to identify DOA levels. The presented system learns to discriminate between five DOA classes assessed by human experts whose judgements were based on EEG mid-latency auditory evoked potentials (MLAEPs) and clinical observations. We found that our system closely mimicked the behavior of the human expert, thus proving the utility of the method. Further analyses on the features extracted by our technique indicated that information related to DOA is mostly distributed across frequency bands and that the presence of high frequencies (&gt;80Hz), which reflect mostly muscle activity, is beneficial for DOA detection.},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
pages = {191–202},
numpages = {12},
keywords = {TESPAR, MLP, MLAEP, EEG, Depth of anesthesia}
}

@inproceedings{10.1609/aaai.v33i01.33015441,
author = {Wu, Pengxiang and Chen, Chao and Yi, Jingru and Metaxas, Dimitris},
title = {Point cloud processing via recurrent set encoding},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015441},
doi = {10.1609/aaai.v33i01.33015441},
abstract = {We present a new permutation-invariant network for 3D point cloud processing. Our network is composed of a recurrent set encoder and a convolutional feature aggregator. Given an unordered point set, the encoder firstly partitions its ambient space into parallel beams. Points within each beam are then modeled as a sequence and encoded into subregional geometric features by a shared recurrent neural network (RNN). The spatial layout of the beams is regular, and this allows the beam features to be further fed into an efficient 2D convolutional neural network (CNN) for hierarchical feature aggregation. Our network is effective at spatial feature learning, and competes favorably with the state-of-the-arts (SOTAs) on a number of benchmarks. Meanwhile, it is significantly more efficient compared to the SOTAs.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {667},
numpages = {9},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1007/s11265-021-01676-w,
author = {Ting, Yu-Ching and Lo, Fang-Wen and Tsai, Pei-Yun},
title = {Implementation for Fetal ECG Detection from Multi-channel Abdominal Recordings with 2D Convolutional Neural Network},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {9},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01676-w},
doi = {10.1007/s11265-021-01676-w},
abstract = {A convolutional neural network (CNN)-based approach for fetal ECG detection from the abdominal ECG recording is proposed. The flow contains a pre-processing phase and a classification phase. In the pre-processing phase, short-time Fourier transform is applied to obtain the spectrogram, which is sent to 2D CNN for classification. The classified results from multiple channels are then fused and high detection accuracy up to 95.2% is achieved and the CNN-based approach outperforms the conventional algorithm. The hardware of this fetal ECG detector composed of the spectrogram processor and 2D CNN classifier is then implemented on the FPGA platform. Because the two dimensions of the spectrogram and the kernel are asymmetric, a pre-fetch mechanism is designed to eliminate the long latency resulted from data buffering for large-size convolution. From the implementation results, it takes 20258 clock cycles for inference and almost 50% computation cycles are reduced. The power consumption is 12.33mW at 324KHz and 1V for real-time operations. The implementation demonstrates the feasibility of real-time applications in wearable devices.},
journal = {J. Signal Process. Syst.},
month = sep,
pages = {1101–1113},
numpages = {13},
keywords = {Short-time Fourier transform, Convolutional neural network, Fetal electrocardiogram}
}

@article{10.1007/s00530-016-0505-x,
author = {Kushwaha, Alok Kumar and Srivastava, Subodh and Srivastava, Rajeev},
title = {Multi-view human activity recognition based on silhouette and uniform rotation invariant local binary patterns},
year = {2017},
issue_date = {July      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {4},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-016-0505-x},
doi = {10.1007/s00530-016-0505-x},
abstract = {This paper addresses the problem of silhouette-based human activity recognition. Most of the previous work on silhouette based human activity recognition focus on recognition from a single view and ignores the issue of view invariance. In this paper, a system framework has been presented to recognize a view invariant human activity recognition approach that uses both contour-based pose features from silhouettes and uniform rotation local binary patterns for view invariant activity representation. The framework is composed of three consecutive modules: (1) detecting and locating people by background subtraction, (2) combined scale invariant contour-based pose features from silhouettes and uniform rotation invariant local binary patterns (LBP) are extracted, and (3) finally classifying activities of people by Multiclass Support vector machine (SVM) classifier. The rotation invariant nature of uniform LBP provides view invariant recognition of multi-view human activities. We have tested our approach successfully in the indoor and outdoor environment results on four multi-view datasets namely: our own view point dataset, VideoWeb Multi-view dataset [28], i3DPost multi-view dataset [29], and WVU multi-view human action recognition dataset [30]. The experimental results show that the proposed method of multi-view human activity recognition is robust, flexible and efficient.},
journal = {Multimedia Syst.},
month = jul,
pages = {451–467},
numpages = {17},
keywords = {Multiclass support vector machine (SVM), Local binary patterns, Human activity recognition, Features extraction}
}

@article{10.1016/j.infsof.2011.09.003,
author = {Conejero, Jos\'{e} M. and Figueiredo, Eduardo and Garcia, Alessandro and Hern\'{a}ndez, Juan and Jurado, Elena},
title = {On the relationship of concern metrics and requirements maintainability},
year = {2012},
issue_date = {February, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.003},
doi = {10.1016/j.infsof.2011.09.003},
abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {212–238},
numpages = {27},
keywords = {Stability, Requirements engineering, Product lines, Maintainability, Crosscutting, Concern metrics}
}

@inproceedings{10.1007/978-3-030-33982-1_3,
author = {Zhao, Yao and Xu, Zhuoming and Hu, Wei},
title = {Leveraging Context Information for Joint Entity and Relation Linking},
year = {2019},
isbn = {978-3-030-33981-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33982-1_3},
doi = {10.1007/978-3-030-33982-1_3},
abstract = {As an important module in most knowledge base question answering (KBQA) systems, entity and relation linking maps proper nouns and relational phrases to corresponding semantic constructs (entities and relations, respectively) in a given KB. Because different entities/relations may have the same mentions, joint disambiguation has been proposed to identify the exact entity/relation from a list of candidates using context information. Existing joint disambiguation methods, like the method in EARL (Entity and Relation Linker), mainly focus on modeling the co-occurrence probabilities of different entities and relations in input questions, while paying little attention to other non-mention expressions (e.g., wh-words). In this paper, we propose the Extended Entity and Relation Linker (EEARL), which leverages full context information to improve linking accuracy. EEARL firstly extracts the context information for each mention and the attribute features for each entity/relation via character-level and word-level LSTMs and constructs context vectors and feature vectors, respectively, and then calculates the similarity between the two vectors to re-score all the candidates. Experimental results on two benchmark datasets (LC-QuAD and QALD) show that EEARL outperforms EARL and several baseline methods in terms of both entity linking and relation linking accuracy.},
booktitle = {Web and Big Data: APWeb-WAIM 2019 International Workshops, KGMA and DSEA, Chengdu, China, August 1–3, 2019, Revised Selected Papers},
pages = {23–36},
numpages = {14},
keywords = {Context information, knowledge base question answering, Joint entity and relation linking, Relation linking, Entity linking},
location = {Chengdu, China}
}

@article{10.1016/j.artint.2019.103181,
author = {Vanzo, Andrea and Croce, Danilo and Bastianelli, Emanuele and Basili, Roberto and Nardi, Daniele},
title = {Grounded language interpretation of robotic commands through structured learning},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {278},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2019.103181},
doi = {10.1016/j.artint.2019.103181},
journal = {Artif. Intell.},
month = jan,
numpages = {21},
keywords = {Human-Robot interaction, Grounded language learning, Automatic interpretation of robotic commands, Spoken language understanding}
}

@inproceedings{10.5555/1888212.1888215,
author = {Perina, Alessandro and Jojic, Nebojsa and Castellani, Umberto and Cristani, Marco and Murino, Vittorio},
title = {Object recognition with hierarchical stel models},
year = {2010},
isbn = {3642155669},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a new generative model, and a new image similarity kernel based on a linked hierarchy of probabilistic segmentations. The model is used to efficiently segment multiple images into a consistent set of image regions. The segmentations are provided at several levels of granularity and links among them are automatically provided. Model training and inference in it is faster than most local feature extraction algorithms, and yet the provided image segmentation, and the segment matching among images provide a rich backdrop for image recognition, segmentation and registration tasks.},
booktitle = {Proceedings of the 11th European Conference on Computer Vision: Part VI},
pages = {15–28},
numpages = {14},
location = {Heraklion, Crete, Greece},
series = {ECCV'10}
}

@article{10.1016/j.patcog.2009.12.012,
author = {Derrac, Joaqu\'{\i}n and Garc\'{\i}a, Salvador and Herrera, Francisco},
title = {IFS-CoCo: Instance and feature selection based on cooperative coevolution with nearest neighbor rule},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.012},
doi = {10.1016/j.patcog.2009.12.012},
abstract = {Feature and instance selection are two effective data reduction processes which can be applied to classification tasks obtaining promising results. Although both processes are defined separately, it is possible to apply them simultaneously. This paper proposes an evolutionary model to perform feature and instance selection in nearest neighbor classification. It is based on cooperative coevolution, which has been applied to many computational problems with great success. The proposed approach is compared with a wide range of evolutionary feature and instance selection methods for classification. The results contrasted through non-parametric statistical tests show that our model outperforms previously proposed evolutionary approaches for performing data reduction processes in combination with the nearest neighbor rule.},
journal = {Pattern Recogn.},
month = jun,
pages = {2082–2105},
numpages = {24},
keywords = {Nearest neighbor, Instance selection, Feature selection, Evolutionary algorithms, Cooperative coevolution}
}

@inproceedings{10.5555/3540261.3542303,
author = {Hahn, Meera and Chaplot, Devendra and Tulsiani, Shubham and Mukadam, Mustafa and Rehg, James M. and Gupta, Abhinav},
title = {No RL, no simulation: learning to navigate without navigating},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future image-based navigation tasks that use RL or Simulation.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2042},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1145/2658761.2658768,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki},
title = {Efficient testing of software product lines via centralization (short paper)},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658768},
doi = {10.1145/2658761.2658768},
abstract = {Software product line~(SPL) engineering manages families of software products that share common features. However, cost-effective test case generation for an SPL is challenging. Applying existing test case generation techniques to each product variant separately may test common code in a redundant way. Moreover, it is difficult to share the test results among multiple product variants. In this paper, we propose the use of centralization, which combines multiple product variants from the same SPL and generates test cases for the entire system. By taking into account all variants, our technique generally avoids generating redundant test cases for common software components. Our case study on three SPLs shows that compared with testing each variant independently, our technique is more efficient and achieves higher test coverage.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {49–52},
numpages = {4},
keywords = {random testing, automatic test generation, Software Product Lines},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@article{10.1504/IJCSE.2017.082882,
title = {Hierarchical regression test case selection using slicing},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {2},
issn = {1742-7185},
url = {https://doi.org/10.1504/IJCSE.2017.082882},
doi = {10.1504/IJCSE.2017.082882},
abstract = {In this paper, we propose a novel regression test case selection approach by decomposing an object-oriented OO program into packages, classes, methods and statements that are affected by some modification made to the program. This decomposition is based on the proposed hierarchical slicing of an OO program. By mapping these decompositions to the existing test suite, we select a new reduced regression test suite and add some new test cases, if necessary, to retest the modified program. We apply hierarchical slicing on a suitable intermediate graph proposed for representing an OO program. This intermediate graph representation corresponds to all the possible dependences among the different parts of an OO program. We improve the scalability of the intermediate graph to a considerable extent by identifying and removing the redundant edges from the graph and thus detect the affected program parts in less time. The average reduction in time achieved for all the ten programs under experimentation is approximately 28.1%. The test cases that cover these affected parts of the program are then selected for regression testing. The average reduction in the number of test cases selected for regression testing of experimental programs is approximately 56.3%.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {179–197},
numpages = {19}
}

@inbook{10.5555/2167748.2167761,
author = {Wojna, Arkadiusz},
title = {Analogy-based reasoning in classifier construction},
year = {2005},
isbn = {3540298304},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Analogy-based reasoning methods in machine learning make it possible to reason about properties of objects on the basis of similarities between objects. A specific similarity based method is the k nearest neighbors (k-nn) classification algorithm. In the k-nn algorithm, a decision about a new object x is inferred on the basis of a fixed number k of the objects most similar to x in a given set of examples. The primary contribution of the dissertation is the introduction of two new classification models based on the k-nn algorithm.The first model is a hybrid combination of the k-nn algorithm with rule induction. The proposed combination uses minimal consistent rules defined by local reducts of a set of examples. To make this combination possible the model of minimal consistent rules is generalized to a metric-dependent form. An effective polynomial algorithm implementing the classification model based on minimal consistent rules has been proposed by Bazan. We modify this algorithm in such a way that after addition of the modified algorithm to the k-nn algorithm the increase of the computation time is inconsiderable. For some tested classification problems the combined model was significantly more accurate than the classical k-nn classification algorithm.For many real-life problems it is impossible to induce relevant global mathematical models from available sets of examples. The second model proposed in the dissertation is a method for dealing with such sets based on locally induced metrics. This method adapts the notion of similarity to the properties of a given test object. It makes it possible to select the correct decision in specific fragments of the space of objects. The method with local metrics improved significantly the classification accuracy of methods with global models in the hardest tested problems.The important issues of quality and efficiency of the k-nn based methods are a similarity measure and the performance time in searching for the most similar objects in a given set of examples, respectively. In this dissertation both issues are studied in detail and some significant improvements are proposed for the similarity measures and for the search methods found in the literature.},
booktitle = {Transactions on Rough Sets IV},
pages = {277–374},
numpages = {98}
}

@inproceedings{10.5555/3540261.3541104,
author = {Cai, Dongqi and Yao, Anbang and Chen, Yurong},
title = {Dynamic normalization and relay for video action recognition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Convolutional Neural Networks (CNNs) have been the dominant model for video action recognition. Due to the huge memory and compute demand, popular action recognition networks need to be trained with small batch sizes, which makes learning discriminative spatial-temporal representations for videos become a challenging problem. In this paper, we present Dynamic Normalization and Relay (DNR), an improved normalization design, to augment the spatial-temporal representation learning of any deep action recognition model, adapting to small batch size training settings. We observe that state-of-the-art action recognition networks usually apply the same normalization parameters to all video data, and ignore the dependencies of the estimated normalization parameters between neighboring frames (at the same layer) and between neighboring layers (with all frames of a video clip). Inspired by this, DNR introduces two dynamic normalization relay modules to explore the potentials of cross-temporal and cross-layer feature distribution dependencies for estimating accurate layer-wise normalization parameters. These two DNR modules are instantiated as a light-weight recurrent structure conditioned on the current input features, and the normalization parameters estimated from the neighboring frames based features at the same layer or from the whole video clip based features at the preceding layers. We first plug DNR into prevailing 2D CNN backbones and test its performance on public action recognition datasets including Kinetics and Something-Something. Experimental results show that DNR brings large performance improvements to the baselines, achieving over 4.4% absolute margins in top-1 accuracy without training bells and whistles. More experiments on 3D backbones and several latest 2D spatial-temporal networks further validate its effectiveness.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {843},
numpages = {15},
series = {NIPS '21}
}

@article{10.1016/j.neucom.2010.12.014,
author = {Schulz, Hannes and M\"{u}ller, Andreas and Behnke, Sven},
title = {Exploiting local structure in Boltzmann machines},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {9},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2010.12.014},
doi = {10.1016/j.neucom.2010.12.014},
abstract = {Restricted Boltzmann machines (RBM) are well-studied generative models. For image data, however, standard RBMs are suboptimal, since they do not exploit the local nature of image statistics. We modify RBMs to focus on local structure by restricting visible-hidden interactions. We model long-range dependencies using direct or indirect lateral interaction between hidden variables. While learning in our model is much faster, it retains generative and discriminative properties of RBMs of similar complexity.},
journal = {Neurocomput.},
month = apr,
pages = {1411–1417},
numpages = {7},
keywords = {Restricted Boltzmann machines, Low-level vision, Generative models}
}

@inproceedings{10.5555/265221.265238,
author = {de Kraker, Klaas Jan and Dohmen, Maurice and Bronsvoort, Willem F.},
title = {Multiple-way feature conversion—opening a view},
year = {1997},
isbn = {041280980X},
publisher = {Chapman &amp; Hall, Ltd.},
address = {GBR},
booktitle = {Proceedings of the Fifth IFIP TC5/WG5.2 International Workshop on Geometric Modeling in Computer Aided Design on Product Modeling for Computer Integrated Design and Manufacture},
pages = {203–212},
numpages = {10},
keywords = {product modeling, feature modeling, feature conversion, concurrent engineering},
location = {Airlie, Virginia, USA},
series = {GMCAD '96}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@inproceedings{10.1145/1173706.1173738,
author = {Czarnecki, Krzysztof and Pietroszek, Krzysztof},
title = {Verifying feature-based model templates against well-formedness OCL constraints},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173738},
doi = {10.1145/1173706.1173738},
abstract = {Feature-based model templates have been recently proposed as a approach for modeling software product lines. Unfortunately, templates are notoriously prone to errors that may go unnoticed for long time. This is because such an error is usually exhibited for some configurations only, and testing all configurations is typically not feasible in practice. In this paper, we present an automated verification procedure for ensuring that no ill-structured template instance will be generated from a correct configuration. We present the formal underpinnings of our proposed approach, analyze its complexity, and demonstrate its practical feasibility through a prototype implementation.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {211–220},
numpages = {10},
keywords = {software-product lines, model-driven development, model templates, metaprogramming, formal verification, feature modeling, feature interaction, configuration, UML, OCL},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@inproceedings{10.5555/3045390.3045598,
author = {Michaeli, Tomer and Wang, Weiran and Livescu, Karen},
title = {Nonparametric canonical correlation analysis},
year = {2016},
publisher = {JMLR.org},
abstract = {Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster's theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1967–1976},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inbook{10.5555/3454287.3454823,
author = {Wang, Siqi and Zeng, Yijie and Liu, Xinwang and Zhu, En and Yin, Jianping and Xu, Chuanfu and Kloft, Marius},
title = {Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite the wide success of deep neural networks (DNN), little progress has been made on end-to-end unsupervised outlier detection (UOD) from high dimensional data like raw images. In this paper, we propose a framework named E3 Outlier, which can perform UOD in a both effective and end-to-end manner: First, instead of the commonly-used autoencoders in previous end-to-end UOD methods, E3 Outlier for the first time leverages a discriminative DNN for better representation learning, by using surrogate supervision to create multiple pseudo classes from original unla-belled data. Next, unlike classic UOD that utilizes data characteristics like density or proximity, we exploit a novel property named inlier priority to enable end-to-end UOD by discriminative DNN. We demonstrate theoretically and empirically that the intrinsic class imbalance of inliers/outliers will make the network prioritize minimizing inliers' loss when inliers/outliers are indiscriminately fed into the network for training, which enables us to differentiate outliers directly from DNN's outputs. Finally, based on inlier priority, we propose the negative entropy based score as a simple and effective outlierness measure. Extensive evaluations show that E3 Outlier significantly advances UOD performance by up to 30% AUROC against state-of-the-art counterparts, especially on relatively difficult benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {536},
numpages = {14}
}

@article{10.1016/j.patcog.2008.05.019,
author = {Farhangfar, Alireza and Kurgan, Lukasz and Dy, Jennifer},
title = {Impact of imputation of missing values on classification error for discrete data},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {12},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.05.019},
doi = {10.1016/j.patcog.2008.05.019},
abstract = {Numerous industrial and research databases include missing values. It is not uncommon to encounter databases that have up to a half of the entries missing, making it very difficult to mine them using data analysis methods that can work only with complete data. A common way of dealing with this problem is to impute (fill-in) the missing values. This paper evaluates how the choice of different imputation methods affects the performance of classifiers that are subsequently used with the imputed data. The experiments here focus on discrete data. This paper studies the effect of missing data imputation using five single imputation methods (a mean method, a Hot deck method, a Nai@?ve-Bayes method, and the latter two methods with a recently proposed imputation framework) and one multiple imputation method (a polytomous regression based method) on classification accuracy for six popular classifiers (RIPPER, C4.5, K-nearest-neighbor, support vector machine with polynomial and RBF kernels, and Nai@?ve-Bayes) on 15 datasets. This experimental study shows that imputation with the tested methods on average improves classification accuracy when compared to classification without imputation. Although the results show that there is no universally best imputation method, Nai@?ve-Bayes imputation is shown to give the best results for the RIPPER classifier for datasets with high amount (i.e., 40% and 50%) of missing data, polytomous regression imputation is shown to be the best for support vector machine classifier with polynomial kernel, and the application of the imputation framework is shown to be superior for the support vector machine with RBF kernel and K-nearest-neighbor. The analysis of the quality of the imputation with respect to varying amounts of missing data (i.e., between 5% and 50%) shows that all imputation methods, except for the mean imputation, improve classification error for data with more than 10% of missing data. Finally, some classifiers such as C4.5 and Nai@?ve-Bayes were found to be missing data resistant, i.e., they can produce accurate classification in the presence of missing data, while other classifiers such as K-nearest-neighbor, SVMs and RIPPER benefit from the imputation.},
journal = {Pattern Recogn.},
month = dec,
pages = {3692–3705},
numpages = {14},
keywords = {Single imputation, Multiple imputations, Missing values, Imputation of missing values, Classification}
}

@article{10.1016/j.ijar.2021.07.015,
author = {Bodewes, Tjebbe and Scutari, Marco},
title = {Learning Bayesian networks from incomplete data with the node-average likelihood},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {138},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.07.015},
doi = {10.1016/j.ijar.2021.07.015},
journal = {Int. J. Approx. Reasoning},
month = nov,
pages = {145–160},
numpages = {16},
keywords = {Incomplete data, Score-based structure learning, Bayesian networks}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Software engineering, Evolution, Automation, Automated production systems}
}

@inproceedings{10.1007/978-3-030-98682-7_6,
author = {Bestmann, Marc and Engelke, Timon and Fiedler, Niklas and G\"{u}ldenstein, Jasper and Gutsche, Jan and Hagge, Jonas and Vahl, Florian},
title = {TORSO-21 Dataset: Typical Objects in&nbsp;RoboCup Soccer 2021},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_6},
doi = {10.1007/978-3-030-98682-7_6},
abstract = {We present a dataset specifically designed to be used as a benchmark to compare vision systems in the RoboCup Humanoid Soccer domain. The dataset is composed of a collection of images taken in various real-world locations as well as a collection of simulated images. It enables comparing vision approaches with a meaningful and expressive metric. The contributions of this paper consist of providing a comprehensive and annotated dataset, an overview of the recent approaches to vision in RoboCup, methods to generate vision training data in a simulated environment, and an approach to increase the variety of a dataset by automatically selecting a diverse set of images from a larger pool. Additionally, we provide a baseline of YOLOv4 and YOLOv4-tiny on this dataset.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {65–77},
numpages = {13},
keywords = {Deep learning, Vision dataset, Computer vision},
location = {Sydney, NSW, Australia}
}

@article{10.1007/s11280-018-0582-1,
author = {Pawar, Karishma and Attar, Vahida},
title = {Deep learning approaches for video-based anomalous activity detection},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0582-1},
doi = {10.1007/s11280-018-0582-1},
abstract = {The pervasive use of cameras at indoor and outdoor premises on account of recording the activities has resulted into deluge of long video data. Such surveillance videos are characterized by single or multiple entities (persons, objects) performing sequential/concurrent activities. It is often interesting to detect suspicious behavior of such entities in an automated manner without any intervention of human personnel, and to this end, anomalous activity detection from surveillance videos is an important research domain in Computer Vision. Detecting the anomalous activities from videos is very challenging due to equivocal nature of anomalies, context at which events took place, lack of ample size of anomalous ground truth training data and also other factors associated with variation in environment conditions, illumination conditions and working status of capturing cameras. Though automated visual surveillance is one of the highly sought-after research domains, use of deep learning techniques for anomalous activity detection is still in nascent stage. Deep learning models like convolution neural networks, auto-encoders, Long Short Term Memory network models have achieved remarkable performance on different domains like image classification, object detection, speech processing, and expediting towards achieving excellence in anomaly detection tasks. This paper aims at studying and analyzing deep learning techniques for video-based anomalous activity detection. As outcome of the study, the graphical taxonomy has been put forth based on kinds of anomalies, level of anomaly detection, and anomaly measurement for anomalous activity detection. The focus has been given on various anomaly detection frameworks having deep learning techniques as their core methodology. Deep learning approaches from both the perspectives of accuracy oriented anomaly detection and real-time processing oriented anomaly detection are compared. This paper also sheds light upon research issues and challenges, application domains, benchmarked datasets and future directions in the domain of deep learning based anomaly detection.},
journal = {World Wide Web},
month = mar,
pages = {571–601},
numpages = {31},
keywords = {video surveillance, real time detection, deep learning, computer vision, anomaly modeling, anomalous activity detection}
}

@inproceedings{10.5555/3305890.3305963,
author = {Pad, Pedram and Salehi, Farnood and Celis, Elisa and Thiran, Patrick and Unser, Michael},
title = {Dictionary learning based on sparse distribution tomography},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new statistical dictionary learning algorithm for sparse signals that is based on an α-stable innovation model. The parameters of the underlying model—that is, the atoms of the dictionary, the sparsity index α and the dispersion of the transform-domain coefficients—are recovered using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm. Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily α-stable. We evaluate our algorithm by performing two types of experiments: image in-painting and image denoising. In both cases, we find that our approach is competitive with state-of-the-art dictionary learning techniques. Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of ℓp-norms that constitutes the foundation of our approach.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2731–2740},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1145/1754288.1754295,
author = {Indukuri, Kishore Varma and Krishna, P. Radha},
title = {Mining e-contract documents to classify clauses},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754295},
doi = {10.1145/1754288.1754295},
abstract = {E-contracts begin as legal documents and end up as processes that help organizations abide by legal rules while fulfilling contract terms. As contracts are complex, their deployment is predominantly established and fulfilled with significant human involvement. One of the key difficulties with any kind of contract processing is the legal ambiguity, which makes it difficult to address any violation of the contract terms. Thus, there is a need to track clauses for the contract activities under execution and violation of clauses. This necessitates deriving clause patterns from e-contract documents and map to their respective activities for further monitoring and fulfillment of e-contracts during their enactment. In this paper, we present a classification approach to extract clause patterns from e-contract documents. This is a challenging task as activities and clauses are mostly derived from both legal and business process driven contract knowledge.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {7},
numpages = {5},
keywords = {text analytics, e-contracts, data mining},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@article{10.1016/j.dsp.2021.103106,
author = {Zhang, Hai and Xie, Qiangqiang and Lu, Bei and Gai, Shan},
title = {Dual attention residual group networks for single image deraining},
year = {2021},
issue_date = {Sep 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103106},
doi = {10.1016/j.dsp.2021.103106},
journal = {Digit. Signal Process.},
month = sep,
numpages = {11},
keywords = {Single image rain removal, Channel attention, Residual groups, Spatial attention}
}

@inproceedings{10.1007/978-3-030-58545-7_8,
author = {Wang, Hu and Wu, Qi and Shen, Chunhua},
title = {Soft Expert Reward Learning for Vision-and-Language Navigation},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_8},
doi = {10.1007/978-3-030-58545-7_8},
abstract = {Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert’s behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {126–141},
numpages = {16},
keywords = {Soft expert distillation, Self perceiving reward, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-grained test case prioritization for integration testing of delta-oriented software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3371158.3371176,
author = {Jayasimha, Aditya and Gangavarapu, Tushaar and Kamath, S. Sowmya and Krishnan, Gokul S.},
title = {Deep Neural Learning for Automated Diagnostic Code Group Prediction Using Unstructured Nursing Notes},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371176},
doi = {10.1145/3371158.3371176},
abstract = {Disease prediction, a central problem in clinical care and management, has gained much significance over the last decade. Nursing notes documented by caregivers contain valuable information concerning a patient's state, which can aid in the development of intelligent clinical prediction systems. Moreover, due to the limited adaptation of structured electronic health records in developing countries, the need for disease prediction from such clinical text has garnered substantial interest from the research community. The availability of large, publicly available databases such as MIMIC-III, and advancements in machine and deep learning models with high predictive capabilities have further facilitated research in this direction. In this work, we model the latent knowledge embedded in the unstructured clinical nursing notes, to address the clinical task of disease prediction as a multi-label classification of ICD-9 code groups. We present EnTAGS, which facilitates aggregation of the data in the clinical nursing notes of a patient, by modeling them independent of one another. To handle the sparsity and high dimensionality of clinical nursing notes effectively, our proposed EnTAGS is built on the topics extracted using Non-negative matrix factorization. Furthermore, we explore the applicability of deep learning models for the clinical task of disease prediction, and assess the reliability of the proposed models using standard evaluation metrics. Our experimental evaluation revealed that the proposed approach consistently exceeded the state-of-the-art prediction model by 1.87% in accuracy, 12.68% in AUPRC, and 11.64% in MCC score.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {152–160},
numpages = {9},
keywords = {Natural Language Processing, Multi-label Classification, Healthcare Analytics, Disease Prediction, Deep Learning, Clinical Decision Support Systems},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@phdthesis{10.5555/AAI28713793,
author = {Palaparthi, Anil Kumar Reddy and A., Weiss, Jeffrey and D., Rabbitt, Richard and II, Dorval, Alan D., and M, Barkmeier-Kraemer, Julie},
advisor = {R, Titze, Ingo},
title = {Computational Motor Learning and Control of the Vocal Source for Voice Production},
year = {2021},
isbn = {9798780644439},
publisher = {The University of Utah},
abstract = {Voice production is a motor skill and requires the coordinated function of many brain regions, namely the brainstem, cerebellum, basal ganglia, diencephalon, and cerebral hemispheres. The vocal system can be subdivided into three major components: lungs, larynx, and vocal tract. Lung pressure drives the airflow in the trachea towards the larynx. The airflow causes the vocal folds in the larynx (vocal source) to oscillate under certain prephonatory conditions, generating audible pulses of airflow into the vocal tract. The vocal tract filters these pulses and radiates the sound into the air. The intrinsic laryngeal muscles play a significant role in voice production. They position the glottis, the space between the vocal folds in several pre-phonatory positions that facilitate vocal fold vibration. The resulting glottal flow is the vocal source. This project aims to develop a control system that controls the vocal source based on four acoustic and four somatosensory features. Nonlinear control theory and artificial neural networks were used to develop the controllers. A voice simulator with a biomechanical model of the vocal system, LeTalker, was used to model the voice production mechanism. In Aim 1, interrelationships between the intrinsic laryngeal muscles and lung pressure in producing various acoustic and somatosensory features during phonation were obtained. In Aim 2, feedforward and feedback controllers based on acoustic and somatosensory features to control the vocal source were developed. In Aim 3, the controllers' sensitivity and performance were assessed using perturbation analysis. The results demonstrated that the control system was able to generate the lung pressure and muscle activations such that the four acoustic and four somatosensory targets were reached with high accuracy. It was observed that for most of the test cases, the control system produces lung pressure and muscle activations that result in phonation that is within ±15 Hz of the targeted fo and ±2 dB of the targeted SPL. The three studies conducted in this dissertation can be a stepping stone to simulate and study various motor disorders related to voice.},
note = {AAI28713793}
}

@inproceedings{10.23919/ICCAS50221.2020.9268247,
author = {Yoo, Hwiyeon and Kim, Nuri and Park, Jeongho and Oh, Songhwai},
title = {Path-Following Navigation Network Using Sparse Visual Memory},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.23919/ICCAS50221.2020.9268247},
doi = {10.23919/ICCAS50221.2020.9268247},
abstract = {Following a demonstration path without observing exact location of an agent is a challenging navigation problem. Especially, considering the probabilistic transition function of the agent makes the problem hard to solve with an exact action decision, so learning-based approaches have been used to solve this task. For example, a previous method by Kumar and Gupta et al., robust path following network (RPF), is a neural-network-based method using visual memories of the demonstration. Although the RPF shows good performances on the path-following task, it does not consider the efficiency of the visual memory since it requires the entire visual memory of the demonstration. In this paper, we propose a path-following network using sparse memory of the demonstration path that can deal with various sparsity of the visual memory. For each time step, the proposed network makes soft attention on the sparse memory to control the agent. We test the proposed model on the Habitat simulator using MatterPort3D dataset with various sparsity of memory. The experimental results show that the proposed method achieves 81.9% of success rate and 73.7% of SPL on a model with 0.8 memory sparsity, and also the results of the models with other memory sparsity achieve reasonable performances compare to the baseline methods.},
booktitle = {2020 20th International Conference on Control, Automation and Systems (ICCAS)},
pages = {883–886},
numpages = {4},
location = {Busan, Korea (South)}
}

@inproceedings{10.5555/3015812.3015844,
author = {Ren, Yafeng and Zhang, Yue and Zhang, Meishan and Ji, Donghong},
title = {Context-sensitive Twitter sentiment classification using neural network},
year = {2016},
publisher = {AAAI Press},
abstract = {Sentiment classification on Twitter has attracted increasing research in recent years. Most existing work focuses on feature engineering according to the tweet content itself. In this paper, we propose a context-based neural network model for Twitter sentiment analysis, incorporating contextualized features from relevant Tweets into the model in the form of word embedding vectors. Experiments on both balanced and unbalanced datasets show that our proposed models outperform the current state-of-the-art.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {215–221},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.4018/IJEIS.2019040104,
author = {Sbai, Hanae and El Faquih, Loubna and Fredj, Mounia},
title = {A Novel Tool for Configurable Process Evolution and Service Derivation},
year = {2019},
issue_date = {Apr 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-1115},
url = {https://doi.org/10.4018/IJEIS.2019040104},
doi = {10.4018/IJEIS.2019040104},
abstract = {In recent years, variability management in business processes is considered a key of reuse. Research works in this field focused mainly on variability modeling and resolution; whereas, evolution has been somehow neglected. In fact, new business requirements may occur, and business processes must evolve in order to meet the new needs. Furthermore, the evolution at business layer represented by configurable processes impact the IT layer represented by services. In this case, it is necessary to synchronize the changes between these two layers. In other words, the alignment of configurable processes and configurable services must occur to maintain an integrated view of an organization. This can be reached by the concept of service-based configurable processes. The study of existing tools in this domain shows the lack of solutions integrating both the evolution management, and the change propagation with respect to the variability. This article aims to represent the CPMEv, a novel tool for evolution management of service-based configurable processes.},
journal = {Int. J. Enterp. Inf. Syst.},
month = apr,
pages = {58–75},
numpages = {18},
keywords = {Variability, Evolution, Configurable Services, Change Propagation, Alignment}
}

@article{10.1016/j.patcog.2007.06.007,
author = {Li, Yun and Wu, Zhong-Fu},
title = {Fuzzy feature selection based on min-max learning rule and extension matrix},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {1},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2007.06.007},
doi = {10.1016/j.patcog.2007.06.007},
abstract = {In many systems, such as fuzzy neural network, we often adopt the language labels (such as large, medium, small, etc.) to split the original feature into several fuzzy features. In order to reduce the computation complexity of the system after the fuzzification of features, the optimal fuzzy feature subset should be selected. In this paper, we propose a new heuristic algorithm, where the criterion is based on min-max learning rule and fuzzy extension matrix is designed as the search strategy. The algorithm is proved in theory and has shown its high performance over several real-world benchmark data sets.},
journal = {Pattern Recogn.},
month = jan,
pages = {217–226},
numpages = {10},
keywords = {Min-max rule, Fuzzy set theory, Feature selection, Extension matrix}
}

@inproceedings{10.1609/aaai.v33i01.33014951,
author = {Shu, Yang and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin},
title = {Transferable curriculum for weakly-supervised domain adaptation},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014951},
doi = {10.1609/aaai.v33i01.33014951},
abstract = {Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that "source-domain engineering" becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weakly-supervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {608},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3448016.3457321,
author = {Cai, Shaofeng and Zheng, Kaiping and Chen, Gang and Jagadish, H. V. and Ooi, Beng Chin and Zhang, Meihui},
title = {ARM-Net: Adaptive Relation Modeling Network for Structured Data},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457321},
doi = {10.1145/3448016.3457321},
abstract = {Relational databases are the de facto standard for storing and querying structured data, and extracting insights from structured data requires advanced analytics. Deep neural networks (DNNs) have achieved super-human prediction performance in particular data types, e.g., images. However, existing DNNs may not produce meaningful results when applied to structured data. The reason is that there are correlations and dependencies across combinations of attribute values in a table, and these do not follow simple additive patterns that can be easily mimicked by a DNN. The number of possible such cross features is combinatorial, making them computationally prohibitive to model. Furthermore, the deployment of learning models in real-world applications has also highlighted the need for interpretability, especially for high-stakes applications, which remains another issue of concern to DNNs. In this paper, we present ARM-Net, an adaptive relation modeling network tailored for structured data, and a lightweight framework ARMOR based on ARM-Net for relational data analytics. The key idea is to model feature interactions with cross features selectively and dynamically, by first transforming the input features into exponential space, and then determining the interaction order and interaction weights adaptively for each cross feature. We propose a novel sparse attention mechanism to dynamically generate the interaction weights given the input tuple, so that we can explicitly model cross features of arbitrary orders with noisy features filtered selectively. Then during model inference, ARM-Net can specify the cross features being used for each prediction for higher accuracy and better interpretability. Our extensive experiments on real-world datasets demonstrate that ARM-Net consistently outperforms existing models and provides more interpretable predictions for data-driven decision making.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {207–220},
numpages = {14},
keywords = {structured data, neural networks, multi-head gated attention, interpretability, feature interaction, feature importance},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1016/j.cmpb.2017.09.011,
author = {Su, Jie and Liu, Shuai and Song, Jinming},
title = {A segmentation method based on HMRF for the aided diagnosis of acute myeloid leukemia},
year = {2017},
issue_date = {December 2017},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2017.09.011},
doi = {10.1016/j.cmpb.2017.09.011},
abstract = {Background and objectivesThe diagnosis of acute myeloid leukemia (AML) is purely dependent on counting the percentages of blasts (&gt;20%) in the peripheral blood or bone marrow. Manual microscopic examination of peripheral blood or bone marrow aspirate smears is time consuming and less accurate. The first and very important step in blast recognition is the segmentation of the cells from the background for further cell feature extraction and cell classification. In this paper, we aimed to utilize computer technologies in image analysis and artificial intelligence to develop an automatic program for blast recognition and counting in the aspirate smears. MethodsWe proposed a method to analyze the aspirate smear images, which first performs segmentation of the cells by k-means cluster, then builds cell image representing model by HMRF (Hidden-Markov Random Field), estimates model parameters through probability of EM (expectation maximization), carries out convergence iteration until optimal value, and finally achieves second stage refined segmentation. Furthermore, the segmentation results are compared with several other methods using six classes of cells respectively. ResultsThe proposed method was applied to six groups of cells from 61 bone marrow aspirate images, and compared with other algorithms for its performance on the analysis of the whole images, the segmentation of nucleus, and the efficiency of calculation. It showed improved segmentation results in both the cropped images and the whole images, which provide the base for down-stream cell feature extraction and identification. ConclusionsSegmentation of the aspirate smear images using the proposed method helps the analyst in differentiating six groups of cells and in the determination of blasts counting, which will be of great significance for the diagnosis of acutemyeloidleukemia.},
journal = {Comput. Methods Prog. Biomed.},
month = dec,
pages = {115–123},
numpages = {9},
keywords = {K-means, HMRF, Expectation Maximization (EM), Cell segmentation, Blast, Acute myeloid leukemia (AML)}
}

@inproceedings{10.1609/aaai.v33i01.3301200,
author = {Liao, Dongliang and Xu, Jin and Li, Gongfu and Huang, Weijie and Liu, Weiqing and Li, Jing},
title = {Popularity prediction on online articles with deep fusion of temporal process and content features},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.3301200},
doi = {10.1609/aaai.v33i01.3301200},
abstract = {Predicting the popularity of online article sheds light to many applications such as recommendation, advertising and information retrieval. However, there are several technical challenges to be addressed for developing the best of predictive capability. (1) The popularity fuctuates under impacts of external factors, which are unpredictable and hard to capture. (2) Content and meta-data features, largely determining the online content popularity, are usually multi-modal and non-trivial to model. (3) Besides, it also needs to figure out how to integrate temporal process and content features modeling for popularity prediction in different lifecycle stages of online articles. In this paper, we propose a Deep Fusion of Temporal process and Content features (DFTC) method to tackle them. For modeling the temporal popularity process, we adopt the recurrent neural network and convolutional neural network. For multi-modal content features, we exploit the hierarchical attention network and embedding technique. Finally, a temporal attention fusion is employed for dynamically integrating all these parts. Using datasets collected from WeChat, we show that the proposed model significantly outperforms state-of-the-art approaches on popularity prediction.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {25},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1145/3169795,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas and Ruan, Wenjie},
title = {Duplicate Detection in Programming Question Answering Communities},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3169795},
doi = {10.1145/3169795},
abstract = {Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage “ranking-classification” problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {37},
numpages = {21},
keywords = {question quality, latent semantics, classification, association rules, Community-based question answering}
}

@article{10.1016/j.eswa.2019.112948,
author = {Chen, Xiaojun and Jia, Shengbin and Xiang, Yang},
title = {A review: Knowledge reasoning over knowledge graph},
year = {2020},
issue_date = {Mar 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {141},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.112948},
doi = {10.1016/j.eswa.2019.112948},
journal = {Expert Syst. Appl.},
month = mar,
numpages = {21},
keywords = {Neural network-based reasoning, Distributed representation-based reasoning, Rule-based reasoning, Reasoning, Knowledge graph}
}

@article{10.1007/s11042-014-2293-7,
author = {Gudigar, Anjan and Chokkadi, Shreesha and U, Raghavendra},
title = {A review on automatic detection and recognition of traffic sign},
year = {2016},
issue_date = {January   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {1},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-014-2293-7},
doi = {10.1007/s11042-014-2293-7},
abstract = {Evidently, Intelligent Transport System (ITS) has progressed tremendously all its way. The core of ITS are detection and recognition of traffic sign, which are designated to fulfill safety and comfort needs of driver. This paper provides a critical review on three major steps in Automatic Traffic Sign Detection and Recognition(ATSDR) system i.e., segmentation, detection and recognition in the context of vision based driver assistance system. In addition, it focuses on different experimental setups of image acquisition system. Further, discussion on possible future research challenges is made to make ATSDR more efficient, which inturn produce a wide range of opportunities for the researchers to carry out the detailed analysis of ATSDR and to incorporate the future aspects in their research.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {333–364},
numpages = {32},
keywords = {Segmentation, Safety, Intelligent transport system, Image acquisition, Detection, Classification}
}

@article{10.1016/j.neucom.2017.07.037,
author = {Li, Honggui and Trocan, Maria},
title = {Deep neural network based single pixel prediction for unified video coding},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {272},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.07.037},
doi = {10.1016/j.neucom.2017.07.037},
abstract = {Classical video prediction methods exploit directly and shallowly the intra-frame, inter-frame and multi-view similarities within the video sequences; the proposed video prediction methods indirectly and intensively transform the frame correlations into nonlinear mappings by using a general deep neural network (DNN) with single output node. Traditional DNN based video prediction algorithms wholly and coarsely forecast the next frame, but the proposed video prediction algorithms severally and precisely anticipate single pixel of future frame in order to achieve high prediction accuracy and low computation cost. First of all, general DNN based prediction algorithms for intra-frame coding, inter-frame coding and multi-view coding are presented respectively. Then, general DNN based prediction algorithm for unified video coding is raised, which relies on the preceding three prediction algorithms. It is evaluated by simulation experiments that the proposed methods hold better performance than state of the art High Efficiency Video Coding (HEVC) in peak signal to noise ratio (PSNR) and bit per pixel (BPP) in the situation of low bitrate transmission. It is also verified by experimental results that the proposed general DNN architecture possesses higher prediction accuracy and lower computation load than those of conventional DNN architectures. It is further testified by experimental results that the proposed methods are very suitable for multi-view videos with small correlations and big disparities.},
journal = {Neurocomput.},
month = jan,
pages = {558–570},
numpages = {13},
keywords = {Video prediction, Unified video coding, Multi-view coding, Intra-frame coding, Inter-frame coding, Deep neural network}
}

@inproceedings{10.1007/978-3-030-29859-3_13,
author = {Bu, Seok-Jun and Cho, Sung-Bae},
title = {Genetic Algorithm-Based Deep Learning Ensemble for Detecting Database Intrusion via Insider Attack},
year = {2019},
isbn = {978-3-030-29858-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29859-3_13},
doi = {10.1007/978-3-030-29859-3_13},
abstract = {A database Intrusion Detection System (IDS) based on Role-based Access Control (RBAC) mechanism that has capability of learning and adaptation learns SQL transaction patterns represented by roles to detect insider attacks. In this paper, we parameterize the rules for partitioning the entire query set into multiple areas with simple chromosomes and propose an ensemble of multiple deep learning models that can effectively model the tree structural characteristics of SQL transactions. Experimental results on a large synthetic query dataset verify that it quantitatively outperforms other ensemble methods and machine learning methods including deep learning models, in terms of 10-fold cross validation and chi-square validation.},
booktitle = {Hybrid Artificial Intelligent Systems: 14th International Conference, HAIS 2019, Le\'{o}n, Spain, September 4–6, 2019, Proceedings},
pages = {145–156},
numpages = {12},
keywords = {Deep learning ensemble, Genetic algorithms, Database intrusion detection, Role-based access control},
location = {Le\'{o}n, Spain}
}

@inproceedings{10.1007/978-3-030-98682-7_11,
author = {Blumenkamp, Jan and Baude, Andreas and Laue, Tim},
title = {Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_11},
doi = {10.1007/978-3-030-98682-7_11},
abstract = {Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {127–139},
numpages = {13},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3297156.3297275,
author = {Zhou, Yue and Ji, Yang and Tian, Chujie},
title = {A Prediction Model of Wechat Official Account's Article Reading Volume},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297275},
doi = {10.1145/3297156.3297275},
abstract = {With the development of WeChat, more and more shopping malls choose WeChat official account to issue preferential information, attract consumers' attention and improve the communication effect. As a reflection of the spread of information in shopping malls, the reading volume of WeChat official account's articles is the goal of this study. Reading volume is affected by many factors. The study discusses two aspects: historical data and the statistical characteristics. Considering the time and space dependence in the articles, this paper proposes a hierarchical Long Short-Term Memory(LSTM) model. The low-level network modeling time dimension features, and the high-level network modeling space dimension feature. The prediction only based on statistical features does not include the historical reading data, it has a good fitting result for the short-term model. The prediction only based on historical data can better predict the long-term growth trend, but loses the influence of the title and other features. In order to solve the limitations of the two methods, this study proposes a hybrid prediction model to predict the reading volume based on historical data and statistical characteristics. Experiments show that the method proposed in this paper has better prediction accuracy.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {548–555},
numpages = {8},
keywords = {reading volume, prediction, WeChat, LSTM},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1007/978-3-030-27544-0_11,
author = {A\c{s}\i{}k, Okan and G\"{o}rer, Binnur and Ak\i{}n, H. Levent},
title = {End-to-End Deep Imitation Learning: Robot Soccer Case Study},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_11},
doi = {10.1007/978-3-030-27544-0_11},
abstract = {In imitation learning, behavior learning is generally done using the features extracted from the demonstration data. Recent deep learning algorithms enable the development of machine learning methods that can get high dimensional data as an input. In this work, we use imitation learning to teach the robot to dribble the ball to the goal. We use B-Human robot software to collect demonstration data and a deep convolutional network to represent the policies. We use top and bottom camera images of the robot as input and speed commands as outputs. The CNN policy learns the mapping between the series of images and speed commands. In 3D realistic robotics simulator experiments, we show that the robot is able to learn to search the ball and dribble the ball, but it struggles to align to the goal. The best-proposed policy model learns to score 4 goals out of 20 test episodes.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {137–149},
numpages = {13},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1007/978-3-030-32248-9_54,
author = {Han, Shuo and Carass, Aaron and Prince, Jerry L.},
title = {Hierarchical Parcellation of the Cerebellum},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_54},
doi = {10.1007/978-3-030-32248-9_54},
abstract = {Parcellation of the cerebellum in an MR image has been used to study regional associations with both motion and cognitive functions. Despite the fact that the division of the cerebellum is defined hierarchically—i.e., the cerebellum can be divided into lobes and the lobes can be further divided into lobules—previous automatic methods to parcellate the cerebellum do not utilize this information. In this work, we propose a method based on convolutional neural networks&nbsp;(CNNs) to explicitly incorporate the hierarchical organization of the cerebellum. The network is constructed in a tree structure with each node representing a cerebellar region and having child nodes that further subdivide the region into finer substructures. Thus, our CNN is aware of the hierarchical organization of the cerebellum. Furthermore, by selecting tree nodes to represent the hierarchical properties of a given training sample, our network can be trained with heterogeneous training data that are labeled to different hierarchical depths. The proposed method was compared with a state-of-the-art cerebellum parcellation network. Our approach shows promising results as a first parcellation method to take the cerebellar hierarchical organization into consideration.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {484–491},
numpages = {8},
location = {Shenzhen, China}
}

@inproceedings{10.1109/ASEW.2008.4686323,
author = {Brcina, Robert and Riebisch, Matthias},
title = {Architecting for evolvability by means of traceability and features},
year = {2008},
isbn = {9781424427765},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASEW.2008.4686323},
doi = {10.1109/ASEW.2008.4686323},
abstract = {The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systems' evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {III–72–III–81},
location = {L'Aquila, Italy},
series = {ASE'08}
}

@inproceedings{10.5555/3045118.3045288,
author = {Shah, Amar and Knowles, David A. and Ghahramani, Zoubin},
title = {An empirical study of stochastic variational algorithms for the beta bernoulli process},
year = {2015},
publisher = {JMLR.org},
abstract = {Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we find that different posterior dependencies are important in BPFA relative to LDA. Particularly, approximations able to model intra-local variable dependence perform best.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1594–1603},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@article{10.1016/j.sigpro.2020.107466,
author = {Zhu, Qi and Xu, Xiangyu and Yuan, Ning and Zhang, Zheng and Guan, Donghai and Huang, Sheng-Jun and Zhang, Daoqiang},
title = {Latent correlation embedded discriminative multi-modal data fusion},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2020.107466},
doi = {10.1016/j.sigpro.2020.107466},
journal = {Signal Process.},
month = jun,
numpages = {11},
keywords = {Sparse representation, Self-paced learning, Classification, Multi-modal data fusion}
}

@inproceedings{10.1111/cgf.14116,
author = {Ghorbani, S. and Wloka, C. and Etemad, A. and Brubaker, M. A. and Troje, N. F.},
title = {Probabilistic character motion synthesis using a hierarchical deep latent variable model},
year = {2020},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.1111/cgf.14116},
doi = {10.1111/cgf.14116},
abstract = {We present a probabilistic framework to generate character animations based on weak control signals, such that the synthesized motions are realistic while retaining the stochastic nature of human movement. The proposed architecture, which is designed as a hierarchical recurrent model, maps each sub-sequence of motions into a stochastic latent code using a variational autoencoder extended over the temporal domain. We also propose an objective function which respects the impact of each joint on the pose and compares the joint angles based on angular distance. We use two novel quantitative protocols and human qualitative assessment to demonstrate the ability of our model to generate convincing and diverse periodic and non-periodic motion sequences without the need for strong control signals.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
articleno = {21},
numpages = {15},
location = {Virtual Event, Canada},
series = {SCA '20}
}

@article{10.1007/s00371-013-0802-8,
author = {Gois, Jo\~{a}o Paulo and Trevisan, Diogo Fernando and Batagelo, Harlen Costa and Mac\^{e}do, Ives},
title = {Generalized Hermitian Radial Basis Functions Implicits from polygonal mesh constraints},
year = {2013},
issue_date = {June      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {6–8},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-013-0802-8},
doi = {10.1007/s00371-013-0802-8},
abstract = {In this work we investigate a generalized interpolation approach using radial basis functions to reconstruct implicit surfaces from polygonal meshes. With this method, the user can define with great flexibility three sets of constraint interpolants: points, normals, and tangents; allowing to balance computational complexity, precision, and feature modeling. Furthermore, this flexibility makes possible to avoid untrustworthy information, such as normals estimated on triangles with bad aspect ratio. We present results of the method for applications related to the problem of modeling 2D curves from polygons and 3D surfaces from polygonal meshes. We also apply the method to problems involving subdivision surfaces and front-tracking of moving boundaries. Finally, as our technique generalizes the recently proposed HRBF Implicits technique, comparisons with this approach are also conducted.},
journal = {Vis. Comput.},
month = jun,
pages = {651–661},
numpages = {11},
keywords = {Surface reconstruction, Radial basis functions, Mesh interpolation, Generalized Hermitian data}
}

@article{10.1016/j.engappai.2021.104473,
author = {Liu, Ze-yu and Liu, Jian-wei and Zuo, Xin and Hu, Ming-fei},
title = {Multi-scale iterative refinement network for RGB-D salient object detection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104473},
doi = {10.1016/j.engappai.2021.104473},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {16},
keywords = {Multi-scale refinement, RGB-D image, Salient object detection}
}

@article{10.1016/j.neucom.2019.04.017,
author = {Lin, Jiatai and Liu, Zhi and Chen, C.L. Philip and Zhang, Yun},
title = {A wavelet broad learning adaptive filter for forecasting and cancelling the physiological tremor in teleoperation},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {356},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.017},
doi = {10.1016/j.neucom.2019.04.017},
journal = {Neurocomput.},
month = sep,
pages = {170–183},
numpages = {14},
keywords = {Incremental learning, Self-paced wavelet auto-encoder(SPWAE), Wavelet broad learning adaptive filter(WBLAF)}
}

@article{10.1016/j.neunet.2006.01.018,
author = {Raudys, \v{S}arnas},
title = {Trainable fusion rules. I. Large sample size case},
year = {2006},
issue_date = {December, 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {19},
number = {10},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2006.01.018},
doi = {10.1016/j.neunet.2006.01.018},
abstract = {A wide selection of standard statistical pattern classification algorithms can be applied as trainable fusion rules while designing neural network ensembles. A focus of the present two-part paper is finite sample effects: the complexity of base classifiers and fusion rules; the type of outputs provided by experts to the fusion rule; non-linearity of the fusion rule; degradation of experts and the fusion rule due to the lack of information in the design set; the adaptation of base classifiers to training set size, etc. In the first part of this paper, we consider arguments for utilizing continuous outputs of base classifiers versus categorical outputs and conclude: if one succeeds in having a small number of expert networks working perfectly in different parts of the input feature space, then crisp outputs may be preferable over continuous outputs. Afterwards, we oppose fixed fusion rules versus trainable ones and demonstrate situations where weighted average fusion can outperform simple average fusion. We present a review of statistical classification rules, paying special attention to these linear and non-linear rules, which are employed rarely but, according to our opinion, could be useful in neural network ensembles. We consider ideal and sample-based oracle decision rules and illustrate characteristic features of diverse fusion rules by considering an artificial two-dimensional (2D) example where the base classifiers perform well in different regions of input feature space.},
journal = {Neural Netw.},
month = dec,
pages = {1506–1516},
numpages = {11},
keywords = {Neural network ensembles, Multiple classifiers systems, Learning set, Generalization error, Fusion, Complexity, Classifier combination}
}

@article{10.1109/TCBB.2019.2961667,
author = {Huang, Hai-Hui and Liang, Yong},
title = {A Novel Cox Proportional Hazards Model for High-Dimensional Genomic Data in Cancer Prognosis},
year = {2019},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2961667},
doi = {10.1109/TCBB.2019.2961667},
abstract = {The Cox proportional hazards model is a popular method to study the connection between feature and survival time. Because of the high-dimensionality of genomic data, existing Cox models trained on any specific dataset often generalize poorly to other independent datasets. In this paper, we suggest a novel strategy for the Cox model. This strategy is included a new learning technique, self-paced learning (SPL), and a new gene selection method, SCAD-Net penalty. The SPL method is adopted to aid to build a more accurate prediction with its built-in mechanism of learning from easy samples first and adaptively learning from hard samples. The SCAD-Net penalty has fixed the problem of the SCAD method without an inherent mechanism to fuse the prior graphical information. We combined the SPL with the SCAD-Net penalty to the Cox model (SSNC). The simulation shows that the SSNC outperforms the benchmark in terms of prediction and gene selection. The analysis of a large-scale experiment across several cancer datasets shows that the SSNC method not only results in higher prediction accuracies but also identifies markers that satisfactory stability across another validation dataset. The demo code for the proposed method is provided in supplemental file.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {1821–1830},
numpages = {10}
}

@inproceedings{10.1145/775047.775083,
author = {Yu, Hwanjo and Han, Jiawei and Chang, Kevin Chen-Chuan},
title = {PEBL: positive example based learning for Web page classification using SVM},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775047.775083},
doi = {10.1145/775047.775083},
abstract = {Web page classification is one of the essential techniques for Web mining. Specifically, classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web. However, constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples. For instance, in order to construct a "homepage" classifier, one needs to collect a sample of homepages (positive examples) and a sample of non-homepages (negative examples). In particular, collecting negative training examples requires arduous work and special caution to avoid biasing them. We introduce in this paper the Positive Example Based Learning (PEBL) framework for Web page classification which eliminates the need for manually collecting negative training examples in pre-processing. We present an algorithm called Mapping-Convergence (M-C) that achieves classification accuracy (with positive and unlabeled data) as high as that of traditional SVM (with positive and negative data). Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM, the M-C algorithm performs as well as traditional SVM.},
booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {239–248},
numpages = {10},
keywords = {unlabeled data, labeled data, SVM (Support Vector Machine), Mapping-Convergence (M-C) algorithm},
location = {Edmonton, Alberta, Canada},
series = {KDD '02}
}

@article{10.1023/A:1008158907779,
author = {Shanahan, J. and Thomas, B. and Mirmehdi, M. and Martin, T. and Campbell, N. and Baldwin, J.},
title = {A Soft Computing Approach to Road Classification},
year = {2000},
issue_date = {December 2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0921-0296},
url = {https://doi.org/10.1023/A:1008158907779},
doi = {10.1023/A:1008158907779},
abstract = {Current learning approaches to computer vision have mainly focussed on low-level image processing and object recognition, while tending to ignore high-level processing such as understanding. Here we propose an approach to object recognition that facilitates the transition from recognition to understanding. The proposed approach embraces the synergistic spirit of soft computing, exploiting the global search powers of genetic programming to determine fuzzy probabilistic models. It begins by segmenting the images into regions using standard image processing approaches, which are subsequently classified using a discovered fuzzy Cartesian granule feature classifier. Understanding is made possible through the transparent and succinct nature of the discovered models. The recognition of roads in images is taken as an illustrative problem in the vision domain. The discovered fuzzy models while providing high levels of accuracy (97%), also provide understanding of the problem domain through the transparency of the learnt models. The learning step in the proposed approach is compared with other techniques such as decision trees, na\"{\i}ve Bayes and neural networks using a variety of performance criteria such as accuracy, understandability and efficiency.},
journal = {J. Intell. Robotics Syst.},
month = dec,
pages = {349–387},
numpages = {39}
}

@article{10.1016/j.chb.2015.12.007,
author = {Xing, Wanli and Chen, Xin and Stein, Jared and Marcinkowski, Michael},
title = {Temporal predication of dropouts in MOOCs},
year = {2016},
issue_date = {May 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2015.12.007},
doi = {10.1016/j.chb.2015.12.007},
abstract = {Massive open online courses (MOOCs) have recently taken center stage in discussions surrounding online education, both in terms of their potential as well as their high dropout rates. The high attrition rates associated with MOOCs have often been described in terms of a scale-efficacy tradeoff. Building from the large numbers associated with MOOCs and the ability to track individual student performance, this study takes an initial step towards a mechanism for the early and accurate identification of students at risk for dropping out. Focusing on struggling students who remain active in course discussion forums and who are already more likely to finish a course, we design a temporal modeling approach, one which prioritizes the at-risk students in order of their likelihood to drop out of a course. In identifying only a small subset of at-risk students, we seek to provide systematic insight for instructors so they may better provide targeted support for those students most in need of intervention. Moreover, we proffer appending historical features to the current week of features for model building and to introduce principle component analysis in order to identify the breakpoint for turning off the features of previous weeks. This appended modeling method is shown to outperform simpler temporal models which simply sum features. To deal with the kind of data variability presented by MOOCs, this study illustrates the effectiveness of an ensemble stacking generalization approach to build more robust and accurate prediction models than the direct application of base learners. Propose a temporal modeling approach for students' dropout behavior in MOOCs.Demonstrate the advantage of appended feature modeling space based on PCA over a summed features modeling space.Explore the power of the ensemble learning method (stacking generalization) in enhancing the prediction ability.},
journal = {Comput. Hum. Behav.},
month = may,
pages = {119–129},
numpages = {11},
keywords = {Stacking, Prediction, MOOC, Learning analytics, Dropout, Algorithm}
}

@phdthesis{10.5555/2518534,
author = {Menon, Aditya Krishna},
advisor = {Elkan, Charles},
title = {Latent feature models for dyadic prediction},
year = {2013},
isbn = {9781267998538},
publisher = {University of California at San Diego},
address = {USA},
abstract = {Following the Netflix prize, the collaborative filtering problem has gained significant attention within machine learning, spawning novel models and theoretical analyses. In parallel, the growth of social media has driven research in link prediction, with the aim of determining whether two individuals in a network are likely to know each other. Both problems involve the prediction of label (star ratings or friendship) between a pair of entities (user-movie or user-user). We call this general problem dyadic prediction. The problem arises in several other guises: predicting student responses to test questions, military disputes between nations, and clickthrough rates of webpages on ads, to name a few. In general, each such domain employs a markedly different approach, obscuring the underlying similarity of the problems being solved. This dissertation aims to explore the use of a single general method, based on latent feature modelling, for generic dyadic prediction problems. To this end, we make three contributions. First, we propose a generic  framework  with which to analyze dyadic prediction problems. This lets one reason about seemingly disparate problems in a unified manner. Second, we propose a  model  based on the log-linear framework, which is applicable to each of the aforementioned problems. The model learns  latent features  from dyadic data, and estimates a probability distribution over labels. Third, we systematically explore  applications  of our latent feature model to domains such as collaborative filtering, link prediction, and clickthrough rate prediction. In all cases, we show performance comparable or superior to existing state-of-the-art methods. For clickthrough rate prediction, ours represents the first application of latent feature modelling to the problem, demonstrating the value in a single framework with which to reason about these problems. We also show that latent feature modelling is scalable to datasets with hundreds of millions of observations on a single machine (the Netflix prize dataset), and hundreds of billions of observations on a small cluster (Yahoo! ad click data). We conclude with a discussion of future research directions, including transferring information from one network to another, and adapting to domains with extreme label sparsity.},
note = {AAI3557100}
}

@inbook{10.5555/3454287.3455259,
author = {Cao, Yuan and Gu, Quanquan},
title = {Generalization bounds of stochastic gradient descent for wide and deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent random feature (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of \~{O}(n-1/2) that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {972},
numpages = {11}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Design diversity, IoT, Internet of things, Software architectures for dynamic environments, Runtime architecture evaluation, Run-time architecture evaluation}
}

@article{10.1016/j.patcog.2017.04.024,
author = {Ren, Jianfeng and Jiang, Xudong},
title = {Regularized 2-D complex-log spectral analysis and subspace reliability analysis of micro-Doppler signature for UAV detection},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {69},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.04.024},
doi = {10.1016/j.patcog.2017.04.024},
abstract = {The proposed 2-D regularized complex-log-Fourier transform better represents mDS.The proposed subspace reliability analysis better removes unreliable dimensions.The proposed approach demonstrates superior performance for UAV detection. Unmanned aerial vehicle (UAV) has become an important radar target recently because of its wide applications and potential security threats. Traditionally, visual features such as spectrogram were often extracted for human operators to identify the micro-Doppler signature (mDS) of UAVs, i.e. sinusoidal modulation. In this paper, the authors aim to design a system for machine automatic classification of UAVs from other targets, particularly from birds as both UAVs and birds are small and slow-moving radar targets. Most existing mDS representations such as spectrogram, cepstrogram and cadence velocity diagram discard the phase spectrum, and only make use of the magnitude spectrum. Whats more, people often take the logarithm of the spectrum to enlarge the weak mDS but without sufficient care, as noise may be enlarged at the same time. The authors thus propose a regularized 2-D complex-log-Fourier transform to address these problems. Furthermore, the authors propose an object-oriented dimension-reduction technique: subspace reliability analysis, which directly removes the unreliable feature dimensions of two class-conditional covariance matrices in two separate subspaces. On the benchmark dataset, the proposed approach demonstrates better performance than the state-of-the-art approaches. More specifically, the proposed approach significantly reduces the equal error rate of the second best approach, cadence velocity diagram, from 6.68% to 3.27%.},
journal = {Pattern Recogn.},
month = sep,
pages = {225–237},
numpages = {13},
keywords = {UAV detection, Subspace reliability analysis, Radar, Micro-Doppler signature, 2-D regularized complex-log-Fourier transform}
}

@article{10.1016/j.cl.2018.04.004,
author = {Zhao, Tian and Huang, Xiaobing},
title = {Design and implementation of DeepDSL: A DSL for deep learning},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.04.004},
doi = {10.1016/j.cl.2018.04.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {39–70},
numpages = {32}
}

@article{10.1016/j.patcog.2019.106972,
author = {Dong, Ganggang and Liu, Hongwei and Kuang, Gangyao and Chanussot, Jocelyn},
title = {Target recognition in SAR images via sparse representation in the frequency domain},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {96},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.106972},
doi = {10.1016/j.patcog.2019.106972},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Target recognition, Transformed domain, Sparse representation}
}

@article{10.1016/j.ins.2019.12.046,
author = {Chen, Dongzi and Yang, Qinli and Liu, Jiaming and Zeng, Zhu},
title = {Selective prototype-based learning on concept-drifting data streams},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {516},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.046},
doi = {10.1016/j.ins.2019.12.046},
journal = {Inf. Sci.},
month = apr,
pages = {20–32},
numpages = {13},
keywords = {99-00, 00-01, Prototype, Classification, Concept drift, Data stream}
}

@inproceedings{10.1007/978-3-030-58571-6_2,
author = {Du, Heming and Yu, Xin and Zheng, Liang},
title = {Learning Object Relation Graph and Tentative Policy for Visual Navigation},
year = {2020},
isbn = {978-3-030-58570-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58571-6_2},
doi = {10.1007/978-3-030-58571-6_2},
abstract = {Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII},
pages = {19–34},
numpages = {16},
keywords = {Visual navigation, Tentative policy learning, Imitation learning, Graph},
location = {Glasgow, United Kingdom}
}

@article{10.5555/1460332.1460337,
author = {Vityaev, E. E. and Lapardin, K. A. and Khomicheva, I. V. and Proskura, A. L.},
title = {Transcription factor binding site recognition by regularity matrices based on the natural classification method},
year = {2008},
issue_date = {December 2008},
publisher = {IOS Press},
address = {NLD},
volume = {12},
number = {5},
issn = {1088-467X},
abstract = {A principally new approach to the classifications of nucleotide sequences based on the "natural" classification concept is proposed. As a result of "natural" classification of the nucleotide sequences, we obtain regularity matrices, where nucleotides are interconnected by regularities. Method, algorithm and software system DNANatClass for performing the "natural" classification have been developed. Experimental results comparing weight matrices with regularity matrices are presented. In this experiment, site recognition by regularity matrices appears to be more accurate than by weight matrixes.},
journal = {Intell. Data Anal.},
month = nov,
pages = {495–512},
numpages = {18}
}

@inproceedings{10.1145/3459637.3481915,
author = {Chen, Bo and Wang, Yichao and Liu, Zhirong and Tang, Ruiming and Guo, Wei and Zheng, Hongkun and Yao, Weiwei and Zhang, Muyu and He, Xiuqiang},
title = {Enhancing Explicit and Implicit Feature Interactions via Information Sharing for Parallel Deep CTR Models},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481915},
doi = {10.1145/3459637.3481915},
abstract = {Effectively modeling feature interactions is crucial for CTR prediction in industrial recommender systems. The state-of-the-art deep CTR models with parallel structure (e.g., DCN) learn explicit and implicit feature interactions through independent parallel networks. However, these models suffer from trivial sharing issues, namely insufficient sharing in hidden layers and excessive sharing in network input, limiting the model's expressiveness and effectiveness. Therefore, to enhance information sharing between explicit and implicit feature interactions, we propose a novel deep CTR model EDCN. EDCN introduces two advanced modules, namely bridge module and regulation module, which work collaboratively to capture the layer-wise interactive signals and learn discriminative feature distributions for each hidden layer of the parallel networks. Furthermore, two modules are lightweight and model-agnostic, which can be generalized well to mainstream parallel deep CTR models. Extensive experiments and studies are conducted to demonstrate the effectiveness of EDCN on two public datasets and one industrial dataset. Moreover, the compatibility of two modules over various parallel-structured models is verified, and they have been deployed onto the online advertising platform in Huawei, where a one-month A/B test demonstrates the improvement over the base parallel-structured model by 7.30% and 4.85% in terms of CTR and eCPM, respectively.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3757–3766},
numpages = {10},
keywords = {neural network, feature interactions, click-through rate prediction},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1007/978-3-030-26142-9_9,
author = {Wang, Yunyun and Zhao, Dan and Li, Yun and Chen, Kejia and Xue, Hui},
title = {The Most Related Knowledge First: A Progressive Domain Adaptation Method},
year = {2019},
isbn = {978-3-030-26141-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26142-9_9},
doi = {10.1007/978-3-030-26142-9_9},
abstract = {In domain adaptation, how to select and transfer related knowledge is critical for learning. Inspired by the fact that human usually transfer from the more related experience to the less related one, in this paper, we propose a novel progressive domain adaptation (PDA) model, which attempts to transfer source knowledge by considering the transfer order based on relevance. Specifically, PDA transfers source instances iteratively from the most related ones to the least related ones, until all related source instances have been adopted. It is an iterative learning process, source instances adopted in each iteration are determined by a gradually annealed weight such that the later iteration will introduce more source instances. Further, a reverse classification performance is used to set the termination of iteration. Experiments on real datasets demonstrate the competiveness of PDA compared with the state-of-arts.},
booktitle = {Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2019 Workshops, BDM, DLKT, LDRC, PAISI, WeL, Macau, China, April 14–17, 2019, Revised Selected Papers},
pages = {90–102},
numpages = {13},
keywords = {Reverse classification, Iteration, Progressive transfer, Domain adaptation},
location = {Macau, China}
}

@inproceedings{10.5555/3540261.3542304,
author = {Zhang, Jiangning and Xu, Chao and Li, Jian and Chen, Wenzhou and Wang, Yabiao and Tai, Ying and Chen, Shuo and Wang, Chengjie and Huang, Feiyue and Liu, Yong},
title = {Analogous to evolutionary algorithm: designing a unified sequence model},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2043},
numpages = {15},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-58539-6_16,
author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
title = {Improving Vision-and-Language Navigation with Image-Text Pairs from the Web},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_16},
doi = {10.1007/978-3-030-58539-6_16},
abstract = {Following a navigation instruction such as ‘Walk down the stairs and stop at the brown sofa’ requires embodied AI agents to ground referenced scene elements referenced (e.g. ‘stairs’) to visual content in the environment (pixels corresponding to ‘stairs’). We ask the following question – can we leverage abundant ‘disembodied’ web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘...stop at the brown sofa’) and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN – outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful – with their combination resulting in further gains.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {259–274},
numpages = {16},
keywords = {Embodied AI, Transfer learning, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/2911451.2911510,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Engineering Quality and Reliability in Technology-Assisted Review},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911510},
doi = {10.1145/2911451.2911510},
abstract = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {75–84},
numpages = {10},
keywords = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1007/978-3-030-78270-2_74,
author = {Yun, Yue and Dai, Huan and Cao, Ruoqi and Zhang, Yupei and Shang, Xuequn},
title = {Self-paced Graph Memory Network for Student GPA Prediction and Abnormal Student Detection},
year = {2021},
isbn = {978-3-030-78269-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78270-2_74},
doi = {10.1007/978-3-030-78270-2_74},
abstract = {Student learning performance prediction (SLPP) is a crucial step in high school education. However, traditional methods fail to consider abnormal students. In this study, we organized every student’s learning data as a graph to use the schema of graph memory networks (GMNs). To distinguish the students and make GMNs learn robustly, we proposed to train GMNs in an “easy-to-hard” process, leading to self-paced graph memory network (SPGMN). SPGMN chooses the low-difficult samples as a batch to tune the model parameters in each training iteration. This approach not only improves the robustness but also rearranges the student sample from normal to abnormal. The experiment results show that SPGMN achieves a higher prediction accuracy and more robustness in comparison with traditional methods. The resulted student sequence reveals the abnormal student has a different pattern in course selection to normal students.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part II},
pages = {417–421},
numpages = {5},
keywords = {Abnormal student detection, Graph memory networks, Self-paced learning, Student learning performance prediction},
location = {Utrecht, The Netherlands}
}

@article{10.1007/s10772-020-09719-6,
author = {Zhang, Hongkai and Wu, Chao and Hao, Jie and Li, Yang},
title = {RETRACTED ARTICLE: Application of semantic speech recognition in designing of robust adaptive model for DFIG wind energy conversion system},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {1},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-020-09719-6},
doi = {10.1007/s10772-020-09719-6},
abstract = {Application of semantic speech recognition in designing of robust adaptive model for the DFIG wind energy conversion system is proposed in this paper. In order to reduce the labor intensity and environmental impact of the monitoring personnel and improve the speed and efficiency of the wind energy censuses, and reduce the cost of wind energy censuses, an intelligent wireless wind energy monitor with the speech analytic framework is designed, which is a convenient solution to these difficulties. In the speech recognition section, we have two major novelties. (1) In order to ensure the accuracy of substitution, this paper uses phonemes as the basic unit for the substitution of domain words. (2) The Euclidean distance in the feature space is equivalent to the cosine distance. In the test phase, complex similarity can be directly used to calculate scores. We use the Labview to implement the system, and the robustness test is done. The expeirment setting is based on the latest methodology. Through the experiment, after comparing with modern state-of-the-art methodologies, the performance of the proposed model is verified.},
journal = {Int. J. Speech Technol.},
month = mar,
pages = {47–56},
numpages = {10},
keywords = {Artificial intelligence, Energy conversion, DFIG, Adaptive model, Speech recognition}
}

@article{10.1016/j.robot.2009.03.006,
author = {Cherubini, A. and Giannone, F. and Iocchi, L. and Lombardo, M. and Oriolo, G.},
title = {Policy gradient learning for a humanoid soccer robot},
year = {2009},
issue_date = {July, 2009},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {57},
number = {8},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2009.03.006},
doi = {10.1016/j.robot.2009.03.006},
abstract = {In humanoid robotic soccer, many factors, both at low-level (e.g., vision and motion control) and at high-level (e.g., behaviors and game strategies), determine the quality of the robot performance. In particular, the speed of individual robots, the precision of the trajectory, and the stability of the walking gaits, have a high impact on the success of a team. Consequently, humanoid soccer robots require fine tuning, especially for the basic behaviors. In recent years, machine learning techniques have been used to find optimal parameter sets for various humanoid robot behaviors. However, a drawback of learning techniques is time consumption: a practical learning method for robotic applications must be effective with a small amount of data. In this article, we compare two learning methods for humanoid walking gaits based on the Policy Gradient algorithm. We demonstrate that an extension of the classic Policy Gradient algorithm that takes into account parameter relevance allows for better solutions when only a few experiments are available. The results of our experimental work show the effectiveness of the policy gradient learning method, as well as its higher convergence rate, when the relevance of parameters is taken into account during learning.},
journal = {Robot. Auton. Syst.},
month = jul,
pages = {808–818},
numpages = {11},
keywords = {Motion control, Machine learning, Humanoid robotics}
}

@inproceedings{10.1109/COMPSAC.2012.95,
author = {Sabouri, Hamideh and Jaghoori, Mohammad Mahdi and Boer, Frank de and Khosravi, Ramtin},
title = {Scheduling and Analysis of Real-Time Software Families},
year = {2012},
isbn = {9780769547367},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2012.95},
doi = {10.1109/COMPSAC.2012.95},
abstract = {A software product line describes explicitly the commonalities of and differences between different products in a family of (software) systems. A formalization of these commonalities and differences amounts to reduced development, analysis and maintenance costs in the practice of software engineering. An important feature common to next-generation real-time software systems is the need of application-level control over scheduling for optimized utilization of resources provided by for example many-core and cloud infrastructures. In this paper, we introduce a formal model of real-time software product lines which supports variability in scheduling policies and rigorous and efficient techniques for modular schedulability analysis.},
booktitle = {Proceedings of the 2012 IEEE 36th Annual Computer Software and Applications Conference},
pages = {680–689},
numpages = {10},
keywords = {Software Product Lines, Real-Time, Formal Methods, Automata Theory, Application-level Scheduling},
series = {COMPSAC '12}
}

@article{10.1016/j.neucom.2019.11.001,
author = {Li, Huafeng and Zhou, Weiyan and Yu, Zhengtao and Yang, Biao and Jin, Huaiping},
title = {Person re-identification with dictionary learning regularized by stretching regularization and label consistency constraint},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {379},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.001},
doi = {10.1016/j.neucom.2019.11.001},
journal = {Neurocomput.},
month = feb,
pages = {356–369},
numpages = {14},
keywords = {Stretch regularization, Label consistency constraint, Dictionary learning, Person re-identification}
}

@article{10.1155/2021/9315700,
author = {Hu, Ming and Ahmed, Syed Hassan},
title = {Decision-Making Model of Product Modeling Big Data Design Scheme Based on Neural Network Optimized by Genetic Algorithm},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/9315700},
doi = {10.1155/2021/9315700},
abstract = {At present, machine learning artificial neural network technology, as one of the core technologies of enterprises, has received unprecedented attention. This technology is widely used in automatic driving, pattern recognition, teaching aid, product modeling, and other fields. According to the development of product design, this paper analyzes the factors that affect the decision-making of product design. The neural network optimized by genetic algorithm is studied, and the technical analysis of neural network algorithm before and after optimization is mainly carried out. The basic process of product modeling design model based on image processing under the background of big data is introduced. The multidirectional group decision-making model of product modeling design scheme in big data cloud environment is constructed. The final decision model can improve the overall design efficiency, shorten the manufacturing period, and provide a new idea for product modeling design.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {8}
}

@article{10.1007/s00779-011-0468-z,
author = {Broek, Egon L. and Sluis, Frans and Dijkstra, Ton},
title = {Cross-validation of bimodal health-related stress assessment},
year = {2013},
issue_date = {February  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {2},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-011-0468-z},
doi = {10.1007/s00779-011-0468-z},
abstract = {This study explores the feasibility of objective and ubiquitous stress assessment. 25 post-traumatic stress disorder patients participated in a controlled storytelling (ST) study and an ecologically valid reliving (RL) study. The two studies were meant to represent an early and a late therapy session, and each consisted of a "happy" and a "stress triggering" part. Two instruments were chosen to assess the stress level of the patients at various point in time during therapy: (i) speech, used as an objective and ubiquitous stress indicator and (ii) the subjective unit of distress (SUD), a clinically validated Likert scale. In total, 13 statistical parameters were derived from each of five speech features: amplitude, zero-crossings, power, high-frequency power, and pitch. To model the emotional state of the patients, 28 parameters were selected from this set by means of a linear regression model and, subsequently, compressed into 11 principal components. The SUD and speech model were cross-validated, using 3 machine learning algorithms. Between 90% (2 SUD levels) and 39% (10 SUD levels) correct classification was achieved. The two sessions could be discriminated in 89% (for ST) and 77% (for RL) of the cases. This report fills a gap between laboratory and clinical studies, and its results emphasize the usefulness of Computer Aided Diagnostics (CAD) for mental health care.},
journal = {Personal Ubiquitous Comput.},
month = feb,
pages = {215–227},
numpages = {13},
keywords = {Validity, Stress, Speech, Post-traumatic stress disorder (PTSD), Machine learning, Computer aided diagnostics (CAD)}
}

@inproceedings{10.1145/3350546.3352496,
author = {Rafailidis, Dimitrios},
title = {Bayesian Deep Learning with Trust and Distrust in Recommendation Systems},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352496},
doi = {10.1145/3350546.3352496},
abstract = {Exploiting the selections of social friends and foes can efficiently face the data scarcity of user preferences and the cold-start problem. In this paper, we present a Social Deep Pairwise Learning model, namely SDPL. According to the Bayesian Pairwise Ranking criterion, we design a loss function with multiple ranking criteria based on the selections of users, and those in their friends and foes to improve the accuracy in the top-k recommendation task. We capture the nonlinearity in user preferences and the social information of trust and distrust relationships by designing a deep learning architecture. In each backpropagation step, we perform social negative sampling to meet the multiple ranking criteria of our loss function. Our experiments on a benchmark dataset from Epinions, among the largest publicly available that has been reported in the relevant literature, demonstrate the effectiveness of the proposed approach, outperforming other state-of-the art methods. In addition, we show that our deep learning strategy plays an important role in capturing the nonlinear associations between user preferences and the social information of trust and distrust relationships, and demonstrate that our social negative sampling strategy is a key factor in SDPL.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {18–25},
numpages = {8},
keywords = {social relationships, deep learning, collaborative filtering, Pairwise learning},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1007/978-3-030-89370-5_18,
author = {Tian, Yuze and Zhong, Xian and Liu, Wenxuan and Jia, Xuemei and Zhao, Shilei and Ye, Mang},
title = {Random Walk Erasing with Attention Calibration for Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_18},
doi = {10.1007/978-3-030-89370-5_18},
abstract = {Action recognition in videos has attracted growing research interests because of the explosive surveillance data in social security applications. In this process, due to the distraction and deviation of the network caused by occlusions, human action features usually suffer different degrees of performance degradation. Considering the occlusion scene in the wild, we find that the occluded objects usually move unpredictably but continuously. Thus, we propose a random walk erasing with attention calibration (RWEAC) for action recognition. Specifically, we introduce the random walk erasing (RWE) module to simulate the unknown occluded real conditions in frame sequence, expanding the diversity of data samples. In the case of erasing (or occlusion), the attention area is sparse. We leverage the attention calibration (AC) module to force the attention to stay stable in other regions of interest. In short, our novel RWEAC network enhances the ability to learn comprehensive features in a complex environment and make the feature representation robust. Experiments are conducted on the challenging video action recognition UCF101 and HMDB51 datasets. The extensive comparison results and ablation studies demonstrate the effectiveness and strength of the proposed method.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {236–251},
numpages = {16},
keywords = {Siamese network, Attention calibration, Data augmentation, Random walk erasing, Action recognition},
location = {Hanoi, Vietnam}
}

@article{10.1016/j.neucom.2008.04.038,
author = {Zhou, Juan and Rajapakse, Jagath C.},
title = {Fuzzy approach to incorporate hemodynamic variability and contextual information for detection of brain activation},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {71},
number = {16–18},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2008.04.038},
doi = {10.1016/j.neucom.2008.04.038},
abstract = {We propose to use fuzzy c-means clustering with contextual modeling on features extracted from fMRI data for detection of brain activation. Five discriminating features are extracted from fMRI data by using a sequence of temporal-sliding-windows. Fuzzy membership maps of individual subjects obtained through clustering with spatial regularization is capable of taking into account both hemodynamic variability and contextual information of brain activation. The present method outperforms statistical parametric mapping (SPM) approach on experiments with synthetic fMRI data contaminated by both independent and correlated noise. Performance on real fMRI data are comparable to those obtained with SPM.},
journal = {Neurocomput.},
month = oct,
pages = {3184–3192},
numpages = {9},
keywords = {fMRI, Individual variability, Fuzzy c-means clustering, Feature extraction, Contextual modeling, Brain activation}
}

@inproceedings{10.1145/304012.304033,
author = {Khodakovsky, Andrei and Schr\"{o}der, Peter},
title = {Fine level feature editing for subdivision surfaces},
year = {1999},
isbn = {1581130805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304012.304033},
doi = {10.1145/304012.304033},
booktitle = {Proceedings of the Fifth ACM Symposium on Solid Modeling and Applications},
pages = {203–211},
numpages = {9},
keywords = {surface deformation, subdivision, multiresolution editing, hierarchical modelling, feature modeling},
location = {Ann Arbor, Michigan, USA},
series = {SMA '99}
}

@inproceedings{10.1145/3460418.3479281,
author = {Adhikari, Aakriti and Sur, Sanjib},
title = {MilliPose: Facilitating Full Body Silhouette Imaging from Millimeter-Wave Device},
year = {2021},
isbn = {9781450384612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460418.3479281},
doi = {10.1145/3460418.3479281},
abstract = {This paper proposes MilliPose, a system that facilitates full human body silhouette imaging and 3D pose estimation from millimeter-wave (mmWave) devices. Unlike existing vision-based motion capture systems, MilliPose is not privacy-invasive and is capable of working under obstructions, poor visibility, and low light conditions. MilliPose leverages machine-learning models based on conditional Generative Adversarial Networks and Recurrent Neural Network to solve the challenges of poor resolution, specularity, and variable reflectivity with existing mmWave imaging systems. Our preliminary results show the efficacy of MilliPose in accurately predicting body joint locations under natural human movement.},
booktitle = {Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers},
pages = {1–3},
numpages = {3},
keywords = {Recurrent Neural Network, Millimeter-Wave, Conditional Generative Adversarial Networks},
location = {Virtual, USA},
series = {UbiComp/ISWC '21 Adjunct}
}

@article{10.1016/j.dsp.2018.12.007,
author = {Yue, Guanghui and Hou, Chunping and Yan, Weiqing and Choi, Lark Kwon and Zhou, Tianwei and Hou, Yonghong},
title = {Blind quality assessment for screen content images via convolutional neural network},
year = {2019},
issue_date = {Aug 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2018.12.007},
doi = {10.1016/j.dsp.2018.12.007},
journal = {Digit. Signal Process.},
month = aug,
pages = {21–30},
numpages = {10},
keywords = {Convolutional neural network (CNN), Image quality assessment (IQA), Blind/no reference (NR), Screen content image (SCI)}
}

@article{10.5555/3135535.3135553,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with neurological diseases using PCA and NPCA},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
abstract = {In this study, we wanted to discriminate between 30 patients who suffer from Parkinson's disease (PD) and 20 patients with other neurological diseases (ND). All participants were asked to pronounce sustained vowel /a/ hold as long as possible at comfortable level. The analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used principal component analysis (PCA) and nonlinear PCA (NPCA). These techniques reduce the number of parameters and select the most effective ones to be used for classification. Support vector machine and k-nearest neighbor with different kernels was used for classification. We obtained accuracy up to 88% for discrimination between PD patients ND patients using KNN with k equal to three and five.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {673–683},
numpages = {11},
keywords = {SVM, PCA, Neurological diseases, NPCA, KNN, Feature selection}
}

@inproceedings{10.1145/3387940.3392089,
author = {Ahlgren, John and Berezin, Maria Eugenia and Bojarczuk, Kinga and Dulskyte, Elena and Dvortsova, Inna and George, Johann and Gucevska, Natalija and Harman, Mark and L\"{a}mmel, Ralf and Meijer, Erik and Sapora, Silvia and Spahr-Summers, Justin},
title = {WES: Agent-based User Interaction Simulation on Real Infrastructure},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392089},
doi = {10.1145/3387940.3392089},
abstract = {We introduce the Web-Enabled Simulation (WES) research agenda, and describe FACEBOOK's WW system. We describe the application of WW to reliability, integrity and privacy at FACEBOOK1, where it is used to simulate social media interactions on an infrastructure consisting of hundreds of millions of lines of code. The WES agenda draws on research from many areas of study, including Search Based Software Engineering, Machine Learning, Programming Languages, Multi Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We conclude with a set of open problems and research challenges to motivate wider investigation.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {276–284},
numpages = {9},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1145/3337798,
author = {Jiang, Zhe and Sainju, Arpan Man and Li, Yan and Shekhar, Shashi and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337798},
doi = {10.1145/3337798},
abstract = {Class ambiguity refers to the phenomenon whereby similar features correspond to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. The problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost. Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble) and thus cannot effectively minimize class ambiguity. In contrast, we propose a spatial ensemble framework that explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. We further extend our spatial ensemble learning framework with spatial dependency between nearby zones based on the spatial autocorrelation effect. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {43},
numpages = {25},
keywords = {spatial heterogeneity, spatial ensemble, local models, class ambiguity, Spatial classification}
}

@inproceedings{10.5555/3495724.3496081,
author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
title = {Object goal navigation using goal-oriented semantic exploration},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, 'Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {357},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1109/IROS51168.2021.9636743,
author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv},
title = {Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636743},
doi = {10.1109/IROS51168.2021.9636743},
abstract = {We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {1562–1569},
numpages = {8},
location = {Prague, Czech Republic}
}

@inproceedings{10.1145/2984751.2985721,
author = {Yokota, Tomohiro and Hashida, Tomoko},
title = {Hand Gesture and On-body Touch Recognition by Active Acoustic Sensing throughout the Human Body},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985721},
doi = {10.1145/2984751.2985721},
abstract = {In this paper, we present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around the periphery of the wrist. Our approach can recognize hand gestures and on-body touch concurrently in real-time and is expected to obtain rich input variations by combining them. We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on the forearm, and touches on the back of the hand.},
booktitle = {Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},
pages = {113–115},
numpages = {3},
keywords = {on-body touch, machine learning, hand gestures, combined input, acoustic sensing},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@article{10.1007/s10664-021-09964-6,
author = {Duchien, Laurence and Gr\"{u}nbacher, Paul and Th\"{u}m, Thomas},
title = {Foreword to the Special Issue on Configurable Systems},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09964-6},
doi = {10.1007/s10664-021-09964-6},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {3}
}

@article{10.1007/s10664-008-9094-4,
author = {Lee, Jihyun and Kang, Sungwon and Kim, Chang-Ki},
title = {Software architecture evaluation methods based on cost benefit analysis and quantitative decision making},
year = {2009},
issue_date = {August    2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9094-4},
doi = {10.1007/s10664-008-9094-4},
abstract = {Since many parts of the architecture evaluation steps of the Cost Benefit Analysis Method (CBAM) depend on the stakeholders' empirical knowledge and intuition, it is very important that such an architecture evaluation method be able to faithfully reflect the knowledge of the experts in determining Architectural Strategy (AS). However, because CBAM requires the stakeholders to make a consensus or vote for collecting data for decision making, it is difficult to accurately reflect the stakeholders' knowledge in the process. In order to overcome this limitation of CBAM, we propose the two new CBAM-based methods for software architecture evaluation, which respectively adopt the Analytic Hierarchy Process (AHP) and the Analytic Network Process (ANP). Since AHP and ANP use pair-wise comparison they are suitable for a cost and benefit analysis technique since its purpose is not to calculate correct values of benefit and cost but to decide AS with highest return on investment. For that, we first define a generic process of CBAM and develop variations from the generic process by applying AHP and ANP to obtain what we call the CBAM+AHP and CBAM+ANP methods. These new methods not only reflect the knowledge of experts more accurately but also reduce misjudgments. A case study comparison of CBAM and the two new methods is conducted using an industry software project. Because the cost benefit analysis process that we present is generic, new cost benefit analysis techniques with capabilities and characteristics different from the three methods we examine here can be derived by adopting various different constituent techniques.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {453–475},
numpages = {23},
keywords = {Software architecture evaluation, CBAM, ANP, AHP}
}

@inproceedings{10.1145/3474085.3475546,
author = {Yang, Shanmin and Yang, Xiao and Lin, Yi and Cheng, Peng and Zhang, Yi and Zhang, Jianwei},
title = {Heterogeneous Face Recognition with Attention-guided Feature Disentangling},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475546},
doi = {10.1145/3474085.3475546},
abstract = {This paper proposes an attention-guided feature disentangling framework (AgFD) to eliminate the large cross-modality discrepancy for Heterogeneous Face Recognition (HFR). Existing HFR methods either focus only on extracting identity features or impose linear/no independence constraints on the decomposed components. Instead, our AgFD disentangles the facial representation and forces intrinsic independence between identity features and identity-irrelevant variations. To this end, an Attention-based Residual Decomposition Module (AbRDM) and an Adversarial Decorrelation Module (ADM) are presented. AbRDM provides hierarchical complementary feature disentanglement, while ADM is introduced for decorrelation learning. Extensive experiments on the challenging CASIA NIR-VIS 2.0 Database, Oulu-CASIA NIR&amp;VIS Database, BUAA-VisNir Database, and IIIT-D Viewed Sketch Database demonstrate the generalization ability and competitive performance of the proposed method.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4137–4145},
numpages = {9},
keywords = {self-attention, heterogeneous face recognition, disentangled representation learning, de-correlation},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1007/s11634-016-0266-6,
author = {Hayashi, Kenichi},
title = {Asymptotic comparison of semi-supervised and supervised linear discriminant functions for heteroscedastic normal populations},
year = {2018},
issue_date = {June      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {2},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-016-0266-6},
doi = {10.1007/s11634-016-0266-6},
abstract = {It has been reported that using unlabeled data together with labeled data to construct a discriminant function works successfully in practice. However, theoretical studies have implied that unlabeled data can sometimes adversely affect the performance of discriminant functions. Therefore, it is important to know what situations call for the use of unlabeled data. In this paper, asymptotic relative efficiency is presented as the measure for comparing analyses with and without unlabeled data under the heteroscedastic normality assumption. The linear discriminant function maximizing the area under the receiver operating characteristic curve is considered. Asymptotic relative efficiency is evaluated to investigate when and how unlabeled data contribute to improving discriminant performance under several conditions. The results show that asymptotic relative efficiency depends mainly on the heteroscedasticity of the covariance matrices and the stochastic structure of observing the labels of the cases.},
journal = {Adv. Data Anal. Classif.},
month = jun,
pages = {315–339},
numpages = {25},
keywords = {Semi-supervised learning, Receiver operating characteristic curve, Missing data, Linear discriminant function, Labeling mechanism, Area under the ROC curve, 68T10, 62H30, 62G20}
}

@article{10.5555/1718098.1718104,
author = {El-Raouf, Amal Abd},
title = {Hierarchical clustering of distributed object-oriented software systems: a generic solution for software-hardware mismatch problem},
year = {2009},
issue_date = {November 2009},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {8},
number = {11},
issn = {1109-2750},
abstract = {During the software lifecycle, the software structure is subject to many changes in order to fulfill the customer's requirements. In Distributed Object Oriented systems, software engineers face many challenges to solve the software-hardware mismatch problem in which the software structure does not match the customer's underlying hardware. A major design problem of Object Oriented software systems is the efficient distribution of software classes among the different nodes in the system while maintaining two features: low-coupling and high software quality. In this paper, we present a new methodology for efficiently restructuring Distributed Object Oriented software systems to improve the overall system performance and to solve the softwarehardware mismatch problem. Our method has two main phases. In the first phase, we use the hierarchical clustering method to restructure the target software application. As a result, all the possible clustering solutions that could be applied to the target software application are generated. In the second phase, we decide on the best-fit clustering solution according to the customer hardware organization.},
journal = {W. Trans. on Comp.},
month = nov,
pages = {1780–1789},
numpages = {10},
keywords = {software restructuring, performance analysis, object oriented software, low coupling, hierarchical clustering, distributed systems}
}

@article{10.1145/3478093,
author = {Zhang, Qian and Wang, Dong and Zhao, Run and Yu, Yinggang and Shen, Junjie},
title = {Sensing to Hear: Speech Enhancement for Mobile Devices Using Acoustic Signals},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478093},
doi = {10.1145/3478093},
abstract = {Voice interactions and voice messages on mobile phones are rapidly growing in popularity. However, the user experience of these services is still worse than desired in noisy environments, especially in multi-talker scenarios, where the phone can only provide low-quality voice recordings. Speech enhancement using only audio as the input remains a grand challenge in these scenarios. In this paper, we handle this with the help of the emerging acoustic sensing technology. The key insight is that the inaudible acoustic signals emitted by speakers of phones can capture the subtle lip movements when people speak. Instead of enabling lip reading for the classification of limited voice commands, we further unlock the potential of acoustic sensing and leverage the captured lip information to improve the voice recording quality. We propose WaveVoice, a joint audio-sensory deep learning method for end-to-end speech enhancement on mobile phones. The model of WaveVoice is structured as an encoder-decoder network, in which audio and acoustic sensing data are processed through two individual CNN branches, respectively, and then fused into a joint network to generate enhanced speech. In addition, to improve the performance on new users, a self-supervised learning methodology is developed to adapt the model to extract speaker-specific features. We construct a dataset to train and evaluate WaveVoice. We also perform online tests under various noisy conditions to show the applicability of our system in real-world scenarios. Experimental results show that WaveVoice can effectively reconstruct the target clean speech from the noisy audio signals, and yield notably superior performance compared with the audio-only encoder-decoder model and the state-of-the-art speech enhancement methods. Given its promising performance, we believe that WaveVoice has made a substantial contribution to the advancement of mobile voice input.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {137},
numpages = {30},
keywords = {mobile speech enhancement, lip movement, deep learning, acoustic sensing}
}

@article{10.1016/j.jvcir.2017.01.012,
author = {Li, Zhiyong and Gao, Song and Nai, Ke},
title = {Robust object tracking based on adaptive templates matching via the fusion of multiple features},
year = {2017},
issue_date = {April 2017},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {44},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2017.01.012},
doi = {10.1016/j.jvcir.2017.01.012},
abstract = {A novel tracker based on adaptive template matching via the fusion of multiple features, named ATM-MF, is proposed.Double templates matching is used to improve the accuracy and robustness of the proposed tracker.Better tracking performance on challenging sequences is obtained. Moving object tracking under complex scenes remains to be a challenging problem because the appearance of a target object can be drastically changed due to several factors, such as occlusions, illumination, pose, scale change and deformation. This study proposes an adaptive multifeature fusion strategy, in which the target appearance is modeled based on timed motion history image with HSV color histogram features and edge orientation histogram features. The variances based on the similarities between the candidate patches and the target templates are used for adaptively adjusting the weight of each feature. Double templates matching, including online and offline template matching, is adopted to locate the target object in the next frame. Experimental evaluations on challenging sequences demonstrate the accuracy and robustness of the proposed algorithm in comparison with several state-of-the-art algorithms.},
journal = {J. Vis. Comun. Image Represent.},
month = apr,
pages = {1–20},
numpages = {20},
keywords = {Visual tracking, Timed motion history image, Feature fusion, Double templates matching}
}

@article{10.1007/s00371-020-01931-4,
author = {Meng, Xiaoyan and Zhang, Guoliang and Jia, Songmin and Li, Xiuzhi and Zhang, Xiangyin},
title = {Auxiliary criterion conversion via spatiotemporal semantic encoding and feature entropy for action recognition},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {37},
number = {7},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-020-01931-4},
doi = {10.1007/s00371-020-01931-4},
abstract = {Video-based action recognition in realistic scenes is a core technology for human–computer interaction and smart surveillance. Although the trajectory features with the bag of visual words have confirmed promising performance, spatiotemporal interactive information cannot be effectively encoded which is valuable for classification. To address this issue, we propose a spatiotemporal semantic feature (ST-SF) and implement the conversion of it to the auxiliary criterion based on the information entropy theory. First, we present a text-based relevance analysis method to estimate the textual labels of objects most relevant to actions, which are employed to train the more targeted detectors based on the deep network. False detections are optimized by the inter-frame cooperativity and dynamic programming to construct the valid tubes. Then, we design the ST-SF to encode the interactive information, and the concept and calculation of feature entropy are defined based on the spatial distribution of ST-SFs on the training set. Finally, we achieve a two-stage classification strategy using the resulting decision gains. Experimental results on three publicly available datasets demonstrate that our method is robust and improves upon the state-of-the-art algorithms.},
journal = {Vis. Comput.},
month = jul,
pages = {1673–1690},
numpages = {18},
keywords = {Text-based relevance analysis, Bag-of-visual-words model, Feature entropy, Spatiotemporal semantic feature, Action recognition}
}

@inproceedings{10.1145/2993148.2997625,
author = {Zhang, Biqiao},
title = {Improving the generalizability of emotion recognition systems: towards emotion recognition in the wild},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993148.2997625},
doi = {10.1145/2993148.2997625},
abstract = {Emotion recognition in the wild requires the ability to adapt to complex and changeable application scenarios, which necessitates the generalizability of automatic emotion recognition systems. My PhD thesis focuses on methods to address factors that negatively impact the generalizability of automatic emotion recognition systems, such as the ambiguity in emotion labels, the effects of expression style (e.g., speech and music), variation in recording environments, and individual differences. In particular, I propose to tease apart the influence of these factors from emotion using multi-task learning for both feature learning and emotion inference. Results from my completed works have demonstrated that classifiers that take the influence of corpus (simulating environmental differences), expression style and gender of speaker into consideration generalize better across corpus, compared to those that do not.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
pages = {582–586},
numpages = {5},
keywords = {Multi-task Learning, Cross-corpus, Automatic Emotion Recognition},
location = {Tokyo, Japan},
series = {ICMI '16}
}

@inproceedings{10.5555/3540261.3541724,
author = {Weihs, Luca and Jain, Unnat and Liu, Iou-Jen and Salvador, Jordi and Lazebnik, Svetlana and Kembhavi, Aniruddha and Schwing, Alexander},
title = {Bridging the imitation gap by adaptive insubordination},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In practice, imitation learning is preferred over pure reinforcement learning whenever it is possible to design a teaching agent to provide expert supervision. However, we show that when the teaching agent makes decisions with access to privileged information that is unavailable to the student, this information is marginalized during imitation learning, resulting in an "imitation gap" and, potentially, poor results. Prior work bridges this gap via a progression from imitation learning to reinforcement learning. While often successful, gradual progression fails for tasks that require frequent switches between exploration and memorization. To better address these tasks and alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR). ADVISOR dynamically weights imitation and reward-based reinforcement learning losses during training, enabling on-the-fly switching between imitation and exploration. On a suite of challenging tasks set within gridworlds, multi-agent particle environments, and high-fidelity 3D simulators, we show that on-the-fly switching with ADVISOR outperforms pure imitation, pure reinforcement learning, as well as their sequential and parallel combinations.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1463},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-91825-5_3,
author = {Das, Susmoy and Sharma, Arpit},
title = {State Space Minimization Preserving Embeddings for Continuous-Time Markov Chains},
year = {2021},
isbn = {978-3-030-91824-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91825-5_3},
doi = {10.1007/978-3-030-91825-5_3},
abstract = {This paper defines embeddings which allow one to construct an action labeled continuous-time Markov chain (ACTMC) from a state labeled continuous-time Markov chain (SCTMC) and vice versa. We prove that these embeddings preserve strong forward bisimulation and strong backward bisimulation. We define weak backward bisimulation for ACTMCs and SCTMCs, and also prove that our embeddings preserve both weak forward and weak backward bisimulation. Next, we define the invertibility criteria and the inverse of these embeddings. Finally, we prove that an ACTMC can be minimized by minimizing its embedded model, i.e. SCTMC and taking the inverse of the embedding. Similarly, we prove that an SCTMC can be minimized by minimizing its embedded model, i.e. ACTMC and taking the inverse of the embedding.},
booktitle = {Performance Engineering and Stochastic Modeling: 17th European Workshop, EPEW 2021, and 26th International Conference, ASMTA 2021, Virtual Event, December 9–10 and December 13–14, 2021, Proceedings},
pages = {44–61},
numpages = {18},
keywords = {Embeddings, Stochastic systems, Bisimulation equivalence, Behavioral equivalence, Markov chain},
location = {Milan, Italy}
}

@article{10.1016/j.neucom.2016.09.070,
author = {Tareef, Afaf and Song, Yang and Cai, Weidong and Huang, Heng and Chang, Hang and Wang, Yue and Fulham, Michael and Feng, Dagan and Chen, Mei},
title = {Automatic segmentation of overlapping cervical smear cells based on local distinctive features and guided shape deformation},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {221},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.070},
doi = {10.1016/j.neucom.2016.09.070},
abstract = {Automated segmentation of cells from cervical smears poses great challenge to biomedical image analysis because of the noisy and complex background, poor cytoplasmic contrast and the presence of fuzzy and overlapping cells. In this paper, we propose an automated segmentation method for the nucleus and cytoplasm in a cluster of cervical cells based on distinctive local features and guided sparse shape deformation. Our proposed approach is performed in two stages: segmentation of nuclei and cellular clusters, and segmentation of overlapping cytoplasm. In the first stage, a set of local discriminative shape and appearance cues of image superpixels is incorporated and classified by the Support Vector Machine (SVM) to segment the image into nuclei, cellular clusters, and background. In the second stage, a robust shape deformation framework is proposed, based on Sparse Coding (SC) theory and guided by representative shape features, to construct the cytoplasmic shape of each overlapping cell. Then, the obtained shape is refined by the Distance Regularized Level Set Evolution (DRLSE) model. We evaluated our approach using the ISBI 2014 challenge dataset, which has 135 synthetic cell images for a total of 810 cells. Our results show that our approach outperformed existing approaches in segmenting overlapping cells and obtaining accurate nuclear boundaries. HighlightsA fully automated segmentation method is proposed for overlapping cervical cells.Our approach is based on superpixel-based features and guided shape deformation.Our shape initialization procedure is able to work with the different cell types.The practicality of our approach in segmenting highly overlapping cells is proved.Our approach outperformed existing approaches in nuclei and cytoplasm segmentation.},
journal = {Neurocomput.},
month = jan,
pages = {94–107},
numpages = {14},
keywords = {Sparse coding, Shape deformation, Overlapping cervical smear cells, Feature extraction, Distance regularized level set}
}

@article{10.1145/3432195,
author = {Mao, Wenguang and Sun, Wei and Wang, Mei and Qiu, Lili},
title = {DeepRange: Acoustic Ranging via Deep Learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432195},
doi = {10.1145/3432195},
abstract = {Acoustic ranging is a technique for estimating the distance between two objects using acoustic signals, which plays a critical role in many applications, such as motion tracking, gesture/activity recognition, and indoor localization. Although many ranging algorithms have been developed, their performance still degrades significantly under strong noise, interference and hardware limitations. To improve the robustness of the ranging system, in this paper we develop a Deep learning based Ranging system, called DeepRange. We first develop an effective mechanism to generate synthetic training data that captures noise, speaker/mic distortion, and interference in the signals and remove the need of collecting a large volume of training data. We then design a deep range neural network (DRNet) to estimate distance. Our design is inspired by signal processing that ultra-long convolution kernel sizes help to combat the noise and interference. We further apply an ensemble method to enhance the performance. Moreover, we analyze and visualize the network neurons and filters, and identify a few important findings that can be useful for improving the design of signal processing algorithms. Finally, we implement and evaluate DeepRangeusing 11 smartphones with different brands and models, 4 environments (i.e., a lab, a conference room, a corridor, and a cubic area), and 10 users. Our results show that DRNet significantly outperforms existing ranging algorithms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {143},
numpages = {23},
keywords = {Ranging, Motion Tracking, Convolutional Neural Network, Acoustic Sensing}
}

@article{10.1155/2020/7917021,
author = {Zhang, Cheng and He, Dan and Zhang, Qingchen},
title = {A Deep Multiscale Fusion Method via Low-Rank Sparse Decomposition for Object Saliency Detection Based on Urban Data in Optical Remote Sensing Images},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/7917021},
doi = {10.1155/2020/7917021},
abstract = {The urban data provides a wealth of information that can support the life and work for people. In this work, we research the object saliency detection in optical remote sensing images, which is conducive to the interpretation of urban scenes. Saliency detection selects the regions with important information in the remote sensing images, which severely imitates the human visual system. It plays a powerful role in other image processing. It has successfully made great achievements in change detection, object tracking, temperature reversal, and other tasks. The traditional method has some disadvantages such as poor robustness and high computational complexity. Therefore, this paper proposes a deep multiscale fusion method via low-rank sparse decomposition for object saliency detection in optical remote sensing images. First, we execute multiscale segmentation for remote sensing images. Then, we calculate the saliency value, and the proposal region is generated. The superpixel blocks of the remaining proposal regions of the segmentation map are input into the convolutional neural network. By extracting the depth feature, the saliency value is calculated and the proposal regions are updated. The feature transformation matrix is obtained based on the gradient descent method, and the high-level semantic prior knowledge is obtained by using the convolutional neural network. The process is iterated continuously to obtain the saliency map at each scale. The low-rank sparse decomposition of the transformed matrix is carried out by robust principal component analysis. Finally, the weight cellular automata method is utilized to fuse the multiscale saliency graphs and the saliency map calculated according to the sparse noise obtained by decomposition. Meanwhile, the object priors knowledge can filter most of the background information, reduce unnecessary depth feature extraction, and meaningfully improve the saliency detection rate. The experiment results show that the proposed method can effectively improve the detection effect compared to other deep learning methods.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {14}
}

@inproceedings{10.5555/647124.717610,
author = {Giudice, F. and Rosa, G. La and Risitano, A.},
title = {Product Recovery-Cycles Design: Extension of Useful Life},
year = {2001},
isbn = {1402073275},
publisher = {Kluwer, B.V.},
address = {NLD},
booktitle = {Proceedings of the IFIP TC5/WG5.2 &amp; WG5.3 Conference on Feature Modelling and Advanced Design-for-the-Life-Cycle Systems: Feature Based Product Life-Cycle Modelling},
pages = {165–185},
numpages = {21},
series = {FEATS '01}
}

@inproceedings{10.1007/978-3-030-98682-7_9,
author = {Antonioni, Emanuele and Riccio, Francesco and Nardi, Daniele},
title = {Improving Sample Efficiency in&nbsp;Behavior Learning by&nbsp;Using Sub-optimal Planners for&nbsp;Robots},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_9},
doi = {10.1007/978-3-030-98682-7_9},
abstract = {The design and implementation of behaviors for robots operating in dynamic and complex environments are becoming mandatory in nowadays applications. Reinforcement learning is consistently showing remarkable results in learning effective action policies and in achieving super-human performance in various tasks – without exploiting prior knowledge. However, in robotics, the use of purely learning-based techniques is still subject to strong limitations. Foremost, sample efficiency. Such techniques, in fact, are known to require large training datasets, and long training sessions, in order to develop effective action policies. Hence in this paper, to alleviate such constraint, and to allow learning in such robotic scenarios, we introduce SErP (Sample Efficient robot Policies), an iterative algorithm to improve the sample-efficiency of learning algorithms. SErP exploits a sub-optimal planner (here implemented with a monitor-replanning algorithm) to lead the exploration of the learning agent through its initial iterations. Intuitively, SErP exploits the planner as an expert in order to enable focused exploration and to avoid portions of the search space that are not effective to solve the task of the robot. Finally, to confirm our insights and to show the improvements that SErP carries with, we report the results obtained in two different robotic scenarios: (1) a cartpole scenario and (2) a soccer-robots scenario within the RoboCup@Soccer SPL environment.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {103–114},
numpages = {12},
keywords = {Decision-making, Reinforcement learning, Automated planning},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.neucom.2017.01.093,
author = {Tareef, Afaf and Song, Yang and Huang, Heng and Wang, Yue and Feng, Dagan and Chen, Mei and Cai, Weidong},
title = {Optimizing the cervix cytological examination based on deep learning and dynamic shape modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {248},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.01.093},
doi = {10.1016/j.neucom.2017.01.093},
abstract = {The task of segmenting nuclei and cytoplasm in Papanicolau smear images is one of the most challenging tasks in automated cervix cytological analysis owing to the high degree of overlapping, the multiform shape of the cells and their complex structures resulting from inconsistent staining, poor contrast, and the presence of inflammatory cells. This article presents a robust variational segmentation framework based on superpixelwise convolutional neutral network and a learned shape prior enabling an accurate analysis of overlapping cervical mass. The cellular components of Pap image are first classified by automatic feature learning and classification model. Then, a learning shape prior model is employed to delineate the actual contour of each individual cytoplasm inside the overlapping mass. The shape prior is dynamically modeled during the segmentation process as a weighted linear combination of shape templates from an over-complete shape dictionary under sparsity constraints. We provide quantitative and qualitative assessment of the proposed method using two databases of 153 cervical cytology images, with 870 cells in total, synthesised by accumulating real isolated cervical cells to generate overlapping cellular masses with a varying number of cells and degree of overlap. The experimental results have demonstrated that our methodology can successfully segment nuclei and cytoplasm from highly overlapping mass. Our segmentation is also competitive when compared to the state-of-the-art methods.},
journal = {Neurocomput.},
month = jul,
pages = {28–40},
numpages = {13},
keywords = {Sparse approximation, Overlapping cell segmentation, Level set evolution, Feature learning, Convolutional neural network}
}

@article{10.1007/s10489-020-01694-4,
author = {Tian, Qiuting and Han, Dezhi and Li, Kuan-Ching and Liu, Xingao and Duan, Letian and Castiglione, Arcangelo},
title = {An intrusion detection approach based on improved deep belief network},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01694-4},
doi = {10.1007/s10489-020-01694-4},
abstract = {In today’s interconnected society, cyberattacks have become more frequent and sophisticated, and existing intrusion detection systems may not be adequate in the complex cyberthreat landscape. For instance, existing intrusion detection systems may have overfitting, low classification accuracy, and high false positive rate (FPR) when faced with significantly large volume and variety of network data. An intrusion detection approach based on improved deep belief network (DBN) is proposed in this paper&nbsp;to mitigate the above problems, where the dataset is processed by probabilistic mass function (PMF) encoding and Min-Max normalization method to simplify the data preprocessing. Furthermore, a combined sparsity penalty term based on Kullback-Leibler (KL) divergence and non-mean Gaussian distribution is introduced in the likelihood function of the unsupervised training phase of DBN, and sparse constraints retrieve the sparse distribution of the dataset, thus avoiding the problem of feature homogeneity and overfitting. Finally, simulation experiments are performed on the NSL-KDD and UNSW-NB15 public datasets. The proposed method achieves 96.17% and 86.49% accuracy, respectively. Experimental results show that compared with the state-of-the-art methods, the proposed method achieves significant improvement in classification accuracy and FPR.},
journal = {Applied Intelligence},
month = oct,
pages = {3162–3178},
numpages = {17},
keywords = {Probabilistic mass function (PMF), likelihood function, Kullback-Leibler (KL) divergence, Deep belief network (DBN), Intrusion detection}
}

@inbook{10.5555/3454287.3454321,
author = {Anderson, Peter and Shrivastava, Ayush and Parikh, Devi and Batra, Dhruv and Lee, Stefan},
title = {Chasing ghosts: instruction following as bayesian state tracking},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) [1] within the framework of Bayesian state tracking – learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet [2] baseline when predicting the goal location on the map. On the full VLN task, i.e., navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {34},
numpages = {11}
}

@inproceedings{10.1007/978-3-030-98682-7_5,
author = {Fernandes, Roberto and Rodrigues, Walber M. and Barros, Edna},
title = {Dataset and&nbsp;Benchmarking of&nbsp;Real-Time Embedded Object Detection for&nbsp;RoboCup SSL},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_5},
doi = {10.1007/978-3-030-98682-7_5},
abstract = {When producing a model to object detection in a specific context, the first obstacle is to have a dataset labeling the desired classes. In RoboCup, some leagues already have more than one dataset to train and evaluate a model. However, in the Small Size League (SSL), there is not such dataset available yet. This paper presents an open-source dataset to be used as a benchmark for real-time object detection in SSL. This work also presented a pipeline to train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a low-power embedded system. This pipeline is used to evaluate the proposed dataset with state-of-art optimized models. In this dataset, the MobileNet SSD v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS), while running on an SSL robot.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {53–64},
numpages = {12},
keywords = {Object detection, Deep learning, Benchmark, Dataset},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.sigpro.2019.01.018,
author = {Ahmed, Talal and Bajwa, Waheed U.},
title = {ExSIS: Extended sure independence screening for ultrahigh-dimensional linear models},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.01.018},
doi = {10.1016/j.sigpro.2019.01.018},
journal = {Signal Process.},
month = jun,
pages = {33–48},
numpages = {16},
keywords = {Sparse signal processing, High-dimensional statistics, Linear models, Sure screening, Variable screening, Variable selection}
}

@inproceedings{10.5555/265221.265244,
author = {van Holland, Winfried and Bronsvoort, Willem F.},
title = {Assembly features and sequence planning},
year = {1997},
isbn = {041280980X},
publisher = {Chapman &amp; Hall, Ltd.},
address = {GBR},
booktitle = {Proceedings of the Fifth IFIP TC5/WG5.2 International Workshop on Geometric Modeling in Computer Aided Design on Product Modeling for Computer Integrated Design and Manufacture},
pages = {275–284},
numpages = {10},
keywords = {feature modelling, clustering, assembly sequence planning, assembly modelling, assembly features},
location = {Airlie, Virginia, USA},
series = {GMCAD '96}
}

@article{10.1007/s10009-014-0341-2,
author = {Filho, Jo\~{a}o Bosco and Barais, Olivier and Acher, Mathieu and Le Noir, J\'{e}r\^{o}me and Legay, Axel and Baudry, Benoit},
title = {Generating counterexamples of model-based software product lines},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-014-0341-2},
doi = {10.1007/s10009-014-0341-2},
abstract = {In a model-based software product line (MSPL), the variability of the domain is characterized in a variability model and the core artifacts are base models conforming to a modeling language (also called metamodel). A realization model connects the features of the variability model to the base model elements, triggering operations over these elements based on a configuration. The design space of an MSPL is extremely complex to manage for the engineer, since the number of variants may be exponential and the derived product models have to be conforming to numerous well-formedness and business rules. In this paper, the objective is to provide a way to generate MSPLs, called counterexamples (also called antipatterns), that can produce invalid product models despite a valid configuration in the variability model. We describe the foundations and motivate the usefulness of counterexamples (e.g., inference of guidelines or domain-specific rules to avoid earlier the specification of incorrect mappings; testing oracles for increasing the robustness of derivation engines given a modeling language). We provide a generic process, based on the common variability language (CVL) to randomly search the space of MSPLs for a specific modeling language. We develop LineGen a tool on top of CVL and modeling technologies to support the methodology and the process. LineGen targets different scenarios and is flexible to work either with just a domain metamodel as input or also with pre-defined variability models and base models. We validate the effectiveness of this process for three formalisms at different scales (up to 247 metaclasses and 684 rules). We also apply the approach in the context of a real industrial scenario involving a large-scale metamodel.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {585–600},
numpages = {16},
keywords = {Software product lines, Model-based engineering, Counterexamples}
}

@inproceedings{10.5555/3044805.3044969,
author = {Cuturi, Marco and Doucet, Arnaud},
title = {Fast computation of wasserstein barycenters},
year = {2014},
publisher = {JMLR.org},
abstract = {We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–685–II–693},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.5555/646473.692986,
author = {Slezak, Dominik and Wroblewski, Jakub},
title = {Approximate Bayesian Network Classifiers},
year = {2002},
isbn = {354044274X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Bayesian network (BN) is a directed acyclic graph encoding probabilistic independence statements between variables. BN with decision attribute as a root can be applied to classification of new cases, by synthesis of conditional probabilities propagated along the edges. We consider approximate BNs, which almost keep entropy of a decision table. They have usually less edges than classical BNs. They enable to model and extend the well-known Naive Bayes approach. Experiments show that classifiers based on approximate BNs can be very efficient.},
booktitle = {Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing},
pages = {365–372},
numpages = {8},
series = {TSCTC '02}
}

@article{10.1016/j.specom.2019.10.006,
author = {Michelsanti, Daniel and Tan, Zheng-Hua and Sigurdsson, Sigurdur and Jensen, Jesper},
title = {Deep-learning-based audio-visual speech enhancement in presence of Lombard effect},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.006},
doi = {10.1016/j.specom.2019.10.006},
journal = {Speech Commun.},
month = dec,
pages = {38–50},
numpages = {13},
keywords = {Speech intelligibility, Speech quality, Deep learning, Audio-visual speech enhancement, Lombard effect}
}

@article{10.1155/2015/635840,
author = {Krishnamurti, Sridhar},
title = {Application of neural network modeling to identify auditory processing disorders in school-age children},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-7594},
url = {https://doi.org/10.1155/2015/635840},
doi = {10.1155/2015/635840},
abstract = {P300 Auditory Event-Related Potentials (P3AERPs) were recorded in nine school-age children with auditory processing disorders and nine age- and gender-matched controls in response to tone burst stimuli presented at varying rates (1/second or 3/second) under varying levels of competing noise (0 dB, 40 dB, or 60 dB SPL). Neural network modeling results indicated that speed of information processing and task-related demands significantly influenced P3AERP latency in children with auditory processing disorders. Competing noise and rapid stimulus rates influenced P3AERP amplitude in both groups.},
journal = {Adv. Artif. Neu. Sys.},
month = jan,
articleno = {5},
numpages = {1}
}

@inproceedings{10.1007/978-3-030-32248-9_56,
author = {Parvathaneni, Prasanna and Bao, Shunxing and Nath, Vishwesh and Woodward, Neil D. and Claassen, Daniel O. and Cascio, Carissa J. and Zald, David H. and Huo, Yuankai and Landman, Bennett A. and Lyu, Ilwoo},
title = {Cortical Surface Parcellation Using Spherical Convolutional Neural Networks},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_56},
doi = {10.1007/978-3-030-32248-9_56},
abstract = {We present cortical surface parcellation using spherical deep convolutional neural networks. Traditional multi-atlas cortical surface parcellation requires inter-subject surface registration using geometric features with slow processing speed on a single subject (2–3&nbsp;h). Moreover, even optimal surface registration does not necessarily produce optimal cortical parcellation as parcel boundaries are not fully matched to the geometric features. In this context, a choice of training features is important for accurate cortical parcellation. To utilize the networks efficiently, we propose cortical parcellation-specific input data from an irregular and complicated structure of cortical surfaces. To this end, we align ground-truth cortical parcel boundaries and use their resulting deformation fields to generate new pairs of deformed geometric features and parcellation maps. To extend the capability of the networks, we then smoothly morph cortical geometric features and parcellation maps using the intermediate deformation fields. We validate our method on 427 adult brains for 49 labels. The experimental results show that our method outperforms traditional multi-atlas and naive spherical U-Net approaches, while achieving full cortical parcellation in less than a minute.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {501–509},
numpages = {9},
keywords = {Cortical surface parcellation, Spherical deformation, Spherical U-Net, Surface registration},
location = {Shenzhen, China}
}

@article{10.1109/TASLP.2016.2536478,
author = {Zhang, Xiao-Lei and Wang, DeLiang},
title = {A deep ensemble learning method for monaural speech separation},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2536478},
doi = {10.1109/TASLP.2016.2536478},
abstract = {Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {967–977},
numpages = {11},
keywords = {multicontext networks, monaural speech separation, masking-based separation, mapping-based separation, ensemble learning, deep neural networks}
}

@inproceedings{10.1145/2245276.2231938,
author = {Sardinha, Alberto and Yu, Yijun and Niu, Nan and Rashid, Awais},
title = {EA-tracer: identifying traceability links between code aspects and early aspects},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231938},
doi = {10.1145/2245276.2231938},
abstract = {Early aspects are crosscutting concerns that are identified and addressed at the requirements and architecture level, while code aspects are crosscutting concerns that manifest at the code level. Currently, there are many approaches to address the identification and modularization of these cross-cutting concerns at each level, but very few techniques try to analyze the relationship between early aspects and code aspects. This paper presents a tool for automating the process of identifying traceability links between requirements-level aspects and code aspects, which is a first step towards an in-depth analysis. We also present an empirical evaluation of the tool with a real-life Web-based information system and a software product line for handling data on mobile devices. The results show that we can identify traceability links between early aspects and code aspects with a high accuracy.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1035–1042},
numpages = {8},
location = {Trento, Italy},
series = {SAC '12}
}

@article{10.5555/2370969.2370974,
author = {Wojna, Arkadiusz},
title = {Center-Based Indexing in Vector and Metric Spaces},
year = {2003},
issue_date = {August 2003},
publisher = {IOS Press},
address = {NLD},
volume = {56},
number = {3},
issn = {0169-2968},
abstract = {The paper addresses the problem of indexing data for k nearest neighbors (k-nn) search. Given a collection of data objects and a similarity measure the searching goal is to find quickly the k most similar objects to a given query object. We present a top-down indexing method that employs a widely used scheme of indexing algorithms. It starts with the whole set of objects at the root of an indexing tree and iteratively splits data at each level of indexing hierarchy. In the paper two different data models are considered. In the first, objects are represented by vectors from a multi-dimensional vector space. The second, more general, is based on an assumption that objects satisfy only the axioms of a metric space. We propose an iterative k-means algorithm for tree node splitting in case of a vector space and an iterative k-approximate-centers algorithm in case when only a metric space is provided. The experiments show that the iterative k-means splitting procedure accelerates significantly k-nn searching over the one-step procedure used in other indexing structures such as GNAT, SS-tree and M-tree and that the relevant representation of a tree node is an important issue for the performance of the search process. We also combine different search pruning criteria used in BST, GHT nad GNAT structures into one and show that such a combination outperforms significantly each single pruning criterion. The experiments are performed for benchmark data sets of the size up to several hundreds of thousands of objects. The indexing tree with the k-means splitting procedure and the combined search criteria is particularly effective for the largest tested data sets for which this tree accelerates searching up to several thousands times.},
journal = {Fundam. Inf.},
month = aug,
pages = {285–310},
numpages = {26}
}

@article{10.1109/TASLP.2020.3019646,
author = {Nguyen, Thi Ngoc Tho and Gan, Woon-Seng and Ranjan, Rishabh and Jones, Douglas L.},
title = {Robust Source Counting and DOA Estimation Using Spatial Pseudo-Spectrum and Convolutional Neural Network},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3019646},
doi = {10.1109/TASLP.2020.3019646},
abstract = {Many signal processing-based methods for sound source direction-of-arrival estimation produce a spatial pseudo-spectrum of which the local maxima strongly indicate the source directions. Due to different levels of noise, reverberation and different number of overlapping sources, the spatial pseudo-spectra are noisy even after smoothing. In addition, the number of sources is often unknown. As a result, selecting the peaks from these spectra is susceptible to error. Convolutional neural network has been successfully applied to many image processing problems in general and direction-of-arrival estimation in particular. In addition, deep learning-based methods for direction-of-arrival estimation show good generalization to different environments. We propose to use a 2D convolutional neural network with multi-task learning to robustly estimate the number of sources and the directions-of-arrival from short-time spatial pseudo-spectra, which have useful directional information from audio input signals. This approach reduces the tendency of the neural network to learn unwanted association between sound classes and directional information, and helps the network generalize to unseen sound classes. The simulation and experimental results show that the proposed methods outperform other directional-of-arrival estimation methods in different levels of noise and reverberation, and different number of sources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {2626–2637},
numpages = {12}
}

@inproceedings{10.1007/978-3-030-29551-6_44,
author = {Lin, Jiping and Zhou, Yu and Kang, Junhao},
title = {Low-Sampling Imagery Data Recovery by Deep Learning Inference and Iterative Approach},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_44},
doi = {10.1007/978-3-030-29551-6_44},
abstract = {Block-based compressed sensing (CS) recovery aims to reconstruct the high quality image from only a small number of observations in a block-wise manner. However, when the sampling rate is very low and the existence of additive noise, there are usually some block artifacts and detail blurs which degrades the reconstructed quality. In this paper, we propose an efficient method which takes both the advantages of deep learning (DL) framework and iterative approaches. First, a deep multi-layer perceptron (DMLP) is constructed to obtain the initial reconstructed image. Then, an efficient iterative approach is applied to keep the consistence and smoothness between the adjacent blocks. The proposed method demonstrates its efficacy on benchmark datasets.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {488–493},
numpages = {6},
keywords = {Iterative approach, Deep learning, Compressed sensing},
location = {Athens, Greece}
}

@article{10.4018/jsse.2010102004,
author = {Nhlabatsi, Armstrong and Nuseibeh, Bashar and Yu, Yijun},
title = {Security Requirements Engineering for Evolving Software Systems: A Survey},
year = {2010},
issue_date = {January 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {1},
issn = {1947-3036},
url = {https://doi.org/10.4018/jsse.2010102004},
doi = {10.4018/jsse.2010102004},
abstract = {Long-lived software systems often undergo evolution over an extended period. Evolution of these systems is inevitable as they need to continue to satisfy changing business needs, new regulations and standards, and introduction of novel technologies. Such evolution may involve changes that add, remove, or modify features; or that migrate the system from one operating platform to another. These changes may result in requirements that were satisfied in a previous release of a system not being satisfied in subsequent versions. When evolutionary changes violate security requirements, a system may be left vulnerable to attacks. In this article we review current approaches to security requirements engineering and conclude that they lack explicit support for managing the effects of software evolution. We then suggest that a cross fertilization of the areas of software evolution and security engineering would address the problem of maintaining compliance to security requirements of software systems as they evolve.},
journal = {Int. J. Secur. Softw. Eng.},
month = jan,
pages = {54–73},
numpages = {20},
keywords = {Software Evolution, Security Requirements Engineering, Entailment Relation}
}

@article{10.1177/0278364917691112,
author = {Bastianelli, Emanuele and Castellucci, Giuseppe and Croce, Danilo and Basili, Roberto and Nardi, Daniele},
title = {Structured learning for spoken language understanding in human-robot interaction},
year = {2017},
issue_date = {6 2017},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {36},
number = {5–7},
issn = {0278-3649},
url = {https://doi.org/10.1177/0278364917691112},
doi = {10.1177/0278364917691112},
abstract = {Robots are slowly becoming a part of everyday life, being marketed for commercial applications such as telepresence, cleaning or entertainment. Thus, the ability to interact via natural language with non-expert users is becoming a key requirement. Even if user utterances can be efficiently recognized and transcribed by automatic speech recognition systems, several issues arise in translating them into suitable robotic actions and most of the existing solutions are strictly related to a specific scenario. In this paper, we present an approach to the design of natural language interfaces for human robot interaction, to translate spoken commands into computational structures that enable the robot to execute the intended request. The proposed solution is achieved by combining a general theory of language semantics, i.e. frame semantics, with state-of-the-art methods for robust spoken language understanding, based on structured learning algorithms. The adopted data driven paradigm allows the development of a fully functional natural language processing chain, that can be initialized by re-using available linguistic tools and resources. In addition, it can be also specialized by providing small sets of examples representative of a target newer domain. A systematic benchmarking resource, in terms of a rich and multi-layered spoken corpus has also been created and it has been used to evaluate the natural language processing chain. Our results show that our processing chain, trained with generic resources, provides a solid baseline for command understanding in a service robot domain. Moreover, when domain-dependent resources are provided to the system, the accuracy of the achieved interpretation always improves.},
journal = {Int. J. Rob. Res.},
month = jun,
pages = {660–683},
numpages = {24},
keywords = {natural language processing, machine learning for natural language understanding, human-robot interaction, Spoken language understanding}
}

@article{10.1016/j.infsof.2016.08.005,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Requirements monitoring frameworks},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.005},
doi = {10.1016/j.infsof.2016.08.005},
abstract = {Analyze the characteristics and application areas of monitoring approaches proposed in different domains.Systematically identify frameworks supporting requirements monitoring.Analyze to what extent the monitoring frameworks support requirements monitoring in SoS. ContextSoftware systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. ObjectiveThe aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. MethodWe performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. ResultsWe identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. ConclusionsWe conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {89–109},
numpages = {21},
keywords = {Systems of systems, Systematic literature review, Requirements monitoring}
}

@inproceedings{10.1145/1134285.1134336,
author = {Anvik, John and Hiew, Lyndon and Murphy, Gail C.},
title = {Who should fix this bug?},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134336},
doi = {10.1145/1134285.1134336},
abstract = {Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {361–370},
numpages = {10},
keywords = {problem tracking, machine learning, issue tracking, bug triage, bug report assignment},
location = {Shanghai, China},
series = {ICSE '06}
}

@inproceedings{10.5555/3540261.3540707,
author = {Chen, Shizhe and Guhur, Pierre-Louis and Schmid, Cordelia and Laptev, Ivan},
title = {History aware multimodal transformer for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {446},
numpages = {14},
series = {NIPS '21}
}

@article{10.1016/j.jss.2019.02.026,
author = {Butting, Arvid and Eikermann, Robert and Kautz, Oliver and Rumpe, Bernhard and Wortmann, Andreas},
title = {Systematic composition of independent language features},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.026},
doi = {10.1016/j.jss.2019.02.026},
journal = {J. Syst. Softw.},
month = jun,
pages = {50–69},
numpages = {20}
}

@article{10.1016/j.knosys.2016.08.027,
author = {Sang, Yongsheng and Lv, Jiancheng and Qu, Hong and Yi, Zhang},
title = {Shortest path computation using pulse-coupled neural networks with restricted autowave},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.08.027},
doi = {10.1016/j.knosys.2016.08.027},
abstract = {Finding shortest paths is an important problem in transportation and communication networks. This paper develops a Pulse-Coupled Neural Network (PCNN) model to efficiently compute a single-pair shortest path. Unlike most of the existing PCNN models, the proposed model is endowed with a special mechanism, called on-forward/off-backward; if a neuron fires, its neighboring neurons in a certain forward region will be excited, whereas the neurons in a backward region will be inhibited. As a result, the model can produce a restricted autowave that propagates at different speeds corresponding to different directions, which is different from the completely nondeterministic PCNN models. Compared with some traditional methods, the proposed PCNN model significantly reduces the computational cost of searching for the shortest path. Experimental results further confirmed the efficiency and effectiveness of the proposed model.},
journal = {Know.-Based Syst.},
month = dec,
pages = {1–11},
numpages = {11},
keywords = {Shortest path, Restricted autowave, Pulse-coupled neural networks, On-forward/off-backward}
}

@inproceedings{10.5555/2025896.2025909,
author = {Przyby\l{}ek, Adam},
title = {Systems evolution and software reuse in object-oriented programming and aspect-oriented programming},
year = {2011},
isbn = {9783642219511},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Every new programming technique makes claims that software engineers want to hear. Such is the case with aspect-oriented programming (AOP). This paper describes a quasi-controlled experiment which compares the evolution of two functionally equivalent programs, developed in two different paradigms. The aim of the study is to explore the claims that software developed with aspect-oriented languages is easier to maintain and reuse than this developed with object-oriented languages. We have found no evidence to support these claims.},
booktitle = {Proceedings of the 49th International Conference on Objects, Models, Components, Patterns},
pages = {163–178},
numpages = {16},
keywords = {separation of concerns, reusability, maintainability, AOP},
location = {Zurich, Switzerland},
series = {TOOLS'11}
}

@article{10.1145/2180921.2180923,
author = {Anwikar, Vallabh and Naik, Ravindra and Contractor, Adnan and Makkapati, Hemanth},
title = {Domain-driven technique for functionality identification in source code},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2180921.2180923},
doi = {10.1145/2180921.2180923},
abstract = {While migrating existing software systems to Software Product Lines, finding out the functionalities in the software is critical. For maintenance activities like deleting or changing existing features, or adding new similar features, identifying and extracting functionalities from the software is significant. This paper describes a technique for creating mapping between the source code and functionalities implemented by it while exploiting the domain knowledge. The technique is based on the notion of function variables that are used by developers for expressing functionality in the source code. By tracking the known values of the function variables and evaluating the conditions that use them, the mapping is identified. Our technique makes use of static data ow analysis and partial evaluation, and is designed with automation perspective. After applying to few samples representing real-life code structure and programming practices, the technique identified precise mapping of the detailed program elements to functions},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–8},
numpages = {8},
keywords = {partial evaluation, functionality identification, function variables}
}

@article{10.1007/s10462-020-09907-5,
author = {Uma Maheswari, S. and Shahina, A. and Nayeemulla Khan, A.},
title = {Understanding Lombard speech: a review of compensation techniques towards improving speech based recognition systems},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09907-5},
doi = {10.1007/s10462-020-09907-5},
abstract = {Building voice-based Artificial Intelligence (AI) systems that can efficiently interact with humans through speech has become plausible today due to rapid strides in efficient data-driven AI techniques. Such a human–machine voice interaction in real world would often involve a noisy ambience, where humans tend to speak with additional vocal effort than in a quiet ambience, to mitigate the noise-induced suppression of vocal self-feedback. This noise induced change in the vocal effort is called Lombard speech. In order to build intelligent conversational devices that can operate in a noisy ambience, it is imperative to study the characteristics and processing of Lombard speech. Though the progress of research on Lombard speech started several decades ago, it needs to be explored further in the current scenario which is seeing an explosion of voice-driven applications. The system designed to work with normal speech spoken in a quiet ambience fails to provide the same performance in changing environmental contexts. Different contexts lead to different styles of Lombard speech and hence there arises a need for efficient ways of handling variations in speaking styles in noise. The Lombard speech is also more intelligible than normal speech of a speaker. Applications like public announcement systems with speech output interface should talk with varying degrees of vocal effort to enhance naturalness in a way that humans adapt to speak in noise, in real time. This review article is an attempt to summarize the progress of work on the possible ways of processing Lombard speech to build smart and robust human–machine interactive systems with speech input–output interface, irrespective of operating environmental contexts, for different application needs. This article is a comprehensive review of the studies on Lombard speech, highlighting the key differences observed in acoustic and perceptual analysis of Lombard speech and detailing the Lombard effect compensation methods towards improving the robustness of speech based recognition systems.},
journal = {Artif. Intell. Rev.},
month = apr,
pages = {2495–2523},
numpages = {29},
keywords = {Lombard speech synthesis, Lombard effect compensation, Automatic recognition systems, Perceptual analysis, Acoustic analysis, Lombard speech}
}

@article{10.1007/s11280-019-00766-x,
author = {Hu, Rongyao and Zhu, Xiaofeng and Zhu, Yonghua and Gan, Jiangzhang},
title = {Robust SVM with adaptive graph learning},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-019-00766-x},
doi = {10.1007/s11280-019-00766-x},
abstract = {Support Vector Machine (SVM) has been widely applied in real application due to its efficient performance in the classification task so that a large number of SVM methods have been proposed. In this paper, we present a novel SVM method by taking the dynamic graph learning and the self-paced learning into account. To do this, we propose utilizing self-paced learning to assign important samples with large weights, learning a transformation matrix for conducting feature selection to remove redundant features, and learning a graph matrix from the low-dimensional data of original data to preserve the data structure. As a consequence, both the important samples and the useful features are used to select support vectors in the SVM framework. Experimental analysis on four synthetic and sixteen benchmark data sets demonstrated that our method outperformed state-of-the-art methods in terms of both binary classification and multi-class classification tasks.},
journal = {World Wide Web},
month = may,
pages = {1945–1968},
numpages = {24},
keywords = {SVM, Graph learning, Feature selection, Self-paced learning}
}

@article{10.1155/2020/8826568,
author = {Asare, Sarpong Kwadwo and You, Fei and Nartey, Obed Tettey and Rakhshan, Vahid},
title = {A Semisupervised Learning Scheme with Self-Paced Learning for Classifying Breast Cancer Histopathological Images},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1687-5265},
url = {https://doi.org/10.1155/2020/8826568},
doi = {10.1155/2020/8826568},
abstract = {The unavailability of large amounts of well-labeled data poses a significant challenge in many medical imaging tasks. Even in the likelihood of having access to sufficient data, the process of accurately labeling the data is an arduous and time-consuming one, requiring expertise skills. Again, the issue of unbalanced data further compounds the abovementioned problems and presents a considerable challenge for many machine learning algorithms. In lieu of this, the ability to develop algorithms that can exploit large amounts of unlabeled data together with a small amount of labeled data, while demonstrating robustness to data imbalance, can offer promising prospects in building highly efficient classifiers. This work proposes a semisupervised learning method that integrates self-training and self-paced learning to generate and select pseudolabeled samples for classifying breast cancer histopathological images. A novel pseudolabel generation and selection algorithm is introduced in the learning scheme to generate and select highly confident pseudolabeled samples from both well-represented classes to less-represented classes. Such a learning approach improves the performance by jointly learning a model and optimizing the generation of pseudolabels on unlabeled-target data to augment the training data and retraining the model with the generated labels. A class balancing framework that normalizes the class-wise confidence scores is also proposed to prevent the model from ignoring samples from less represented classes (hard-to-learn samples), hence effectively handling the issue of data imbalance. Extensive experimental evaluation of the proposed method on the BreakHis dataset demonstrates the effectiveness of the proposed method.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {16}
}

@inproceedings{10.1145/2553062.2553065,
author = {Schneider, Chris and Barker, Adam and Dobson, Simon},
title = {Autonomous Fault Detection in Self-Healing Systems: Comparing Hidden Markov Models and Artificial Neural Networks},
year = {2014},
isbn = {9781450325141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2553062.2553065},
doi = {10.1145/2553062.2553065},
abstract = {Autonomously detecting and recovering from faults is one approach for reducing the operational complexity and costs associated with managing computing environments. We present a novel methodology for autonomously generating investigation leads that help identify systems faults. Specifically, when historical feature data is present, Hidden Markov Models can be used to heuristically identify the root cause of a fault in an unsupervised manner. This approach improves the state of the art by allowing self-healing systems to detect faults with greater autonomy than existing methodologies, and thus further reduce operational costs.},
booktitle = {Proceedings of International Workshop on Adaptive Self-Tuning Computing Systems},
pages = {24–31},
numpages = {8},
keywords = {self-healing systems, machine learning, hidden markov models, fault detection, autonomic computing, artificial neural networks},
location = {Vienna, Austria},
series = {ADAPT '14}
}

@article{10.1007/s00521-018-3560-8,
author = {Anwar, Zeeshan and Afzal, Hammad and Bibi, Nazia and Abbas, Haider and Mohsin, Athar and Arif, Omar},
title = {A hybrid-adaptive neuro-fuzzy inference system for multi-objective regression test suites optimization},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3560-8},
doi = {10.1007/s00521-018-3560-8},
abstract = {Regression testing is a mandatory activity of software development life cycle, which is performed to ensure that modifications have not caused any adverse effects on the system’s functionality. With every change in software in the maintenance phase, the size of regression test suite grows as new test cases are written to validate changes. The bigger size of regression test suite makes the testing expensive and time-consuming. Optimization of regression test suite is a possible solution to cope with this problem. Various techniques of optimization have been proposed; however, there is no perfect solution for the problem and therefore, requires better solutions to improve the optimization process. This paper presents a novel technique named as hybrid-adaptive neuro-fuzzy inference system tuned with genetic algorithm and particle swarm optimization algorithm that is used to optimize the regression test suites. Evaluation of the proposed approach is performed on benchmark test suites including “previous date problem” and “Siemens print token.” Experimental results are compared with existing state-of-the-art techniques, and results show that the proposed approach is more effective for the reduction in a regression test suites with higher requirement coverage. The size of regression test suites can be reduced up to 48% using the proposed approach without reducing the fault detection rate.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {7287–7301},
numpages = {15},
keywords = {Adaptive neuro-fuzzy inference system, Particle swarm algorithm, Genetic algorithm, Regression test suite optimization}
}

@inproceedings{10.1007/978-3-030-58539-6_2,
author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
title = {SoundSpaces: Audio-Visual Navigation in&nbsp;3D&nbsp;Environments},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_2},
doi = {10.1007/978-3-030-58539-6_2},
abstract = {Moving around in the world is naturally a multisensory experience, but today’s embodied agents are deaf—restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception. Project: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {17–36},
numpages = {20},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10489-021-02238-0,
author = {Xiao, Xinshuang and Xu, Yitian},
title = {Multi-target regression via self-parameterized Lasso and refactored target space},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02238-0},
doi = {10.1007/s10489-021-02238-0},
abstract = {Multi-target regression (MTR) aims at simultaneously predicting multiple continuous target variables based on the same set of input variables. It has been used to solve some challenging problems. A self-parameterized Lasso for MTR is proposed in this paper, which is applied to refactored target space via linear combinations of existing targets. Our approach can simultaneously model intrinsic inter-target correlations and input-target correlations, which makes full use of the information contained in the data. Meanwhile, this information helps automatically generate the parameters needed in the model. Compared with the common method, which requires a manual setting of parameters and is expensive to optimize these parameters, our method can save a lot of time while no reduction in performance. Besides, our method can be used not only for MTR tasks but also for multi-classification tasks. The experimental results show that our method performs well in different tasks and has a wide range of applications.},
journal = {Applied Intelligence},
month = oct,
pages = {6743–6751},
numpages = {9},
keywords = {Self-parameterized, Multi-classification, Lasso, Multi-target regression}
}

@inproceedings{10.1007/978-3-030-58580-8_28,
author = {Beeching, Edward and Dibangoye, Jilles and Simonin, Olivier and Wolf, Christian},
title = {Learning to Plan with Uncertain Topological Maps},
year = {2020},
isbn = {978-3-030-58579-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58580-8_28},
doi = {10.1007/978-3-030-58580-8_28},
abstract = {We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy. Our main contribution is a data driven learning based approach for planning under uncertainty in&nbsp;topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure. Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map. Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm. By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III},
pages = {473–490},
numpages = {18},
keywords = {Visual navigation, Topological maps, Graph neural networks},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/1882992.1883014,
author = {Zhang, Jintao and Huan, Jun},
title = {Novel biological network features discovery for in silico identification of drug targets},
year = {2010},
isbn = {9781450300308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882992.1883014},
doi = {10.1145/1882992.1883014},
abstract = {In silico identification of potential drug targets is a crucial task for drug discovery. Traditional approaches utilize only protein sequence or structural information to predict drug targets, and achieve limited successes. Since cellular proteins function in the context of interaction networks by interacting with other cellular macromolecules, analysis of topological features of proteins in such networks reveal important insights on the potential druggability of proteins. In this paper, we first introduced ten novel topological features extracted from the human protein-protein interaction network. When designing these new features, we specially emphasized the roles of three disease-related groups of proteins: known drug targets, disease genes, and essential genes. Based on these novel network features, we built highly accurate models with up to 80% classification accuracy using support vector machines, L1-regularized logistic regression, and k-nearest neighbors to predict drug target, and analyzed the relevance of each feature to the proteins' druggability. Moreover, we combined our network features with a set of protein sequence features, and achieved more robust experimental performance. With the framework of integrating both network and sequence features, our method can also be used to prioritize multiple candidate proteins according to their predicted druggability.},
booktitle = {Proceedings of the 1st ACM International Health Informatics Symposium},
pages = {144–152},
numpages = {9},
keywords = {network feature, machine learning, human protein interactome, drug target},
location = {Arlington, Virginia, USA},
series = {IHI '10}
}

@inproceedings{10.1007/978-3-030-66823-5_24,
author = {Campari, Tommaso and Eccher, Paolo and Serafini, Luciano and Ballan, Lamberto},
title = {Exploiting Scene-Specific Features for Object Goal Navigation},
year = {2020},
isbn = {978-3-030-66822-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66823-5_24},
doi = {10.1007/978-3-030-66823-5_24},
abstract = {Can the intrinsic relation between an object and the room in which it is usually located help agents in the Visual Navigation Task? We study this question in the context of Object Navigation, a problem in which an agent has to reach an object of a specific class while moving in a complex domestic environment. In this paper, we introduce a new reduced dataset that speeds up the training of navigation models, a notoriously complex task. Our proposed dataset permits the training of models that do not exploit online-built maps in reasonable times even without the use of huge computational resources. Therefore, this reduced dataset guarantees a significant benchmark and it can be used to identify promising models that could be then tried on bigger and more challenging datasets. Subsequently, we propose the SMTSC model, an attention-based model capable of exploiting the correlation between scenes and objects contained in them, highlighting quantitatively how the idea is correct.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part IV},
pages = {406–421},
numpages = {16},
keywords = {Reinforcement Learning, ObjectGoal Navigation, Visual Navigation},
location = {Glasgow, United Kingdom}
}

@article{10.5555/3190793.3190806,
author = {Shi, W. Y. and Chiao, J. -C.},
title = {Neural network based real-time heart sound monitor using a wireless wearable wrist sensor},
year = {2018},
issue_date = {March     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {94},
number = {3},
issn = {0925-1030},
abstract = {A new method is presented using a wearable wrist sensor to estimate acoustic parameters S1 and S2 of the heart sounds based on the neural network technique. Using the signal processing method, the heart conditions can be analyzed and monitored in real time and potentially in a long term with a wrist device. The velocities and time delays of the cardiac pulse waves in blood vessels were experimentally acquired and calculated at different artery locations on the human body. Signal attenuation of the pulses from the heart to the wrist radial artery was analyzed and a pulse-waveform travel model in blood vessels was proposed. A band-pass filter is applied to the pulse waves at various artery locations to reveal the heart sound features S1 and S2 existed in the pulse waves. In order to obtain accurate acoustic parameters, a neural network with two layers and 500 nonlinear tansig neurons was employed to estimate the heart sounds using the pulse waveforms from the wrist radial artery. It is encouraging to find that the acoustic parameters of estimated heart sounds by the trained neural network have only 1% average errors compared with the original heart sounds. The effects of various analog-to-digital conversion resolutions and sample rates were empirically analyzed. When the maximum value of errors is allowed within 2.15%, a 10,000-Hz sample rate and 12-bit resolution should be an appropriate selection for lower power consumption. Using the trained neural network, the new estimation method has been verified by a sensor with Bluetooth communication strapped on the wrist, thus mobility is not limited for the person whose heart sounds need to be monitored.},
journal = {Analog Integr. Circuits Signal Process.},
month = mar,
pages = {383–393},
numpages = {11},
keywords = {Wireless sensor networks, Stethoscope, Neural network, Digital signal processing}
}

@inproceedings{10.1007/978-3-030-00308-1_5,
author = {Rizzi, Caroline and Johnson, Colin G. and Vargas, Patricia A.},
title = {Fear Learning for Flexible Decision Making in RoboCup: A Discussion},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_5},
doi = {10.1007/978-3-030-00308-1_5},
abstract = {In this paper, we address the stagnation of RoboCup competitions in the fields of contextual perception, real-time adaptation and flexible decision-making, mainly in regards to the Standard Platform League (SPL). We argue that our Situation-Aware FEar Learning (SAFEL) model has the necessary tools to leverage the SPL competition in these fields of research, by allowing robot players to learn the behaviour profile of the opponent team at runtime. Later, players can use this knowledge to predict when an undesirable outcome is imminent, thus having the chance to act towards preventing it. We discuss specific scenarios where SAFEL’s associative learning could help to increase the positive outcomes of a team during a soccer match by means of contextual adaptation.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {59–70},
numpages = {12},
keywords = {Affective computing, Brain emotional model, Contextual fear conditioning, Cognitive learning, RoboCup},
location = {Nagoya, Japan}
}

@article{10.5555/1466818.1466820,
author = {Zak, Andrzej},
title = {Ships classification basing on acoustic signatures},
year = {2008},
issue_date = {April 2008},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {4},
number = {4},
issn = {1790-5052},
abstract = {The paper presents the technique of artificial neural networks used as classifier of hydroacoustic signatures generated by moving ship. The main task of proposed solution is to classify the objects which made the underwater noises. Firstly, the measurements were carried out dynamically by running ship past stationary hydrophones, mounted on tripods 1 m above the sea bottom. Secondly to identify the source of noise the level of vibration were measured on board by accelerometers, which were installed on important components of machinery. On the base of this measurement there was determined the sound pressure level, noise spectra and spectograms, transmission of acoustic energy via the hull into water. More over it was checked by using coherence function that components of underwater noise has its origin in vibrations of ship's mechanisms. Basing on this research it was possible to create the hydroacoustic signature or so called "acoustic portrait" of moving ship. Next during the complex ships' measurements on Polish Navy Test and Evaluation Acoustic Range hydroacoustic noises generated by moving ship were acquired. Basing on these results the classifier of acoustic signatures using artificial neural network was worked out. From the technique of artificial neural networks the Kohonen networks which belongs to group of self organizing networks where chosen to solve the research problem of classification. The choice was caused by some advantages of mentioned kind of neural networks like: they are ideal for finding relationships amongst complex sets of data, they have possibility to self expand the set of answers for new input vectors. To check the correctness of classifier work the research in which the number of right classification for presented and not presented before hydroacoustic signatures were made. Some results of research were presented on this paper. Described method actually is extended and its application is provided as assistant subsystem for hydrolocations systems of Polish Naval ships.},
journal = {WSEAS Trans. Sig. Proc.},
month = apr,
pages = {137–149},
numpages = {13},
keywords = {self-organizing map, hydroacousitc signatures, classification, Kohonen's neural networks}
}

@inproceedings{10.5555/2997189.2997322,
author = {Kumar, M. Pawan and Packer, Benjamin and Koller, Daphne},
title = {Self-paced learning for latent variable models},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1189–1197},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@article{10.1016/j.patrec.2021.06.029,
author = {Chaudhary, Sachin and Dudhane, Akshay and Patil, Prashant W. and Murala, Subrahmanyam and Talbar, Sanjay},
title = {Motion estimation in hazy videos},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.06.029},
doi = {10.1016/j.patrec.2021.06.029},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {130–138},
numpages = {9},
keywords = {Motion estimation, Scene understanding, 65D17, 65D05, 41A10, 41A05}
}

@inproceedings{10.5555/1614108.1614112,
author = {Bromberg, Ilana and Morris, Jeremy and Fosler-Lussier, Eric},
title = {Joint versus independent phonological feature models within CRF phone recognition},
year = {2007},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We compare the effect of joint modeling of phonological features to independent feature detectors in a Conditional Random Fields framework. Joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes. We find that joint modeling provides superior performance to the independent models on the TIMIT phone recognition task. We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent.},
booktitle = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers},
pages = {13–16},
numpages = {4},
location = {Rochester, New York},
series = {NAACL-Short '07}
}

@inproceedings{10.1145/3377930.3390167,
author = {Anjum, Muhammad Sheraz and Ryan, Conor},
title = {Scalability analysis of grammatical evolution based test data generation},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390167},
doi = {10.1145/3377930.3390167},
abstract = {Heuristic-based search techniques have been increasingly used to automate different aspects of software testing. Several studies suggest that variable interdependencies may exist in branching conditions of real-life programs, and these dependencies result in the need for highly precise data values (such as of the form i=j=k) for code coverage analysis. This requirement makes it very difficult for Genetic Algorithm (GA)-based approach to successfully search for the required test data from vast search spaces of real-life programs.Ariadne is the only Grammatical Evolution (GE)-based test data generation system, proposed to date, that uses grammars to exploit variable interdependencies to improve code coverage. Ariadne has been compared favourably to other well-known test data generation techniques in the literature; however, its scalability has not yet been tested for increasingly complex programs.This paper presents the results of a rigorous analysis performed to examine Ariadne's scalability. We also designed and employed a large set of highly scalable 18 benchmark programs for our experiments. Our results suggest that Ariadne is highly scalable as it exhibited 100% coverage across all the programs of increasing complexity with significantly smaller search costs than GA-based approaches, which failed even with huge search budgets.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1213–1221},
numpages = {9},
keywords = {variable interdependencies, software testing, search based software testing, scalability, grammatical evolution, evolutionary testing, code coverage analysis, automatic test data generation},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1007/s11276-020-02536-4,
author = {Ding, Weilong and Wang, Zhe and Chen, Jun and Xia, Yanqing and Wang, Jianwu and Zhao, Zhuofeng},
title = {Potential trend discovery for highway drivers on spatio‐temporal data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-020-02536-4},
doi = {10.1007/s11276-020-02536-4},
abstract = {Inter-city transportation plays an important role in modern cities, and has accumulated massive spatio-temporal data from various sensors by IoT (Internet of things) technologies. Travel characteristics and future trends of highway behind data are valuable for traffic guidance and personalized service. As a routine domain analysis, trend discovery for highway drivers faces challenges in processing efficiency and predictive accuracy. Insufficient profiles of those drivers are available directly, sensible executive latency on huge data is hard to guarantee, and inadequate features among spatio-temporal correlations hinder the analytical accuracy. In this paper, a travel-characteristic based method is proposed to discover the potential trend of payment identity for highway drivers. Considering time, space, subjective preference and objective property, travel characteristics are modeled on monthly data from highway toll stations, through which predictive errors can be reduced by gradient boosting classification. With real-world data of one Chinese provincial highway network, extensive experiments and case studies show that our method has second-level executive latency with more than 85% F1-score for trend discovery.},
journal = {Wirel. Netw.},
month = jul,
pages = {3407–3422},
numpages = {16},
keywords = {Big data, Highway, Ensemble learning, Potential trend, Travel characteristics, Spatio‐temporal data}
}

@article{10.1016/j.specom.2007.07.006,
author = {Yapanel, Umit H. and Hansen, John H. L.},
title = {A new perceptually motivated MVDR-based acoustic front-end (PMVDR) for robust automatic speech recognition},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {2},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2007.07.006},
doi = {10.1016/j.specom.2007.07.006},
abstract = {Acoustic feature extraction from speech constitutes a fundamental component of automatic speech recognition (ASR) systems. In this paper, we propose a novel feature extraction algorithm, perceptual-MVDR (PMVDR), which computes cepstral coefficients from the speech signal. This new feature representation is shown to better model the speech spectrum compared to traditional feature extraction approaches. Experimental results for small (40-word digits) to medium (5k-word dictation) size vocabulary tasks show varying degree of consistent improvements across different experiments; however, the new front-end is most effective in noisy car environments. The PMVDR front-end uses the minimum variance distortionless response (MVDR) spectral estimator to represent the upper envelope of the speech signal. Unlike Mel frequency cepstral coefficients (MFCCs), the proposed front-end does not utilize a filterbank. The effectiveness of the PMVDR approach is demonstrated by comparing speech recognition accuracies with the traditional MFCC front-end and recently proposed PMCC front-end in both noise-free and real adverse environments. For speech recognition in noisy car environments, a 40-word vocabulary task, PMVDR front-end provides a 36% relative decrease in word error rate (WER) over the MFCC front-end. Under simulated speaker stress conditions, a 35-word vocabulary task, the PMVDR front-end yields a 27% relative decrease in the WER. For a noise-free dictation task, a 5k-word vocabulary task, again a relative 8% reduction in the WER is reported. Finally, a novel analysis technique is proposed to quantify noise robustness of an acoustic front-end. This analysis is conducted for the acoustic front-ends analyzed in the paper and results are presented.},
journal = {Speech Commun.},
month = feb,
pages = {142–152},
numpages = {11},
keywords = {Robust speech recognition, Noise-robustness analysis, Acoustic feature extraction}
}

@inproceedings{10.1145/3240508.3240582,
author = {Jiang, Yangbangyan and Yang, Zhiyong and Xu, Qianqian and Cao, Xiaochun and Huang, Qingming},
title = {When to Learn What: Deep Cognitive Subspace Clustering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240582},
doi = {10.1145/3240508.3240582},
abstract = {Subspace clustering aims at clustering data points drawn from a union of low-dimensional subspaces. Recently deep neural networks are introduced into this problem to improve both representation ability and precision for non-linear data. However, such models are sensitive to noise and outliers, since both difficult and easy samples are treated equally. On the contrary, in the human cognitive process, individuals tend to follow a learning paradigm from easy to hard and less to more. In other words, human beings always learn from simple concepts, then absorb more complicated ones gradually. Inspired by such learning scheme, in this paper, we propose a robust deep subspace clustering framework based on the principle of human cognitive process. Specifically, we measure the easinesses of samples dynamically so that our proposed method could gradually utilize instances from easy to more complex ones in a robust way. Meanwhile, a promising solution is designed to update the weights and parameters using an alternative optimization strategy, followed by a theoretical analysis to demonstrated the rationality of the proposed method. Experimental results on three popular benchmark datasets demonstrate the validity of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {718–726},
numpages = {9},
keywords = {subspace clustering, self-paced learning, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/1655925.1656071,
author = {Kim, Semin and Min, Hyunseok and Jeon, Jaehyun and Ro, Yong Man and Han, Seungwan},
title = {Malicious content filtering based on semantic features},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656071},
doi = {10.1145/1655925.1656071},
abstract = {This paper proposes a method to filtering malicious contents using semantic features. In conventional content based approach, low-level features such as color and texture are used to filter malicious contents. But, it is difficult to detect them because of semantic gaps between the low-level features and global concepts. In this paper, global concepts are divided into several semantic features. These semantic features are used to classify the global concept of malicious contents. We design semantic features and construct semantic classifier. In experiment, we evaluate the performance to filter malicious contents by comparing low-level features and semantic features. Results show that our proposed method has better performance than the method using only low-level features.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {802–806},
numpages = {5},
keywords = {semantic, naked, malicious, filtering},
location = {Seoul, Korea},
series = {ICIS '09}
}

@article{10.1016/j.knosys.2017.07.003,
author = {Varshney, Devesh and Kumar, Sandeep and Gupta, Vineet},
title = {Predicting information diffusion probabilities in social networks},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {133},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.07.003},
doi = {10.1016/j.knosys.2017.07.003},
abstract = {Proposed approach predicts information diffusion probability in the social networks.A machine learning based Bayesian network model is trained and tested.Method utilizes diffusion history, network and content based features for modeling.Method utilizes LDA topic model to formulate user interests and message semantics.Experiments are performed on Twitter data to show the effectiveness of the method. In past few years, social networking has significantly contributed to online presence of users. These social networks are hosts to a number of viral phenomena. This has fetched a lot of attention from various researchers and marketers all over the world. Major portion of the studies done in the field of information diffusion through social networks has focused on the problem of influence maximization. These methods demand the diffusion probabilities associated with the links in the social networks to be provided as inputs. However, the problem of computing these diffusion probabilities has not been as widely explored as the problem of influence maximization. In this paper, we tackle the problem of predicting the probabilities of diffusion of a message through the links of a social network. This paper presents a Bayesian network based approach for solving the aforesaid problem. In addition to the features related to the social network, this machine learning based Bayesian framework utilizes user interests and content similarity modeled using the latent topic information. We evaluate the proposed method using the data obtained from the well-known social network platform - Twitter.},
journal = {Know.-Based Syst.},
month = oct,
pages = {66–76},
numpages = {11},
keywords = {Social network analysis, Information diffusion, Diffusion probability, Diffusion network, Bayesian network modeling}
}

@inproceedings{10.1145/3395351.3399421,
author = {Acar, Abbas and Fereidooni, Hossein and Abera, Tigist and Sikder, Amit Kumar and Miettinen, Markus and Aksu, Hidayet and Conti, Mauro and Sadeghi, Ahmad-Reza and Uluagac, Selcuk},
title = {Peek-a-boo: i see your smart home activities, even encrypted!},
year = {2020},
isbn = {9781450380065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395351.3399421},
doi = {10.1145/3395351.3399421},
abstract = {A myriad of IoT devices such as bulbs, switches, speakers in a smart home environment allow users to easily control the physical world around them and facilitate their living styles through the sensors already embedded in these devices. Sensor data contains a lot of sensitive information about the user and devices. However, an attacker inside or near a smart home environment can potentially exploit the innate wireless medium used by these devices to exfiltrate sensitive information from the encrypted payload (i.e., sensor data) about the users and their activities, invading user privacy. With this in mind, in this work, we introduce a novel multi-stage privacy attack against user privacy in a smart environment. It is realized utilizing state-of-the-art machine-learning approaches for detecting and identifying the types of IoT devices, their states, and ongoing user activities in a cascading style by only passively sniffing the network traffic from smart home devices and sensors. The attack effectively works on both encrypted and unencrypted communications. We evaluate the efficiency of the attack with real measurements from an extensive set of popular off-the-shelf smart home IoT devices utilizing a set of diverse network protocols like WiFi, ZigBee, and BLE. Our results show that an adversary passively sniffing the traffic can achieve very high accuracy (above 90%) in identifying the state and actions of targeted smart home devices and their users. To protect against this privacy leakage, we also propose a countermeasure based on generating spoofed traffic to hide the device states and demonstrate that it provides better protection than existing solutions.},
booktitle = {Proceedings of the 13th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {207–218},
numpages = {12},
keywords = {wifi, smart-home, privacy, network traffic, ZigBee, BLE},
location = {Linz, Austria},
series = {WiSec '20}
}

@inproceedings{10.5555/795666.796550,
author = {Alon, N. and Dar, S. and Parnas, M. and Ron, D.},
title = {Testing of clustering},
year = {2000},
isbn = {0769508502},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A set X of points in /spl Rfr//sup d/ is (k,b)-clusterable if X can be partitioned into k subsets (clusters) so that the diameter (alternatively, the radius) of each cluster is at most b. We present algorithms that by sampling from a set X, distinguish between the case that X is (k,b)-clusterable and the case that X is /spl epsiv/-far from being (k,b')-clusterable for any given 0&gt;/spl epsiv//spl les/1 and for b'/spl ges/b. In /spl epsiv/-far from being (k,b')-clusterable we mean that more than /spl epsiv/.|X| points should be removed from X so that it becomes (k,b')-clusterable. We give algorithms for a variety of cost measures that use a sample of size independent of |X|, and polynomial in k and 1//spl epsiv/. Our algorithms can also be used to find approximately good clusterings. Namely, these are clusterings of all but an /spl epsiv/-fraction of the points in X that have optimal (or close to optimal) cost. The benefit of our algorithms is that they construct an implicit representation of such clusterings in time independent of |X|. That is, without actually having to partition all points in X, the implicit representation can be used to answer queries concerning the cluster any given point belongs to.},
booktitle = {Proceedings of the 41st Annual Symposium on Foundations of Computer Science},
pages = {240},
keywords = {statistical analysis, sampling, pattern clustering, optimal cost, lower bounds, cost measures, computational complexity, clustering testing},
series = {FOCS '00}
}

@inproceedings{10.1145/3175516.3175532,
author = {Mujalli, Randa Oqab and L\'{o}pez, Griselda and Garach, Laura},
title = {Modeling Injury Severity of Vehicular Traffic Crashes},
year = {2017},
isbn = {9781450363501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175516.3175532},
doi = {10.1145/3175516.3175532},
abstract = {Data mining techniques constitute an alternative approach that has received increasing attention from researchers in recent years in road safety analysis field. In this paper Bayesian networks were used to develop models in order to identify factors that affect the injury severity of a crash within urban areas based on traffic crashes data on Jordanian roads collected for three years (2009-2011). The following variables were found to have a significant effect on classifying crashes according to their injury severity: lighting of roadway, crash type, road type, crash manner, surface condition, number of lanes, number of vehicles, gradient, type of pavement, traffic control devices, and speed limit. The results of this research can be used to determine the factors that should be taken into consideration when designing a new roadway or for improving safety of existing roads.},
booktitle = {Proceedings of the 2017 International Conference on Automation, Control and Robots},
pages = {51–55},
numpages = {5},
keywords = {traffic crashes, injury severity, Data mining},
location = {Wuhan, China},
series = {ICACR 2017}
}

@inproceedings{10.5555/3172077.3172256,
author = {Ren, Yazhou and Zhao, Peng and Sheng, Yongpan and Yao, Dezhong and Xu, Zenglin},
title = {Robust softmax regression for multi-class classification with self-paced learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Softmax regression, a generalization of Logistic regression (LR) in the setting of multi-class classification, has been widely used in many machine learning applications. However, the performance of softmax regression is extremely sensitive to the presence of noisy data and outliers. To address this issue, we propose a model of robust softmax regression (RoSR) originated from the self-paced learning (SPL) paradigm for multi-class classification. Concretely, RoSR equipped with the soft weighting scheme is able to evaluate the importance of each data instance. Then, data instances participate in the classification problem according to their weights. In this way, the influence of noisy data and outliers (which are typically with small weights) can be significantly reduced. However, standard SPL may suffer from the imbalanced class influence problem, where some classes may have little influence in the training process if their instances are not sensitive to the loss. To alleviate this problem, we design two novel soft weighting schemes that assign weights and select instances locally for each class. Experimental results demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2641–2647},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1016/j.compbiomed.2019.103380,
author = {Kakati, Tulika and Bhattacharyya, Dhruba K. and Barah, Pankaj and Kalita, Jugal K.},
title = {Comparison of Methods for Differential Co-expression Analysis for Disease Biomarker Prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.103380},
doi = {10.1016/j.compbiomed.2019.103380},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {12},
keywords = {Disease biomarkers, Parkinson's disease, Alzheimer's disease, miRNA expression, Gene expression, Empirical study, Differential co-expression analysis}
}

@inproceedings{10.1109/ICASSP.2001.941166,
author = {Taira, S.},
title = {Automatic classification of QAM signals by neural networks},
year = {2001},
isbn = {0780370414},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICASSP.2001.941166},
doi = {10.1109/ICASSP.2001.941166},
abstract = {In this paper, automatic classification of QAM signals including 64-state QAM and 256-state QAM is discussed. Three layer neural networks whose input data are the histogram distribution of instantaneous amplitude at symbol points are used for the classification. The evaluation of the classification performance is carried out for both cases in which the synchronization of symbol timing is assured at the receiver and not assured. Good classification results are obtained by the computer simulations at SNR/spl ges/10 dB. The influence of the number of symbol points which are used for the calculation of the histogram is also discussed.},
booktitle = {Proceedings of the Acoustics, Speech, and Signal Processing, 200. on IEEE International Conference - Volume 02},
pages = {1309–1312},
numpages = {4},
series = {ICASSP '01}
}

@article{10.1016/j.jcp.2007.01.021,
author = {Waterson, N. P. and Deconinck, H.},
title = {Design principles for bounded higher-order convection schemes - a unified approach},
year = {2007},
issue_date = {May, 2007},
publisher = {Academic Press Professional, Inc.},
address = {USA},
volume = {224},
number = {1},
issn = {0021-9991},
url = {https://doi.org/10.1016/j.jcp.2007.01.021},
doi = {10.1016/j.jcp.2007.01.021},
abstract = {The design of bounded, higher-order convection schemes is considered with a view to selecting those discretizations giving good resolution of sharp gradients, while at the same time providing competitive accuracy and convergence behaviour when applied to smooth, recirculating flows. The present work contains a detailed classification and analysis and extensive tables of most non-linear scalar convection schemes so far proposed within the cell-centred, finite-volume framework. The analysis includes a review and comparison of the two most frequently-used non-linear approaches, flux limiters (FL) and normalized variables (NV), and the three major boundedness criteria typically employed: total-variation diminishing (TVD), positivity and the convection-boundedness criterion (CBC). All NV schemes considered are converted to FL form to allow direct comparison and classification of a wide range of schemes. Several specific design principles for positive non-linear schemes are considered and it is shown how these can be applied to understand the relative performance of different approaches. Finally the performance of many existing schemes is compared and ranked on the basis of two scalar convection test cases, one smooth and one discontinuous, which demonstrates the wide variation in both accuracy and convergence behaviour of the various schemes and the benefits of the design principles considered.},
journal = {J. Comput. Phys.},
month = may,
pages = {182–207},
numpages = {26},
keywords = {TVD, Normalized variables, Non-linear, Higher-order, Flux limiters, Discretization, Convection, Bounded}
}

@article{10.1007/s11042-020-09907-1,
author = {Zhong, Yuanhong and Zhang, Jing and Zhou, Zhaokun and Cheng, Xinyu and Huang, Guan and Li, Qiang},
title = {Recovery of image and video based on compressive sensing via tensor approximation and Spatio-temporal correlation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {5},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09907-1},
doi = {10.1007/s11042-020-09907-1},
abstract = {In recent years, block-based compressive sensing (BCS) has been extensively studied because it can reduce computational complexity and data storage by dividing the image into smaller patches, but the performance of the reconstruction algorithm is not satisfactory. In this paper, a new reconstruction model for image and video is proposed. The model makes full use of spatio-temporal correlation and utilizes low-rank tensor approximation to improve the quality of the reconstructed image and video. For image recovery, the proposed model obtains a low-rank approximation of a tensor formed by non-local similar patches, and improves the reconstruction quality from a spatial perspective by combining non-local similarity and low-rank property. For video recovery, the reconstruction process is divided into two phases. In the first phase, each frame of the video sequence is regarded as an independent image to be reconstructed by taking advantage of spatial property. The second phase performs tensor approximation through searching similar patches within frames near the target frame, to achieve reconstruction by putting the spatio-temporal correlation into full play. The resulting model is solved by an efficient Alternating Direction Method of Multipliers (ADMM) algorithm. A series of experiments show that the quality of the proposed model is comparable to the current state-of-the-art recovery methods.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {7433–7450},
numpages = {18},
keywords = {Spatio-temporal correlation, High order singular value decomposition (HOSVD), Low-rank tensor approximation, Image and video recovery, Block-based compressive sensing}
}

@article{10.1016/j.eswa.2019.06.016,
author = {Huang, Hai-Hui and Liang, Yong},
title = {An integrative analysis system of gene expression using self-paced learning and SCAD-Net},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.06.016},
doi = {10.1016/j.eswa.2019.06.016},
journal = {Expert Syst. Appl.},
month = nov,
pages = {102–112},
numpages = {11},
keywords = {SPS-NL, SPS-Net, SCAD-Net, NSCLC, Gene expression, Variable selection, Regularization, Meta-analysis, Integrative analysis system}
}

@inproceedings{10.1007/978-3-319-06826-8_29,
author = {Kagawa, Kosuke and Tamagawa, Susumu and Yamaguchi, Takahira},
title = {An Automatic sameAs Link Discovery from Wikipedia},
year = {2013},
isbn = {978-3-319-06825-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-06826-8_29},
doi = {10.1007/978-3-319-06826-8_29},
abstract = {Spelling variants of words or word sense ambiguity takes many costs in such processes as Data Integration, Information Searching, data preprocessing for Data Mining, and so on. It is useful to construct relations between a word or phrases and a representative name of the entity to meet these demands. To reduce the costs, this paper discusses how to automatically discover “sameAs” and “meaningOf” links from Japanese Wikipedia. In order to do so, we gathered relevant features such as IDF, string similarity, number of hypernym, and so on. We have identified the link-based score on salient features based on SVM results with 960,000 anchor link pairs. These case studies show us that our link discovery method goes well with more than 70&nbsp;% precision/recall rate.},
booktitle = {Semantic Technology: Third Joint International Conference, JIST 2013, Seoul, South Korea, November 28--30, 2013, Revised Selected Papers},
pages = {399–413},
numpages = {15},
keywords = {Ontology, Wikipedia, sameAs link, Synonym, Disambiguation, Spelling variants},
location = {Seoul, Korea (Republic of)}
}

@inproceedings{10.1007/978-3-030-58604-1_7,
author = {Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
title = {Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments},
year = {2020},
isbn = {978-3-030-58603-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58604-1_7},
doi = {10.1007/978-3-030-58604-1_7},
abstract = {We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some transfer, we find significantly lower absolute performance in the continuous setting – suggesting that performance in prior ‘navigation-graph’ settings may be inflated by the strong implicit assumptions. Code at 
.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII},
pages = {104–120},
numpages = {17},
keywords = {Embodied agents, Vision-and-Language Navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3042817.3042922,
author = {Das, Mrinal Kanti and Bhattacharya, Suparna and Bhattacharyya, Chiranjib and Gopinath, K.},
title = {Subtle topic models and discovering subtly manifested software concerns automatically},
year = {2013},
publisher = {JMLR.org},
abstract = {In a recent pioneering approach LDA was used to discover cross cutting concerns (CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed in (Williamson et al., 2010) for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models (STM). STM uses a generalized stick breaking process (GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {II–253–II–261},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{10.1007/978-3-642-33712-3_58,
author = {B$#261;k, S$#322;awomir and Charpiat, Guillaume and Corv\'{e}e, Etienne and Br\'{e}mond, Fran$#231;ois and Thonnat, Monique},
title = {Learning to match appearances by correlations in a covariance metric space},
year = {2012},
isbn = {9783642337116},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33712-3_58},
doi = {10.1007/978-3-642-33712-3_58},
abstract = {This paper addresses the problem of appearance matching across disjoint camera views. Significant appearance changes, caused by variations in view angle, illumination and object pose, make the problem challenging. We propose to formulate the appearance matching problem as the task of learning a model that selects the most descriptive features for a specific class of objects. Learning is performed in a covariance metric space using an entropy-driven criterion. Our main idea is that different regions of the object appearance ought to be matched using various strategies to obtain a distinctive representation. The proposed technique has been successfully applied to the person re-identification problem, in which a human appearance has to be matched across non-overlapping cameras. We demonstrate that our approach improves state of the art performance in the context of pedestrian recognition.},
booktitle = {Proceedings of the 12th European Conference on Computer Vision - Volume Part III},
pages = {806–820},
numpages = {15},
keywords = {re-identification, covariance matrix, appearance matching},
location = {Florence, Italy},
series = {ECCV'12}
}

@article{10.4018/IJBDCN.2019070106,
author = {Patil, Vilas K and Nagarale, P.P.},
title = {Prediction of L10 and Leq Noise Levels Due to Vehicular Traffic in Urban Area Using ANN and Adaptive Neuro-Fuzzy Interface System (ANFIS) Approach},
year = {2019},
issue_date = {Jul 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-0631},
url = {https://doi.org/10.4018/IJBDCN.2019070106},
doi = {10.4018/IJBDCN.2019070106},
abstract = {Recently in urban areas, road traffic noise is one of the primary sources of noise pollution. Variation in noise level is impacted by the synthesis of traffic and the percentage of heavy vehicles. Presentation to high noise levels may cause serious impact on the health of an individual or community residing near the roadside. Thus, predicting the vehicular traffic noise level is important. The present study aims at the formulation of regression, an artificial neural network (ANN) and an adaptive neuro-fuzzy interface system (ANFIS) model using the data of observed noise levels, traffic volume, and average speed of vehicles for the prediction of L10 and Leq. Measured noise levels are compared to the noise levels predicted by the experimental model. It is observed that the ANFIS approach is more superior when compared to output given by regression and an ANN model. Also, there exists a positive correlation between measured and predicted noise levels. The proposed ANFIS model can be utilized as a tool for traffic direction and planning of new roads in zones of similar land use pattern.},
journal = {Int. J. Bus. Data Commun. Netw.},
month = jul,
pages = {92–105},
numpages = {14},
keywords = {Vehicular Traffic Noise Prediction, Regression, Modeling, Artificial Neural Network, ANFIS}
}

@article{10.1016/j.neucom.2015.11.101,
author = {Gu, Ke and Zhai, Guangtao and Lin, Weisi and Yang, Xiaokang and Zhang, Wenjun},
title = {Learning a blind quality evaluation engine of screen content images},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {196},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.11.101},
doi = {10.1016/j.neucom.2015.11.101},
abstract = {We in this paper investigate how to blindly predict the visual quality of a screen content image (SCI). With the popularity of multi-client and remote-controlling systems, SCIs and the relevant applications have been a hot research topic. In general, SCIs contain texts or graphics in cartoons, ebooks or captures of computer screens. As for blind quality assessment (QA) of natural scene images (NSIs), it has been well established since NSIs possess certain statistical properties. SCIs however do not have reliable statistic models so far and thus the associated blind QA task is hard to be addressed. Aiming at solving this problem, we first extract 13 perceptual-inspired features with the free energy based brain theory and structural degradation model. In order to avoid the overfitting and guarantee the independence of training and testing samples, we then collect 100,000 images and use their objective quality scores computed via a high-accuracy full-reference QA method for SCIs as labels, before learning a new blind quality measure from aforementioned 13 features to the objective quality score. Experimental results performed on a large-scale screen image quality assessment database (SIQAD) demonstrate that the proposed blind quality metric has a good correlation with human perception of quality, even superior to state-of-the-art full-, reduced- and no-reference QA algorithms.},
journal = {Neurocomput.},
month = jul,
pages = {140–149},
numpages = {10},
keywords = {Statistical model, Screen content images (SCIs), Machine learning, Image quality assessment (IQA), Blind/no-reference (NR)}
}

@inproceedings{10.1145/3412841.3442106,
author = {Liu, Tianen and Khuri, Natalia},
title = {Classification of drug prescribing information using long short-term memory networks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442106},
doi = {10.1145/3412841.3442106},
abstract = {Information about drug's safety and efficacy is publicly available to healthcare providers and consumers in the United States. Yet, it remains challenging to find this information for special populations of patients. These populations include pregnant, lactating, nursing women, elderly, and pediatric patients. Motivated by the unmet need for the accurate and efficient extraction of information, we trained a multi-class Long Short-Term Memory classifier with over 90,000 semi-structured labeled texts. The classifier achieved excellent performance when tested on an unseen dataset of 20,000 texts, reaching between 95% to 99% accuracy for the five classes. The classifier significantly outperformed the baseline model trained using Na\"{\i}ve Bayes algorithm, especially in the classification of texts containing information relevant to nursing mothers.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1086–1089},
numpages = {4},
keywords = {biomedical text classification, drug labels, long short-term memory network, na\"{\i}ve bayes classifier},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.5555/595586.595657,
author = {Tzafestas, Spyros G.},
title = {Editorial: Neural-Fuzzy Applications in Computer Vision},
year = {2000},
issue_date = {December 2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0921-0296},
journal = {J. Intell. Robotics Syst.},
month = dec,
pages = {309–315},
numpages = {7}
}

@article{10.1016/j.eswa.2008.08.062,
author = {Suh, Jong Hwan and Park, Sang Chan},
title = {Service-oriented Technology Roadmap (SoTRM) using patent map for R&amp;D strategy of service industry},
year = {2009},
issue_date = {April, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.08.062},
doi = {10.1016/j.eswa.2008.08.062},
abstract = {As a consequence of the service economy, R&amp;D of the service industry has become more essential nowadays. Therefore, technology roadmaps are required for selection and concentration of services and related technologies. However, now there are problems and challenging issues as follows. First, there is no objective and systematic method or analysis tool to evaluate emerging technologies for services. Second, current technology roadmaps do not provide technology's priority oriented to the service side. Therefore, we propose a patent map and a Service-oriented Technology Roadmap using the patent map, i.e. SoTRM. Our patent map is a three-dimensional visualization method and analysis tool based on keywords, which contributes to evaluating emerging technologies for services. It does not only overcome the subjectivity of experts, but it also discovers technologies missed out by experts initially. And SoTRM is a technology roadmap customized for the service industry. Based on four layers of patents, keywords, technologies, and services, the layer of service-oriented technologies provides the order of technologies in a service-oriented aspect. It also gives guidelines to assign roles in R&amp;D to public and private sectors. As a result, we provide an objective and systematic framework required to form a technology roadmap oriented to services for R&amp;D strategy of the service industry. Eventually, it helps decision makers from public and private sectors to select and concentrate on the first things among services and the related technologies in R&amp;D of the service industry, and thereby to find the direction of distributing investment funds into technologies for services.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {6754–6772},
numpages = {19},
keywords = {Technology roadmap, Service-oriented, Portfolio matrix, Patent analysis, Clustering}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00022,
author = {Azanza, Maider and Irastorza, Arantza and Medeiros, Raul and D\'{\i}az, Oscar},
title = {Onboarding in software product lines: concept maps as welcome guides},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00022},
doi = {10.1109/ICSE-SEET52601.2021.00022},
abstract = {With a volatile labour and technological market, onboarding is becoming increasingly important. The process of incorporating a new developer, a.k.a. the newcomer, into a software development team is reckoned to be lengthy, frustrating and expensive. Newcomers face personal, interpersonal, process and technical barriers during their incorporation, which, in turn, affects the overall productivity of the whole team. This problem exacerbates for Software Product Lines (SPLs), where their size and variability combine to make onboarding even more challenging, even more so for developers that are transferred from the Application Engineering team into the Domain Engineering team, who will be our target newcomers. This work presents concept maps on the role of sensemaking scaffolds to help to introduce these newcomers into the SPL domain. Concept maps, used as knowledge visualisation tools, have been proven to be helpful for meaningful learning. Our main insight is to capture concepts of the SPL domain and their interrelationships in a concept map, and then, present them incrementally, helping newcomers grasp the SPL and aiding them in exploring it in a guided manner while avoiding information overload. This work's contributions are four-fold. First, concept maps are proposed as a representation to introduce newcomers into the SPL domain. Second, concept maps are presented as the means for a guided exploration of the SPL core assets. Third, a feature-driven concept map construction process is introduced. Last, the usefulness of concept maps as guides for SPL onboarding is tested through a formative evaluation.Link to the online demo: https://rebrand.ly/wacline-cmap},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {122–133},
numpages = {12},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@article{10.1016/j.jss.2015.05.006,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
title = {Feature extraction approaches from natural language requirements for reuse in software product lines},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.05.006},
doi = {10.1016/j.jss.2015.05.006},
abstract = {Hybrid NLP approaches were more common for extracting textual requirements.There is a mixture of automated and semi-automated approaches from IR and data mining.Support tools were not made available to the public.Not all studies use software metrics in conjunction with experiments and case studies.Reconfirm practitioners guidelines' absence from selected studies (Alves et al., 2010). Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
journal = {J. Syst. Softw.},
month = aug,
pages = {132–149},
numpages = {18},
keywords = {Systematic literature review, Software product lines, Requirements reuse, Natural language requirements, Feature extractions}
}

@inproceedings{10.1145/2970276.2970327,
author = {Moonen, Leon and Di Alesio, Stefano and Binkley, David and Rolfsnes, Thomas},
title = {Practical guidelines for change recommendation using association rule mining},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970327},
doi = {10.1145/2970276.2970327},
abstract = {Association rule mining is an unsupervised learning technique that infers relationships among items in a data set. This technique has been successfully used to analyze a system's change history and uncover evolutionary coupling between system artifacts. Evolutionary coupling can, in turn, be used to recommend artifacts that are potentially affected by a given set of changes to the system. In general, the quality of such recommendations is affected by (1) the values selected for various parameters of the mining algorithm, (2) characteristics of the set of changes used to derive a recommendation, and (3) characteristics of the system's change history for which recommendations are generated.In this paper, we empirically investigate the extent to which certain choices for these factors affect change recommendation. Specifically, we conduct a series of systematic experiments on the change histories of two large industrial systems and eight large open source systems, in which we control the size of the change set for which to derive a recommendation, the measure used to assess the strength of the evolutionary coupling, and the maximum size of historical changes taken into account when inferring these couplings. We use the results from our study to derive a number of practical guidelines for applying association rule mining for change recommendation.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {732–743},
numpages = {12},
keywords = {parameter tuning, change recommendations, change impact analysis, association rule mining, Evolutionary coupling},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.5555/3540261.3541282,
author = {Zhang, Jiwen and Wei, Zhongyu and Fan, Jianqing and Peng, Jiajie},
title = {Curriculum learning for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1021},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-71827-5_11,
author = {Estienne, Th\'{e}o and Vakalopoulou, Maria and Battistella, Enzo and Carr\'{e}, Alexandre and Henry, Th\'{e}ophraste and Lerousseau, Marvin and Robert, Charlotte and Paragios, Nikos and Deutsch, Eric},
title = {Deep Learning Based Registration Using&nbsp;Spatial Gradients and Noisy Segmentation Labels},
year = {2020},
isbn = {978-3-030-71826-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71827-5_11},
doi = {10.1007/978-3-030-71827-5_11},
abstract = {Image registration is one of the most challenging problems in medical image analysis. In the recent years, deep learning based approaches became quite popular, providing fast and performing registration strategies. In this short paper, we summarise our work presented on Learn2Reg challenge 2020. The main contributions of our work rely on (i) a symmetric formulation, predicting the transformations from source to target and from target to source simultaneously, enforcing the trained representations to be similar and (ii) integration of variety of publicly available datasets used both for pretraining and for augmenting segmentation labels. Our method reports a mean dice of 0.64 for task 3 and 0.85 for task 4 on the test sets, taking third place on the challenge. Our code and models are publicly available at  and .},
booktitle = {Segmentation, Classification, and Registration of Multi-Modality Medical Imaging Data: MICCAI 2020 Challenges, ABCs 2020, L2R 2020, TN-SCUI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {87–93},
numpages = {7},
location = {Lima, Peru}
}

@inproceedings{10.1145/3474124.3474171,
author = {Gupta, Manas and Chandra, Satish},
title = {Speech Emotion Recognition Using MFCC and Wide Residual Network},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474171},
doi = {10.1145/3474124.3474171},
abstract = {Emotion recognition from speech has been a topic of research from many years due to its importance in human-computer interaction. While a lot of work has been done upon recognizing emotions through facial expressions, recognition of emotions through speech is still a challenging task in Machine Learning due to the obscure knowledge about the effectiveness of different speech features. In this work, Mel-frequency cepstral coefficients (MFCCs) has been used as a feature extractor for speech files. Further, classification of speech signals has been done using Convolution Neural Network (CNN) in the form of Wide Residual Network (WRN) followed by a Dense Neural Network (DNN). To train and test this approach we used Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and Toronto Emotional Speech Set (TESS) databases together. Results show that the proposed approach is gives an accuracy of 90.09% in recognizing emotions from speech into 8 categories.},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {320–327},
numpages = {8},
keywords = {Wide Residual Network, Neural Networks, MFCC},
location = {Noida, India},
series = {IC3-2021}
}

@inproceedings{10.1007/978-3-030-58607-2_18,
author = {Qi, Yuankai and Pan, Zizheng and Zhang, Shengping and van den Hengel, Anton and Wu, Qi},
title = {Object-and-Action Aware Model for Visual Language Navigation},
year = {2020},
isbn = {978-3-030-58606-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58607-2_18},
doi = {10.1007/978-3-030-58607-2_18},
abstract = {Vision-and-Language Navigation (VLN) is unique in that it requires turning relatively general natural-language instructions into robot agent actions, on the basis of visible environments. This requires to extract value from two very different types of natural-language information. The first is object description (e.g., ‘table’, ‘door’), each presenting as a tip for the agent to determine the next action by finding the item visible in the environment, and the second is action specification (e.g., ‘go straight’, ‘turn left’) which allows the robot to directly predict the next movements without relying on visual perceptions. However, most existing methods pay few attention to distinguish these information from each other during instruction encoding and mix together the matching between textual object/action encoding and visual perception/orientation features of candidate viewpoints. In this paper, we propose an Object-and-Action Aware Model (OAAM) that processes these two different forms of natural language based instruction separately. This enables each process to match object-centered/action-centered instruction to their own counterpart visual perception/action orientation flexibly. However, one side-issue caused by above solution is that an object mentioned in instructions may be observed in the direction of two or more candidate viewpoints, thus the OAAM may not predict the viewpoint on the shortest path as the next action. To handle this problem, we design a simple but effective path loss to penalize trajectories deviating from the ground truth path. Experimental results demonstrate the effectiveness of the proposed model and path loss, and the superiority of their combination with a 50% SPL score on the R2R dataset and a 40% CLS score on the R4R dataset in unseen environments, outperforming the previous state-of-the-art.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X},
pages = {303–317},
numpages = {15},
keywords = {Reward shaping, Modular network, Vision-and-Language Navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/2968826.2968936,
author = {Valera, Isabel and Ghahramani, Zoubin},
title = {General table completion using a bayesian nonparametric model},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Even though heterogeneous databases can be found in a broad variety of applications, there exists a lack of tools for estimating missing data in such databases. In this paper, we provide an efficient and robust table completion tool, based on a Bayesian nonparametric latent feature model. In particular, we propose a general observation model for the Indian buffet process (IBP) adapted to mixed continuous (real-valued and positive real-valued) and discrete (categorical, ordinal and count) observations. Then, we propose an inference algorithm that scales linearly with the number of observations. Finally, our experiments over five real databases show that the proposed approach provides more robust and accurate estimates than the standard IBP and the Bayesian probabilistic matrix factorization with Gaussian observations.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {981–989},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{10.1016/j.jss.2017.01.031,
author = {Lucas, Edson M. and Oliveira, Toacy C. and Farias, Kleinner and Alencar, Paulo S.C.},
title = {CollabRDL},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.031},
doi = {10.1016/j.jss.2017.01.031},
abstract = {Extends the Reuse Description Language (RDL) to address collaborative reuse processesIncludes three new commands in RDL, including ROLE, PARALLEL and DOPARALLELCollabRDL can represent parallelism, synchronization, multiple-choice and roleCollabRDL is capable of representing critical workflow patterns Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort.},
journal = {J. Syst. Softw.},
month = sep,
pages = {505–527},
numpages = {23},
keywords = {Software reuse, Reuse process, Language, Framework, Collaboration}
}

@inproceedings{10.5555/645654.665656,
author = {Pal, Sankar K.},
title = {Soft Computing Pattern Recognition: Principles, Integrations, and Data Mining},
year = {2001},
isbn = {3540430709},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Relevance of fuzzy logic, artificial neural networks, genetic algorithms and rough sets to pattern recognition and image processing problems is described through examples. Different integrations of these soft computing tools are illustrated. Evolutionary rough fuzzy network which is based on modular principle is explained, as an example of integrating all the four tools for efficient classification and rule generation, with its various characterstics. Significance of soft computing approach in data mining and knowledge discovery is finally discussed along with the scope of future research.},
booktitle = {Proceedings of the Joint JSAI 2001 Workshop on New Frontiers in Artificial Intelligence},
pages = {261–271},
numpages = {11}
}

@article{10.1145/3287058,
author = {Maruri, H\'{e}ctor A. Cordourier and Lopez-Meyer, Paulo and Huang, Jonathan and Beltman, Willem Marco and Nachman, Lama and Lu, Hong},
title = {V-Speech: Noise-Robust Speech Capturing Glasses Using Vibration Sensors},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287058},
doi = {10.1145/3287058},
abstract = {Smart glasses are often used in public environments or industrial scenarios that are relatively noisy. Background noise and sound from competing speakers deteriorate voice communication or performance of automatic speech recognition (ASR). Typically, signal processing techniques are used to reduce noise and enhance voice quality, but they have limitations in performance, hardware and/or computing resources. Voice capturing techniques using bone conducting on the head have been proposed in some experimental and commercial devices, with good robustness against environmental noise, but limited by signal distortions inherent to the capturing method. We present V-Speech, a novel sensing and signal processing solution that enables speech recognition and human-to-human communication in very noisy environments. It captures the voice signal with a vibration sensor located in the nasal pads of smart glasses and performs a transformation to the sensor signal in order to mimic that of a regular microphone in low noise conditions. The signal transformation is key, as it eliminates the "nasal distortion" that is introduced for nasal phonemes in the speech induced vibrations of the nasal bone. The output of V-Speech has low noise, sounds natural, and can be used in voice communication or as input to an off-the-shelf ASR service. We evaluated V-Speech in noise-free and noisy conditions with 30 volunteer speakers uttering 145 phrases and validated its performance on ASR engines and with assessments of voice quality using the Perceptual Evaluation of Speech Quality (PESQ) metric. The results show in extreme noise conditions a mean improvement of 50% for Word Error Rate (WER), and 1.0 on a scale of 5.0 for PESQ. In addition, real life recordings were made under various representative noise conditions, some with sound pressure levels of 93 dBA, which require hearing protection. Subjective listening tests were conducted according to a modified ITU P.835 approach to determine intelligibility, naturalness and overall quality. Under these extreme conditions, where V-Speech achieved 30 dB SNR, subjective results show the speech is intelligible, and the naturalness of the speech is rated as fair to good. This enables clear voice communication in challenging work environments, for example in places with industrial, factory, mining and construction noise. With our proposed smart switching technique between a regular microphone signal and V-Speech, the optimal quality can be maintained from low to high noise conditions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {180},
numpages = {23},
keywords = {Voice capturing, Vibration sensing, Smart glasses, Head worn devices, Accelerometer}
}

@article{10.1016/j.artmed.2006.06.003,
author = {Botros, Andrew and van Dijk, Bas and Killian, Matthijs},
title = {AutoNRTTM: An automated system that measures ECAP thresholds with the Nucleus® FreedomTM cochlear implant via machine intelligence},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {40},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2006.06.003},
doi = {10.1016/j.artmed.2006.06.003},
abstract = {Objective: AutoNRT(TM) is an automated system that measures electrically evoked compound action potential (ECAP) thresholds from the auditory nerve with the Nucleus^(R) Freedom(TM) cochlear implant. ECAP thresholds along the electrode array are useful in objectively fitting cochlear implant systems for individual use. This paper provides the first detailed description of the AutoNRT algorithm and its expert systems, and reports the clinical success of AutoNRT to date. Methods: AutoNRT determines thresholds by visual detection, using two decision tree expert systems that automatically recognise ECAPs. The expert systems are guided by a dataset of 5393 neural response measurements. The algorithm approaches threshold from lower stimulus levels, ensuring recipient safety during postoperative measurements. Intraoperative measurements use the same algorithm but proceed faster by beginning at stimulus levels much closer to threshold. When searching for ECAPs, AutoNRT uses a highly specific expert system (specificity of 99% during training, 96% during testing; sensitivity of 91% during training, 89% during testing). Once ECAPs are established, AutoNRT uses an unbiased expert system to determine an accurate threshold. Throughout the execution of the algorithm, recording parameters (such as implant amplifier gain) are automatically optimised when needed. Results: In a study that included 29 intraoperative and 29 postoperative subjects (a total of 418 electrodes), AutoNRT determined a threshold in 93% of cases where a human expert also determined a threshold. When compared to the median threshold of multiple human observers on 77 randomly selected electrodes, AutoNRT performed as accurately as the 'average' clinician. Conclusions: AutoNRT has demonstrated a high success rate and a level of performance that is comparable with human experts. It has been used in many clinics worldwide throughout the clinical trial and commercial launch of Nucleus Custom Sound(TM) Suite, significantly streamlining the clinical procedures associated with cochlear implant use.},
journal = {Artif. Intell. Med.},
month = may,
pages = {15–28},
numpages = {14},
keywords = {Threshold estimation, Pattern recognition, Neural response telemetry, Machine learning, Electrically evoked compound action potential, Decision trees, Cochlear implants, Automated systems}
}

@article{10.1016/j.patcog.2016.11.018,
author = {Ma, Xiaolong and Zhu, Xiatian and Gong, Shaogang and Xie, Xudong and Hu, Jianming and Lam, Kin-Man and Zhong, Yisheng},
title = {Person re-identification by unsupervised video matching},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {65},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.018},
doi = {10.1016/j.patcog.2016.11.018},
journal = {Pattern Recogn.},
month = may,
pages = {197–210},
numpages = {14},
keywords = {Time shift, Spatio-temporal pyramids, Temporal sequence matching, Video matching, Gait recognition, Action recognition, Person re-identification}
}

@inproceedings{10.5555/3020419.3020482,
author = {Weng, Paul},
title = {Axiomatic foundations for a class of generalized expected utility: algebraic expected utility},
year = {2006},
isbn = {0974903922},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, we provide two axiomatizations of algebraic expected utility, which is a particular generalized expected utility, in a von Neumann-Morgenstern setting, i.e. uncertainty representation is supposed to be given and here to be described by a plausibility measure valued on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility,...) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty representation, autoduality of the decision criterion and possibility of modeling decision maker's attitude toward uncertainty.},
booktitle = {Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence},
pages = {520–527},
numpages = {8},
location = {Cambridge, MA, USA},
series = {UAI'06}
}

@article{10.1016/j.neunet.2015.09.005,
author = {Beyeler, Michael and Oros, Nicolas and Dutt, Nikil and Krichmar, Jeffrey L.},
title = {A GPU-accelerated cortical neural network model for visually guided robot navigation},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {72},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2015.09.005},
doi = {10.1016/j.neunet.2015.09.005},
abstract = {Humans and other terrestrial animals use vision to traverse novel cluttered environments with apparent ease. On one hand, although much is known about the behavioral dynamics of steering in humans, it remains unclear how relevant perceptual variables might be represented in the brain. On the other hand, although a wealth of data exists about the neural circuitry that is concerned with the perception of self-motion variables such as the current direction of travel, little research has been devoted to investigating how this neural circuitry may relate to active steering control. Here we present a cortical neural network model for visually guided navigation that has been embodied on a physical robot exploring a real-world environment. The model includes a rate based motion energy model for area V1, and a spiking neural network model for cortical area MT. The model generates a cortical representation of optic flow, determines the position of objects based on motion discontinuities, and combines these signals with the representation of a goal location to produce motor commands that successfully steer the robot around obstacles toward the goal. The model produces robot trajectories that closely match human behavioral data. This study demonstrates how neural signals in a model of cortical area MT might provide sufficient motion information to steer a physical robot on human-like paths around obstacles in a real-world environment, and exemplifies the importance of embodiment, as behavior is deeply coupled not only with the underlying model of brain function, but also with the anatomical constraints of the physical body it controls.},
journal = {Neural Netw.},
month = dec,
pages = {75–87},
numpages = {13},
keywords = {Spiking neural network, Robot navigation, Obstacle avoidance, Motion energy, MT, GPU}
}

@inproceedings{10.5555/978-3-030-59716-0_fm,
title = {Front Matter},
year = {2020},
isbn = {978-3-030-59715-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part III},
pages = {i–xxxvi},
location = {Lima, Peru}
}

@article{10.1145/3385398,
author = {Shaikhha, Amir and Elseidy, Mohammed and Mihaila, Stephan and Espino, Daniel and Koch, Christoph},
title = {Synthesis of Incremental Linear Algebra Programs},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3385398},
doi = {10.1145/3385398},
abstract = {This article targets the Incremental View Maintenance (IVM) of sophisticated analytics (such as statistical models, machine learning programs, and graph algorithms) expressed as linear algebra programs. We present LAGO, a unified framework for linear algebra that automatically synthesizes efficient incremental trigger programs, thereby freeing the user from error-prone manual derivations, performance tuning, and low-level implementation details. The key technique underlying our framework is abstract interpretation, which is used to infer various properties of analytical programs. These properties give the reasoning power required for the automatic synthesis of efficient incremental triggers. We evaluate the effectiveness of our framework on a wide range of applications from regression models to graph computations.},
journal = {ACM Trans. Database Syst.},
month = aug,
articleno = {12},
numpages = {44},
keywords = {materialized views, incremental linear algebra, domain-specific languages, compilation, abstract interpretation, Incremental view maintenance (IVM)}
}

@inproceedings{10.1007/978-3-030-68452-5_30,
author = {Meena, Leetesh and Chaurasiya, Vijay Kumar and Purohit, Neetesh and Singh, Dhananjay},
title = {Comparison of SVM and Random Forest Methods for Online Signature Verification},
year = {2020},
isbn = {978-3-030-68451-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68452-5_30},
doi = {10.1007/978-3-030-68452-5_30},
abstract = {Signatures are widely used for authentication purposes. Signature verification has many applications in banking, in crossing international borders, in boarding of planes etc. For verifying identity of a person, signature is legally and widely accepted biometric trait. This work presents two simple and efficient methods for online signature verification. The paper proposes Support Vector Machine (SVM) and Random Forest Method for verifying online signatures. To measure the performance of algorithm f1 score is used and experiments were performed on SUSIG dataset. In Method-1, data after preprocessing is taken as feature set and in Method-2 feature vector are made by concatenation of bins of different attributes of the signature. The attributes taken are kth derivative of x and y coordinates, and kth derivatives of pressure. The classification is performed on extracted features using SVM and Random Forest. The performances of proposed methods were evaluated by using confusion matrix on SUSIG dataset. Results show that the proposed methods are capable of verifying online signatures with acceptable level of accuracy.},
booktitle = {Intelligent Human Computer Interaction: 12th International Conference, IHCI 2020, Daegu, South Korea, November 24–26, 2020, Proceedings, Part II},
pages = {288–299},
numpages = {12},
keywords = {Online signature., Feature extraction, Confusion matrix, Classification, Random forest, SVM},
location = {Daegu, Korea (Republic of)}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Variability, Performance feature interaction, Highly configurable software system, Feature-interaction prediction, Feature interaction, Feature, Control-flow feature interaction}
}

@inproceedings{10.1007/11527886_77,
author = {Wang, Yang},
title = {Constraint-Sensitive privacy management for personalized web-based systems},
year = {2005},
isbn = {3540278850},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527886_77},
doi = {10.1007/11527886_77},
abstract = {This research aims at reconciling web personalization with privacy constraints imposed by legal restrictions and by users' privacy preferences. We propose a software product line architecture approach, where our privacyenabling user modeling architecture can dynamically select personalization methods that satisfy current privacy constraints to provide personalization services. A feasibility study is being carried out with the support of an existing user modeling server and a software architecture based development environment.},
booktitle = {Proceedings of the 10th International Conference on User Modeling},
pages = {524–526},
numpages = {3},
location = {Edinburgh, UK},
series = {UM'05}
}

@article{10.1155/2018/9032945,
author = {Chalearnnetkul, Pongsagorn and Suvonvorn, Nikom and Gutierrez, Pedro Antonio},
title = {Multiview Layer Fusion Model for Action Recognition Using RGBD Images},
year = {2018},
issue_date = {2018},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2018},
issn = {1687-5265},
url = {https://doi.org/10.1155/2018/9032945},
doi = {10.1155/2018/9032945},
abstract = {Vision-based action recognition encounters different challenges in practice, including recognition of the subject from any viewpoint, processing of data in real time, and offering privacy in a real-world setting. Even recognizing profile-based human actions, a subset of vision-based action recognition, is a considerable challenge in computer vision which forms the basis for an understanding of complex actions, activities, and behaviors, especially in healthcare applications and video surveillance systems. Accordingly, we introduce a novel method to construct a layer feature model for a profile-based solution that allows the fusion of features for multiview depth images. This model enables recognition from several viewpoints with low complexity at a real-time running speed of 63 fps for four profile-based actions: standing/walking, sitting, stooping, and lying. The experiment using the Northwestern-UCLA 3D dataset resulted in an average precision of 86.40%. With the i3DPost dataset, the experiment achieved an average precision of 93.00%. With the PSU multiview profile-based action dataset, a new dataset for multiple viewpoints which provides profile-based action RGBD images built by our group, we achieved an average precision of 99.31%.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {22}
}

@article{10.1007/s11257-011-9114-8,
author = {Wang, Yang and Kobsa, Alfred},
title = {A PLA-based privacy-enhancing user modeling framework and its evaluation},
year = {2013},
issue_date = {March     2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-011-9114-8},
doi = {10.1007/s11257-011-9114-8},
abstract = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data.},
journal = {User Modeling and User-Adapted Interaction},
month = mar,
pages = {41–82},
numpages = {42},
keywords = {User modeling, User experiment, Product line architecture, Privacy preferences, Privacy laws, Performance evaluation, Disclosure behavior, Compliance}
}

@article{10.1016/j.patrec.2019.03.021,
author = {Ding, Songtao and Qu, Shiru and Xi, Yuling and Sangaiah, Arun Kumar and Wan, Shaohua},
title = {Image caption generation with high-level image features},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {123},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.03.021},
doi = {10.1016/j.patrec.2019.03.021},
journal = {Pattern Recogn. Lett.},
month = may,
pages = {89–95},
numpages = {7},
keywords = {Faster R-CNN, Bottom-up attention mechanism, Language model, Image captioning}
}

@article{10.1016/j.specom.2021.05.009,
author = {Avila, Anderson R. and O’Shaughnessy, Douglas and Falk, Tiago H.},
title = {Automatic speaker verification from affective speech using Gaussian mixture model based estimation of neutral speech characteristics},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.05.009},
doi = {10.1016/j.specom.2021.05.009},
journal = {Speech Commun.},
month = sep,
pages = {21–31},
numpages = {11},
keywords = {Transfer learning, Intra-speaker variability, Affective speech, Speaker verification}
}

@inproceedings{10.5555/1153922.1154326,
author = {Qi, Yanjun and Hauptmann, A. and Liu, Ting},
title = {Supervised classification for video shot segmentation},
year = {2003},
isbn = {0780379659},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we explore supervised classification methods for video shot segmentation. We transform the temporal segmentation problem into a multi-class categorization issue. This approach provides a uniform framework for using different kinds of features extracted from the video and for detecting various types of shot boundaries. The approach utilizes manual labeled training data and a simple classification structure, which eliminates arbitrary thresholds and achieves more reliable estimation than previous threshold-based methods. Contrastive experiments on 13 videos (/spl sim/4 hours) show excellent performance on the 2001 TREC video track shot classification task in terms of precision and recall.},
booktitle = {Proceedings of the 2003 International Conference on Multimedia and Expo - Volume 1},
pages = {689–692},
numpages = {4},
series = {ICME '03}
}

@inproceedings{10.1007/978-3-642-24955-6_8,
author = {Mutoh, Yoshitaka and Kashimori, Yoshiki},
title = {Neural model of auditory cortex for binding sound intensity and frequency information in bat's echolocation},
year = {2011},
isbn = {9783642249549},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-24955-6_8},
doi = {10.1007/978-3-642-24955-6_8},
abstract = {Most species of bats making echolocation use the sound pressure level (SPL) and Doppler-shifted frequency of ultrasonic echo pulse to measure the size and velocity of target. The neural circuits for detecting these target features are specialized for amplitude and frequency analysis of the second harmonic constant frequency (CF2) component of Doppler-shifted echoes. The neuronal circuits involved in detecting these features have been well established. However, it is not yet clear the neural mechanism by which these neuronal circuits detect the amplitude and frequency of echo signals. We present here neural models for detecting SPL amplitude and Doppler-shifted frequency of echo sound reflecting a target. Using the model, we show that the tuning property of frequency is changed depending on the feedback connections between cortical and subcortical neurons. We also show SPL amplitude is detected by integrating input signals emanating from ipsi and contralatreal subcortical neurons.},
booktitle = {Proceedings of the 18th International Conference on Neural Information Processing - Volume Part I},
pages = {62–69},
numpages = {8},
keywords = {neural model, frequency tuning, echolocation, auditory system, SPL amplitude},
location = {Shanghai, China},
series = {ICONIP'11}
}

@inproceedings{10.1145/3371158.3371189,
author = {Pal, Bithika and Jenamani, Mamata},
title = {Personalized Ranking in Collaborative Filtering: Exploiting l-th Order Transitive Relations of Social Ties},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371189},
doi = {10.1145/3371158.3371189},
abstract = {The use of social information in collaborative filtering is highly encouraged, as it can improve the recommendation accuracy by handling the cold start issue. The intuition of social recommendation is to reflect one's personal choice by its social neighbors. Though there exists a considerable amount of studies in this domain, no attention is paid to incorporate the transitive relationships of social ties in the ranking problem. In this paper, we exploit the lth order transitive relations of a user and extend the popular Social Bayesian Personalized Ranking (SBPR) model. The use of transitive relation creates a more granular pairwise ranking of items for a particular user and levels the user's personal choice based on the order of its social neighbors. We implement the model and conduct experiments on two real-world recommendation datasets with different values of l. We show that our model outperforms state-of-the-art pairwise ranking techniques.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {214–218},
numpages = {5},
keywords = {Social Network, Recommendation, Ranking, Personalization},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@article{10.1007/s10009-017-0466-1,
author = {Chadli, Mounir and Kim, Jin H. and Larsen, Kim G. and Legay, Axel and Naujokat, Stefan and Steffen, Bernhard and Traonouez, Louis-Marie},
title = {High-level frameworks for the specification and verification of scheduling problems},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-017-0466-1},
doi = {10.1007/s10009-017-0466-1},
abstract = {Over the years, schedulability of Cyber-Physical Systems (CPS) has mainly been performed by analytical methods. These techniques are known to be effective but limited to a few classes of scheduling policies. In a series of recent work, we have shown that schedulability analysis of CPS could be performed with a model-based approach and extensions of verification tools such as UPPAAL. One of our main contributions has been to show that such models are flexible enough to embed various types of scheduling policies, which goes beyond those in the scope of analytical tools. However, the specification of scheduling problems with model-based approaches requires a substantial modeling effort, and a deep understanding of the techniques employed in order to understand their results. In this paper we propose simplicity-driven high-level specification and verification frameworks for various scheduling problems. These frameworks consist of graphical and user-friendly languages for describing scheduling problems. The high-level specifications are then automatically translated to formal models, and results are transformed back into the comprehensible model view. To construct these frameworks we exploit a meta-modeling approach based on the tool generator Cinco . Additionally we propose in this paper two new techniques for scheduling analysis. The first performs runtime monitoring using the CUSUM algorithm to detect alarming change in the system. The second performs optimization using efficient statistical techniques. We illustrate our frameworks and techniques on two case studies.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = aug,
pages = {397–422},
numpages = {26},
keywords = {Statistical model-checking, Scheduling, Meta-modeling, High-level language, Hierarchical scheduling, Formal methods, Energy}
}

@inproceedings{10.1007/978-3-642-25719-3_6,
author = {Ye, Xujiong and Beddoe, Gareth and Slabaugh, Greg},
title = {A bayesian approach for false positive reduction in CTC CAD},
year = {2010},
isbn = {9783642257186},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25719-3_6},
doi = {10.1007/978-3-642-25719-3_6},
abstract = {This paper presents an automated detection method for identifying colonic polyps and reducing false positives (FPs) in CT images. It formulates the problem of polyp detection as a probability calculation through a unified Bayesian statistical model. The polyp likelihood is modeled with a combination of shape and intensity features. A second principal curvature PDE provides a shape model; and the partial volume effect is considered in modeling of the polyp intensity distribution. The performance of the method was evaluated on a large multi-center dataset of colonic CT scans. Both qualitative and quantitative experimental results demonstrate the potential of the proposed method.},
booktitle = {Proceedings of the Second International Conference on Virtual Colonoscopy and Abdominal Imaging: Computational Challenges and Clinical Opportunities},
pages = {40–46},
numpages = {7},
keywords = {colonic polyp detection, colon CAD, Bayesian framework},
location = {Beijing, China},
series = {MICCAI'10}
}

@inproceedings{10.1007/978-3-030-90370-1_17,
author = {Huang, Linan and Zhu, Quanyan},
title = {Combating Informational Denial-of-Service (IDoS) Attacks: Modeling and Mitigation of&nbsp;Attentional Human Vulnerability},
year = {2021},
isbn = {978-3-030-90369-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90370-1_17},
doi = {10.1007/978-3-030-90370-1_17},
abstract = {This work proposes a new class of proactive attacks called the Informational Denial-of-Service (IDoS) attacks that exploit the attentional human vulnerability. By generating a large volume of feints, IDoS attacks deplete the cognitive resources of human operators to prevent humans from identifying the real attacks hidden among feints. This work aims to formally define IDoS attacks, quantify their consequences, and develop human-assistive security technologies to mitigate the severity level and risks of IDoS attacks. To this end, we use the semi-Markov process to model the sequential arrivals of feints and real attacks with category labels attached in the associated alerts. The assistive technology strategically manages human attention by highlighting selective alerts periodically to prevent the distraction of other alerts. A data-driven approach is applied to evaluate human performance under different Attention Management (AM) strategies. Under a representative special case, we establish the computational equivalency between two dynamic programming representations to reduce the computation complexity and enable online learning with samples of reduced size and zero delays. A case study corroborates the effectiveness of the learning framework. The numerical results illustrate how AM strategies can alleviate the severity level and the risk of IDoS attacks. Furthermore, the results show that the minimum risk is achieved with a proper level of intentional inattention to alerts, which we refer to as the law of rational risk-reduction inattention.},
booktitle = {Decision and Game Theory for Security: 12th International Conference, GameSec 2021, Virtual Event, October 25–27, 2021, Proceedings},
pages = {314–333},
numpages = {20},
keywords = {Cognitive load, Attention management, Risk analysis, Temporal-difference learning, Cyber feint attack, Alert fatigue, Human vulnerability}
}

@article{10.1007/s10664-020-09912-w,
author = {Damasceno, Carlos Diego Nascimento and Mousavi, Mohammad Reza and Simao, Adenilso da Silva},
title = {Learning by sampling: learning behavioral family models from software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09912-w},
doi = {10.1007/s10664-020-09912-w},
abstract = {Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSMDiff. Subsequently, we report on our experience on learning family models by employing product sampling. Using 105 products of six product lines expressed in terms of Mealy machines, we evaluate the precision of family models learned from products selected from different settings of the T-wise product sampling criterion. We show that product sampling can lead to models as precise as those learned by exhaustive analysis and hence, reduce the costs for family model learning.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {46},
keywords = {T-wise sampling, Family model, Model learning, Software product lines}
}

@article{10.1504/ijbra.2020.104853,
author = {Deshpande, Sumukh and James, Anne and Franklin, Chris and Leach, Lindsey and Yang, Jianhua},
title = {Identification of novel flowering genes using RNA-Seq pipeline employing combinatorial approach in Arabidopsis thaliana time-series apical shoot meristem data},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {1},
issn = {1744-5485},
url = {https://doi.org/10.1504/ijbra.2020.104853},
doi = {10.1504/ijbra.2020.104853},
abstract = {Floral transition is a crucial event in the reproductive cycle of a flowering plant during which many genes are expressed that govern the transition phase. Identification of additional genes connected to flowering genes is vital since they may regulate flowering genes and vice versa. Through our study, expression values of additional genes have been found similar to flowering genes FLC and LFY in the transition phase. The presented approach plays a crucial role in this discovery. An RNA-Seq computational pipeline was developed for identification of novel genes involved in floral transition from A. thaliana apical shoot meristem time-series data. By intersecting differentially expressed genes (DEGs) from Cuffdiff, DEseq and edgeR, we identified 30 genes as principle regulators in the transition phase. Additionally, expression profiles of highly connected genes from network analysis revealed 76 genes with non-functional association and high correlation to FLC and LFY suggesting their potential role in floral regulation.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {25–58},
numpages = {33},
keywords = {Arabidopsis thaliana, enrichment, differential expression, step analysis, Cuffdiff, pipeline, flowering, apical shoot}
}

@article{10.1109/TASLP.2018.2848698,
author = {Dubey, Harishchandra and Sangwan, Abhijeet and Hansen, John H. L.},
title = {Leveraging Frequency-Dependent Kernel and DIP-Based Clustering for Robust Speech Activity Detection in Naturalistic Audio Streams},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2848698},
doi = {10.1109/TASLP.2018.2848698},
abstract = {Speech activity detection SAD is front-end in most speech systems, e.g., speaker verification, speech recognition&nbsp;etc. Supervised SAD typically leverages machine learning models trained on annotated data. For applications like zero-resource speech processing and NIST-OpenSAT-2017 public safety communications task, it might not be feasible to collect SAD annotations. SAD is challenging for naturalistic audio streams containing multiple noise-sources simultaneously. We propose a novel frequency-dependent kernel FDK based SAD features. FDK provides enhanced spectral decomposition from which several statistical descriptors are derived. FDK statistical descriptors are combined by principal component analysis into one-dimensional FDK-SAD features. We further proposed two decision backends: First, variable model-size Gaussian mixture model VMGMM; and second, Hartigan dip-based robust feature clustering. While VMGMM is a model-based approach, the DipSAD is nonparametric. We used both backends for comparative evaluations in two phases: first, standalone SAD performance; and second, the effect of SAD on text-dependent speaker verification using RedDots data. The NIST-OpenSAD-2015 and NIST-OpenSAT-2017 corpora are used for standalone SAD evaluations. We establish two Center for Robust Speech Systems CRSS corpora namely CRSS-PLTL-II and CRSS long-duration naturalistic noise corpus. The CRSS corpora facilitate standalone SAD evaluations on naturalistic audio streams. We performed comparative studies of the proposed approaches with multiple baselines including SohnSAD, rSAD, semisupervised Gaussian mixture model, and Gammatone spectrogram features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2056–2071},
numpages = {16}
}

@book{10.1145/3107990,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2},
year = {2018},
isbn = {9781970001716},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {21},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces: user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces that often include biosignals.This edited collection is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This second volume of the handbook begins with multimodal signal processing, architectures, and machine learning. It includes recent deep-learning approaches for processing multisensorial and multimodal user data and interaction, as well as context-sensitivity. A further highlight is processing of information about users' states and traits, an exciting emerging capability in next-generation user interfaces. These chapters discuss real-time multimodal analysis of emotion and social signals from various modalities and perception of affective expression by users. Further chapters discuss multimodal processing of cognitive state using behavioral and physiological signals to detect cognitive load, domain expertise, deception, and depression. This collection of chapters provides walk-through examples of system design and processing, information on tools and practical resources for developing and evaluating new systems, and terminology, and tutorial support for mastering this rapidly expanding field. In the final section of this volume, experts exchange views on the timely and controversial challenge topic of multimodal deep learning. The discussion focuses on how multimodal-multisensor interfaces are most likely to advance human performance during the next decade.}
}

@article{10.1007/s11263-019-01259-0,
author = {Yin, Jiahang and Wu, Ancong and Zheng, Wei-Shi},
title = {Fine-Grained Person Re-identification},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {6},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01259-0},
doi = {10.1007/s11263-019-01259-0},
abstract = {Person re-identification (re-id) plays a critical role in tracking people via surveillance systems by matching people across non-overlapping camera views at different locations. Although most re-id methods largely depend on the appearance features of a person, such methods always assume that the appearance information (particularly color) is distinguishable. However, distinguishing people who dress in very similar clothes (especially the same type of clothes, e.g. uniform) is ineffective if relying only on appearance cues. We call this problem the fine-grained person re-identification (FG re-id) problem. To solve this problem, rather than relying on clothing color, we propose to exploit two types of local dynamic pose features: motion-attentive local dynamic pose feature and joint-specific local dynamic pose feature. They are complementary to each other and describe identity-specific pose characteristics, which are found to be more unique and discriminative against similar appearance between people. A deep neural network is formed to learn these local dynamic pose features and to jointly quantify motion and global visual cues. Due to the lack of a suitable benchmark dataset for evaluating the FG re-id problem, we also contribute a fine-grained person re-identification (FGPR) dataset, which contains 358 identities. Extensive evaluations on the FGPR dataset show that our proposed model achieves the best performance compared with related person re-id and fine-grained recognition methods for FG re-id. In addition, we verify that our method is still effective for conventional video-based person re-id.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {1654–1672},
numpages = {19},
keywords = {Visual surveillance, Fine-grained cross-view matching, Person re-identification}
}

@article{10.1016/j.rcim.2016.01.008,
author = {Luo, Hao and Wang, Kai and Kong, Xiang T.R. and Lu, Shaoping and Qu, Ting},
title = {Synchronized production and logistics via ubiquitous computing technology},
year = {2017},
issue_date = {June 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {45},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.01.008},
doi = {10.1016/j.rcim.2016.01.008},
abstract = {The integration of manufacturing and logistics has drawn widespread research attentions in recent years. This paper focuses on the Synchronized Production and Logistics (SPL), which is operational level integration. SPL\'{z}is defined as synchronizing the processing, moving and storing of raw material, WIP and finished product within one manufacturing unit by high level information sharing and joint scheduling to achieve synergic decision, execution and overall performance improvement. Through analysing the requirements and challenges in real life industry, the ubiquitous computing is adopted as an enabling technology and an Ubi-SPL (Synchronized Production and Logistics via Ubiquitous Technology) framework is proposed. This framework is consists of four layers, which creates a close decision-execution loop by linking the frontline real time data, user feedback and optimized decision together. A real life case study of applying Ubi-SPL solution in a chemical industry has been conducted. The implementation results show that the proposed Ubi-SPL solution can significantly improve the overall performance in both production and logistics service. This paper focuses on the Synchronized Production and Logistics.The ubiquitous computing is adopted as an enabling technology.Synchronized Production &amp; Logistics via Ubiquitous Technics framework is proposed.A real life case study in a chemical industry has been conducted.},
journal = {Robot. Comput.-Integr. Manuf.},
month = jun,
pages = {99–115},
numpages = {17},
keywords = {Ubiquitous manufacturing, Synchronized production and logistics, RFID}
}

@inproceedings{10.1145/3184558.3191523,
author = {Dalmia, Ayushi and J, Ganesh and Gupta, Manish},
title = {Towards Interpretation of Node Embeddings},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191523},
doi = {10.1145/3184558.3191523},
abstract = {Recently there have been a large number of studies on embedding large-scale information networks using low-dimensional, neighborhood and community aware node representations. Though the performance of these embedding models have been better than traditional methods for graph mining applications, little is known about what these representations encode, or why a particular node representation works better for certain tasks. Our work presented here constitutes the first step in decoding the black-box of vector embeddings of nodes by evaluating their effectiveness in encoding elementary properties of a node such as page rank, degree, closeness centrality, clustering coefficient, etc. We believe that a node representation is effective for an application only if it encodes the application-specific elementary properties of nodes. To unpack the elementary properties encoded in a node representation, we evaluate the representations on the accuracy with which they can model each of these properties. Our extensive study of three state-of-the-art node representation models (DeepWalk, node2vec and LINE) on four different tasks and six diverse graphs reveal that node2vec and LINE best encode the network properties of sparse and dense graphs respectively. We correlate the model performance obtained for elementary property prediction tasks with the high-level downstream applications such as link prediction and node classification, and visualize the task performance vector of each model to understand the semantic similarity between the embeddings learned by various models. Our first study of the node embedding models for outlier detection reveals that node2vec and DeepWalk identify outliers well for sparse and dense graphs respectively. Our analysis highlights that the proposed elementary property prediction tasks help in unearthing the important features responsible for the given node embedding model to perform well for a given downstream task. This understanding would facilitate in picking the right model for a given downstream task.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {945–952},
numpages = {8},
keywords = {graph representation, model interpretability, neural networks},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1007/s00371-017-1442-1,
author = {Dehshibi, Mohammad Mahdi and Shanbehzadeh, Jamshid},
title = {Cubic norm and kernel-based bi-directional PCA: toward age-aware facial kinship verification},
year = {2019},
issue_date = {January   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {1},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-017-1442-1},
doi = {10.1007/s00371-017-1442-1},
abstract = {A recent challenge in computer vision is exploring the cardinality of a relationship among multiple visual entities to answer questions like whether the subjects in a photograph have a kin relationship. This paper tackles kinship recognition from the aging viewpoint in which the system could find the parent of a child where the input image of the parent belongs to the age range that is lower than the child is. Technical contributions of this research are twofold. (1) An efficient discriminative feature space is constructed by proposing kernelized bi-directional PCA to form a topological cubic feature space. Cubic feature space in conjunction with the introduced cubic norm is used to solve the kinship problem. (2) To fill the gap of aging effect in finding a kin relation, a semi-supervised learning paradigm is proposed. To do this, first, the pooling layer of a convolutional neural network is modified to do a soft pooling. Then, the last pooling layer, as a rich feature vector, is fed into density-based spatial clustering of applications with noise algorithm. This pre-classification phase would be useful when there is no aggregation on how many classes should be used in the age group estimation task. Finally, by adding kernel computation to sparse representation classifier, the age classification is done. Evaluation of the proposed method on five publicly available facial kinship datasets shows the superiority of the proposed method over both the state-of-the-art kinship verification methods and what is known as human decision-making.},
journal = {Vis. Comput.},
month = jan,
pages = {23–40},
numpages = {18},
keywords = {Kinship verification, Kernelized bi-directional PCA, Feature extraction, Cube norm, Convolutional neural network, Age estimation, Age clustering}
}

@inproceedings{10.1145/3366423.3380187,
author = {Lian, Defu and Liu, Qi and Chen, Enhong},
title = {Personalized Ranking with Importance Sampling},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380187},
doi = {10.1145/3366423.3380187},
abstract = {As the task of predicting a personalized ranking on a set of items, item recommendation has become an important way to address information overload. Optimizing ranking loss aligns better with the ultimate goal of item recommendation, so many ranking-based methods were proposed for item recommendation, such as collaborative filtering with Bayesian Personalized Ranking (BPR) loss, and Weighted Approximate-Rank Pairwise (WARP) loss. However, the ranking-based methods can not consistently beat regression-based models with the gravity regularizer. The key challenge in ranking-based optimization is difficult to fully use the limited number of negative samples, particularly when they are not so informative. To this end, we propose a new ranking loss based on importance sampling so that more informative negative samples can be better used. We then design a series of negative samplers from simple to complex, whose informativeness of negative samples is from less to more. With these samplers, the loss function is easy to use and can be optimized by popular solvers. The proposed algorithms are evaluated with five real-world datasets of varying size and difficulty. The results show that they consistently outperform the state-of-the-art item recommendation algorithms, and the relative improvements with respect to NDCG@50 are more than 19.2% on average. Moreover, the loss function is verified to make better use of negative samples and to require fewer negative samples when they are more informative.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1093–1103},
numpages = {11},
keywords = {Personalized Ranking, Negative Sampling, Implicit Feedback},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/1806799.1806872,
author = {Rastkar, Sarah and Murphy, Gail C. and Murray, Gabriel},
title = {Summarizing software artifacts: a case study of bug reports},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806872},
doi = {10.1145/1806799.1806872},
abstract = {Many software artifacts are created, maintained and evolved as part of a software development project. As software developers work on a project, they interact with existing project artifacts, performing such activities as reading previously filed bug reports in search of duplicate reports. These activities often require a developer to peruse a substantial amount of text. In this paper, we investigate whether it is possible to summarize software artifacts automatically and effectively so that developers could consult smaller summaries instead of entire artifacts. To provide focus to our investigation, we consider the generation of summaries for bug reports. We found that existing conversation-based generators can produce better results than random generators and that a generator trained specifically on bug reports can perform statistically better than existing conversation-based generators. We demonstrate that humans also find these generated summaries reasonable indicating that summaries might be used effectively for many tasks.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {505–514},
numpages = {10},
keywords = {machine learning, human-centric software engineering},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/3442391.3442398,
author = {Bressan, Lucas and de Oliveira, Andr\'{e} Luiz and Campos, Fernanda and Capilla, Rafael},
title = {A variability modeling and transformation approach for safety-critical systems},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442398},
doi = {10.1145/3442391.3442398},
abstract = {Safety-critical autonomous systems are becoming highly variant-intensive with thousands of variations points within a single product. Modeling these systems requires the specification of safety properties, but the diversity of these properties makes hard to configure these systems manually to prevent emerging hazards and fault behaviors. Because existing software variability techniques provide rudimentary mechanisms for mapping variability constructs to functional safety models, we describe in this paper an experience report showing how a novel annotative modeling approach and tool can be used to derive system models enriched with functional safety information. We validate our approach using a case study from the automotive domain and we estimate the effort reduction in the tasks comparing our approach with two similar tools.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {7},
keywords = {Variability modeling, Safety critical systems, Reuse, Functional safety},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1007/11668855_4,
author = {Cho, Eun Sook and Cha, Jung Eun and Yang, Young Jong},
title = {MARMI-RE: a method and tools for legacy system modernization},
year = {2004},
isbn = {3540321330},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11668855_4},
doi = {10.1007/11668855_4},
abstract = {Software evolution is the process of adapting an existing software system to conform to an enhanced set of requirements. Software reengineering is software evolution performed in systematic way. Especially software system is fundamentally different from developing one from scratch. Consequently, tools to support evolution must go beyond forward engineering tools. This paper presents a reengineering method and tools for software evolution or modernization. The paper briefly describes MARMI-RE methodology before presenting the individual tools and how they interoperate to support legacy system modernization. We expect that our proposed methodology can be used flexibly because it presents various scenarios of migration process.},
booktitle = {Proceedings of the Second International Conference on Software Engineering Research, Management and Applications},
pages = {42–57},
numpages = {16},
location = {Los Angeles, CA},
series = {SERA'04}
}

@inproceedings{10.1109/ICComm.2016.7528326,
author = {Irofti, Paul and Dumitrescu, Bogdan},
title = {Regularized algorithms for dictionary learning},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICComm.2016.7528326},
doi = {10.1109/ICComm.2016.7528326},
abstract = {Dictionary learning (DL) for sparse representation is a difficult optimization problem for which several successful algorithms exist, although none can be claimed the best. A common problem is a possible stall in the evolution of the algorithm, due to nearly linearly dependent atoms. The proposed cure was to regularize the error criterion using either the norm of the representations or an atom coherence measure. However, only gradient-based algorithms have been proposed for the regularized problems. We give here regularized versions of Approximate K-SVD and other algorithms related to it and investigate numerically their behavior. The experiments show that the new regularized algorithms are able to reduce the representation error, and thus produce better dictionaries, when the imposed sparsity is not very high.},
booktitle = {2016 International Conference on Communications (COMM)},
pages = {439–442},
numpages = {4},
location = {Bucharest, Romania}
}

@article{10.1007/s10845-009-0375-6,
author = {Narayanasamy, R. and Padmanabhan, P.},
title = {Comparison of regression and artificial neural network model for the prediction of springback during air bending process of interstitial free steel sheet},
year = {2012},
issue_date = {June      2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-009-0375-6},
doi = {10.1007/s10845-009-0375-6},
abstract = {This paper compares the regression and neural network modeling for predicting springback of interstial free steel sheet during air bending process. In this investigation, punch travel, strain hardening exponent, punch radius, punch velocity and width of the sheet were considered as input variables and springback as response variable. It has been observed that the ANN modeling process has been able to predict the springback with higher accuracy when compared with regression model.},
journal = {J. Intell. Manuf.},
month = jun,
pages = {357–364},
numpages = {8},
keywords = {Springback, Regression model, IF steel sheet, ANN model}
}

@inproceedings{10.1145/3139958.3140044,
author = {Jiang, Zhe and Li, Yan and Shekhar, Shashi and Rampi, Lian and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity: A Summary of Results},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140044},
doi = {10.1145/3139958.3140044},
abstract = {Class ambiguity refers to the phenomenon whereby samples with similar features belong to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. SEL problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost (finding an optimal zone partition is NP-hard). Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble), and thus cannot effectively minimize class ambiguity. In contrast, our spatial ensemble framework explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {23},
numpages = {10},
keywords = {spatial heterogeneity, spatial ensemble, local models, class ambiguity, Spatial classification},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1007/978-3-030-34585-3_18,
author = {Serra, Angela and della Pietra, Antonio and Herdener, Marcus and Tagliaferri, Roberto and Esposito, Fabrizio},
title = {Automatic Discrimination of Auditory Stimuli Perceived by the Human Brain},
year = {2018},
isbn = {978-3-030-34584-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34585-3_18},
doi = {10.1007/978-3-030-34585-3_18},
abstract = {Humans are able to perceive small difference of sound frequency but it is still unknown how the difference in frequency information is represented at the level of the primary sensory cortex. Indeed, analysis of fMRI imaging identified tonotopic maps through the auditory pathways to the primary sensory cortex. These maps are unfortunately too coarse to show ultra-fine discrimination. Then, the hypothesis is that this small frequency differences are recognised thanks to the information coming from a large set of auditory neurons. To investigate this possibility, a multi-voxel pattern discriminating analysis of the response of BOLD-fMRI in the bilateral auditory cortex to tonal stimuli with different shift in frequency was performed. Our results suggest that small shifts in the frequency are easily classified compared with big shifts and that multiple areas of the auditory cortex are involved in the tone recognition.},
booktitle = {Computational Intelligence Methods for Bioinformatics and Biostatistics: 15th International Meeting, CIBB 2018, Caparica, Portugal, September 6–8, 2018, Revised Selected Papers},
pages = {205–211},
numpages = {7},
keywords = {BOLD-fMRI, Auditory cortex, Tonotonic maps},
location = {Caparica, Portugal}
}

@article{10.1145/2207243.2207253,
author = {Munson, M. Arthur},
title = {A study on the importance of and time spent on different modeling steps},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/2207243.2207253},
doi = {10.1145/2207243.2207253},
abstract = {Applying data mining and machine learning algorithms requires many steps to prepare data and to make use of modeling results. This study investigates two questions: (1) how time consuming are the pre- and post-processing steps? (2) how much research energy is spent on these steps? To answer these questions I surveyed practitioners about their experiences in applying modeling techniques and categorized data mining and machine learning research papers from 2009 according to the modeling step(s) they addressed. Survey results show that model building consumes only 14% of the time spent on a typical project; the remaining time is spent on pre- and post-processing steps. Both survey responses and the categorization of research papers show that data mining and machine learning researchers spend the majority of their energy on algorithms for constructing models and significantly less energy on other steps. These findings collectively suggest that there are research opportunities to simplify the steps that precede and follow model building.},
journal = {SIGKDD Explor. Newsl.},
month = may,
pages = {65–71},
numpages = {7}
}

@inproceedings{10.1145/2037342.2037348,
author = {Fischer, Andreas and Frinken, Volkmar and Forn\'{e}s, Alicia and Bunke, Horst},
title = {Transcription alignment of Latin manuscripts using hidden Markov models},
year = {2011},
isbn = {9781450309165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037342.2037348},
doi = {10.1145/2037342.2037348},
abstract = {Transcriptions of historical documents are a valuable source for extracting labeled handwriting images that can be used for training recognition systems. In this paper, we introduce the Saint Gall database that includes images as well as the transcription of a Latin manuscript from the 9th century written in Carolingian script. Although the available transcription is of high quality for a human reader, the spelling of the words is not accurate when compared with the handwriting image. Hence, the transcription poses several challenges for alignment regarding, e.g., line breaks, abbreviations, and capitalization. We propose an alignment system based on character Hidden Markov Models that can cope with these challenges and efficiently aligns complete document pages. On the Saint Gall database, we demonstrate that a considerable alignment accuracy can be achieved, even with weakly trained character models.},
booktitle = {Proceedings of the 2011 Workshop on Historical Document Imaging and Processing},
pages = {29–36},
numpages = {8},
keywords = {transcription alignment, hidden Markov models, handwriting recognition},
location = {Beijing, China, USA},
series = {HIP '11}
}

@article{10.1016/j.cogsys.2019.09.021,
author = {Vityaev, Evgenii},
title = {Consciousness as a logically consistent and prognostic model of reality},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2019.09.021},
doi = {10.1016/j.cogsys.2019.09.021},
journal = {Cogn. Syst. Res.},
month = jan,
pages = {231–246},
numpages = {16},
keywords = {Concepts, Integrated information, Natural concepts, Natural classification, Categorization, Clustering}
}

@inproceedings{10.5555/646014.677489,
author = {Dampos, Te\'{o}filo Em\'{\i}dio de and Feris, Rog\'{e}rio Schimidt and Junior, Roberto Marcondes Cesar},
title = {Improved Face/spl times/Non-Face Discrimination using Fourier Descriptors through Feature Selection},
year = {2000},
isbn = {0769508782},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This work presents a new method to discriminate face from non-face images using Fourier descriptors. The first step of our approach consists in applying a horizontal edge detection filter in the input image, followed by the extraction of a 1D signal from the computed edge map. Then we calculate the Fourier descriptors from this signal and classify the image using statistical classifiers. In order to improve our results, we applied a feature selection algorithm. Preliminary performance assessment results have shown that this approach is superior than traditional transform based methods. Besides, these results showed that our method might be used to develop a fast face detection system.},
booktitle = {Proceedings of the 13th Brazilian Symposium on Computer Graphics and Image Processing},
pages = {28–35},
numpages = {8},
keywords = {transform based methods, statistical classifiers, performance assessment, image classification, horizontal edge detection filter, feature selection, face recognition, face discrimination, face detection, Fourier descriptors, 1D signal extraction},
series = {SIBGRAPI '00}
}

@article{10.1016/j.imavis.2011.09.002,
author = {Hollingsworth, Karen and Bowyer, Kevin W. and Flynn, Patrick J.},
title = {Useful features for human verification in near-infrared periocular images},
year = {2011},
issue_date = {October, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {29},
number = {11},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2011.09.002},
doi = {10.1016/j.imavis.2011.09.002},
abstract = {The periocular region is the part of the face immediately surrounding the eye, and researchers have recently begun to investigate how to use the periocular region for recognition. Understanding how humans recognize faces helped computer vision researchers develop algorithms for face recognition. Likewise, understanding how humans analyze periocular images could benefit researchers developing algorithms for periocular recognition. We conducted two experiments to determine how humans analyze periocular images. In these experiments, we presented pairs of images and asked volunteers to determine whether the two images showed eyes from the same subject or from different subjects. In the first experiment, subjects were paired randomly to create different-subject queries. Our volunteers correctly determined the relationship between the two images in 92% of the queries. In the second experiment, we considered multiple factors in forming different-subject pairs; queries were formed from pairs of subjects with the same gender and race, and with similar eye color, makeup, eyelash length, and eye occlusion. In addition, we limited the amount of time volunteers could view a query pair. On this harder experiment, the correct verification rate was 79%. We asked volunteers to describe what features in the images were helpful to them in making their decisions. In both experiments, eyelashes were reported to be the most helpful feature.},
journal = {Image Vision Comput.},
month = oct,
pages = {707–715},
numpages = {9},
keywords = {Periocular recognition, Ocular biometrics, Near-infrared light}
}

@article{10.1016/j.patcog.2018.11.028,
author = {Zhao, Lijun and Bai, Huihui and Liang, Jie and Zeng, Bing and Wang, Anhong and Zhao, Yao},
title = {Simultaneous color-depth super-resolution with conditional generative adversarial networks},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.028},
doi = {10.1016/j.patcog.2018.11.028},
journal = {Pattern Recogn.},
month = apr,
pages = {356–369},
numpages = {14},
keywords = {Edge detection, Image smoothing, Super-resolution, Generative adversarial networks}
}

@inproceedings{10.1145/3340555.3353739,
author = {Wu, Suowei and Du, Zhengyin and Li, Weixin and Huang, Di and Wang, Yunhong},
title = {Continuous Emotion Recognition in Videos by Fusing Facial Expression, Head Pose and Eye Gaze},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3353739},
doi = {10.1145/3340555.3353739},
abstract = {Continuous emotion recognition is of great significance in affective computing and human-computer interaction. Most of existing methods for video based continuous emotion recognition utilize facial expression. However, besides facial expression, other clues including head pose and eye gaze are also closely related to human emotion, but have not been well explored in continuous emotion recognition task. On the one hand, head pose and eye gaze could result in different degrees of credibility of facial expression features. On the other hand, head pose and eye gaze carry emotional clues themselves, which are complementary to facial expression. Accordingly, in this paper we propose two ways to incorporate these two clues into continuous emotion recognition. They are respectively an attention mechanism based on head pose and eye gaze clues to guide the utilization of facial features in continuous emotion recognition, and an auxiliary line which helps extract more useful emotion information from head pose and eye gaze. Experiments are conducted on the Recola dataset, a database for continuous emotion recognition, and the results show that our framework outperforms other state-of-the-art methods due to the full use of head pose and eye gaze clues in addition to facial expression for continuous emotion recognition.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {40–48},
numpages = {9},
keywords = {Head Pose, Facial Expression, Eye gaze, Continuous Emotion Recognition, Attention},
location = {Suzhou, China},
series = {ICMI '19}
}

@article{10.1023/A:1018659009494,
author = {Beiu, Valeriu and Draghici, Sorin and De Pauw, Thiery},
title = {A Constructive Approach to Calculating Lower Entropy Bounds},
year = {1999},
issue_date = {Feb. 1999},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {1},
issn = {1370-4621},
url = {https://doi.org/10.1023/A:1018659009494},
doi = {10.1023/A:1018659009494},
abstract = {This paper presents a constructive approach to estimating the size
of a neural network necessary to solve a given classification problem. The
results are derived using an information entropy approach in the context of
limited precision integer weights. Such weights are particularly suited for
hardware implementations since the area they occupy is limited, and the
computations performed with them can be efficiently implemented in
hardware. The considerations presented use an information entropy
perspective and calculate lower bounds on the number of bits needed in
order to solve a given classification problem. These bounds are obtained by
approximating the classification hypervolumes with the volumes of several
regular (i.e., highly symmetric) n-dimensional bodies. The bounds given
here allow the user to choose the appropriate size of a neural network such
that: (i) the given classification problem can be solved, and (ii) the
network architecture is not oversized. All considerations presented take
into account the restrictive case of limited precision integer weights, and
therefore can be directly applied when designing VLSI implementations of
neural networks.},
journal = {Neural Process. Lett.},
month = feb,
pages = {1–12},
numpages = {12},
keywords = {number of bits, n-dimensional volumes, limited and integer weights, constructive algorithms, complexity, classification problems}
}

@article{10.1145/3170432,
author = {Dayarathna, Miyuru and Perera, Srinath},
title = {Recent Advancements in Event Processing},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3170432},
doi = {10.1145/3170432},
abstract = {Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {33},
numpages = {36},
keywords = {data stream processing, complex event processing, Event processing}
}

@article{10.1016/j.eswa.2012.02.043,
author = {Mart\'{\i}nez-Vargas, J.D. and Sepulveda-Cano, L.M. and Travieso-Gonzalez, C. and Castellanos-Dominguez, G.},
title = {Detection of obstructive sleep apnoea using dynamic filter-banked features},
year = {2012},
issue_date = {August, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {10},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.02.043},
doi = {10.1016/j.eswa.2012.02.043},
abstract = {Highlights This study presents a methodology for OSA detection based on HRV time series. The HRV is characterized by dynamic banked features extracted from its spectrogram. The spectral splitting scheme to search frequency boundaries is introduced. Attained results show similar performance compared with another outcomes. An easier clinical interpretation of HRV-derived parameters is obtained. There is a need for developing simple signal processing algorithms for less costly, reliable and noninvasive Obstructive Sleep Apnoea (OSA) diagnosing. One of the promising directions is to provide the OSA analysis based on the heart rate variability (HRV), which clearly shows a non-stationary behavior. So, a feature extraction approach, being capable of capturing the dynamic heart rate information and suitable for OSA detection, remains an open issue. Grounded on discriminating capability of frequency bands of HRV activity between normal and OSA patients, features can be extracted. However, some HRV normal spectrograms resemble like pathological ones, and vice versa; so, prior to extract the feature set, the energy spatial contribution contained in each sub band should be clarified. This paper presents a methodology for OSA detection based on a set of short-time feature banked features that are extracted from the spectrogram of the HRV time series. The methodology introduces the spectral splitting scheme, which searches for spectral components with alike stochastic behavior improving the OSA detection accuracy. Two different splitting approaches are considered (heuristic and relevance-based); both of them performing minute-by-minute classification comparable with other outcomes that are reported in literature, but avoiding more complex methods or more computed features. For validation purposes, the methodology is tested on 1-min HRV-segments estimated from 50 Physionet database recordings. Using a parallel combining k-nn classifier, the assessed dynamic feature set reaches as much as 80% value of accuracy, for both considered approaches of spectral splitting. Attained results can be oriented in research focused on finding alternative methods used for less costly and noninvasive OSA diagnosing with the additional benefit of easier clinical interpretation of HRV-derived parameters.},
journal = {Expert Syst. Appl.},
month = aug,
pages = {9118–9128},
numpages = {11},
keywords = {Spectral splitting, Obstructive sleep apnea, Heart rate variability, Dynamic filter-banked features}
}

@article{10.1016/S1877-0509(20)30921-2,
title = {Contents},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(20)30921-2},
doi = {10.1016/S1877-0509(20)30921-2},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–xvi},
numpages = {14}
}

@article{10.1007/s11042-020-08849-y,
author = {Ullah, Inam and Jian, Muwei and Hussain, Sumaira and Guo, Jie and Yu, Hui and Wang, Xing and Yin, Yilong},
title = {A brief survey of visual saliency detection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {45–46},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-08849-y},
doi = {10.1007/s11042-020-08849-y},
abstract = {Salient object detection models mimic the behavior of human beings and capture the most salient region/object from the images or scenes, this field contains many important applications in both computer vision and pattern recognition tasks. Despite hundreds of models that have been proposed in this field, but still, it requires a large room for research. This paper demonstrates a detailed overview of the recent progress of saliency detection models in terms of heuristic-based techniques and deep learning-based techniques. we have discussed and reviewed its co-related fields, such as Eye-fixation-prediction, RGBD salient-object-detection, co-saliency object detection, and video-saliency-detection models. We have reviewed the key issues of the current saliency models and discussed future trends and recommendations. The broadly utilized datasets and assessment strategies are additionally investigated in this paper.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {34605–34645},
numpages = {41},
keywords = {Saliency model, Salient object, Visual cues, Saliency detection}
}

@article{10.3233/SW-160242,
author = {Hitzler, Pascal and Cruz, Isabel and Zhang, Ziqi},
title = {Effective and efficient Semantic Table Interpretation using TableMiner+},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {6},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-160242},
doi = {10.3233/SW-160242},
abstract = {This article introduces TableMiner+, a Semantic Table Interpretation method that annotates Web tables in a both effective and efficient way. Built on our previous work TableMiner, the extended version advances state-of-the-art in several ways. First, it improves annotation accuracy by making innovative use of various types of contextual information both inside and outside tables as features for inference. Second, it reduces computational overheads by adopting an incremental, bootstrapping approach that starts by creating preliminary and partial annotations of a table using ‘sample’ data in the table, then using the outcome as ‘seed’ to guide interpretation of remaining contents. This is then followed by a message passing process that iteratively refines results on the entire table to create the final optimal annotations. Third, it is able to handle all annotation tasks of Semantic Table Interpretation (e.g., annotating a column, or entity cells) while state-of-the-art methods are limited in different ways. We also compile the largest dataset known to date and extensively evaluate TableMiner+ against four baselines and two re-implemented (near-identical, as adaptations are needed due to the use of different knowledge bases) state-of-the-art methods. TableMiner+ consistently outperforms all models under all experimental settings. On the two most diverse datasets covering multiple domains and various table schemata, it achieves improvement in F1 by between 1 and 42 percentage points depending on specific annotation tasks. It also significantly reduces computational overheads in terms of wall-clock time when compared against classic methods that ‘exhaustively’ process the entire table content to build features for inference. As a concrete example, compared against a method based on joint inference implemented with parallel computation, the non-parallel implementation of TableMiner+ achieves significant improvement in learning accuracy and almost orders of magnitude of savings in wall-clock time.},
journal = {Semant. Web},
month = jan,
pages = {921–957},
numpages = {37},
keywords = {table annotation, Semantic Table Interpretation, linked data, Relation Extraction, named entity disambiguation, named entity recognition, Web table}
}

@inproceedings{10.5555/2980539.2980703,
author = {Rosales, R\'{o}mer and Sclaroff, Stan},
title = {Learning body pose via Specialized Maps},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion.},
booktitle = {Proceedings of the 15th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {1263–1270},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}

@inproceedings{10.5555/2934046.2934169,
author = {Opitz, David W. and Basak, Subhash C. and Gute, Brian D.},
title = {Hazard assessment modeling: an evolutionary ensemble approach},
year = {1999},
isbn = {1558606114},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper presents a novel and effective genetic algorithm approach for generating computational models for hazard assessment. With millions of proposed chemicals being registered each year, it is impossible to come even remotely close to completing the battery of tests needed for the proper understanding of the toxic effects of these chemicals. Computer models can give quick, cheap, and environmentally friendly hazard assessments of chemicals. Our approach works by first extracting a hierarchy of theoretical descriptors of the structure of a compound, then filtering these numerous descriptors with a genetic algorithm approach to ensemble feature selection. We tested the utility of our approach by modeling the acute aquatic toxicity (LC50) of a congeneric set of 69 benzene derivatives. Our results demonstrate a very important point: that our method is able to accurately predict toxicity directly from structure.},
booktitle = {Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 2},
pages = {1643–1650},
numpages = {8},
location = {Orlando, Florida},
series = {GECCO'99}
}

@article{10.1007/s00500-018-3475-4,
author = {de Ara\'{u}jo, Sidnei A. and de Barros, Daniel F. and da Silva, Ernani M. and Cardoso, Marcos V.},
title = {Applying computational intelligence techniques to improve the decision making of business game players},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3475-4},
doi = {10.1007/s00500-018-3475-4},
abstract = {Business games have been widely used as differentiated pedagogical tools to provide experiential learning for business students. However, a critical problem with these tools is the issue of how to give feedback to students during the runtime of the simulation, especially in view of the high number of players involved in the game and the large amount of data generated in the simulations. In this scenario, intelligent mechanisms are desirable to make knowledge-based inferences, providing information which can assist both the players and the instructors facilitating the gaming process. In this work, we present an innovative knowledge-based approach focused on business games. Firstly, we apply data mining techniques to identify the behavioral patterns of players, based on their previous decisions stored in the database of a business game called business management simulator (BMS) that is used as a support tool for teaching concepts of production management, sales and business strategies. Secondly, based on these patterns, we develop a fuzzy inference system (FIS) to predict players’ performance based on their decisions in the game. Experimental results from a comparison of the real performance of players with the performance calculated by the proposed FIS show that this approach is very useful in the business game analyzed here, since it can help students during the simulation runtime, allowing them to improve their decisions. It is also clear that the proposed approach can be easily adapted to other business games, and particularly those with a similar structure to that of BMS.},
journal = {Soft Comput.},
month = sep,
pages = {8753–8763},
numpages = {11},
keywords = {Decision making, Decision tree, Fuzzy logic, Data mining, Computational intelligence, Knowledge-based approach, Business game}
}

@inproceedings{10.5555/844379.844675,
author = {Sabourin, R. and Genest, G.},
title = {An extended-shadow-code based approach for off-line signature verification. II. Evaluation of several multi-classifier combination strategies},
year = {1995},
isbn = {0818671289},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {For pt.I see Proc. 12th ICPR, p.450-3. In a real situation, the choice of the best representation R(/spl gamma/) for the implementation of a signature verification system able to cope with all types of handwriting is a very difficult task. This study is original in that the design of the integrated classifiers E(x) is based on a large number of individual classifiers e/sub k/(x) (or signature representations R(/spl gamma/)) in an attempt to overcome in some way the need for feature selection. In this paper, the authors present a first systematical evaluation of a multi-classifier-based approach for off-line signature verification. Two types of integrated classifiers based on kNN or minimum distance classifiers and 15 types of representation related to the ESC used as a shape factor have been evaluated using a signature database of 800 images (20 writers/spl times/40 signatures per writer) in the context of random forgeries.},
booktitle = {Proceedings of the Third International Conference on Document Analysis and Recognition (Volume 1) - Volume 1},
pages = {197},
keywords = {random forgeries, pattern classification, off-line signature verification, multi-classifier combination, minimum distance classifiers, integrated classifiers, handwriting recognition, handwriting},
series = {ICDAR '95}
}

@article{10.1007/s11042-019-7381-2,
author = {Dad, Nisrine and En-nahnahi, Noureddine and El Alaoui Ouatik, Said},
title = {Quaternion Harmonic moments and extreme learning machine for color object recognition},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7381-2},
doi = {10.1007/s11042-019-7381-2},
abstract = {The quaternary orthogonal moments have been widely used as color image descriptors owe to their remarkable color and shape information encapsulation capability. Their computation, however, depends on finding the optimal value of a unit pure quaternion parameter, which is done empirically and with no warranty of optimality. We propose a 2D color object recognition method that relies on the quaternion-valued parameter-free disc-harmonic moment invariants (QHMs) fed into the quaternion extreme learning machine (QELM). The role of this latter is to maintain the correlation between the four parts, real and imaginary, of the quaternary descriptor coefficients. Several datasets are used for recognition experiments. We draw the conclusion that: (1) our quaternion-valued QHMs invariants outperform other quaternary moments, (2) the quaternion-valued moment invariants give results better than the modulus-based moment invariants and (3) the QELM yields results better than the state-of-the-art classifiers.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {20935–20959},
numpages = {25},
keywords = {Extreme learning machine, Back-propagation neural networks, Color object recognition, Spherical harmonics, Zernike moments, Disc-Harmonic moments, Color image feature extraction, Quaternion algebra}
}

@article{10.1016/j.neucom.2015.10.046,
author = {Zhang, Peng and Zhuo, Tao and Zhang, Yanning and Tao, Dapeng and Cheng, Jun},
title = {Online tracking based on efficient transductive learning with sample matching costs},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {175},
number = {PA},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.10.046},
doi = {10.1016/j.neucom.2015.10.046},
abstract = {Visual tracking has been a popular and attractive topic in computer vision for a long time. In recent decades, many challenge problems in object tracking has been effectively resolved by using learning based tracking strategies. Number of investigations carried on learning theory found that when labeled samples are limited, the learning performance can be sufficiently improved by exploiting unlabeled ones. Therefore, one of the most important issue for semi-supervised learning is how to assign the labels to the unlabeled samples, which is also the principal focus of transductive learning. Unfortunately, considering the efficiency requirement of online tracking, the optimization scheme employed by the traditional transductive learning is hard to be applied to online tracking problems because of its large computational cost during sample labeling. In this paper, we proposed an efficient transductive learning for online tracking by utilizing the correspondences among the generated unlabeled and labeled samples. Those variational correspondences are modeled by a matching costs function to achieve more efficient learning of representative separators. With a strategy of fixed budget for support vectors, the proposed learning is updated by using a weighted accumulative average of model coefficients. We evaluated the proposed tracking on benchmark database, the experiment results have demonstrated an outstanding performance via comparing with the other state-of-the-art trackers.},
journal = {Neurocomput.},
month = jan,
pages = {166–176},
numpages = {11},
keywords = {Transductive learning, Tracking, Online, Matching costs, Efficient}
}

@inproceedings{10.1145/3475957.3484448,
author = {Vlasenko, Bogdan and Prasad, RaviShankar and Magimai.-Doss, Mathew},
title = {Fusion of Acoustic and Linguistic Information using Supervised Autoencoder for Improved Emotion Recognition},
year = {2021},
isbn = {9781450386784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475957.3484448},
doi = {10.1145/3475957.3484448},
abstract = {Automatic recognition of human emotion has a wide range of applications and has always attracted increasing attention. Expressions of human emotions can apparently be identified across different modalities of communication, such as speech, text, mimics, etc. The "Multimodal Sentiment Analysis in Real-life Media' (MuSe) 2021 challenge provides an environment to develop new techniques to recognize human emotions or sentiments using multiple modalities (audio, video, and text) over in-the-wild data. The challenge encourages to jointly model the information across audio, video and text modalities, for improving emotion recognition. The present paper describes our attempt towards the MuSe-Sent task in the challenge. The goal of the sub-challenge is to perform turn-level prediction of emotions within the arousal and valence dimensions. In the paper, we investigate different approaches to optimally fuse linguistic and acoustic information for emotion recognition systems. The proposed systems employ features derived from these modalities, and uses different deep learning architectures to explore their cross-dependencies. Wide range of acoustic and linguistic features provided by organizers and recently established acoustic embedding wav2vec 2.0 are used for modeling the inherent emotions. In this paper we compare discriminative characteristics of hand-crafted and data-driven acoustic features in a context of emotional classification in arousal and valence dimensions. Ensemble based classifiers were compared with advanced supervised autoendcoder (SAE) technique with Bayesian Optimizer hyperparameter tuning approach. Comparison of uni- and bi-modal classification techniques showed that joint modeling of acoustic and linguistic cues could improve classification performance compared to individual modalities. Experimental results show improvement over the proposed baseline system, which focuses on fusion of acoustic and text based information, on the test set evaluation.},
booktitle = {Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge},
pages = {51–59},
numpages = {9},
keywords = {wav2vec2, supervised auto encoders, late fusion, emotion recognition, bag-of-audio-words},
location = {Virtual Event, China},
series = {MuSe '21}
}

@article{10.1504/IJAISC.2013.056838,
author = {Paulraj, M. P. and Andrew, Allan Melvin},
title = {Classification of interior noise comfort level of Proton model cars using feedforward neural network},
year = {2013},
issue_date = {September 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {3},
number = {4},
issn = {1755-4950},
url = {https://doi.org/10.1504/IJAISC.2013.056838},
doi = {10.1504/IJAISC.2013.056838},
abstract = {In this research, a Proton model cars noise comfort level classification system has been developed to detect the noise comfort level in cars using artificial neural network. This research focuses on developing a database consisting of car sound samples measured from different Proton make models in stationary and moving state. In the stationary condition, the sound pressure level is measured at 1,300 RPM, 2,000 RPM and 3,000 RPM while in moving condition, the sound is recorded using dB Orchestra while the car is moving at constant speed from 30 km/h up to 110 km/h. Subjective test is conducted to find the jury's evaluation for the specific sound sample. The feature set is then feed to the neural network model to classify the comfort level. The spectral power feature gives the highest classification accuracy of 88.42%.},
journal = {Int. J. Artif. Intell. Soft Comput.},
month = sep,
pages = {344–359},
numpages = {16}
}

@inproceedings{10.5555/3155562.3155664,
author = {Krismayer, Thomas and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Mining constraints for event-based monitoring in systems of systems},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {826–831},
numpages = {6},
keywords = {systems of systems, event-based monitoring, Constraint mining},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@article{10.1016/j.jss.2016.01.038,
title = {Towards semi-automated assignment of software change requests},
year = {2016},
issue_date = {May 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {115},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.01.038},
doi = {10.1016/j.jss.2016.01.038},
abstract = {We present a configurable approach to assign Change Requests to software developers.It supports contextual information necessary to dynamic environments.The approach relies on Rule-Based Expert System and machine learning techniques.It shows an improvement of accuracy up to 46.5% over other approaches. Change Requests (CRs) are key elements to software maintenance and evolution. Finding the appropriate developer to a CR is crucial for obtaining the lowest, economically feasible, fixing time. Nevertheless, assigning CRs is a labor-intensive and time consuming task. In this paper, we report on a questionnaire-based survey with practitioners to understand the characteristics of CR assignment, and on a semi-automated approach for CR assignment which combines rule-based and machine learning techniques. In accordance with the results of the survey, the proposed approach emphasizes the use of contextual information, essential to effective assignments, and puts the development team in control of the assignment rules, toward making its adoption easier. The assignment rules can be either extracted from the assignment history or created from scratch. An empirical validation was performed through an offline experiment with CRs from a large software project. The results pointed out that the approach is up to 46.5% more accurate than other approaches which relying solely on machine learning techniques. This indicates that a rule-based approach is a viable and simple method to leverage CR assignments.},
journal = {J. Syst. Softw.},
month = may,
pages = {82–101},
numpages = {20}
}

@inproceedings{10.5555/3491440.3491568,
author = {Guo, Dan and Wang, Yang and Song, Peipei and Wang, Meng},
title = {Recurrent relational memory network for unsupervised image captioning},
year = {2021},
isbn = {9780999241165},
abstract = {Unsupervised image captioning with no annotations is an emerging challenge in computer vision, where the existing arts usually adopt GAN (Generative Adversarial Networks) models. In this paper, we propose a novel memory-based network rather than GAN, named Recurrent Relational Memory Network (R2M). Unlike complicated and sensitive adversarial learning that non-ideally performs for long sentence generation, R2M implements a concepts-to-sentence memory translator through two-stage memory mechanisms: fusion and recurrent memories, correlating the relational reasoning between common visual concepts and the generated words for long periods. R2M encodes visual context through unsupervised training on images, while enabling the memory to learn from irrelevant textual corpus via supervised fashion. Our solution enjoys less learnable parameters and higher computational efficiency than GAN-based methods, which heavily bear parameter sensitivity. We experimentally validate the superiority of R2M than state-of-the-arts on all benchmark datasets.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {128},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{10.1145/3230709,
author = {Liu, Wenhe and Chang, Xiaojun and Yan, Yan and Yang, Yi and Hauptmann, Alexander G.},
title = {Few-Shot Text and Image Classification via Analogical Transfer Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230709},
doi = {10.1145/3230709},
abstract = {Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {20},
keywords = {classification, Transfer learning}
}

@inproceedings{10.1145/3474085.3475317,
author = {Wang, Wen and Cao, Yang and Zhang, Jing and He, Fengxiang and Zha, Zheng-Jun and Wen, Yonggang and Tao, Dacheng},
title = {Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475317},
doi = {10.1145/3474085.3475317},
abstract = {Detection transformers have recently shown promising object detection results and attracted increasing attention. However, how to develop effective domain adaptation techniques to improve its cross-domain performance remains unexplored and unclear. In this paper, we delve into this topic and empirically find that direct feature distribution alignment on the CNN backbone only brings limited improvements, as it does not guarantee domain-invariant sequence features in the transformer for prediction. To address this issue, we propose a novel Sequence Feature Alignment (SFA) method that is specially designed for the adaptation of detection transformers. Technically, SFA consists of a domain query-based feature alignment (DQFA) module and a token-wise feature alignment (TDA) module. In DQFA, a novel domain query is used to aggregate and align global context from the token sequence of both domains. DQFA reduces the domain discrepancy in global feature representations and object relations when deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA aligns token features in the sequence from both domains, which reduces the domain gaps in local and instance-level feature representations in the transformer encoder and decoder, respectively. Besides, a novel bipartite matching consistency loss is proposed to enhance the feature discriminability for robust object detection. Experiments on three challenging benchmarks show that SFA outperforms state-of-the-art domain adaptive object detection methods. Code has been made available at: https://github.com/encounter1997/SFA.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1730–1738},
numpages = {9},
keywords = {object detection, matching consistency, feature alignment, domain adaptation, detection transformer},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1609/aaai.v33i01.33019005,
author = {Wu, Xiang and Huang, Huaibo and Patel, Vishal M. and He, Ran and Sun, Zhenan},
title = {Disentangled variational representation for heterogeneous face recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019005},
doi = {10.1609/aaai.v33i01.33019005},
abstract = {Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1105},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1504/IJCAT.2009.024073,
author = {Zhao, Mansuo and Tien, David},
title = {An adaptive segmentation method using MRF model for suburban aerial images},
year = {2009},
issue_date = {March 2009},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {34},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2009.024073},
doi = {10.1504/IJCAT.2009.024073},
abstract = {An adaptive image segmentation method using Markov random field model for suburban aerial images is presented in this paper. The image is modelled as a collection of regions characterised by slowly moving averages and standard deviation. Decreasing sized windows are used to calculate the moving averages during the iteration process. A function based weighting parameter between the two components in the energy function is also used to improve the performance of unsupervised segmentation. A hierarchical implementation scheme is also introduced to reduce the computation load and increase the segmentation speed.},
journal = {Int. J. Comput. Appl. Technol.},
month = mar,
pages = {235–240},
numpages = {6},
keywords = {hierarchical implementation, aerial suburban images, adaptive image segmentation, Markov random field model}
}

@article{10.1186/s13677-020-00170-1,
author = {Ding, Weilong and Xia, Yanqing and Wang, Zhe and Chen, Zhenyu and Gao, Xingyu},
title = {An ensemble-learning method for potential traffic hotspots detection on heterogeneous spatio-temporal data in highway domain},
year = {2020},
issue_date = {Dec 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {9},
number = {1},
issn = {2192-113X},
url = {https://doi.org/10.1186/s13677-020-00170-1},
doi = {10.1186/s13677-020-00170-1},
abstract = {Inter-city highway plays an important role in modern urban life and generates sensory data with spatio-temporal characteristics. Its current situation and future trends are valuable for vehicles guidance and transportation security management. As a domain routine analysis, daily detection of traffic hotspots faces challenges in efficiency and precision, because huge data deteriorates processing latency and many correlative factors cannot be fully considered. In this paper, an ensemble-learning based method for potential traffic hotspots detection is proposed. Considering time, space, meteorology, and calendar conditions, daily traffic volume is modeled on heterogeneous data, and trends predictive error can be reduced through gradient boosting regression technology. Using real-world data from one Chinese provincial highway, extensive experiments and case studies show our methods with second-level executive latency with a distinct improvement in predictive precision.},
journal = {J. Cloud Comput.},
month = may,
numpages = {11},
keywords = {Big Data, Highway, Ensemble learning, Traffic trends, Spatio-temporal data}
}

@article{10.1007/s11334-006-0022-8,
author = {Pecora, Federico and Rasconi, Riccardo and Cortellessa, Gabriella and Cesta, Amedeo},
title = {User-oriented problem abstractions in scheduling},
year = {2006},
issue_date = {Mar 2006},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {1},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-006-0022-8},
doi = {10.1007/s11334-006-0022-8},
abstract = {In this paper we describe a modeling framework aimed at facilitating the customization and deployment of artificial intelligence (AI) scheduling technology in real-world contexts. Specifically, we describe an architecture aimed at facilitating software product line development in the context of scheduling systems. The framework is based on two layers of abstraction: a first layer providing an interface with the scheduling technology, on top of which we define a formalism to abstract domain-specific concepts. We show how this two-layer modeling framework provides a versatile formalism for defining user-oriented problem abstractions, which is pivotal for facilitating interaction between domain experts and technologists. Moreover, we describe a graphical user interface (GUI)-enhanced tool which allows the domain expert to interact with the underlying core scheduling technology in domain-specific terms. This is achieved by automatically instantiating an abstract GUI template on top of the second modeling layer.},
journal = {Innov. Syst. Softw. Eng.},
month = mar,
pages = {1–16},
numpages = {16},
keywords = {Fast prototyping, Reuse, Scheduling Tool customization, Domain elicitation}
}

@inproceedings{10.1007/978-3-030-32239-7_52,
author = {Xie, Yutong and Lu, Hao and Zhang, Jianpeng and Shen, Chunhua and Xia, Yong},
title = {Deep Segmentation-Emendation Model for Gland Instance Segmentation},
year = {2019},
isbn = {978-3-030-32238-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32239-7_52},
doi = {10.1007/978-3-030-32239-7_52},
abstract = {Accurate and automated gland instance segmentation on histology microscopy images can assist pathologists to diagnose the malignancy degree of colorectal adenocarcinoma. To address this problem, many deep convolutional neural network (DCNN) based methods have been proposed, most of which aim to generate better segmentation by improving the model structure and loss function. Few of them, however, focus on further emendating the inferred predictions, thus missing a chance to refine the obtained segmentation results. In this paper, we propose the deep segmentation-emendation (DSE) model for gland instance segmentation. This model consists of a segmentation network (Seg-Net) and an emendation network (Eme-Net). The Seg-Net is dedicated to generating segmentation results, and the Eme-Net learns to predict the inconsistency between the ground truth and the segmentation results generated by Seg-Net. The predictions made by Eme-Net can in turn be used to refine the segmentation result. We evaluated our DSE model against five recent deep learning models on the 2015 MICCAI Gland Segmentation challenge (GlaS) dataset and against two deep learning models on the colorectal adenocarcinoma (CRAG) dataset. Our results indicate that using Eme-Net results in significant improvement in segmentation accuracy, and the proposed DSE model is able to substantially outperform all the rest models in gland instance segmentation on both datasets.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I},
pages = {469–477},
numpages = {9},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3292500.3330877,
author = {Wang, Yuandong and Yin, Hongzhi and Chen, Hongxu and Wo, Tianyu and Xu, Jie and Zheng, Kai},
title = {Origin-Destination Matrix Prediction via Graph Convolution: a New Perspective of Passenger Demand Modeling},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330877},
doi = {10.1145/3292500.3330877},
abstract = {Ride-hailing applications are becoming more and more popular for providing drivers and passengers with convenient ride services, especially in metropolises like Beijing or New York. To obtain the passengers' mobility patterns, the online platforms of ride services need to predict the number of passenger demands from one region to another in advance. We formulate this problem as an Origin-Destination Matrix Prediction (ODMP) problem. Though this problem is essential to large-scale providers of ride services for helping them make decisions and some providers have already put it forward in public, existing studies have not solved this problem well. One of the main reasons is that the ODMP problem is more challenging than the common demand prediction. Besides the number of demands in a region, it also requires the model to predict the destinations of them. In addition, data sparsity is a severe issue. To solve the problem effectively, we propose a unified model, Grid-Embedding based Multi-task Learning (GEML) which consists of two components focusing on spatial and temporal information respectively. The Grid-Embedding part is designed to model the spatial mobility patterns of passengers and neighboring relationships of different areas, the pre-weighted aggregator of which aims to sense the sparsity and range of data. The Multi-task Learning framework focuses on modeling temporal attributes and capturing several objectives of the ODMP problem. The evaluation of our model is conducted on real operational datasets from UCAR and Didi. The experimental results demonstrate the superiority of our GEML against the state-of-the-art approaches.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1227–1235},
numpages = {9},
keywords = {multi-task learning, graph convolution, demand prediction},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.5555/1787943.1787950,
author = {Shafiq, M. Zubair and Farooq, Muddassar and Khayam, Syed Ali},
title = {A comparative study of fuzzy inference systems, neural networks and adaptive neuro fuzzy inference systems for portscan detection},
year = {2008},
isbn = {3540787607},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Worms spread by scanning for vulnerable hosts across the Internet. In this paper we report a comparative study of three classification schemes for automated portscan detection. These schemes include a simple Fuzzy Inference System (FIS) that uses classical inductive learning, a Neural Network that uses back propagation algorithm and an Adaptive Neuro Fuzzy Inference System (ANFIS) that also employs back propagation algorithm. We carry out an unbiased evaluation of these schemes using an endpoint based traffic dataset. Our results show that ANFIS (though more complex) successfully combines the benefits of the classical FIS and Neural Network to achieve the best classification accuracy.},
booktitle = {Proceedings of the 2008 Conference on Applications of Evolutionary Computing},
pages = {52–61},
numpages = {10},
keywords = {portscan detection, neural networks, information theoretic features, adaptive neuro fuzzy inference system (ANFIS)},
location = {Naples, Italy},
series = {Evo'08}
}

@article{10.1016/j.eswa.2017.07.021,
author = {Zhou, Feng and Jiao, Jianxin Roger and Yang, Xi Jessie and Lei, Baiying},
title = {Augmenting feature model through customer preference mining by hybrid sentiment analysis},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {89},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.07.021},
doi = {10.1016/j.eswa.2017.07.021},
abstract = {We use sentiment analysis of online product reviewers to extract customer preference information.The proposed sentiment analysis method is a hybrid combination of various affective lexicons.We adopt the commented features from product users to enhance the basic feature.We incorporate the customer preference information as attribute into the model.We demonstrate the feasibility and potential of the proposed method via an application case. A feature model is an essential tool to identify variability and commonality within a product line of an enterprise, assisting stakeholders to configure product lines and to discover opportunities for reuse. However, the number of product variants needed to satisfy individual customer needs is still an open question, as feature models do not incorporate any direct customer preference information. In this paper, we propose to incorporate customer preference information into feature models using sentiment analysis of user-generated online product reviews. The proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough-set technique. It is able to predict sentence sentiments for individual product features with acceptable accuracy, and thus augment a feature model by integrating positive and negative opinions of the customers. Such opinionated customer preference information is regarded as one attribute of the features, which helps to decide the number of variants needed within a product line. Finally, we demonstrate the feasibility and potential of the proposed method via an application case of Kindle Fire HD tablets.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {306–317},
numpages = {12},
keywords = {Sentiment analysis, Product line planning, Feature model, Customer preference mining}
}

@article{10.5555/3288443.3288514,
author = {L\"{u}si, Iiris and Bolotnikova, Anastasia and Daneshmand, Morteza and Ozcinar, Cagri and Anbarjafari, Gholamreza},
title = {Optimal image compression via block-based adaptive colour reduction with minimal contour effect},
year = {2018},
issue_date = {Dec 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {23},
issn = {1380-7501},
abstract = {Current image acquisition devices require tremendous amounts of storage for saving the data returned. This paper overcomes the latter drawback through proposing a colour reduction technique which first subdivides the image into patches, and then makes use of fuzzy c-means and fuzzy-logic-based inference systems, in order to cluster and reduce the number of the unique colours present in each patch, iteratively. The colours available in each patch are quantised, and the emergence of false edges is checked for, by means of the Sobel edge detection algorithm, so as to minimise the contour effect. At the compression stage, a methodology taking advantage of block-based singular value decomposition and wavelet difference reduction is adopted. Considering 35000 sample images from various databases, the proposed method outperforms centre cut, moment-preserving threshold, inter-colour correlation, generic K-means and quantisation by dimensionality reduction.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {30939–30968},
numpages = {30},
keywords = {Image compression, Colour image processing, Block processing, Adaptive colour reduction}
}

@inproceedings{10.1145/3078971.3078996,
author = {Shi, Haoyue and Chen, Jia and Hauptmann, Alexander G.},
title = {Joint Saliency Estimation and Matching using Image Regions for Geo-Localization of Online Video},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3078996},
doi = {10.1145/3078971.3078996},
abstract = {In this paper, we study automatic geo-localization of online event videos. Different from general image localization task through matching, the appearance of an environment during significant events varies greatly from its daily appearance, since there are usually crowds, decorations or even destruction when a major event happens. This introduces a major challenge: matching the event environment to the daily environment, e.g. as recorded by Google Street View. We observe that some regions in the image, as part of the environment, still preserve the daily appearance even though the whole image (environment) looks quite different. Based on this observation, we formulate the problem as joint saliency estimation and matching at the image region level, as opposed to the key point or whole-image level. As image-level labels of daily environment are easily generated with GPS information, we treat region based saliency estimation and matching as a weakly labeled learning problem over the training data. Our solution is to iteratively optimize saliency and the region-matching model. For saliency optimization, we derive a closed form solution, which has an intuitive explanation. For region matching model optimization, we use self-paced learning to learn from the pseudo labels generated by (sub-optimal) saliency values. We conduct extensive experiments on two challenging public datasets: Boston Marathon 2013 and Tokyo Time Machine. Experimental results show that our solution significantly improves over matching on whole images and the automatically learned saliency is a strong predictor of distinctive building areas.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {383–391},
numpages = {9},
keywords = {video geo-localization, region saliency, region matching},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@inproceedings{10.1145/3098279.3098564,
author = {Ghosh, Surjya and Ganguly, Niloy and Mitra, Bivas and De, Pradipta},
title = {TapSense: combining self-report patterns and typing characteristics for smartphone based emotion detection},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098564},
doi = {10.1145/3098279.3098564},
abstract = {Typing based communication applications on smartphones, like WhatsApp, can induce emotional exchanges. The effects of an emotion in one session of communication can persist across sessions. In this work, we attempt automatic emotion detection by jointly modeling the typing characteristics, and the persistence of emotion. Typing characteristics, like speed, number of mistakes, special characters used, are inferred from typing sessions. Self reports recording emotion states after typing sessions capture persistence of emotion. We use this data to train a personalized machine learning model for multi-state emotion classification. We implemented an Android based smartphone application, called TapSense, that records typing related metadata, and uses a carefully designed Experience Sampling Method (ESM) to collect emotion self reports. We are able to classify four emotion states - happy, sad, stressed, and relaxed, with an average accuracy (AUCROC) of 84% for a group of 22 participants who installed and used TapSense for 3 weeks.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {2},
numpages = {12},
keywords = {experience sampling method, emotion persistence, emotion detection, Smartphone typing, SMOTE, Markov chain},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@article{10.1504/ijbidm.2021.111744,
author = {Scheidler, Anne Antonia and Rabe, Markus},
title = {Integral verification and validation for knowledge discovery procedure models},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {1},
issn = {1743-8195},
url = {https://doi.org/10.1504/ijbidm.2021.111744},
doi = {10.1504/ijbidm.2021.111744},
abstract = {This paper explains why the knowledge discovery in database (KDD) procedure models lacks verification and validation (V&amp;V) mechanisms and introduces an approach for integral V&amp;V. Based on a generic model for knowledge discovery, a structure named 'KDD triangle model' is presented. This model has a modular design and can be adapted for other KDD procedure models. This has the benefit of allowing existing projects for improving their quality assurance in knowledge discovery. In this paper, the different phases of the developed triangle model for KDD are discussed. One special focus is on the phase results and related testing mechanisms. This paper also describes possible V&amp;V techniques for the developed integral V&amp;V mechanism to ensure direct applicability of the model.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {73–87},
numpages = {14},
keywords = {quality assurance, verification and validation, procedure model, data mining, KDD, knowledge discovery in databases}
}

@inproceedings{10.1145/2911451.2911481,
author = {Lak, Parisa},
title = {A Novel Approach to Define and Model Contextual Features in Recommender Systems},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911481},
doi = {10.1145/2911451.2911481},
abstract = {Recommender Systems(RS) provide more accurate and more relevant recommendations using contextual feature(s). This accuracy improvement is at the cost of computational expenses. Therefore, finding and selecting the most relevant contextual features is an important problem. Moreover, modeling and incorporating the selected contextual features in RS algorithms has an impact on both the accuracy and computational cost. We are conducting a series of studies to detect, define, select, model and incorporate the most relevant contextual features for RS algorithms. The feature detection, definition and selection approach involves the evaluation of features derived from implicit and explicit information. The selected features from this approach can be modeled and incorporated in any selected RS algorithm.In our recent works, we also propose a series of algorithms that incorporates multiple contextual features in the baseline matrix factorization (MF) algorithm. We use the selected contextual features to modify user biases and item biases in the baseline MF.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1161},
numpages = {1},
keywords = {collaborative filtering, context aware recommender systems, context awareness, contextual feature selection, item based recommender system, matrix factorization, recommender systems, user base recommender systems},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@article{10.1007/s11263-018-1134-y,
author = {Deng, Jiankang and Roussos, Anastasios and Chrysos, Grigorios and Ververas, Evangelos and Kotsia, Irene and Shen, Jie and Zafeiriou, Stefanos},
title = {The Menpo Benchmark for Multi-pose 2D and 3D Facial Landmark Localisation and Tracking},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {6–7},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1134-y},
doi = {10.1007/s11263-018-1134-y},
abstract = {In this article, we present the Menpo 2D and Menpo 3D benchmarks, two new datasets for multi-pose 2D and 3D facial landmark localisation and tracking. In contrast to the previous benchmarks such as 300W and 300VW, the proposed benchmarks contain facial images in both semi-frontal and profile pose. We introduce an elaborate semi-automatic methodology for providing high-quality annotations for both the Menpo 2D and Menpo 3D benchmarks. In Menpo 2D benchmark, different visible landmark configurations are designed for semi-frontal and profile faces, thus making the 2D face alignment full-pose. In Menpo 3D benchmark, a united landmark configuration is designed for both semi-frontal and profile faces based on the correspondence with a 3D face model, thus making face alignment not only full-pose but also corresponding to the real-world 3D space. Based on the considerable number of annotated images, we organised Menpo 2D Challenge and Menpo 3D Challenge for face alignment under large pose variations in conjunction with CVPR 2017 and ICCV 2017, respectively. The results of these challenges demonstrate that recent deep learning architectures, when trained with the abundant data, lead to excellent results. We also provide a very simple, yet effective solution, named Cascade Multi-view Hourglass Model, to 2D and 3D face alignment. In our method, we take advantage of all 2D and 3D facial landmark annotations in a joint way. We not only capitalise on the correspondences between the semi-frontal and profile 2D facial landmarks but also employ joint supervision from both 2D and 3D facial landmarks. Finally, we discuss future directions on the topic of face alignment.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {599–624},
numpages = {26},
keywords = {Menpo challenge, 3D face alignment, 2D face alignment}
}

@article{10.1007/s10515-010-0066-8,
author = {Apel, Sven and K\"{a}stner, Christian and Gr\"{o}βlinger, Armin and Lengauer, Christian},
title = {Type safety for feature-oriented product lines},
year = {2010},
issue_date = {September 2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0066-8},
doi = {10.1007/s10515-010-0066-8},
abstract = {A feature-oriented product line is a family of programs that share a common set of features. A feature implements a stakeholder's requirement and represents a design decision or configuration option. When added to a program, a feature involves the introduction of new structures, such as classes and methods, and the refinement of existing ones, such as extending methods. A feature-oriented decomposition enables a generator to create an executable program by composing feature code solely on the basis of the feature selection of a user--no other information needed. A key challenge of product line engineering is to guarantee that only well-typed programs are generated. As the number of valid feature combinations grows combinatorially with the number of features, it is not feasible to type check all programs individually. The only feasible approach is to have a type system check the entire code base of the feature-oriented product line. We have developed such a type system on the basis of a formal model of a feature-oriented Java-like language. The type system guaranties type safety for feature-oriented product lines. That is, it ensures that every valid program of a well-typed product line is well-typed. Our formal model including type system is sound and complete.},
journal = {Automated Software Engg.},
month = sep,
pages = {251–300},
numpages = {50},
keywords = {Type systems, Software product lines, Safe composition, Feature-oriented programming, Feature featherweight Java}
}

@inproceedings{10.1145/2813524.2813529,
author = {Hines, Christopher and Sethu, Vidhyasaharan and Epps, Julien},
title = {Twitter: A New Online Source of Automatically Tagged Data for Conversational Speech Emotion Recognition},
year = {2015},
isbn = {9781450337502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2813524.2813529},
doi = {10.1145/2813524.2813529},
abstract = {In the space of affect detection in multimedia, there is a strong demand for more tagged data in order to better understand human emotions, the way they are expressed, and approaches for detecting them automatically. Unfortunately, emotion datasets are typically small due to the manual process of annotating them with emotional labels. In response, we present for the first time the application of automatically tagged Twitter data to the problem of speech emotion recognition (SER). SER has been shown to benefit from the combination of acoustic and linguistic features, albeit when the linguistic training data is from the same database as the test data. Using the presence of emoticons for automatic tagging, we compile a corpus of over 800,000 tweets that is totally independent from our evaluation database. By supplementing an acoustic classifier with linguistic information, we classify the spontaneous content within the USC-IEMOCAP corpus on valence and activation descriptors. With comparison to prior literature, we demonstrate performance improvements for valence of 2% and 6% over an acoustic-only system, using linguistic training data from Twitter and IEMOCAP respectively.},
booktitle = {Proceedings of the 1st International Workshop on Affect &amp; Sentiment in Multimedia},
pages = {9–14},
numpages = {6},
keywords = {twitter, linguistics, emotion recognition, IEMOCAP},
location = {Brisbane, Australia},
series = {ASM '15}
}

@article{10.1007/s10115-020-01447-2,
author = {Sun, Yatong and Guo, Guibing and Chen, Xu and Zhang, Penghai and Wang, Xingwei},
title = {Exploiting review embedding and user attention for item recommendation},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {62},
number = {8},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-020-01447-2},
doi = {10.1007/s10115-020-01447-2},
abstract = {As a valuable source of user preferences and item properties, reviews have been widely leveraged in many approaches to enhance the performance of recommender systems. Although encouraging success has been obtained, there are two more weaknesses need to be addressed. (1) Most approaches represent users or items merely based on the modeling of review texts, but ignore the potential and latent preferences beyond textual information. (2) Existing methods tend to blindly merge all the previous reviews for user profiling. However, it may be less effective because different interacted items may play distinct roles. Hence, indiscriminately aligning interacted items may limit the model flexibility and performance. In this paper, with the desire to fill these gaps, we design a novel attentive deep review-based recommendation method. In specific, we complement the item representation by an auxiliary vector, based on which a user is then attentively profiled by her posted items to predict the likeness for the target item. Extensive experiments on five real-world datasets demonstrate that our model can not only significantly outperform the state-of-the-art methods, but also provide intuitive explanations to the recommendations.},
journal = {Knowl. Inf. Syst.},
month = aug,
pages = {3015–3038},
numpages = {24},
keywords = {User reviews, Recommender systems, Deep neural model, Attention network}
}

@article{10.1016/j.neunet.2010.12.010,
author = {Funamizu, Akihiro and Kanzaki, Ryohei and Takahashi, Hirokazu},
title = {Distributed representation of tone frequency in highly decodable spatio-temporal activity in the auditory cortex},
year = {2011},
issue_date = {May, 2011},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {24},
number = {4},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2010.12.010},
doi = {10.1016/j.neunet.2010.12.010},
abstract = {Although the place code of tone frequency, or tonotopic map, has been widely accepted in the auditory cortex, tone-evoked activation becomes less frequency-specific at moderate or high sound pressure levels. This implies that sound frequency is not represented by a simple place code but that the information is distributed spatio-temporally irrespective of the focal activation. In this study, using a decoding-based analysis, we investigated multi-unit activities in the auditory cortices of anesthetized rats to elucidate how a tone frequency is represented in the spatio-temporal neural pattern. We attempted sequential dimensionality reduction (SDR), a specific implementation of recursive feature elimination (RFE) with support vector machine (SVM), to identify the optimal spatio-temporal window patterns for decoding test frequency. SDR selected approximately a quarter of the windows, and SDR-identified window patterns led to significantly better decoding than spatial patterns, in which temporal structures were eliminated, or high-spike-rate patterns, in which windows with high spike rates were selectively extracted. Thus, the test frequency is also encoded in temporal as well as spatial structures of neural activities and low-spike-rate windows. Yet, SDR recruited more high-spike-rate windows than low-spike-rate windows, resulting in a highly dispersive pattern that probably offers an advantage of discrimination ability. Further investigation of SVM weights suggested that low-spike-rate windows play significant roles in fine frequency differentiation. These findings support the hypothesis that the auditory cortex adopts a distributed code in tone frequency representation, in which high- and low-spike-rate activities play mutually complementary roles.},
journal = {Neural Netw.},
month = may,
pages = {321–332},
numpages = {12},
keywords = {Support vector machine, Multi-unit recording, Machine learning, Dimensionality reduction, Auditory cortex}
}

@inbook{10.5555/3454287.3454706,
author = {Tu, Tao and Paisley, John and Haufe, Stefan and Sajda, Paul},
title = {A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inferring effective connectivity between spatially segregated brain regions is important for understanding human brain dynamics in health and disease. Non-invasive neuroimaging modalities, such as electroencephalography (EEG) and functional magnetic resonance imaging (fMRI), are often used to make measurements and infer connectivity. However most studies do not consider integrating the two modalities even though each is an indirect measure of the latent neural dynamics and each has its own spatial and/or temporal limitations. In this study, we develop a linear state-space model to infer the effective connectivity in a distributed brain network based on simultaneously recorded EEG and fMRI data. Our method first identifies task-dependent and subject-dependent regions of interest (ROI) based on the analysis of fMRI data. Directed influences between the latent neural states at these ROIs are then modeled as a multivariate autogressive (MVAR) process driven by various exogenous inputs. The latent neural dynamics give rise to the observed scalp EEG measurements via a biophysically informed linear EEG forward model. We use a mean-field variational Bayesian approach to infer the posterior distribution of latent states and model parameters. The performance of the model was evaluated on two sets of simulations. Our results emphasize the importance of obtaining accurate spatial localization of ROIs from fMRI. Finally, we applied the model to simultaneously recorded EEG-fMRI data from 10 subjects during a Face-Car-House visual categorization task and compared the change in connectivity induced by different stimulus categories.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {419},
numpages = {10}
}

@inproceedings{10.1109/IWCDM.2011.47,
author = {Fu, Ziying and Tang, Jia and Zhang, Cun and Bai, Jing and Chen, Qicai},
title = {Neural Inhibition in Forward Masking of the Mouse Inferior Collicular Neurons: In vivo Intracellular Recording},
year = {2011},
isbn = {9780769545851},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IWCDM.2011.47},
doi = {10.1109/IWCDM.2011.47},
abstract = {Under free field stimulation condition, the forward masking of inferior collicular (IC) neurons of mouse (Mus musculus, Km) was examined by a pair of sound stimuli with different gap and fixed amplitude (80 dB SPL) in the present study. Total 32 IC neurons were obtained and the forward masking of 24 among these neurons was studied. The action potential (AP) firing to the probe in 58.3% of IC neurons could be completely recovery to the same degree with single tone stimulus when the gap of paired sound was longer than the duration of inhibitory postsynaptic potential (IPSP) evoked by masker occurring at post-AP. The AP firing to the probe in the remaining (41.7%) IC neurons could be completely recovery at paired sound gap less than or equal to the duration of the masker-evoked IPSP occurring at post-AP. However, the response latency to the probe could be just completely recovery when paired sound gap was longer than the duration of maskerevoked IPSP occurring at post-AP in almost all the IC neurons (23/24, 95.8%, P&lt;0.05). These results suggested that membrane potential state of the IC neurons, IPSP and synaptic depression might participate in the generation of forward masking.},
booktitle = {Proceedings of the 2011 First International Workshop on Complexity and Data Mining},
pages = {104–107},
numpages = {4},
keywords = {intracellular recording, inhibitory postsynaptic potential, inferior collicular neurons, forward masking},
series = {IWCDM '11}
}

@article{10.1016/j.fss.2008.05.017,
author = {Juarez, Jose M. and Guil, Francisco and Palma, Jose and Marin, Roque},
title = {Temporal similarity by measuring possibilistic uncertainty in CBR},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {160},
number = {2},
issn = {0165-0114},
url = {https://doi.org/10.1016/j.fss.2008.05.017},
doi = {10.1016/j.fss.2008.05.017},
abstract = {Similarity is an essential concept in case-based reasoning (CBR). In domains in which time plays a relevant role, CBR systems require good temporal similarity measures to compare cases. Temporal cases are traditionally represented by a set of temporal features, defining time series and temporal event sequences. In the particular situation where these features are not homogeneous (i.e. combination of qualitative and quantitative information), systems find difficulties in performing the CBR cycle. Furthermore, temporal similarity measures cannot directly apply the efficient time series techniques, requiring new approaches to deal with these heterogeneous sequences. To this end, recent proposals are focused on direct matching between pairs of features within sequences, mainly based on classical distances. However, three limitations to the traditional approaches have been identified: (1) they do not consider the implicit temporal relations amongst all features of the sequence (ignoring a large amount of temporal information); (2) they ignore the uncertainty produced in any process of analogy; (3) they are designed to compare pairs of sequences, limiting their use to basic aspects of the Retrieval step of CBR (no benefits on other CBR steps). Temporal constraint networks have proved to be useful tools for temporal representation and reasoning, and can be easily extended to manage imprecision and uncertainty. An approach to solve similarity problems could be the transformation of these heterogeneous sequences into uncertain temporal relations, obtaining a temporal constraint network. The overall uncertainty of this network can be considered as an effective indicator of the sequences similarity. Therefore, this paper proposes a non-classical approach to measure temporal similarity of cases which are heterogeneous temporal event sequences. Given two or more sequences, the temporal similarity is measured by describing a unique temporal scenario of possibilistic temporal relations and calculating the uncertainty produced.},
journal = {Fuzzy Sets Syst.},
month = jan,
pages = {214–230},
numpages = {17},
keywords = {Temporal constraint networks, Possibility theory, Case-based reasoning}
}

@article{10.1155/2021/1057371,
author = {Yang, Yinghui and cheikhrouhou, omar},
title = {The Potential Energy of Artificial Intelligence Technology in University Education Reform from the Perspective of Communication Science},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/1057371},
doi = {10.1155/2021/1057371},
abstract = {In today’s rapid development of science and technology, science is everywhere in people’s lives, and science communication is everywhere. Science and communication are not only not far away but also very close. Since machine learning algorithms with deep learning as a theme have achieved great success in the fields of vision and speech recognition, as well as the large amount of data resources that cloud computing, big data, and other technologies can provide, the development speed of artificial intelligence has been greatly improved, and it has had a significant impact in various industries in the society, and the country has put forward the concept of intelligent education for this purpose. However, there have been few systematic discussions on the combination of artificial intelligence with education and teaching. Therefore, this article uses artificial intelligence technology to study the potential energy space of artificial intelligence technology in college education reform from the perspective of science communication, designs and implements an online education platform for colleges and universities, and conducts a trial of platform use in a domestic college and universities. Some teachers and students conduct a satisfaction survey after the platform is used, and the conclusions show that whether in the teacher group or the student group, most teachers and students are relatively satisfied with the online education platform designed in this article. The reform of college education includes many aspects. This article is a research study on the form of college education, changing from traditional offline education to online platform education. This research can provide a certain reference for the reform of college education.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {7}
}

@inproceedings{10.1145/3485832.3485896,
author = {Walker, Payton and Saxena, Nitesh},
title = {Evaluating the Effectiveness of Protection Jamming Devices in Mitigating Smart Speaker Eavesdropping Attacks Using Gaussian White Noise},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485896},
doi = {10.1145/3485832.3485896},
abstract = {Protection Jamming Devices (PJD) are specialized tools designed to sit on top of virtual assistant (VA) smart speakers and hinder them from “hearing” nearby user speech. PJDs aim to protect you from eavesdropping attacks by injecting a jamming signal directly into the microphones of the smart speaker. However, current signal processing routines can be used to reduce noise and enhance speech contained in noisy audio samples. Therefore, we identify a potential vulnerability for speech eavesdropping via smart speaker recordings, even when a PJD is being used. If an attacker can gain access to or facilitate smart speaker recordings they may be able to compromise a user’s speech with successful noise cancellation. Specifically, we are interested in the potential for Gaussian white noise (GWN) to be an effective jamming signal for a PJD. To our knowledge, the effectiveness of white noise and PJDs to protect against eavesdropping attacks has yet to receive a systematic evaluation that includes physical experiments with an actual PJD implementation. In this work we construct our own PJD, specialized for consistent experimentation, to simulate an attack scenario where recordings from a smart speaker, in the presence of normal speech and the PJDs jamming signal, are recovered. We perform substantial data collection under different settings to build a repository of 1500 recovered audio samples. We applied post-processing on our dataset and conducted an extensive signal/speech quality analysis including both time and frequency domain inspection, and evaluation of metrics including cross-correlation, SNR, and PESQ. Lastly, we performed feature extraction (MFCC) and built machine learning classifiers for tasks including speech (digit) recognition, speaker identification, and gender recognition. We also attempted song recognition using the Shazam app. For all speech recognition tasks that we attempted, we were able to achieve classification accuracies above that of random guessing (46% for digit recognition, 51% for speaker identification, 80% for gender identification), as well as demonstrate successful song recognition. These results highlight the real potential for attackers to compromise user speech, to some extent, using smart speaker recordings; even if the smart speaker is protected by a PJD.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {414–424},
numpages = {11},
keywords = {eavesdropping, jamming, speech masking, white noise},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@article{10.1016/j.infsof.2015.08.007,
author = {Sep\'{u}lveda, Samuel and Cravero, Ania and Cachero, Cristina},
title = {Requirements modeling languages for software product lines},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.08.007},
doi = {10.1016/j.infsof.2015.08.007},
abstract = {There is a concern for generating proposals with higher levels of expressiveness.There is not a strong relationship between the proposals and SPL development process.There is a need for better ways to validate the modeling proposals.The proposals have a low level of empirical validation and adoption in industry.The level of maturity, expressive power and tool support of the proposals is low. Display Omitted Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs.Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption.Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013.Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46% of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6% of the languages have been empirically validated, 41% report some kind of industry adoption and 71% of the languages are independent from any development process. Last but not least, 57% of the languages have been proposed by the academia, while 43% have been the result of a joint effort between academia and industry.Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–36},
numpages = {21},
keywords = {Systematic literature review, Software product lines, Requirements engineering, Modeling languages}
}

@article{10.1147/JRD.2011.2163277,
author = {Picheny, M. and Nahamoo, D. and Goel, V. and Kingsbury, B. and Ramabhadran, B. and Rennie, S. J. and Saon, G.},
title = {Trends and advances in speech recognition},
year = {2011},
issue_date = {September/October 2011},
publisher = {IBM Corp.},
address = {USA},
volume = {55},
number = {5},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2011.2163277},
doi = {10.1147/JRD.2011.2163277},
abstract = {One of the earliest successful applications of machine-learning techniques to pattern recognition was the application of information-theoretic principles to speech recognition. Previous approaches relied heavily on expert input through the painstaking analysis of data to relate speech signals to the word sequences that produced them. Such methodologies were completely displaced by casting the speech recognition problem in a probabilistic framework by modeling the joint probability distribution of speech signals and word sequences. At the beginning of the 21st century, the amount of data and computation to train and build models has increased exponentially, and the emergence of new machine-learning algorithms and methodologies has opened new vistas in approaching complex pattern recognition problems. This is enabled by a new set of machine-learning techniques referred to as graphical models, with computationally tractable training algorithms. Closely related are neural-network modeling techniques, and there has been a resurgence of interest in the application of neural-network concepts, such as deep networks to speech recognition. The explosion of data has caused the development of new ways to capture the key features in massive amounts of data using efficient methods deploying exemplar-based sparse representations. Lastly, all of these different approaches can be tied together in a principled fashion using another variation of graphical models: an exponential model framework. This paper describes the current state of the art in speech recognition systems and highlights the developments that are expected to produce major breakthroughs in our ability to automatically recognize speech using computers.},
journal = {IBM J. Res. Dev.},
month = sep,
pages = {447–464},
numpages = {18}
}

@inproceedings{10.5555/526253.791734,
author = {Schaffer, M. and Chen, T. and Cantoni, V. and Lombardi, L. and Mosconi, M. and Savini, M. and Setti, A.},
title = {A VLSI architecture for 2D object classification based on tree matching},
year = {1995},
isbn = {0818671343},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents a real-time classification algorithm for 2D object contours using a multi-resolution tree model which is implemented in a modular VLSI architecture. The hardware implementation takes advantage of pipelining, parallelism, and the speed of VLSI technology to perform real-time object classification. Using the multi-resolution tree model, the classification algorithm is invariant under 2D similarity transformations and recognizes the visible portions of occluded objects. The VLSI classification system is implemented in 0.8 /spl mu/m CMOS and is capable of performing 34000 matchings per second.},
booktitle = {Proceedings of the Computer Architectures for Machine Perception},
pages = {138},
keywords = {tree matching, systolic arrays, real-time systems, real-time classification algorithm, pipelining, pipeline processing, parallelism, object recognition, multi-resolution tree model, image matching, image classification, digital signal processing chips, computer vision, classification algorithm, VLSI classification system, VLSI architecture, VLSI, CMOS digital integrated circuits, 2D similarity transformations, 2D object classification, 0.8 mum},
series = {CAMP '95}
}

@proceedings{10.1145/2993148,
title = {ICMI '16: Proceedings of the 18th ACM International Conference on Multimodal Interaction},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3372278.3390712,
author = {Hao, Li and Hou, Liping and Song, Yuantao and Lu, Ke and Xue, Jian},
title = {A Lightweight Gated Global Module for Global Context Modeling in Neural Networks},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3390712},
doi = {10.1145/3372278.3390712},
abstract = {Global context modeling has been used to achieve better performance in various computer-vision-related tasks, such as classification, detection, segmentation and multimedia retrieval applications. However, most of the existing global mechanisms display problems regarding convergence during training. In this paper, we propose a novel gated global module (GGM) that is lightweight and yet effective in terms of achieving better integration of global information in relation to feature representation. Regarding the original structure of the network as a local block, our module infers global information in parallel with local information, and then a gate function is applied to generate global guidance which is applied to the output of the local module to capture representative information. The proposed GGM can be easily integrated with common CNN architectures and is training friendly. We used a classification task as an example to verify the effectiveness of the proposed GGM, and extensive experiments on ImageNet and CIFAR demonstrated that our method can be widely applied and is conducive to integrating global information into common networks.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {532–539},
numpages = {8},
keywords = {global context, gated global module, convolutional neural networks},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@article{10.1016/j.cogsys.2019.10.006,
author = {Ramirez-Pedraza, Raymundo and Vargas, Natividad and Sandoval, Carlos and del Valle-Padilla, Juan Luis and Ramos, F\'{e}lix},
title = {A bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2019.10.006},
doi = {10.1016/j.cogsys.2019.10.006},
journal = {Cogn. Syst. Res.},
month = jan,
pages = {293–303},
numpages = {11},
keywords = {Goal-driven, Motor system, Spatial attention, Planning, Decision-making, Brain model}
}

@inproceedings{10.1145/3448016.3457281,
author = {Zheng, Kaiping and Chen, Gang and Herschel, Melanie and Ngiam, Kee Yuan and Ooi, Beng Chin and Gao, Jinyang},
title = {PACE: Learning Effective Task Decomposition for Human-in-the-loop Healthcare Delivery},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457281},
doi = {10.1145/3448016.3457281},
abstract = {Human-in-the-loop data analysis involves both machine learning models and humans in analytic tasks. In healthcare applications, human-in-the-loop data analysis is crucial in that the model can handle "easy" tasks and hand over "hard" ones to medical experts for assistance and medical judgment, where easy tasks are the ones for which the model can provide high accuracy and hard tasks vice versa. In this process, how to decompose tasks in an effective manner is an important stage. To achieve task decomposition, classification with a reject option is a solution. However, existing studies either directly implement a reject option or dive into the theoretical details of the rejection mechanism. Different from such studies, we aim to optimize general classifiers with a reject option and hence, optimize task decomposition for healthcare applications.To this end, we first introduce task decomposition for healthcare applications, which is a crucial stage in human-in-the-loop healthcare delivery. We then devise a framework PACE to learn effective task decomposition concentrating on delivering high performance on the easy tasks. PACE is two-level: on the macro level, PACE employs the Self-Paced Learning method to select easy tasks for each training iteration; on the micro level, PACE adapts the weights of selected tasks through its weighted loss revision strategy. Experimental results in two real-world healthcare datasets show that PACE outperforms baselines in terms of their performance on the easy tasks which are expected to be solved by the learning model.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2156–2168},
numpages = {13},
keywords = {task decomposition, human-in-the-loop, healthcare},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3338906.3342508,
author = {Radavelli, Marco},
title = {Using software testing to repair models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342508},
doi = {10.1145/3338906.3342508},
abstract = {Software testing is an important phase in the software development process, aiming at locating faults in artifacts, and achieve some confidence that the software behaves according to specification. There exists many software testing techniques applied to debugging, fault-localization, and repair of code, however, to the best of our knowledge, the application of software testing to locating faults in models and automatically repair them, is still an open issue. We present a project that investigates the use of software testing methods to automatically repair model artifacts, to support engineers in maintaining them consistent with the implementation and specification. We describe the research approach, the structure of the devised test-driven repair processes, present results in the cases of combinatorial models and feature models, and finally discuss future work of applying testing to repair models for other scenarios, such as timed automata.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1253–1255},
numpages = {3},
keywords = {timed automata, software testing, software product lines, search-based software engineering, mutation, model repair, CIT},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/2029604.2029611,
author = {Hourdakis, Emmanouil and Trahanias, Panos},
title = {Observational learning based on models of overlapping pathways},
year = {2011},
isbn = {9783642217371},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Brain imaging studies in macaque monkeys have recently shown that the observation and execution of specific types of grasp actions activate the same regions in the parietal, primary motor and somatosensory lobes. In the present paper we consider how learning via observation can be implemented in an artificial agent based on the above overlapping pathway of activations. We demonstrate that the circuitry developed for action execution can be activated during observation, if the agent is able to perform action association, i.e. relate its own actions with the ones of the demonstrator. In addition, by designing the model to activate the same neural codes during execution and observation, we show how the agent can accomplish observational learning of novel objects.},
booktitle = {Proceedings of the 21st International Conference on Artificial Neural Networks - Volume Part II},
pages = {48–55},
numpages = {8},
keywords = {observational learning, grasping behaviors, connectionist model, computational brain modeling},
location = {Espoo, Finland},
series = {ICANN'11}
}

@article{10.1177/1059712312445902,
author = {Hourdakis, Emmanouil and Trahanias, Panos},
title = {Computational modeling of observational learning inspired by the cortical underpinnings of human primates},
year = {2012},
issue_date = {August    2012},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {20},
number = {4},
issn = {1059-7123},
url = {https://doi.org/10.1177/1059712312445902},
doi = {10.1177/1059712312445902},
abstract = {Recent neuroscientific evidence in human and non-human primates indicates that the regions that become active during motor execution and motor observation overlap extensively in the cerebral cortex. This suggests that to observe an action, these primates employ their motor and somatosensation areas in order to simulate it internally. In line with this finding, in the current paper, we examine relevant neuroscientific evidence in order to design a computational agent that can facilitate observational learning of reaching movements. For this reason, we develop a novel motor control system, inspired from contemporary theories of motor control, and demonstrate how it can be used during observation to facilitate learning, without the active involvement of the agent's body. Our results show that novel motor skills can be acquired only by observation, by optimizing the peripheral components of the agent's motion.},
journal = {Adapt Behav},
month = aug,
pages = {237–256},
numpages = {20},
keywords = {overlapping pathways, liquid state machines, computational modeling, Observational learning}
}

@article{10.1016/j.compag.2018.04.005,
author = {Mu\~{n}oz-Benavent, P. and Andreu-Garc\'{\i}a, G. and Valiente-Gonz\'{a}lez, Jos\'{e} M. and Atienza-Vanacloig, V. and Puig-Pons, V. and Espinosa, V.},
title = {Enhanced fish bending model for automatic tuna sizing using computer vision},
year = {2018},
issue_date = {Jul 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {150},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2018.04.005},
doi = {10.1016/j.compag.2018.04.005},
journal = {Comput. Electron. Agric.},
month = jul,
pages = {52–61},
numpages = {10},
keywords = {Biomass estimation, Automatic fish sizing, Fisheries management, Computer vision, Underwater stereo-vision}
}

@inproceedings{10.5555/876895.880907,
title = {On the Mutual Definability of Fuzzy Tolerance Relations and Fuzzy Tolerance Coverings},
year = {1995},
isbn = {0818671181},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Abstract: Studies the mathematical foundations of cluster analysis, i.e. the correspondences between binary relations ("similarity relations") and systems of sets ("clusters") with respect to a fixed universe. For a long time, from crisp set theory and the classical (crisp) theory of universal algebras, such correspondences have been well-known as bijections and lattice isomorphisms between the class of equivalence relations on a universe /spl Uscr/ and the class of partitions of /spl Uscr/. In the middle of the 1960s, there began the study of tolerance relations, i.e. binary relations where, in contrast to equivalence relations, only reflexivity and symmetry are assumed. It was proved that there exists a bijection between the class of tolerance relations on a universe U and a class of special coverings of /spl Uscr/. Schmechel (1995) generalized the classical result about crisp equivalence relations and crisp partitions to the "fuzzy case", i.e. to several classes of fuzzy equivalence relations and corresponding classes of fuzzy partitions. This paper contains a generalization of the result on crisp tolerance relations and crisp coverings to fuzzy tolerance relations and special sets of fuzzy clusters.},
booktitle = {Proceedings of the 25th International Symposium on Multiple-Valued Logic},
pages = {140},
keywords = {universal algebras, tolerance analysis, systems of sets, symmetry, similarity relations, reflexivity, pattern recognition, mutual definability, lattice isomorphisms, fuzzy tolerance relations, fuzzy tolerance coverings, fuzzy set theory, fuzzy partitions, fuzzy equivalence relations, fuzzy clusters, fixed universe, equivalence classes, crisp tolerance relations, crisp set theory, crisp coverings, cluster analysis, binary relations, bijections},
series = {ISMVL '95}
}

@inproceedings{10.1145/3462244.3479920,
author = {Heitmann, Nils and Rosner, Thomas and Chakraborty, Samarjit},
title = {Mass-deployable Smartphone-based Objective Hearing Screening with Otoacoustic Emissions},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479920},
doi = {10.1145/3462244.3479920},
abstract = {Since hearing loss is one of the most widespread disabilities and can often be addressed by early detection and intervention, there is a strong interest in technologies for cost-effective and mass-deployable hearing screening. Towards this, smartphones have been used for subjective tests where a sequence of tones are played to a subject who has to appropriately respond upon hearing them. But such tests are inappropriate where, e.g., children are involved who cannot provide reliable feedback, or the test takes too long. In this paper, we investigate an alternative modality to develop an objective screening test using smartphones. It relies on how the cochlea actively distorts tones emitted into the ear. By measuring these distorted signals, it is possible to reliably deduce the subject’s hearing health. But smartphones are not designed to detect such low signals, and the suitability of a phone depends on the signal processing characteristics of the phone’s hardware. In this paper we investigate this issue in detail and conclude that some smartphones are suitable for objective screening tests that require no interaction with a subject. This opens up new screening options that were not available before and have immense societal implications in developing countries.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {653–661},
numpages = {9},
keywords = {smartphones, otoacoustic emissions, hearing loss},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@inproceedings{10.1145/3357384.3358163,
author = {Huang, Chao and Shi, Baoxu and Zhang, Xuchao and Wu, Xian and Chawla, Nitesh V.},
title = {Similarity-Aware Network Embedding with Self-Paced Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358163},
doi = {10.1145/3357384.3358163},
abstract = {Network embedding, which aims to learn low-dimensional vector representations for nodes in a network, has shown promising performance for many real-world applications, such as node classification and clustering. While various embedding methods have been developed for network data, they are limited in their assumption that nodes are correlated with their neighboring nodes with the same similarity degree. As such, these methods can be suboptimal for embedding network data. In this paper, we propose a new method named SANE, short for Similarity-Aware Network Embedding, to learn node representations by explicitly considering different similarity degrees between connected nodes in a network. In particular, we develop a new framework based on self-paced learning by accounting for both the explicit relations (i.e., observed links) and implicit relations (i.e., unobserved node similarities) in network representation learning. To justify our proposed model, we perform experiments on two real-world network data. Experiments results show that SNAE outperforms state-of-the-art embedding models on the tasks of node classification and node clustering.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2113–2116},
numpages = {4},
keywords = {self-paced learning, network embedding, deep neural network},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3185066.3185088,
author = {Choi, Heejin and Kim, Jaeseok and Park, Jinuk and Kim, Juntae and Hahn, Minsoo},
title = {Low-dimensional representation of spectral envelope using deep auto-encoder for speech synthesis},
year = {2018},
isbn = {9781450363792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185066.3185088},
doi = {10.1145/3185066.3185088},
abstract = {This paper proposes a deep auto-encoder structure to extract robust spectral features for statistical parametric speech synthesis. The technique allows us to compress the low-dimensional features from high dimensional spectral envelope without degradation for full-band speech in a data-driven way. We carried out a subjective evaluation and found that the optimum auto-encoder architecture. Experimental results showed that an analysis-by-synthesis using the proposed auto-encoder has lower reconstruction error of spectral envelope than conventional mel-cepstral analysis in narrow-band as well as full-band. Our results confirm that the proposed method increases the quality of synthesized speech in text-to-speech experiments.},
booktitle = {Proceedings of the 2018 2nd International Conference on Mechatronics Systems and Control Engineering},
pages = {107–111},
numpages = {5},
keywords = {Vocoder, Statistical parametric speech synthesis, Spectral envelope, Deep auto-encoder},
location = {Amsterdam, Netherlands},
series = {ICMSCE 2018}
}

@inproceedings{10.1145/3326937.3341253,
author = {Ye, Hui and Ma, Xiaopeng and Pan, Qingfeng and Fang, Huaqiang and Xiang, Hang and Shao, Tongzhen},
title = {An adaptive approach for anomaly detector selection and fine-tuning in time series},
year = {2019},
isbn = {9781450367837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326937.3341253},
doi = {10.1145/3326937.3341253},
abstract = {The anomaly detection of time series is a hotspot of time series data mining. The own characteristics of different anomaly detectors determine the abnormal data that they are good at. There is no detector can be optimizing in all types of anomalies. Moreover, it still has difficulties in industrial production due to problems such as a single detector can't be optimized at different time windows of the same time series. This paper proposes an adaptive model based on time series characteristics and selecting appropriate detector and run-time parameters for anomaly detection, which is called ATSDLN(Adaptive Time Series Detector Learning Network). We take the time series as the input of the model, and learn the time series representation through FCN. In order to realize the adaptive selection of detectors and run-time parameters according to the input time series, the outputs of FCN are the inputs of two sub-networks: the detector selection network and the run-time parameters selection network. In addition, the way that the variable layer width design of the parameter selection sub-network and the introduction of transfer learning make the model be with more expandability. Through experiments, it is found that ATSDLN can select appropriate anomaly detector and run-time parameters, and have strong expandability, which can quickly transfer. We investigate the performance of ATSDLN in public data sets, our methods outperform other methods in most cases with higher effect and better adaptation. We also show experimental results on public data sets to demonstrate how model structure and transfer learning affect the effectiveness.},
booktitle = {Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data},
articleno = {4},
numpages = {7},
keywords = {transfer learning, time series, self-adaption, joint learning network, anomaly detection},
location = {Anchorage, Alaska},
series = {DLP-KDD '19}
}

@inproceedings{10.1145/3269206.3271732,
author = {Gaur, Manas and Kursuncu, Ugur and Alambo, Amanuel and Sheth, Amit and Daniulaityte, Raminta and Thirunarayan, Krishnaprasad and Pathak, Jyotishman},
title = {"Let Me Tell You About Your Mental Health!": Contextualized Classification of Reddit Posts to DSM-5 for Web-based Intervention},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271732},
doi = {10.1145/3269206.3271732},
abstract = {Social media platforms are increasingly being used to share and seek advice on mental health issues. In particular, Reddit users freely discuss such issues on various subreddits, whose structure and content can be leveraged to formally interpret and relate subreddits and their posts in terms of mental health diagnostic categories. There is prior research on the extraction of mental health-related information, including symptoms, diagnosis, and treatments from social media; however, our approach can additionally provide actionable information to clinicians about the mental health of a patient in diagnostic terms for web-based intervention. Specifically, we provide a detailed analysis of the nature of subreddit content from domain expert's perspective and introduce a novel approach to map each subreddit to the best matching DSM-5 (Diagnostic and Statistical Manual of Mental Disorders - 5th Edition) category using multi-class classifier. Our classification algorithm analyzes all the posts of a subreddit by adapting topic modeling and word-embedding techniques, and utilizing curated medical knowledge bases to quantify relationship to DSM-5 categories. Our semantic encoding-decoding optimization approach reduces the false-alarm-rate from 30% to 2.5% over a comparable heuristic baseline, and our mapping results have been verified by domain experts achieving a kappa score of 0.84.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {753–762},
numpages = {10},
keywords = {semantic social computing, semantic encoding and decoding, reddit, mental health, medical knowledge bases, dsm-5, drug abuse ontology},
location = {Torino, Italy},
series = {CIKM '18}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3102304.3102321,
author = {Jiang, Jie and Pozza, Riccardo and Gunnarsd\'{o}ttir, Kristr\'{u}n and Gilbert, Nigel and Moessner, Klaus},
title = {Recognising Activities at Home: Digital and Human Sensors},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102321},
doi = {10.1145/3102304.3102321},
abstract = {What activities take place at home? When do they occur, for how long do they last and who is involved? Asking such questions is important in social research on households, e.g., to study energy-related practices, assisted living arrangements and various aspects of family and home life. Common ways of seeking the answers rest on self-reporting which is provoked by researchers (interviews, questionnaires, surveys) or non-provoked (time use diaries). Longitudinal observations are also common, but all of these methods are expensive and time-consuming for both the participants and the researchers. The advances of digital sensors may provide an alternative. For example, temperature, humidity and light sensors report on the physical environment where activities occur, while energy monitors report information on the electrical devices that are used to assist the activities. Using sensor-generated data for the purposes of activity recognition is potentially a very powerful means to study activities at home. However, how can we quantify the agreement between what we detect in sensor-generated data and what we know from self-reported data, especially non-provoked data? To give a partial answer, we conduct a trial in a household in which we collect data from a suite of sensors, as well as from a time use diary completed by one of the two occupants. For activity recognition using sensor-generated data, we investigate the application of mean shift clustering and change points detection for constructing features that are used to train a Hidden Markov Model. Furthermore, we propose a method for agreement evaluation between the activities detected in the sensor data and that reported by the participants based on the Levenshtein distance. Finally, we analyse the use of different features for recognising different types of activities.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {17},
numpages = {11},
keywords = {Time use diaries, Time series, Social research, Sensors, Internet of things, Activity recognition},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3412841.3442029,
author = {Ferreira, Fabio and Silva, Luciana Lourdes and Valente, Marco Tulio},
title = {Software engineering meets deep learning: a mapping study},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442029},
doi = {10.1145/3412841.3442029},
abstract = {Deep Learning (DL) is being used nowadays in many traditional Software Engineering (SE) problems and tasks. However, since the renaissance of DL techniques is still very recent, we lack works that summarize and condense the most recent and relevant research conducted at the intersection of DL and SE. Therefore, in this paper, we describe the first results of a mapping study covering 81 papers about DL &amp; SE. Our results confirm that DL is gaining momentum among SE researchers over the years and that the top-3 research problems tackled by the analyzed papers are documentation, defect prediction, and testing.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1542–1549},
numpages = {8},
keywords = {deep learning, software engineering},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/1835804.1835940,
author = {Zhang, Jianwen and Song, Yangqiu and Zhang, Changshui and Liu, Shixia},
title = {Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835940},
doi = {10.1145/1835804.1835940},
abstract = {Mining cluster evolution from multiple correlated time-varying text corpora is important in exploratory text analytics. In this paper, we propose an approach called evolutionary hierarchical Dirichlet processes (EvoHDP) to discover interesting cluster evolution patterns from such text data. We formulate the EvoHDP as a series of hierarchical Dirichlet processes~(HDP) by adding time dependencies to the adjacent epochs, and propose a cascaded Gibbs sampling scheme to infer the model. This approach can discover different evolving patterns of clusters, including emergence, disappearance, evolution within a corpus and across different corpora. Experiments over synthetic and real-world multiple correlated time-varying data sets illustrate the effectiveness of EvoHDP on discovering cluster evolution patterns.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1079–1088},
numpages = {10},
keywords = {multiple correlated time-varying corpora, mixture models, dirichlet processes, clustering, bayesian nonparametric methods},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2695664.2695907,
author = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Han\^{e}ne},
title = {Implementation and evaluation of an approach for extracting feature models from documented UML use case diagrams},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695907},
doi = {10.1145/2695664.2695907},
abstract = {Software product lines (SPL) aim at facing the increasing costs of software products by reusing core assets of existing products in a given domain. They are often described using feature models which, as we proposed in a previous work, can be built from possibly incomplete, documented UML use case diagrams assets using the Formal Concept Analysis method, semantic model and trigger model. In order to evaluate this approach, we present in this paper the UC2FM-tool which automates all its steps. In addition, we report on a comparison of the values of quality metrics of feature models produced by our approach with those of existing feature models built by experts for five different domains.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1602–1609},
numpages = {8},
keywords = {software product lines, measurement, feature model, evaluation},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1145/3423209,
author = {Tran, Tung and Kavuluru, Ramakanth and Kilicoglu, Halil},
title = {Attention-Gated Graph Convolutions for Extracting Drug Interaction Information from Drug Labels},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3423209},
doi = {10.1145/3423209},
abstract = {Preventable adverse events as a result of medical errors present a growing concern in the healthcare system. As drug-drug interactions (DDIs) may lead to preventable adverse events, being able to extract DDIs from drug labels into a machine-processable form is an important step toward effective dissemination of drug safety information. Herein, we tackle the problem of jointly extracting mentions of drugs and their interactions, including interaction outcome, from drug labels. Our deep learning approach entails composing various intermediate representations, including graph-based context derived using graph convolutions (GCs) with a novel attention-based gating mechanism (holistically called GCA), which are combined in meaningful ways to predict on all subtasks jointly. Our model is trained and evaluated on the 2018 TAC DDI corpus. Our GCA model in conjunction with transfer learning performs at 39.20% F1 and 26.09% F1 on entity recognition (ER) and relation extraction (RE), respectively, on the first official test set and at 45.30% F1 and 27.87% F1 on ER and RE, respectively, on the second official test set. These updated results lead to improvements over our prior best by up to 6 absolute F1 points. After controlling for available training data, the proposed model exhibits state-of-the-art performance for this task.},
journal = {ACM Trans. Comput. Healthcare},
month = jan,
articleno = {10},
numpages = {19},
keywords = {relation extraction, multi-task learning, drug-drug interactions, Neural networks}
}

@article{10.1016/j.specom.2019.10.002,
author = {Howson, Phil J. and Monahan, Philip J.},
title = {Perceptual motivation for rhotics as a class},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.002},
doi = {10.1016/j.specom.2019.10.002},
journal = {Speech Commun.},
month = dec,
pages = {15–28},
numpages = {14},
keywords = {Classes, Natural, Rhotics, Rhotic Typology, Phonetics-phonology, Speech Perception}
}

@article{10.1016/j.cad.2010.02.004,
author = {Langerak, Thomas R.},
title = {Local parameterization of freeform shapes using freeform feature recognition},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {42},
number = {8},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2010.02.004},
doi = {10.1016/j.cad.2010.02.004},
abstract = {Form feature modeling is a much used shape modeling technique that offers high-level control over a shape. When a feature-based interpretation of shape data is not available, e.g. when a shape is obtained by a laser range scanner or from a database of shapes, then the features must be reconstructed through feature recognition. Many methods for the recognition of machining features exist, but these methods cannot be used for freeform feature recognition, of which the complexity is much larger. In this paper, a new freeform feature recognition method is presented that is based on a new definition of the freeform feature concept. The method uses a three-step approach to feature recognition, in which first the global shape of a feature is matched to the target shape model. In a second step, this global shape is locally adapted to the target shape by adapting the definition of the feature. Finally, if the desired configuration of the feature has been determined, it can be used to reconstruct the target's shape. In the first two steps, an evolutionary approach is taken to maximizing the similarity between the feature and the target shape. Finally, the target shape is reconstructed to incorporate the recognized feature. An extensive application example is given and the method is validated by applying it to a large number of artificially created test cases.},
journal = {Comput. Aided Des.},
month = aug,
pages = {682–692},
numpages = {11},
keywords = {Shape reuse, Parameterization, Freeform feature, Feature recognition, Evolutionary computation}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@article{10.1109/TNET.2021.3056772,
author = {Ruby, Rukhsana and Zhong, Shuxin and ElHalawany, Basem M. and Luo, Hanjiang and Wu, Kaishun},
title = {SDN-Enabled Energy-Aware Routing in Underwater Multi-Modal Communication Networks},
year = {2021},
issue_date = {June 2021},
publisher = {IEEE Press},
volume = {29},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3056772},
doi = {10.1109/TNET.2021.3056772},
abstract = {Despite extensive research efforts, underwater sensor networks (UWSNs) still suffer from serious performance issues due to their inefficient and uncoordinated channel access and resource management. For example, due to the lack of holistic knowledge on the network resources, existing decentralized routing protocols fail to provide globally optimal performance. On the other hand, Software Defined Networking (SDN), as a promising paradigm to provide prominent centralized solutions, can be employed to address the aforementioned issues in UWSNs. Indeed, SDN brings unprecedented opportunities to improve the network performance through the development of advanced algorithms at controllers. In this paper, we study the routing problem in such a network with new features including centralized route decision, global network-state awareness, seamless route discovery while considering the optimization of several long-term global performance metrics. We formulate the entire routing problem of a multi-modal UWSN as an optimization problem while considering the interference phenomenon of ad hoc scenarios and some long-term global performance metrics of an ideal routing protocol. Our formulated problem nicely captures all possible flexibilities of a sensor node no matter it has the full-duplex or half-duplex functionality. Upon the formulation, we recognize the NP-hard nature of the problem for all possible scenarios. We adopt a rounding technique based on the convex programming relaxation concept to solve the formulated routing problem that considers full-duplex scenarios, whereas we solve the problem for half-duplex scenarios using a greedy method upon interpreting it as a submodular function maximization problem. Through extensive simulation via our Python-based in-house simulator, we verify that our proposed globally optimal routing scheme always outperforms three existing decentralized routing protocols (each of these protocols are selected from each of three prominent protocol types, i.e., flooding, cross-layer information and adaptive machine learning based, respectively) in terms of reliability, latency, energy efficiency, lifetime and fairness.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {965–978},
numpages = {14}
}

@inproceedings{10.1145/3240508.3240681,
author = {Liu, Lingbo and Zhang, Ruimao and Peng, Jiefeng and Li, Guanbin and Du, Bowen and Lin, Liang},
title = {Attentive Crowd Flow Machines},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240681},
doi = {10.1145/3240508.3240681},
abstract = {Traffic flow prediction is crucial for urban traffic management and public safety. Its key challenges lie in how to adaptively integrate the various factors that affect the flow changes. In this paper, we propose a unified neural network module to address this problem, called Attentive Crowd Flow Machine~(ACFM), which is able to infer the evolution of the crowd flow by learning dynamic representations of temporally-varying data with an attention mechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units connected with a convolutional layer for spatial weight prediction. The first LSTM takes the sequential flow density representation as input and generates a hidden state at each time-step for attention map inference, while the second LSTM aims at learning the effective spatial-temporal feature expression from attentionally weighted crowd flow features. Based on the ACFM, we further build a deep architecture with the application to citywide crowd flow prediction, which naturally incorporates the sequential and periodic data as well as other external influences. Extensive experiments on two standard benchmarks (i.e., crowd flow in Beijing and New York City) show that the proposed method achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1553–1561},
numpages = {9},
keywords = {traffic flow prediction, spatial-temporal modeling, mobility data, memory and attention neural networks},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3366423.3380074,
author = {Almerekhi, Hind and Kwak, Haewoon and Salminen, Joni and Jansen, Bernard J.},
title = {Are These Comments Triggering? Predicting Triggers of Toxicity in Online Discussions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380074},
doi = {10.1145/3366423.3380074},
abstract = {Understanding the causes or triggers of toxicity adds a new dimension to the prevention of toxic behavior in online discussions. In this research, we define toxicity triggers in online discussions as a non-toxic comment that lead to toxic replies. Then, we build a neural network-based prediction model for toxicity trigger. The prediction model incorporates text-based features and derived features from previous studies that pertain to shifts in sentiment, topic flow, and discussion context. Our findings show that triggers of toxicity contain identifiable features and that incorporating shift features with the discussion context can be detected with a ROC-AUC score of 0.87. We discuss implications for online communities and also possible further analysis of online toxicity and its root causes.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3033–3040},
numpages = {8},
keywords = {trigger detection, toxicity, online discussion, neural networks, Reddit},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1007/s11042-020-09435-y,
author = {Huang, Kuan and Sun, Tanfeng and Jiang, Xinghao and Dong, Yi and Fang, Qianan},
title = {Combined features for steganalysis against PU partition mode-based steganography in HEVC},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {41–42},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09435-y},
doi = {10.1007/s11042-020-09435-y},
abstract = {Many innovation modules introduced by High Efficiency Video Coding (HEVC) are remaining unexplored in the steganography domain. In this paper, a novel steganalytic approach is proposed against the PU partition mode-based steganographic methods which makes use of some innovative features in HEVC. Firstly, the influence of multilevel information embedding on the group proportion and the group proportion difference before and after recompression is analyzed. Then the statistical distribution of these two aspects are modeled as two different feature sets, and these two feature sets are combined for the final feature design to generate a 24-dimensional classification feature. Finally, a feature optimization is applied to reduce the 24-dimensional steganalysis feature to a 6-dimensional feature. It is demonstrated in experiment results that the proposed features have achieved a more accurate detection rate than current steganalysis methods under various circumstances, especially in the low embedding level situation.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {31147–31164},
numpages = {18},
keywords = {Information hiding, Steganography, PU partition mode, Video steganalysis}
}

@article{10.1016/j.eswa.2007.05.024,
author = {Berge, P. R. and Adde, L. and Espinosa, G. and Stavdahl, \O{}.},
title = {ENIGMA - Enhanced interactive general movement assessment},
year = {2008},
issue_date = {May, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.05.024},
doi = {10.1016/j.eswa.2007.05.024},
abstract = {General movement assessment is an accurate clinical method for predicting severe neurological dysfunctions such as cerebral palsy in young infants. Development of a computer-based diagnosis support system based on the General Movement Assessment method is dependent on features being effectively elicited from a General Movement expert. We present ENIGMA, a software tool for General Movement knowledge elicitation and modeling. Video and motion data were collected in 15 recordings containing both normal and abnormal general movements from the fidgety period of infant development. ENIGMA shows video in synchrony with different visualized features of recorded motion data. Movement patterns are modeled through an iterative and incremental process, where the General Movement expert is guiding the modeling process through comparing movement patterns observed in video with corresponding visual patterns observed in visualized features, and giving feedback to the knowledge engineer. Three visualized features were developed for exploring the so-called fidgety movements. The interactive work procedure introduced by ENIGMA enabled explicit motion features to be defined based on unconscious expert knowledge. Normal fidgety movements were found to be partly characterized by periodic patterns. Our results demonstrate that ENIGMA is a capable tool for General Movement expert knowledge elicitation. It facilitates the modeling process and provides a basis for detailed discussions. Clinical and technical concepts are communicated well through visual notions.},
journal = {Expert Syst. Appl.},
month = may,
pages = {2664–2672},
numpages = {9},
keywords = {Visualized features, Signal visualization, General movements, Expert knowledge modeling}
}

@article{10.1155/2011/831261,
author = {Nanda, Santosh Kumar and Tripathy, Debi Prasad},
title = {Application of functional link artificial neural network for prediction of machinery noise in opencast mines},
year = {2011},
issue_date = {January 2011},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2011},
issn = {1687-7101},
url = {https://doi.org/10.1155/2011/831261},
doi = {10.1155/2011/831261},
abstract = {Functional link-based neural network models were applied to predict opencast mining machineries noise. The paper analyzes the prediction capabilities of functional link neural network based noise prediction models vis-\`{a}-vis existing statistical models. In order to find the actual noise status in opencast mines, some of the popular noise prediction models, for example, ISO-9613-2, CONCAWE, VDI, and ENM, have been applied in mining and allied industries to predict the machineries noise by considering various attenuation factors. Functional link artificial neural network (FLANN), polynomial perceptron network (PPN), and Legendre neural network (LeNN) were used to predict the machinery noise in opencast mines. The case study is based on data collected from an opencast coal mine of Orissa, India. From the present investigations, it could be concluded that the FLANN model give better noise prediction than the PPN and LeNN model.},
journal = {Adv. Fuzzy Sys.},
month = jan,
articleno = {4},
numpages = {11}
}

@inproceedings{10.5555/3172077.3172239,
author = {Murugesan, Keerthiram and Carbonell, aime},
title = {Self-paced multitask learning with shared knowledge},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {This paper introduces self-paced task selection to multitask learning, where instances from more closely related tasks are selected in a progression of easier-to-harder tasks, to emulate an effective human education strategy, but applied to multitask machine learning. We develop the mathematical foundation for the approach based on iterative selection of the most appropriate task, learning the task parameters, and updating the shared knowledge, optimizing a new bi-convex loss function. This proposed method applies quite generally, including to multitask feature learning, multitask learning with alternating structure optimization, etc. Results show that in each of the above formulations self-paced (easier-to-harder) task selection outperforms the baseline version of these methods in all the experiments.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2522–2528},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1155/2021/2260488,
author = {Lu, Ruili and Jiao, Pengfei and Wang, Yinghui and Wu, Huaming and Chen, Xue and Xiong, Fei},
title = {Layer Information Similarity Concerned Network Embedding},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1076-2787},
url = {https://doi.org/10.1155/2021/2260488},
doi = {10.1155/2021/2260488},
abstract = {Great achievements have been made in network embedding based on single-layer networks. However, there are a variety of scenarios and systems that can be presented as multiplex networks, which can reveal more interesting patterns hidden in the data compared to single-layer networks. In the field of network embedding, in order to project the multiplex network into the latent space, it is necessary to consider richer structural information among network layers. However, current methods for multiplex network embedding mostly focus on the similarity of nodes in each layer of the network, while ignoring the similarity between different layers. In this paper, for multiplex network embedding, we propose a Layer Information Similarity Concerned Network Embedding (LISCNE) model considering the similarities between layers. Firstly, we introduce the common vector for each node shared by all layers and layer vectors for each layer where common vectors obtain the overall structure of the multiplex network and layer vectors learn semantics for each layer. We get the node embeddings in each layer by concatenating the common vectors and layer vectors with the consideration that the node embedding is related not only to the surrounding neighbors but also to the overall semantics. Furthermore, we define an index to formalize the similarity between different layers and the cross-network association. Constrained by layer similarity, the layer vectors with greater similarity are closer to each other and the aligned node embedding in these layers is also closer. To evaluate our proposed model, we conduct node classification and link prediction tasks to verify the effectiveness of our model, and the results show that LISCNE can achieve better or comparable performance compared to existing baseline methods.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@article{10.1016/j.neunet.2011.11.004,
author = {McCarron, Michael and Azimi-Sadjadi, Mahmood R. and Mungiole, Michael},
title = {An operationally adaptive system for rapid acoustic transmission loss prediction},
year = {2012},
issue_date = {March, 2012},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {27},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2011.11.004},
doi = {10.1016/j.neunet.2011.11.004},
abstract = {An operationally adaptive (OA) system for prediction of acoustic transmission loss (TL) in the atmosphere is developed in this paper. This system uses expert neural network predictors, each corresponding to a specific range of source elevation. The outputs of the expert predictors are combined using a weighting mechanism and a nonlinear fusion system. Using this prediction methodology the computational intractability of traditional acoustic propagation models is eliminated. The proposed system is tested on a synthetically generated acoustic data set for a wide range of geometric, source, environmental, and operational conditions. The results show a significant improvement in both accuracy and reliability over a benchmark prediction system.},
journal = {Neural Netw.},
month = mar,
pages = {91–99},
numpages = {9},
keywords = {Fusion method, Feedforward neural network, Acoustic propagation}
}

@inproceedings{10.1007/978-3-642-35101-3_70,
author = {Budden, David and Fenn, Shannon and Walker, Josiah and Mendes, Alexandre},
title = {A novel approach to ball detection for humanoid robot soccer},
year = {2012},
isbn = {9783642351006},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35101-3_70},
doi = {10.1007/978-3-642-35101-3_70},
abstract = {The ability to accurately track a ball is a critical issue in humanoid robot soccer, made difficult by processor limitations and resultant inability to process all available data from a high-definition image. This paper proposes a computationally efficient method of determining position and size of balls in a RoboCup environment, and compares the performance to two common methods: one utilising Levenberg-Marquardt least squares circle fitting, and the other utilising a circular Hough transform. The proposed method is able to determine the position of a non-occluded tennis ball with less than 10% error at a distance of 5 meters, and a half-occluded ball with less than 20% error, overall outperforming both compared methods whilst executing 300 times faster than the circular Hough transform method. The proposed method is described fully in the context of a colour based vision system, with an explanation of how it may be implemented independent of system paradigm. An extension to allow tracking of multiple balls utilising unsupervised learning and internal cluster validation is described.},
booktitle = {Proceedings of the 25th Australasian Joint Conference on Advances in Artificial Intelligence},
pages = {827–838},
numpages = {12},
keywords = {robotic soccer, object recognition, feature extraction, computer vision, clustering, Robotics},
location = {Sydney, Australia},
series = {AI'12}
}

@inproceedings{10.1145/3129676.3129687,
author = {Mofrad, Asieh Abolpour and Yazidi, Anis and Hammer, Hugo Lewi},
title = {Solving Stochastic Point Location Problem in a Dynamic Environment with Weak Estimation},
year = {2017},
isbn = {9781450350273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129676.3129687},
doi = {10.1145/3129676.3129687},
abstract = {The Stochastic Point Location (SPL) problem introduced by Oommen [7] can be summarized as searching for an unknown point in the interval under a possibly faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic environment which in turn informs it about the direction of the search. Since the environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem which was pioneered by Oommen [7] two decades ago relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution. However, the latter strategy yields rather poor accuracy for low resolutions. In this paper, we present sophisticated tracking methods that outperform Oommen strategy [7]. Our methods revolve around tracking some key statistical properties of the underlying random walk using the family of weak estimators. Furthermore, we address the settings where the point location is non-stationary, i.e. LM is searching with uncertainty for a (possibly moving) point in an interval. In such settings, asymptotic results are no longer applicable. Simulation results show that the proposed methods outperform Oommen method for estimating point location by reducing the estimated error up to 75%.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {30–35},
numpages = {6},
keywords = {Stochastic point location (SPL), Stochastic learning weak estimator (SLWE), Random walk},
location = {Krakow, Poland},
series = {RACS '17}
}

@inproceedings{10.5555/3505326.3505356,
author = {Ravari, Yaser Norouzzadeh and Spronck, Pieter and Sifa, Rafet and Drachen, Anders},
title = {Predicting victory in a hybrid online competitive game: the case of Destiny},
year = {2017},
isbn = {978-1-57735-791-9},
publisher = {AAAI Press},
abstract = {Competitive multi-player game play is a common feature in major commercial titles, and has formed the foundation for esports. In this paper, the question whether it is possible to predict match outcomes in First Person Shooter-type multiplayer competitive games with mixed genres is addressed. The case employed is Destiny, which forms a hybrid title combining Massively Multi-player Online Role-Playing game features and First-Person Shooter games. Destiny provides the opportunity to investigate prediction of the match outcome, as well as the influence of performance metrics on the match results in a hybrid multi-player major commercial title. Two groups of models are presented for predicting match results: One group predicts match results for each individual game mode and the other group predicts match results in general, without considering specific game modes. Models achieve a performance between 63% and 99% in terms of average precision, with a higher performance recorded for the models trained on specific multi-player game modes, of which Destiny has several. We also analyzed performance metrics and their influence for each model. The results show that many key shooter performance metrics such as Kill/Death ratio are relevant across game modes, but also that some performance metrics are mainly important for specific competitive game modes. The results indicate that reliable match prediction is possible in FPS-type esports games.},
booktitle = {Proceedings of the Thirteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
articleno = {30},
numpages = {7},
location = {Little Cottonwood Canyon, Utah, USA},
series = {AIIDE'17}
}

@inproceedings{10.1145/2517288.2517298,
author = {Zhao, Jianyu and Wang, Peng and Huang, Kai},
title = {A semi-supervised approach for author disambiguation in KDD CUP 2013},
year = {2013},
isbn = {9781450324953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517288.2517298},
doi = {10.1145/2517288.2517298},
abstract = {Name disambiguation, which aims to identify multiple names which correspond to one person and same names which refer to different persons, is one of the most important basic problems in many areas such as natural language processing, information retrieval and digital libraries. Microsoft academic search data in KDD Cup 2013 Track 2 task brings one such challenge to the researchers in the knowledge discovery and data mining community. Besides the real-world and large-scale characteristic, the Track 2 task raises several challenges: (1) Consideration of both synonym and polysemy problems; (2) Existence of huge amount of noisy data with missing attributes; (3) Absence of labeled data that makes this challenge a cold start problem.In this paper, we describe our solution to Track 2 of KDD Cup 2013. The challenge of this track is author disambiguation, which aims at identifying whether authors are the same person by using academic publication data. We propose a multi-phase semi-supervised approach to deal with the challenge. First, we preprocess the dataset and generate features for models, then construct a coauthor-based network and employ community detection to accomplish first-phase disambiguation task, which handles the cold-start problem. Second, using results in first phase, we use support vector machine and various other models to utilize noisy data with missing attributes in the dataset. Further, we propose a self-taught procedure to solve ambiguity in coauthor information, boosting performance of results from other models. Finally, by blending results from different models, we finally achieves 6th place with 0.98717 mean F-score on public leaderboard and 7th place with 0.98651 mean F-score on private leaderboard.},
booktitle = {Proceedings of the 2013 KDD Cup 2013 Workshop},
articleno = {10},
numpages = {8},
keywords = {support vector machine, self-taught model, community detection, cold start problem, author disambiguation},
location = {Chicago, Illinois},
series = {KDD Cup '13}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Personalized recommendations, Recommender systems, Product-line configuration, Feature model, Product lines}
}

@article{10.1016/j.cmpb.2011.07.015,
author = {Cabezas, Mariano and Oliver, Arnau and Llad\'{o}, Xavier and Freixenet, Jordi and Bach Cuadra, Meritxell},
title = {A review of atlas-based segmentation for magnetic resonance brain images},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {104},
number = {3},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2011.07.015},
doi = {10.1016/j.cmpb.2011.07.015},
abstract = {Abstract: Normal and abnormal brains can be segmented by registering the target image with an atlas. Here, an atlas is defined as the combination of an intensity image (template) and its segmented image (the atlas labels). After registering the atlas template and the target image, the atlas labels are propagated to the target image. We define this process as atlas-based segmentation. In recent years, researchers have investigated registration algorithms to match atlases to query subjects and also strategies for atlas construction. In this paper we present a review of the automated approaches for atlas-based segmentation of magnetic resonance brain images. We aim to point out the strengths and weaknesses of atlas-based methods and suggest new research directions. We use two different criteria to present the methods. First, we refer to the algorithms according to their atlas-based strategy: label propagation, multi-atlas methods, and probabilistic techniques. Subsequently, we classify the methods according to their medical target: the brain and its internal structures, tissue segmentation in healthy subjects, tissue segmentation in fetus, neonates and elderly subjects, and segmentation of damaged brains. A quantitative comparison of the results reported in the literature is also presented.},
journal = {Comput. Methods Prog. Biomed.},
month = dec,
pages = {e158–e177},
numpages = {20},
keywords = {Segmentation, Magnetic resonance imaging, Brain, Automated methods, Atlas}
}

@inproceedings{10.5555/795696.798219,
author = {Melone, N. P. and McGuire, T. W. and Chan, L. W. and Gerwing, T. A.},
title = {Effects of DSS, modeling, and exogenous factors on decision quality and confidence},
year = {1995},
isbn = {0818669403},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {118 undergraduate business students participated in an experiment which required that they manage production and workforce with the objective of minimizing costs using one of two forms of computer-based DSS. Using a 2/spl times/2 design, subjects were randomly assigned to one of four experimental conditions: the (1) presence or (2) absence of a modeling feature, crossed with a (3) "good" or (4) "bad" decision environment implemented as a decreasing or increasing sales trend. Results indicate that subjects with access to a modeling feature performed better than those without such capability; subjects' confidence, however, was not correlated with environmental effects or the availability of a modeling feature. We also found a strong correlation between confidence and actual decision quality for all conditions. All these results are in the opposite direction from those of a previously published study using the same task but a different DSS. Finally, we found no evidence of overconfidence, on average, in any of our conditions.},
booktitle = {Proceedings of the 28th Hawaii International Conference on System Sciences},
pages = {152},
keywords = {workforce management, undergraduate business students, statistical analysis, sales management, production management, overconfidence, modelling, modeling feature, increasing sales, human resource management, human factors, exogenous factors, environmental effects, decreasing sales, decision support systems, decision quality, decision environment, cost minimization, correlation, confidence, DSS},
series = {HICSS '95}
}

@inproceedings{10.5555/2033831.2033839,
author = {Grabczewski, Krzysztof},
title = {Separability of split value criterion with weighted separation gains},
year = {2011},
isbn = {9783642231988},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An analysis of the Separability of Split Value criterion in some particular applications has led to conclusions about possible improvements of the criterion. Here, the new formulation of the SSV criterion is presented and examined. The results obtained for 21 different benchmark datasets are presented and discussed in comparison with the most popular decision tree node splitting criteria like information gain and Gini index. Because the new SSV definition introduces a parameter, some empirical analysis of the new parameter is presented. The new criterion turned out to be very successful in decision tree induction processes.},
booktitle = {Proceedings of the 7th International Conference on Machine Learning and Data Mining in Pattern Recognition},
pages = {88–98},
numpages = {11},
keywords = {split criteria, separability, decision trees},
location = {New York, NY},
series = {MLDM'11}
}

@inproceedings{10.1145/3292500.3330728,
author = {Ma, Xiaoyang and Zhang, Lan and Xu, Lan and Liu, Zhicheng and Chen, Ge and Xiao, Zhili and Wang, Yang and Wu, Zhengtao},
title = {Large-scale User Visits Understanding and Forecasting with Deep Spatial-Temporal Tensor Factorization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330728},
doi = {10.1145/3292500.3330728},
abstract = {Understanding and forecasting user visits is of great importance for a variety of tasks, e.g., online advertising, which is one of the most profitable business models for Internet services. Publishers sell advertising spaces in advance with user visit volume and attributes guarantees. There are usually tens of thousands of attribute combinations in an online advertising system. The key problem is how to accurately forecast the number of user visits for each attribute combination. Many traditional work characterizing temporal trends of every single time series are quite inefficient for large-scale time series. Recently, a number of models based on deep learning or matrix factorization have been proposed for high-dimensional time series forecasting. However, most of them neglect correlations among attribute combinations, or are tailored for specific applications, resulting in poor adaptability for different business scenarios.Besides, sophisticated deep learning models usually cause high time and space complexity. There is still a lack of an efficient highly scalable and adaptable solution for accurate high-dimensional time series forecasting. To address this issue, in this work, we conduct a thorough analysis on large-scale user visits data and propose a novel deep spatial-temporal tensor factorization framework, which provides a general design for high-dimensional time series forecasting. We deployed the proposed framework in Tencent online guaranteed delivery advertising system, and extensively evaluated the effectiveness and efficiency of the framework in two different large-scale application scenarios. The results show that our framework outperforms existing methods in prediction accuracy. Meanwhile, it significantly reduces the parameter number and is resistant to incomplete data with up to 20% missing values.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2403–2411},
numpages = {9},
keywords = {user visits forecasting, high-dimensional time series forecasting, guaranteed delivery advertising},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/2661806.2661818,
author = {Mitra, Vikramjit and Shriberg, Elizabeth and McLaren, Mitchell and Kathol, Andreas and Richey, Colleen and Vergyri, Dimitra and Graciarena, Martin},
title = {The SRI AVEC-2014 Evaluation System},
year = {2014},
isbn = {9781450331197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661806.2661818},
doi = {10.1145/2661806.2661818},
abstract = {Though depression is a common mental health problem with significant impact on human society, it often goes undetected. We explore a diverse set of features based only on spoken audio to understand which features correlate with self-reported depression scores according to the Beck depression rating scale. These features, many of which are novel for this task, include (1) estimated articulatory trajectories during speech production, (2) acoustic characteristics, (3) acoustic-phonetic characteristics and (4) prosodic features. Features are modeled using a variety of approaches, including support vector regression, a Gaussian backend and decision trees. We report results on the AVEC-2014 depression dataset and find that individual systems range from 9.18 to 11.87 in root mean squared error (RMSE), and from 7.68 to 9.99 in mean absolute error (MAE). Initial fusion brings further improvement; fusion and feature selection work is still in progress.},
booktitle = {Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge},
pages = {93–101},
numpages = {9},
keywords = {time series prediction, support vector regression, robust signal analysis, prosody, depression, decision trees, articulatory features, acoustic features},
location = {Orlando, Florida, USA},
series = {AVEC '14}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {regularization, configurable systems, adversarial learning, Software performance prediction},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.1016/j.jss.2019.03.064,
author = {Pradhan, Dipesh and Wang, Shuai and Ali, Shaukat and Yue, Tao and Liaaen, Marius},
title = {Employing rule mining and multi-objective search for dynamic test case prioritization},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {153},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.064},
doi = {10.1016/j.jss.2019.03.064},
journal = {J. Syst. Softw.},
month = jul,
pages = {86–104},
numpages = {19},
keywords = {Black-box regression testing, Search, Dynamic test case prioritization, Rule mining, Multi-objective optimization}
}

@article{10.3233/JIFS-169433,
author = {Srivastava, Smriti and Gopal and Bhardwaj, Saurabh and Thampi, Sabu M. and El-Alfy, El-Sayed M. and Mitra, Sushmita and Trajkovic, Ljiljana},
title = {Multi-scenario dataset for speaker recognition},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {34},
number = {3},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169433},
doi = {10.3233/JIFS-169433},
abstract = {The present work describes different research techniques for collecting and organizing speech database in different scenario at the institute and successfully structuring the text independent speaker identification database in Indian context. In order to get the Multi-Scenario dataset, each speaker performed multiple sessions recording in reading style with English and Hindi language with same passages but under different conditions. This work analyzed different scenario affecting the performance of speaker recognition system when tested under dissimilar training conditions. Here four different scenarios are considered; sensor and environment, language, aging and health. To study the effect of sensor, language and environment on the performance of ASR system a database of 200 speaker was created. Under different environmental conditions, four different types of sensors in parallel configuration were used to study the sensor mismatch conditions over testing and training phase. The database contains speech samples of the individual in English and Hindi in read speech styles under two environment i.e. a controlled recording chamber and library. To study the aging effect, an aging NSIT speaker database (AG-NSIT-SD) of 53 famous personalities was collected from online source varying over a period of 10–20 years. Further to study the effect of health, a cough and cold NSIT speaker database (CC-NSIT-SD) of 38 speakers was also collected to study the performance of system. Apart from this, the effect of different noise types on the speaker identification was also studied on different sensors.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1385–1392},
numpages = {8},
keywords = {cough and cold database, aging database, speaker database, Speaker identification}
}

@inbook{10.5555/2554542.2554558,
author = {Metzler, Saskia and Nieuwenhuisen, Matthias and Behnke, Sven},
title = {Learning visual obstacle detection using color histogram features},
year = {2012},
isbn = {9783642320590},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Perception of the environment is crucial in terms of successfully playing soccer. Especially the detection of other players improves game play skills, such as obstacle avoidance and path planning. Such information can help refine reactive behavioral strategies, and is conducive to team play capabilities. Robot detection in the RoboCup Standard Platform League is particularly challenging as the Nao robots are limited in computing resources and their appearance is predominantly white in color like the field lines. This paper describes a vision-based multilevel approach which is integrated into the B-Human Software Framework and evaluated in terms of speed and accuracy. On the basis of color segmented images, a feed-forward neural network is trained to discriminate between robots and non-robots. The presented algorithm initially extracts image regions which potentially depict robots and prepares them for classification. Preparation comprises calculation of color histograms as well as linear interpolation in order to obtain network inputs of a specific size. After classification by the neural network, a position hypothesis is generated.},
booktitle = {Robot Soccer World Cup XV},
pages = {149–161},
numpages = {13}
}

@article{10.1016/j.compbiolchem.2017.01.001,
author = {Kavas, Musa and Kzldoan, Aslhan Kurt and Abanoz, Bra},
title = {Comparative genome-wide phylogenetic and expression analysis of SBP genes from potato (Solanum tuberosum)},
year = {2017},
issue_date = {April 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {67},
number = {C},
issn = {1476-9271},
url = {https://doi.org/10.1016/j.compbiolchem.2017.01.001},
doi = {10.1016/j.compbiolchem.2017.01.001},
abstract = {Display Omitted Identified 15 members of SBP in potato, that showed resemblance to Arabidopsis SBP-box genes.Found to be distributed in nine chromosomes and eight of them included miR156 and miRNA157 target sites.Expression pattern of StSBP was confirmed in the context of early flowering response. Flowering time is a very important phase in transition to reproductive stage of life in higher plants. SQUAMOSA promoter-binding protein (SBP) gene family encodes plant-specific transcription factors that are involved in regulation of several developmental processes, especially flowering. Although SBP-box genes have been identified in different plants, there have been no study indicating the regulatory effect of SBP box in potato flowering. Here, we report for the first time the identification and characterization of SBP-box transcription factors as well as determination of expression level of SBP-box genes in Solanum tuberosum L. an important crop worldwide. Fifteen different SBP-box transcription factor genes were identified in potato genome. They were found to be distributed in nine chromosomes and eight of them included miR156 and miRNA157 target sites. Characterization of amino acid sequences were performed and protein interactions were predicted. In addition, expression levels of five S. tuberosum SBP-box genes were analysed by both in silico and qRT-PCR. All these results provide a better understanding of functional role of SBP-box gene family members in flowering time in potato.},
journal = {Comput. Biol. Chem.},
month = apr,
pages = {131–140},
numpages = {10},
keywords = {Transcription factors, SBP, Potato, Flowering genes}
}

@inproceedings{10.1145/2896839.2896845,
author = {Karim, Muhammad Rezaul and Al Alam, S. M. Didar and Kabeer, Shaikh Jeeshan and Ruhe, G\"{u}nther and Baluta, Basil and Mahmud, Shafquat},
title = {Applying data analytics towards optimized issue management: an industrial case study},
year = {2016},
isbn = {9781450341547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896839.2896845},
doi = {10.1145/2896839.2896845},
abstract = {This document describes our experience of applying data analytics at Plexina, a leading IT company working in the healthcare domain. The main goal of the project was to identify factors currently affecting issue management and to make analytics based suggestions for optimizing the process. Various statistical and machine learning techniques were applied on a data set extracted from six releases of Plexina, containing more than 666 issues. Statistical techniques successfully identified the various factors that leads to estimation inaccuracy related to issues as well as identified the hidden relationships existing among various variables. The employed predictive analytic models was also successful to some extent, in predicting effort estimation related inaccuracy associated with the issues. The insights provided by the entire data analytics study can be of great help to product managers or the developers to make more informed decisions. In addition, the guidelines presented in this paper based on the lessons learnt can be applied to other data analytics and academia-industry collaboration project.},
booktitle = {Proceedings of the 4th International Workshop on Conducting Empirical Studies in Industry},
pages = {7–13},
numpages = {7},
keywords = {issue management, industry-academia collaboration, data analytics, case study},
location = {Austin, Texas},
series = {CESI '16}
}

@article{10.1016/j.knosys.2010.10.004,
author = {Li, Jingpeng and Burke, Edmund K. and Qu, Rong},
title = {Integrating neural networks and logistic regression to underpin hyper-heuristic search},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {24},
number = {2},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2010.10.004},
doi = {10.1016/j.knosys.2010.10.004},
abstract = {A hyper-heuristic often represents a heuristic search method that operates over a space of heuristic rules. It can be thought of as a high level search methodology to choose lower level heuristics. Nearly 200 papers on hyper-heuristics have recently appeared in the literature. A common theme in this body of literature is an attempt to solve the problems in hand in the following way: at each decision point, first employ the chosen heuristic(s) to generate a solution, then calculate the objective value of the solution by taking into account all the constraints involved. However, empirical studies from our previous research have revealed that, under many circumstances, there is no need to carry out this costly 2-stage determination and evaluation at all times. This is because many problems in the real world are highly constrained with the characteristic that the regions of feasible solutions are rather scattered and small. Motivated by this observation and with the aim of making the hyper-heuristic search more efficient and more effective, this paper investigates two fundamentally different data mining techniques, namely artificial neural networks and binary logistic regression. By learning from examples, these techniques are able to find global patterns hidden in large data sets and achieve the goal of appropriately classifying the data. With the trained classification rules or estimated parameters, the performance (i.e. the worth of acceptance or rejection) of a resulting solution during the hyper-heuristic search can be predicted without the need to undertake the computationally expensive 2-stage of determination and calculation. We evaluate our approaches on the solutions (i.e. the sequences of heuristic rules) generated by a graph-based hyper-heuristic proposed for exam timetabling problems. Time complexity analysis demonstrates that the neural network and the logistic regression method can speed up the search significantly. We believe that our work sheds light on the development of more advanced knowledge-based decision support systems.},
journal = {Know.-Based Syst.},
month = mar,
pages = {322–330},
numpages = {9},
keywords = {Neural network, Logistic regression, Hyper-heuristic, Educational timetabling, Data mining}
}

@article{10.1016/j.jvcir.2006.11.001,
author = {Chen, Qiang and Luo, Jian and Heng, Pheng Ann and De-shen, Xia},
title = {Fast and active texture segmentation based on orientation and local variance},
year = {2007},
issue_date = {April, 2007},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {18},
number = {2},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2006.11.001},
doi = {10.1016/j.jvcir.2006.11.001},
abstract = {This paper describes a fast and active texture segmentation approach based on the orientation and the local variance. First, a set of feature images are extracted using the orientation and the local variance. To reduce the computational complexity, a separability measurement method, which is used for selecting four feature images with good separability in four orientations, is proposed in this paper. To improve the segmentation, we adopt a nonlinear diffusion filtering to smooth the four feature images. Finally, a variational framework incorporating these features in a level set based, unsupervised segmentation process is adopted. To improve the computational speed, instead of solving the Euler-Lagrange equation, we calculate the energy, with level set representation, to solve the variational framework. Segmentation results of various synthetic and real textured images has demonstrated that our method has good performance and efficiency.},
journal = {J. Vis. Comun. Image Represent.},
month = apr,
pages = {119–129},
numpages = {11},
keywords = {Texture segmentation, Separability, Orientation and local variance, Nonlinear diffusion, Level set, Active image segmentation}
}

@article{10.1007/s10994-021-05982-z,
author = {Hanna, Josiah P. and Desai, Siddharth and Karnan, Haresh and Warnell, Garrett and Stone, Peter},
title = {Grounded action transformation for sim-to-real reinforcement learning},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {9},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-05982-z},
doi = {10.1007/s10994-021-05982-z},
abstract = {Reinforcement learning in simulation is a promising alternative to the prohibitive sample cost of reinforcement learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the target, physical system. Grounded simulation learning (gsl) is a general framework that promises to address this issue by altering the simulator to better match the real world (Farchy et al. 2013 in Proceedings of the 12th international conference on autonomous agents and multiagent systems (AAMAS)). This article introduces a new algorithm for gsl—Grounded Action Transformation (GAT)—and applies it to learning control policies for a humanoid robot. We evaluate our algorithm in controlled experiments where we show it to allow policies learned in simulation to transfer to the real world. We then apply our algorithm to learning a fast bipedal walk on a humanoid robot and demonstrate a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk. This striking empirical success notwithstanding, further empirical analysis shows that gat may struggle when the real world has stochastic state transitions. To address this limitation we generalize gat to the stochasticgat (sgat) algorithm and empirically show that sgat leads to successful real world transfer in situations where gat may fail to find a good policy. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for applying reinforcement learning to learn robot control policies entirely in simulation.},
journal = {Mach. Learn.},
month = sep,
pages = {2469–2499},
numpages = {31},
keywords = {Bipedal locomotion, Sim-to-real, Robotics, Reinforcement learning}
}

@inproceedings{10.1145/1568296.1568312,
author = {Giannone, Cristina and Basili, Roberto and Del Vescovo, Chiara and Naggar, Paolo and Moschitti, Alessandro},
title = {Kernel-based relation extraction from investigative data},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568312},
doi = {10.1145/1568296.1568312},
abstract = {In a specific process of business intelligence, i.e. investigation on organized crime, empirical language processing technologies can play a crucial role. In the data used on investigative activities, such as police interrogatory or electronic eavesdropping and wiretap, it is customary to find out expressions in non conventional languages as dialects, slangs or coded words. The recognition and storage of complex relations among subjects mentioned in these sources is a very difficult and time consuming task, ultimately based on pools of experts. We discuss here an inductive relation extraction platform that opens the way to much cheaper and consistent workflows. SVMs here are employed to produce a set of possible interpretations for domain relevant concepts. An ontology population process is here realized, where further reasoning can be applied to proof the overall consistency of the extracted information. The empirical investigation presented here shows that accurate results, comparable to the expert teams, can be achieved, and parametrization allows to fine tune the system behavior for fitting the specific domain requirements.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {93–100},
numpages = {8},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1504/IJAOSE.2011.043643,
author = {Nunes, Ingrid and Lucena, Carlos J. P. De and Cowan, Donald and Kulesza, Uir\'{a} and Alencar, Paulo and Nunes, Camila},
title = {Developing multi-agent system product lines: from requirements to code},
year = {2011},
issue_date = {November 2011},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {4},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2011.043643},
doi = {10.1504/IJAOSE.2011.043643},
abstract = {Many modern software systems have autonomous, open, context-aware and highly-interactive properties. The agent abstraction with its autonomous and pro-active characteristics and the related discipline of agent-oriented software engineering (AOSE) are promising paradigms to address these types of systems. Even though agents are frequently being adopted, little effort has been directed in AOSE methodologies toward extensive software reuse techniques, which can provide both reduced time-to-market and lower development costs. Multi-agent system product lines (MAS-PLs) are the result of the integration of AOSE with software product lines (SPLs). SPLs bring many reuse benefits to the agent domain through the exploitation of common characteristics among family members. In this context, this paper presents a domain engineering process for developing MAS-PLs. It defines activities and work products, whose purposes include supporting agent variability and providing agent feature traceability, both not addressed by current SPL and AOSE approaches.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = nov,
pages = {353–389},
numpages = {37}
}

@article{10.1016/j.neucom.2010.12.004,
author = {Hourdakis, Emmanouil and Savaki, Helen E. and Trahanias, Panos},
title = {Computational modeling of cortical pathways involved in action execution and action observation},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {7},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2010.12.004},
doi = {10.1016/j.neucom.2010.12.004},
abstract = {Brain imaging studies in macaque monkeys have recently shown that the observation and execution of specific types of grasp actions activate the same regions in the parietal, primary motor and somatosensory lobes. This extended overlapping pathway of activations provides new insights on how primates are able to learn during observation. It suggests that an observed behavior is recognized by simulating it using the circuitry developed for action execution. In the present paper we consider how learning via observation can be implemented in an artificial agent based on the above overlapping pathway of activations. We demonstrate that the circuitry developed for action execution can be activated during observation, if the agent is able to perform action association, i.e. to relate its own actions with the ones of the demonstrator. Following this intuition, a computational model of observation/execution of grasp actions is developed and used in experiments to study its properties and learning capacities. Results show that the agent is able to associate novel objects with known behaviors only by observation. Model investigation after training reveals that, during observation, not only the same regions, but also the same neural representations are activated, implying that an observed action is understood by employing the same neural codes used for its execution.},
journal = {Neurocomput.},
month = mar,
pages = {1135–1155},
numpages = {21},
keywords = {Observational learning, Grasping behaviors, Connectionist model, Computational brain modeling}
}

@inproceedings{10.5555/3504035.3504989,
author = {Ieva, Carlo and Gotlieb, Arnaud and Kaci, Souhila and Lazaar, Nadjib},
title = {Discovering program topoi through clustering},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Understanding source code of large open-source software projects is very challenging when there is only little documentation. New developers face the task of classifying a huge number of files and functions without any help. This paper documents a novel approach to this problem, called FEAT, that automatically extracts topoi from source code by using hierarchical agglomerative clustering. Program topoi summarize the main capabilities of a software system by presenting to developers clustered lists of functions together with an index of their relevant words. The clustering method used in FEAT exploits a new hybrid distance which combines both textual and structural elements automatically extracted from source code and comments. The experimental evaluation of FEAT shows that this approach is suitable to understand open-source software projects of size approaching 2,000 functions and 150 files, which opens the door for its deployment in the open-source community.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {954},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.neucom.2007.11.032,
author = {Li, Jing and Allinson, Nigel M.},
title = {A comprehensive review of current local features for computer vision},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {71},
number = {10–12},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2007.11.032},
doi = {10.1016/j.neucom.2007.11.032},
abstract = {Local features are widely utilized in a large number of applications, e.g., object categorization, image retrieval, robust matching, and robot localization. In this review, we focus on detectors and local descriptors. Both earlier corner detectors, e.g., Harris corner detector, and later region detectors, e.g., Harris affine region detector, are described in brief. Most kinds of descriptors are described and summarized in a comprehensive way. Five types of descriptors are included, which are filter-based descriptors, distribution-based descriptors, textons, derivative-based descriptors and others. Finally, the matching methods and different applications with respect to the local features are also mentioned. The objective of this review is to provide a brief introduction for new researchers to the local feature research field, so that they can follow an appropriate methodology according to their specific requirements.},
journal = {Neurocomput.},
month = jun,
pages = {1771–1787},
numpages = {17},
keywords = {Textons, Region detectors, Local features, Filter-based descriptors, Feature detectors, Feature descriptors, Distribution-based descriptors, Derivative-based descriptors, Corner detectors}
}

@inproceedings{10.1145/2906388.2906415,
author = {Roy, Nirupam and Roy Choudhury, Romit},
title = {Listening through a Vibration Motor},
year = {2016},
isbn = {9781450342698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2906388.2906415},
doi = {10.1145/2906388.2906415},
abstract = {This paper demonstrates the feasibility of using the vibration motor in mobile devices as a sound sensor, almost like a microphone. We show that the vibrating mass inside the motor -- designed to oscillate to changing magnetic fields -- also responds to air vibrations from nearby sounds. With appropriate processing, the responses become intelligible, to the extent that humans can understand the vibra-motor recorded words with greater than 80% average accuracy. Even off-the-shelf speech recognition softwares are able to decode at 60% accuracy, without any training or machine learning. We present our overall techniques and results through a system called VibraPhone, and discuss implications to both sensing and security.},
booktitle = {Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {57–69},
numpages = {13},
keywords = {vibratory communication, vibration motor, vibration, vibra-motor, speech processing, speech, sound recording, smartphone security, smartphone privacy, smartphone, ripple, nirupam roy, communication through physical vibration},
location = {Singapore, Singapore},
series = {MobiSys '16}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {performance assessment, multobjective optimisation, multi-criteria optimisation, metric, metaheuristic, measure, indicator, heuristic, exact method, evolutionary algorithms, Quality evaluation}
}

@article{10.1109/TASLP.2014.2377581,
author = {Nielsen, Jens Brehm Bagger and Nielsen, Jakob and Larsen, Jan},
title = {Perception-based personalization of hearing aids using Gaussian processes and active learning},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2377581},
doi = {10.1109/TASLP.2014.2377581},
abstract = {Personalization of multi-parameter hearing aids involves an initial fitting followed by a manual knowledge-based trial-and-error fine-tuning from ambiguous verbal user feedback. The result is an often suboptimal HA setting whereby the full potential of modern hearing aids is not utilized. This article proposes an interactive hearing-aid personalization system that obtains an optimal individual setting of the hearing aids from direct perceptual user feedback. Results obtained with ten hearing-impaired subjects show that ten to twenty pairwise user assessments between different settings--equivalent to 5-10 min--is sufficient for personalization of up to four hearing-aid parameters. A setting obtained by the system was significantly preferred by the subject over the initial fitting, and the obtained setting could be reproduced with reasonable precision. The system may have potential for clinical usage to assist both the hearing-care professional and the user.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {162–173},
numpages = {12},
keywords = {personalization, pairwise comparisons, individualization, hearing aids, active learning, Gaussian process (GP)}
}

@article{10.1109/TASLP.2018.2860786,
author = {Salehi, Haniyeh and Suelzle, David and Folkeard, Paula and Parsa, Vijay},
title = {Learning-Based Reference-Free Speech Quality Measures for Hearing Aid Applications},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860786},
doi = {10.1109/TASLP.2018.2860786},
abstract = {Objective measures of speech quality are highly desirable in benchmarking and monitoring the performance of hearing aids HAs. Existing HA speech quality indices such as the hearing aid speech quality index HASQI are intrusive in that they require a properly time-aligned and frequency-shaped reference signal to predict the quality of HA output. Two new reference-free HA speech quality indices are proposed in this paper, based on a model that amalgamates perceptual linear prediction PLP, hearing loss HL modeling, and machine learning concepts. For the first index, HL-modified PLP coefficients and their statistics were used as the feature set, which was subsequently mapped to the predicted quality scores using support vector regression SVR. For the second index, HL-impacted gammatone auditory filterbank energies and their second-order statistics constituted the feature set, which was again mapped using SVR. Two databases involving HA recordings were collected and utilized for the evaluation of the robustness and generalizability of the two indices. Experimental results showed that the index based on the gammatone filterbank energies not only correlated well with HA quality ratings by hearing impaired listeners, but also exhibited robust performance across different test conditions and was comparable to the full-reference HASQI performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2277–2288},
numpages = {12}
}

@inproceedings{10.1145/2491411.2491432,
author = {Lohar, Sugandha and Amornborvornwong, Sorawit and Zisman, Andrea and Cleland-Huang, Jane},
title = {Improving trace accuracy through data-driven configuration and composition of tracing features},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491432},
doi = {10.1145/2491411.2491432},
abstract = {Software traceability is a sought-after, yet often elusive quality in large software-intensive systems primarily because the cost and effort of tracing can be overwhelming. State-of-the art solutions address this problem through utilizing trace retrieval techniques to automate the process of creating and maintaining trace links. However, there is no simple one- size-fits all solution to trace retrieval. As this paper will show, finding the right combination of tracing techniques can lead to significant improvements in the quality of generated links. We present a novel approach to trace retrieval in which the underlying infrastructure is configured at runtime to optimize trace quality. We utilize a machine-learning approach to search for the best configuration given an initial training set of validated trace links, a set of available tracing techniques specified in a feature model, and an architecture capable of instantiating all valid configurations of features. We evaluate our approach through a series of experiments using project data from the transportation, healthcare, and space exploration domains, and discuss its implementation in an industrial environment. Finally, we show how our approach can create a robust baseline against which new tracing techniques can be evaluated.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {378–388},
numpages = {11},
keywords = {Trace retrieval, configuration, trace configuration},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.5555/3294996.3295020,
author = {Yurochkin, Mikhail and Nguyen, Xuan Long and Vasiloglou, Nikolaos},
title = {Multi-way interacting regression via factorization machines},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting.1},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {2595–2603},
numpages = {9},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{10.5555/1981848.1981873,
author = {Zhang, W. Y. and Zhang, S.},
title = {A semantic grid service architecture for computer supported cooperative work in multidisciplinary design},
year = {2006},
isbn = {9608457475},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Although the current computer-aided design (CAD) and computer network technologies have laid the foundation for the emerging fields of computer supported cooperative work (CSCW) in multidisciplinary design, the heterogeneity of multidisciplinary design knowledge representation is still a major obstacle to sharing and exchanging multidisciplinary design knowledge for multidisciplinary collaborative design. To overcome this, an ontology-based service-oriented modeling approach is presented in this paper for distributed management of multidisciplinary design knowledge in the Semantic Grid, enabling to add semantics to grid services to endow them with semantic capabilities needed for their flexible deployment and reuse in multidisciplinary collaborative design. A service-oriented multi-agent system architecture is laid out in the Semantic Grid to address, in an open, loosely coupled and integrated manner, the life cycle of multidisciplinary collaborative design activities involved in publishing, discovering and reusing various grid services.},
booktitle = {Proceedings of the 10th WSEAS International Conference on Computers},
pages = {132–137},
numpages = {6},
keywords = {CSCW, collaborative design, knowledge management, multidisciplinary design, ontology, semantic grid, service-oriented modeling},
location = {Athens, Greece},
series = {ICCOMP'06}
}

@inproceedings{10.1145/2539150.2539194,
author = {Wani, Mudassir and Alrubaian, Majed A. and Abulaish, Muhammad},
title = {A User-Centric Feature Identification and Modeling Approach to Infer Social Ties in OSNs},
year = {2013},
isbn = {9781450321136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2539150.2539194},
doi = {10.1145/2539150.2539194},
abstract = {This paper aims to identify user-centric features to calculate the strength of social ties between Online Social Network (OSN) users, and models the same using Latent Space Model (LSM). The modeling approach processes a socio-centric user-set as the users are directly (friend) or indirectly (friend-of-friend) related to a seed (target) user, which makes it easier to identify social ties between users as compared to random sampling from a set of diverse OSN users. For a given user, interaction data up to two levels is modeled and analyzed to generate a user-centric social network. Eleven different features related to Facebook have been identified to calculate the strength of social ties between users. LSM is used to visualize relationships in user-centric historical data and to estimate the probability of social ties between OSN users. The users are plotted using LSM in a three-dimensional (3D) social space around a seed user, and a link probability function is devised to calculate the probability of link between any two users with respect to the persona of the seed user. A sphere of influence around each user demarcating its active influence area is also identified and discussed in this paper.},
booktitle = {Proceedings of International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {107–114},
numpages = {8},
keywords = {Link prediction, Social network analysis, feature identification, social network modeling},
location = {Vienna, Austria},
series = {IIWAS '13}
}

@article{10.1155/ASP.2005.2136,
author = {Assaleh, Khaled and Al-Rousan, M.},
title = {Recognition of Arabic sign language alphabet using polynomial classifiers},
year = {2005},
issue_date = {1 January 2005},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2005},
issn = {1110-8657},
url = {https://doi.org/10.1155/ASP.2005.2136},
doi = {10.1155/ASP.2005.2136},
abstract = {Building an accurate automatic sign language recognition system is of great importance in facilitating efficient communication with deaf people. In this paper, we propose the use of polynomial classifiers as a classification engine for the recognition of Arabic sign language (ArSL) alphabet. Polynomial classifiers have several advantages over other classifiers in that they do not require iterative training, and that they are highly computationally scalable with the number of classes. Based on polynomial classifiers, we have built an ArSL system and measured its performance using real ArSL data collected from deaf people. We show that the proposed system provides superior recognition results when compared with previously published results using ANFIS-based classification on the same dataset and feature extraction methodology. The comparison is shown in terms of the number of misclassified test patterns. The reduction in the rate of misclassified patterns was very significant. In particular, we have achieved a 36% reduction of misclassifications on the training data and 57% on the test data.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {2136–2145},
numpages = {10},
keywords = {Arabic sign language, adaptive neuro-fuzzy inference systems, feature extraction, hand gestures, polynomial classifiers}
}

@inproceedings{10.5555/2022767.2022790,
author = {Vityaev, Evgenii and Smerdov, Stanislav},
title = {On the problem of prediction},
year = {2007},
isbn = {9783642221392},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We consider predictionsprovided by Inductive-Statistical (I-S). inference. It was noted by Hempel that I-S inference is statistically ambiguous. To avoid this problem Hempel introduced the Requirement of Maximal Specificity (RMS). We define the formal notion of RMS in terms of probabilistic logic, and maximally specific rules (MS-rules), i. e. rules satisfying RMS. Then we prove that any set of MS-rules draws no contradictions in I-S inference, therefore predictions based on MS-rules avoid statistical ambiguity. I-S inference may be used for predictions in knowledge bases or expert systems. In the last we need to calculate the probabilistic estimations for predictions. Though one may use existing probabilistic logics or "quantitative deductions" to obtain these estimations, instead we define a semantic probabilistic inference and prove that it approximates logical inference in some sense. We also developed a program system 'Discovery' which realizes this inference and was successfully applied to the solution of many practical tasks.},
booktitle = {Proceedings of the First International Conference on Knowledge Processing and Data Analysis},
pages = {280–296},
numpages = {17},
keywords = {machine learning, probabilistic logic programming, probability and logic synthesis, scientific discovery},
location = {Novosibirsk, Russia},
series = {KONT'07/KPP'07}
}

@inproceedings{10.1145/3178876.3186131,
author = {Ding, Daizong and Zhang, Mi and Pan, Xudong and Wu, Duocai and Pu, Pearl},
title = {Geographical Feature Extraction for Entities in Location-based Social Networks},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186131},
doi = {10.1145/3178876.3186131},
abstract = {Location-based embedding is a fundamental problem to solve in location-based social networks (LBSN). In this paper, we propose a geographical convolutional neural tensor network (GeoCNTN) as a generic embedding model. GeoCNTN first takes the raw location data and extracts from it a more well-conditioned representation by our proposed Geo-CMeans algorithm. We then use a convolutional neural network (CNN) and an embedding structure to extract individual latent structural patterns from the preprocessed data. Finally, we apply a neural tensor network (NTN) to craft the implicitly related features we have obtained into a unified geographical feature. The advantages of our GeoCNTN mainly come from its novel neural network structure, which intrinsically offers a mechanism to extract latent structural features from the geographical data, as well as its wide applicability in various LBSN-related tasks. From two case studies, i.e. link prediction and entity classification in user-group LBSN, we evaluate the embedding efficacy of our model. Results show that GeoCNTN significantly performs better on at least two tasks, with improvement by 9% w.r.t. NDCG and 11% w.r.t. F1 score respectively, using the Meetup-USA dataset.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {833–842},
numpages = {10},
keywords = {deep learning, feature embedding, location-based social networks},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.5555/648033.744208,
author = {Muthig, Dirk and Patzke, Thomas},
title = {Generic Implementation of Product Line Components},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An argument pro component-based software development is the idea of constructing software systems by assembling preexisting components instead of redeveloping similar or identical functionality always from scratch. Unfortunately, integrating existing components practically means adaptation and use rather than use only, which makes an ideal component-based development hard to realize in practice. Product line engineering, however, tackles this problem by making components as generic as needed for a particular product family and thus allows component reuse. Such a component covers variabilities and thus its implementation must consider variabilities as well.In this paper, we describe a process for implementing generic product line components and give an overview of variability mechanisms at the implementation level, illustrated by a running example, a generic test component.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {313–329},
numpages = {17},
series = {NODe '02}
}

@article{10.1016/j.cie.2019.03.051,
author = {B\'{a}ez, Sarah\'{\i} and Angel-Bello, Francisco and Alvarez, Ada and Meli\'{a}n-Batista, Bel\'{e}n},
title = {A hybrid metaheuristic algorithm for a parallel machine scheduling problem with dependent setup times},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.03.051},
doi = {10.1016/j.cie.2019.03.051},
journal = {Comput. Ind. Eng.},
month = may,
pages = {295–305},
numpages = {11},
keywords = {Parallel machine scheduling, Total completion time, Setup time, GRASP, Variable neighborhood search}
}

@inproceedings{10.5555/2986459.2986604,
author = {Lou, Xinghua and Hamprecht, Fred A.},
title = {Structured learning for cell tracking},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology. Reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research. Existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search. We propose a structured learning approach that allows to learn optimum parameters automatically from a training set. This allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
pages = {1296–1304},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/3327345.3327491,
author = {Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff A.},
title = {Diverse ensemble evolution: curriculum data-model marriage},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study a new method "Diverse Ensemble Evolution (DivE2)" to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model's current expertise and an intra- and inter-model diversity reward. DivE2 schedules, over the course of training epochs, the relative importance of these characteristics; it starts by selecting easy samples for each model, and then gradually adjusts towards the models having specialized and complementary expertise on subsets of the training data, thereby encouraging high accuracy of the ensemble. We utilize an intra-model diversity term on data assigned to each model, and an inter-model diversity term on data assigned to pairs of models, to penalize both within-model and cross-model redundancy. We formulate the data-model marriage problem as a generalized bipartite matching, represented as submodular maximization subject to two matroid constraints. DivE2 solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data-model marriage while the continuous part updates model parameters based on the assignments. In experiments, DivE2 outperforms other ensemble training methods under a variety of model aggregation techniques, while also maintaining competitive efficiency.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5909–5920},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{10.1162/COLI_a_00182,
author = {Riezler, Stefan},
title = {On the problem of theoretical terms in empirical computational linguistics},
year = {2014},
issue_date = {March 2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {40},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00182},
doi = {10.1162/COLI_a_00182},
abstract = {Philosophy of science has pointed out a problem of theoretical terms in empirical sciences. This problem arises if all known measuring procedures for a quantity of a theory presuppose the validity of this very theory, because then statements containing theoretical terms are circular. We argue that a similar circularity can happen in empirical computational linguistics, especially in cases where data are manually annotated by experts. We define a criterion of T-non-theoretical grounding as guidance to avoid such circularities, and exemplify how this criterion can be met by crowdsourcing, by task-related data annotation, or by data in the wild. We argue that this criterion should be considered as a necessary condition for an empirical science, in addition to measures for reliability of data annotation.},
journal = {Comput. Linguist.},
month = mar,
pages = {235–245},
numpages = {11}
}

@inproceedings{10.1145/2964284.2964329,
author = {Xu, Dan and Alameda-Pineda, Xavier and Song, Jingkuan and Ricci, Elisa and Sebe, Nicu},
title = {Academic Coupled Dictionary Learning for Sketch-based Image Retrieval},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2964329},
doi = {10.1145/2964284.2964329},
abstract = {In the last few years, the query-by-visual-example paradigm gained popularity, specially for content based retrieval systems. As sketches represent a natural way of expressing a synthetic query, recent research efforts focused on developing algorithmic solutions to address the sketch-based image retrieval (SBIR) problem. Within this context, we propose a novel approach for SBIR that, unlike previous methods, is able to exploit the visual complexity inherently present in sketches and images. We introduce academic learning, a paradigm in which the sample learning order is constructed both from the data, as in self-paced learning, and from partial curricula. We propose an instantiation of this paradigm within the framework of coupled dictionary learning to address the SBIR task. We also present an efficient algorithm to learn the dictionaries and the codes, and to pace the learning combining the reconstruction error, the prior knowledge suggested by the partial curricula and the cross-domain code coherence. In order to evaluate the proposed approach, we report an extensive experimental validation showing that the proposed method outperforms the state-of-the-art in coupled dictionary learning and in SBIR on three different publicly available datasets.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1326–1335},
numpages = {10},
keywords = {dictionary learning, self-paced and curriculum learning, sketch-based image retrieval},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.1007/978-3-030-01716-3_25,
author = {Zhang, Yue and Zhang, Liying and Liu, Yao},
title = {Linked Document Classification by Network Representation Learning},
year = {2018},
isbn = {978-3-030-01715-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01716-3_25},
doi = {10.1007/978-3-030-01716-3_25},
abstract = {Network Representation Learning (NRL) can learn a latent space representation of each vertex in a topology network structure to reflect linked information. Recently, NRL algorithms have been applied to obtain document embedding in linked document network, such as citation websites. However, most existing document representation methods with NRL are unsupervised and they cannot combine NRL with a concrete task-specific NLP tasks. So in this paper, we propose a unified end-to-end hybrid Linked Document Classification (LDC) model which can capture semantic features and topological structure of documents to improve the performance of document classification. In addition, we investigate to use a more flexible strategy to capture structure similarity to improve the traditional rigid extraction of linked document topology structure. The experimental results suggest that our proposed model outperforms other document classification methods especially in the case of having less training sets.},
booktitle = {Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data: 17th China National Conference, CCL 2018, and 6th International Symposium, NLP-NABD 2018, Changsha, China, October 19–21, 2018, Proceedings},
pages = {302–313},
numpages = {12},
keywords = {Document classification, NRL, Flexible random walk strategy},
location = {Changsha, China}
}

@article{10.1016/j.jvcir.2018.06.022,
author = {Wang, Meng and Tian, Qi and EI Saddik, Abdulmotaleb and Lux, Mathias and Tie, Yun},
title = {Multimedia analysis with collective intelligence},
year = {2018},
issue_date = {Aug 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {55},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2018.06.022},
doi = {10.1016/j.jvcir.2018.06.022},
journal = {J. Vis. Comun. Image Represent.},
month = aug,
pages = {433},
numpages = {1}
}

@article{10.1016/j.knosys.2013.12.010,
author = {Danger, Roxana and Pla, Ferran and Molina, Antonio and Rosso, Paolo},
title = {Towards a Protein-Protein Interaction information extraction system: Recognizing named entities},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.12.010},
doi = {10.1016/j.knosys.2013.12.010},
abstract = {The majority of biological functions of any living being are related to Protein-Protein Interactions (PPI). PPI discoveries are reported in form of research publications whose volume grows day after day. Consequently, automatic PPI information extraction systems are a pressing need for biologists. In this paper we are mainly concerned with the named entity detection module of PPIES (the PPI information extraction system we are implementing) which recognizes twelve entity types relevant in PPI context. It is composed of two sub-modules: a dictionary look-up with extensive normalization and acronym detection, and a Conditional Random Field classifier. The dictionary look-up module has been tested with Interaction Method Task (IMT), and it improves by approximately 10% the current solutions that do not use Machine Learning (ML). The second module has been used to create a classifier using the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA'04) data set. It does not use any external resources, or complex or ad hoc post-processing, and obtains 77.25%, 75.04% and 76.13 for precision, recall, and F1-measure, respectively, improving all previous results obtained for this data set.},
journal = {Know.-Based Syst.},
month = feb,
pages = {104–118},
numpages = {15},
keywords = {Biomedical named entity recognition, Conditional random field, Dictionary look-up, Protein-Protein Interaction, Support vector machine}
}

@inproceedings{10.1145/1858996.1859010,
author = {Berger, Thorsten and She, Steven and Lotufo, Rafael and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {Variability modeling in the real: a perspective from the operating systems domain},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859010},
doi = {10.1145/1858996.1859010},
abstract = {Variability models represent the common and variable features of products in a product line. Several variability modeling languages have been proposed in academia and industry; however, little is known about the practical use of such languages. We study and compare the constructs, semantics, usage and tools of two variability modeling languages, Kconfig and CDL. We provide empirical evidence for the real-world use of the concepts known from variability modeling research. Since variability models provide basis for automated tools (feature dependency checkers and product configurators), we believe that our findings will be of interest to variability modeling language and tool designers.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {configuration, empirical software engineering, feature models, product line architectures, variability modeling},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.5555/2009542.2009560,
author = {Mart\'{\i}nez-Rams, Ernesto A. and Garcer\'{a}n-Hern\'{a}ndez, Vicente},
title = {A speaker recognition system based on an auditory model and neural nets: performance at different levels of sound pressure and of gaussian white noise},
year = {2011},
isbn = {9783642213250},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper performs the assessment of an auditory model based on a human nonlinear cochlear filter-bank and on Neural Nets. The efficiency of this system in speaker recognition tasks has been tested at different levels of voice pressure and different levels of noise. The auditory model yields five psychophysical parameters with which a neural network is trained. We used a number of Spanish words from the 'Ahumada' database as uttered by native male speakers.},
booktitle = {Proceedings of the 4th International Conference on Interplay between Natural and Artificial Computation: New Challenges on Bioinspired Applications - Volume Part II},
pages = {157–166},
numpages = {10},
location = {Canary Islands, Spain},
series = {IWINAC'11}
}

@inproceedings{10.1145/3292500.3330878,
author = {Jiang, Zhe and Sainju, Arpan Man},
title = {Hidden Markov Contour Tree: A Spatial Structured Model for Hydrological Applications},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330878},
doi = {10.1145/3292500.3330878},
abstract = {Spatial structured models are predictive models that capture dependency structure between samples based on their locations in the space. Learning such models plays an important role in many geoscience applications such as water surface mapping, but it also poses significant challenges due to implicit dependency structure in continuous space and high computational costs. Existing models often assume that the dependency structure is based on either spatial proximity or network topology, and thus cannot incorporate complex dependency structure such as contour and flow direction on a 3D potential surface. To fill the gap, this paper proposes a novel spatial structured model called hidden Markov contour tree (HMCT), which generalizes the traditional hidden Markov model from a total order sequence to a partial order polytree. HMCT also advances existing work on hidden Markov trees through capturing complex contour structures on a 3D surface. We propose efficient model construction and learning algorithms. Evaluations on real world hydrological datasets show that our HMCT outperforms multiple baseline methods in classification performance and that HMCT is scalable to large data sizes (e.g., classifying millions of samples in seconds).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {804–813},
numpages = {10},
keywords = {3D surface, flood mapping, hidden Markov contour tree, spatial structured model, structured prediction},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.5555/648110.748126,
author = {Liu, Feng and Zhuang, Yueting and Luo, Zhongxiang and Pan, Yunhe},
title = {A Robust Algorithm for Video Based Human Motion Tracking},
year = {2002},
isbn = {3540002626},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present a robust algorithm to capture rapid human motion with self-occlusion. Instead of predicting the position of each human feature, the interest-region of full body is estimated. Then candidate features are extracted through the overall search in the interest-region. To establish the correspondence between candidate features and actual features, an adaptive Bayes classifier is constructed based on the time-varied models of feature attributions. At last, a hierarchical human feature model is adopted to verify and accomplish the feature correspondence. To improve the efficiency, we propose a multiresolution search strategy: the initial candidate feature set is estimated at the low resolution image and successively refined at higher resolution levels. The experiment demonstrates the effectiveness of our algorithm.},
booktitle = {Proceedings of the Third IEEE Pacific Rim Conference on Multimedia: Advances in Multimedia Information Processing},
pages = {1161–1168},
numpages = {8},
series = {PCM '02}
}

@inproceedings{10.5555/1888258.1888291,
author = {Du, Nan and Wang, Hao and Faloutsos, Christos},
title = {Analysis of large multi-modal social networks: patterns and a generator},
year = {2010},
isbn = {364215879X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {On-line social networking sites often involve multiple relations simultaneously. While people can build an explicit social network by adding each other as friends, they can also form several implicit social networks through their daily interactions like commenting on people's posts, or tagging people's photos. So given a real social networking system which changes over time, what can we say about people's social behaviors ? Do their daily interactions follow any pattern ? The majority of earlier work mainly mimics the patterns and properties of a single type of network. Here, we model the formation and co-evolution of multi-modal networks emerging from different social relations such as "who-adds-whom-as-friend" and "who-comments-on-whose-post" simultaneously. The contributions are the following : (a) we propose a new approach called EigenNetwork Analysis for analyzing time-evolving networks, and use it to discover temporal patterns with people's social interactions; (b) we report inherent correlation between friendship and co-occurrence in on-line settings; (c) we design the first multimodal graph generator xSocial 1 that is capable of producing multiple weighted time-evolving networks, which match most of the observed patterns so far. Our study was performed on two real datasets (Nokia FriendView and Flickr) with 100,000 and 50,000,000 records respectively, each of which corresponds to a different social service, and spans up to two years of activity.},
booktitle = {Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part I},
pages = {393–408},
numpages = {16},
keywords = {graph generator, multi-modal networks, social network analysis},
location = {Barcelona, Spain},
series = {ECML PKDD'10}
}

@article{10.1109/34.310694,
author = {Beni, G. and Liu, X.},
title = {A Least Biased Fuzzy Clustering Method},
year = {1994},
issue_date = {September 1994},
publisher = {IEEE Computer Society},
address = {USA},
volume = {16},
number = {9},
issn = {0162-8828},
url = {https://doi.org/10.1109/34.310694},
doi = {10.1109/34.310694},
abstract = {A new operational definition of cluster is proposed, and a fuzzy clustering algorithm with minimal biases is formulated by making use of the maximum entropy principle to maximize the entropy of the centroids with respect to the data points (clustering entropy). The authors make no assumptions on the number of clusters or their initial positions. For each value of an adimensional scale parameter /spl beta/', the clustering algorithm makes each data point iterate towards one of the cluster's centroids, so that both hard and fuzzy partitions are obtained. Since the clustering algorithm can make a multiscale analysis of the given data set one can obtain both hierarchy and partitioning type clustering. The relative stability with respect to /spl beta/' of each cluster structure is defined as the measurement of cluster validity. The authors determine the specific value of /spl beta/' which corresponds to the optimal positions of cluster centroids by minimizing the entropy of the data points with respect to the centroids (clustered entropy). Examples are given to show how this least biased method succeeds in getting perceptually correct clustering results.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = sep,
pages = {954–960},
numpages = {7},
keywords = {cluster structure, clustering entropy, entropy, fuzzy partitions, fuzzy set theory, hard partitions, least biased fuzzy clustering method, maximum entropy principle, minimal biases, multiscale analysis, pattern recognition, relative stability}
}

@article{10.1007/s00607-017-0543-z,
author = {Agrawal, Tarun K. and Sahu, Aryabartta and Ghose, Manojit and Sharma, R.},
title = {Scheduling chained multiprocessor tasks onto large multiprocessor system},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {99},
number = {10},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-017-0543-z},
doi = {10.1007/s00607-017-0543-z},
abstract = {In this paper, we proposed an effective approach for scheduling of multiprocessor unit time tasks with chain precedence on to large multiprocessor system. In this work, we considered splitable and non-splitable multiprocessor tasks, which is a new and interesting dimension to the generalized scheduling problem. The proposed longest chain maximum processor scheduling algorithm is proved to be optimal for uniform chains and monotone (non-increasing/non-decreasing) chains for both splitable and non-splitable multiprocessor unit time tasks chain. Scheduling arbitrary chains of non-splitable multiprocessor unit time tasks is proved to be NP-complete problem. But scheduling arbitrary chains of splitable multiprocessor unit time tasks is still an open problem to be proved whether it is NP-complete or can be solved in polynomial time. We have used three heuristics (a) maximum criticality first, (b) longest chain maximum criticality first and (c) longest chain maximum processor first for scheduling of arbitrary chains. We have also compared the performance of all three scheduling heuristics and found out that the proposed longest chain maximum processor first performs better in most of the cases. Also we have evaluated the performance of the mentioned heuristics by scheduling scientific work-flows on real multi-processor server platform and analyzed power and performance trade-off of the same scheduling policies.},
journal = {Computing},
month = oct,
pages = {1007–1028},
numpages = {22},
keywords = {68W01, Chain of task, Multiphase applications, Multiprocessor, Scheduling, Splitable}
}

@article{10.2478/v10006-008-0012-0,
author = {Uysal, Ismail and Sathyendra, Harsha and Harris, John},
title = {Towards Spike-Based Speech Processing: A Biologically Plausible Approach to Simple Acoustic Classification},
year = {2008},
issue_date = {Jun 2008},
publisher = {Walter de Gruyter &amp; Co.},
address = {USA},
volume = {18},
number = {2},
issn = {1641-876X},
url = {https://doi.org/10.2478/v10006-008-0012-0},
doi = {10.2478/v10006-008-0012-0},
abstract = {Shortcomings of automatic speech recognition (ASR) applications are becoming more evident as they are more widely used in real life. The inherent non-stationarity associated with the timing of speech signals as well as the dynamical changes in the environment make the ensuing analysis and recognition extremely difficult. Researchers often turn to biology seeking clues to make better engineered systems, and ASR is no exception with the usage of feature sets such as Mel frequency cepstral coefficients, which employ filter banks similar to cochlear filter banks in frequency distribution and bandwidth. In this paper, we delve deeper into the mechanics of the human auditory system to take this biological inspiration to the next level. The main goal of this research is to investigate the computation potential of spike trains produced at the early stages of the auditory system for a simple acoustic classification task. First, various spike coding schemes from temporal to rate coding are explored, together with various spike-based encoders with various simplicity levels such as rank order coding and liquid state machine. Based on these findings, a biologically plausible system architecture is proposed for the recognition of phonetically simple acoustic signals which makes exclusive use of spikes for computation. The performance tests show superior performance on a noisy vowel data set when compared with a conventional ASR system.},
journal = {Int. J. Appl. Math. Comput. Sci.},
month = jun,
pages = {129–137},
numpages = {9},
keywords = {Spike coding, synchrony coding, phase locking, speech perception, psychoacoustics, speech recognition}
}

@inproceedings{10.1145/3425898.3426959,
author = {Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Correctness-by-construction for feature-oriented software product lines},
year = {2020},
isbn = {9781450381741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425898.3426959},
doi = {10.1145/3425898.3426959},
abstract = {Software product lines are increasingly used to handle the growing demand of custom-tailored software variants. They provide systematic reuse of software paired with variability mechanisms in the code to implement whole product families rather than single software products. A common domain of application for product lines are safety-critical systems, which require behavioral correctness to avoid dangerous situations in-field. While most approaches concentrate on post-hoc verification for product lines, we argue that a stepwise approach to create correct programs may be beneficial for developers to manage the growing variability. Correctness-by-construction is such a stepwise approach to create programs using a set of small, tractable refinement rules that guarantee the correctness of the program with regard to its specification. In this paper, we propose the first approach to develop correct-by-construction software product lines using feature-oriented programming. First, we extend correctness-by-construction by two refinement rules for variation points in the code. Second, we give a proof for the soundness of the proposed rules. Third, we implement our technique in a tool called VarCorC and show the applicability of the tool by conducting two case studies.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {22–34},
numpages = {13},
keywords = {correctness-by-construction, feature-oriented programming, formal verification, software product lines},
location = {Virtual, USA},
series = {GPCE 2020}
}

@article{10.1504/IJWET.2015.069359,
author = {Berkane, Mohamed Lamine and Seinturier, Lionel and Boufaida, Mahmoud},
title = {Using variability modelling and design patterns for self-adaptive system engineering: application to smart-home},
year = {2015},
issue_date = {May 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1},
issn = {1476-1289},
url = {https://doi.org/10.1504/IJWET.2015.069359},
doi = {10.1504/IJWET.2015.069359},
abstract = {Adaptability is an increasingly important requirement for many systems, in particular for those that are deployed in dynamically changing environments. The purpose is to let the systems react and adapt autonomously to changing executing conditions without human intervention. Due to the large number of variability decisions e.g., user needs, environment characteristics and the current lack of reusable adaptation expertise, it becomes increasingly difficult to build a system that satisfies all the requirements and constraints that might arise during its lifetime. In this paper, we propose an approach for developing policies for self-adaptive systems at multiple levels of abstraction. This approach is the first that allows the combination of variability with feature model and reusability with design pattern into a single solution for product derivation that gives strong support to develop self-adaptive systems in a modular way. We demonstrate the feasibility of the proposed approach with a use case based on a smart home scenario.},
journal = {Int. J. Web Eng. Technol.},
month = may,
pages = {65–93},
numpages = {29}
}

@inproceedings{10.5555/646926.710094,
author = {Fan, Shao and Voon, Ling Keck and Ng, Wan Sing},
title = {3D Prostate Surface Detection from Ultrasound Images Based on Level Set Method},
year = {2002},
isbn = {3540442251},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Accurate detection of prostate boundaries is required in many diagnostic and treatment procedures for prostate diseases. In this paper, a new approach based on level set method to perform 3D prostate surface detection from transrectal ultrasound (TRUS) images is presented. Contrary to many other deformable models, level set method offers several advantages such as minimal need for user input, flexible topology, and straightforward extension to 3D. However, it is subject to "boundary leaking" problem for ultrasound image segmentation due to the poor image quality. In this work, we first develop a fast discrimination method to extract the prostate region, then this region information, instead of the spatial image gradient, is incorporated into the level set method to remedy the "boundary leaking" problem. Various experimental results show the effectiveness of the proposed method.},
booktitle = {Proceedings of the 5th International Conference on Medical Image Computing and Computer-Assisted Intervention-Part II},
pages = {389–396},
numpages = {8},
series = {MICCAI '02}
}

@article{10.1007/s10270-020-00814-5,
author = {Mussbacher, Gunter and Combemale, Benoit and Kienzle, J\"{o}rg and Abrah\~{a}o, Silvia and Ali, Hyacinth and Bencomo, Nelly and B\'{u}r, M\'{a}rton and Burgue\~{n}o, Loli and Engels, Gregor and Jeanjean, Pierre and J\'{e}z\'{e}quel, Jean-Marc and K\"{u}hn, Thomas and Mosser, S\'{e}bastien and Sahraoui, Houari and Syriani, Eugene and Varr\'{o}, D\'{a}niel and Weyssow, Martin},
title = {Opportunities in intelligent modeling assistance},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00814-5},
doi = {10.1007/s10270-020-00814-5},
abstract = {Modeling is requiring increasingly larger efforts while becoming indispensable given the complexity of the problems we are solving. Modelers face high cognitive load to understand a multitude of complex abstractions and their relationships. There is an urgent need to better support tool builders to ultimately provide modelers with intelligent modeling assistance that learns from previous modeling experiences, automatically derives modeling knowledge, and provides context-aware assistance. However, current intelligent modeling assistants (IMAs) lack adaptability and flexibility for tool builders, and do not facilitate understanding the differences and commonalities of IMAs for modelers. Such a patchwork of limited IMAs is a lost opportunity to provide modelers with better support for the creative and rigorous aspects of software engineering. In this expert voice, we present a conceptual reference framework (RF-IMA) and its properties to identify the foundations for intelligent modeling assistance. For tool builders, RF-IMA aims to help build IMAs more systematically. For modelers, RF-IMA aims to facilitate comprehension, comparison, and integration of IMAs, and ultimately to provide more intelligent support. We envision a momentum in the modeling community that leads to the implementation of RF-IMA and consequently future IMAs. We identify open challenges that need to be addressed to realize the opportunities provided by intelligent modeling assistance.},
journal = {Softw. Syst. Model.},
month = sep,
pages = {1045–1053},
numpages = {9},
keywords = {Model-based software engineering, Intelligent modeling assistance, Integrated development environment, Artificial intelligence, Development data, Feedback}
}

@article{10.1016/j.neunet.2007.04.025,
author = {Wichern, Gordon and Azimi-Sadjadi, Mahmood R. and Mungiole, Michael},
title = {2007 Special Issue: Environmentally adaptive acoustic transmission loss prediction in turbulent and nonturbulent atmospheres},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {20},
number = {4},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2007.04.025},
doi = {10.1016/j.neunet.2007.04.025},
abstract = {An environmentally adaptive system for prediction of acoustic transmission loss (TL) in the atmosphere is developed in this paper. This system uses several back propagation neural network predictors, each corresponding to a specific environmental condition. The outputs of the expert predictors are combined using a fuzzy confidence measure and a nonlinear fusion system. Using this prediction methodology the computational intractability of traditional acoustic model-based approaches is eliminated. The proposed TL prediction system is tested on two synthetic acoustic data sets for a wide range of geometrical, source and environmental conditions including both nonturbulent and turbulent atmospheres. Test results of the system showed root mean square (RMS) errors of 1.84 dB for the nonturbulent and 1.36 dB for the turbulent conditions, respectively, which are acceptable levels for near real-time performance. Additionally, the environmentally adaptive system demonstrated improved TL prediction accuracy at high frequencies and large values of horizontal separation between source and receiver.},
journal = {Neural Netw.},
month = may,
pages = {484–497},
numpages = {14},
keywords = {Atmospheric acoustics, Fuzzy-logic fusion, Parabolic equation, Turbulent scattering}
}

@inproceedings{10.1007/978-3-319-23826-5_34,
author = {Baeza-Yates, Ricardo and Mayo-Casademont, Mart\'{\i} and Rello, Luz},
title = {Feasibility of Word Difficulty Prediction},
year = {2015},
isbn = {9783319238258},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23826-5_34},
doi = {10.1007/978-3-319-23826-5_34},
abstract = {We present a machine learning algorithm to predict how difficult is a word for a person with dyslexia. To train the algorithm we used a data set of words labeled as easy or difficult. The algorithm predicts correctly slightly above 72% of our instances, showing the feasibility of building such a predictive solution for this problem. The main purpose of our work is to be able to weight words in order to perform lexical simplification in texts read by people with dyslexia. Since the main feature used by the classifier, and the only that is not computed in constant time, is the number of similar words in a dictionary, we did a study on the different methods that exist to compute efficiently this feature. This algorithmic comparison is interesting on its own sake and shows that two algorithms can solve the problem in less than a second.},
booktitle = {Proceedings of the 22nd International Symposium on String Processing and Information Retrieval - Volume 9309},
pages = {362–373},
numpages = {12},
location = {London, UK},
series = {SPIRE 2015}
}

