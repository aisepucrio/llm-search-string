@inproceedings{10.1007/978-3-319-35122-3_5,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {Tax-PLEASE--Towards Taxonomy-Based Software Product Line Engineering},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_5},
doi = {10.1007/978-3-319-35122-3_5},
abstract = {Modern software systems, in particular in mobile and cloud-based applications, exist in many different variants in order to adapt to changing user requirements or application contexts. Software product line engineering allows developing these software systems by managed large-scale reuse in order to achieve shorter time to market. Traditional software product line engineering approaches use a domain variability model which only captures the configuration options of the product variants, but does not provide any guideline for designing and implementing reusable artifacts. In contrast, software taxonomies structure software domains from an abstract specification of the functionality to concrete implementable variants by successive correctness-preserving refinements. In this paper, we propose a novel software product line engineering process based on a taxonomy-based domain analysis. The taxonomy's hierarchy provides guidelines for designing and implementing the product line's reusable artifacts while at the same time specifying possible configuration options. By deriving reusable product line artifacts from a software taxonomy, the well-defined structuring of the reusable artifacts yields improved maintainability and evolvability of the product line.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {63–70},
numpages = {8},
keywords = {Taxonomy-Based Software Construction TABASCO, Software Product Line SPL},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2970276.2970288,
author = {Schw\"{a}gerl, Felix and Westfechtel, Bernhard},
title = {SuperMod: tool support for collaborative filtered model-driven software product line engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970288},
doi = {10.1145/2970276.2970288},
abstract = {The increase in productivity implied by model-driven software product line engineering is weakened by the complexity exposed to the user having to manage a multi-variant model. Recently, a new paradigm has emerged: filtered software product line engineering transfers the established check-out/modify/commit workflow from version control to variability management, allowing to iteratively develop the multi-variant model in a single-variant view. This paper demonstrates SuperMod, a tool that supports collaborative filtered model-driven product line engineering, implemented for and with the Eclipse Modeling Framework. Concerning variability management, the tool offers capabilities for editing feature models and specifying feature configurations, both being well-known formalisms in product line engineering. Furthermore, collaborative editing of product lines is provided through distributed version control. The accompanying video shows that SuperMod seamlessly integrates into existing tool landscapes, reduces the complexity of multi-variant editing, automates a large part of variability management, and ensures consistency. A tool demonstration video is available here: http://youtu.be/5XOk3x5kjFc},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {822–827},
numpages = {6},
keywords = {version control, software product line engineering, filtered editing, Model-driven software engineering},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {variability, transfer learning, software product lines, machine learning, deep neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {variability management, software architecture},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-319-35122-3_2,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Automated Composition of Service Mashups Through Software Product Line Engineering},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_2},
doi = {10.1007/978-3-319-35122-3_2},
abstract = {The growing number of online resources, including data and services, has motivated both researchers and practitioners to provide methods and tools for non-expert end-users to create desirable applications by putting these resources together leading to the so called mashups. In this paper, we focus on a class of mashups referred to as service mashups. A service mashup is built from existing services such that the developed service mashup offers added-value through new functionalities. We propose an approach which adopts concepts from software product line engineering and automated AI planning to support the automated composition of service mashups. One of the advantages of our work is that it allows non-experts to build and optimize desired mashups with little knowledge of service composition. We report on the results of the experimentation that we have performed which support the practicality and scalability of our proposed work.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {20–38},
numpages = {19},
keywords = {Workflow optimization, Software product lines, Service mashups, Planning, Feature model, Automated composition},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {production stations, core assets, VR application features},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and T\"{e}rnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {software product lines, robocode, reverse-engineering, extractive software product line adoption, education},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {software product lines, industry, academia, SPLC},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106223,
author = {Iglesias, Aitziber and Lu, Hong and Arellano, Crist\'{o}bal and Yue, Tao and Ali, Shaukat and Sagardui, Goiuria},
title = {Product Line Engineering of Monitoring Functionality in Industrial Cyber-Physical Systems: A Domain Analysis},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106223},
doi = {10.1145/3106195.3106223},
abstract = {In recent years, manufacturing technology is evolving and progressively becoming more dynamic and complex. This means that manufacturing technology (e.g., based on Industry 4.0) should be able to control the production process at runtime by monitoring physical elements and adapting itself. Such functionality is aimed at increasing production effectiveness and reducing the production cost. We argue that monitoring process can be viewed as a software product line having commonalities and variability. To support our argument, we analyzed and conducted domain analysis of two monitoring systems of Industrial Cyber-Physical Systems (ICPSs) from two industrial domains including automated warehouses and press machines. Based on the domain analysis, we present a common solution for monitoring including a software product line. With such product line, a user can configure, monitor, and visualize data of an ICPS at runtime. However, such solution could not handle the dynamic functionality related to monitoring of ICPS. Thus, we propose the use of dynamic product line and present a set of research questions that must be addressed for such solution.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {195–204},
numpages = {10},
keywords = {Software Product Line, Industrial domains, Dynamic Software Product Line, Cyber Physical System},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {software product line, requirements engineering, natural language processing, feature model extraction, NLTK},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {variability, process mining, process discovery, configuration workflow, clustering},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3297156.3297203,
author = {Hasbi, Muhamad and Budiardjo, Eko K. and Wibowo, Wahyu C.},
title = {Reverse Engineering in Software Product Line - A Systematic Literature Review},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297203},
doi = {10.1145/3297156.3297203},
abstract = {Reverse engineering is the information extraction process on system by identifying and analyzing the components that are part of that system. We analyze existing research that related with reverse engineering process on software product line. There are two product line processes according to Software product line engineering framework they are domain engineering process and application engineering process. We investigate reverse engineering in domain engineering process (domain requirements, domain design, and domain realization, domain quality assurance). We performed a systematic literature review. A manual search resulting 71 papers considered for analysis. Results: The majority of reverse engineering studied in three domain activity in domain engineering process. That is requirement engineering, domain design and domain realization. There are inconsistent correlations between features in the reverse engineering process. These approaches extract features without constraints between its features. Conclusions: Reverse engineering methods are needed that are able to identify and maintain a consistent correlation between features in application engineering and domain engineering in the reverse engineering process. Finally, we provide gaps from existing research and show opportunities for future research.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {174–179},
numpages = {6},
keywords = {systematic review, software product line, domain engineering, Reverse engineering},
location = {Shenzhen, China},
series = {CSAI '18}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Software product line, Predictive analytics, Literature review, Industrial SPL tools, Inconsistencies, Automated feature selection, Artificial intelligence}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {product-line analysis, Software product lines}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {software product lines, machine learning, configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2934466.2934483,
author = {Richenhagen, Johannes and Rumpe, Bernhard and Schlo\ss{}er, Axel and Schulze, Christoph and Thissen, Kevin and von Wenckstern, Michael},
title = {Test-driven semantical similarity analysis for software product line extraction},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934483},
doi = {10.1145/2934466.2934483},
abstract = {Software product line engineering rests upon the assumption that a set of products share a common base of similar functionality. The correct identification of similarities between different products can be a time-intensive task. Hence, this paper proposes an automated semantical similarity analysis supporting software product line extraction and maintenance. Under the assumption of an already identified compatible interface, the degree of semantical similarity is identified based on provided test cases. Therefore, the analysis can also be applied in a test-driven development. This is done by translating available test sequences for both components into two I/O extended finite automata and performing an abstraction of the defined behavior until a simulation relation is established. The test-based approach avoids complexity issues regarding the state space explosion problem, a common issue in model checking. The proposed approach is applied on different variants and versions of industrially used software components provided by an automotive supplier to demonstrate the method's applicability.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {174–183},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/1321631.1321730,
author = {Dhungana, Deepak and Rabiser, Rick and Gr\"{u}nbacher, Paul and Neumayer, Thomas},
title = {Integrated tool support for software product line engineering},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321730},
doi = {10.1145/1321631.1321730},
abstract = {Product line engineering comprises many heterogeneous activities such as capturing the variability of reusable assets, supporting the derivation of products from the product line, evolving the product line, or tailoring the approach to the specifics of a domain. The inherent complexity of product lines implicates that tool support is inevitable to facilitate smooth performance and to avoid costly errors. Product line engineering tools have to support heterogeneous stakeholders involved in diverse activities. Tool integration therefore is of particular importance to foster their seamless cooperation. However, the integration is difficult to achieve due to the diversity of models and work products. This paper describes the DOPLER tool suite which has been developed to provide such integrated support. The tool suite is flexible and extensible to support domain-specific needs},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {533–534},
numpages = {2},
keywords = {variability modeling, product line tools, product line engineering, product derivation, multi-team modeling, model evolution},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.artmed.2021.102165,
author = {de Siqueira, Vilson Soares and Borges, Mois\'{e}s Marcos and Furtado, Rog\'{e}rio Gomes and Dourado, Colandy Nunes and da Costa, Ronaldo Martins},
title = {Artificial intelligence applied to support medical decisions for the automatic analysis of echocardiogram images: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102165},
doi = {10.1016/j.artmed.2021.102165},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {19},
keywords = {Echocardiogram, Echocardiography, Machine Learning, Deep Learning}
}

@inproceedings{10.1109/CEC48606.2020.9185675,
author = {Ibias, Alfredo and Llana, Luis},
title = {Feature Selection using Evolutionary Computation Techniques for Software Product Line Testing},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC48606.2020.9185675},
doi = {10.1109/CEC48606.2020.9185675},
abstract = {Software product lines are an excellent mechanism in the development of software. Testing software product lines is an intensive process where selecting the right features where to focus it can be a critical task. Selecting the best combination of features from a software product line is a complex problem addressed in the literature. In this paper, we address the problem of finding the combination of features with the highest probability of being requested from a software product line with probabilities. We use Evolutive Computation techniques to address this problem. Specifically, we use the Ant Colony Optimization algorithm to find the best combination of features. Our results report that our framework overcomes the limitations of the brute force algorithm.},
booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {E-learning, Software Product line, Variability, heterogeneity, scale, systematic literature review, variety},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {multi-objective algorithms, searchbased PLA design, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/j.knosys.2019.104883,
author = {Ayala, Inmaculada and Amor, Mercedes and Horcas, Jose-Miguel and Fuentes, Lidia},
title = {A goal-driven software product line approach for evolving multi-agent systems in the Internet of Things},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {184},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104883},
doi = {10.1016/j.knosys.2019.104883},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {Software product line, Evolution, Internet of Things, MAS-PL, Goal models, GORE}
}

@article{10.1016/j.chb.2017.04.026,
author = {Gharsellaoui, Hamza and Maazoun, Jihen and Bouassida, Nadia and Ahmed, Samir Ben and Ben-Abdallah, Hanene},
title = {A Software Product Line Design Based Approach for Real-time Scheduling of Reconfigurable Embedded Systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2017.04.026},
doi = {10.1016/j.chb.2017.04.026},
journal = {Comput. Hum. Behav.},
month = feb,
numpages = {11},
keywords = {Real-time scheduling, Reconfigurable embedded systems, SPL design, UML marte}
}

@inproceedings{10.1145/2362536.2362580,
author = {Hamza, Haitham S. and Martinez, Jabier and Thurimella, Anil Kumar and Deogun, Jitender S.},
title = {Third International Workshop on Knowledge-Oriented Product Line Engineering (KOPLE 2012)},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362580},
doi = {10.1145/2362536.2362580},
abstract = {Software Product Line Engineering (PLE) exploits systematic reuse by identifying and methodically reusing software artifacts to develop different but related software systems. Developing Product Lines requires analysis skills to identify, model, and encode domain and product knowledge into artifacts that can be systematically reused across the development life-cycle. As such, Knowledge plays a paramount role in the success of the various activities of PLE. The objective of the KOPLE workshop series is to bring together SPL researchers and practitioners from academia and industry to investigate the role of Knowledge in PLE. Knowledge is usually encapsulated in PL architectures in a tacit or implicit way, and this may appear to be sufficient for industry to implement successful product lines. Nevertheless, KOPLE also aims to become a discussion forum about techniques and methods to convert from tacit to explicit Knowledge in PLE and to process and use this Knowledge for optimizing and innovating PLE processes.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {292–293},
numpages = {2},
keywords = {conceptual graphs, knowledge engineering, ontology, product lines, software reuse, tacit knowledge},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@article{10.1007/s10270-017-0614-9,
author = {Guizzo, Giovani and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Applying design patterns in the search-based optimization of software product line architectures},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0614-9},
doi = {10.1007/s10270-017-0614-9},
abstract = {The design of the product line architecture (PLA) is a difficult activity that can benefit from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research field, not completely explored yet. Some works apply the patterns manually and for a specific domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1487–1512},
numpages = {26},
keywords = {Design pattern, Search-based software engineering, Software product line architecture}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@article{10.4018/IJWSR.2019010103,
author = {Sun, Chang-ai and Wang, Zhen and Wang, Ke and Xue, Tieheng and Aiello, Marco},
title = {Adaptive BPEL Service Compositions via Variability Management: A Methodology and Supporting Platform},
year = {2019},
issue_date = {January 2019},
publisher = {IGI Global},
address = {USA},
volume = {16},
number = {1},
issn = {1545-7362},
url = {https://doi.org/10.4018/IJWSR.2019010103},
doi = {10.4018/IJWSR.2019010103},
abstract = {Service-Oriented Architectures are a popular development paradigm to enable distributed applications constructed from independent web services. When coordinated, web services are an infrastructure to fulfill dynamic and vertical integration of business. They may face frequent changes of both requirements and execution environments. Static and predefined service compositions using business process execution language BPEL are not able to cater for such rapid and unpredictable context shifts. The authors propose a variability management-based adaptive and configurable service composition approach that treats changes as first-class citizens and consists of identifying, expressing, realizing, and managing changes of service compositions. The proposed approach is realized with a language called VxBPEL to support variability in service compositions and a platform for design, execution, analysis, and maintenance of VxBPEL-based service compositions. Four case studies validate the feasibility of the proposed approach while exhibiting good performance of the supporting platform.},
journal = {Int. J. Web Serv. Res.},
month = jan,
pages = {37–69},
numpages = {33},
keywords = {Adaptive Systems, Business Process Execution Language, Service Composition, Service Oriented Architectures, Variability Management}
}

@inproceedings{10.1145/1964138.1964139,
author = {Silva, Alan Pedro da and Costa, Evandro and Bittencourt, Ig Ibert and Brito, Patrick H. S. and Holanda, Olavo and Melo, Jean},
title = {Ontology-based software product line for building semantic web applications},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964139},
doi = {10.1145/1964138.1964139},
abstract = {The Software Product Lines (SPL) has proved very effective in building large-scale software. However, few works seek to adjust the approach of software product line to applications in the context of semantic web. This is because applications in this context assume the use of semantic services and intelligent agents. As a result, it is necessary that there are assets that provide adequate interoperability both semantic services and intelligent agents. In this sense, it is proposed in this paper the use of ontologies for the specification of entire a project of a SPL. With this, it can be a sufficiently formal specification that can be interpreted by both software engineers and computational algorithms.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {1},
numpages = {6},
keywords = {ontology, semantic web, software product line},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@inproceedings{10.1109/ASE.2015.106,
author = {Pietsch, Christopher and Kehrer, Timo and Kelter, Udo and Reuling, Dennis and Ohrndorf, Manuel},
title = {SiPL: a delta-based modeling framework for software product line engineering},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.106},
doi = {10.1109/ASE.2015.106},
abstract = {Model-based development has become a widely-used approach to implement software, e.g. for embedded systems. Models replace source code as primary executable artifacts in these cases. Software product line technologies for these domains must be able to generate models as instances of an SPL. This need is addressed among others by an implementation technology for SPLs known as delta modeling. Current approaches to delta modeling require deltas to be written manually using delta languages, and they offer only very limited support for creating and testing a network of deltas. This paper presents a new approach to delta modeling and a supporting tool suite: the abstract notion of a delta is refined to be a consistency-preserving edit script which is generated by comparing two models. The rich structure of edit scripts allows us to detect conflicts and further relations between deltas statically and to implement restructurings in delta sets such as the merging of two deltas. We illustrate the tooling using a case study.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {852–857},
numpages = {6},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/2364412.2364425,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Towards an incremental automata-based approach for software product-line model checking},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364425},
doi = {10.1145/2364412.2364425},
abstract = {Most model-checking algorithms are based on automata theory. For instance, determining whether or not a transition system satisfies a Linear Temporal Logic (LTL) formula requires computing strongly connected component of its transition graph. In Software Product-Line (SPL) engineering, the model checking problem is more complex due to the huge amount of software products that may compose the line. Indeed, one has to determine the exact subset of those products that do not satisfy an intended property. Efficient dedicated verification methods have been recently developed to answer this problem. However, most of them does not allow incremental verification. In this paper, we introduce an automata-based incremental approach for SPL model checking. Our method makes use of previous results to determine whether or not the addition of conservative features (i.e., features that do not remove behaviour from the system) preserves the satisfaction of properties expressed in LTL. We provide a detailed description of the approach and propose algorithms that implement it. We discuss how our method can be combined with SPL dedicated verification methods, viz. Featured Transition Systems.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {74–81},
numpages = {8},
keywords = {model checking, modularity, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.1145/2739482.2764650,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {A Search Based Approach Towards Robust Optimization in Software Product Line Scoping},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764650},
doi = {10.1145/2739482.2764650},
abstract = {Software product line (SPL) scoping is important for planning upfront investment. One challenge with scoping comes from inaccuracies in estimated parameters and uncertainty in environment. In this paper, a method to incorporate uncertainty in SPL scoping optimization and its application to generate robust solutions is proposed. We model scoping optimization as a multi-objective problem with profit and stability as heuristics. To evaluate our proposal, a number of experiments are conducted. Analysis of results show that both performance stability and feasibility stability were improved providing the product line manager enhanced decision-making support.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1415–1416},
numpages = {2},
keywords = {multi-objective, robust optimization, software product line portfolio scoping, uncertainty},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1109/CEC.2018.8477803,
author = {Luiz Jakubovski Filho, Helson and Nascimento Ferreira, Thiago and Regina Vergilio, Silvia},
title = {Incorporating User Preferences in a Software Product Line Testing Hyper-Heuristic Approach},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC.2018.8477803},
doi = {10.1109/CEC.2018.8477803},
abstract = {To perform the variability testing of Software Product Lines (SPLs) a set of products, represented in the Feature Model (FM), should be selected. Such selection is impacted by conflicting factors and has been efficiently solved by Evolutionary Multi-objective Algorithms in combination with hyper-heuristics. However, many times there is a cost budget or coverage level to be satisfied during the test, which are difficult to be incorporated as objective functions. Due to this, the choice of the best solution to be used in practice is not always easy. To deal with this situation, this paper introduces a preference-based hyper-heuristic approach to solve this problem. The approach implements the preference-based algorithm r-NSGA-II working with the random and FRRMAB selection methods. This last one uses a reward function based on r-dominance concept that takes into consideration a Reference Point provided by the tester. Our approach outperforms existing approaches, as well as the traditional algorithm r-NSGA-II, generating a reduced number of non-interesting solutions from the tester's point of view, that is, considering the provided Region of Interest (ROI).},
booktitle = {2018 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Rio de Janeiro, Brazil}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@inproceedings{10.1007/978-3-319-13365-2_20,
author = {Rahman, Musfiqur and Ripon, Shamim},
title = {Using Bayesian Networks to Model and Analyze Software Product Line Feature Model},
year = {2014},
isbn = {9783319133645},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-13365-2_20},
doi = {10.1007/978-3-319-13365-2_20},
abstract = {Proper management of requirements plays a significant role in the successful development of any software product family. Application of AI, Bayesian Network (BN) in particular, is gaining much interest in Software Engineering, mainly in predicting software defects and software reliability. Feature analysis and its associated decision making is a suitable target area where BN can make remarkable effect. In SPL, a feature tree portrays various types of features as well as captures the relationships among them. This paper applies BN in modeling and analyzing features in a feature tree. Various feature analysis rules are first modeled and then verified in BN. The verification confirms the definition of the rules and thus these rules can be used in various decision making stages in SPL.},
booktitle = {Proceedings of the 8th International Workshop on Multi-Disciplinary Trends in Artificial Intelligence - Volume 8875},
pages = {220–231},
numpages = {12},
keywords = {Bayesian Networks, Dead feature, False Optional, Software Product Line},
location = {Bangalore, India},
series = {MIWAI 2014}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.5220/0005156003160321,
author = {Alexandre, Fr\'{e}d\'{e}ric and Carrere, Maxime and Kassab, Randa},
title = {Feature, Configuration, History},
year = {2014},
isbn = {9789897580543},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005156003160321},
doi = {10.5220/0005156003160321},
abstract = {Artificial Neural Networks are very efficient adaptive models but one of their recognized weaknesses is about information representation, often carried out in an input vector without a structure. Beyond the classical elaboration of a hierarchical representation in a series of layers, we report here inspiration from neuroscience and argue for the design of heterogenous neural networks, processing information at feature, configuration and history levels of granularity, and interacting very efficiently for high-level and complex decision making. This framework is built from known characteristics of the sensory cortex, the hippocampus and the prefrontal cortex and is exemplified here in the case of pavlovian conditioning, but we propose that it can be advantageously applied in a wider extent, to design flexible and versatile information processing with neuronal computation.},
booktitle = {Proceedings of the International Joint Conference on Computational Intelligence - Volume 3},
pages = {316–321},
numpages = {6},
keywords = {Computational Neuroscience, Information Representation, Pavlovian Conditioning.},
location = {Rome, Italy},
series = {IJCCI 2014}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Domain specific modeling, Model driven - software product lines, Quality evaluation, Sensitivity points, Software architecture, Variability management}
}

@inproceedings{10.5555/1753235.1753266,
author = {Hubaux, Arnaud and Classen, Andreas and Heymans, Patrick},
title = {Formal modelling of feature configuration workflows},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {In software product line engineering, the configuration process can be a long and complex undertaking that involves many participants. When configuration is supported by feature diagrams, two challenges are to modularise the feature diagram into related chunks, and to schedule them as part of the configuration process. Existing work has only focused on the first of these challenges and, for the rest, assumes that feature diagram modules are configured sequentially. This paper addresses the second challenge. It suggests using YAWL, a state-of-the-art workflow language, to represent the configuration workflow while feature diagrams model the available configuration options. The principal contribution of the paper is a new combined formalism: feature configuration workflows. A formal semantics is provided so as to pave the way for unambiguous tool specification and safer reasoning about of the configuration process. The work is motivated and illustrated through a configuration scenario taken from the space industry.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {221–230},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@inproceedings{10.1109/ASE.2009.16,
author = {Lauenroth, Kim and Pohl, Klaus and Toehning, Simon},
title = {Model Checking of Domain Artifacts in Product Line Engineering},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.16},
doi = {10.1109/ASE.2009.16},
abstract = {In product line engineering individual products are derived from the domain artifacts of the product line. The reuse of the domain artifacts is constraint by the product line variability. Since domain artifacts are reused in several products, product line engineering benefits from the verification of domain artifacts. For verifying development artifacts, model checking is a well-established technique in single system development. However, existing model checking approaches do not incorporate the product line variability and are hence of limited use for verifying domain artifacts. In this paper we present an extended model checking approach which takes the product line variability into account when verifying domain artifacts. Our approach is thus able to verify that every permissible product (specified with I/O-automata) which can be derived from the product line fulfills the specified properties (specified with CTL). Moreover, we use two examples to validate the applicability of our approach and report on the preliminary validation results.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {269–280},
numpages = {12},
keywords = {Domain Artifact Verification, Model Checking, Product Line Engineering, Variability},
series = {ASE '09}
}

@article{10.1155/2021/1057371,
author = {Yang, Yinghui and cheikhrouhou, omar},
title = {The Potential Energy of Artificial Intelligence Technology in University Education Reform from the Perspective of Communication Science},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/1057371},
doi = {10.1155/2021/1057371},
abstract = {In today’s rapid development of science and technology, science is everywhere in people’s lives, and science communication is everywhere. Science and communication are not only not far away but also very close. Since machine learning algorithms with deep learning as a theme have achieved great success in the fields of vision and speech recognition, as well as the large amount of data resources that cloud computing, big data, and other technologies can provide, the development speed of artificial intelligence has been greatly improved, and it has had a significant impact in various industries in the society, and the country has put forward the concept of intelligent education for this purpose. However, there have been few systematic discussions on the combination of artificial intelligence with education and teaching. Therefore, this article uses artificial intelligence technology to study the potential energy space of artificial intelligence technology in college education reform from the perspective of science communication, designs and implements an online education platform for colleges and universities, and conducts a trial of platform use in a domestic college and universities. Some teachers and students conduct a satisfaction survey after the platform is used, and the conclusions show that whether in the teacher group or the student group, most teachers and students are relatively satisfied with the online education platform designed in this article. The reform of college education includes many aspects. This article is a research study on the form of college education, changing from traditional offline education to online platform education. This research can provide a certain reference for the reform of college education.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {7}
}

@article{10.1145/3437479.3437485,
author = {Yoo, Shin and Aleti, Aldeida and Turhan, Burak and Minku, Leandro L. and Miranskyy, Andriy and Meri\c{c}li, \c{C}etin},
title = {The 8th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3437479.3437485},
doi = {10.1145/3437479.3437485},
abstract = {The International Workshop on Realizing Arti cial Intelligence Synergies in Software Engineering (RAISE) aims to present the state of the art in the crossover between Software Engineering and Arti cial Intelligence. This workshop explored not only the appli- cation of AI techniques to SE problems but also the application of SE techniques to AI problems. Software has become critical for realizing functions central to our society. For example, software is essential for nancial and transport systems, energy generation and distribution systems, and safety-critical medical applications. Software development costs trillions of dollars each year yet, still, many of our software engineering methods remain mostly man- ual. If we can improve software production by smarter AI-based methods, even by small margins, then this would improve a crit- ical component of the international infrastructure, while freeing up tens of billions of dollars for other tasks.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {23–24},
numpages = {2}
}

@inproceedings{10.1145/1808937.1808942,
author = {Estublier, Jacky and Dieng, Idrissa A. and Leveque, Thomas},
title = {Software product line evolution: the Selecta system},
year = {2010},
isbn = {9781605589688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808937.1808942},
doi = {10.1145/1808937.1808942},
abstract = {The current technology gives little room for the different kinds of evolution needed for any software product line (SPL): evolution of the associated engineering environment, evolution of the market and SPL scope, evolution of the products and variability. The paper describes how these different evolution needs are addressed in the CADSE and Selecta systems. The solution we propose uses metamodeling and generation for the engineering environment evolution, composition for scope and market evolution, a component database and a selection language for the product and variability evolution. The paper presents the Selecta system and shortly discusses the experience.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Product Line Approaches in Software Engineering},
pages = {32–39},
numpages = {8},
keywords = {IDE, evolution, product families, product lines, software environments},
location = {Cape Town, South Africa},
series = {PLEASE '10}
}

@inproceedings{10.1007/978-3-642-33176-3_7,
author = {ter Beek, Maurice H. and Muccini, Henry and Pelliccione, Patrizio},
title = {Assume-guarantee testing of evolving software product line architectures},
year = {2012},
isbn = {9783642331756},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33176-3_7},
doi = {10.1007/978-3-642-33176-3_7},
abstract = {Despite some work on testing software product lines, maintaining the quality of products when a software product line evolves is still an open problem. In this paper, we propose a novel assume-guarantee testing approach as a solution to the following research question: how can we verify the correct functioning of products of an software product line when core components evolve? The underlying idea is to retest only some of the products that conform to the software product line architecture and to infer, using assume-guarantee reasoning, the correctness of the other products. Assume-guarantee reasoning moreover permits the retesting of only those components that are affected by the changes.},
booktitle = {Proceedings of the 4th International Conference on Software Engineering for Resilient Systems},
pages = {91–105},
numpages = {15},
keywords = {assume-guarantee testing, compositional verification, evolving software product lines, software testing},
location = {Pisa, Italy},
series = {SERENE'12}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@article{10.1016/j.compbiomed.2021.104354,
author = {Makridis, Christos A. and Zhao, David Y. and Bejan, Cosmin A. and Alterovitz, Gil},
title = {Leveraging machine learning to characterize the role of socio-economic determinants on physical health and well-being among veterans},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104354},
doi = {10.1016/j.compbiomed.2021.104354},
journal = {Comput. Biol. Med.},
month = jun,
numpages = {8},
keywords = {Health informatics, Machine learning, Subjective well-being, Socioeconomics, Veterans}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@article{10.1016/j.cose.2017.09.012,
author = {Anisetti, M. and Ardagna, C.A. and Damiani, E. and El Ioini, N. and Gaudenzi, F.},
title = {Modeling time, probability, and configuration constraints for continuous cloud service certification},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {72},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2017.09.012},
doi = {10.1016/j.cose.2017.09.012},
abstract = {Cloud computing proposes a paradigm shift where resources and services are allocated, provisioned, and accessed at runtime and on demand. New business opportunities emerge for service providers and their customers, at a price of an increased uncertainty on how their data are managed and their applications operate once stored/deployed in the cloud. This scenario calls for assurance solutions that formally assess the working of the cloud and its services/processes. Current assurance techniques increasingly rely on model-based verification, but fall short to provide sound checks on the validity and correctness of their assessment over time. The approach in this paper aims to close this gap catching unexpected behaviors emerging when a verified service is deployed in the target cloud. We focus on certification-based assurance techniques, which provide customers with verifiable and formal evidence on the behavior of cloud services/processes. We present a trustworthy cloud certification scheme based on the continuous verification of model correctness against real and synthetic service execution traces, according to time, probability, and configuration constraints, and attack flows. We test the effectiveness of our approach in a real scenario involving ATOS SA eHealth application deployed on top of open source IaaS OpenStack.},
journal = {Comput. Secur.},
month = jan,
pages = {234–254},
numpages = {21},
keywords = {Assurance, Certification, Cloud, Compliance, Security}
}

@inproceedings{10.1007/978-3-030-26250-1_32,
author = {Robin, Jacques and Mazo, Raul and Madeira, Henrique and Barbosa, Raul and Diaz, Daniel and Abreu, Salvador},
title = {A Self-certifiable Architecture for Critical Systems Powered by Probabilistic Logic Artificial Intelligence},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_32},
doi = {10.1007/978-3-030-26250-1_32},
abstract = {We present a versatile architecture for AI-powered self-adaptive self-certifiable critical systems. It aims at supporting semi-automated low-cost re-certification for self-adaptive systems after each adaptation of their behavior to a persistent change in their operational environment throughout their lifecycle.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {391–397},
numpages = {7},
keywords = {AI certification, Autonomic architecture, Argumentation, Rule-based constraint solving, Probabilistic logic machine learning},
location = {Turku, Finland}
}

@inproceedings{10.1109/CLOUD.2014.104,
author = {Gherardi, Luca and Hunziker, Dominique and Mohanarajah, Gajamohan},
title = {A Software Product Line Approach for Configuring Cloud Robotics Applications},
year = {2014},
isbn = {9781479950638},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CLOUD.2014.104},
doi = {10.1109/CLOUD.2014.104},
abstract = {The computational requirements of the increasingly sophisticated algorithms used in today's robotics software applications have outpaced the onboard processors of the average robot. Furthermore, the development and configuration of these applications are difficult tasks that require expertise in diverse domains, including software engineering, control engineering, and computer vision. As a solution to these problems, this paper extends and integrates our previous works, which are based on two promising techniques: Cloud Robotics and Software Product Lines. Cloud Robotics provides a powerful and scalable environment to offload the computationally expensive algorithms resulting in low-cost processors and light-weight robots. Software Product Lines allow the end user to deploy and configure complex robotics applications without dealing with low-level problems such as configuring algorithms and designing architectures. This paper discusses the proposed method in depth, and demonstrates its advantages with a case study.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Cloud Computing},
pages = {745–752},
numpages = {8},
keywords = {Cloud Computing, Robotics, Software Product Lines},
series = {CLOUD '14}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@article{10.1007/s11219-011-9156-5,
author = {Roos-Frantz, Fabricia and Benavides, David and Ruiz-Cort\'{e}s, Antonio and Heuer, Andr\'{e} and Lauenroth, Kim},
title = {Quality-aware analysis in product line engineering with the orthogonal variability model},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9156-5},
doi = {10.1007/s11219-011-9156-5},
abstract = {Software product line engineering is about producing a set of similar products in a certain domain. A variability model documents the variability amongst products in a product line. The specification of variability can be extended with quality information, such as measurable quality attributes (e.g., CPU and memory consumption) and constraints on these attributes (e.g., memory consumption should be in a range of values). However, the wrong use of constraints may cause anomalies in the specification which must be detected (e.g., the model could represent no products). Furthermore, based on such quality information, it is possible to carry out quality-aware analyses, i.e., the product line engineer may want to verify whether it is possible to build a product that satisfies a desired quality. The challenge for quality-aware specification and analysis is threefold. First, there should be a way to specify quality information in variability models. Second, it should be possible to detect anomalies in the variability specification associated with quality information. Third, there should be mechanisms to verify the variability model to extract useful information, such as the possibility to build a product that fulfils certain quality conditions (e.g., is there any product that requires less than 512 MB of memory?). In this article, we present an approach for quality-aware analysis in software product lines using the orthogonal variability model (OVM) to represent variability. We propose to map variability represented in the OVM associated with quality information to a constraint satisfaction problem and to use an off-the-shelf constraint programming solver to automatically perform the verification task. To illustrate our approach, we use a product line in the automotive domain which is an example that was created in a national project by a leading car company. We have developed a prototype tool named FaMa-OVM, which works as a proof of concepts. We were able to identify void models, dead and false optional elements, and check whether the product line example satisfies quality conditions.},
journal = {Software Quality Journal},
month = sep,
pages = {519–565},
numpages = {47},
keywords = {Automated analysis, Orthogonal variability model, Quality modelling, Quality-aware analysis, Software product lines}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.jss.2007.12.797,
author = {Ajila, Samuel A. and Kaba, Ali B.},
title = {Evolution support mechanisms for software product line process},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {10},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.12.797},
doi = {10.1016/j.jss.2007.12.797},
abstract = {Software product family process evolution needs specific support for incremental change. Product line process evolution involves in addition to identifying new requirements the building of a meta-process describing the migration from the old process to the new one. This paper presents basic mechanisms to support software product line process evolution. These mechanisms share four strategies - change identification, change impact, change propagation, and change validation. It also examines three kinds of evolution processes - architecture, product line, and product. In addition, change management mechanisms are identified. Specifically we propose support mechanisms for static local entity evolution and complex entity evolution including transient evolution process. An evolution model prototype based on dependency relationships structure of the various product line artifacts is developed.},
journal = {J. Syst. Softw.},
month = oct,
pages = {1784–1801},
numpages = {18},
keywords = {Feature-based object oriented model, Meta-process, Product line architecture, Software development process, Software product line process evolution, Transient process, Use case modeling}
}

@inproceedings{10.1145/3084226.3084276,
author = {Zhou, Shulin and Li, Shanshan and Liu, Xiaodong and Xu, Xiangyang and Zheng, Si and Liao, Xiangke and Xiong, Yun},
title = {Easier Said Than Done: Diagnosing Misconfiguration via Configuration Constraints Analysis: A Study of the Variance of Configuration Constraints in Source Code},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084276},
doi = {10.1145/3084226.3084276},
abstract = {Misconfigurations have drawn tremendous attention for their increasing prevalence and severity, and the main causes are the complexity of configurations as well as the lack of domain knowledge for software. To diagnose misconfigurations, one typical approach is to find out the conditions that configuration options should satisfy, which we refer to as configuration constraints. Current researches only handled part of the situations of configuration constraints in source code, which provide only limited help for misconfiguration diagnosis. To better extract configuration constraints, we conduct a comprehensive manual study on the existence and variance of the configuration constraints in source code from five pieces of popular open-source software. We summarized several findings from different aspects, including the general statistics about configuration constraints, the general features for specific configurations, and the obstacles in extraction of configuration constraints. Based on the findings, we propose several suggestions to maximize the automation of constraints extraction.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {196–201},
numpages = {6},
keywords = {Misconfiguration, configuration constraints, misconfiguration diagnosis},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@inproceedings{10.1145/2934466.2946047,
author = {Li, Li and Martinez, Jabier and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Traon, Yves Le},
title = {Mining families of android applications for extractive SPL adoption},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946047},
doi = {10.1145/2934466.2946047},
abstract = {The myriads of smart phones around the globe gave rise to a vast proliferation of mobile applications. These applications target an increasing number of user profiles and tasks. In this context, Android is a leading technology for their development and on-line markets are the main means for their distribution. In this paper we motivate, from two perspectives, the mining of these markets with the objective to identify families of apps variants in the wild. The first perspective is related to research activities where building realistic case studies for evaluating extractive SPL adoption techniques are needed. The second is related to a large-scale, world-wide and time-aware study of reuse practice in an industry which is now flourishing among all others within the software engineering community. This study is relevant to assess potential for SPLE practices adoption. We present initial implementations of the mining process and we discuss analyses of variant families.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {271–275},
numpages = {5},
keywords = {android, appvariants, mining software repositories, reverse engineering, software product line engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/1858996.1859021,
author = {Kim, Chang Hwan Peter and Batory, Don and Khurshid, Sarfraz},
title = {Eliminating products to test in a software product line},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859021},
doi = {10.1145/1858996.1859021},
abstract = {A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Developing a set of programs with commonalities and variabilities in this way can significantly reduce both the time and cost of software development. However, as the number of programs may be exponential in the number of features, testing an SPL, the phase to which the majority of software development is dedicated, becomes especially challenging [12].Indeed, scale is the biggest challenge in testing or checking the properties of programs in a product line. Even a product line with just 10 optional features has over a thousand (210) distinct programs. As an example of a situation where every program must be considered, suppose that every program of an SPL outputs a String that each feature might modify.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {139–142},
numpages = {4},
keywords = {feature oriented programming, software product lines, static analysis, testing},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {artificial intelligence, binary decision diagrams, configurable system, decision models, feature models, knownledge compilation, product configuration, satisfiability solving, software configuration, software product line},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1155/2019/4368036,
author = {Deli\'{c}, Vlado and Peri\'{c}, Zoran and Se\v{c}ujski, Milan and Jakovljevi\'{c}, Nik\v{s}a and Nikoli\'{c}, Jelena and Mi\v{s}kovi\'{c}, Dragi\v{s}a and Simi\'{c}, Nikola and Suzi\'{c}, Sini\v{s}a and Deli\'{c}, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.5555/1732003.1732062,
author = {Chiang, Chia-Chu and Marshall, Bill},
title = {Applying software product line technology to prototyping of real-time object tracking},
year = {2009},
isbn = {9781424427932},
publisher = {IEEE Press},
abstract = {In this paper, a software product line (SPL) architecture that explicitly captures common features of real-time object tracking systems using Cricket wireless sensors is presented. A software product line process is also presented including user requirements, architecture design, component development, and systems integration. The focus lies on the application of SPL to object location tracking, such as supply chains and transportation. Thus, this paper introduces three prototypes of the SPL member system including shop navigator systems, low/no visibility navigation systems (LVNS), and games for mazes where each one is created from the software product line architecture. The prototypes are experimented with and lessons are learned.},
booktitle = {Proceedings of the 2009 IEEE International Conference on Systems, Man and Cybernetics},
pages = {2099–2104},
numpages = {6},
keywords = {cricket, incremental development, indoor positioning, object tracking, rapid prototyping, software architectural styles, software architecture, software product line (SPL)},
location = {San Antonio, TX, USA},
series = {SMC'09}
}

@article{10.1145/2000799.2000803,
author = {Dehlinger, Josh and Lutz, Robyn R.},
title = {Gaia-PL: A Product Line Engineering Approach for Efficiently Designing Multiagent Systems},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000799.2000803},
doi = {10.1145/2000799.2000803},
abstract = {Agent-oriented software engineering (AOSE) has provided powerful and natural, high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of AOSE partially depends on whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious development methods. Specifically, AOSE does not adequately address requirements specifications as reusable assets. Software product line engineering is a reuse technology that supports the systematic development of a set of similar software systems through understanding, controlling, and managing their common, core characteristics and their differing variation points. In this article, we present an extension to the Gaia AOSE methodology, named Gaia-PL (Gaia-Product Line), for agent-based distributed software systems that enables requirements specifications to be easily reused. We show how our methodology uses a product line perspective to promote reuse in agent-based software systems early in the development life cycle so that software assets can be reused throughout system development and evolution. We also present results from an application to show how Gaia-PL provided reuse that reduced the design and development effort for a large, multiagent system.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {17},
numpages = {27},
keywords = {Agent-oriented software engineering, software product line engineering}
}

@inproceedings{10.1145/1985441.1985458,
author = {Krishnan, Sandeep and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Empirical evaluation of reliability improvement in an evolving software product line},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985458},
doi = {10.1145/1985441.1985458},
abstract = {Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), highreuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {103–112},
numpages = {10},
keywords = {change, failures, reliability, reuse, software product lines},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.5555/2004685.2005507,
author = {Engstr\"{o}m, Emelie and Runeson, Per},
title = {Decision Support for Test Management and Scope Selection in a Software Product Line Context},
year = {2011},
isbn = {9780769543451},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In large software organizations with a product line development approach, system test planning and scope selection is a complex tasks for which tool support is needed. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of double testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. This paper discusses the need and challenges of providing decision support for test planning and test selection in a product line context, and highlights possible paths towards a pragmatic implementation of context-specific decision support of various levels of automation. With existing regression testing approaches it is possible to provide automated decision support in a few specific cases, while test management in general may be supported through visualization of test execution coverage, the testing space and the delta between the sufficiently tested system and the system under test. A better understanding of the real world context and how to map research results to the same is needed.},
booktitle = {Proceedings of the 2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
pages = {262–265},
numpages = {4},
keywords = {decision support, regression testing, software product line testing, test coverage, test selection, visualization},
series = {ICSTW '11}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Graph Laplacian, Nearest neighborhood graph, Self-paced learning, Semi-supervised learning}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Class activation mapping, Convolutional neural network, Sentiment analysis, Weakly supervised learning, Word localization}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Feature Extraction, Feature Terms Identification, Requirement Documents, Software Product Lines},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@inproceedings{10.1145/3236405.3237205,
author = {Ziadi, Tewfik and Martinez, Jabier and T\"{e}rnava, Xhevahire},
title = {Teaching projects and research objectives in SPL extraction},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3237205},
doi = {10.1145/3236405.3237205},
abstract = {This year at SPLC we present a teaching and research project where a group of master students analysed a variability-rich domain and extracted an SPL (The Robocode SPL). We present the results of such extraction augmented with an analysis and a quantification regarding the time and effort spent. The research objective was to get and share data about an end-to-end SPL extraction which is usually unavailable in industrial cases because of their large size, complexity, and duration. We provide all the material to replicate, reproduce or extend the case study so it can be easily reused for teaching by anyone in our community. However, we were asking ourselves how can we leverage such case study for teaching to pursue research objectives. In this position paper, we aim to outline our initial ideas that we want to enrich with the others' viewpoints during SPLTea. Towards planning the settings of future teaching projects around this Robocode SPL case study, which can be the timely research objectives that we can identify? Can we involve others in planning this project in their institutions to get further relevant results?},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {44–45},
numpages = {2},
keywords = {extractive software product line adoption, reverse-engineering, software product lines, teaching},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3474301,
author = {Schmid, Klaus and Rabiser, Rick and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Bridging the gap: voices from industry and research on industrial relevance of SPLC},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3474301},
doi = {10.1145/3461001.3474301},
abstract = {Product line engineering emerged from a fruitful interaction of applied research in academia, industry research, and software engineering practice. SPLC was created as the primary venue to exchange ideas on this emerging topic and integrate the communities. Yet, today, SPLC is mostly regarded as an academic conference with little industry participation. Since a strong integration of academia and industry is often seen positive, here, we try to better understand motivations for practitioners to visit academic conferences like SPLC and the impact this has on such conferences. This analysis is based on nine systematic interviews with practitioners and researchers, who have been members of the SPLC community and other leading software engineering communities for a long time. Our preliminary results clarify the relevance and interest of practitioners and researchers to exchange knowledge and learn when attending scientific software engineering conferences such as SPLC. Yet, the results also highlight the differences between the goals of industry and academic conference participants, which often lead to inefficiencies and even barriers for constructive interaction at scientific conferences such as SPLC. We use this as a basis for pointing out further discussion points, both from the perspective of the interviewees as well as the authors.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {184–189},
numpages = {6},
keywords = {SPLC, industry participation, industry-academia relations, scientific software engineering conferences, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {complex software product line, relational concept analysis, reverse engineering, variability extraction},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.5555/2567709.2502615,
author = {Thom, Markus and Palm, G\"{u}nther},
title = {Sparse activity and sparse connectivity in supervised learning},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches.},
journal = {J. Mach. Learn. Res.},
month = apr,
pages = {1091–1143},
numpages = {53},
keywords = {sparse activity, sparse connectivity, sparseness projection, supervised learning}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {DSML, SPL, methodology, software factory, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.eswa.2014.08.046,
author = {Kazemian, H.B. and Ahmed, S.},
title = {Comparisons of machine learning techniques for detecting malicious webpages},
year = {2015},
issue_date = {February 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.08.046},
doi = {10.1016/j.eswa.2014.08.046},
abstract = {3 supervised and 2 unsupervised techniques are modeled to detect malicious webpages.Supervised machine learning (ML) techniques accuracies are above 89%.Unsupervised ML techniques accuracies have at least a silhouette coefficient of 0.87.Information obtained from URLs, page links, semantics and visual features of webpages.Chrome extension, lightweight and heavyweight classifiers and online learning are used. This paper compares machine learning techniques for detecting malicious webpages. The conventional method of detecting malicious webpages is going through the black list and checking whether the webpages are listed. Black list is a list of webpages which are classified as malicious from a user's point of view. These black lists are created by trusted organizations and volunteers. They are then used by modern web browsers such as Chrome, Firefox, Internet Explorer, etc. However, black list is ineffective because of the frequent-changing nature of webpages, growing numbers of webpages that pose scalability issues and the crawlers' inability to visit intranet webpages that require computer operators to log in as authenticated users. In this paper therefore alternative and novel approaches are used by applying machine learning algorithms to detect malicious webpages. In this paper three supervised machine learning techniques such as K-Nearest Neighbor, Support Vector Machine and Naive Bayes Classifier, and two unsupervised machine learning techniques such as K-Means and Affinity Propagation are employed. Please note that K-Means and Affinity Propagation have not been applied to detection of malicious webpages by other researchers. All these machine learning techniques have been used to build predictive models to analyze large number of malicious and safe webpages. These webpages were downloaded by a concurrent crawler taking advantage of gevent. The webpages were parsed and various features such as content, URL and screenshot of webpages were extracted to feed into the machine learning models. Computer simulation results have produced an accuracy of up to 98% for the supervised techniques and silhouette coefficient of close to 0.96 for the unsupervised techniques. These predictive models have been applied in a practical context whereby Google Chrome can harness the predictive capabilities of the classifiers that have the advantages of both the lightweight and the heavyweight classifiers.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {1166–1177},
numpages = {12},
keywords = {Affinity Propagation, K-Means, K-Nearest Neighbor, Naive Bayes, Supervised and unsupervised learning, Support Vector Machine}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {feature extraction, requirement documents, reverse engineering, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1007/s10846-016-0449-6,
author = {Liu, Weihui and Chen, Diansheng and Steil, Jochen},
title = {Analytical Inverse Kinematics Solver for Anthropomorphic 7-DOF Redundant Manipulators with Human-Like Configuration Constraints},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {86},
number = {1},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-016-0449-6},
doi = {10.1007/s10846-016-0449-6},
abstract = {It is a common belief that service robots shall move in a human-like manner to enable natural and convenient interaction with a human user or collaborator. In particular, this applies to anthropomorphic 7-DOF redundant robot manipulators that have a shoulder-elbow-wrist configuration. On the kinematic level, human-like movement then can be realized by means of selecting a redundancy resolution for the inverse kinematics (IK), which realizes human-like movement through respective nullspace preferences. In this paper, key positions are introduced and defined as Cartesian positions of the manipulator's elbow and wrist joints. The key positions are used as constraints on the inverse kinematics in addition to orientation constraints at the end-effector, such that the inverse kinematics can be calculated through an efficient analytical scheme and realizes human-like configurations. To obtain suitable key positions, a correspondence method named wrist-elbow-in-line is derived to map key positions of human demonstrations to the real robot for obtaining a valid analytical inverse kinematics solution. A human demonstration tracking experiment is conducted to evaluate the end-effector accuracy and human-likeness of the generated motion for a 7-DOF Kuka-LWR arm. The results are compared to a similar correspondance method that emphasizes only the wrist postion and show that the subtle differences between the two different correspondence methods may lead to significant performance differences. Furthermore, the wrist-elbow-in-line method is validated as more stable in practical application and extended for obstacle avoidance.},
journal = {J. Intell. Robotics Syst.},
month = apr,
pages = {63–79},
numpages = {17},
keywords = {Correspondance problem, Human-like motion, Inverse kinematics, Redundancy resolution}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Software Product Line (SPL) adoption, Taxonomy-Based Software Construction (TABASCO) toolkit},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1007/978-3-540-88582-5_50,
author = {Jiang, Michael and Zhang, Jing and Zhao, Hong and Zhou, Yuanyuan},
title = {Enhancing Software Product Line Maintenance with Source Code Mining},
year = {2008},
isbn = {9783540885818},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-88582-5_50},
doi = {10.1007/978-3-540-88582-5_50},
abstract = {Large-scale reuse and accelerated software development have been some of the key attractions behind software product lines. Various strategies and processes have been developed to facilitate product line development, maintenance, and evolution. However, experiences with software product lines also showed that it is a rather challenging task to maintain software product lines and families over a long period of time. The time and effort needed to manage and maintain product lines increase and quality degrades as product lines evolve. Without proper methods and tools to support the evolution, the cost can outweigh the benefits.This paper describes an approach to simplifying the maintenance of software product lines and improving software quality by integrating traditional software maintenance practices with pattern-based source code mining for defect detection and correction. Our case studies were performed in an industrial setting where the evolution of multiple mobile phone models of a product line was investigated.},
booktitle = {Proceedings of the Third International Conference on Wireless Algorithms, Systems, and Applications},
pages = {538–547},
numpages = {10},
keywords = {Product Line, Reuse, Software Maintenance},
location = {Dallas, Texas},
series = {WASA '08}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3472729,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard Paul},
title = {Requirements-driven reuse recommendation},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472729},
doi = {10.1145/3461001.3472729},
abstract = {This tutorial explores requirements-based reuse recommendation for product line assets in the context of clone-and-own product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210},
numpages = {1},
keywords = {SPL adoption, similarity, software reuse},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2020390.2020397,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Are change metrics good predictors for an evolving software product line?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020397},
doi = {10.1145/2020390.2020397},
abstract = {Background: Previous research on three years of early data for an Eclipse product identified some predictors of failure-prone files that work well for that data set. Additionally, Eclipse has been used to explore characteristics of product line software in previous research.Aims: To assess whether change metrics are good predictors of failure-prone files over time for the family of products in the evolving Eclipse product line.Method: We repeat, to the extent possible, the decision tree portion of the prior study to assess our ability to replicate the method, and then extend it by including four more recent years of data. We compare the most prominent predictors with the previous study's results. We then look at the data for three additional Eclipse products as they evolved over time. We explore whether the set of good predictors change over time for one product and whether the set differs among products.Results: We find that change metrics are consistently good and incrementally better predictors across the evolving products in Eclipse. There is also some consistency regarding which change metrics are the best predictors.Conclusion: Change metrics are good predictors for failure-prone files for the Eclipse product line. A small subset of these change metrics is fairly stable and consistent across products and releases.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {change metrics, failure-prone files, post-release defects, prediction, reuse, software product lines},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/3461001.3474452,
author = {He\ss{}, Tobias and Sundermann, Chico and Th\"{u}m, Thomas},
title = {On the scalability of building binary decision diagrams for current feature models},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3474452},
doi = {10.1145/3461001.3474452},
abstract = {Binary decision diagrams (BDD) have been proposed for numerous product-line analyses. These analyses typically exploit properties unique to decision diagrams, such as negation in constant time and space. Furthermore, the existence of a BDD representing the configuration space of a product line removes the need to employ SAT or #SAT solvers for their analysis. Recent work has shown that the performance of state-of-the-art BDD libraries is significantly lower than previously reported and hypothesized. In this work, we provide an assessment of the state-of-the-art of BDD scalability in this domain and explain why previous results on the scalability of BDDs do not apply to more recent product-line instances.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {131–135},
numpages = {5},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1016/j.procs.2018.05.082,
author = {Hitesh and Kumari, A. Charan},
title = {Feature Selection Optimization in SPL using Genetic Algorithm},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.082},
doi = {10.1016/j.procs.2018.05.082},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1477–1486},
numpages = {10},
keywords = {Software product line, Genetic Algorithm, Feature Model, Software Product Line Engineering}
}

@article{10.1016/j.cmpb.2018.06.010,
author = {Akbulut, Akhan and Ertugrul, Egemen and Topcu, Varol},
title = {Fetal health status prediction based on maternal clinical history using machine learning techniques},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {163},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2018.06.010},
doi = {10.1016/j.cmpb.2018.06.010},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
pages = {87–100},
numpages = {14},
keywords = {Machine learning, Medical diagnosis, Risk prediction, Pregnancy, Fetal health, Prognosis, m-Health}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {SAT, benchmark, configurable systems, sampling, software product lines, variability model},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {ArgoUML SPL benchmark, dynamic feature localization, spectrum-based localization},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@inproceedings{10.1145/2499777.2500717,
author = {Lee, Jaejoon},
title = {Dynamic feature deployment and composition for dynamic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500717},
doi = {10.1145/2499777.2500717},
abstract = {We aim to tackle problems with feature interoperability in dynamic software product lines: a feature allows collaborations with other features not conceived when it is deployed. In this position paper, we propose a Dynamic Feature Deployment (DFD) idea, which is a model-driven approach to support seamless integration of new features and changes of product configuration at runtime. The approach is based on a feature-modelling technique that directly deals with flexibility of reusable software assets in software product line engineering. We also propose a Hybrid between Passive/Active Behaviours (Hy-PAB) architecture model to support two extreme sets of behaviours for DFD: an active coordinating behaviour that controls the interactions with other features, and a passive subordinating behaviour that allows other features to control their interactions with other features.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {114–116},
numpages = {3},
keywords = {dynamic software product line, feature interoperability, feature modelling, smart home systems, software architecture},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/978-3-642-04211-9_23,
author = {Nunes, Ingrid and Lucena, Carlos J. and Cowan, Donald and Alencar, Paulo},
title = {Building Service-Oriented User Agents Using a Software Product Line Approach},
year = {2009},
isbn = {9783642042102},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-04211-9_23},
doi = {10.1007/978-3-642-04211-9_23},
abstract = {This paper presents an approach to develop service-oriented user agents using the Software Product LineSPL engineering paradigm. The approach comprises activities and models to support building service-oriented customized agents that automate user tasks based on service orchestration involving multiple agents in open environments, and takes advantage of the synergy of Service-oriented ArchitectureSOA, Multi-agent SystemMAS and Software Product LineSPL. The domain-based process involves extended domain analysis with goals and variability, domain design with the specification of agent services and plans, and domain implementation.},
booktitle = {Proceedings of the 11th International Conference on Software Reuse: Formal Foundations of Reuse and Domain Engineering},
pages = {236–245},
numpages = {10},
keywords = {Multi-agent Systems, Personalization, Service-oriented Architectures, Software Product Lines, User Agents},
location = {Falls Church, Virginia},
series = {ICSR '09}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {artificial neural networks, empirical study, reusability, survey, systematic reuse},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {configuration, evaluation, feature models, recommender systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {AutoML, NAS, configuration search, feature model, neural architecture search},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-55789-8_59,
author = {Abeyrathna, Kuruge Darshana and Granmo, Ole-Christoffer and Goodwin, Morten},
title = {Integer Weighted Regression Tsetlin Machines},
year = {2020},
isbn = {978-3-030-55788-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55789-8_59},
doi = {10.1007/978-3-030-55789-8_59},
abstract = {The Regression Tsetlin Machine (RTM) addresses the lack of interpretability impeding state-of-the-art nonlinear regression models. It does this by using conjunctive clauses in propositional logic to capture the underlying non-linear frequent patterns in the data. These, in turn, are combined into a continuous output through summation, akin to a linear regression function, however, with non-linear components and binary weights. However, the resolution of the RTM output is proportional to the number of clauses employed. This means that computation cost increases with resolution. To address this problem, we here introduce integer weighted RTM clauses. Our integer weighted clause is a compact representation of multiple clauses that capture the same sub-pattern—w repeating clauses are turned into one, with an integer weight w. This reduces computation cost w times, and increases interpretability through a sparser representation. We introduce a novel learning scheme, based on so-called stochastic searching on the line. We evaluate the potential of the integer weighted RTM empirically using two artificial datasets. The results show that the integer weighted RTM is able to acquire on par or better accuracy using significantly less computational resources compared to regular RTM and an RTM with real-valued weights.},
booktitle = {Trends in Artificial Intelligence Theory and Applications. Artificial Intelligence Practices: 33rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2020, Kitakyushu, Japan, September 22-25, 2020, Proceedings},
pages = {686–694},
numpages = {9},
keywords = {Tsetlin machines, Regression tsetlin machines, Weighted tsetlin machines, Interpretable machine learning, Stochastic searching on the line},
location = {Kitakyushu, Japan}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {consumption, energy, measurement, mitigation, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {active automata learning, featured transition systems, model learning, software product lines, variability mining},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.5555/1885639.1885642,
author = {Bagheri, Ebrahim and Di Noia, Tommaso and Ragone, Azzurra and Gasevic, Dragan},
title = {Configuring software product line feature models based on Stakeholders' soft and hard requirements},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature modeling is a technique for capturing commonality and variability. Feature models symbolize a representation of the possible application configuration space, and can be customized based on specific domain requirements and stakeholder goals. Most feature model configuration processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called P(N). Furthermore, we formalize the representation of soft constraints in fuzzy P(N) and explain how semi-automated feature model configuration is performed. The model configuration derivation process that we propose respects the soundness and completeness properties.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {16–31},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/1147249.1147254,
author = {Fischbein, Dario and Uchitel, Sebastian and Braberman, Victor},
title = {A foundation for behavioural conformance in software product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147254},
doi = {10.1145/1147249.1147254},
abstract = {Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {39–48},
numpages = {10},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/3233027.3233033,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse engineering variability from requirement documents based on probabilistic relevance and word embedding},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233033},
doi = {10.1145/3233027.3233033},
abstract = {Feature and variability extraction from different artifacts is an indispensable activity to support systematic integration of single software systems and Software Product Line (SPL). Beyond manually extracting variability, a variety of approaches, such as feature location in source code and feature extraction in requirements, has been proposed to provide an automatic identification of features and their variation points. Compared with source code, requirements contain more complete variability information and provide traceability links to other artifacts from early development phases. In this paper, we propose a method to automatically extract features and relationships based on a probabilistic relevance and word embedding. In particular, our technique consists of three steps: First, we apply word2vec to obtain a prediction model, which we use to determine the word level similarity of requirements. Second, based on word level similarity and the significance of a word in a domain, we compute the requirements level similarity using probabilistic relevance. Third, we adopt hierarchical clustering to group features and we define four criteria to detect variation points between identified features. We perform a case study to evaluate the usability and robustness of our method and to compare it with the results of other related approaches. Initial results reveal that our approach identifies the majority of features correctly and also extracts variability information with reasonable accuracy.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {121–131},
numpages = {11},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233054,
author = {Kaindl, Hermann and Mannion, Mike},
title = {Software reuse and mass customisation: feature modelling vs. case-based reasoning},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233054},
doi = {10.1145/3233027.3233054},
abstract = {Several socio-economic trends are driving customer demands towards individualisation. Many suppliers are responding by offering supplier-led software product design customization choices ("mass customization"). Some are also offering customer-led software product design choices ("mass personalization"). This tutorial introduces these concepts and explores the implications for software product line development. One particular technical challenge is being able to respond to and manage at scale the increasing variety of common, supplier-led and customer-led features. We discuss two different approaches to address this challenge. One is grounded in feature modelling; the other in case-based reasoning. Both approaches aim to support the identification and selection of similar products. However they each place different emphases on these activities, use different product descriptions, and deploy different product derivation methods. Accordingly, each approach has different key properties, benefits and limitations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {304},
numpages = {1},
keywords = {case-based reasoning, feature model, mass customisation, reuse},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {constraints, dynamic product line models, product line engineering, simulation, tool, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {feature modelling, software product lines, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-87589-3_48,
author = {Machado Reyes, Diego and Chao, Hanqing and Homayounieh, Fatemeh and Hahn, Juergen and Kalra, Mannudeep K. and Yan, Pingkun},
title = {Cardiovascular Disease Risk Improves COVID-19 Patient Outcome Prediction},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_48},
doi = {10.1007/978-3-030-87589-3_48},
abstract = {The pandemic of coronavirus disease 2019 (COVID-19) has severely impacted the world. Several studies suggest an increased risk for COVID-19 patients with underlying cardiovascular diseases (CVD). However, it is challenging to quantify such risk factors and integrate them into patient condition evaluation. This paper presents machine learning methods to assess CVD risk scores from chest computed tomography together with laboratory data, demographics, and deep learning extracted lung imaging features to increase the outcome prediction accuracy for COVID-19 patients. The experimental results demonstrate an overall increase in prediction performance when the CVD severity score was added to the feature set. The machine learning methods obtained their best performance when all categories of the features were used for the patient outcome prediction. With the best attained area under the curve of 0.888, the presented research may assist physicians in clinical decision-making process on managing COVID-19 patients.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {467–476},
numpages = {10},
keywords = {COVID-19, Machine learning, Cardiovascular disease, Chest CT, Severity score},
location = {Strasbourg, France}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@article{10.1023/A:1018931617448,
author = {Dershowitz, Nachum},
title = {Artificial intelligence: Retrospective/prospective},
year = {2000},
issue_date = {2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1–4},
issn = {1012-2443},
url = {https://doi.org/10.1023/A:1018931617448},
doi = {10.1023/A:1018931617448},
abstract = {Symbolic programming and formal reasoning are two significant areas to which research in artificial intelligence has contributed much. The emergence of global repositories of accessible data presents new challenges for knowledge representation and logical inference.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = jan,
pages = {3–5},
numpages = {3}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3362789.3362923,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto},
title = {Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362923},
doi = {10.1145/3362789.3362923},
abstract = {Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1007–1011},
numpages = {5},
keywords = {Automatic generation, Domain engineering, High-level requirements, Information Dashboards, Meta-modeling},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@phdthesis{10.5555/1354508,
author = {Dehlinger, Joshua Jon},
advisor = {Lutz, Robyn R.},
title = {Incorporating product-line engineering techniques into agent-oriented software engineering for efficiently building safety-critical, multi-agent systems},
year = {2007},
isbn = {9780549154877},
publisher = {Iowa State University},
address = {USA},
abstract = {Safety-critical, agent-based systems are being developed without mechanisms and analysis techniques to discover, analyze and verify software requirements and prevent potential hazards. Agent-oriented, software-based approaches have provided powerful and natural high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of agent-oriented software development partially depends upon whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious software development methods. Further, agent-oriented software engineering (AOSE) currently does not adequately address: (1)&nbsp;requirements (specification) reuse in a way that is amenable to the reduction of the development cost by utilizing reusable assets, and (2)&nbsp;analysis techniques to evaluate safety. This dissertation offers our AOSE methodology, Gaia-PL (Gaia-Product Line) for open, agent-based distributed software systems to capture requirements specifications that can be easily reused. Our methodology uses a product-line perspective to promote reuse in agent-based, software systems early in the development lifecycle so that software assets can be reused throughout the development lifecycle and system evolution. The main contribution of this work is a requirements specification pattern that captures the dynamically changing design configurations of agents. Reuse is achieved by adopting a product-line approach into AOSE. Requirements specifications reuse is the ability to easily use previously defined requirements specifications from an earlier system and apply them to a new, slightly different system. This can significantly reduce the development time and cost of building an agent-based system.For safety-critical agent-based systems, this dissertation incorporates reuse-oriented safety analysis methods for AOSE to allow the discovery of new safety requirements and the verification that the design satisfies the safety requirements. Specifically, Product-Line Software Fault Tree Analysis (PL-SFTA) and its automated tool, PLFaultCAT (  P roduct-  L ine  Fault  Tree  C reation and  A nalysis  T ool), have been created to provide the technique and tool support for the safety analysis of safety-critical software product lines. The PL-SFTA allows for the identification of new safety requirements and the analysis of safety-critical requirements and requirement interactions. An AOSE-adapted Software Failure Modes, Effects and Criticality Analysis (SFMECA) technique has been created to support the derivation of a safety analysis asset using the specifications of Gaia-PL allowing for the identification of possible hazard scenarios and the failure points of specific agent roles. Using the assets generated via PL-SFTA and SFMECA, Bi-Directional Safety Analysis (BDSA) is shown to aid in the completeness of PL-SFTA and SFMECA, help verify the safety properties and strengthen the safety case when safety compliance to safety standards of the multi-agent system is necessary.Results from an application to a large, safety-critical, multi-agent system product-line show that Gaia-PL provides strong reuse capabilities. Evaluation of the Gaia-PL methodology used in conjunction with the PL-SFTA, SFMECA and BDSA safety analysis techniques shows that safety analysis of an agent-based software system is feasible, reusable and efficient.},
note = {AAI3274890}
}

@inproceedings{10.1145/3106195.3106201,
author = {Kim, Jongwook and Batory, Don and Dig, Danny},
title = {Refactoring Java Software Product Lines},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106201},
doi = {10.1145/3106195.3106201},
abstract = {Refactoring is a staple of Object-Oriented (OO) program development. It should be a staple of OO Software Product Line (SPL) development too. X15 is the first tool to support the refactoring of Java SPL codebases. X15 (1) uses Java custom annotations to encode variability in feature-based Java SPLs, (2) projects a view of an SPL product (a program that corresponds to a legal SPL configuration), and (3) allows programmers to edit and refactor the product, propagating changes back to the SPL codebase. Case studies apply 2316 refactorings in 8 public Java SPLs and show that X15 is as efficient, expressive, and scalable as a state-of-the-art feature-unaware Java refactoring engine.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {59–68},
numpages = {10},
keywords = {refactoring, software product lines},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {150% model, family model, model learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-89370-5_18,
author = {Tian, Yuze and Zhong, Xian and Liu, Wenxuan and Jia, Xuemei and Zhao, Shilei and Ye, Mang},
title = {Random Walk Erasing with Attention Calibration for Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_18},
doi = {10.1007/978-3-030-89370-5_18},
abstract = {Action recognition in videos has attracted growing research interests because of the explosive surveillance data in social security applications. In this process, due to the distraction and deviation of the network caused by occlusions, human action features usually suffer different degrees of performance degradation. Considering the occlusion scene in the wild, we find that the occluded objects usually move unpredictably but continuously. Thus, we propose a random walk erasing with attention calibration (RWEAC) for action recognition. Specifically, we introduce the random walk erasing (RWE) module to simulate the unknown occluded real conditions in frame sequence, expanding the diversity of data samples. In the case of erasing (or occlusion), the attention area is sparse. We leverage the attention calibration (AC) module to force the attention to stay stable in other regions of interest. In short, our novel RWEAC network enhances the ability to learn comprehensive features in a complex environment and make the feature representation robust. Experiments are conducted on the challenging video action recognition UCF101 and HMDB51 datasets. The extensive comparison results and ablation studies demonstrate the effectiveness and strength of the proposed method.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {236–251},
numpages = {16},
keywords = {Action recognition, Random walk erasing, Data augmentation, Attention calibration, Siamese network},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/3336294.3336315,
author = {Wolschke, Christian and Becker, Martin and Schneickert, S\"{o}ren and Adler, Rasmus and MacGregor, John},
title = {Industrial Perspective on Reuse of Safety Artifacts in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336315},
doi = {10.1145/3336294.3336315},
abstract = {In the future, safety-critical industrial products will have to be maintained and variants will have to be produced. In order to do this economically, the safety artifacts of the components should also be reused. At present, however, it is still unclear how this reuse could take place. Moreover this reuse is complicated, by the different situations in the various industries involved and by the corresponding standards applied.Current industrial practice for certification processes relies on a component-based view of reuse. We investigate the possibilities of product lines with managed processes for reuse also across multiple domains.In order to identify the challenges and possible solutions, we conducted interviews with industry partners from the domains of ICT, Rail, Automotive, and Industrial Automation, and from small- and medium-sized enterprises to large organizations. The semi-structured interviews identified the characteristics of current safety engineering processes, the handling of general variety and reuse, the approach followed for safety artifacts, and the need for improvement.In addition, a detailed literature survey summarizes existing approaches. We investigate which modularity concepts exist for dealing with safety, how variability concepts integrate safety, by which means process models can consider safety, and how safety cases are evolved while maintenance takes place. An overview of similar research projects complements the analysis.The identified challenges and potential solution proposals show how safety is related to Software Product Lines.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {143–154},
numpages = {12},
keywords = {modular safety, open source certification, product line certification, safety reuse, safety standards},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Software product line, Feature extraction, Information retrieval, Community detection}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {Family-based model checking, Software Product Lines, mCRL2},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3336295,
author = {Beek, Maurice H. ter and Damiani, Ferruccio and Lienhardt, Michael and Mazzanti, Franco and Paolini, Luca},
title = {Static Analysis of Featured Transition Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336295},
doi = {10.1145/3336294.3336295},
abstract = {A Featured Transition System (FTS) is a formal behavioural model for software product lines, which represents the behaviour of all the products of an SPL in a single compact structure by associating transitions with features that condition their existence in products. In general, an FTS may contain featured transitions that are unreachable in any product (so called dead transitions) or, on the contrary, mandatorily present in all products for which their source state is reachable (so called false optional transitions), as well as states from which only for certain products progress is possible (so called hidden deadlocks). In this paper, we provide algorithms to analyse an FTS for such ambiguities and to transform an ambiguous FTS into an unambiguous FTS. The scope of our approach is twofold. First and foremost, an ambiguous model is typically undesired as it gives an unclear idea of the SPL. Second, an unambiguous FTS paves the way for efficient family-based model checking. We apply our approach to illustrative examples from the literature.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {39–51},
numpages = {13},
keywords = {behavioural model, featured transition systems, formal specification, software product lines, static analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Feature Identification, Natural Language Documents, Reverse Engineering, Software Product Lines, Systematic Literature Review, Variability Extraction},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@inproceedings{10.1145/2934466.2934484,
author = {Vasilevskiy, Anatoly and Chauvel, Franck and Haugen, \O{}ystein},
title = {Toward robust product realisation in software product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934484},
doi = {10.1145/2934466.2934484},
abstract = {Product derivation is a building process of products from selected features in software product lines (SPLs). Realisation paves the way for automatic product derivation. A realisation defines a mapping between abstract features in a feature tree and their implementation artefacts in a model, and therefore governs the derivation of a new product. We experience that a realisation is not always straightforward and robust against modifications in the model. In the paper, we introduce an approach to build robust realisations. It consists of automated planning techniques and a layered architecture to yield a product. We demonstrate how our approach can leverage modern means of software design, development and validation. We evaluate the approach on a use-case provided by an industry partner and compare our technique to the existing realisation layer in the Base Variability Resolution (BVR) language.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {184–193},
numpages = {10},
keywords = {automated planning, bvr, fragment substitution, model, product derivation, product line, realisation, variation point},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647648.2647649,
author = {Raschke, Wolfgang and Zilli, Massimiliano and Loinig, Johannes and Weiss, Reinhold and Steger, Christian and Kreiner, Christian},
title = {Embedding research in the industrial field: a case of a transition to a software product line},
year = {2014},
isbn = {9781450330459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647648.2647649},
doi = {10.1145/2647648.2647649},
abstract = {Java Cards [4, 5] are small resource-constrained embedded systems that have to fulfill rigorous security requirements. Multiple application scenarios demand diverse product performance profiles which are targeted towards markets such as banking applications and mobile applications. In order to tailor the products to the customer's needs we implemented a Software Product Line (SPL). This paper reports on the industrial case of an adoption to a SPL during the development of a highly-secure software system. In order to provide a scientific method which allows the description of research in the field, we apply Action Research (AR). The rationale of AR is to foster the transition of knowledge from a mature research field to practical problems encountered in the daily routine. Thus, AR is capable of providing insights which might be overlooked in a traditional research approach. In this paper we follow the iterative AR process, and report on the successful transfer of knowledge from a research project to a real industrial application.},
booktitle = {Proceedings of the 2014 International Workshop on Long-Term Industrial Collaboration on Software Engineering},
pages = {3–8},
numpages = {6},
keywords = {action research, knowledge transfer, software reuse},
location = {Vasteras, Sweden},
series = {WISE '14}
}

@article{10.1016/j.artmed.2021.102162,
author = {Naranjo, Lizbeth and P\'{e}rez, Carlos J. and Campos-Roca, Yolanda and Madruga, Mario},
title = {Replication-based regularization approaches to diagnose Reinke's edema by using voice recordings},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102162},
doi = {10.1016/j.artmed.2021.102162},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {10},
keywords = {Acoustic features, Classification, Reinke's edema, Regularization, Replicated measurements, Variable selection}
}

@inproceedings{10.1145/2364412.2364422,
author = {Damiani, Ferruccio and Owe, Olaf and Dovland, Johan and Schaefer, Ina and Johnsen, Einar Broch and Yu, Ingrid Chieh},
title = {A transformational proof system for delta-oriented programming},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364422},
doi = {10.1145/2364412.2364422},
abstract = {Delta-oriented programming is a modular, yet flexible technique to implement software product lines. To efficiently verify the specifications of all possible product variants of a product line, it is usually infeasible to generate all product variants and to verify them individually. To counter this problem, we propose a transformational proof system in which the specifications in a delta module describe changes to previous specifications. Our approach allows each delta module to be verified in isolation, based on symbolic assumptions for calls to methods which may be in other delta modules. When product variants are generated from delta modules, these assumptions are instantiated by the actual guarantees of the methods in the considered product variant and used to derive the specifications of this product variant.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {53–60},
numpages = {8},
keywords = {program verification, proof system, software product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {case-based reasoning, commonality and variability, feature-based representation, product lines, similarity metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2648511.2648526,
author = {Acher, Mathieu and Baudry, Benoit and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Customization and 3D printing: a challenging playground for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648526},
doi = {10.1145/2648511.2648526},
abstract = {3D printing is gaining more and more momentum to build customized product in a wide variety of fields. We conduct an exploratory study of Thingiverse, the most popular Website for sharing user-created 3D design files, in order to establish a possible connection with software product line (SPL) engineering. We report on the socio-technical aspects and current practices for modeling variability, implementing variability, configuring and deriving products, and reusing artefacts. We provide hints that SPL-alike techniques are practically used in 3D printing and thus relevant. Finally, we discuss why the customization in the 3D printing field represents a challenging playground for SPL engineering.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {142–146},
numpages = {5},
keywords = {3D printing, customization, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {configurations, program analysis, testing, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1613/jair.1.11688,
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11688},
doi = {10.1613/jair.1.11688},
abstract = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1183–1317},
numpages = {135},
keywords = {natural language, machine learning, computer vision, deep learning}
}

@inproceedings{10.1145/2019136.2019162,
author = {Quinton, Cl\'{e}ment and Mosser, S\'{e}bastien and Parra, Carlos and Duchien, Laurence},
title = {Using multiple feature models to design applications for mobile phones},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019162},
doi = {10.1145/2019136.2019162},
abstract = {The design of a mobile phone application is a tedious task according to its intrinsic variability. Software designers must take into account in their development process the versatility of available platforms (e.g., Android, iPhone). In addition to this, the variety of existing devices and their divergences (e.g., frontal camera, GPS) introduce another layer of complexity in the development process. These two dimensions can be formalized as Software Product Lines (SPL), independently defined. In this paper, we use a dedicated metamodel to bridge the gap between an application SPL and a mobile device one. This meta-model is also the support for the product derivation process. The approach is implemented in a framework named Applide, and is used to successfully derive customer relationship management software on different devices.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {23},
numpages = {8},
keywords = {application for mobile phones, feature model, meta-model, smartphones, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2364412.2364439,
author = {Vale, Tassio and Figueiredo, Gustavo Bittencourt and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A study on service identification methods for software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364439},
doi = {10.1145/2364412.2364439},
abstract = {The combination of service-orientation and software product line engineering, called Service-Oriented Product Line Engineering (SOPLE) have received attention by researchers and practitioners in the last years, and these areas can address issues of each other. One service-orientation issue is service identification. It consists of determining candidate services to a service-oriented environment based on pre-existing software artifacts, e.g., business process, source code, and so on. In order to provide a systematic identification of services, there are many available service identification methods in the literature, regarding different understanding of services, goals, and techniques. Due to this heterogeneity, this paper presents an in-depth comparison of service identification methods as well as a recommendation of the most suitable ones in the SOPLE context. This work can help the decision making of the most suitable method according to stakeholders' needs.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {156–163},
numpages = {8},
keywords = {service identification, service-oriented computing, service-oriented product lines, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s10270-020-00791-9,
author = {Westfechtel, Bernhard and Greiner, Sandra},
title = {Extending single- to multi-variant model transformations by trace-based propagation of variability annotations},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00791-9},
doi = {10.1007/s10270-020-00791-9},
abstract = {Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {853–888},
numpages = {36},
keywords = {Model transformation, Software product line, Annotative variability}
}

@inproceedings{10.1145/3109729.3109760,
author = {Basile, Davide and Di Giandomenico, Felicita and Gnesi, Stefania},
title = {FMCAT: Supporting Dynamic Service-based Product Lines},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109760},
doi = {10.1145/3109729.3109760},
abstract = {We describe FMCAT, a toolkit for Featured Modal Contract Automata (FMCA). FMCAT supports the analysis of dynamic service product lines, i.e., applications consisting of ensembles of interacting services organized as product lines. Services are modelled as FMCA, with features identifying obligations and requirements of services. Service requirements can be either permitted or necessary, whereas the latter are further partitioned according to their criticality. A notion of agreement among service contracts is used to characterise safety.We show how FMCAT can be used to (i) specify dynamic service product line, (ii) efficiently identify all valid products, and to synthesise a safe orchestration of services for either (iii) a single product, or (iv) the whole service product line. FMCAT exploits the theory of FMCA to efficiently perform the above tasks by only visiting a subset of valid products, and it is equipped with a GUI.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {3–8},
numpages = {6},
keywords = {Featured Modal Contract Automata Tool, Product line, Services},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {clone and own, families of software products, software reuse},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1007/978-3-030-87589-3_19,
author = {Yan, Yutong and Conze, Pierre-Henri and Lamard, Mathieu and Zhang, Heng and Quellec, Gwenol\'{e} and Cochener, B\'{e}atrice and Coatrieux, Gouenou},
title = {Deep Active Learning for Dual-View Mammogram Analysis},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_19},
doi = {10.1007/978-3-030-87589-3_19},
abstract = {Supervised deep learning on medical imaging requires massive manual annotations, which are expertise-needed and time-consuming to perform. Active learning aims at reducing annotation efforts by adaptively selecting the most informative samples for labeling. We propose in this paper a novel deep active learning approach for dual-view mammogram analysis, especially for breast mass segmentation and detection, where the necessity of labeling is estimated by exploiting the consistency of predictions arising from craniocaudal (CC) and mediolateral-oblique (MLO) views. Intuitively, if mass segmentation or detection is robustly performed, prediction results achieved on CC and MLO views should be consistent. Exploiting the inter-view consistency is hence a good way to guide the sampling mechanism which iteratively selects the next image pairs to be labeled by an oracle. Experiments on public DDSM-CBIS and INbreast datasets demonstrate that comparable performance with respect to fully-supervised models can be reached using only 6.83% (9.56%) of labeled data for segmentation (detection). This suggests that combining dual-view mammogram analysis and active learning can strongly contribute to the development of computer-aided diagnosis systems.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {180–189},
numpages = {10},
keywords = {Breast cancer, Mass segmentation, Mass detection, Dual-view mammogram analysis, Active learning, Computer-aided diagnosis},
location = {Strasbourg, France}
}

@article{10.1007/s10586-019-03012-1,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Ther\'{o}n, Roberto and Amo Filv\`{a}, Daniel and Fonseca Escudero, David},
title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-03012-1},
doi = {10.1007/s10586-019-03012-1},
abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm.},
journal = {Cluster Computing},
month = sep,
pages = {1803–1816},
numpages = {14},
keywords = {SPL, Domain engineering, Meta-model, Information dashboards, Feature model, Artificial intelligence, Automatic configuration}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3168365.3168378,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Towards the Extraction of Variability Information to Assist Variability Modelling of Complex Product Lines},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168378},
doi = {10.1145/3168365.3168378},
abstract = {Software product line engineering gathers a set of methods that rely on systematic reuse and mass customisation to reduce the development time and cost of a set of similar software systems. Boolean feature models are the de facto standard used to represent product line variability in terms of features, a feature being a distinguishable characteristic of one or several softwares. The extractive adoption of a product line from a set of individually developed softwares requires to extract variability information from a collection of software descriptions to model their variability. With the appearance of more and more complex software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of boolean feature models, as multi-valued attributes or UML-like cardinalities have since been proposed to support variability modelling in complex product lines. In this paper, we propose research directions to address the issue of extracting more complex variability information, as a part of extended feature models synthesis from software descriptions. We consider the capabilities of Formal Concept Analysis, a mathematical framework for knowledge discovery, along with two of its extensions called Pattern Structures and Relational Concept Analysis, to answer this problematic. These frameworks bring theoretical foundations to complex variability extraction algorithms.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {113–120},
numpages = {8},
keywords = {Reverse Engineering, Software Product Line, Variability Extraction},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {GCC, feature models, software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {bit-blasting, feature model, model counting, numerical features, propositional formula, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3369393,
author = {Ding, Yuhang and Fan, Hehe and Xu, Mingliang and Yang, Yi},
title = {Adaptive Exploration for Unsupervised Person Re-identification},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3369393},
doi = {10.1145/3369393},
abstract = {Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {3},
numpages = {19},
keywords = {Person re-identification, deep learning, domain adaptation, unsupervised learning}
}

@inproceedings{10.5555/1884371.1884417,
author = {S\o{}gaard, Anders and Rish\o{}j, Christian},
title = {The effect of semi-supervised learning on parsing long distance dependencies in German and Swedish},
year = {2010},
isbn = {3642147690},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper shows how the best data-driven dependency parsers available today [1] can be improved by learning from unlabeled data. We focus on German and Swedish and show that labeled attachment scores improve by 1.5%-2.5%. Error analysis shows that improvements are primarily due to better recovery of long distance dependencies.},
booktitle = {Proceedings of the 7th International Conference on Advances in Natural Language Processing},
pages = {406–417},
numpages = {12},
keywords = {dependency parsing, long distance dependencies, semi-supervised learning},
location = {Reykjavik, Iceland},
series = {IceTAL'10}
}

@inproceedings{10.1145/1858996.1859064,
author = {Boucher, Quentin and Classen, Andreas and Heymans, Patrick and Bourdoux, Arnaud and Demonceau, Laurent},
title = {Tag and prune: a pragmatic approach to software product line implementation},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859064},
doi = {10.1145/1858996.1859064},
abstract = {To realise variability at the code level, product line methods classically advocate usage of inheritance, components, frameworks, aspects or generative techniques. However, these might require unaffordable paradigm shifts for the developers if the software was not thought at the outset as a product line. Furthermore, these techniques can be conflicting with a company's coding practices or external regulations.These concerns were the motivation for the industry-university collaboration described in this paper where we develop a minimally intrusive coding technique based on tags. It is supported by a toolchain and is now in use in the partner company for the development of flight grade satellite communication software libraries.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {333–336},
numpages = {4},
keywords = {code tagging, feature diagram},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {e-discovery, electronic discovery, predictive coding, technology-assisted review},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1609/aaai.v33i01.33015725,
author = {Zhang, Biqiao and Kong, Yuqing and Essl, Georg and Provost, Emily Mower},
title = {undefined-similarity preservation loss for soft labels: a demonstration on cross-corpus speech emotion recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015725},
doi = {10.1609/aaai.v33i01.33015725},
abstract = {In this paper, we propose a Deep Metric Learning (DML) approach that supports soft labels. DML seeks to learn representations that encode the similarity between examples through deep neural networks. DML generally presupposes that data can be divided into discrete classes using hard labels. However, some tasks, such as our exemplary domain of speech emotion recognition (SER), work with inherently subjective data, data for which it may not be possible to identify a single hard label. We propose a family of loss functions, undefined-Similarity Preservation Loss (undefined-SPL), based on the dual form of undefined-divergence for DML with soft labels. We show that the minimizer of undefined-SPL preserves the pairwise label similarities in the learned feature embeddings. We demonstrate the efficacy of the proposed loss function on the task of cross-corpus SER with soft labels. Our approach, which combines undefined-SPL and classification loss, significantly outperforms a baseline SER system with the same structure but trained with only classification loss in most experiments. We show that the presented techniques are more robust to over-training and can learn an embedding space in which the similarity between examples is meaningful.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {702},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1145/3243316,
author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
title = {Unsupervised Person Re-identification: Clustering and Fine-tuning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3243316},
doi = {10.1145/3243316},
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {83},
numpages = {18},
keywords = {Large-scale person re-identification, clustering, convolutional neural network, unsupervised learning}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {optimization, service-oriented product line, swarm intelligence},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Semi-supervised learning, Speech emotion recognition, Domain adaptation, Deep neural networks}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Cyber-Physical Systems, Fault Detection, Highly-Configurable Systems, Product Line Testing, Search-Based Software Engineering, Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/1964138.1964143,
author = {Hamza, Haitham S. and Aly, Gamal M.},
title = {Using product line architectures to leverage systematic reuse of business knowledge: an industrial experience},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964143},
doi = {10.1145/1964138.1964143},
abstract = {Software Product Line Engineering (PLE) exploits systematic reuse by identifying and methodically reusing software artifacts to develop different but related software systems. Developing product lines requires analysis skills to identify, model, and encode domain and product knowledge into artifacts that can be systematically reused across the development life-cycle. As such, knowledge plays a paramount role in the success of the various activities of PLE. This paper investigates the role of PLE in identifying and codifying tacit business knowledge in two industrial case studies in the domain of Enterprise Resource Planning (ERP) systems.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {5},
numpages = {5},
keywords = {enterprise resource planning (ERP) systems, product-line engineering, software product lines, systematic reuse},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {boolean functions, feature interactions, fourier transform, performance},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.neunet.2021.03.022,
author = {Zhong, Yongjian and Du, Bo and Xu, Chang},
title = {Learning to reweight examples in multi-label classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.03.022},
doi = {10.1016/j.neunet.2021.03.022},
journal = {Neural Netw.},
month = oct,
pages = {428–436},
numpages = {9},
keywords = {Multi-label classification, Self-paced learning, Reweight instance}
}

@article{10.1016/j.patcog.2021.108164,
author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
title = {Training object detectors from few weakly-labeled and many unlabeled images},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108164},
doi = {10.1016/j.patcog.2021.108164},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Object detection, Weakly-supervised learning, Semi-supervised learning, Unlabelled set}
}

@inproceedings{10.1007/978-3-030-87196-3_28,
author = {Wu, Yicheng and Xu, Minfeng and Ge, Zongyuan and Cai, Jianfei and Zhang, Lei},
title = {Semi-supervised Left Atrium Segmentation with Mutual Consistency&nbsp;Training},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_28},
doi = {10.1007/978-3-030-87196-3_28},
abstract = {Semi-supervised learning has attracted great attention in the field of machine learning, especially for medical image segmentation tasks, since it alleviates the heavy burden of collecting abundant densely annotated data for training. However, most of existing methods underestimate the importance of challenging regions (e.g. small branches or blurred edges) during training. We believe that these unlabeled regions may contain more crucial information to minimize the uncertainty prediction for the model and should be emphasized in the training process. Therefore, in this paper, we propose a novel Mutual Consistency Network (MC-Net) for semi-supervised left atrium segmentation from 3D MR images. Particularly, our MC-Net consists of one encoder and two slightly different decoders, and the prediction discrepancies of two decoders are transformed as an unsupervised loss by our designed cycled pseudo label scheme to encourage mutual consistency. Such mutual consistency encourages the two decoders to have consistent and low-entropy predictions and enables the model to gradually capture generalized features from these unlabeled challenging regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it obtains impressive performance gains by exploiting the unlabeled data effectively. Our MC-Net outperforms six recent semi-supervised methods for left atrium segmentation, and sets the new state-of-the-art performance on the LA database.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {297–306},
numpages = {10},
keywords = {Semi-supervised learning, Mutual consistency, Cycled pseudo label},
location = {Strasbourg, France}
}

@inproceedings{10.1145/2791060.2791068,
author = {B\'{e}can, Guillaume and Behjati, Razieh and Gotlieb, Arnaud and Acher, Mathieu},
title = {Synthesis of attributed feature models from product descriptions},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791068},
doi = {10.1145/2791060.2791068},
abstract = {Many real-world product lines are only represented as nonhierarchical collections of distinct products, described by their configuration values. As the manual preparation of feature models is a tedious and labour-intensive activity, some techniques have been proposed to automatically generate boolean feature models from product descriptions. However, none of these techniques is capable of synthesizing feature attributes and relations among attributes, despite the huge relevance of attributes for documenting software product lines. In this paper, we introduce for the first time an algorithmic and parametrizable approach for computing a legal and appropriate hierarchy of features, including feature groups, typed feature attributes, domain values and relations among these attributes. We have performed an empirical evaluation by using both randomized configuration matrices and real-world examples. The initial results of our evaluation show that our approach can scale up to matrices containing 2,000 attributed features, and 200,000 distinct configurations in a couple of minutes.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {1–10},
numpages = {10},
keywords = {attributed feature models, product descriptions},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-030-58577-8_17,
author = {Pan, Lili and Ai, Shijie and Ren, Yazhou and Xu, Zenglin},
title = {Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_17},
doi = {10.1007/978-3-030-58577-8_17},
abstract = {Deep discriminative models (e.g.&nbsp;deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model—self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {271–287},
numpages = {17},
keywords = {Underrepresented examples, Self-paced learning, Entropy, Deep regression forests},
location = {Glasgow, United Kingdom}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {curriculum learning, reinforcement learning, self-paced learning, tempered inference, rl-as-inference}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/2791060.2793677,
author = {D\"{u}dder, Boris and Rehof, Jakob and Heineman, George T.},
title = {Synthesizing type-safe compositions in feature oriented software designs using staged composition},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2793677},
doi = {10.1145/2791060.2793677},
abstract = {The composition of features that interact with each other is challenging. Algebraic formalisms have been proposed by various authors to describe feature compositions and their interactions. The intention of feature compositions is the composition of fragments of documents of any kind to a product that fulfills users' requirements expressed by a feature selection. These modules often include code modules of typed programming languages whereas the proposed algebraic formalism is agnostic to types. This situation can lead to product code which is not type correct. In addition, types can carry semantic information on a program or module. We present a type system and connect it to an algebraic formalism thereby allowing automatic synthesis of feature compositions yielding well-typed programs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {398–401},
numpages = {4},
keywords = {automatic program synthesis, combinatory logic, feature composition, type theory},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-030-89370-5_24,
author = {Xu, Lu and Zhong, Xian and Liu, Wenxuan and Zhao, Shilei and Yang, Zhengwei and Zhong, Luo},
title = {Subspace Enhancement and Colorization Network for Infrared Video Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_24},
doi = {10.1007/978-3-030-89370-5_24},
abstract = {Human action recognition is an essential area of research in the field of computer vision. However, existing methods ignore the essence of infrared image spectral imaging. Compared with the visible modality with all three channels, the infrared modality with approximate single-channel pays more attention to the lightness contrast and loses the channel information. Therefore, we explore channel duplication and tend to investigate more appropriate feature presentations. We propose a subspace enhancement and colorization network (S2ECNet) to recognize infrared video action recognition. Specifically, we apply the subspace enhancement (S2E) module to promote edge contour extraction with subspace. Meanwhile, a subspace colorization (S2C) module is utilized for better completing missing semantic information. What is more, the optical flow provides effective supplements for temporal information. Experiments conducted on the infrared action recognition dataset InfAR demonstrates the competitiveness of the proposed method compared with the state-of-the-arts.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {321–336},
numpages = {16},
keywords = {Infrared video action recognition, Subspace enhancement, Subspace colorization, Optical flow, Feature fusion},
location = {Hanoi, Vietnam}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Commonality decision, Product platform scope, Software product line engineering}
}

@inproceedings{10.1145/2499777.2500714,
author = {Huang, Changyun and Kamei, Yasutaka and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
title = {Using alloy to support feature-based DSL construction for mining software repositories},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500714},
doi = {10.1145/2499777.2500714},
abstract = {The Mining Software Repositories (MSR) field reveals knowledge for software development by analyzing data stored in repositories such as source control and bug trace systems. In order to reveal the knowledge, MSR researchers need to perform complicated procedures iteratively. To help the complex work of MSR practitioners, we study the construction of domain specific languages (DSLs) for MSR. We have conducted feature-oriented domain analysis (FODA) on MSR and developed a DSL based on the feature model. In this paper, we expand our previous work and propose to construct not a single DSL but a DSL family. A DSL family consists of a series of DSLs with commonality in their domain but suitable to specific applications of MSR. To readily construct these DSLs, we use Alloy to encode the feature model. Our encoding includes not only the DSL features and their relations but also some composition rules that can be used to generate the syntax of DSLs. Based on this, we can automatically derive the language elements to construct DSLs suitable to specific purposes of MSR.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {86–89},
numpages = {4},
keywords = {DSL, FODA, SPL, mining software repositories},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/978-3-030-21290-2_42,
author = {Reinhartz-Berger, Iris and Shimshoni, Ilan and Abdal, Aviva},
title = {Behavior-Derived Variability Analysis: Mining Views for Comparison and Evaluation},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_42},
doi = {10.1007/978-3-030-21290-2_42},
abstract = {The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {675–690},
numpages = {16},
keywords = {Software Product Line Engineering, Variability analysis, Requirements specifications, Feature diagrams},
location = {Rome, Italy}
}

@article{10.1007/s42979-021-00541-8,
author = {Saber, Takfarinas and Brevet, David and Botterweck, Goetz and Ventresque, Anthony},
title = {Reparation in Evolutionary Algorithms for Multi-objective Feature Selection in Large Software Product Lines},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {3},
url = {https://doi.org/10.1007/s42979-021-00541-8},
doi = {10.1007/s42979-021-00541-8},
abstract = {Software Product Lines Engineering is the area of software engineering that aims to systematise the modelling, creation and improvement of groups of interconnected software systems by formally expressing possible alternative products in the form of Feature Models. Deriving a software product/system from a feature model is called Feature Configuration. Engineers select the subset of features (software components) from a feature model that suits their needs, while respecting the underlying relationships/constraints of the system–which is challenging on its own. Since there exist several (and often antagonistic) perspectives on which the quality of software could be assessed, the problem is even more challenging as it becomes a multi-objective optimisation problem. Current multi-objective feature selection in software product line approaches (e.g., SATIBEA) combine the scalability of a genetic algorithm (IBEA) with a solution reparation approach based on a SAT solver or one of its derivatives. In this paper, we propose MILPIBEA, a novel hybrid algorithm which combines IBEA with the accuracy of a mixed-integer linear programming (MILP) reparation. We show that the MILP reparation modifies fewer features from the original infeasible solutions than the SAT reparation and in a shorter time. We also demonstrate that MILPIBEA outperforms SATIBEA on average on various multi-objective performance metrics, especially on the largest feature models. The other major challenge in software engineering in general and in software product lines, in particular, is evolution. While the change in software components is common in the software engineering industry, the particular case of multi-objective optimisation of evolving software product lines is not well-tackled yet. We show that MILPIBEA is not only able to better take advantage of the evolution than SATIBEA, but it is also the one that continues to improve the quality of the solutions when SATIBEA stagnates. Overall, IBEA performs better when combined with MILP instead of SAT reparation when optimising the multi-objective feature selection in large and evolving software product lines.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {14},
keywords = {Software product line, Feature selection, Multi-objective optimisation, Evolutionary algorithm, Reparation, Mixed-integer linear programming}
}

@inproceedings{10.1145/3462757.3466101,
author = {McConnell, Devin J. and Zhu, James and Pandya, Sachin and Aguiar, Derek},
title = {Case-level prediction of motion outcomes in civil litigation},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466101},
doi = {10.1145/3462757.3466101},
abstract = {Lawyers regularly predict court outcomes to make strategic decisions, including when, if at all, to sue or settle, what to argue, and how to reduce their clients' liability risk. Yet, lawyer predictions tend to be poorly calibrated and biased, which exacerbate unjustifiable disparities in civil case outcomes. Current machine learning (ML) approaches for predicting court outcomes are typically constrained to final dispositions or are based on features unavailable in real-time during litigation, like judicial opinions. Here, we present the first ML-based methods to support lawyer and client decision making in real-time for motion filings in civil proceedings. Using the State of Connecticut Judicial Branch administrative data and court case documents, we trained six classifiers to predict motion to strike outcomes in tort and vehicular cases between July 1, 2004 and February 18, 2019. Integrating dense word embeddings from complaint documents, which contain information specific to the claims alleged, with the Judicial Branch data improved classification accuracy across all models. Subsequent models defined using a novel attorney case-entropy feature, dense word embeddings using corpus specific TF-IDF weightings, and algorithmic classification rules yielded the best predictor, Adaboost, with a classification accuracy of 64.4%. An analysis of feature importance weights confirmed the usefulness of incorporating attorney case-entropy and natural language features from complaint documents. Since all features used in model training are available during litigation, these methods will help lawyers make better predictions than they otherwise could given disparities in lawyer and client resources. All ML models, training code, and evaluation scripts are available at https://github.com/aguiarlab/motionpredict.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {99–108},
numpages = {10},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.5555/1753235.1753241,
author = {John, Isabel and Eisenbarth, Michael},
title = {A decade of scoping: a survey},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Scoping can be defined as the process of deciding in which parts of an organization's products, features and domains systematic reuse is economically useful. It generally is the first phase in product line engineering. For a decade now scoping has been recognized as a discipline of it's own in product line engineering. So it's time to look at what has been done in scoping in the last years and what is still to be done. In this survey, we identify and characterize existing scoping approaches with the main goal to derive open areas and research questions for further research in scoping. We analyze and compare existing approaches and derive open and partially addressed research questions that can be tackled by researchers in product line engineering in the next years.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {31–40},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/3371425.3371432,
author = {Song, Yoojeong and Lee, Jongwoo},
title = {Design of stock price prediction model with various configuration of input features},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371432},
doi = {10.1145/3371425.3371432},
abstract = {The interest rate of Korea has remained around 1.75 % annually for years. However, the interest rates of developed countries such as the United States of America are 2.25 ~ 2.5%. Korea is very low compared to this, so it is hard to save money on a deposit or installment saving. Therefore, many people want to use stock investment methods to gain high interest rates despite the high risk. Many people are predicting whether stock prices will rise or fall for investments on their subjective opinion. However, in the field of computer engineering, many people try to predict the stock price using artificial neural network, which has been proven to have good performance through many studies. The direction of stock forecasting research using artificial neural networks is very diverse such as model structure, composition of input feature, composition of target vector and so on. In this paper, we design three stock price prediction model with various input features that have specific characteristic. We hypothesized that, for effective stock price prediction through artificial neural networks, using implicit meaning data. We also questioned which of the implicit data would be most predictive. To prove it, we suggest three stock price prediction model. We implemented these three models and experimented to performance evaluation. Through this, we find out what kind of features would be effective for stock price prediction.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {3},
numpages = {5},
keywords = {artificial neural network, binary feature, input feature configuration, stock prediction, technical analysis},
location = {Sanya, China},
series = {AIIPCC '19}
}

@article{10.5555/3322706.3361993,
author = {Glimsdal, Sondre and Granmo, Ole-Christoffer},
title = {Thompson sampling guided stochastic searching on the line for deceptive environments with applications to root-finding problems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the "left" or to the "right" of the arm pulled, with the feedback being erroneous with probability 1 - π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1910–1933},
numpages = {24},
keywords = {deceptive environment, probabilistic bisection search, searching on the line, stochastic point location, thompson sampling}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {co-training, multi-modal, once and for all, self-supervised learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {multi-objective evolutionary algorithms, product line architecture design, search-based software engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362554,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Enablers and inhibitors for speed with reuse},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362554},
doi = {10.1145/2362536.2362554},
abstract = {An open issue in industry is software reuse in the context of large scale Agile product development. The speed offered by agile practices is needed to hit the market, while reuse is needed for long-term productivity, efficiency, and profit. The paper presents an empirical investigation of factors influencing speed and reuse in three large product developing organizations seeking to implement Agile practices. The paper identifies, through a multiple case study with 3 organizations, 114 business-, process-, organizational-, architecture-, knowledge- and communication factors with positive or negative influences on reuse, speed or both. Contributions are a categorized inventory of influencing factors, a display for organizing factors for the purpose of process improvement work, and a list of key improvement areas to address when implementing reuse in organizations striving to become more Agile. Categories identified include good factors with positive influences on reuse or speed, harmful factors with negative influences, and complex factors involving inverse or ambiguous relationships. Key improvement areas in the studied organizations are intra-organizational communication practices, reuse awareness and practices, architectural integration and variability management. Results are intended to support process improvement work in the direction of Agile product development. Feedback on results from the studied organizations has been that the inventory captures current situations, and is useful for software process improvement work.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {116–125},
numpages = {10},
keywords = {agile software development, embedded systems, enablers, inhibitors, software process improvement (SPI), software reuse, speed},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3268866.3268889,
author = {Robinson, Carl Peter and Li, Baihua and Meng, Qinggang and Pain, Matthew},
title = {Effectiveness of Surface Electromyography in Pattern Classification for Upper Limb Amputees},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268889},
doi = {10.1145/3268866.3268889},
abstract = {This study was undertaken to explore 18 time domain (TD) and time-frequency domain (TFD) feature configurations to determine the most discriminative feature sets for classification. Features were extracted from the surface electromyography (sEMG) signal of 17 hand and wrist movements and used to perform a series of classification trials with the random forest classifier. Movement datasets for 11 intact subjects and 9 amputees from the NinaPro online database repository were used. The aim was to identify any optimum configurations that combined features from both domains and whether there was consistency across subject type for any standout features. This work built on our previous research to incorporate the TFD, using a Discrete Wavelet Transform with a Daubechies wavelet. Findings report configurations containing the same features combined from both domains perform best across subject type (TD: root mean square (RMS), waveform length, and slope sign changes; TFD: RMS, standard deviation, and energy). These mixed-domain configurations can yield optimal performance (intact subjects: 90.98%; amputee subjects: 75.16%), but with only limited improvement on single-domain configurations. This suggests there is limited scope in attempting to build a single absolute feature configuration and more focus should be put on enhancing the classification methodology for adaptivity and robustness under actual operating conditions.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {107–112},
numpages = {6},
keywords = {Classification, Feature Extraction, Machine Learning, Myoelectric Control, Surface Electromyography},
location = {Beijing, China},
series = {AIPR '18}
}

@article{10.1007/s10462-020-09907-5,
author = {Uma Maheswari, S. and Shahina, A. and Nayeemulla Khan, A.},
title = {Understanding Lombard speech: a review of compensation techniques towards improving speech based recognition systems},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09907-5},
doi = {10.1007/s10462-020-09907-5},
abstract = {Building voice-based Artificial Intelligence (AI) systems that can efficiently interact with humans through speech has become plausible today due to rapid strides in efficient data-driven AI techniques. Such a human–machine voice interaction in real world would often involve a noisy ambience, where humans tend to speak with additional vocal effort than in a quiet ambience, to mitigate the noise-induced suppression of vocal self-feedback. This noise induced change in the vocal effort is called Lombard speech. In order to build intelligent conversational devices that can operate in a noisy ambience, it is imperative to study the characteristics and processing of Lombard speech. Though the progress of research on Lombard speech started several decades ago, it needs to be explored further in the current scenario which is seeing an explosion of voice-driven applications. The system designed to work with normal speech spoken in a quiet ambience fails to provide the same performance in changing environmental contexts. Different contexts lead to different styles of Lombard speech and hence there arises a need for efficient ways of handling variations in speaking styles in noise. The Lombard speech is also more intelligible than normal speech of a speaker. Applications like public announcement systems with speech output interface should talk with varying degrees of vocal effort to enhance naturalness in a way that humans adapt to speak in noise, in real time. This review article is an attempt to summarize the progress of work on the possible ways of processing Lombard speech to build smart and robust human–machine interactive systems with speech input–output interface, irrespective of operating environmental contexts, for different application needs. This article is a comprehensive review of the studies on Lombard speech, highlighting the key differences observed in acoustic and perceptual analysis of Lombard speech and detailing the Lombard effect compensation methods towards improving the robustness of speech based recognition systems.},
journal = {Artif. Intell. Rev.},
month = apr,
pages = {2495–2523},
numpages = {29},
keywords = {Lombard speech, Acoustic analysis, Perceptual analysis, Automatic recognition systems, Lombard effect compensation, Lombard speech synthesis}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@inproceedings{10.1007/978-3-030-93046-2_7,
author = {Guo, Jingwen and Lu, Zhisheng and Wang, Ti and Huang, Weibo and Liu, Hong},
title = {Object Goal Visual Navigation Using Semantic Spatial Relationships},
year = {2021},
isbn = {978-3-030-93045-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93046-2_7},
doi = {10.1007/978-3-030-93046-2_7},
abstract = {The target-driven visual navigation is a popular learning-based method and has been successfully applied to a wide range of applications. However, it has some disadvantages, including being ineffective at adapting to unseen environments. In this paper, a navigation method based on Semantic Spatial Relationships (SSR) is proposed and is shown to have more reliable performance when dealing with novel conditions. The construction of joint semantic hierarchical feature vector allows for learning implicit relationship between current observation and target objects, which benefits from construction of prior knowledge graph and semantic space. This differs from the traditional target driven methods, which integrate the visual input vector directly into the reinforcement learning path planning module. Moreover, the proposed method takes both local and global features of observed image into consideration and is thus less conservative and more robust in regards to random scenes. An additional analysis indicates that the proposed SSR performs well on classical metrics. The effectiveness of the proposed SSR model is demonstrated comparing with state-of-the-art methods in unknown scenes.},
booktitle = {Artificial Intelligence: First CAAI International Conference, CICAI 2021, Hangzhou, China, June 5–6, 2021, Proceedings, Part I},
pages = {77–88},
numpages = {12},
keywords = {Visual navigation, Semantic graph, Hierarchical relationship},
location = {Hangzhou, China}
}

@inproceedings{10.1145/2364412.2364444,
author = {Filho, Jo\~{a}o Bosco Ferreira and Barais, Olivier and Baudry, Benoit and Viana, Windson and Andrade, Rossana M. C.},
title = {An approach for semantic enrichment of software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364444},
doi = {10.1145/2364412.2364444},
abstract = {Software Product Lines (SPLs) have evolved and gained attention as one of the most promising approaches for software reuse. Feature models are the main technique to represent domain variability in SPLs. However, there are other domain aspects, besides variability, which cannot be expressed in a feature model. Also, these diagrams were not designed to facilitate information retrieval, interoperability and inference. In contrast, ontologies seem to be the best solution to meet these requirements. Therefore, this work presents an approach for semantic enrichment of SPLs using ontologies. Our proposal provides methods to add domain information besides variability description, and a top-ontology that specifies generic concepts and relations in an SPL, working as a guide model for information addition. The proposed approach reuses the existing SPL feature model, adding semantic descriptions in a less intrusive way than modifying the feature model notation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {188–195},
numpages = {8},
keywords = {knowledge, ontology, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.procs.2019.12.173,
author = {Chemingui, Houssem and Gam, Ines and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Product Line Configuration Meets Process Mining},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {164},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.173},
doi = {10.1016/j.procs.2019.12.173},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {199–210},
numpages = {12},
keywords = {Product line engineering, configuration process, process mining, enhancing, configuration difficulties}
}

@article{10.1016/j.engappai.2019.08.015,
author = {Tavasoli, Hanane and Oommen, B. John and Yazidi, Anis},
title = {On utilizing weak estimators to achieve the online classification of data streams},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.08.015},
doi = {10.1016/j.engappai.2019.08.015},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {11–31},
numpages = {21},
keywords = {Weak estimators, Learning automata, Non-stationary environments, Classification in data streams}
}

@article{10.5555/3455716.3455897,
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
title = {Curriculum learning for reinforcement learning domains: a framework and survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {181},
numpages = {50},
keywords = {curriculum learning, reinforcement learning, transfer learning}
}

@inproceedings{10.1145/2019136.2019169,
author = {Duran-Limon, Hector A. and Castillo-Barrera, Francisco E. and Lopez-Herrejon, Roberto E.},
title = {Towards an ontology-based approach for deriving product architectures},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019169},
doi = {10.1145/2019136.2019169},
abstract = {Software product line (SPL) engineering has proven to improve software quality and shorten costs and development time. An important aspect in the product line development process involves variability, which is the ability of a system for being customised, changed, or extended. Approaches are required for modelling and resolving variability as well as for verifying the selections. In this paper, we outline our ongoing research towards an approach that automates the derivation of product architectures from an SPL architecture. The proposed approach relies on ontology-based reasoning and model-driven techniques, the former supports the validation of the generated architectures and the generation of the transformation rules while the latter realises the actual target product architectures. We sketch our approach with a voice over IP case example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {29},
numpages = {5},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s10270-011-0220-1,
author = {Hubaux, Arnaud and Heymans, Patrick and Schobbens, Pierre-Yves and Deridder, Dirk and Abbasi, Ebrahim Khalil},
title = {Supporting multiple perspectives in feature-based configuration},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0220-1},
doi = {10.1007/s10270-011-0220-1},
abstract = {Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {641–663},
numpages = {23},
keywords = {Feature diagram, Feature-based configuration, Multi-view, Separation of concerns, Software product line engineering}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@book{10.5555/30423,
author = {Rich, Elaine},
title = {Artificial intelligence},
year = {1983},
isbn = {0070522618},
publisher = {McGraw-Hill, Inc.},
address = {USA},
abstract = {The goal of this book is to provide programmers and computer scientists with a readable introduction to the problems and techniques of artificial intelligence (A.I.). The book can be used either as a text for a course on A.I. or as a self-study guide for computer professionals who want to learn what A.I. is all about.The book was designed as the text for a one-semester, introductory graduate course in A.I. In such a course, it should be possible to cover all of the material in the book. I also require that students read ten or fifteen selected papers from the literature so that they become familiar with the way in which A.I. research is conducted.The book can also serve as the text for a one semester undergraduate A.I. course, but it will not be possible to cover all of the material. Chapters 1-3, 5, 7, and 8 describe basic techniques for problem solving and knowledge representation, and so should be covered as completely as possible. Then, with whatever time remains, topics selected from the remaining chapters can be discussed.To use this book effectively, students should have some background in both computer science and mathematics. As computer science background, they should have experience programming and they should feel comfortable with the material in an undergraduate data structures course. They should be familiar with the use of recursion as a program control structure. And they should be able to do simple analyses of the time complexity of algorithms. As mathematical background, students should have the equivalent of an undergraduate course in logic, including predicate logic with quantifiers and the basic notion of a decision procedure.This book contains, spread throughout it, many references to the A.I. research literature. These references are important for two reasons. First, they make it possible for the student to pursue individual topics in greater depth than is possible within the space restrictions of this book. This is the common reason for including references in a survey text. The second reason that these references have been included is more specific to the content of this book. A.I. is a relatively new discipline. In many areas of the field there is still not complete agreement on how things should be done. The references to the source literature guarantee that students have access not just to one approach, but to as many as possible of those whose eventual success still needs to be determined by further research, both theoretical and empirical.Since the ultimate goal of A.I. is the construction of programs that solve hard problems, no study of A.I. is complete without some experience writing programs. Most A.I. programs are currently written in LISP or in a higher-level language based on LISP. But there is no standard dialect of LISP available, so any attempt to include actual LISP code as part of a text will inevitably lead to a great deal of frustration as students find that they cannot run the examples in the book on the machine they are using. For this reason, the algorithms presented in this book are described in sufficient detail to enable students to exploit them in their programs, but they are not expressed in code. A good book on the use of LISP in A.I. (such as [Winston, 1981; Charniak, 1980]) and a manual for the local dialect of LISP that students will be using are necessary supplements to this book.}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Graph-based semi-supervised learning, Self-paced learning, Multi-view learning, Semi-supervised feature selection}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.3103/S1060992X19020048,
author = {Yakovenko, A. A.},
title = {A Hybrid Learning Approach for Adaptive Classification of Acoustic Signals Using the Simulated Responses of Auditory Nerve Fibers},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X19020048},
doi = {10.3103/S1060992X19020048},
journal = {Opt. Mem. Neural Netw.},
month = apr,
pages = {118–128},
numpages = {11},
keywords = {adaptive pattern classification, auditory periphery model, machine perception, neural responses, radial basis functions, self-organizing maps, unsupervised learning}
}

@inproceedings{10.1145/2791060.2791110,
author = {McVoy, Larry},
title = {Preliminary product line support in BitKeeper},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791110},
doi = {10.1145/2791060.2791110},
abstract = {One of the challenges of implementing a product line process is finding the appropriate tools for automation. One of our larger customers was implementing a product line process by-hand in a labor intensive and fragile way. We collaborated with them to evolve our distributed version control system, BitKeeper, into a tool that could handle their performance and product line requirements. The resulting product line generated several complex CPUs (around a billion transistors each).In this paper, we describe their by-hand process for producing different variations of a computer processor; we'll provide some background on the distributed version control system they were using; we'll describe the architectural changes implemented in BitKeeper for supporting product line work flows; we'll describe some of the changes we did to increase performance and provide some benchmark results comparing BitKeeper to Git, and we'll describe the work flow resulting from using the new architecture to replace their by-hand process.In the final section we'll discuss the current limitations of the existing tool, and describe how we plan on evolving it to overcome those limitations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {245–252},
numpages = {8},
keywords = {code reuse, configuration management, software product lines, version control},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3480433.3480447,
author = {Selitskiy, Stanislav and Christou, Nikolaos and Selitskaya, Natalya},
title = {Isolating Uncertainty of the Face Expression Recognition with the Meta-Learning Supervisor Neural Network},
year = {2021},
isbn = {9781450384148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480433.3480447},
doi = {10.1145/3480433.3480447},
abstract = {We investigate whether the well-known poor performance of the head-on usage of the convolutional neural networks for the facial expression recognition task may be improved in terms of reducing the false positive and false negative errors. An uncertainty isolating technique is used that introduces an additional “unknown” class. A self-attention supervisor artificial neural network is used to “learn about learning” of the underlying convolutional neural networks, in particular, to learn patterns of the underlying neural network parameters that accompany wrong or correct verdicts. A novel data set containing artistic makeup and occlusions images is used to aggravate the problem of the training data not representing the test data distribution.},
booktitle = {2021 5th International Conference on Artificial Intelligence and Virtual Reality (AIVR)},
pages = {104–112},
numpages = {9},
keywords = {Face expression recognition, Meta-learning, Self-attention, Uncertainty isolation},
location = {Kumamoto, Japan},
series = {AIVR 2021}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {Software product line, configuration management, critical path analysis, product derivation, weighted approach}
}

@inproceedings{10.1007/978-3-030-64694-3_17,
author = {Benmerzoug, Amine and Yessad, Lamia and Ziadi, Tewfik},
title = {Analyzing the Impact of Refactoring Variants on Feature Location},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_17},
doi = {10.1007/978-3-030-64694-3_17},
abstract = {Due to the increasing importance of feature location process, several studies evaluate the performance of different techniques based on IR strategies and a set of software variants as input artifacts. The proposed techniques attempt to improve the results obtained but it is often a difficult task. None of the existing feature location techniques considers the changing nature of the input artifacts, which may undergo series of refactoring changes. In this paper, we investigate the impact of refactoring variants on the feature location techniques. We first evaluate the performance of two techniques through the ArgoUML SPL benchmark when the variants are refactored. We then discuss the degraded results and the possibility of restoring them. Finally, we outline a process of variant alignment that aims to preserve the performance of the feature location.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {279–291},
numpages = {13},
keywords = {Software Product Line, Feature location, Refactoring},
location = {Hammamet, Tunisia}
}

@inproceedings{10.1007/978-3-030-63486-5_23,
author = {Yuan, Fangming and Neubert, Peer and Protzel, Peter},
title = {LocalSPED: A Classification Pipeline that Can Learn Local Features for Place Recognition Using a Small Training Set},
year = {2020},
isbn = {978-3-030-63485-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63486-5_23},
doi = {10.1007/978-3-030-63486-5_23},
abstract = {Visual place recognition is a key component for visual-SLAM. The current state-of-art methods use CNNs (Convolutional Neural Networks) to extract either a holistic descriptor or local features from the images. In recent work, a holistic descriptor method with the name SPED was proposed. In this paper, SPED is extended to a local feature configuration called LocalSPED by applying several modifications and by introducing a novel feature pooling method. Several variations of SPED and LocalSPED are trained on a smaller dataset and their performances are evaluated on several benchmark datasets. In the experiments, LocalSPED handles the decreased training set size significantly better than the original SPED approach and provides better place recognition results.},
booktitle = {Towards Autonomous Robotic Systems: 21st Annual Conference, TAROS 2020, Nottingham, UK, September 16, 2020, Proceedings},
pages = {209–213},
numpages = {5},
keywords = {Place recognition, Local features, Robotics},
location = {Nottingham, United Kingdom}
}

@article{10.1016/j.asoc.2016.07.048,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
title = {Extracting features from online software reviews to aid requirements reuse},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.048},
doi = {10.1016/j.asoc.2016.07.048},
abstract = {Display Omitted The extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have the access.Online reviews for software products can be used as input for features extraction to assist requirements reuse.Techniques from unsupervised learning and Natural Language Processing is employed as a propose solutions to Requirements Reuse problem.The approach obtained a precision of 87% (62% average) and a recall of 86% (82% average), when evaluated against the truth data set created manually. Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1297–1315},
numpages = {19},
keywords = {Latent semantic analysis, Natural language processing, Requirements reuse, Software engineering, Unsupervised learning}
}

@article{10.1504/ijbm.2021.112219,
author = {Gao, Feng and Luo, Daizhong and Ma, Xinqiang},
title = {Research on facial expression recognition of video stream based on OpenCV},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {1},
issn = {1755-8301},
url = {https://doi.org/10.1504/ijbm.2021.112219},
doi = {10.1504/ijbm.2021.112219},
abstract = {In order to overcome the poor performance of expression similarity measurement in traditional video stream facial expression recognition methods, an OpenCV based facial expression recognition method is proposed. In this method, the video stream face detection image is obtained by the window detection of various features in each position for the video stream image through the cascade classifier, and the image preprocessing is implemented. Based on OpenCV, the most important eyes and mouth in the facial expression are modeled, the eye feature model and mouth feature model are constructed, and the facial expression recognition of the video stream is realised through the constructed model. The experimental results show that the performance of expression similarity measurement is better, and the recognition rate of different expressions is more than 90%.},
journal = {Int. J. Biometrics},
month = jan,
pages = {114–129},
numpages = {15},
keywords = {OpenCV, video stream, face, facial expression recognition}
}

@inproceedings{10.5555/3504035.3504869,
author = {Fan, Xin and Liu, Risheng and Huyan, Kang and Feng, Yuyao and Luo, Zhongxuan},
title = {Self-reinforced cascaded regression for face alignment},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {834},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {autonomic computing, mobile systems},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s10664-021-09940-0,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {An empirical investigation of organic software product lines},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09940-0},
doi = {10.1007/s10664-021-09940-0},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems that is increasingly being applied to novel and emerging domains. In this work we investigate the use of software product line engineering in one of these new domains, synthetic biology. In synthetic biology living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse DNA parts. In this paper we perform an investigation of domain engineering that leverages an open-source repository of more than 45,000 reusable DNA parts. We show the feasibility of these new types of product line models by identifying features and related artifacts in up to 93.5% of products, and that there is indeed both commonality and variability. We then construct feature models for four commonly engineered functions leading to product lines ranging from 10 to 7.5 \texttimes{} 1020 products. In a case study we demonstrate how we can use the feature models to help guide new experimentation in aspects of application engineering. Finally, in an empirical study we demonstrate the effectiveness and efficiency of automated reverse engineering on both complete and incomplete sets of products. In the process of these studies, we highlight key challenges and uncovered limitations of existing SPL techniques and tools which provide a roadmap for making SPL engineering applicable to new and emerging domains.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {43},
keywords = {Software product lines, Synthetic biology, Reverse engineering, BioBricks}
}

@article{10.1016/j.engappai.2021.104473,
author = {Liu, Ze-yu and Liu, Jian-wei and Zuo, Xin and Hu, Ming-fei},
title = {Multi-scale iterative refinement network for RGB-D salient object detection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104473},
doi = {10.1016/j.engappai.2021.104473},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {16},
keywords = {Salient object detection, RGB-D image, Multi-scale refinement}
}

@article{10.1145/1842713.1842717,
author = {Robinson, William N. and Ding, Yi},
title = {A survey of customization support in agent-based business process simulation tools},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/1842713.1842717},
doi = {10.1145/1842713.1842717},
abstract = {Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework. We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
articleno = {14},
numpages = {29},
keywords = {Agent-based modeling, application frameworks, encapsulation, event-driven simulation, modularity, software product line engineering}
}

@article{10.1016/j.infsof.2009.11.001,
author = {Rabiser, Rick and Gr\"{u}nbacher, Paul and Dhungana, Deepak},
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
year = {2010},
issue_date = {March, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.001},
doi = {10.1016/j.infsof.2009.11.001},
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {324–346},
numpages = {23},
keywords = {Product derivation, Product line engineering, Software product line, Systematic literature review}
}

@article{10.1016/j.infsof.2012.07.010,
author = {Buchmann, Thomas and Dotor, Alexander and Westfechtel, Bernhard},
title = {MOD2-SCM: A model-driven product line for software configuration management systems},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.010},
doi = {10.1016/j.infsof.2012.07.010},
abstract = {Context: Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible. Objective: Our objective is to create a model-driven product line for SCM systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of SCM systems. Method: We developed a method and a tool set for model-driven software product line engineering which we applied to the SCM domain. For domain analysis, we applied the FORM method, resulting in a layered feature model for SCM systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific SCM system is configured by selecting features from the feature model and elements of the domain model realizing these features. Results: Due to the orthogonality of both feature model and domain model, a very large number of SCM systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion. Conclusion: The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of SCM systems since it support the modular composition of SCM systems at the model rather than the code level.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {630–650},
numpages = {21},
keywords = {Code generation, Executable models, Feature models, Model transformation, Model-driven software engineering, Software configuration management, Software product line engineering}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {Software product line testing, dimensionality reduction, many-objective problems},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1145/3397481.3450678,
author = {Reyes, Guillermo and Alles, Alexandra},
title = {Multi-modal Multi-scale Attention Guidance in Cyber-Physical Environments},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450678},
doi = {10.1145/3397481.3450678},
abstract = {This work proposes a new method for guiding a user’s attention towards objects of interest in a cyber-physical environment (CPE). CPEs are environments that contain several computing systems that interact with each other and with the physical world. These environments contain several sensors (cameras, eye trackers, etc.) and output devices (lamps, screens, speakers, etc.). These devices can be used to first track the user’s position, orientation, and focus of attention to then find the most suitable output device to guide the user’s attention towards a target object. We argue that the most suitable device in this context is the one that attracts attention closest to the target and is salient enough to capture the user’s attention. The method is implemented as a function which estimates the ”closeness” and ”salience” of each visual and auditive output device in the environment. Some parameters of this method are then evaluated through a user study in the context of a virtual reality supermarket. The results show that multi-modal guidance can lead to better guiding performance. However, this depends on the set parameters.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {356–365},
numpages = {10},
keywords = {Attention, Attention Guidance, Cyber-Physical Environments, Intelligent Environments, Multi-modal, Multi-scale},
location = {College Station, TX, USA},
series = {IUI '21}
}

@article{10.1007/s10664-020-09853-4,
author = {Hajri, Ines and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
title = {Automating system test case classification and prioritization for use case-driven testing in product lines},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09853-4},
doi = {10.1007/s10664-020-09853-4},
abstract = {Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation was performed in the context of an industrial product family in the automotive domain. Results provide empirical evidence that we propose a practical and beneficial way to classify and prioritize system test cases for industrial product lines.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3711–3769},
numpages = {59},
keywords = {Product Line Engineering, Use case driven development, Regression testing, Test case selection and prioritization, Automotive, Requirements engineering}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Energy Aware, Machine Learning, Software Product Line, Web System},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.5555/3367471.3367606,
author = {Yang, Liang and Chen, Zhiyang and Gu, Junhua and Guo, Yuanfang},
title = {Dual self-paced graph convolutional network: towards reducing attribute distortions induced by topology},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {The success of graph convolutional neural networks (GCNNs) based semi-supervised node classification is credited to the attribute smoothing (propagating) over the topology. However, the attributes may be interfered by the utilization of the topology information. This distortion will induce a certain amount of misclassifications of the nodes, which can be correctly predicted with only the attributes. By analyzing the impact of the edges in attribute propagations, the simple edges, which connect two nodes with similar attributes, should be given priority during the training process compared to the complex ones according to curriculum learning. To reduce the distortions induced by the topology while exploit more potentials of the attribute information, Dual Self-Paced Graph Convolutional Network (DSP-GCN) is proposed in this paper. Specifically, the unlabelled nodes with confidently predicted labels are gradually added into the training set in the node-level self-paced learning, while edges are gradually, from the simple edges to the complex ones, added into the graph during the training process in the edge-level self-paced learning. These two learning strategies are designed to mutually reinforce each other by coupling the selections of the edges and unlabelled nodes. Experimental results of transductive semi-supervised node classification on many real networks indicate that the proposed DSP-GCN has successfully reduced the attribute distortions induced by the topology while it gives superior performances with only one graph convolutional layer.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {4062–4069},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.5555/1865609.1865655,
author = {Liu, Yanxi and Popplestone, Robin J.},
title = {Symmetry constraint inference in assembly planning: automatic assembly configuration specification},
year = {1990},
isbn = {026251057X},
publisher = {AAAI Press},
abstract = {In this paper we shall discuss how to treat the automatic generation of assembly task specifications as a constraint satisfaction problem (CSP) over finite and infinite domains. Conceptually it is straightforward to formulate assembly planning in terms of CSP, however the choice of constraint representation and of the order in which the constraints are applied is nontrivial if a computationally tractable system design is to be achieved. This work investigates a subtle interaction between a pair of interleaving constraints, namely the kinematic and the spatial occupancy constraints. While finding one consistent solution to a general CSP is NP-complete, our work shows how to reduce the combinatorics in problems arising in assembly using the symmetries of assembly components. Group theory, being the standard mathematical theory of symmetry, is used extensively in this work since both robots and assembly components are threedimensional rigid bodies whose features have certain symmetries. This forms part of our high-level robot assembly task planner in which geometric solid modelling, group theory and CSP are combined into one computationally effective framework.},
booktitle = {Proceedings of the Eighth National Conference on Artificial Intelligence - Volume 2},
pages = {1038–1044},
numpages = {7},
location = {Boston, Massachusetts},
series = {AAAI'90}
}

@article{10.1016/j.engappai.2018.06.002,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Yu, Yang and Yuan, Bo-Wen and Tang, Jia-Fu},
title = {Integration of an improved dynamic ensemble selection approach to enhance one-vs-one scheme},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.002},
doi = {10.1016/j.engappai.2018.06.002},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {43–53},
numpages = {11},
keywords = {Dynamic selection, Heterogeneous ensemble, One-vs-one, Decomposition strategy, Multi-class classification}
}

@inproceedings{10.5555/3504035.3504690,
author = {Neill, James O' and Buitelaar, Paul},
title = {Few shot transfer learning between word relatedness and similarity tasks using a gated recurrent siamese network},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Word similarity and word relatedness are fundamental to natural language processing and more generally, understanding how humans relate concepts in semantic memory. A growing number of datasets are being proposed as evaluation benchmarks, however, the heterogeneity and focus of each respective dataset makes it difficult to draw plausible conclusions as to how a unified semantic model would perform. Additionally, we want to identify the transferability of knowledge obtained from one task to another, within the same domain and across domains. Hence, this paper first presents an evaluation and comparison of eight chosen datasets tested using the best performing regression models. As a baseline, we present regression models that incorporate both lexical features and word embeddings to produce consistent and competitive results compared to the state of the art. We present our main contribution, the best performing model across seven of the eight datasets - a Gated Recurrent Siamese Network that learns relationships between lexical word definitions. A parameter transfer learning strategy is employed for the Siamese Network. Subsequently, we present a secondary contribution which is the best performing non-sequential model: an Inductive and Transductive Transfer Learning strategy for transferring decision trees within a Random Forest to a target task that is learned from only few instances. The method involves measuring semantic distance between hidden factored matrix representations of decision tree traversal matrices.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {655},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s11042-020-10443-1,
author = {Rao, Champakamala Sundar and Karunakara, K.},
title = {A comprehensive review on brain tumor segmentation and classification of MRI images},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10443-1},
doi = {10.1007/s11042-020-10443-1},
abstract = {In the analysis of medical images, one of the challenging tasks is the recognition of brain tumours via medical resonance images (MRIs). The diagnosis process is still tedious due to its complexity and considerable variety in tissues of tumor perception. Therefore, the necessities of tumor identification techniques are improving nowadays for medical applications. In the past decades, different approaches in the segmentation of various precisions and complexity degree have been accomplished, which depends on the simplicity and the benchmark of the technique. An overview of this analysis is to give out the summary of the semi-automatic techniques for brain tumor segmentation and classification utilizing MRI. An enormous amount of MRI based image data is accomplished using deep learning approaches. There are several works, dealing on the conventional approaches for MRI-based segmentation of brain tumor. Alternatively, in this review, we revealed the latest trends in the methods of deep learning. Initially, we explain the several threads in MRI pre-processing, including registration of image, rectification of bias field, and non-brain tissue dismissal. And terminally, the present state evaluation of algorithm is offered and forecasting the growths to systematise the MRI-based brain tumor into a regular cyclic routine in the clinical field are focussed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17611–17643},
numpages = {33},
keywords = {MRI, Brain tumor, Segmentation, Bias field, Tissue, Image processing}
}

@article{10.1016/j.eswa.2018.04.033,
author = {Sreevani and Murthy, C.A. and Chanda, Bhabatosh},
title = {Generation of compound features based on feature interaction for classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.04.033},
doi = {10.1016/j.eswa.2018.04.033},
journal = {Expert Syst. Appl.},
month = oct,
pages = {61–73},
numpages = {13},
keywords = {Feature extraction, Feature selection, Compound features, Semi-features, Information theory, Feature interaction, Mutual information}
}

@inproceedings{10.1609/aaai.v33i01.33019005,
author = {Wu, Xiang and Huang, Huaibo and Patel, Vishal M. and He, Ran and Sun, Zhenan},
title = {Disentangled variational representation for heterogeneous face recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019005},
doi = {10.1609/aaai.v33i01.33019005},
abstract = {Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1105},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3483899.3483905,
author = {Freire, Willian and Tonh\~{a}o, Simone and Bonetti, Tiago and Shigenaga, Marcelo and Cadette, William and Felizardo, Fernando and Amaral, Aline and OliveiraJr, Edson and Colanzi, Thelma},
title = {On the configuration of multi-objective evolutionary algorithms for PLA design optimization},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483905},
doi = {10.1145/3483899.3483905},
abstract = {Search-based algorithms have been successfully applied in the Product Line Architecture (PLA) optimization using the seminal approach called Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). This approach produces a set of alternative PLA designs intending to improve the different factors being optimized. Currently, the MOA4PLA uses the NSGA-II algorithm, a multi-objective evolutionary algorithm (MOEA) that can optimize several architectural properties simultaneously. Despite the promising results, studying the best values for the algorithm parameters is essential to obtain even better results. This is also crucial to ease the adoption of MOA4PLA by newcomers or non-expert companies willing to start using search-based software engineering to PLA design. Three crossover operators for the PLA design optimization were proposed recently. However, reference values for parameters have not been defined for PLA design optimization using crossover operators. In this context, the objective of this work is conducting an experimental study to discover which are the most effective crossover operators and the best values to configure the MOEA parameters, such as population size, number of generations, and mutation and crossover rates. A quantitative analysis based on quality indicators and statistical tests was performed using four PLA designs to determine the most suitable parameter values to the search-based algorithm. Empirical results pointed out the best combination of crossover operators and the most suitable values to configure MOA4PLA.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@article{10.1016/j.ijar.2007.03.006,
author = {Peterson, Leif E. and Coleman, Matthew A.},
title = {Machine learning-based receiver operating characteristic (ROC) curves for crisp and fuzzy classification of DNA microarrays in cancer research},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2007.03.006},
doi = {10.1016/j.ijar.2007.03.006},
abstract = {Receiver operating characteristic (ROC) curves were generated to obtain classification area under the curve (AUC) as a function of feature standardization, fuzzification, and sample size from nine large sets of cancer-related DNA microarrays. Classifiers used included k-nearest neighbor (kNN), naive Bayes classifier (NBC), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), learning vector quantization (LVQ1), logistic regression (LOG), polytomous logistic regression (PLOG), artificial neural networks (ANN), particle swarm optimization (PSO), constricted particle swarm optimization (CPSO), kernel regression (RBF), radial basis function networks (RBFN), gradient descent support vector machines (SVMGD), and least squares support vector machines (SVMLS). For each data set, AUC was determined for a number of combinations of sample size, total sum[-log(p)] of feature t-tests, with and without feature standardization and with (fuzzy) and without (crisp) fuzzification of features. Altogether, a total of 2,123,530 classification runs were made. At the greatest level of sample size, ANN resulted in a fitted AUC of 90%, while PSO resulted in the lowest fitted AUC of 72.1%. AUC values derived from 4NN were the most dependent on sample size, while PSO was the least. ANN depended the most on total statistical significance of features used based on sum[-log(p)], whereas PSO was the least dependent. Standardization of features increased AUC by 8.1% for PSO and -0.2% for QDA, while fuzzification increased AUC by 9.4% for PSO and reduced AUC by 3.8% for QDA. AUC determination in planned microarray experiments without standardization and fuzzification of features will benefit the most if CPSO is used for lower levels of feature significance (i.e., sum[-log(p)]~50) and ANN is used for greater levels of significance (i.e., sum[-log(p)]~500). When only standardization of features is performed, studies are likely to benefit most by using CPSO for low levels of feature statistical significance and LVQ1 for greater levels of significance. Studies involving only fuzzification of features should employ LVQ1 because of the substantial gain in AUC observed and low expense of LVQ1. Lastly, PSO resulted in significantly greater levels of AUC (89.5% average) when feature standardization and fuzzification were performed. In consideration of the data sets used and factors influencing AUC which were investigated, if low-expense computation is desired then LVQ1 is recommended. However, if computational expense is of less concern, then PSO or CPSO is recommended.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {17–36},
numpages = {20},
keywords = {Area under the curve (AUC), DNA microarrays, Fuzzy classification, Gene expression, Receiver operator characteristic (ROC) curve, Soft computing}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {bipartite networks, community detection, graphon clustering, regularization of random graphs, spectral clustering, stochastic block model, sub-Gaussian biclustering}
}

@article{10.1007/s10270-016-0516-2,
author = {Damiani, Ferruccio and Faitelson, David and Gladisch, Christoph and Tyszberowicz, Shmuel},
title = {A novel model-based testing approach for software product lines},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0516-2},
doi = {10.1007/s10270-016-0516-2},
abstract = {Model-based testing relies on a model of the system under test. FineFit is a framework for model-based testing of Java programs. In the FineFit approach, the model is expressed by a set of tables based on Parnas tables. A software product line is a family of programs (the products) with well-defined commonalities and variabilities that are developed by (re)using common artifacts. In this paper, we address the issue of using the FineFit approach to support the development of correct software product lines. We specify a software product line as a specification product line where each product is a FineFit specification of the corresponding software product. The main challenge is to concisely specify the software product line while retaining the readability of the specification of a single system. To address this, we used delta-oriented programming, a recently proposed flexible approach for implementing software product lines, and developed: (1) delta tables as a means to apply the delta-oriented programming idea to the specification of software product lines; and (2) DeltaFineFit as a novel model-based testing approach for software product lines.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1223–1251},
numpages = {29},
keywords = {Alloy, Delta-oriented programming, Java, Model-based testing, Refinement, Software product line}
}

@inproceedings{10.5555/3367243.3367442,
author = {Li, Longyuan and Yan, Junchi and Yang, Xiaokang and Jin, Yaohui},
title = {Learning interpretable deep state space model for probabilistic time series forecasting},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Probabilistic time series forecasting involves estimating the distribution of future based on its history, which is essential for risk management in downstream decision-making. We propose a deep state space model for probabilistic time series forecasting whereby the non-linear emission model and transition model are parameterized by networks and the dependency is modeled by recurrent neural nets. We take the automatic relevance determination (ARD) view and devise a network to exploit the exogenous variables in addition to time series. In particular, our ARD network can incorporate the uncertainty of the exogenous variables and eventually helps identify useful exogenous variables and suppress those irrelevant for forecasting. The distribution of multi-step ahead forecasts are approximated by Monte Carlo simulation. We show in experiments that our model produces accurate and sharp probabilistic forecasts. The estimated uncertainty of our forecasting also realistically increases over time, in a spontaneous manner.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {2901–2908},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1145/3321408.3326676,
author = {Yan, Liu and Hu, Wenxin and Han, Longzhe},
title = {Optimize SPL test cases with adaptive simulated annealing genetic algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326676},
doi = {10.1145/3321408.3326676},
abstract = {In Software Product Line (SPL) testing, reduced test suite with high coverage is useful for early features interaction detection. sGA (simplified genetic algorithm) and SAGA(simulated annealing genetic algorithm) can generate high coverage test suite. However, small probability mutations in updating test suite may reduce search efficiency and thus miss better solutions. An improved test cases generation method based on ASAGA (Adaptive simulated annealing genetic algorithm) is proposed. Experiments on SPLOT (Software Product Lines Online Tools) feature models show that the proposed hybrid ASAGA method can ensure local optimization accuracy and achieve smaller-size test suite with higher coverage.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {148},
numpages = {7},
keywords = {ASAGA, feature model, similarity measurement, software test, test case},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Cyber-physical systems, Product line engineering, Automated configuration, Multi-stage and multi-step configuration process, Constraint classification, Variability modeling, Real-world case studies}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Dynamic Software Product Line, Feature Model, Modeling},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1007/s00034-021-01657-1,
author = {Pravin, Sheena Christabel and Palanivelan, M.},
title = {A Hybrid Deep Ensemble for Speech Disfluency Classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01657-1},
doi = {10.1007/s00034-021-01657-1},
abstract = {In this paper, a novel Hybrid Deep Ensemble (HDE) is proposed for automatic speech disfluency classification on a sparse speech dataset. Categorizations of speech disfluencies for diagnosis of speech disorders have so long relied on sophisticated deep learning models. Such a task can be accomplished by a straightforward approach with high accuracy by the proposed model which is an optimal combination of diverse machine learning and deep learning algorithms in a hierarchical arrangement which includes a deep autoencoder that yields the compressed latent features. The proposed model has shown considerable improvement in downgrading processing time overcoming the issues of cumbersome hyper-parameter tuning and huge data demand of the deep learning algorithms with high classification accuracy. Experimental results show that the proposed Hybrid Deep Ensemble has superior performance compared to the individual base learners, and the deep neural network as well. The proposed model and the baseline models were evaluated in terms of Cohen’s kappa coefficient, Hamming loss, Jaccard score, F-score and classification accuracy.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3968–3995},
numpages = {28},
keywords = {Hybrid Deep Ensemble, Speech disfluency classification, Sparse speech dataset, Deep autoencoder, Latent features}
}

@article{10.1007/s11063-020-10286-9,
author = {Li, Li and Zhao, Kaiyi and Li, Sicong and Sun, Ruizhi and Cai, Saihua},
title = {Extreme Learning Machine for Supervised Classification with Self-paced Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10286-9},
doi = {10.1007/s11063-020-10286-9},
abstract = {The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1723–1744},
numpages = {22},
keywords = {Classification, Extreme learning machine, Self-paced learning, Accuracy}
}

@article{10.1007/s10009-019-00528-0,
author = {Dimovski, Aleksandar S.},
title = {CTL⋆ family-based model checking using variability abstractions and modal transition systems},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-019-00528-0},
doi = {10.1007/s10009-019-00528-0},
abstract = {Variational systems can produce a (potentially huge) number of related systems, known as products or variants, by using features (configuration options) to mark the variable functionality. In many of the application domains, their rigorous verification and analysis are very important, yet the appropriate tools rarely are able to analyse variational systems. Recently, this problem was addressed by designing specialized so-called family-based model checking algorithms, which allow simultaneous verification of all variants in a single run by exploiting the commonalities between the variants. Yet, their computational cost still greatly depends on the number of variants (the size of configuration space), which is often huge. Moreover, their implementation and maintenance represent a costly research and development task. One of the most promising approaches to fighting the configuration space explosion problem is variability abstractions, which simplify variability away from variational systems. In this work, we show how to achieve efficient family-based model checking of CTL⋆ temporal properties using variability abstractions and off-the-shelf (single-system) tools. We use variability abstractions for deriving abstract family-based model checking, where the variability model of a variational system is replaced with an abstract (smaller) version of it, called modal transition system, which preserves the satisfaction of both universal and existential temporal properties, as expressible in CTL⋆. Modal transition systems contain two kinds of transitions, termed may- and must-transitions, which are defined by the conservative (over-approximating) abstractions and their dual (under-approximating) abstractions, respectively. The variability abstractions can be combined with different partitionings of the configuration space to infer suitable divide-and-conquer verification plans for the given variational system. We illustrate the practicality of this approach for several variational systems using the standard version of (single-system) NuSMV model checker.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {35–55},
numpages = {21},
keywords = {Software product line engineering, Family-based model checking, Abstract interpretation, Modal transition systems, Featured transition systems, CTL* temporal logic}
}

@inproceedings{10.5555/3060832.3060891,
author = {Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting},
title = {Self-paced boost learning for classification},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Effectiveness and robustness are two essential aspects of supervised learning studies. For effective learning, ensemble methods are developed to build a strong effective model from ensemble of weak models. For robust learning, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously enhancing the learning effectiveness and robustness, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a max-margin boosting optimization with self-paced sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1932–1938},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.5555/2946645.3053434,
author = {Szab\'{o}, Zolt\'{a}n and Sriperumbudur, Bharath K. and P\'{o}czos, Barnab\'{a}s and Gretton, Arthur},
title = {Learning theory for distribution regression},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; G\"{a}rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5272–5311},
numpages = {40},
keywords = {Kernel ridge regression, mean embedding, minimax optimality, multi-instance learning, two-Stage sampled distribution regression}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Feature model, Software product line, Defect, Product line model, Quality}
}

@inproceedings{10.1007/978-3-030-86380-7_23,
author = {Krysi\'{n}ska, Izabela and Morzy, Miko\l{}aj and Kajdanowicz, Tomasz},
title = {Curriculum Learning Revisited: Incremental Batch Learning with Instance Typicality Ranking},
year = {2021},
isbn = {978-3-030-86379-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86380-7_23},
doi = {10.1007/978-3-030-86380-7_23},
abstract = {The technique of curriculum learning mimics cognitive mechanisms observed in human learning, where simpler concepts are presented prior to gradual introduction of more difficult concepts. Until now, the major obstacle for curriculum methods was the lack of a reliable method for estimating the difficulty of training instances. In this paper we show that, instead of trying to assess the difficulty of learning instances, a simple graph-based method of computing the typicality of instances can be used in conjunction with curriculum methods. We design new batch schedulers which organize ordered instances into batches of varying size and learning difficulty. Our method does not require any changes to the architecture of trained models, we improve the training merely by manipulating the order and frequency of instance presentation to the model.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part IV},
pages = {279–291},
numpages = {13},
keywords = {Curriculum learning, Typicality, Batch training},
location = {Bratislava, Slovakia}
}

@inproceedings{10.5555/3524938.3524952,
author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
title = {Learning what to defer for maximum independent sets},
year = {2020},
publisher = {JMLR.org},
abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {14},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1145/2647908.2655969,
author = {ter Beek, Maurice H. and Mazzanti, Franco},
title = {VMC: recent advances and challenges ahead},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655969},
doi = {10.1145/2647908.2655969},
abstract = {The variability model checker VMC accepts a product family specified as a Modal Transition System (MTS) with additional variability constraints. Consequently, it offers behavioral variability analyses over both the family and its valid product behavior. This ranges from product derivation and simulation to efficient on-the-fly model checking of logical properties expressed in a variability-aware version of action-based CTL. In this paper, we first explain the reasons and assumptions underlying the choice for a modeling and analysis framework based on MTSs. Subsequently, we present recent advances on proving inheritance of behavioral analysis properties from a product family to its valid products. Finally, we illustrate challenges remaining for the future.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {70–77},
numpages = {8},
keywords = {behavioral variability, model checking, product families},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/j.neucom.2019.11.001,
author = {Li, Huafeng and Zhou, Weiyan and Yu, Zhengtao and Yang, Biao and Jin, Huaiping},
title = {Person re-identification with dictionary learning regularized by stretching regularization and label consistency constraint},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {379},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.001},
doi = {10.1016/j.neucom.2019.11.001},
journal = {Neurocomput.},
month = feb,
pages = {356–369},
numpages = {14},
keywords = {Person re-identification, Dictionary learning, Label consistency constraint, Stretch regularization}
}

@article{10.1016/j.jss.2013.10.010,
author = {White, Jules and Galindo, Jos\'{e} A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
title = {Evolving feature model configurations in software product lines},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.10.010},
doi = {10.1016/j.jss.2013.10.010},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
journal = {J. Syst. Softw.},
month = jan,
pages = {119–136},
numpages = {18},
keywords = {Feature model, Multi-step configuration, Software product line}
}

@inproceedings{10.5555/1753235.1753245,
author = {Cetina, Carlos and Haugen, \O{}ystein and Zhang, Xiaorui and Fleurey, Franck and Pelechano, Vicente},
title = {Strategies for variability transformation at run-time},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {More and more approaches propose to use Software Product Lines (SPLs) modelling techniques to implement dynamic adaptive systems. The resulting Dynamic Software Product Lines (DSPLs) present new challenges since the variability transformations used to derive alternative configurations have to be intensively used at runtime. This paper proposes to use the Common Variability Language (CVL) for modelling runtime variability and evaluates a set of alternative strategies for implementing the associated variability transformations. All the proposed strategies have been implemented and evaluated on the case-study of a smart-home system. Results show that the proposed strategies provide the same reconfiguration service with significant differences in quality-of-service.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {61–70},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Digital phenotyping, Digital medicine, Paralinguistics, Machine learning, Speech elicitation, Valence}
}

@inproceedings{10.5555/3504035.3504406,
author = {Gong, Tieliang and Wang, Guangtao and Ye, Jieping and Xu, Zongben and Lin, Ming},
title = {Margin based PU learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The PU learning problem concerns about learning from positive and unlabeled data. A popular heuristic is to iteratively enlarge training set based on some margin-based criterion. However, little theoretical analysis has been conducted to support the success of these heuristic methods. In this work, we show that not all margin-based heuristic rules are able to improve the learned classifiers iteratively. We find that a so-called large positive margin oracle is necessary to guarantee the success of PU learning. Under this oracle, a provable positive-margin based PU learning algorithm is proposed for linear regression and classification under the truncated Gaussian distributions. The proposed algorithm is able to reduce the recovering error geometrically proportional to the positive margin. Extensive experiments on real-world datasets verify our theory and the state-of-the-art performance of the proposed PU learning algorithm.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {371},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.5555/3524938.3525259,
author = {Geng, Sinong and Nassif, Houssam and Manzanares, Carlos A. and Reppen, A. Max and Sircar, Ronnie},
title = {Deep PQR: solving inverse reinforcement learning using anchor actions},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the Q- function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {321},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1609/aaai.v33i01.33015117,
author = {Tang, Ying-Peng and Huang, Sheng-Jun},
title = {Self-paced active learning: query the right thing at the right time},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015117},
doi = {10.1609/aaai.v33i01.33015117},
abstract = {Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {628},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1007/978-3-030-73197-7_29,
author = {Du, Yuntao and Chen, Yinghao and Cui, Fengli and Zhang, Xiaowen and Wang, Chongjun},
title = {Cross-Domain Error Minimization for Unsupervised Domain Adaptation},
year = {2021},
isbn = {978-3-030-73196-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73197-7_29},
doi = {10.1007/978-3-030-73197-7_29},
abstract = {Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Previous methods focus on learning domain-invariant features to decrease the discrepancy between the feature distributions as well as minimizing the source error and have made remarkable progress. However, a recently proposed theory reveals that such a strategy is not sufficient for a successful domain adaptation. It shows that besides a small source error, both the discrepancy between the feature distributions and the discrepancy between the labeling functions should be small across domains. The discrepancy between the labeling functions is essentially the cross-domain errors which are ignored by existing methods. To overcome this issue, in this paper, a novel method is proposed to integrate all the objectives into a unified optimization framework. Moreover, the incorrect pseudo labels widely used in previous methods can lead to error accumulation during learning. To alleviate this problem, the pseudo labels are obtained by utilizing structural information of the target domain besides source classifier and we propose a curriculum learning based strategy to select the target samples with more accurate pseudo-labels during training. Comprehensive experiments are conducted, and the results validate that our approach outperforms state-of-the-art methods.},
booktitle = {Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II},
pages = {429–448},
numpages = {20},
keywords = {Transfer learning, Domain adaptation, Cross-domain errors},
location = {Taipei, Taiwan}
}

@inproceedings{10.1145/2647908.2655961,
author = {Seidl, Christoph and Domachowska, Irena},
title = {Teaching variability engineering to cognitive psychologists},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655961},
doi = {10.1145/2647908.2655961},
abstract = {In research of cognitive psychology, experiments to measure cognitive processes may be run in many similar yet slightly different configurations. Variability engineering offers techniques to handle variable configurations both conceptually and technically. However, these techniques are largely unknown to cognitive psychologists so that experiment configurations are specified informally or too coarse grain. This is problematic, because it becomes difficult to get an overview of paradigm configurations used in the so far conducted experiments. Variability engineering techniques provide, i.a., concise notations for capturing variability in software and can also be used to express the configurable nature of a wide range of experiments in cognitive psychology. Furthermore, it enables cognitive psychologists to structure configuration knowledge, to identify suitably similar experiment setups and to more efficiently identify individual configuration options as relevant reasons for a particular effect in the outcome of an experiment. In this paper, we present experiences with teaching variability engineering to cognitive psychologists along with a suitable curriculum.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {16–23},
numpages = {8},
keywords = {cognitive psychology, feature model, teaching, variability engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1109/SPLC.2008.28,
author = {Chae, Wonseok and Blume, Matthias},
title = {Building a Family of Compilers},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.28},
doi = {10.1109/SPLC.2008.28},
abstract = {We have developed and maintained a set of closely related compilers. Although much of their code is duplicated and shared, they have been maintained separately because they are treated as different compilers. Even if they were merged together, the combined code would become too complicated to serve as the base for another extension. We describe our experience to address this problem by adopting the product line engineering paradigm to build a family of compilers. This paradigm encourages developers to focus on developing a set of compilers rather than on developing one particular compiler. We show engineering activities for a family of compilers from product line analysis through product line architecture design to product line component design. Then, we present how to build particular compilers from core assets resulting from the previous activities and how to take advantage of modern programming language technology to organize this task. Our experience demonstrates that the product line engineering as a developing paradigm can ease the construction of a family of compilers.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {307–316},
numpages = {10},
keywords = {compilers, feature-oriented, module system, product line engineering, standard ml},
series = {SPLC '08}
}

@inproceedings{10.1007/978-3-030-43680-3_11,
author = {Saber, Takfarinas and Brevet, David and Botterweck, Goetz and Ventresque, Anthony},
title = {MILPIBEA: Algorithm for&nbsp;Multi-objective Features Selection in&nbsp;(Evolving) Software Product Lines},
year = {2020},
isbn = {978-3-030-43679-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-43680-3_11},
doi = {10.1007/978-3-030-43680-3_11},
abstract = {Software Product Lines Engineering (SPLE) proposes techniques to model, create and improve groups of related software systems in a systematic way, with different alternatives formally expressed, e.g., as Feature Models. Selecting the ‘best’ software system(s) turns into a problem of improving the quality of selected subsets of software features (components) from feature models, or as it is widely known, Feature Configuration. When there are different independent dimensions to assess how good a software product is, the problem becomes even more challenging – it is then a multi-objective optimisation problem. Another big issue for software systems is evolution where software components change. This is common in the industry but, as far as we know, there is no algorithm designed to the particular case of multi-objective optimisation of evolving software product lines. In this paper we present MILPIBEA, a novel hybrid algorithm which combines the scalability of a genetic algorithm (IBEA) with the accuracy of a mixed-integer linear programming solver (IBM ILOG CPLEX). We also study the behaviour of our solution (MILPIBEA) in contrast with SATIBEA, a state-of-the-art algorithm in static software product lines. We demonstrate that MILPIBEA outperforms SATIBEA on average, especially for the most challenging problem instances, and that MILPIBEA is the one that continues to improve the quality of the solutions when SATIBEA stagnates (in the evolving context).},
booktitle = {Evolutionary Computation in Combinatorial Optimization: 20th European Conference, EvoCOP 2020, Held as Part of EvoStar 2020, Seville, Spain, April 15–17, 2020, Proceedings},
pages = {164–179},
numpages = {16},
keywords = {Software product line, Feature selection, Multi-objective optimisation, Evolutionary algorithm, Mixed-integer linear programming},
location = {Seville, Spain}
}

@article{10.1016/j.ins.2019.12.015,
author = {Xiao, Yanshan and Yang, Xiaozhou and Liu, Bo},
title = {A new self-paced method for multiple instance boosting learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {515},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.015},
doi = {10.1016/j.ins.2019.12.015},
journal = {Inf. Sci.},
month = apr,
pages = {80–90},
numpages = {11},
keywords = {Multiple instance learning, Multiple instance boost learning, Self-Paced learning}
}

@inproceedings{10.1007/978-3-030-52237-7_5,
author = {Carpenter, Dan and Emerson, Andrew and Mott, Bradford W. and Saleh, Asmalina and Glazewski, Krista D. and Hmelo-Silver, Cindy E. and Lester, James C.},
title = {Detecting Off-Task Behavior from Student Dialogue in Game-Based Collaborative Learning},
year = {2020},
isbn = {978-3-030-52236-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-52237-7_5},
doi = {10.1007/978-3-030-52237-7_5},
abstract = {Collaborative game-based learning environments integrate game-based learning and collaborative learning. These environments present students with a shared objective and provide them with a means to communicate, which allows them to share information, ask questions, construct explanations, and work together toward their shared goal. A key challenge in collaborative learning is that students may engage in unproductive discourse, which may affect learning activities and outcomes. Collaborative game-based learning environments that can detect this off-task behavior in real-time have the potential to enhance collaboration between students by redirecting the conversation back to more productive topics. This paper investigates the use of dialogue analysis to classify student conversational utterances as either off-task or on-task. Using classroom data collected from 13 groups of four students, we trained off-task dialogue models for text messages from a group chat feature integrated into Crystal Island: EcoJourneys, a collaborative game-based learning environment for middle school ecosystem science. We evaluate the effectiveness of the off-task dialogue models, which use different word embeddings (i.e., word2vec, ELMo, and BERT), as well as predictive off-task dialogue models that capture varying amounts of contextual information from the chat log. Results indicate that predictive off-task dialogue models that incorporate a window of recent context and represent the sequential nature of the chat messages achieve higher predictive performance compared to models that do not leverage this information. These findings suggest that off-task dialogue models for collaborative game-based learning environments can reliably recognize and predict students’ off-task behavior, which introduces the opportunity to adaptively scaffold collaborative dialogue.},
booktitle = {Artificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part I},
pages = {55–66},
numpages = {12},
keywords = {Off-task behavior, Computer-supported collaborative learning, Collaborative game-based learning, Game-based learning environments, Dialogue analysis},
location = {Ifrane, Morocco}
}

@inproceedings{10.3115/1218955.1219031,
author = {Niu, Cheng and Li, Wei and Srihari, Rohini K.},
title = {Weakly supervised learning for cross-document person name disambiguation supported by information extraction},
year = {2004},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1218955.1219031},
doi = {10.3115/1218955.1219031},
abstract = {It is fairly common that different people are associated with the same name. In tracking person entities in a large document pool, it is important to determine whether multiple mentions of the same name across documents refer to the same entity or not. Previous approach to this problem involves measuring context similarity only based on co-occurring words. This paper presents a new algorithm using information extraction support in addition to co-occurring words. A learning scheme with minimal supervision is developed within the Bayesian framework. Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features. Statistical annealing is applied to derive the final entity coreference chains by globally fitting the pairwise context similarities. Benchmarking shows that our new approach significantly outperforms the existing algorithm by 25 percentage points in overall F-measure.},
booktitle = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
pages = {597–es},
location = {Barcelona, Spain},
series = {ACL '04}
}

@inproceedings{10.1007/978-3-319-24888-2_3,
author = {Ma, Guangkai and Gao, Yaozong and Wang, Li and Wu, Ligang and Shen, Dinggang},
title = {Soft-Split Random Forest for Anatomy Labeling},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_3},
doi = {10.1007/978-3-319-24888-2_3},
abstract = {Random Forest (RF) has been widely used in the learning-based labeling. In RF, each sample is directed from the root to each leaf based on the decisions made in the interior nodes, also called splitting nodes. The splitting nodes assign a testing sample to either left or right child based on the learned splitting function. The final prediction is determined as the average of label probability distributions stored in all arrived leaf nodes. For ambiguous testing samples, which often lie near the splitting boundaries, the conventional splitting function, also referred to as hard split function, tends to make wrong assignments, hence leading to wrong predictions. To overcome this limitation, we propose a novel soft-split random forest (SSRF) framework to improve the reliability of node splitting and finally the accuracy of classification. Specifically, a soft split function is employed to assign a testing sample into both left and right child nodes with their certain probabilities, which can effectively reduce influence of the wrong node assignment on the prediction accuracy. As a result, each testing sample can arrive at multiple leaf nodes, and their respective results can be fused to obtain the final prediction according to the weights accumulated along the path from the root node to each leaf node. Besides, considering the importance of context information, we also adopt a Haar-features based context model to iteratively refine the classification map. We have comprehensively evaluated our method on two public datasets, respectively, for labeling hippocampus in MR images and also labeling three organs in Head &amp; Neck CT images. Compared with the hard-split RF (HSRF), our method achieved a notable improvement in labeling accuracy.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {17–25},
numpages = {9},
location = {Munich, Germany}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Optimal feature selection, Software product line, Evolutionary algorithm, Multi-objective optimization}
}

@article{10.1007/s00500-015-1643-3,
author = {Diaz-Valenzuela, Irene and Loia, Vincenzo and Martin-Bautista, Maria J. and Senatore, Sabrina and Vila, M. Amparo},
title = {Automatic constraints generation for semisupervised clustering: experiences with documents classification},
year = {2016},
issue_date = {June      2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {6},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-1643-3},
doi = {10.1007/s00500-015-1643-3},
abstract = {In the last times, semi-supervised clustering has been an area that has received a lot of attention. It is distinguished from more traditional unsupervised approaches on the use of a small amount of supervision to "steer" clustering. Unfortunately in the real world, the supervision is not always available: data to process are often too large and so the cost (in terms of time and human resources) for user-provided information is not conceivable. To address this issue, this work presents an automatic generation of the supervision, by the analysis of the data structure itself. This analysis is performed using a partitional clustering algorithm that discovers relationships between pairs of instances that may be used as a semi-supervision in the clustering process. The methodology has been studied in the document clustering domain, an area where novel approaches for accurate documents classifications are strongly required. Experimental result shows the validity of this approach.},
journal = {Soft Comput.},
month = jun,
pages = {2329–2339},
numpages = {11}
}

@article{10.1016/j.patcog.2019.107173,
author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
title = {Unsupervised domain adaptive re-identification: Theory and practice},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107173},
doi = {10.1016/j.patcog.2019.107173},
journal = {Pattern Recogn.},
month = jun,
numpages = {11},
keywords = {Person re-identification, Unsupervised domain adaptation}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Apriori Algorithm, DNA, K-NN, Naive Bayes, Rough Set}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1007/978-3-030-89363-7_28,
author = {Dai, Huan and Zhang, Yupei and Yun, Yue and Shang, Xuequn},
title = {An Improved Deep Model for Knowledge Tracing and Question-Difficulty Discovery},
year = {2021},
isbn = {978-3-030-89362-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89363-7_28},
doi = {10.1007/978-3-030-89363-7_28},
abstract = {Knowledge Tracing (KT) aims to analyze a student’s acquisition of skills over time by examining the student’s performance on questions of those skills. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, DKT and its variants often lead to oscillation results on a skill’s state may due to it ignoring the skill’s difficulty or the question’s difficulty. As a result, even when a student performs well on a skill, the prediction of that skill’s mastery level decreases instead, and vice versa. This is undesirable and unreasonable because student’s performance is expected to transit gradually over time. In this paper, we propose to learn the knowledge tracing model in a “simple-to-difficult” process, leading to a method of Self-paced Deep Knowledge Tracing (SPDKT). SPDKT learns the difficulty of per question from the student’s responses to optimize the question’s order and smooth the learning process. With mitigating the cause of oscillations, SPDKT has the capability of robustness to the puzzling questions. The experiments on real-world datasets show SPDKT achieves state-of-the-art performance on question response prediction and reaches interesting interpretations in education.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part II},
pages = {362–375},
numpages = {14},
keywords = {Knowledge tracing, Self-paced learning, Deep learning, Personalized education},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {DLV, configuration knowledge, default logic, distributed runtime adaptation, problem-solution feature interactions, software product line engineering},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, Andr\'{e} and Batista, Thais and Cacho, N\'{e}lio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {cloud computing, cloud platforms, health watcher system, services, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1609/aaai.v33i01.33014951,
author = {Shu, Yang and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin},
title = {Transferable curriculum for weakly-supervised domain adaptation},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014951},
doi = {10.1609/aaai.v33i01.33014951},
abstract = {Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that "source-domain engineering" becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weakly-supervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {608},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Constraint solving, Feature models, Multi-objective evolutionary algorithms, Search-based software engineering, Software product lines}
}

@inproceedings{10.5220/0005679102800287,
author = {Diedrich, Alexander and B\"{o}ttcher, Bj\"{o}rn and Niggemann, Oliver},
title = {Exposing Design Mistakes During Requirements Engineering by Solving Constraint Satisfaction Problems to Obtain Minimum Correction Subsets},
year = {2016},
isbn = {9789897581724},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005679102800287},
doi = {10.5220/0005679102800287},
abstract = {In recent years, the complexity of production plants and therefore of the underlying automation systems has grown significantly. This makes the manual design of automation systems increasingly difficult. As a result, errors are found only during production, plant modifications are hindered by not maintainable automation solutions and criteria such as energy efficiency or cost are often not optimized. This work shows how utilizing Minimum Correction Subsets (MCS) of a Constraint Satisfaction Problem improves the collaboration of automation system designers and prevents inconsistent requirements and thus subsequent errors in the design. This opens up a new field of application for constraint satisfaction techniques. As a use case, an example from the field of automation system design is presented. To meet the automation industry\^{a} s requirement for standardised solutions that assure reliability, the calculation of MCS is formulated in such a way that most constraint solvers can be used without any extensions. Experimental results with typical problems demonstrate the practicalness concerning runtime and hardware resources.},
booktitle = {Proceedings of the 8th International Conference on Agents and Artificial Intelligence},
pages = {280–287},
numpages = {8},
keywords = {Constraint Satisfaction, Feature Models, Minimum Correction Subsets., Product Line Engineering},
location = {Rome, Italy},
series = {ICAART 2016}
}

@inproceedings{10.1109/SPLC.2011.47,
author = {Chen, Sheng and Erwig, Martin},
title = {Optimizing the Product Derivation Process},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.47},
doi = {10.1109/SPLC.2011.47},
abstract = {Feature modeling is widely used in software product-line engineering to capture the commonalities and variabilities within an application domain. As feature models evolve, they can become very complex with respect to the number of features and the dependencies among them, which can cause the product derivation based on feature selection to become quite time consuming and error prone. We address this problem by presenting techniques to find good feature selection sequences that are based on the number of products that contain a particular feature and the impact of a selected feature on the selection of other features. Specifically, we identify a feature selection strategy, which brings up highly selective features early for selection. By prioritizing feature selection based on the selectivity of features our technique makes the feature selection process more efficient. Moreover, our approach helps with the problem of unexpected side effects of feature selection in later stages of the selection process, which is commonly considered a difficult problem. We have run our algorithm on the e-Shop and Berkeley DB feature models and also on some automatically generated feature models. The evaluation results demonstrate that our techniques can shorten the product derivation processes significantly.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {35–44},
numpages = {10},
keywords = {Decision Sequence, Feature Model, Feature Selection},
series = {SPLC '11}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Method Engineering, Method Oriented Architecture MOA, Semantic Web, Software Development, Software Product Line}
}

@article{10.1016/j.knosys.2019.105185,
author = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Xie, Shengli and Su, Chun-Yi},
title = {Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105185},
doi = {10.1016/j.knosys.2019.105185},
journal = {Know.-Based Syst.},
month = feb,
numpages = {10},
keywords = {Graph-regularization, Semi-supervised learning, Multi-view clustering, Non-negative matrix factorization}
}

@article{10.5555/2946645.2946709,
author = {Adi, Yossi and Keshet, Joseph},
title = {StructED: risk minimization in structured prediction},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents STRUCTED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from http://adiyoss.github.io/StructED/.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2282–2286},
numpages = {5},
keywords = {CRF, direct loss minimization, structural SVM, structured prediction}
}

@inproceedings{10.1609/aaai.v33i01.33014147,
author = {Lee, Jay Yoon and Mehta, Sanket Vaibhav and Wick, Michael and Tristan, Jean-Baptiste and Carbonell, Jaime},
title = {Gradient-based inference for networks with output constraints},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014147},
doi = {10.1609/aaai.v33i01.33014147},
abstract = {Practitioners apply neural networks to increasingly complex problems in natural language processing, such as syntactic parsing and semantic role labeling that have rich output structures. Many such structured-prediction problems require deterministic constraints on the output values; for example, in sequence-to-sequence syntactic parsing, we require that the sequential outputs encode valid trees. While hidden units might capture such properties, the network is not always able to learn such constraints from the training data alone, and practitioners must then resort to post-processing. In this paper, we present an inference method for neural networks that enforces deterministic constraints on outputs without performing rule-based post-processing or expensive discrete search. Instead, in the spirit of gradient-based training, we enforce constraints with gradient-based inference (GBI): for each input at test-time, we nudge continuous model weights until the network's unconstrained inference procedure generates an output that satisfies the constraints. We study the efficacy of GBI on three tasks with hard constraints: semantic role labeling, syntactic parsing, and sequence transduction. In each case, the algorithm not only satisfies constraints, but improves accuracy, even when the underlying network is state-of-the-art.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {510},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3297280.3297479,
author = {Ne\v{s}i\'{c}, Damir and Nyberg, Mattias and Gallina, Barbara},
title = {Constructing product-line safety cases from contract-based specifications},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297479},
doi = {10.1145/3297280.3297479},
abstract = {Safety cases are used to argue that safety-critical systems satisfy the requirements that are determined to mitigate the potential hazards in the systems operating environment. Although typically a manual task, safety cases have been successfully created for systems without many configuration options. However, in highly configurable systems, typically developed as a Product Line (PL), arguing about each possible configuration, and ensuring the completeness of the safety case are still open research problems. This paper presents a novel and general approach, based on Contract-Based Specification (CBS), for the construction of a safety case for an arbitrary PL. Starting from a general CBS framework, we present a PL extensions that allows expressing configurable systems and preserves the properties of the original CBS framework. Then, we define the transformation from arbitrary PL models, created using extended CBS framework, to a safety case argumentation-structure, expressed using the Goal Structuring Notation. Finally, the approach is exemplified on a simplified, but real, and currently produced system by Scania CV AB.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2022–2031},
numpages = {10},
keywords = {contract-based specification, product line engineering, safety case},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1016/j.patcog.2017.10.005,
author = {Zhou, Sanping and Wang, Jinjun and Meng, Deyu and Xin, Xiaomeng and Li, Yubing and Gong, Yihong and Zheng, Nanning},
title = {Deep self-paced learning for person re-identification},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.005},
doi = {10.1016/j.patcog.2017.10.005},
abstract = {We propose a novel deep self-paced learning algorithm to supervise the learning of deep neural network, in which a soft polynomial regularizer term is proposed to gradually involve the faithful samples into training process in a self-paced manner.We optimize the gradient back-propagation of relative distance metric by introducing a symmetric regularizer term, which can convert the back-propagation from the asymmetric mode to a symmetric one.We build an effective part-based deep neural network, in which features of different body parts are first discriminately learned in the convolutional layers and then fused in the fully connected layers. Person re-identification(Re-ID) usually suffers from noisy samples with background clutter and mutual occlusion, which makes it extremely difficult to distinguish different individuals across the disjoint camera views. In this paper, we propose a novel deep self-paced learning(DSPL) algorithm to alleviate this problem, in which we apply a self-paced constraint and symmetric regularization to help the relative distance metric training the deep neural network, so as to learn the stable and discriminative features for person Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive the adaptive weights to samples based on both the training loss and model age. As a result, the high-confidence fidelity samples will be emphasized and the low-confidence noisy samples will be suppressed at early stage of the whole training process. Such a learning regime is naturally implemented under a self-paced learning(SPL) framework, in which samples weights are adaptively updated based on both model age and sample loss using an alternative optimization method. Secondly, we introduce a symmetric regularizer term to revise the asymmetric gradient back-propagation derived by the relative distance metric, so as to simultaneously minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Finally, we build a part-based deep neural network, in which the features of different body parts are first discriminately learned in the lower convolutional layers and then fused in the higher fully connected layers. Experiments on several benchmark datasets have demonstrated the superior performance of our method as compared with the state-of-the-art approaches.},
journal = {Pattern Recogn.},
month = apr,
pages = {739–751},
numpages = {13},
keywords = {Convolutional neural network, Metric learning, Person re-identification, Self-paced learning}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {curriculum learning, linear regression, hinge loss minimization}
}

@article{10.1016/j.infsof.2015.11.004,
author = {Heradio, Ruben and Perez-Morago, Hector and Fernandez-Amoros, David and Javier Cabrerizo, Francisco and Herrera-Viedma, Enrique},
title = {A bibliometric analysis of 20 years of research on software product lines},
year = {2016},
issue_date = {April 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {72},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.11.004},
doi = {10.1016/j.infsof.2015.11.004},
abstract = {Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality.Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way.Method: Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis.Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations.Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {1–15},
numpages = {15},
keywords = {Bibliometrics, Performance analysis, Science mapping, Software product lines}
}

@article{10.1016/j.future.2019.07.013,
author = {Shen, Rongbo and Yan, Kezhou and Tian, Kuan and Jiang, Cheng and Zhou, Ke},
title = {Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.07.013},
doi = {10.1016/j.future.2019.07.013},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {668–679},
numpages = {12},
keywords = {Breast cancer, Mammography, Mass detection, Deep active learning, Self-paced learning}
}

@inproceedings{10.1145/3078971.3079003,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079003},
doi = {10.1145/3078971.3079003},
abstract = {Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {32–40},
numpages = {9},
keywords = {big data, concept detection, noisy data, prior knowledge, video understanding, web label, webly-supervised learning},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@article{10.1016/j.engappai.2018.06.010,
author = {Chin, Cheng Siong and Ji, Xi},
title = {Adaptive online sequential extreme learning machine for frequency-dependent noise data on offshore oil rig},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.010},
doi = {10.1016/j.engappai.2018.06.010},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {226–241},
numpages = {16},
keywords = {Multiple frequency dependent data, Extreme learning machine, Oil-rig, Noise prediction, Training time, Root mean square error}
}

@inproceedings{10.1145/3177148.3180085,
author = {Surendranath, Ajay and Jayagopi, Dinesh Babu},
title = {Curriculum Learning for Depth Estimation with Deep Convolutional Neural Networks},
year = {2018},
isbn = {9781450352901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177148.3180085},
doi = {10.1145/3177148.3180085},
abstract = {Curriculum learning is a machine learning technique adapted from the way humans acquire knowledge and skills, initially mastering simple tasks and progressing to more complex tasks. The work explores curriculum training by creating multiple levels of dataset with increasing complexity on which the trainings are performed. The experiments demonstrated that there is an average of 12% improvement test loss when compared to a non-curriculum approach. The experiment also demonstrates the advantage of creating synthetic dataset and how it aids in the overall improvement of accuracy. An improvement of 26% is attained on the test error loss when curriculum trained model was compared to training on a limited real world dataset. The work also goes onto propose a novel learning approach, the Self Paced Learning approach with Error-Diversity (SPL-ED) An overall reduction of 32% in the test loss is observed when compared to the non-curriculum training limited to real-world dataset.},
booktitle = {Proceedings of the 2nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
pages = {95–100},
numpages = {6},
keywords = {Curriculum Learning, Depth Estimation},
location = {Rabat, Morocco},
series = {MedPRAI '18}
}

@inproceedings{10.1007/978-3-030-22999-3_4,
author = {Havelock, Jessica and Oommen, B. John and Granmo, Ole-Christoffer},
title = {On Using “Stochastic Learning on the Line” to Design Novel Distance Estimation Methods for Three-Dimensional Environments},
year = {2019},
isbn = {978-3-030-22998-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22999-3_4},
doi = {10.1007/978-3-030-22999-3_4},
abstract = {We consider the unsolved problem of Distance Estimation (DE) when the inputs are the x and y coordinates (i.e., the latitudinal and longitudinal positions) of the points under consideration, and the elevation/altitudes of the points specified, for example, in terms of their z coordinates (3DDE). The aim of the problem is to yield an accurate value for the real (road) distance between the points specified by all the three coordinates of the cities in question (This is a typical problem encountered in a GISs and GPSs.). In our setting, the distance between any pair of cities is assumed to be computed by merely having access to the coordinates and known inter-city distances of a small subset of the cities, where these are also specified in terms of their 3D coordinates. The 2D variant of the problem has, typically, been tackled by utilizing parametric functions called “Distance Estimation Functions” (DEFs). To solve the 3D problem, we resort to the Adaptive Tertiary Search (ATS) strategy, proposed by Oommen et al., to affect the learning. By utilizing the information provided in the 3D coordinates of the nodes and the true road distances from this subset, we propose a scheme to estimate the inter-nodal distances. In this regard, we use the ATS strategy to calculate the best parameters for the DEF. While “Goodness-of-Fit” (GoF) functions can be used to show that the results are competitive, we show that they are rather not necessary to compute the parameters. Our results demonstrate the power of the scheme, even though we completely move away from the traditional GoF-based paradigm that has been used for four decades. Our results conclude that the 3DDE yields results that are far superior to those obtained by the corresponding 2DDE.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 32nd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2019, Graz, Austria, July 9–11, 2019, Proceedings},
pages = {39–49},
numpages = {11},
keywords = {Road distance estimation, Estimating real-life distances, Learning Automata, Adaptive Tertiary Search, Stochastic Point Location},
location = {Graz, Austria}
}

@inproceedings{10.1145/3409073.3409084,
author = {Congyi, Deng and Guangshun, Shi},
title = {Method for Detecting Android Malware Based on Ensemble Learning},
year = {2020},
isbn = {9781450377645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409073.3409084},
doi = {10.1145/3409073.3409084},
abstract = {In recent years, we have become increasingly dependent on smart devices. Android is an operating system mainly used on mobile devices, where hundreds of millions of users can download various apps through many application stores. Under these circumstances, a large number of malicious apps can be put into the application stores by developers to achieve the purpose of attacking, controlling user devices, and even stealing user information and property. Therefore, it is necessary to identify malwares in mass apps through analysis and detection to remind users. We propose an idea of detecting and discriminating Android malware based on an ensemble learning method. Firstly, a static analysis of AndroidManifest file in APK is performed to extract features such as permission calls, component calls, and intents in system. Then we use XGBoost method, an implementation of ensemble learning, to detect malicious applications. The conclusion is that this system performs very well in Android malware detection.},
booktitle = {Proceedings of the 2020 5th International Conference on Machine Learning Technologies},
pages = {28–31},
numpages = {4},
keywords = {Android Malware, Ensemble Learning, Malware Detection, Static Analysis},
location = {Beijing, China},
series = {ICMLT '20}
}

@article{10.1155/2019/8127869,
author = {Zhu, Qi and Yuan, Ning and Guan, Donghai and Deng, Ke},
title = {Cognitive Driven Multilayer Self-Paced Learning with Misclassified Samples},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1076-2787},
url = {https://doi.org/10.1155/2019/8127869},
doi = {10.1155/2019/8127869},
abstract = {In recent years, self-paced learning (SPL) has attracted much attention due to its improvement to nonconvex optimization based machine learning algorithms. As a methodology introduced from human learning, SPL dynamically evaluates the learning difficulty of each sample and provides the weighted learning model against the negative effects from hard-learning samples. In this study, we proposed a cognitive driven SPL method, i.e., retrospective robust self-paced learning (R2SPL), which is inspired by the following two issues in human learning process: the misclassified samples are more impressive in upcoming learning, and the model of the follow-up learning process based on large number of samples can be used to reduce the risk of poor generalization in initial learning phase. We simultaneously estimated the degrees of learning-difficulty and misclassified in each step of SPL and proposed a framework to construct multilevel SPL for improving the robustness of the initial learning phase of SPL. The proposed method can be viewed as a multilayer model and the output of the previous layer can guide constructing robust initialization model of the next layer. The experimental results show that the R2SPL outperforms the conventional self-paced learning models in classification task.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@inproceedings{10.1007/978-3-030-74251-5_8,
author = {Esteves, Diego and Marcelino, Jos\'{e} and Chawla, Piyush and Fischer, Asja and Lehmann, Jens},
title = {HORUS-NER: A Multimodal Named Entity Recognition Framework for Noisy Data},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_8},
doi = {10.1007/978-3-030-74251-5_8},
abstract = {Recent work based on Deep Learning presents state-of-the-art (SOTA) performance in the named entity recognition (NER) task. However, such models still have the performance drastically reduced in noisy data (e.g., social media, search engines), when compared to the formal domain (e.g., newswire). Thus, designing and exploring new methods and architectures is highly necessary to overcome current challenges. In this paper, we shift the focus of existing solutions to an entirely different perspective. We investigate the potential of embedding word-level features extracted from images and news. We performed a very comprehensive study in order to validate the hypothesis that images and news (obtained from an external source) may boost the task on noisy data, revealing very interesting findings. When our proposed architecture is used: (1) We beat SOTA in precision with simple CRFs models (2) The overall performance of decision trees-based models can be drastically improved. (3) Our approach overcomes off-the-shelf models for this task. (4) Images and text consistently increased recall over different datasets for SOTA, but at cost of precision. All experiment configurations, data and models are publicly available to the research community at},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {89–100},
numpages = {12},
keywords = {Named entity recognition, WNUT, Noisy text, Information retrieval, Images, Text, Multi-modal},
location = {Porto, Portugal}
}

@article{10.1016/j.neucom.2019.04.066,
author = {Zhu, Qi and Yuan, Ning and Huang, Jiashuang and Hao, Xiaoke and Zhang, Daoqiang},
title = {Multi-modal AD classification via self-paced latent correlation analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.066},
doi = {10.1016/j.neucom.2019.04.066},
journal = {Neurocomput.},
month = aug,
pages = {143–154},
numpages = {12},
keywords = {Multi-modal fusion, Feature extraction, Low-rank, Self-paced learning, Computer-aided diagnosis}
}

@article{10.1177/02783649211035177,
author = {Morales, Marco and Tapia, Lydia and S\'{a}nchez-Ante, Gildardo and Hutchinson, Seth and Chou, Glen and Berenson, Dmitry and Ozay, Necmiye},
title = {Learning constraints from demonstrations with grid and parametric representations},
year = {2021},
issue_date = {Sep 2021},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {40},
number = {10–11},
issn = {0278-3649},
url = {https://doi.org/10.1177/02783649211035177},
doi = {10.1177/02783649211035177},
abstract = {We extend the learning from demonstration paradigm by providing a method for learning unknown constraints shared across tasks, using demonstrations of the tasks, their cost functions, and knowledge of the system dynamics and control constraints. Given safe demonstrations, our method uses hit-and-run sampling to obtain lower cost, and thus unsafe, trajectories. Both safe and unsafe trajectories are used to obtain a consistent representation of the unsafe set via solving an integer program. Our method generalizes across system dynamics and learns a guaranteed subset of the constraint. In addition, by leveraging a known parameterization of the constraint, we modify our method to learn parametric constraints in high dimensions. We also provide theoretical analysis on what subset of the constraint and safe set can be learnable from safe demonstrations. We demonstrate our method on linear and nonlinear system dynamics, show that it can be modified to work with suboptimal demonstrations, and that it can also be used to learn constraints in a feature space.},
journal = {Int. J. Rob. Res.},
month = sep,
pages = {1255–1283},
numpages = {29},
keywords = {learning from demonstration, machine learning, motion and path planning}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Image classification, Curriculum learning, Self-paced learning, Multi-modal}
}

@inproceedings{10.5555/3524938.3525166,
author = {Degenne, R\'{e}my and Shao, Han and Koolen, Wouter M.},
title = {Structure adaptive algorithms for stochastic bandits},
year = {2020},
publisher = {JMLR.org},
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the suboptimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {228},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.knosys.2017.03.026,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Tang, Jia-Fu and Herrera, Francisco},
title = {Exploring the effectiveness of dynamic ensemble selection in the one-versus-one scheme},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.026},
doi = {10.1016/j.knosys.2017.03.026},
abstract = {The One-versus-One (OVO) strategy is one of the most common and effective techniques to deal with multi-class classification problems. The basic idea of an OVO scheme is to divide a multi-class classification problem into several easier-to-solve binary classification problems with considering each possible pair of classes from the original problem, which is then built into a binary classifier by an independent base learner. In this study, we propose a novel methodology which attempts to select a group of base classifiers in each pairwise dataset for each unknown pattern. To implement this, the Dynamic Ensemble Selection (DES) method based on a competence measure is employed to select the most appropriate ensemble in each binary classification problem derived from the OVO decomposition. In order to verify the validity and effectiveness of our proposed method, we carry out a thorough experimental study. We first compare our proposal with several state-of-the-art approaches. Then, we perform the comparison of several well-known aggregation strategies to combine the binary ensemble obtained by Dynamic Ensemble Selection. Finally, we explore whether further improvement can be achieved by considering the competence-based method in OVO scheme. The extracted findings drawn from the empirical analysis are supported by the proper statistical analysis and indicate that there is a positive synergy between the DES method and the Distance-based Relative Competence Weighting (DRCW) approach for the OVO scheme.},
journal = {Know.-Based Syst.},
month = jun,
pages = {53–63},
numpages = {11},
keywords = {Decomposition strategies, Dynamic ensemble selection, Multi-classification, One-versus-One, Pairwise learning}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@inproceedings{10.5555/3524938.3525116,
author = {Choo, Davin and Grunau, Christoph and Portmann, Julian and Rozho\v{n}, V\'{a}clav},
title = {k-means++: few more steps yield constant approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k)-approximation in expectation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant ε &gt; 0, with only εk additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {178},
numpages = {9},
series = {ICML'20}
}

@inproceedings{10.1145/2110147.2110161,
author = {Lienhardt, Michael and Clarke, Dave},
title = {Row types for delta-oriented programming},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110161},
doi = {10.1145/2110147.2110161},
abstract = {Delta-oriented programming (DOP) provides a technique for implementing Software Product Lines based on modifications (add, remove, modify) to a core program. Unfortunately, such modifications can introduce errors into a program, especially when type signatures of classes are modified in a non-monotonic fashion. To deal with this problem we present a type system for delta-oriented programs based on row polymorphism. This exercise elucidates the close correspondence between delta-oriented programs and row polymorphism.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {121–128},
numpages = {8},
keywords = {delta-oriented programming, software product line engineering, structural typing},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {BPM, Business process management, PL, Software product line}
}

@inproceedings{10.1007/978-3-030-00308-1_33,
author = {O’Keeffe, Simon and Villing, Rudi},
title = {A Benchmark Data Set and Evaluation of Deep Learning Architectures for Ball Detection in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_33},
doi = {10.1007/978-3-030-00308-1_33},
abstract = {This paper presents a benchmark data set for evaluating ball detection algorithms in the RoboCup Soccer Standard Platform League. We created a labelled data set of images with and without ball derived from vision log files recorded by multiple NAO robots in various lighting conditions. The data set contains 5209 labelled ball image regions and 10924 non-ball regions. Non-ball image regions all contain features that had been classified as a potential ball candidate by an existing ball detector. The data set was used to train and evaluate 252 different Deep Convolutional Neural Network (CNN) architectures for ball detection. In order to control computational requirements, this evaluation focused on networks with 2–5 layers that could feasibly run in the vision and cognition cycle of a NAO robot using two cameras at full frame rate (2&nbsp;\texttimes{}&nbsp;30&nbsp;Hz). The results show that the classification performance of the networks is quite insensitive to the details of the network design including input image size, number of layers and number of outputs at each layer. In an effort to reduce the computational requirements of CNNs we evaluated XNOR-Net architectures which quantize the weights and activations of a neural network to binary values. We examined XNOR-Nets corresponding to the real-valued CNNs we had already tested in order to quantify the effect on classification metrics. The results indicate that ball classification performance degrades by 12% on average when changing from real-valued CNN to corresponding XNOR-Net.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {398–409},
numpages = {12},
keywords = {Convolution neural network, Deep learning, Ball detection, XNOR-Net},
location = {Nagoya, Japan}
}

@article{10.1002/smr.1926,
author = {Horcas, Jos\'{e} Miguel and Monteil, Julien and Bouroche, M\'{e}lanie and Pinto, M\'{o}nica and Fuentes, Lidia and Clarke, Siobh\'{a}n},
title = {Context‐dependent reconfiguration of autonomous vehicles in mixed traffic},
year = {2018},
issue_date = {April 2018},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {30},
number = {4},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.1926},
doi = {10.1002/smr.1926},
abstract = {Human drivers naturally adapt their behaviour depending on the traffic conditions, such as the current weather and road type. Autonomous vehicles need to do the same, in a way that is both safe and efficient in traffic composed of both conventional and autonomous vehicles. In this paper, we demonstrate the applicability of a reconfigurable vehicle controller agent for autonomous vehicles that adapts the parameters of a used car‐following model at runtime, so as to maintain a high degree of traffic quality (efficiency and safety) under different weather conditions. We follow a dynamic software product line approach to model the variability of the car‐following model parameters, context changes and traffic quality, and generate specific configurations for each particular context. Under realistic conditions, autonomous vehicles have only a very local knowledge of other vehicles' variables. We investigate a distributed model predictive controller agent for autonomous vehicles to estimate their behavioural parameters at runtime, based on their available knowledge of the system. We show that autonomous vehicles with the proposed reconfigurable controller agent lead to behaviour similar to that achieved by human drivers, depending on the context.
–
The variability of the autonomous vehicles' behaviour, traffic context and quality is modelled in a Dynamic Software Product Line.–
The behaviour of the autonomous vehicles is adapted at runtime based on the current traffic context.–
A model predictive controller agent for autonomous vehicles optimises their behavioural parameters based on available knowledge in the traffic network.


image
image},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {15},
keywords = {autonomous vehicles, car‐following model, dynamic software product line, reconfiguration, traffic quality}
}

@article{10.1007/s00521-018-3478-1,
author = {Gu, Nannan and Fan, Pengying and Fan, Mingyu and Wang, Di},
title = {Structure regularized self-paced learning for robust semi-supervised pattern classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3478-1},
doi = {10.1007/s00521-018-3478-1},
abstract = {Semi-supervised classification is a hot topic in pattern recognition and machine learning. However, in presence of heavy noise and outliers, the unlabeled training data could be very challenging or even misleading for the semi-supervised classifier. In this paper, we propose a novel structure regularized self-paced learning method for semi-supervised classification problems, which can efficiently learn partially labeled training data sequentially from the simple to the complex ones. The proposed formulation consists of three components: a cost function defined by a mixture of losses, a functional complexity regularizer, and a self-paced regularizer; and the corresponding optimization algorithm involves three iterative steps: classifier updating, sample importance calculating, and pseudo-labeling. In the proposed method, the cost function for classifier updating and sample importance calculating is defined as a combination of the label fitting loss and manifold smoothness loss. Then, the importance of the pseudo-labeled and unlabeled samples is adaptively calculated by the novel cost. Unlabeled samples with high importance values are pseudo-labeled with their current predictions. In this way, labels are efficiently propagated from the labeled samples to the unlabeled ones in the robust self-paced manner. Experimental results on several benchmark data sets are provided to show the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6559–6574},
numpages = {16},
keywords = {Semi-supervised classification, Pattern classification, Self-paced learning, Manifold learning, Locally linear coding}
}

@inproceedings{10.1007/978-3-030-78270-2_74,
author = {Yun, Yue and Dai, Huan and Cao, Ruoqi and Zhang, Yupei and Shang, Xuequn},
title = {Self-paced Graph Memory Network for Student GPA Prediction and Abnormal Student Detection},
year = {2021},
isbn = {978-3-030-78269-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78270-2_74},
doi = {10.1007/978-3-030-78270-2_74},
abstract = {Student learning performance prediction (SLPP) is a crucial step in high school education. However, traditional methods fail to consider abnormal students. In this study, we organized every student’s learning data as a graph to use the schema of graph memory networks (GMNs). To distinguish the students and make GMNs learn robustly, we proposed to train GMNs in an “easy-to-hard” process, leading to self-paced graph memory network (SPGMN). SPGMN chooses the low-difficult samples as a batch to tune the model parameters in each training iteration. This approach not only improves the robustness but also rearranges the student sample from normal to abnormal. The experiment results show that SPGMN achieves a higher prediction accuracy and more robustness in comparison with traditional methods. The resulted student sequence reveals the abnormal student has a different pattern in course selection to normal students.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part II},
pages = {417–421},
numpages = {5},
keywords = {Student learning performance prediction, Self-paced learning, Graph memory networks, Abnormal student detection},
location = {Utrecht, The Netherlands}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Ten\'{o}rio, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, M\'{a}rcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Empirical software engineering, Feature modeling, Ontology, Software product line}
}

@inproceedings{10.1145/1655925.1656013,
author = {Alsawalqah, Hamad I. and Abotsi, Komi S. and Lee, Dan Hyung},
title = {An automated mechanism for organizing and retrieving core asset artifacts for product derivation in SPL},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656013},
doi = {10.1145/1655925.1656013},
abstract = {Software Product Line, SPL, is a software development strategy in which products are developed from a common set of core assets in a prescribed way with product specific features to satisfy specific market segment [1]. The SPL development process is carried out in two phases: the first phase is about building core assets called domain engineering, which has gained a lot of researchers' attention. The second step is about instantiating the specifics of the products by adding to the common part the specific features that identify the product from the other application engineering. For large and complex domains, it is argued that organizing and retrieving the development of artifacts from the core asset required by the application under development is a way of shortening the application development time, thus reduces the time to market. In this paper, we propose an automation mechanism for organizing the core assets using feature based organization to divide the customized domain feature model based on the application features and their dependencies. When that retrieval step where the artifacts are represented by relations that inherit the dependencies between the features in each division of the feature model, takes place, the final result is a set of development artifacts with their traceability links to be customized based on the application variability model and integrated with the application specific artifacts. To demonstrate our work, we applied this mechanism on a watch, a case study in the digital watch domain.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {480–485},
numpages = {6},
keywords = {digital watch, feature model, ontology, product derivation, software product line},
location = {Seoul, Korea},
series = {ICIS '09}
}

@inproceedings{10.1007/978-3-030-32692-0_49,
author = {Peng, Shiqi and Lai, Bolin and Yao, Guangyu and Zhang, Xiaoyun and Zhang, Ya and Wang, Yan-Feng and Zhao, Hui},
title = {Learning-Based Bone Quality Classification Method for Spinal Metastasis},
year = {2019},
isbn = {978-3-030-32691-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32692-0_49},
doi = {10.1007/978-3-030-32692-0_49},
abstract = {Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by +12.54%, +7.23% and +29.06% for blastic, mixed and lytic lesions, respectively, meanwhile +12.33%, +23.21% and +34.25% at vertebrae level.},
booktitle = {Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {426–434},
numpages = {9},
keywords = {Spinal metastasis, Bone quality classification, Multi-task learning, Self-paced learning},
location = {Shenzhen, China}
}

@article{10.1155/2021/4327896,
author = {Xie, Shu-Tong and He, Zong-Bao and Chen, Qiong and Chen, Rong-Xin and Kong, Qing-Zhao and Song, Cun-Ying and Huang, Jiwei},
title = {Predicting Learning Behavior Using Log Data in Blended Teaching},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4327896},
doi = {10.1155/2021/4327896},
abstract = {Online and offline blended teaching mode, the future trend of higher education, has recently been widely used in colleges around the globe. In the article, we conducted a study on students’ learning behavior analysis and student performance prediction based on the data about students’ behavior logs in three consecutive years of blended teaching in a college’s “Java Language Programming” course. Firstly, the data from diverse platforms such as MOOC, Rain Classroom, PTA, and cnBlog are integrated and preprocessed. Secondly, a novel multiclass classification framework, combining the genetic algorithm (GA) and the error correcting output codes (ECOC) method, is developed to predict the grade levels of students. In the framework, GA is designed to realize both the feature selection and binary classifier selection to fit the ECOC models. Finally, key factors affecting grades are identified in line with the optimal subset of features selected by GA, which can be analyzed for teaching significance. The results show that the multiclass classification algorithm designed in this article can effectively predict grades compared with other algorithms. In addition, the selected subset of features corresponding to learning behaviors is pedagogically instructive.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-90439-5_26,
author = {Lakshya},
title = {Behaviour of Sample Selection Techniques Under Explicit Regularization},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_26},
doi = {10.1007/978-3-030-90439-5_26},
abstract = {There is a multitude of sample selection-based learning strategies that have been developed for learning with noisy labels. However, It has also been indicated in the literature that perhaps early stopping is better than fully training the model for getting better performance. It leads us to wonder about the behavior of the sample selection strategies under explicit regularization. To this end, we considered four of the most fundamental sample selection-based models MentorNet, Coteaching, Coteaching-plus and JoCor. We provide empirical results of applying explicit L2 regularization to the above-mentioned approaches. We also compared the results with a baseline - a vanilla CNN model trained with just regularization. We show that under explicit regularization, the pre-conceived ranking of the approaches might change. We also show several instances where the baseline was able to outperform some or all of the existing approaches. Moreover, we show that under explicit regularization, the performance gap between the approaches can also reduce.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {331–340},
numpages = {10}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Extended product line, Product configuration, Systematic literature review}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Adaptive Boosting, Cyber security, Ensemble Learning, Extreme Learning Machine, Fractional Polynomial Kernels, KDD Cup 1999, Kernel Selection, Machine Learning, Multiclass Classification, Multiple Kernel Learning, Network Intrusion Detection}
}

@article{10.1016/j.scico.2013.07.016,
author = {Bettini, Lorenzo and Damiani, Ferruccio and Schaefer, Ina},
title = {Implementing type-safe software product lines using parametric traits},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {97},
number = {P3},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.07.016},
doi = {10.1016/j.scico.2013.07.016},
abstract = {A software product line (SPL) is a set of related software systems with well-defined commonality and variability that are developed by reusing common artifacts. In this paper, we present a novel technique for implementing SPLs by exploiting mechanisms for fine-grained reuse which are orthogonal to class-based inheritance. In our approach the concepts of type, behavior, and state are separated into different and orthogonal linguistic concepts: interfaces, traits and classes, respectively. We formalize our proposal by means of Featherweight Parametric Trait Java (FPTJ), a minimal core calculus where units of product functionality are modeled by parametric traits. Traits are a well-known construct for fine-grained reuse of behavior. Parametric traits are traits parameterized by interface names and class names. Parametric traits are applied to interface names and class names to generate traits that can be assembled in other (possibly parametric) traits or in classes that are used to build products. The composition of product functionality is realized by explicit operators of the calculus, allowing code manipulations for modeling product variability. The FPTJ type system ensures that the products in the SPL are type-safe by inspecting the parametric traits and classes shared by different products only once. Therefore, type-safety of an extension of a (type-safe) FPTJ SPL can be guaranteed by inspecting only the newly added parts. We present a technique for implementing SPLs by mechanisms for fine-grained reuse.We formalize our proposal by means of a minimal core calculus.The type system ensures that all the products in the SPL are type-safe.Type-safety of SPL extensions can be checked by inspecting only newly added parts.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {282–308},
numpages = {27},
keywords = {Featherweight Java, Feature model, Software product line, Trait, Type system}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {Software product line, actor, cross-domain reference architecture, multi product line, reference architecture},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.5555/3524938.3525504,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Extrapolation for large-batch training in deep learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {566},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.5555/3524938.3525278,
author = {Gopi, Sivakanth and Gulhane, Pankaj and Kulkarni, Janardhan and Shen, Judy Hanwen and Shokouhi, Milad and Yekhanin, Sergey},
title = {Differentially private set union},
year = {2020},
publisher = {JMLR.org},
abstract = {We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe U of items, possibly of infinite size, and a database D of users. Each user i contributes a subset Wi ⊆ U of items. We want an (ε,δ)-differentially private Algorithm which outputs a subset S ⊂ UiWi such that the size of S is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, n-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {340},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.1007/978-3-030-26061-3_3,
author = {Akhtiamov, Oleg and Fedotov, Dmitrii and Minker, Wolfgang},
title = {A Comparative Study of Classical and Deep Classifiers for Textual Addressee Detection in Human-Human-Machine Conversations},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_3},
doi = {10.1007/978-3-030-26061-3_3},
abstract = {The problem of addressee detection (AD) arises in multiparty conversations involving several dialogue agents. In order to maintain such conversations in a realistic manner, an automatic spoken dialogue system is supposed to distinguish between computer- and human-directed utterances since the latter utterances either need to be processed in a specific way or should be completely ignored by the system. In the present paper, we consider AD to be a text classification problem and model three aspects of users’ speech (syntactical, lexical, and semantical) that are relevant to AD in German. We compare simple classifiers operating with supervised text representations learned from in-domain data and more advanced neural network-based models operating with unsupervised text representations learned from in- and out-of-domain data. The latter models provide a small yet significant AD performance improvement over the classical ones on the Smart Video Corpus. A neural network-based semantical model determines the context of the first four words of an utterance to be the most informative for AD, significantly surpasses syntactical and lexical text classifiers and keeps up with a baseline multimodal metaclassifier that utilises acoustical information in addition to textual data. We also propose an effective approach to building representations for out-of-vocabulary words.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {20–30},
numpages = {11},
keywords = {Text classification, Speaking style, Human-computer interaction, Spoken dialogue system},
location = {Istanbul, Turkey}
}

@inbook{10.5555/3454287.3455282,
author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
title = {Data parameters: a new family of parameters for learning a differentiable curriculum},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFAR10, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, without any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {995},
numpages = {11}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Software product line testing, Search-Based software engineering, Preference-Based algorithms}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Curriculum learning, Self-paced learning, Learning with privileged information}
}

@article{10.1007/s11042-019-7498-3,
author = {Kaur, Taranjit and Saini, Barjinder Singh and Gupta, Savita},
title = {An adaptive fuzzy K-nearest neighbor approach for MR brain tumor image classification using parameter free bat optimization algorithm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7498-3},
doi = {10.1007/s11042-019-7498-3},
abstract = {This paper presents an automatic diagnosis system for the tumor grade classification through magnetic resonance imaging (MRI). The diagnosis system involves a region of interest (ROI) delineation using intensity and edge magnitude based multilevel thresholding algorithm. Then the intensity and the texture attributes are extracted from the segregated ROI. Subsequently, a combined approach known as Fisher+ Parameter-Free BAT (PFreeBAT) optimization is employed to derive the optimal feature subset. Finally, a novel learning approach dubbed as PFree BAT enhanced fuzzy K-nearest neighbor (FKNN) is proposed by combining FKNN with PFree BAT for the classification of MR images into two categories: High and Low-Grade. In PFree BAT enhanced FKNN, the model parameters, i.e., neighborhood size k and the fuzzy strength parameter m are adaptively specified by the PFree BAT optimization approach. Integrating PFree BAT with FKNN enhances the classification capability of the FKNN. The diagnostic system is rigorously evaluated on four MR images datasets including images from BRATS 2012 database and the Harvard repository using classification performance metrics. The empirical results illustrate that the diagnostic system reached to ceiling level of accuracy on the test MR image dataset via 5-fold cross-validation mechanism. Additionally, the proposed PFree BAT enhanced FKNN is evaluated on the Parkinson dataset (PD) from the UCI repository having the pre-extracted feature space. The proposed PFree BAT enhanced FKNN reached to an average accuracy of 98% and 97.45%. with and without feature selection on PD dataset. Moreover, solely to contrast, the performance of the proposed PFree BAT enhanced FKNN with the existing FKNN variants the experimentations were also done on six other standard datasets from KEEL repository. The results indicate that the proposed learning strategy achieves the best value of accuracy in contrast to the existing FKNN variants.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21853–21890},
numpages = {38},
keywords = {Fuzzy K-nearest neighbor, PFree BAT optimization, Diagnosis system, Model parameters}
}

@article{10.1016/j.eswa.2021.114781,
author = {Kerr, Emmett and McGinnity, T.M. and Coleman, Sonya and Shepherd, Andrea},
title = {Human vital sign determination using tactile sensing and fuzzy triage system},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {175},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114781},
doi = {10.1016/j.eswa.2021.114781},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {16},
keywords = {Fuzzy systems, Automated triage, Signal processing, Tactile sensing, Artificial intelligence, Classification, Human vital sign detection}
}

@inproceedings{10.5555/3540261.3541537,
author = {Peng, Jizong and Wang, Ping and Desrosiers, Christian and Pedersoli, Marco},
title = {Self-paced contrastive learning for semi-supervised medical image segmentation with meta-labels},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classification. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1276},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Cl\'{e}ment and Mann, Zolt\'{a}n \'{A}d\'{a}m and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Adaptation, Reinforcement learning, Feature model, Cloud service},
location = {Dubai, United Arab Emirates}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Data stream classification, Ensemble learning, Recurring concept drift, Unlabeled data}
}

@article{10.1016/j.knosys.2016.05.048,
author = {Zhang, Zhongliang and Krawczyk, Bartosz and Garc\`{\i}a, Salvador and Rosales-P\'{e}rez, Alejandro and Herrera, Francisco},
title = {Empowering one-vs-one decomposition with ensemble learning for multi-class imbalanced data},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.048},
doi = {10.1016/j.knosys.2016.05.048},
abstract = {Extending binary ensemble techniques to multi-class imbalanced data.OVO scheme enhancement for multi-class imbalanced data by ensemble learning.A complete experimental study of comparison of the ensemble learning techniques with OVO.Study of the impact of base classifiers used in the proposed scenario. Multi-class imbalance classification problems occur in many real-world applications, which suffer from the quite different distribution of classes. Decomposition strategies are well-known techniques to address the classification problems involving multiple classes. Among them binary approaches using one-vs-one and one-vs-all has gained a significant attention from the research community. They allow to divide multi-class problems into several easier-to-solve two-class sub-problems. In this study we develop an exhaustive empirical analysis to explore the possibility of empowering the one-vs-one scheme for multi-class imbalance classification problems with applying binary ensemble learning approaches. We examine several state-of-the-art ensemble learning methods proposed for addressing the imbalance problems to solve the pairwise tasks derived from the multi-class data set. Then the aggregation strategy is employed to combine the binary ensemble outputs to reconstruct the original multi-class task. We present a detailed experimental study of the proposed approach, supported by the statistical analysis. The results indicate the high effectiveness of ensemble learning with one-vs-one scheme in dealing with the multi-class imbalance classification problems.},
journal = {Know.-Based Syst.},
month = aug,
pages = {251–263},
numpages = {13},
keywords = {Binary decomposition, Classifier combination, Ensemble learning, Imbalanced data, Multi-class classification}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {Curriculum learning, label propagation, multi-modal learning, semi-supervised learning}
}

@article{10.1016/j.eswa.2021.115218,
author = {Serrano-P\'{e}rez, Jonathan and Sucar, L. Enrique},
title = {Artificial datasets for hierarchical classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115218},
doi = {10.1016/j.eswa.2021.115218},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Artificial datasets, Hierarchical classification, Evaluation}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Service composition, Feature model, Software product lines, Self adaptation}
}

@inproceedings{10.5555/2820656.2820667,
author = {Buchmann, Thomas and Baumgartl, Johannes and Henrich, Dominik and Westfechtel, Bernhard},
title = {Robots and their variability: a societal challenge and a potential solution},
year = {2015},
publisher = {IEEE Press},
abstract = {A robot is essentially a real-time, distributed embedded system operating in a physical environment. Often, control and communication paths within the system are tightly coupled to the actual hardware configuration of the robot. Furthermore, the domain contains a high amount of variability on different levels, ranging from hardware, over software to the environment in which the robot is operated. Today, special robots are used in households to perform monotonous and recurring tasks like vacuuming or mowing the lawn. In the future there may be robots that can be configured and programmed for more complicated tasks, like washing dishes or cleaning up or to assist elderly people. Nowadays, programming a robot is a highly complex and challenging task, which can be carried out only by programmers with dedicated background in robotics. Societal acceptance of robots can only be achieved, if they are easy to program. In this paper we present our approach to provide customized programming environments enabling programmers without background knowledge in robotics to specify robot programs. Our solution was realized using product line techniques.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {27–30},
numpages = {4},
keywords = {DSL, code generation, model-driven development, robot, software product line},
location = {Florence, Italy},
series = {PLEASE '15}
}

@article{10.1016/j.neucom.2019.03.062,
author = {Ren, Yazhou and Que, Xiaofan and Yao, Dezhong and Xu, Zenglin},
title = {Self-paced multi-task clustering},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {350},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.062},
doi = {10.1016/j.neucom.2019.03.062},
journal = {Neurocomput.},
month = jul,
pages = {212–220},
numpages = {9},
keywords = {Multi-task clustering, Self-paced learning, Non-convexity, Soft weighting}
}

@article{10.1007/s10462-011-9241-y,
author = {Stansbury, Richard S. and Agah, Arvin},
title = {A robot decision making framework using constraint programming},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {38},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-011-9241-y},
doi = {10.1007/s10462-011-9241-y},
abstract = {An intelligent robotic system must be capable of making the best decision at any given moment. The criteria for which task is "best" can be derived by performance metrics as well as the ability for it to satisfy all constraints upon the robot and its mission. Constraints may exist based on safety, reliability, accuracy, etc. This paper presents a decision framework capable of assisting a robotic system to select a task that satisfies all constraints as well as is optimized based upon one or more performance criteria. The framework models this decision process as a constraint satisfaction problem using techniques and algorithms from constraint programming and constraint optimization in order to provide a solution in real-time. This paper presents this framework and initial results provided through two demonstrations. The first utilizes simulation to provide an initial proof of concept, and the second, a security robot demonstration, is performed using a physical robot.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {67–83},
numpages = {17},
keywords = {Applied artificial intelligence, Constraint programming, Mobile robots}
}

@article{10.1016/j.knosys.2013.01.018,
author = {Fern\'{a}Ndez, Alberto and L\'{o}Pez, Victoria and Galar, Mikel and Del Jesus, Mar\'{\i}A Jos\'{e} and Herrera, Francisco},
title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.01.018},
doi = {10.1016/j.knosys.2013.01.018},
abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository.},
journal = {Know.-Based Syst.},
month = apr,
pages = {97–110},
numpages = {14},
keywords = {Cost-sensitive learning, Imbalanced data-sets, Multi-classification, Pairwise learning, Preprocessing}
}

@article{10.1016/j.ins.2018.06.014,
author = {Ma, Zilu and Liu, Shiqi and Meng, Deyu and Zhang, Yong and Lo, SioLong and Han, Zhi},
title = {On Convergence Properties of Implicit Self-paced Objective},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {462},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.06.014},
doi = {10.1016/j.ins.2018.06.014},
journal = {Inf. Sci.},
month = sep,
pages = {132–140},
numpages = {9},
keywords = {Self-paced learning, Machine learning, Non-convex optimization, Convergence, 00-01, 99-00}
}

@article{10.1016/j.micpro.2021.103964,
author = {Gokilavani, N. and Bharathi, B.},
title = {Multi-Objective based test case selection and prioritization for distributed cloud environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103964},
doi = {10.1016/j.micpro.2021.103964},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {6},
keywords = {Cloud environment, Software testing, Similarity-based clustering, Test case prioritization, Test case selection, Particle swarm optimization, Software product line}
}

@article{10.1016/j.patcog.2009.12.012,
author = {Derrac, Joaqu\'{\i}n and Garc\'{\i}a, Salvador and Herrera, Francisco},
title = {IFS-CoCo: Instance and feature selection based on cooperative coevolution with nearest neighbor rule},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.012},
doi = {10.1016/j.patcog.2009.12.012},
abstract = {Feature and instance selection are two effective data reduction processes which can be applied to classification tasks obtaining promising results. Although both processes are defined separately, it is possible to apply them simultaneously. This paper proposes an evolutionary model to perform feature and instance selection in nearest neighbor classification. It is based on cooperative coevolution, which has been applied to many computational problems with great success. The proposed approach is compared with a wide range of evolutionary feature and instance selection methods for classification. The results contrasted through non-parametric statistical tests show that our model outperforms previously proposed evolutionary approaches for performing data reduction processes in combination with the nearest neighbor rule.},
journal = {Pattern Recogn.},
month = jun,
pages = {2082–2105},
numpages = {24},
keywords = {Cooperative coevolution, Evolutionary algorithms, Feature selection, Instance selection, Nearest neighbor}
}

@inproceedings{10.1007/978-3-030-77385-4_42,
author = {Halilaj, Lavdim and Dindorkar, Ishan and L\"{u}ttin, J\"{u}rgen and Rothermel, Susanne},
title = {A Knowledge Graph-Based Approach for Situation Comprehension in Driving Scenarios},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_42},
doi = {10.1007/978-3-030-77385-4_42},
abstract = {Making an informed and right decision poses huge challenges for drivers in day-to-day traffic situations. This task vastly depends on many subjective and objective factors, including the current driver state, her destination, personal preferences and abilities as well as surrounding environment. In this paper, we present CoSI (Context and Situation Intelligence), a Knowledge Graph (KG)-based approach for fusing and organizing heterogeneous types and sources of information. The KG serves as a coherence layer representing information in the form of entities and their inter-relationships augmented with additional semantic axioms. Harnessing the power of axiomatic rules and reasoning capabilities enables inferring additional knowledge from what is already encoded. Thus, dedicated components exploit and consume the semantically enriched information to perform tasks such as situation classification, difficulty assessment, and trajectory prediction. Further, we generated a synthetic dataset to simulate real driving scenarios with a large range of driving styles and vehicle configurations. We use KG embedding techniques based on a Graph Neural Network (GNN) architecture for a classification task of driving situations and achieve over 95% accuracy whereas vector-based approaches achieve only 75% accuracy for the same task. The results suggest that the KG-based information representation combined with GNN are well suited for situation understanding tasks as required in driver assistance and automated driving systems.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {699–716},
numpages = {18},
keywords = {Situation comprehension, Knowledge graph, Knowledge graph embedding, Graph neural network}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Business Process Model, Domain-specific rules, Model Driven Architecture, Rule Generation, Variability Model}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Search-based software engineering, Systematic review, Research trends, Spanish community}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {adversarial machine learning, autonomous driving, sensor attack},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.future.2019.12.027,
author = {Al-Sayed, Mustafa M. and Hassan, Hesham A. and Omara, Fatma A.},
title = {An intelligent cloud service discovery framework},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.12.027},
doi = {10.1016/j.future.2019.12.027},
journal = {Future Gener. Comput. Syst.},
month = may,
pages = {438–466},
numpages = {29},
keywords = {Cloud computing, Cloud ontology, OBDA, Information retrieval, Cloud service discovery, Functional features, Non-functional features}
}

@inproceedings{10.1109/ASE.2011.6100118,
author = {Soltani, Samaneh and Asadi, Mohsen and Hatala, Marek and Gasevic, Dragan and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on stakeholders' business concerns},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100118},
doi = {10.1109/ASE.2011.6100118},
abstract = {In Software Product Line Engineering, concrete products of a family can be generated through a configuration process over a feature model. The configuration process selects features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from all the available features in the feature model is a cumbersome task because 1) the stakeholders may have diverse business concerns and limited resources that they can spend on a product and 2) features may have negative and positive contributions on different business concern. Many configurations techniques have been proposed to facilitate software developers' tasks through automated product derivation. However, most of the current proposals for automatic configuration are not devised to cope with business oriented requirements and stakeholders' resource limitations. We propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy the stakeholders' business concerns and resource limitations. We also provide tooling support to facilitate the use of our framework.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {536–539},
numpages = {4},
series = {ASE '11}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Evolutionary algorithm, Metaheuristics, Search based software engineering, Software product line, Systematic mapping study}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Mar\'{\i}n, Carlos Enrique Montenegro and Crespo, Rub\'{e}n Gonz\'{a}lez},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Environmental noise, Road traffic, Vehicle, Classification, Deep learning}
}

@article{10.1016/j.patcog.2018.11.030,
author = {Wang, Xiaohong and Jiang, Xudong and Ren, Jianfeng},
title = {Blood vessel segmentation from fundus image by a cascade classification framework},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.030},
doi = {10.1016/j.patcog.2018.11.030},
journal = {Pattern Recogn.},
month = apr,
pages = {331–341},
numpages = {11},
keywords = {Fundus image, Retinal vessel segmentation, Cascade classification, Dimensionality reduction}
}

@inproceedings{10.1007/978-3-030-23502-4_14,
author = {Sondur, Sanjeev and Kant, Krishna},
title = {Towards Automated Configuration of Cloud Storage Gateways: A Data Driven Approach},
year = {2019},
isbn = {978-3-030-23501-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23502-4_14},
doi = {10.1007/978-3-030-23502-4_14},
abstract = {Cloud storage gateways (CSGs) are an essential part of enterprises to take advantage of the scale and flexibility of cloud object store. A CSG provides clients the impression of a locally configured large size block-based storage device, which needs to be mapped to remote cloud storage which is invariably object based. Proper configuration of the cloud storage gateway is extremely challenging because of numerous parameters involved and interactions among them. In this paper, we study this problem for a commercial CSG product that is typical of offerings in the market. We explore how machine learning techniques can be exploited both for the forward problem (i.e. predicting performance from the configuration parameters) and backward problem (i.e. predicting configuration parameter values from the target performance). Based on extensive testing with real world customer workloads, we show that it is possible to achieve excellent prediction accuracy while ensuring that the model is not overfitted to the data.},
booktitle = {Cloud Computing – CLOUD 2019: 12th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings},
pages = {192–207},
numpages = {16},
keywords = {Cloud storage gateway, Object store, Performance, Configuration management, Machine learning},
location = {San Diego, CA, USA}
}

@inproceedings{10.1109/ICTAI.2014.144,
author = {Oliveira, Pedro and Souza, Matheus and Braga, Ronyerison and Brito, Ricardo and Rab\^{e}lo, Ricardo Lira and Neto, Pedro Santos},
title = {Athena: A Visual Tool to Support the Development of Computational Intelligence Systems},
year = {2014},
isbn = {9781479965724},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2014.144},
doi = {10.1109/ICTAI.2014.144},
abstract = {Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.},
booktitle = {Proceedings of the 2014 IEEE 26th International Conference on Tools with Artificial Intelligence},
pages = {950–959},
numpages = {10},
keywords = {Artificial Intelligence, Computational Intelligence, Service, Tool, Visual Programming},
series = {ICTAI '14}
}

@article{10.1007/s00500-019-04503-4,
author = {Abboud, Ralph and Tekli, Joe},
title = {Integration of nonparametric fuzzy classification with an evolutionary-developmental framework to perform music sentiment-based analysis and composition},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {13},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04503-4},
doi = {10.1007/s00500-019-04503-4},
abstract = {Over the past years, several approaches have been developed to create algorithmic music composers. Most existing solutions focus on composing music that appears theoretically correct or interesting to the listener. However, few methods have targeted sentiment-based music composition: generating music that expresses human emotions. The few existing methods are restricted in the spectrum of emotions they can express (usually to two dimensions: valence and arousal) as well as the level of sophistication of the music they compose (usually monophonic, following translation-based, predefined templates or heuristic textures). In this paper, we introduce a new algorithmic framework for autonomous music sentiment-based expression and composition, titled MUSEC, that perceives an extensible set of six primary human emotions (e.g., anger, fear, joy, love, sadness, and surprise) expressed by a MIDI musical file and then composes (creates) new polyphonic (pseudo) thematic, and diversified musical pieces that express these emotions. Unlike existing solutions, MUSEC is: (i) a hybrid crossover between supervised learning (SL, to learn sentiments from music) and evolutionary computation (for music composition, MC), where SL serves at the fitness function of MC to compose music that expresses target sentiments, (ii) extensible in the panel of emotions it can convey, producing pieces that reflect a target crisp sentiment (e.g., love) or a collection of fuzzy sentiments (e.g., 65% happy, 20% sad, and 15% angry), compared with crisp-only or two-dimensional (valence/arousal) sentiment models used in existing solutions, (iii) adopts the evolutionary-developmental model, using an extensive set of specially designed music-theoretic mutation operators (trille, staccato, repeat, compress, etc.), stochastically orchestrated to add atomic (individual chord-level) and thematic (chord pattern-level) variability to the composed polyphonic pieces, compared with traditional evolutionary solutions producing monophonic and non-thematic music. We conducted a large battery of tests to evaluate MUSEC’s effectiveness and efficiency in both sentiment analysis and composition. It was trained on a specially constructed set of 120 MIDI pieces, including 70 sentiment-annotated pieces: the first significant dataset of sentiment-labeled MIDI music made available online as a benchmark for future research in this area. Results are encouraging and highlight the potential of our approach in different application domains, ranging over music information retrieval, music composition, assistive music therapy, and emotional intelligence.},
journal = {Soft Comput.},
month = jul,
pages = {9875–9925},
numpages = {51},
keywords = {Music sentiment analysis, MIDI, Evolutionary algorithms, Algorithmic composition, Supervised learning, Fuzzy classification}
}

@inproceedings{10.1145/3395260.3395275,
author = {Liu, Bingran},
title = {Neural Question Generation based on Seq2Seq},
year = {2020},
isbn = {9781450377072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395260.3395275},
doi = {10.1145/3395260.3395275},
abstract = {Neural Question Generation is the use of deep neural networks to extract target answers from a given article or paragraph and generate questions based on the target answers. There is a problem in the previous NQG(Neural Question Generation) model, and the generated question does not explicitly connect with the context in the target answer, resulting in a large part of the generated question containing the target answer and the accuracy is not high. In this paper, a QG model based on seq2seq is used, which consists of encode and decoder, and adds the attention mechanism and copy mechanism. We use special tags to replace the target answer of the original paragraph, and use the paragraph and target answer as input to reduce the number of incorrect questions, including the correct answer. Through the partial copy mechanism based on character overlap, we can make the generation problem have higher overlap and relevance at the word level and the input document. Experiments show that our proposed model performs better than before.},
booktitle = {Proceedings of the 2020 5th International Conference on Mathematics and Artificial Intelligence},
pages = {119–123},
numpages = {5},
keywords = {Deep neural network, Question generation, Seq2seq model},
location = {Chengdu, China},
series = {ICMAI '20}
}

@article{10.1016/j.knosys.2019.105424,
author = {Liu, Xiaoshuang and Luo, Senlin and Pan, Limin},
title = {Robust boosting via self-sampling},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105424},
doi = {10.1016/j.knosys.2019.105424},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Boosting, Loss function, Robustness, Self-sampling}
}

@article{10.1007/s10489-020-01730-3,
author = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
title = {Query set centered sparse projection learning for set based image classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01730-3},
doi = {10.1007/s10489-020-01730-3},
abstract = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
journal = {Applied Intelligence},
month = oct,
pages = {3400–3411},
numpages = {12},
keywords = {Query set, Sparse projection learning, Set based image classification, Discriminate subspace learning}
}

@article{10.1109/TCBB.2015.2476790,
author = {Deng, Su-Ping and Zhu, Lin and Huang, De-Shuang},
title = {Predicting hub genes associated with cervical cancer through gene co-expression networks},
year = {2016},
issue_date = {January/February 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2476790},
doi = {10.1109/TCBB.2015.2476790},
abstract = {Cervical cancer is the third most common malignancy in women worldwide. It remains a leading cause of cancer-related death for women in developing countries. In order to contribute to the treatment of the cervical cancer, in our work, we try to find a few key genes resulting in the cervical cancer. Employing functions of several bioinformatics tools, we selected 143 differentially expressed genes (DEGs) associated with the cervical cancer. The results of bioinformatics analysis show that these DEGs play important roles in the development of cervical cancer. Through comparing two differential co-expression networks (DCNs) at two different states, we found a common sub-network and two differential sub-networks as well as some hub genes in three sub-networks. Moreover, some of the hub genes have been reported to be related to the cervical cancer. Those hub genes were analyzed from Gene Ontology function enrichment, pathway enrichment and protein binding three aspects. The results can help us understand the development of the cervical cancer and guide further experiments about the cervical cancer.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {27–35},
numpages = {9},
keywords = {cervical cancer, co-expression network, differentially expressed genes, hub genes}
}

@article{10.3233/THC-218026,
author = {Zhou, Zhiming and Huang, Haihui and Liang, Yong},
title = {Cancer classification and biomarker selection via a penalized logsum network-based logistic regression model},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {S1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-218026},
doi = {10.3233/THC-218026},
journal = {Technol. Health Care},
month = jan,
pages = {287–295},
numpages = {9},
keywords = {Regularization, gene selection, log-sum penalty, network-based knowledge}
}

@inbook{10.5555/3454287.3454459,
author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
title = {Meta-weight-net: learning an explicit mapping for sample weighting},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {172},
numpages = {12}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study}
}

@inproceedings{10.1145/3077981.3078031,
author = {Robinson, Carl Peter and Li, Baihua and Meng, Qinggang and Pain, Matthew T.G.},
title = {Pattern Classification of Hand Movements using Time Domain Features of Electromyography},
year = {2017},
isbn = {9781450352093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077981.3078031},
doi = {10.1145/3077981.3078031},
abstract = {Myoelectric control of prostheses is a long-established technique, using surface electromyography (sEMG) to detect the electrical signals of muscle activity and perform subsequent mechanical actions. Despite several decades' research, robust, responsive and intuitive control schemes remain elusive. Current commercial hardware advances offer a variety of movements but the control systems are unnatural, using sequential switching methods triggered by specific sEMG signals. However, recent research with pattern recognition and simultaneous and proportional control shows good promise for natural myoelectric control. This paper investigates several sEMG time domain features using a series of hand movements performed by 11 subjects, taken from a benchmark database, to determine if optimal classification accuracy is dependent on feature set size. The features were extracted from the data using a sliding window process and applied to five machine learning classifiers, of which Random Forest consistently performed best. Results suggest a few simple features such as Root Mean Square and Waveform Length achieve comparable performance to using the entire feature set, when identifying the hand movements, although further work is required for feature optimisation.},
booktitle = {Proceedings of the 4th International Conference on Movement Computing},
articleno = {27},
numpages = {6},
keywords = {Electromyography, Machine learning, Myoelectric control, Time domain features},
location = {London, United Kingdom},
series = {MOCO '17}
}

@inproceedings{10.1109/SPLC.2008.73,
author = {Benavides, David and Ruiz-Cort\'{e}s, Antonio and Batory, Don and Heymans, Patrick},
title = {First International Workshop on Analysis of Software Product Lines (ASPL'08)},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.73},
doi = {10.1109/SPLC.2008.73},
abstract = {The automation of software product line (SPL) analyses is of growing interest to both practitioners and researchers. In particular, automated analyses of variability models (like feature or decision models) and languages that foster declarative specifications of programs using those models are now common. We note that many of the problems that SPL engineers face are related to configuration problems that have been addressed by the Artificial Intelligence (AI) community. Indeed, the SPL community is using some of their results, e.g., BDD, CSP and SAT solvers.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {385},
series = {SPLC '08}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Variability, Configuration workflow, Process mining, Process discovery, Clustering}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {adaptation, dynamic software product lines, evolution, machine learning},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {benchmark, feature location, reverse engineering, software product lines, variability mining},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2829966.2829967,
author = {Danieli, Morena and Riccardi, Giuseppe and Alam, Firoj},
title = {Emotion Unfolding and Affective Scenes: A Case Study in Spoken Conversations},
year = {2015},
isbn = {9781450339889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2829966.2829967},
doi = {10.1145/2829966.2829967},
abstract = {The manifestation of human emotions evolves over time and space. Most of the work on affective computing research is limited to the association of context-free signal segments, such as utterances and images, to basic emotions. In this paper, we discuss the hypothesis that interpreting emotions requires a conceptual description of their dynamics within the context of their manifestations. We describe the unfolding of emotions through the proposed affective scene framework. Affective scenes are defined in terms of who first expresses the variation in their emotional state in a conversation, how this affects the other speaker's emotional appraisal and response, and which modifications occur from the initial through the final state of the scene. This conceptual framework is applied and evaluated on real human-human conversations drawn from call centers. We show that the automatic classification of affective scenes achieves more than satisfactory results and it benefits from acoustic, lexical and psycholinguistic features of the speech and linguistics signals.},
booktitle = {Proceedings of the International Workshop on Emotion Representations and Modelling for Companion Technologies},
pages = {5–11},
numpages = {7},
keywords = {affective scene, computational paralinguistics, emotion, machine learning, spoken conversation},
location = {Seattle, Washington, USA},
series = {ERM4CT '15}
}

@article{10.1016/j.procs.2019.12.135,
author = {Jamil, Muhammad Abid and Nour, Mohamed K and Alhindi, Ahmad and Awang Abhubakar, Normi Sham and Arif, Muhammad and Aljabri, Tareq Fahad},
title = {Towards Software Product Lines Optimization Using Evolutionary Algorithms},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {163},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.135},
doi = {10.1016/j.procs.2019.12.135},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {527–537},
numpages = {11},
keywords = {Search Based Software Engineering, Software Testing, Software Product Lines, Feature Models, Multi-objective Algorithms}
}

@inproceedings{10.1007/978-3-030-67658-2_34,
author = {Yamaguchi, Akihiro and Maya, Shigeru and Ueno, Ken},
title = {RLTS: Robust Learning Time-Series Shapelets},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_34},
doi = {10.1007/978-3-030-67658-2_34},
abstract = {Shapelets are time-series segments effective for classifying time-series instances. Joint learning of both classifiers and shapelets has been studied in recent years because such a method provides both superior classification performance and interpretable results. For robust learning, we introduce Self-Paced Learning (SPL) and adaptive robust losses into this method. The SPL method can assign latent instance weights by considering not only classification losses but also understandable shapelet discovery. Furthermore, the adaptive robustness introduced into feature vectors is jointly learned with shapelets, a classifier, and latent instance weights. We demonstrate the superiority of AUC and the validity of our approach on UCR time-series datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {595–611},
numpages = {17},
keywords = {Time-series shapelets, Self-paced learning, Robust losses},
location = {Ghent, Belgium}
}

@inproceedings{10.1109/COMPSAC.2015.64,
author = {He, Xiao and Fu, Yanmei and Sun, Chang-Ai and Ma, Zhiyi and Shao, Weizhong},
title = {Towards Model-Driven Variability-Based Flexible Service Compositions},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.64},
doi = {10.1109/COMPSAC.2015.64},
abstract = {In cloud computing, variability becomes a crucial ability of process-based cloud applications. Existing solutions either focused on modeling variability in the architectural model or tried to support dynamic variability management in implementation. An integrated approach that can inherit the virtues from both categories is expected. The paper aims to fill the gap by proposing a model-driven variability-based service composition approach. We propose VxUML to model the variability in the architecture model. Then, we define a set of model transformation rules to convert VxUML into VxBPEL (an extension to standard BPEL supporting variability at the implementation level). Finally, we implement a prototype tool, and present a case study to demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 02},
pages = {298–303},
numpages = {6},
keywords = {Model transformation, Model-driven engineering, Service composition, Variability management},
series = {COMPSAC '15}
}

@inproceedings{10.5555/3172077.3172181,
author = {Li, Hao and Gong, Maoguo},
title = {Self-paced convolutional neural networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks. In order to distinguish the reliable data from the noisy and confusing data, we improve CNNs with self-paced learning (SPL) for enhancing the learning robustness of CNNs. In the proposed self-paced convolutional network (SPCN), each sample is assigned to a weight to reflect the easiness of the sample. Then a dynamic self-paced function is incorporated into the leaning objective of CNN to jointly learn the parameters of CNN and the latent weight variable. SPCN learns the samples from easy to complex and the sample weights can dynamically control the learning rates for converging to better values. To gain more insights of SPCN, theoretical studies are conducted to show that SPCN converges to a stationary solution and is robust to the noisy and confusing data. Experimental results on MNIST and  rectangles  datasets demonstrate that the proposed method outperforms baseline methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2110–2116},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1007/s11042-020-09956-6,
author = {Yadav, Hitesh and Chhikara, Rita and Kumari, A. Charan},
title = {A novel hybrid approach for feature selection in software product lines},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {4},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09956-6},
doi = {10.1007/s11042-020-09956-6},
abstract = {Software Product Line (SPL) customizes software by combining various existing features of the software with multiple variants. The main challenge is selecting valid features considering the constraints of the feature model. To solve this challenge, a hybrid approach is proposed to optimize the feature selection problem in software product lines. The Hybrid approach ‘Hyper-PSOBBO’ is a combination of Particle Swarm Optimization (PSO), Biogeography-Based Optimization (BBO) and hyper-heuristic algorithms. The proposed algorithm has been compared with Bird Swarm Algorithm (BSA), PSO, BBO, Firefly, Genetic Algorithm (GA) and Hyper-heuristic. All these algorithms are performed in a set of 10 feature models that vary from a small set of 100 to a high-quality data set of 5000. The detailed empirical analysis in terms of performance has been carried out on these feature models. The results of the study indicate that the performance of the proposed method is higher to other state-of-the-art algorithms.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {4919–4942},
numpages = {24},
keywords = {Particle swarm optimization, Hyper-heuristic, Biogeography-based optimization, Firefly, Genetic algorithm (GA), Bird swarm optimization (BSA), Software product lines (SPL), Feature model (FM)}
}

@article{10.1016/j.neucom.2014.06.096,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng and Wang, Yanjiang and Lu, Ke},
title = {Manifold regularized kernel logistic regression for web image annotation},
year = {2016},
issue_date = {Jan 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {172},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.06.096},
doi = {10.1016/j.neucom.2014.06.096},
journal = {Neurocomput.},
month = jan,
pages = {3–8},
numpages = {6},
keywords = {Manifold regularization, Kernel logistic regression, Laplacian eigenmaps, Semi-supervised learning, Image annotation}
}

@article{10.1145/3355612,
author = {Shamsolmoali, Pourya and Zareapoor, Masoumeh and Zhou, Huiyu and Yang, Jie},
title = {AMIL: Adversarial Multi-instance Learning for Human Pose Estimation},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3355612},
doi = {10.1145/3355612},
abstract = {Human pose estimation has an important impact on a wide range of applications, from human-computer interface to surveillance and content-based video retrieval. For human pose estimation, joint obstructions and overlapping upon human bodies result in departed pose estimation. To address these problems, by integrating priors of the structure of human bodies, we present a novel structure-aware network to discreetly consider such priors during the training of the network. Typically, learning such constraints is a challenging task. Instead, we propose generative adversarial networks as our learning model in which we design two residual Multiple-Instance Learning (MIL) models with identical architecture—one is used as the generator, and the other one is used as the discriminator. The discriminator task is to distinguish the actual poses from the fake ones. If the pose generator generates results that the discriminator is not able to distinguish from the real ones, then the model has successfully learned the priors. In the proposed model, the discriminator differentiates the ground-truth heatmaps from the generated ones, and later the adversarial loss back-propagates to the generator. Such procedure assists the generator to learn reasonable body configurations and is proved to be advantageous to improve the pose estimation accuracy. Meanwhile, we propose a novel function for MIL. It is an adjustable structure for both instance selection and modeling to appropriately pass the information between instances in a single bag. In the proposed residual MIL neural network, the pooling action adequately updates the instance contribution to its bag. The proposed adversarial residual multi-instance neural network that is based on pooling has been validated on two datasets for the human pose estimation task and successfully outperforms the other state-of-the-art models. The code will be made available on https://github.com/pshams55/AMIL.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {23},
numpages = {23},
keywords = {Pose estimations, adversarial network, multiple-instance learning, neural networks}
}

@phdthesis{10.5555/AAI28544034,
author = {Khoshmanesh, Seyedehzahra and Samik, Basu, and Andrew, Miner, and Hridesh, Rajan, and Karin, Dorman,},
advisor = {R, Lutz, Robyn},
title = {Learning Feature Interactions with and without Specifications},
year = {2021},
isbn = {9798544278207},
publisher = {Iowa State University},
address = {USA},
abstract = {Developers of software product lines and highly configurable systems reuse and combine features (units of functionality) to build new or customize existing products. However, features can interact in ways that are contrary to developers' intent. Predicting whether a new combination of features will produce an unwanted or even hazardous feature interaction is a continuing challenge. Current techniques to detect unwanted feature interactions are costly, slow, and inadequate. In this thesis, we investigate how to detect unwanted feature interactions early in development and that are scalable to large software product lines or highly configurable systems. First, we propose a similarity-based method to identify unwanted feature interactions much earlier in the development process for early detection. It uses knowledge of prior feature interactions stored with the software product line's feature model to help find unwanted interactions between a new feature and existing features. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.Moreover, to learn and automate the detection, we show how detecting unwanted feature interactions can be effectively represented as a link prediction problem. We investigate six link-based similarity metrics and evaluate our approach on a software product line benchmark. Results show that the best machine learning algorithms achieve an accuracy of 0.75 to 1 for classifying feature interactions.Finally, we develop a new approach based on program analysis that extracts feature-relevant learning models from the source code to obtain more semantic details of unwanted feature interactions. The method is capable of learning feature interactions whether constraints on feature combinations are specified or not. If specifications of feature constraints are unavailable, as is common in real-world systems, our approach infers the constraints using feature-related data-flow dependency information. Experimental evaluation on three software product line benchmarks and a highly configurable system shows that this approach is fast and effective.The contribution is to support developers by automatically detecting those feature combinations in a new product or version that can interact in unwanted or unrecognized ways. This enables a better understanding of hidden interactions and identifies software components that should be tested together because their features interact in some configurations.},
note = {AAI28544034}
}

@inproceedings{10.5555/3491440.3491568,
author = {Guo, Dan and Wang, Yang and Song, Peipei and Wang, Meng},
title = {Recurrent relational memory network for unsupervised image captioning},
year = {2021},
isbn = {9780999241165},
abstract = {Unsupervised image captioning with no annotations is an emerging challenge in computer vision, where the existing arts usually adopt GAN (Generative Adversarial Networks) models. In this paper, we propose a novel memory-based network rather than GAN, named Recurrent Relational Memory Network (R2M). Unlike complicated and sensitive adversarial learning that non-ideally performs for long sentence generation, R2M implements a concepts-to-sentence memory translator through two-stage memory mechanisms: fusion and recurrent memories, correlating the relational reasoning between common visual concepts and the generated words for long periods. R2M encodes visual context through unsupervised training on images, while enabling the memory to learn from irrelevant textual corpus via supervised fashion. Our solution enjoys less learnable parameters and higher computational efficiency than GAN-based methods, which heavily bear parameter sensitivity. We experimentally validate the superiority of R2M than state-of-the-arts on all benchmark datasets.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {128},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{10.1007/s10270-020-00814-5,
author = {Mussbacher, Gunter and Combemale, Benoit and Kienzle, J\"{o}rg and Abrah\~{a}o, Silvia and Ali, Hyacinth and Bencomo, Nelly and B\'{u}r, M\'{a}rton and Burgue\~{n}o, Loli and Engels, Gregor and Jeanjean, Pierre and J\'{e}z\'{e}quel, Jean-Marc and K\"{u}hn, Thomas and Mosser, S\'{e}bastien and Sahraoui, Houari and Syriani, Eugene and Varr\'{o}, D\'{a}niel and Weyssow, Martin},
title = {Opportunities in intelligent modeling assistance},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00814-5},
doi = {10.1007/s10270-020-00814-5},
abstract = {Modeling is requiring increasingly larger efforts while becoming indispensable given the complexity of the problems we are solving. Modelers face high cognitive load to understand a multitude of complex abstractions and their relationships. There is an urgent need to better support tool builders to ultimately provide modelers with intelligent modeling assistance that learns from previous modeling experiences, automatically derives modeling knowledge, and provides context-aware assistance. However, current intelligent modeling assistants (IMAs) lack adaptability and flexibility for tool builders, and do not facilitate understanding the differences and commonalities of IMAs for modelers. Such a patchwork of limited IMAs is a lost opportunity to provide modelers with better support for the creative and rigorous aspects of software engineering. In this expert voice, we present a conceptual reference framework (RF-IMA) and its properties to identify the foundations for intelligent modeling assistance. For tool builders, RF-IMA aims to help build IMAs more systematically. For modelers, RF-IMA aims to facilitate comprehension, comparison, and integration of IMAs, and ultimately to provide more intelligent support. We envision a momentum in the modeling community that leads to the implementation of RF-IMA and consequently future IMAs. We identify open challenges that need to be addressed to realize the opportunities provided by intelligent modeling assistance.},
journal = {Softw. Syst. Model.},
month = sep,
pages = {1045–1053},
numpages = {9},
keywords = {Model-based software engineering, Intelligent modeling assistance, Integrated development environment, Artificial intelligence, Development data, Feedback}
}

@inproceedings{10.1109/SPLC.2008.11,
author = {Niu, Nan and Easterbrook, Steve},
title = {On-Demand Cluster Analysis for Product Line Functional Requirements},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.11},
doi = {10.1109/SPLC.2008.11},
abstract = {We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {87–96},
numpages = {10},
keywords = {functional requirements profiles, information-theoretic clustering, overlapping clustering, requirements clustering},
series = {SPLC '08}
}

@article{10.5555/2946645.3007037,
author = {Wei, Ermo and Luke, Sean},
title = {Lenient learning in independent-learner stochastic cooperative games},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional ("Distributed") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2914–2955},
numpages = {42},
keywords = {game theory, independent learner, lenient learning, multiagent learning, reinforcement learning}
}

@inproceedings{10.1145/3459637.3482458,
author = {Keramati, Mahsa and Zohrevand, Zahra and Gl\"{a}sser, Uwe},
title = {Norma: A Hybrid Feature Alignment for Class-Aware Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482458},
doi = {10.1145/3459637.3482458},
abstract = {Unsupervised domain adaptation is the problem of transferring extracted knowledge from a labeled source domain to an unlabeled target domain. To achieve discriminative domain adaptation recent studies take advantage of target sample pseudo-labels to impose class-aware distribution alignment across the source and target domains. Still, they have some shortcomings such as making decisions based on inaccurate pseudo-labeled samples that mislead the adaptation process. In this paper, we propose a progressive deep feature alignment, called Norma, to tackle class-aware unsupervised domain adaptation for image classification by enforcing inter-class compactness and intra-class discrepancy through a hybrid learning process. To this end, Norma's optimization process is defined based on a novel triplet loss which not only addresses soft prototype alignment but also pushes away multiple negative centroids. Also, to extract maximum discriminative domain knowledge per iteration, we propose a joint positive and negative learning procedure along with an uncertainty-guided progressive pseudo-labeling on the basis of prototype-based clustering and conditional probability. Our experimental results on several benchmarks demonstrate that Norma outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {833–843},
numpages = {11},
keywords = {adversarial unsupervised domain adaptation, class-aware alignment, deep-metric learning, image classification, negative learning, pseudo-labeling, transfer learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1109/ICTAI.2014.107,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {A Comparative Analysis of Two Multi-objective Evolutionary Algorithms in Product Line Architecture Design Optimization},
year = {2014},
isbn = {9781479965724},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2014.107},
doi = {10.1109/ICTAI.2014.107},
abstract = {The Product Line Architecture (PLA) design is a multi-objective optimization problem that can be properly solved with search-based algorithms. However, search-based PLA design is an incipient research field. Due to this, works in this field have addressed main points to solve the problem: adequate representation, specific search operators and suitable evaluation fitness functions. Similarly what happens in the search-based design of traditional software, existing works on search-based PLA design use NSGA-II, without evaluating the characteristics of this algorithm, such as the use of crossover operator. Considering this fact, this paper reports results from a comparative analysis of two algorithms, NSGA-II and PAES, to the PLA design problem. PAES was chosen because it implements a different evolution strategy that does not employ crossover. An experimental study was carried out with nine PLAs and results of the conducted study attest that NSGA-II performs better than PAES in the PLA design context.},
booktitle = {Proceedings of the 2014 IEEE 26th International Conference on Tools with Artificial Intelligence},
pages = {681–688},
numpages = {8},
keywords = {multi-objective algorithms, product line architecture design, software product line},
series = {ICTAI '14}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Continuous experimentation, A/B testing, Machine learning, Multi-armed bandits, Combinatorial optimization}
}

@article{10.1016/j.neucom.2019.11.104,
author = {Ren, Yazhou and Huang, Shudong and Zhao, Peng and Han, Minghao and Xu, Zenglin},
title = {Self-paced and auto-weighted multi-view clustering},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.104},
doi = {10.1016/j.neucom.2019.11.104},
journal = {Neurocomput.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Self-paced learning, Multi-view clustering, Soft weighting}
}

@article{10.1007/s10462-018-9675-6,
author = {Vikatos, Pantelis and Gryllos, Prokopios and Makris, Christos},
title = {Marketing campaign targeting using bridge extraction in multiplex social network},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-018-9675-6},
doi = {10.1007/s10462-018-9675-6},
abstract = {In this paper, we introduce a methodology for improving the targeting of marketing campaigns using bridge prediction in communities based on the information of multilayer online social networks. The campaign strategy involves the identification of nodes with high brand loyalty and top-ranking nodes in terms of participation in bridges that will be involved in the evolution of the graph. Our approach is based on an efficient classification model combining topological characteristics of crawled social graphs with sentiment and linguistic traits of user-nodes, popularity in social media as well as meta path-based features of multilayer networks. To validate our approach we present a set of experimental results using a well-defined dataset from Twitter and Foursquare. Our methodology is useful to recommendation systems as well as to marketers who are interested to use social influence and run effective marketing campaigns.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {703–724},
numpages = {22},
keywords = {Social marketing, Influence metric, Link prediction, Graph mining, Sentiment analysis}
}

@inproceedings{10.1145/3434780.3436640,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Theron, Roberto},
title = {Advances in the use of domain engineering to support feature identification and generation of information visualizations},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436640},
doi = {10.1145/3434780.3436640},
abstract = {Information visualization tools are widely used to better understand large and complex datasets. However, to make the most out of them, it is necessary to rely on proper designs that consider not only the data to be displayed, but also the audience and the context. There are tools that already allow users to configure their displays without requiring programming skills, but this research project aims at exploring the automatic generation of information visualizations and dashboards in order to avoid the configuration process, and select the most suitable features of these tools taking into account their contexts. To address this problem, a domain engineering, and machine learning approach is proposed.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1053–1056},
numpages = {4},
keywords = {Automatic generation, Domain engineering, High-level requirements, Information Dashboards, Machine Learning, Meta-modeling},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@inproceedings{10.1007/978-3-030-26061-3_6,
author = {Avci, Umut and Akkurt, Gamze and Unay, Devrim},
title = {A Pattern Mining Approach in Feature Extraction for Emotion Recognition from Speech},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_6},
doi = {10.1007/978-3-030-26061-3_6},
abstract = {We address the problem of recognizing emotions from speech using features derived from emotional patterns. Because much work in the field focuses on using low-level acoustic features, we explicitly study whether high-level features are useful for classifying emotions. For this purpose, we convert a continuous speech signal to a discretized signal and extract discriminative patterns that are capable of distinguishing distinct emotions from each other. Extracted patterns are then used to create a feature set to be fed into a classifier. Experimental results show that patterns alone are good predictors of emotions. When used to build a classifier, pattern features achieve accuracy gains up&nbsp;to 25% compared to state-of-the-art acoustic features.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {54–63},
numpages = {10},
keywords = {Emotion recognition, Speech processing, Pattern mining, Feature extraction},
location = {Istanbul, Turkey}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1155/2012/512159,
author = {Mazzeo, Pier Luigi and Leo, Marco and Spagnolo, Paolo and Nitti, Massimiliano},
title = {Soccer ball detection by comparing different feature extraction methodologies},
year = {2012},
issue_date = {January 2012},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2012},
issn = {1687-7470},
url = {https://doi.org/10.1155/2012/512159},
doi = {10.1155/2012/512159},
abstract = {This paper presents a comparison of different feature extraction methods for automatically recognizing soccer ball patterns through a probabilistic analysis. It contributes to investigate different well-known feature extraction approaches applied in a soccer environment, in order tomeasure robustness accuracy and detection performances. This work, evaluating differentmethodologies, permits to select the one which achieves best performances in terms of detection rate and CPU processing time. The effectiveness of the differentmethodologies is demonstrated by a huge number of experiments on real ball examples under challenging conditions.},
journal = {Adv. in Artif. Intell.},
month = jan,
articleno = {6},
numpages = {1}
}

@article{10.1155/2021/5089236,
author = {Chen, Yu and Tang, Zhong and Ding, Baiyuan},
title = {Research on the Construction of Intelligent Community Emergency Service Platform Based on Convolutional Neural Network},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/5089236},
doi = {10.1155/2021/5089236},
abstract = {Aiming at the shortcomings of the existing community emergency service platform, such as single function, poor scalability, and strong subjectivity, an intelligent community emergency service platform based on convolutional neural network was constructed. Firstly, the requirements analysis of the emergency service platform was carried out, and the functional demand of the emergency service platform was analyzed from the aspects of community environment, safety, infrastructure, health management, emergency response, and so on. Secondly, through logistics network, big data, cloud computing, artificial intelligence, and all kinds of applications, the intelligent community emergency service platform was designed. Finally, a semantic matching emergency question answering system based on convolutional neural network was developed to provide key technical support for the emergency preparation stage of intelligent community. The results show that the intelligent community emergency service platform plays an important role in preventing community emergency events and taking active and effective measures to ensure the health and safety of community residents.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1109/ASE.2013.6693103,
author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
title = {Measuring the structural complexity of feature models},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693103},
doi = {10.1109/ASE.2013.6693103},
abstract = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {automated analysis, feature model, performance measurement, software product line},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {Design smell detection, Machine learning, Software metrics, Project context information, God class}
}

@article{10.1007/s10506-021-09285-5,
author = {Castellanos-Ardila, Julieth Patricia and Gallina, Barbara and Governatori, Guido},
title = {Compliance-aware engineering process plans: the case of space software engineering processes},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0924-8463},
url = {https://doi.org/10.1007/s10506-021-09285-5},
doi = {10.1007/s10506-021-09285-5},
abstract = {Safety-critical systems manufacturers have the duty of care, i.e., they should take correct steps while performing acts that could foreseeably harm others. Commonly, industry standards prescribe reasonable steps in their process requirements, which regulatory bodies trust. Manufacturers perform careful documentation of compliance with each requirement to show that they act under acceptable criteria. To facilitate this task, a safety-centered planning-time framework, called ACCEPT, has been proposed. Based on compliance-by-design, ACCEPT capabilities (i.e., processes and standards modeling, and automatic compliance checking) permit to design Compliance-aware Engineering Process Plans (CaEPP), which are able to show the planning-time allocation of standard demands, i.e., if the elements set down by the standard requirements are present at given points in the engineering process plan. In this paper, we perform a case study to understand if the ACCEPT produced models could support the planning of space software engineering processes. Space software is safety and mission-critical, and it is often the result of industrial cooperation. Such cooperation is coordinated through compliance with relevant standards. In the European context, ECSS-E-ST-40C is the de-facto standard for space software production. The planning of processes in compliance with project-specific ECSS-E-ST-40C applicable requirements is mandatory during contractual agreements. Our analysis is based on qualitative criteria targeting the effort dictated by task demands required to create a CaEPP for software development with ACCEPT. Initial observations show that the effort required to model compliance and processes artifacts is significant. However, such an effort pays off in the long term since models are, to some extend, reusable and flexible. The coverage level of the models is also analyzed based on design decisions. In our opinion, such a level is adequate since it responds to the information needs required by the ECSS-E-ST-40C framework.},
journal = {Artif. Intell. Law},
month = dec,
pages = {587–627},
numpages = {41},
keywords = {Process compliance checking, Software process plan, ECSS-E-ST-40C}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {Unsupervised feature selection, Hypergraph learning, Latent representation learning, Local structure preservation, 00-01, 99-00}
}

@inproceedings{10.1007/978-3-030-00308-1_3,
author = {Hess, Timm and Mundt, Martin and Weis, Tobias and Ramesh, Visvanathan},
title = {Large-Scale Stochastic Scene Generation and Semantic Annotation for Deep Convolutional Neural Network Training in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_3},
doi = {10.1007/978-3-030-00308-1_3},
abstract = {Object detection and classification are essential tasks for any robotics scenario, where data-driven approaches, specifically deep learning techniques, have been widely adopted in recent years. However, in the context of the RoboCup standard platform league these methods have not yet gained comparable popularity in large part due to the lack of (publicly) available large enough data sets that involve a tedious gathering and error-prone manual annotation process. We propose a framework for stochastic scene generation, rendering and automatic creation of semantically annotated ground truth masks. Used as training data in conjunction with deep convolutional neural networks we demonstrate compelling classification accuracy on real-world data in a multi-class setting. An evaluation on multiple neural network architectures with varying depth and representational capacity, corresponding run-times on current NAO-H25 hardware, and required sampled training data is provided.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {33–44},
numpages = {12},
keywords = {Deep Convolutional Neural Networks, RoboCup SPL, Standard Platform League (SPL), Robotics, Static Head Pose},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3127479.3131621,
author = {Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker},
title = {Optimized on-demand data streaming from sensor nodes},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131621},
doi = {10.1145/3127479.3131621},
abstract = {Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {586–597},
numpages = {12},
keywords = {adaptive sampling, on-demand streaming, oversampling, real-time analysis, sensor data, sensor sharing, user-defined sampling},
location = {Santa Clara, California},
series = {SoCC '17}
}

@article{10.1016/j.cviu.2021.103255,
author = {Landi, Federico and Baraldi, Lorenzo and Cornia, Marcella and Corsini, Massimiliano and Cucchiara, Rita},
title = {Multimodal attention networks for low-level vision-and-language navigation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {210},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103255},
doi = {10.1016/j.cviu.2021.103255},
journal = {Comput. Vis. Image Underst.},
month = sep,
numpages = {9},
keywords = {68T01, 68T40, 68T45, Vision-and-language navigation, Embodied AI, Multi-modal attention}
}

@inproceedings{10.5555/3540261.3542579,
author = {Ostapenko, Oleksiy and Rodr\'{\i}guez, Pau and Caccia, Massimo and Charlin, Laurent},
title = {Continual learning via local module composition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2318},
numpages = {15},
series = {NIPS '21}
}

@inproceedings{10.1109/PLEASE.2015.15,
author = {Buchmann, Thomas and Baumgartl, Johannes and Henrich, Dominik and Westfechtel, Bernhard},
title = {Robots and their Variability -- A Societal Challenge and a Potential Solution},
year = {2015},
isbn = {9781467370615},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PLEASE.2015.15},
doi = {10.1109/PLEASE.2015.15},
abstract = {A robot is essentially a real-time, distributed embedded system operating in a physical environment. Often, control and communication paths within the system are tightly coupled to the actual hardware configuration of the robot. Furthermore, the domain contains a high amount of variability on different levels, ranging from hardware, over software to the environment in which the robot is operated. Today, special robots are used in households to perform monotonous and recurring tasks like vacuuming or mowing the lawn. In the future there may be robots that can be configured and programmed for more complicated tasks, like washing dishes or cleaning up or to assist elderly people. Nowadays, programming a robot is a highly complex and challenging task, which can be carried out only by programmers with dedicated background in robotics. Societal acceptance of robots can only be achieved, if they are easy to program. In this paper we present our approach to provide customized programming environments enabling programmers without background knowledge in robotics to specify robot programs. Our solution was realized using product line techniques.},
booktitle = {Proceedings of the 2015 IEEE/ACM 5th International Workshop on Product LinE Approaches in Software Engineering},
pages = {27–30},
numpages = {4},
keywords = {code generation, dsl, model-driven development, robot, software product line},
series = {PLEASE '15}
}

@article{10.1016/j.eswa.2007.09.057,
author = {Wu, Chia-Wei and Tsai, Richard Tzong-Han and Lee, Cheng-Wei and Hsu, Wen-Lian},
title = {Web taxonomy integration with hierarchical shrinkage algorithm and fine-grained relations},
year = {2008},
issue_date = {November, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {35},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.09.057},
doi = {10.1016/j.eswa.2007.09.057},
abstract = {We address the problem of integrating web taxonomies from different real Internet applications. Integrating web taxonomies is to transfer instances from a source to target taxonomy. Unlike the conventional text categorization problem, in taxonomy integration, the source taxonomy contains extra information that can be used to improve the categorization. The major existing methods can be divided in two types: those that use neighboring categories to smooth the document term vector and those that consider the semantic relationship between corresponding categories of the target and source taxonomies to facilitate categorization. In contrast to the first type of approach, which only uses a flattened hierarchy for smoothing, we apply a hierarchy shrinkage algorithm to smooth child documents by their parents. We also discuss the effect of using different hierarchical levels for smoothing. To extend the second type of approach, we extract fine-grain semantic relationships, which consider the relationships between lower-level categories. In addition, we use the cosine similarity to measure the semantic relationships, which achieves better performance than existing methods. Finally, we integrate the existing approaches and the proposed methods into one machine learning model to find the best feature configuration. The results of experiments on real Internet data demonstrate that our system outperforms standard text classifiers by about 10%.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {2123–2131},
numpages = {9},
keywords = {Shrinkage algorithm, Text categorization, Web taxonomy integration}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {decision modelling, design-space exploration, multi-criteria decision analysis, multi-objective optimisation},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3377930.3389815,
author = {Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd},
title = {Multi-objective hyperparameter tuning and feature selection using filter ensembles},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389815},
doi = {10.1145/3377930.3389815},
abstract = {Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well.We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles.While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {471–479},
numpages = {9},
keywords = {evolutionary algorithms, feature selection, hyperparameter optimization, model-based optimization, multiobjective optimization},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1016/j.neucom.2014.12.100,
author = {Garcia, Lu\'{\i}s P.F. and Carvalho, Andr\'{e} C.P.L.F. de and Lorena, Ana C.},
title = {Noise detection in the meta-learning level},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {176},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.12.100},
doi = {10.1016/j.neucom.2014.12.100},
abstract = {The presence of noise in real data sets can harm the predictive performance of machine learning algorithms. There are several noise filtering techniques whose goal is to improve the quality of the data in classification tasks. These techniques usually scan the data for noise identification in a preprocessing step. Nonetheless, this is a non-trivial task and some noisy data can remain unidentified, while safe data can also be removed. The bias of each filtering technique influences its performance on a particular data set. Therefore, there is no single technique that can be considered the best for all domains or data distribution and choosing a particular filter is not straightforward. Meta-learning has been largely used in the last years to support the recommendation of the most suitable machine learning algorithm(s) for a new data set. This paper presents a meta-learning recommendation system able to predict the expected performance of noise filters in noisy data identification tasks. For such, a meta-base is created, containing meta-features extracted from several corrupted data sets along with the performance of some noise filters when applied to these data sets. Next, regression models are induced from this meta-base to predict the expected performance of the investigated filters in the identification of noisy data. The experimental results show that meta-learning can provide a good recommendation of the most promising filters to be applied to new classification data sets.},
journal = {Neurocomput.},
month = feb,
pages = {14–25},
numpages = {12},
keywords = {Characterization measures, Complexity measures, Meta-learning, Noise identification}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Arabic, Classification, Eigenvectors, Fisher, Linear discriminant analysis, Text}
}

@inproceedings{10.5555/3053577.3053583,
author = {Hubaux, Arnaud and Jannach, Dietmar and Drescher, Conrad and Murta, Leonardo and M\"{a}nnist\"{o}, Tomi and Czarnecki, Krzysztof and Heymans, Patrick and Nguyen, Tien and Zanker, Markus},
title = {Unifying software and product configuration: a research roadmap},
year = {2012},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {For more than 30 years, knowledge-based product configuration systems have been successfully applied in many industrial domains. Correspondingly, a large number of advanced techniques and algorithms have been developed in academia and industry to support different aspects of configuration reasoning. While traditional research in the field focused on the configuration of physical artefacts, recognition of the business value of customizable software products led to the emergence of software product line engineering. Despite the significant overlap in research interests, the two fields mainly evolved in isolation. Only limited attempts were made at combining the approaches developed in the different fields. In this paper, we first aim to give an overview of commonalities and differences between software product line engineering and product configuration. We then identify opportunities for cross-fertilization between these fields and finally develop a research agenda to combine their respective techniques. Ultimately, this should lead to a unified configuration approach.},
booktitle = {Proceedings of the 2012 International Conference on Configuration - Volume 958},
pages = {31–35},
numpages = {5},
location = {Montpellier, France},
series = {CONFWS'12}
}

@article{10.1016/j.imavis.2016.06.005,
author = {Leng, Mengjun and Moutafis, Panagiotis and Kakadiaris, Ioannis A.},
title = {Joint prototype and metric learning for image set classification: Application to video face identification},
year = {2017},
issue_date = {Feb 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {58},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.06.005},
doi = {10.1016/j.imavis.2016.06.005},
journal = {Image Vision Comput.},
month = feb,
pages = {204–213},
numpages = {10},
keywords = {Image set classification, Metric learning, Prototype learning, Video face recognition}
}

@article{10.1177/0165551515591724,
author = {Onan, Aytu\u{g}},
title = {Classifier and feature set ensembles for web page classification},
year = {2016},
issue_date = {4 2016},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {42},
number = {2},
issn = {0165-5515},
url = {https://doi.org/10.1177/0165551515591724},
doi = {10.1177/0165551515591724},
abstract = {Web page classification is an important research direction on web mining. The abundant amount of data available on the web makes it essential to develop efficient and robust models for web mining tasks. Web page classification is the process of assigning a web page to a particular predefined category based on labelled data. It serves for several other web mining tasks, such as focused web crawling, web link analysis and contextual advertising. Machine learning and data mining methods have been successfully applied for several web mining tasks, including web page classification. Multiple classifier systems are a promising research direction in machine learning, which aims to combine several classifiers by differentiating base classifiers and/or dataset distributions so that more robust classification models can be built. This paper presents a comparative analysis of four different feature selections correlation, consistency, information gain and chi-square-based feature selection and four different ensemble learning methods Boosting, Bagging, Dagging and Random Subspace based on four different base learners naive Bayes, K-nearest neighbour algorithm, C4.5 algorithm and FURIA algorithm. The article examines the predictive performance of ensemble methods for web page classification. The experimental results indicate that feature selection and ensemble learning can enhance the predictive performance of classifiers in web page classification. For the DMOZ-50 dataset, the highest average predictive performance 88.1% is obtained with the combination of consistency-based feature selection with AdaBoost and naive Bayes algorithms, which is a promising result for web page classification. Experimental results indicate that Bagging and Random Subspace ensemble methods and correlation-based and consistency-based feature selection methods obtain better results in terms of accuracy rates.},
journal = {J. Inf. Sci.},
month = apr,
pages = {150–165},
numpages = {16},
keywords = {Ensemble learning, multiple classifiers, web page classification}
}

@article{10.1155/2020/8845932,
author = {Xu, Hui and Cheng, Hongju},
title = {Distinguishing Hand Drawing Style Based on Multilevel Analytics Framework},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/8845932},
doi = {10.1155/2020/8845932},
abstract = {Hand drawing is an indispensable professional skill in the fields of environmental design, industrial design, architectural engineering, civil engineering, and other engineering design education. Students usually imitate masterpieces to practice basic skills, which is an important link for a beginner. A system for digital management requires a function for an automatic recommendation task of different brushwork skill expressions. Thus, the classification method for brushwork is to combine hand-crafted features generated by DCNN and then use the final features for input to a tree structure classification scheme. The method improvement of the other deep learning models has effectiveness in distinguishing art ontology attributes.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {10}
}

@article{10.3233/HIS-2011-0140,
author = {Ahumada, Hern\'{a}n and Grinblat, Guillermo L. and Uzal, Lucas C. and Ceccatto, Alejandro and Granitto, Pablo M.},
title = {Evaluation of a new hybrid algorithm for highly imbalanced classification problems},
year = {2011},
issue_date = {October 2011},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {4},
issn = {1448-5869},
url = {https://doi.org/10.3233/HIS-2011-0140},
doi = {10.3233/HIS-2011-0140},
abstract = {Many times in classification problems, particularly in critical real world applications, one of the classes has much less samples than the others usually known as the class imbalance problem. In this work we discuss and evaluate the use of the REPMAC algorithm to solve imbalanced problems. Using a clustering method, REPMAC recursively splits the majority class in several subsets, creating a decision tree, until the resulting sub-problems are balanced or easy to solve. We use two diverse clustering methods and three different classifiers coupled with REPMAC to evaluate the new method on several benchmark datasets spanning a wide range of number of features, samples and imbalance degree. We also apply our method to a real world problem, the identification of weed seeds. We find that the good performance of REPMAC is almost independent of the classifier or the clustering method coupled to it, which suggests that its success is mostly related to the use of an appropriate strategy to cope with imbalanced problems.},
journal = {Int. J. Hybrid Intell. Syst.},
month = oct,
pages = {199–211},
numpages = {13},
keywords = {Class Imbalance, Classification, Clustering, Hybrid Systems}
}

@inproceedings{10.1145/3430984.3431022,
author = {Virk, Jitender Singh and Bathula, Deepti R.},
title = {Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431022},
doi = {10.1145/3430984.3431022},
abstract = {Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {145–153},
numpages = {9},
keywords = {CT scans, Neural networks, Semi-supervised learning, domain-specific, image perturbations, integrated gradients, mixed-kernels neural networks, pseudo-labelling, transfer learning},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@article{10.1145/2000791.2000794,
author = {Anvik, John and Murphy, Gail C.},
title = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000794},
doi = {10.1145/2000791.2000794},
abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {10},
numpages = {35},
keywords = {Bug report triage, configuration assistance, machine learning, recommendation, task assignment}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {UML state machine, aspect-oriented modeling, behavioral variability, model-based testing, product line engineering},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/3338906.3342811,
author = {Atlee, Joanne M.},
title = {Living with feature interactions (keynote)},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342811},
doi = {10.1145/3338906.3342811},
abstract = {Feature-oriented software development enables rapid software creation and evolution, through incremental and parallel feature development or through product line engineering. However, in practice, features are often not separate concerns. They behave differently in the presence of other features, and they sometimes interfere with each other in surprising ways.  This talk will explore challenges in feature interactions and their resolutions. Resolution strategies can tackle large classes of interactions, but are imperfect and incomplete, leading to research opportunities in software architecture, composition semantics, and verification.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1},
numpages = {1},
keywords = {Feature Interactions, Feature-Oriented Software Development, Software Correctness},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/3540261.3541724,
author = {Weihs, Luca and Jain, Unnat and Liu, Iou-Jen and Salvador, Jordi and Lazebnik, Svetlana and Kembhavi, Aniruddha and Schwing, Alexander},
title = {Bridging the imitation gap by adaptive insubordination},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In practice, imitation learning is preferred over pure reinforcement learning whenever it is possible to design a teaching agent to provide expert supervision. However, we show that when the teaching agent makes decisions with access to privileged information that is unavailable to the student, this information is marginalized during imitation learning, resulting in an "imitation gap" and, potentially, poor results. Prior work bridges this gap via a progression from imitation learning to reinforcement learning. While often successful, gradual progression fails for tasks that require frequent switches between exploration and memorization. To better address these tasks and alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR). ADVISOR dynamically weights imitation and reward-based reinforcement learning losses during training, enabling on-the-fly switching between imitation and exploration. On a suite of challenging tasks set within gridworlds, multi-agent particle environments, and high-fidelity 3D simulators, we show that on-the-fly switching with ADVISOR outperforms pure imitation, pure reinforcement learning, as well as their sequential and parallel combinations.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1463},
numpages = {13},
series = {NIPS '21}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@article{10.1007/s11263-018-1112-4,
author = {Zhang, Dingwen and Han, Junwei and Zhao, Long and Meng, Deyu},
title = {Leveraging Prior-Knowledge for Weakly Supervised Object Detection Under a Collaborative Self-Paced Curriculum Learning Framework},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {4},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1112-4},
doi = {10.1007/s11263-018-1112-4},
abstract = {Weakly supervised object detection is an interesting yet challenging research topic in computer vision community, which aims at learning object models to localize and detect the corresponding objects of interest only under the supervision of image-level annotation. For addressing this problem, this paper establishes a novel weakly supervised learning framework to leverage both the instance-level prior-knowledge and the image-level prior-knowledge based on a novel collaborative self-paced curriculum learning (C-SPCL) regime. Under the weak supervision, C-SPCL can leverage helpful prior-knowledge throughout the whole learning process and collaborate the instance-level confidence inference with the image-level confidence inference in a robust way. Comprehensive experiments on benchmark datasets demonstrate the superior capacity of the proposed C-SPCL regime and the proposed whole framework as compared with state-of-the-art methods along this research line.},
journal = {Int. J. Comput. Vision},
month = apr,
pages = {363–380},
numpages = {18},
keywords = {Object detection, Self-paced larning, Weakly supervised learning}
}

@article{10.1016/j.artmed.2016.05.004,
title = {An ensemble method for extracting adverse drug events from social media},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {70},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2016.05.004},
doi = {10.1016/j.artmed.2016.05.004},
abstract = {We propose a relation extraction system to distinguish between adverse drug events (ADEs) and non-ADEs on social media.We develop a feature-based method, investigate the effectiveness of feature selection, and analyze the contributions of different features.We investigate whether kernel-based methods can effectively extract ADEs from social media.We propose several classifier ensembles to further enhance ADE extraction capabilities. ObjectiveBecause adverse drug events (ADEs) are a serious health problem and a leading cause of death, it is of vital importance to identify them correctly and in a timely manner. With the development of Web 2.0, social media has become a large data source for information on ADEs. The objective of this study is to develop a relation extraction system that uses natural language processing techniques to effectively distinguish between ADEs and non-ADEs in informal text on social media. Methods and materialsWe develop a feature-based approach that utilizes various lexical, syntactic, and semantic features. Information-gain-based feature selection is performed to address high-dimensional features. Then, we evaluate the effectiveness of four well-known kernel-based approaches (i.e., subset tree kernel, tree kernel, shortest dependency path kernel, and all-paths graph kernel) and several ensembles that are generated by adopting different combination methods (i.e., majority voting, weighted averaging, and stacked generalization). All of the approaches are tested using three data sets: two health-related discussion forums and one general social networking site (i.e., Twitter). ResultsWhen investigating the contribution of each feature subset, the feature-based approach attains the best area under the receiver operating characteristics curve (AUC) values, which are 78.6%, 72.2%, and 79.2% on the three data sets. When individual methods are used, we attain the best AUC values of 82.1%, 73.2%, and 77.0% using the subset tree kernel, shortest dependency path kernel, and feature-based approach on the three data sets, respectively. When using classifier ensembles, we achieve the best AUC values of 84.5%, 77.3%, and 84.5% on the three data sets, outperforming the baselines. ConclusionsOur experimental results indicate that ADE extraction from social media can benefit from feature selection. With respect to the effectiveness of different feature subsets, lexical features and semantic features can enhance the ADE extraction capability. Kernel-based approaches, which can stay away from the feature sparsity issue, are qualified to address the ADE extraction problem. Combining different individual classifiers using suitable combination methods can further enhance the ADE extraction effectiveness.},
journal = {Artif. Intell. Med.},
month = jun,
pages = {62–76},
numpages = {15}
}

@inproceedings{10.1145/3375959.3375967,
author = {Wolde, Behailu Getachew and Boltana, Abiot Sinamo},
title = {Combinatorial Testing Approach for Cloud Mobility Service},
year = {2020},
isbn = {9781450372633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375959.3375967},
doi = {10.1145/3375959.3375967},
abstract = {Currently, software product becomes an essential component in running many stakeholders' activities. For instance, the industries mostly use cloud services to execute their important business functionality. However, by a few input's parameter interacting, this functionality can be pended. Such constraint poses challenging to cover various features of failure especially in ensuring cloud application. One way is to devise a strategy to cover input parameters' characteristics based on Combinatorial testing approach. This technique includes all possible combinations of test inputs for detecting bugs on the System Under Test (SUT). The paper explains the Combinatorial covering arrays to generate relatively exhaustive testing by modeling features of sample services using Feature IDE plugin in Eclipse IDE. This way, we build the input domain model to represent coverage of the existing mobility service running on NEMo Mobility cloud platform. Using this model, covering arrays is applied to generate t-way test cases by leveraging IPOg algorithm, which is implemented in a CiTLab. As a test case management, the JUnit testing framework uses test stubs to validate the test methods of generated test cases on the specified service (SUT).},
booktitle = {Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing Conference},
pages = {6–13},
numpages = {8},
keywords = {CiTLAB, Cloud Mobility Service, Combinatorial Testing, Feature Model, Software Testing},
location = {Kobe, Japan},
series = {AICCC '19}
}

@inproceedings{10.1145/3490035.3490262,
author = {Bansal, Rahul and Biswas, Soma},
title = {CT-DANN: co-teaching meets DANN for wild unsupervised domain adaptation},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490262},
doi = {10.1145/3490035.3490262},
abstract = {Unsupervised domain adaptation aims at leveraging supervision from an annotated source domain for performing tasks like classification/segmentation on an unsupervised target domain. However, a large enough related dataset with clean annotations may not be always available in real scenarios, since annotations are usually obtained from crowd sourcing, and thus are noisy. Here, we consider a more realistic and challenging setting, wild unsupervised domain adaptation (WUDA), where the source domain annotations can be noisy. Standard domain adaptation approaches which directly use these noisy source labels and the unlabeled targets for the domain adaptation task perform poorly, due to severe negative transfer from the noisy source domain. In this work, we propose a novel end-to-end framework, termed CT-DANN (Co-teaching meets DANN), which seamlessly integrates a state-of-the-art approach for handling noisy labels (Co-teaching) with a standard domain adaptation framework (DANN). CT-DANN effectively utilizes all the source samples after accounting for both their noisy labels as well as transferability with respect to the target domain. Extensive experiments on three benchmark datasets with different types and levels of noise and comparison with state-of-the-art WUDA approach justify the effectiveness of the proposed framework.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {5},
numpages = {8},
keywords = {co-teaching, noisy source data, source data weighting, wild unsupervised domain adaptation},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inproceedings{10.1007/978-3-030-33246-4_45,
author = {Gonz\'{a}lez-Rojas, Oscar and Tafurth, Juan},
title = {Multi-cloud Services Configuration Based on Risk Optimization},
year = {2019},
isbn = {978-3-030-33245-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33246-4_45},
doi = {10.1007/978-3-030-33246-4_45},
abstract = {Nowadays risk analysis becomes critical in the Cloud Computing domain due to the increasing number of threats affecting applications running on cloud infrastructures. Multi-cloud environments allow connecting and migrating services from multiple cloud providers to manage risks. This paper addresses the question of how to model and configure multi-cloud services that can adapt to changes in user preferences and threats on individual and composite services. We propose an approach that combines Product Line (PL) and Machine Learning (ML) techniques to model and timely find optimal configurations of large adaptive systems such as multi-cloud services. A three-layer variability modeling on domain, user preferences, and adaptation constraints is proposed to configure multi-cloud solutions. ML regression algorithms are used to quantify the risk of resulting configurations by analyzing how a service was affected by incremental threats over time. An experimental evaluation on a real life electronic identification and trust multi-cloud service shows the applicability of the proposed approach to predict the risk for alternative re-configurations on autonomous and decentralized services that continuously change their availability and provision attributes.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&amp;TC 2019, Rhodes, Greece, October 21–25, 2019, Proceedings},
pages = {733–749},
numpages = {17},
keywords = {Multi-cloud services, Variability modeling, Product line configuration, Risk optimization, Machine learning},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3395035.3425182,
author = {Kaya, Heysem and Verkholyak, Oxana and Markitantov, Maxim and Karpov, Alexey},
title = {Combining Clustering and Functionals based Acoustic Feature Representations for Classification of Baby Sounds},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425182},
doi = {10.1145/3395035.3425182},
abstract = {This paper investigates different fusion strategies as well as provides insights on their effectiveness alongside standalone classifiers in the framework of paralinguistic analysis of infant vocalizations. The combinations of such systems as Support Vector Machines (SVM) and Extreme Learning Machines (ELM) based classifiers, as well as its weighted kernel version are explored, training systems on different acoustic feature representations and implementing weighted score-level fusion of the predictions. The proposed framework is tested on INTERSPEECH ComParE-2019 Baby Sounds corpus, which is a collection of Home Bank infant vocalization corpora annotated for five classes. Adhering to the challenge protocol, using a single test set submission we outperform the challenge baseline Unweighted Average Recall (UAR) score and achieve a comparable result to the state-of-the-art.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {509–513},
numpages = {5},
keywords = {baby sounds classification, computational paralinguistics, extreme learning machines, information fusion, support vector machines},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1007/978-3-030-92273-3_25,
author = {Zheng, Jinfang and Xie, Jinyang and Lyu, Chen and Lyu, Lei},
title = {SS-CCN: Scale Self-guided Crowd Counting Network},
year = {2021},
isbn = {978-3-030-92272-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92273-3_25},
doi = {10.1007/978-3-030-92273-3_25},
abstract = {With the emergence of deep learning, many CNN-based methods have achieved competitive performance in crowd counting, in which how to effectively solve the scale variation problem plays a key role. To tackle with the problem, we present an innovative scale self-guided crowd counting network (SS-CCN) by taking full advantage of scale information in a multi-level network. The proposed SS-CCN highlights crowd information by applying scale enhancement and scale-aware attention modules in multi-level features. Moreover, semantic attention module is applied on deep layers to extract semantic information. Besides, the fine-grained residual module is proposed to further refine the crowd information. Furthermore, we pioneer a scale pyramid loss with different loss functions applied to different scales. Integrating the proposed module, our method can effectively solve the scale variation problem. Extensive experimental results on several public datasets show that our proposed SS-CCN achieves satisfactory and superior performance compared to the state-of-the-art methods.},
booktitle = {Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part IV},
pages = {299–310},
numpages = {12},
keywords = {Crowd counting, Deep learning, Attention mechanism, Scale-aware, Scale pyramid loss},
location = {Sanur, Bali, Indonesia}
}

@article{10.1007/s00146-007-0116-3,
author = {Bundy, Alan},
title = {AI Bridges and Dreams},
year = {2007},
issue_date = {Jun 2007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {4},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-007-0116-3},
doi = {10.1007/s00146-007-0116-3},
abstract = {This paper is a modified version of my acceptance lecture for the 1986 SPL-Insight Award. It turned into something of a personal credo -describing my view of the nature of AIthe potential social benefit of applied AIthe importance of basic AI researchthe role of logic and the methodology of rational constructionthe interplay of applied and basic AI research, andthe importance of funding basic AI. These points are knitted together by an analogy between AI and structural engineering: in particular, between building expert systems and building bridges.},
journal = {AI Soc.},
month = jun,
pages = {659–668},
numpages = {10}
}

@inproceedings{10.5555/2145432.2145538,
author = {Sun, Weiwei and Xu, Jia},
title = {Enhancing Chinese word segmentation using unlabeled data},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-of-vocabulary (OOV) words which appear more than once inside a document. Novel features result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {970–979},
numpages = {10},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/3504035.3504312,
author = {Huang, Wenjun and Liang, Chao and Yu, Yi and Wang, Zheng and Ruan, Weijian and Hu, Ruimin},
title = {Video-based person re-identification via self paced weighting},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Person re-identification (re-id) is a fundamental technique to associate various person images, captured by different surveillance cameras, to the same person. Compared to the single image based person re-id methods, video-based person re-id has attracted widespread attentions because extra space-time information and more appearance cues that can be used to greatly improve the matching performance. However, most existing video-based person re-id methods equally treat all video frames, ignoring their quality discrepancy caused by object occlusion and motions, which is a common phenomenon in real surveillance scenario. Based on this finding, we propose a novel video-based person re-id method via self paced weighting (SPW). Firstly, we propose a self paced outlier detection method to evaluate the noise degree of video sub sequences. Thereafter, a weighted multi-pair distance metric learning approach is adopted to measure the distance of two person image sequences. Experimental results on two public datasets demonstrate the superiority of the proposed method over current state-of-the-art work.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {277},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.5555/3495724.3496750,
author = {Zhang, Dingwen and Tian, Haibin and Han, Jungong},
title = {Few-cost salient object detection with adversarial-paced learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1026},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.5555/3288992.3288997,
author = {Vinci, Giuseppe and Ventura, Val\'{e}rie and Smith, Matthew A. and Kass, Robert E.},
title = {Adjusted regularization of cortical covariance},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {45},
number = {2},
issn = {0929-5313},
abstract = {It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an L1$L_{1}$ penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4.},
journal = {J. Comput. Neurosci.},
month = oct,
pages = {83–101},
numpages = {19},
keywords = {Bayesian inference, False discovery rate, Functional connectivity, Gaussian graphical model, Graphical lasso, High-dimensional estimation, Macaque visual cortex, Penalized maximum likelihood estimation}
}

@inproceedings{10.1007/978-3-030-67832-6_30,
author = {Wang, Fei and Ding, Youdong and Liang, Huan and Wen, Jing},
title = {Discriminative and Selective Pseudo-Labeling for Domain Adaptation},
year = {2021},
isbn = {978-3-030-67831-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67832-6_30},
doi = {10.1007/978-3-030-67832-6_30},
abstract = {Unsupervised domain adaptation aims to transfer the knowledge of source domain to a related but not labeled target domain. Due to the lack of label information of target domain, most existing methods train a weak classifier and directly apply to pseudo-labeling which may downgrade adaptation performance. To address this problem, in this paper, we propose a novel discriminative and selective pseudo-labeling (DSPL) method for domain adaptation. Specifically, we first match the marginal distributions of two domains and increase inter-class distance simultaneously. Then a feature transformation method is proposed to learn a low-dimensional transfer subspace which is discriminative enough. Finally, after data has formed good clusters, we introduce a structured prediction based selective pseudo-labeling strategy which is able to sufficiently exploit target data structure. We conduct extensive experiments on three popular visual datasets, demonstrating the good domian adaptation performance of our method.},
booktitle = {MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part I},
pages = {365–377},
numpages = {13},
keywords = {Unsupervised domain adaptation, Pseudo-labeling, Discriminative learned subspace},
location = {Prague, Czech Republic}
}

@article{10.1016/j.neucom.2021.04.070,
author = {Yang, Zhao and Liu, Jiehao and Liu, Tie and Zhu, Yuanxin and Wang, Li and Tao, Dapeng},
title = {Equidistant distribution loss for person re-identification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {455},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.070},
doi = {10.1016/j.neucom.2021.04.070},
journal = {Neurocomput.},
month = sep,
pages = {255–264},
numpages = {10},
keywords = {Person re-identification, Equidistant distribution loss, Imbalance learning}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Classification, Software product family, Variability management, Variability modeling}
}

@article{10.1007/s11227-017-2011-0,
author = {Mih\u{a}\'{z}Escu, Marian Cristian and Popescu, Paul \'{z}Tefan and Popescu, Elvira},
title = {Data analysis on social media traces for detection of "spam" and "don't care" learners},
year = {2017},
issue_date = {October   2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {73},
number = {10},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-017-2011-0},
doi = {10.1007/s11227-017-2011-0},
abstract = {Classification methods are becoming more and more useful as part of the standard data analyst's toolbox in many application domains. The specific data and domain characteristics of social media tools used in online educational contexts present the challenging problem of training high-quality classifiers that bring important insight into activity patterns of learners. Currently, standard and also very successful model for classification tasks is represented by decision trees. In this paper, we introduce a custom-designed data analysis pipeline for predicting "spam" and "don't care" learners from eMUSE online educational environment. The trained classifiers rely on social media traces as independent variables and on final grade of the learner as dependent variables. Current analysis evaluates performed activities of learners and the similarity of two derived data models. Experiments performed on social media traces from five years and 285 learners show satisfactory classification results that may be further used in productive environment. Accurate identification of "spam" and "don't care" users may have further a great impact on producing better classification models for the rest of the "regular" learners.},
journal = {J. Supercomput.},
month = oct,
pages = {4302–4323},
numpages = {22},
keywords = {Classification, Online educational environment, Ranking, Social media tools, Spam learners}
}

@inproceedings{10.1109/ASE.2015.44,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Traon, Yves le},
title = {Automating the extraction of model-based software product lines from model variants},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.44},
doi = {10.1109/ASE.2015.44},
abstract = {We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVa2PL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVa2PL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an Inflight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants which did not exist are at least structurally valid).},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {396–406},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1145/1183236.1183239,
author = {Bichler, Martin and Kalagnanam, Jayant R.},
title = {Software frameworks for advanced procurement auction markets},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183239},
doi = {10.1145/1183236.1183239},
abstract = {A range of versatile auction formats are coming that allow more flexibility in specifying demand and supply.},
journal = {Commun. ACM},
month = dec,
pages = {104–108},
numpages = {5}
}

@article{10.1016/j.asoc.2016.05.020,
author = {Sachdeva, Jainy and Kumar, Vinod and Gupta, Indra and Khandelwal, Niranjan and Ahuja, Chirag Kamal},
title = {A package-SFERCB-"Segmentation, feature extraction, reduction and classification analysis by both SVM and ANN for brain tumors"},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.020},
doi = {10.1016/j.asoc.2016.05.020},
abstract = {An interactive computer aided dignostic (CAD) system for assisting inexperience young radiologists is developed. The difficulty in brain tumors classification is due to similar size, shape, location, hetrogeniety, presence of oedema, cystic and isointense regions has been the key feature of this research. Genetic Algorithm is employed as it is an easy concept and is well understood by radiologists without going in much depth of engineering.Display Omitted Brain tumors as segmented regions of interests (SROIs) by content based active contour model (CBAC).Feature extraction-intensity and texture based features.Feature reduction by Genetic Algorithm.Classification by Hybrid Models-GA-SVM and GA-ANN. The objective of this experimentation is to develop an interactive CAD system for assisting radiologists in multiclass brain tumor classification. The study is performed on a diversified dataset of 428 post contrast T1-weighted MR images of 55 patients and publically available dataset of 260 post contrast T1-weighted MR images of 10 patients. The first dataset includes primary brain tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), childhood tumor-Medulloblastoma (MED) and Meningioma (MEN), along with secondary tumor-Metastatic (MET). The second dataset consists of Astrocytoma (AS), Low Grade Glioma (LGL) and Meningioma (MEN). The tumor regions are marked by content based active contour (CBAC) model. The regions are than saved as segmented regions of interest (SROIs). 71 intensity and texture feature set is extracted from these SROIs. The features are specifically selected based on the pathological details of brain tumors provided by the radiologist. Genetic Algorithm (GA) selects the set of optimal features from this input set. Two hybrid machine learning models are implemented using GA with support vector machine (SVM) and artificial neural network (ANN) (GA-SVM and GA-ANN) and are tested on two different datasets. GA-SVM is proposed for finding preliminary probability in identifying tumor class and GA-ANN is used for confirmation of accuracy. Test results of the first dataset show that the GA optimization technique has enhanced the overall accuracy of SVM from 79.3% to 91.7% and of ANN from 75.6% to 94.9%. Individual class accuracies delivered by GA-SVM are: AS-89.8%, GBM-83.3%, MED-95.6%, MEN-91.8%, and MET-97.1%. Individual class accuracies delivered by GA-ANN classifier are: AS-96.6%, GBM-86.6%, MED-93.3%, MEN-96%, MET-100%. Similar results are obtained for the second dataset. The overall accuracy of SVM has increased from 80.8% to 89% and that of ANN has increased from 77.5% to 94.1%. Individual class accuracies delivered by GA-SVM are: AS-85.3%, LGL-88.8%, MEN-93%. Individual class accuracies delivered by GA-ANN classifier are: AS-92.6%, LGL-94.4%, MED-95.3%. It is observed from the experiments that GA-ANN classifier has provided better results than GA-SVM. Further, it is observed that along with providing finer results, GA-SVM provides advantage in speed whereas GA-ANN provides advantage in accuracy. The combined results from both the classifiers will benefit the radiologists in forming a better decision for classifying brain tumors.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {151–167},
numpages = {17},
keywords = {Brain tumors, GA-ANN, GA-SVM, Genetic Algorithm (GA)}
}

@article{10.1007/s10586-017-1108-9,
author = {Ilavarasi, A. K. and Sathiyabhama, B.},
title = {An evolutionary feature set decomposition based anonymization for classification workloads: Privacy Preserving Data Mining},
year = {2017},
issue_date = {Dec 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1108-9},
doi = {10.1007/s10586-017-1108-9},
abstract = {Privacy has become an important concern while publishing micro data about a population. The emerging area called privacy preserving data mining (PPDM) focus on individual privacy without compromising data mining results. An adversarial exploitation of published data poses a risk of information disclosure about individuals. On the other hand, imposing privacy constraints on the data results in substantial information loss and compromises the legitimate data analysis. Motivated by the increasing growth of PPDM algorithms, we first investigate the privacy implications and the crosscutting issues between privacy versus utility of data. We present a privacy model that embeds the anonymization procedure in to a learning algorithm and this has mitigated the additional overheads imposed on data mining tasks. Our primary concern about PPDM is that the utility of data should not be compromised by the transformation applied. Different data mining classification workloads are analyzed with the proposed anonymization procedure for any side effects incurred. It is shown empirically that classification accuracy obtained for most of the datasets outperforms the results obtained with original dataset.},
journal = {Cluster Computing},
month = dec,
pages = {3515–3525},
numpages = {11},
keywords = {Anonymization, Classification, Data mining, Decomposition, Evolutionary partitioning, Privacy}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@article{10.1007/s00766-003-0166-0,
author = {Thompson, Jeffrey M. and Heimdahl, Mats P.},
title = {Structuring product family requirements for n-dimensional and hierarchical product lines},
year = {2003},
issue_date = {February  2003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-003-0166-0},
doi = {10.1007/s00766-003-0166-0},
abstract = {The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to a safer system. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. In this paper, we draw on our experiences in applying the software product-line approach to a family of mobile robots, a family of flight guidance systems, and a family of cardiac pacemakers, as well as case studies done by others to (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains and (2) demonstrate our progress towards a solution using a set-theoretic approach to reason about domains of what we call n-dimensional and hierarchical product families.},
journal = {Requir. Eng.},
month = feb,
pages = {42–54},
numpages = {13},
keywords = {Domain Engineering, Product line engineering, Product line modelling, Requirements reuse, Requirements structuring}
}

@article{10.1016/j.specom.2017.04.002,
title = {Fourier model based features for analysis and classification of out-of-breath speech},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.04.002},
doi = {10.1016/j.specom.2017.04.002},
abstract = {A new stressed speech database, named out-of-breath speech (OBS) database, is created, which contains three classes, out-of-breath speech, low out-of-breath speech and normal speech.Four features are proposed using mutual information (MI) on the Fourier parameters for analysis and classification of different classes of OBS database.For multi-class classification, support vector machine (SVM) classifier is used with binary cascade approach.Recognition results show that the proposed features have higher potential to classify the out-of-breath speech, compared to the breathiness, MFCC and TEO-CB-Auto-Env features. This paper presents a new method of feature extraction using Fourier model for analysis of out-of-breath speech. The proposed feature is evaluated using mutual information (MI) on the difference and ratio values of the Fourier parameters, amplitude and frequency. The difference and ratio are calculated between two contiguous values of the Fourier parameters. To analyze the out-of-breath speech, a new stressed speech database, named out-of-breath speech (OBS) database, is created. The database contains three classes of speech, out-of-breath speech, low out-of-breath speech and normal speech. The effectiveness of the proposed features is evaluated with the statistical analysis. The proposed features not only differentiate the normal speech and the out-of-breath speech, but also can discriminate different breath emission levels of speech. Hidden Markov model (HMM) and support vector machine (SVM) are used to evaluate the performance of the proposed features using the OBS database. For multi-class classification problem, SVM classifier is used with binary cascade approach. The performance of the proposed features is compared with the breathiness feature, the mel frequency cepstral coefficient (MFCC) feature and the Teager energy operator (TEO) based critical band TEO autocorrelation envelope (TEO-CB-Auto-Env) feature. The proposed feature outperforms the breathiness feature, the MFCC feature and the TEO-CB-Auto-Env feature.},
journal = {Speech Commun.},
month = jun,
pages = {1–14},
numpages = {14}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-paced co-training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1007/978-3-030-98682-7_17,
author = {Hasselbring, Arne and Baude, Andreas},
title = {Soccer Field Boundary Detection Using Convolutional Neural Networks},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_17},
doi = {10.1007/978-3-030-98682-7_17},
abstract = {Detecting the field boundary is often one of the first steps in the vision pipeline of soccer robots. Conventional methods make use of a (possibly adaptive) green classifier, selection of boundary points and possibly model fitting. We present an approach to predict the coordinates of the field boundary column-wise in the image using a convolutional neural network. This is combined with a method to let the network predict the uncertainty of its output, which allows to fit a line model in which columns are weighted according to the network’s confidence. Experiments show that the resulting models are accurate enough in different lighting conditions as well as real-time capable. Code and data are available online (, ).},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {202–213},
numpages = {12},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {android feature model, defense capability, evolutionary algorithm, malware generation},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.1016/j.neucom.2019.08.002,
author = {Li, Zhenglai and Tang, Chang and Chen, Jiajia and Wan, Cheng and Yan, Weiqing and Liu, Xinwang},
title = {Diversity and consistency learning guided spectral embedding for multi-view clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.002},
doi = {10.1016/j.neucom.2019.08.002},
journal = {Neurocomput.},
month = dec,
pages = {128–139},
numpages = {12},
keywords = {Multi-view clustering, Spectral embedding, Diversity and consistency learning, 00-01, 99-00}
}

@article{10.1016/j.jpdc.2019.08.008,
author = {Mart\'{\i}nez, Daniel and Brewer, Wesley and Strelzoff, Andrew and Wilson, Andrew and Wade, Daniel},
title = {Rotorcraft virtual sensors via deep regression},
year = {2020},
issue_date = {Jan 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.08.008},
doi = {10.1016/j.jpdc.2019.08.008},
journal = {J. Parallel Distrib. Comput.},
month = jan,
pages = {114–126},
numpages = {13},
keywords = {Virtual Sensors, Deep Learning, High Performance Computing, Deep Neural Networks, Evolutionary Optimization}
}

@inproceedings{10.1007/978-3-030-98682-7_13,
author = {Antonioni, Emanuele and Suriani, Vincenzo and Solimando, Filippo and Nardi, Daniele and Bloisi, Domenico D.},
title = {Learning from the Crowd: Improving the Decision Making Process in Robot Soccer Using the Audience Noise},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_13},
doi = {10.1007/978-3-030-98682-7_13},
abstract = {Fan input and support is an important component in many individual and team sports, ranging from athletics to basketball. Audience interaction provides a consistent impact on the athletes’ performance. The analysis of the crowd noise can provide a global indication on the ongoing game situation, less conditioned by subjective factors that can influence a single fan. In this work, we exploit the collective intelligence of the audience of a robot soccer match to improve the performance of the robot players. In particular, audio features extracted from the crowd noiseare used in a Reinforcement Learning process to possibly modify the game strategy. The effectiveness of the proposed approach is demonstrated by experiments on registered crowd noise samples from several past RoboCup SPL matches.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {153–164},
numpages = {12},
keywords = {Crowd noise interpretation, RoboCup SPL, Sound recognition},
location = {Sydney, NSW, Australia}
}

@article{10.1007/s10618-016-0475-9,
author = {Garcia, Lu\'{\i}s P. and Lorena, Ana C. and Matwin, Stan and Carvalho, Andr\'{e} C.},
title = {Ensembles of label noise filters: a ranking approach},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-016-0475-9},
doi = {10.1007/s10618-016-0475-9},
abstract = {Label noise can be a major problem in classification tasks, since most machine learning algorithms rely on data labels in their inductive process. Thereupon, various techniques for label noise identification have been investigated in the literature. The bias of each technique defines how suitable it is for each dataset. Besides, while some techniques identify a large number of examples as noisy and have a high false positive rate, others are very restrictive and therefore not able to identify all noisy examples. This paper investigates how label noise detection can be improved by using an ensemble of noise filtering techniques. These filters, individual and ensembles, are experimentally compared. Another concern in this paper is the computational cost of ensembles, once, for a particular dataset, an individual technique can have the same predictive performance as an ensemble. In this case the individual technique should be preferred. To deal with this situation, this study also proposes the use of meta-learning to recommend, for a new dataset, the best filter. An extensive experimental evaluation of the use of individual filters, ensemble filters and meta-learning was performed using public datasets with imputed label noise. The results show that ensembles of noise filters can improve noise filtering performance and that a recommendation system based on meta-learning can successfully recommend the best filtering technique for new datasets. A case study using a real dataset from the ecological niche modeling domain is also presented and evaluated, with the results validated by an expert.},
journal = {Data Min. Knowl. Discov.},
month = sep,
pages = {1192–1216},
numpages = {25},
keywords = {Ensemble filters, Label noise, Noise filters, Noise ranking, Recommendation system}
}

@article{10.1007/s11042-019-7251-y,
author = {Mei, Jianhan and Wu, Ziming and Chen, Xiang and Qiao, Yu and Ding, Henghui and Jiang, Xudong},
title = {DeepDeblur: text image recovery from blur to sharp},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7251-y},
doi = {10.1007/s11042-019-7251-y},
abstract = {Digital images could be degraded by a variety of blur during the image acquisition (i.e. relative motion of cameras, electronic noise, capturing defocus, and so on). Blurring images can be computationally modeled as the result of a convolution process with the corresponding blur kernel and thus, image deblurring can be regarded as a deconvolution operation. In this paper, we explore to deblur images by approximating blind deconvolutions using a deep neural network. Different deep neural network structures are investigated to evaluate their deblurring capabilities, which contributes to the optimal design of a network architecture. It is found that shallow and narrow networks are not capable of handling complex motion blur. We thus, present a deep network with 20 layers to cope with text image blur. In addition, a novel network structure with Sequential Highway Connections (SHC) is leveraged to gain superior convergence. The experiment results demonstrate the state-of-the-art performance of the proposed framework with the higher visual quality of the delurred images.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {18869–18885},
numpages = {17},
keywords = {Blind deconvolution, Convolutional Neural Network (CNN), Short connection, Text Deblurring}
}

@article{10.1145/1183236.1183242,
author = {Crawford, Diane},
title = {Editorial pointers},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183242},
doi = {10.1145/1183236.1183242},
journal = {Commun. ACM},
month = dec,
pages = {5},
numpages = {2}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {auto-tuning, feature model, genetic algorithm, java virtual machine, optimization},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Model checking, Product-line analysis, Reliability analysis, Software product lines, Verification}
}

@article{10.1016/j.specom.2012.01.002,
author = {Zelinka, Petr and Sigmund, Milan and Schimmel, Jiri},
title = {Impact of vocal effort variability on automatic speech recognition},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {6},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2012.01.002},
doi = {10.1016/j.specom.2012.01.002},
abstract = {The impact of changes in a speaker's vocal effort on the performance of automatic speech recognition has largely been overlooked by researchers and virtually no speech resources exist for the development and testing of speech recognizers at all vocal effort levels. This study deals with speech properties in the whole range of vocal modes - whispering, soft speech, normal speech, loud speech, and shouting. Fundamental acoustic and phonetic changes are documented. The impact of vocal effort variability on the performance of an isolated-word recognizer is shown and effective means of improving the system's robustness are tested. The proposed multiple model framework approach reaches a 50% relative reduction of word error rate compared to the baseline system. A new specialized speech database, BUT-VE1, is presented, which contains speech recordings of 13 speakers at 5 vocal effort levels with manual phonetic segmentation and sound pressure level calibration.},
journal = {Speech Commun.},
month = jul,
pages = {732–742},
numpages = {11},
keywords = {Machine learning, Robust speech recognition, Vocal effort level}
}

@inproceedings{10.1007/978-3-030-58539-6_16,
author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
title = {Improving Vision-and-Language Navigation with Image-Text Pairs from the Web},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_16},
doi = {10.1007/978-3-030-58539-6_16},
abstract = {Following a navigation instruction such as ‘Walk down the stairs and stop at the brown sofa’ requires embodied AI agents to ground referenced scene elements referenced (e.g. ‘stairs’) to visual content in the environment (pixels corresponding to ‘stairs’). We ask the following question – can we leverage abundant ‘disembodied’ web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘...stop at the brown sofa’) and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN – outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful – with their combination resulting in further gains.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {259–274},
numpages = {16},
keywords = {Vision-and-language navigation, Transfer learning, Embodied AI},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {LATEX, constraint programming, generators, machine learning, technical writing, variability modelling},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.patcog.2011.09.011,
author = {Rasmussen, Peter M. and Hansen, Lars K. and Madsen, Kristoffer H. and Churchill, Nathan W. and Strother, Stephen C.},
title = {Model sparsity and brain pattern interpretation of classification models in neuroimaging},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.09.011},
doi = {10.1016/j.patcog.2011.09.011},
abstract = {Interest is increasing in applying discriminative multivariate analysis techniques to the analysis of functional neuroimaging data. Model interpretation is of great importance in the neuroimaging context, and is conventionally based on a 'brain map' derived from the classification model. In this study we focus on the relative influence of model regularization parameter choices on both the model generalization, the reliability of the spatial patterns extracted from the classification model, and the ability of the resulting model to identify relevant brain networks defining the underlying neural encoding of the experiment. For a support vector machine, logistic regression and Fisher's discriminant analysis we demonstrate that selection of model regularization parameters has a strong but consistent impact on the generalizability and both the reproducibility and interpretable sparsity of the models for both @?"2 and @?"1 regularization. Importantly, we illustrate a trade-off between model spatial reproducibility and prediction accuracy. We show that known parts of brain networks can be overlooked in pursuing maximization of classification accuracy alone with either @?"2 and/or @?"1 regularization. This supports the view that the quality of spatial patterns extracted from models cannot be assessed purely by focusing on prediction accuracy. Our results instead suggest that model regularization parameters must be carefully selected, so that the model and its visualization enhance our ability to interpret the brain.},
journal = {Pattern Recogn.},
month = jun,
pages = {2085–2100},
numpages = {16},
keywords = {Classification, Kernel methods, Machine learning, Model interpretation, NPAIRS resampling, Neuroimaging, Pattern analysis, Regularization, Sparsity}
}

@inproceedings{10.1145/1960275.1960283,
author = {Schaefer, Ina and Bettini, Lorenzo and Damiani, Ferruccio},
title = {Compositional type-checking for delta-oriented programming},
year = {2011},
isbn = {9781450306058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960275.1960283},
doi = {10.1145/1960275.1960283},
abstract = {Delta-oriented programming is a compositional approach to flexibly implementing software product lines. A product line is represented by a code base and a product line declaration. The code base consists of a set of delta modules specifying modifications to object-oriented programs. The product line declaration provides the connection of the delta modules with the product features. This separation increases the reusability of delta modules. In this paper, we provide a foundation for compositional type checking of delta-oriented product lines of Java programs by presenting a minimal core calculus for delta-oriented programming. The calculus is equipped with a constraint-based type system that allows analyzing each delta module in isolation, such that that also the results of the analysis can be reused. By combining the analysis results for the delta modules with the product line declaration it is possible to establish that all the products of the product line are well-typed according to the Java type system.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development},
pages = {43–56},
numpages = {14},
keywords = {java, software product line, type system},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@inproceedings{10.1145/3451421.3451427,
author = {Liu, Xiaoli and Li, Jiali and Cao, Peng},
title = {SP-MTFL: A self paced multi-task feature learning method for cognitive performance predicting of Alzheimer's disease},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451427},
doi = {10.1145/3451421.3451427},
abstract = {Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Predicting cognitive performance of subjects from neuroimage measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimer's disease. Multi-task based feature learning (MTFL) have been widely studied to select a discriminative feature subset from MRI features, and improve the performance by incorporating inherent correlations among multiple clinical cognitive measures. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, we propose a self-paced multi-task feature learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task in this study. Experimental results on ADNI are provided, and the comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {23–27},
numpages = {5},
keywords = {Alzheimer's disease, Machine learning, Self-paced learning, multi-task learning, regression},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@article{10.1007/s10772-017-9429-x,
author = {Phu, Vo Ngoc and Tran, Vo Thi and Chau, Vo Thi and Dat, Nguyen Duy and Duy, Khanh Ly},
title = {A decision tree using ID3 algorithm for English semantic analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9429-x},
doi = {10.1007/s10772-017-9429-x},
abstract = {Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {593–613},
numpages = {21},
keywords = {Decision tree, English document opinion mining, English sentiment classification, ID3 algorithm, Sentiment classification, id3}
}

@article{10.1145/3345314,
author = {Wang, Qingyong and Zhou, Yun and Ding, Weiping and Zhang, Zhiguo and Muhammad, Khan and Cao, Zehong},
title = {Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3345314},
doi = {10.1145/3345314},
abstract = {Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {34},
numpages = {12},
keywords = {Lung cancer, bootstrap, classification, random forest, self-paced learning}
}

@article{10.1504/ijguc.2021.119573,
author = {Ji, Hongbo and Wang, Mingyue and Sun, Mingwei and Liu, Qiang},
title = {Neural network classifier based on genetic algorithm image segmentation of subject robot optimisation system},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {4},
issn = {1741-847X},
url = {https://doi.org/10.1504/ijguc.2021.119573},
doi = {10.1504/ijguc.2021.119573},
abstract = {Robot optimisation system is a kind of complex, nonlinear, strong coupling system with serious uncertainty. The effect of image segmentation has become an important index to judge the merits of many algorithms. The purpose of this study is to explore the effect of neural network based on genetic algorithm on image segmentation in the optimisation system of classifier subject robot. The method used in this study is to calculate the pre trained VGGl6 NET model as the pre training model through the framework of genetic algorithm. The resolution of the training picture used is 640 * 480, the learning rate is 10−5, the value of batch size is l, the number of iterations is set to 12,000 and then the trained model is used to detect the image. The results show that the average error of group B of SNN trained by BP algorithm is 11.62%, the SNN trained by SGA has reduced the result to 9.75% and the error reduced to 7.75% by the genetic algorithm in this study. Moreover, genetic algorithm is better in feature point extraction, and the detection rate reaches 94.62%, which is higher than 77.53% and 88.74% of other methods. The missing rate of this study is only 3.04%, far lower than 12.49% and 7.36%. The conclusion is that our genetic algorithm has obvious advantages, small error, high efficiency and applicability. The neural network based on genetic algorithm in this study has a certain value in image segmentation technology.},
journal = {Int. J. Grid Util. Comput.},
month = jan,
pages = {369–379},
numpages = {10},
keywords = {genetic algorithm, neural network classifier, robot optimisation system, image segmentation, feature point extraction}
}

@article{10.1007/s00034-021-01674-0,
author = {Naiemi, Fatemeh and Ghods, Vahid and Khalesi, Hassan},
title = {MOSTL: An Accurate Multi-Oriented Scene Text Localization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {9},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01674-0},
doi = {10.1007/s00034-021-01674-0},
abstract = {Automatic text localization in natural environments is the main element of many applications including self-driving cars, identifying vehicles, and providing scene information to visually impaired people. However, text in the natural and irregular scene has different degrees in orientations, shapes, and colors that make it difficult to detect. In this paper, an accurate multi-oriented scene text localization (MOSTL) is presented to obtain high efficiency of detecting text-based on convolutional neural networks. In the proposed method, an improved ReLU layer (i.ReLU) and an improved inception layer (i.inception) were introduced. Firstly, the proposed structure is used to extract low-level visual features. Then, an extra layer has been used to improve the feature extraction. The i.ReLU and i.inception layers have improved valuable information in text detection. The i.ReLU layers cause to extract some low-level features appropriately. The i.inception layers (specially 3 \texttimes{} 3 convolutions) can obtain broadly varying-sized text more effectively than a linear chain of convolution layer (without inception layers). The output of i.ReLU layers and i.inception layers was fed to an extra layer, which enables MOSTL to detect multi-oriented even curved and vertical texts. We conducted text detection experiments on well-known databases including ICDAR 2019, ICDAR 2017, ICDAR 2015, ICDAR 2003, and MSRA-TD500. MOSTL results yielded performance improvement remarkably.},
journal = {Circuits Syst. Signal Process.},
month = sep,
pages = {4452–4473},
numpages = {22},
keywords = {Scene text localization, Object detection, Multi-oriented, Convolutional neural network, Improved inception layer, Improved ReLU layer, Curved text}
}

@article{10.1016/j.jss.2008.08.026,
author = {Lago, Patricia and Muccini, Henry and van Vliet, Hans},
title = {A scoped approach to traceability management},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.08.026},
doi = {10.1016/j.jss.2008.08.026},
abstract = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way.},
journal = {J. Syst. Softw.},
month = jan,
pages = {168–182},
numpages = {15},
keywords = {Software process management, Software product line, Traceability issues, Traceability paths}
}

@inproceedings{10.1007/978-3-030-32047-8_26,
author = {Khoshmanesh, Seyedehzahra and Lutz, Robyn R.},
title = {Leveraging Feature Similarity for Earlier Detection of Unwanted Feature Interactions in Evolving Software Product Lines},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_26},
doi = {10.1007/978-3-030-32047-8_26},
abstract = {Software product lines enable reuse of shared software across a family of products. As new products are built in the product line, new features are added. The features are units of functionality that provide services to users. Unwanted feature interactions, wherein one feature interferes with another feature’s operation, is a significant problem, especially as large software product lines evolve. Detecting feature interactions is a time-consuming and difficult task for developers. Moreover, feature interactions are often only discovered during testing, at which point costly re-work is needed. This paper proposes a similarity-based method to identify unwanted feature interactions much earlier in the development process. It uses knowledge of prior feature interactions stored with the software product line’s feature model to help find unwanted interactions between a new feature and existing features. The paper describes the framework and algorithms used to detect the feature interactions using three path similarity measures and evaluates the approach on a real-world, evolving software product line. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {293–307},
numpages = {15},
keywords = {Feature interaction, Similarity measures, Software product lines},
location = {Newark, NJ, USA}
}

@article{10.1155/2015/196098,
author = {Yang, Jinfeng and Xiao, Yong and Wang, Jiabing and Ma, Qianli and Shen, Yanhua},
title = {A fast clustering algorithm for data with a few labeled instances},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/196098},
doi = {10.1155/2015/196098},
abstract = {The diameter of a cluster is the maximum intracluster distance between pairs of instances within the same cluster, and the split of a cluster is the minimum distance between instances within the cluster and instances outside the cluster. Given a few labeled instances, this paper includes two aspects. First, we present a simple and fast clustering algorithm with the following property: if the ratio of the minimum split to the maximum diameter (RSD) of the optimal solution is greater than one, the algorithm returns optimal solutions for three clustering criteria. Second, we study the metric learning problem: learn a distance metric to make the RSD as large as possible. Compared with existing metric learning algorithms, one of our metric learning algorithms is computationally efficient: it is a linear programming model rather than a semidefinite programming model used by most of existing algorithms. We demonstrate empirically that the supervision and the learned metric can improve the clustering quality.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {21},
numpages = {1}
}

@article{10.1016/j.ins.2019.02.051,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {         Munec: a mutual neighbor-based clustering algorithm},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {486},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.051},
doi = {10.1016/j.ins.2019.02.051},
journal = {Inf. Sci.},
month = jun,
pages = {148–170},
numpages = {23},
keywords = {Mutual neighbors, Single link, Density, Distance, Clustering}
}

@inproceedings{10.5555/3172077.3172256,
author = {Ren, Yazhou and Zhao, Peng and Sheng, Yongpan and Yao, Dezhong and Xu, Zenglin},
title = {Robust softmax regression for multi-class classification with self-paced learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Softmax regression, a generalization of Logistic regression (LR) in the setting of multi-class classification, has been widely used in many machine learning applications. However, the performance of softmax regression is extremely sensitive to the presence of noisy data and outliers. To address this issue, we propose a model of robust softmax regression (RoSR) originated from the self-paced learning (SPL) paradigm for multi-class classification. Concretely, RoSR equipped with the soft weighting scheme is able to evaluate the importance of each data instance. Then, data instances participate in the classification problem according to their weights. In this way, the influence of noisy data and outliers (which are typically with small weights) can be significantly reduced. However, standard SPL may suffer from the imbalanced class influence problem, where some classes may have little influence in the training process if their instances are not sensitive to the loss. To alleviate this problem, we design two novel soft weighting schemes that assign weights and select instances locally for each class. Experimental results demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2641–2647},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1007/s00500-015-2004-y,
author = {Qin, Jindong and Liu, Xinwang and Pedrycz, Witold},
title = {A multiple attribute interval type-2 fuzzy group decision making and its application to supplier selection with extended LINMAP method},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {12},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-2004-y},
doi = {10.1007/s00500-015-2004-y},
abstract = {Supplier selection is a key issue in supply chain management, which directly impacts the manufacturer's performance. The problem can be viewed as a multiple attribute group decision making (MAGDM) that concerns many conflicting evaluation attributes, both being of qualitative and quantitative nature. Due to the increasing complexity and uncertainty of socio-economic environment, some evaluations of attributes are not adequately represented by numerical assessments and type-1 fuzzy sets. In this paper, we develop some linear programming models with the aid of multidimensional analysis of preference (LINMAP) method to solve interval type-2 fuzzy MAGDM problems, in which the information about attribute weights is incompletely known, and all pairwise comparison judgments over alternatives are represented by IT2FSs. First, we introduce a new distance measure based on the centroid interval between the IT2FSs. Then, we construct the linear programming model to determine the interval type-2 fuzzy positive ideal solution (IT2PIS) and corresponding attributes weight vector. Based on it, an extended LINMAP method to solve MAGDM problem under IT2FSs environment is developed. Finally, a supplier selection example is provided to demonstrate the usefulness of the proposed method.},
journal = {Soft Comput.},
month = jun,
pages = {3207–3226},
numpages = {20},
keywords = {Interval type-2 fuzzy sets (IT2FSs), Linear programming techniques for multidimensional analysis of preference (LINMAP) method, Multiple attribute group decision making (MAGDM), Supplier selection}
}

@inproceedings{10.1007/978-3-030-87199-4_50,
author = {Sedlar, Sara and Alimi, Abib and Papadopoulo, Th\'{e}odore and Deriche, Rachid and Deslauriers-Gauthier, Samuel},
title = {A Spherical Convolutional Neural Network for White Matter Structure Imaging via dMRI},
year = {2021},
isbn = {978-3-030-87198-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87199-4_50},
doi = {10.1007/978-3-030-87199-4_50},
abstract = {Diffusion Magnetic Resonance Imaging (dMRI) is a powerful non-invasive and in-vivo imaging modality for probing brain white matter structure. Convolutional neural networks (CNNs) have been shown to be a powerful tool for many computer vision problems where the signals are acquired on a regular grid and where translational invariance is important. However, as we are considering dMRI signals that are acquired on a sphere, rotational invariance, rather than translational, is desired. In this work, we propose a spherical CNN model with fully spectral domain convolutional and non-linear layers. It provides rotational invariance and is adapted to the real nature of dMRI signals and uniform random distribution of sampling points. The proposed model is positively evaluated on the problem of estimation of neurite orientation dispersion and density imaging (NODDI) parameters on the data from Human Connectome Project (HCP).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III},
pages = {529–539},
numpages = {11},
keywords = {Spherical CNN, Diffusion MRI, White matter micro-structures},
location = {Strasbourg, France}
}

@article{10.1016/j.patrec.2021.08.011,
author = {Mehta, Nancy and Murala, Subrahmanyam},
title = {MSAR-Net: Multi-scale attention based light-weight image super-resolution},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.08.011},
doi = {10.1016/j.patrec.2021.08.011},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {215–221},
numpages = {7},
keywords = {Multi-scale attention residual block, Up and down-sampling projection block, Image super-resolution, 41A05, 41A10, 65D05, 65D17}
}

@inproceedings{10.1007/978-3-030-69532-3_29,
author = {Priisalu, Maria and Paduraru, Ciprian and Pirinen, Aleksis and Sminchisescu, Cristian},
title = {Semantic Synthesis of Pedestrian Locomotion},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_29},
doi = {10.1007/978-3-030-69532-3_29},
abstract = {We present a model for generating 3d articulated pedestrian locomotion in urban scenarios, with synthesis capabilities informed by the 3d scene semantics and geometry. We reformulate pedestrian trajectory forecasting as a structured reinforcement learning (RL) problem. This allows us to naturally combine prior knowledge on collision avoidance, 3d human motion capture and the motion of pedestrians as observed e.g. in Cityscapes, Waymo or simulation environments like Carla. Our proposed RL-based model allows pedestrians to accelerate and slow down to avoid imminent danger (e.g. cars), while obeying human dynamics learnt from in-lab motion capture datasets. Specifically, we propose a hierarchical model consisting of a semantic trajectory policy network that provides a distribution over possible movements, and a human locomotion network that generates 3d human poses in each step. The RL-formulation allows the model to learn even from states that are seldom exhibited in the dataset, utilizing all of the available prior and scene information. Extensive evaluations using both real and simulated data illustrate that the proposed model is on par with recent models such as S-GAN, ST-GAT and S-STGCNN in pedestrian forecasting, while outperforming these in collision avoidance. We also show that our model can be used to plan goal reaching trajectories in urban scenes with dynamic actors.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {470–487},
numpages = {18},
location = {Kyoto, Japan}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {metric threshold, expectation maximization, empirical study}
}

@inproceedings{10.1007/978-3-030-89370-5_19,
author = {Luo, Chao and Bi, Sheng and Dong, Min and Nie, Hongxu},
title = {RGB-D Based Visual Navigation Using Direction Estimation Module},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_19},
doi = {10.1007/978-3-030-89370-5_19},
abstract = {Target-driven visual navigation without mapping works to solve navigation problems that given a target object, mobile robots can navigate to the target object. Recently, visual navigation has been researched and improved largely by learning-based methods. However, their methods lack depth information and spatial perception, using only single RGB images. To overcome these problems, two methods are presented in this paper. Firstly, we encode visual features of objects by dynamic graph convolutional network and extract 3D spatial features for objects by 3D geometry, a high level visual feature for agent to easily understand object relationship. Secondly, as human beings, they solve this problem in two steps, first exploring a new environment to find the target object and second planning a path to arrive. Inspired by the way of humans navigation, we propose direction estimation module (DEM) based on RGB-D images. DEM provides direction estimation of the target object to our learning model by a wheel odometry. Given a target object, first stage, our agent explores an unseen scene to detect the target object. Second stage, when detected the target object, we can estimate current location of the target object by 3D geometry, after that, each step of the agent, DEM will estimate new location of target object, and give direction information of the target object from a first-view image. It can guide our agent to navigate to the target object. Our experiment results outperforms the result of state of the art method in the artificial environment AI2-Thor.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {252–264},
numpages = {13},
keywords = {Visual navigation, Mobile robot, Direction estimation module, Reinforcement learning},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1007/978-3-030-58571-6_2,
author = {Du, Heming and Yu, Xin and Zheng, Liang},
title = {Learning Object Relation Graph and Tentative Policy for Visual Navigation},
year = {2020},
isbn = {978-3-030-58570-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58571-6_2},
doi = {10.1007/978-3-030-58571-6_2},
abstract = {Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII},
pages = {19–34},
numpages = {16},
keywords = {Graph, Imitation learning, Tentative policy learning, Visual navigation},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.eswa.2019.03.031,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {A hierarchical clustering algorithm and an improvement of the single linkage criterion to deal with noise},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.031},
doi = {10.1016/j.eswa.2019.03.031},
journal = {Expert Syst. Appl.},
month = aug,
pages = {96–108},
numpages = {13},
keywords = {Agglomerative, Dissimilarity, Density}
}

@inproceedings{10.1007/978-3-662-43652-3_5,
author = {Milicevic, Aleksandar and Efrati, Ido and Jackson, Daniel},
title = {αRby--An Embedding of Alloy in Ruby},
year = {2014},
isbn = {9783662436516},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-43652-3_5},
doi = {10.1007/978-3-662-43652-3_5},
abstract = {We present αRby--an embedding of the Alloy language in Ruby--and demonstrate the benefits of having a declarative modeling language backed by an automated solver embedded in a traditional object-oriented imperative programming language. This approach aims to bring these two distinct paradigms imperative and declarative together in a novel way. We argue that having the other paradigm available within the same language is beneficial to both the modeling community of Alloy users and the object-oriented community of Ruby programmers. In this paper, we primarily focus on the benefits for the Alloy community, namely, how αRby provides elegant solutions to several well-known, outstanding problems: 1 mixed execution, 2 specifying partial instances, 3 staged model finding.},
booktitle = {Proceedings of the 4th International Conference on Abstract State Machines, Alloy, B, TLA, VDM, and Z - Volume 8477},
pages = {56–71},
numpages = {16},
location = {Toulouse, France},
series = {ABZ 2014}
}

@article{10.5555/2051237.2051253,
author = {Gra\c{c}a, Jo\~{a}o V. and Ganchev, Kuzman and Coheur, Lu\'{\i}sa and Pereira, Fernando and Taskar, Ben},
title = {Controlling complexity in part-of-speech induction},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via parametric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {527–551},
numpages = {25}
}

@inproceedings{10.1145/3377812.3381399,
author = {Abbas, Muhammad},
title = {Variability aware requirements reuse analysis},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381399},
doi = {10.1145/3377812.3381399},
abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {190–193},
numpages = {4},
keywords = {product line, requirements, similarities, software reuse, variability},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1007/978-3-030-69532-3_4,
author = {Huang, Bowen and Zhou, Jinjia and Yan, Xiao and Jing, Ming’e and Wan, Rentao and Fan, Yibo},
title = {CS-MCNet: A Video Compressive Sensing Reconstruction Network with Interpretable Motion Compensation},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_4},
doi = {10.1007/978-3-030-69532-3_4},
abstract = {In this paper, a deep neural network with interpretable motion compensation called CS-MCNet is proposed to realize high-quality and real-time decoding of video compressive sensing. Firstly, explicit multi-hypothesis motion compensation is applied in our network to extract correlation information of adjacent frames (as shown in Fig.&nbsp;1), which improves the recover performance. And then, a residual module further narrows down the gap between reconstruction result and original signal. The overall architecture is interpretable by using algorithm unrolling, which brings the benefits of being able to transfer prior knowledge about the conventional algorithms. As a result, a PSNR of 22&nbsp;dB can be achieved at 64x compression ratio, which is about 4% to 9% better than state-of-the-art methods. In addition, due to the feed-forward architecture, the reconstruction can be processed by our network in real time and up&nbsp;to three orders of magnitude faster than traditional iterative methods.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {54–67},
numpages = {14},
location = {Kyoto, Japan}
}

@article{10.1504/IJBRA.2018.092685,
title = {Subspace module extraction from MI-based co-expression network},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {3},
issn = {1744-5485},
url = {https://doi.org/10.1504/IJBRA.2018.092685},
doi = {10.1504/IJBRA.2018.092685},
abstract = {Most of the existing methods in literature have used proximity measures in the construction of co-expression networks CEN consisting of functional gene modules. This work describes the construction of co-expression network using mutual information MI as a proximity measure with non-linear correlation. The network modules are extracted that are defined over a subset of samples. This method has been tested on several publicly available datasets and the subspace network modules obtained have been validated in terms of both internal and external measures.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {207–234},
numpages = {28}
}

@article{10.1145/1183236.1183254,
author = {Blank, Douglas},
title = {Robots make computer science personal},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183254},
doi = {10.1145/1183236.1183254},
abstract = {They also make it more hands-on, real, practical, and immediate, inspiring a new generation of scientists' deep interest in the field.},
journal = {Commun. ACM},
month = dec,
pages = {25–27},
numpages = {3}
}

@inproceedings{10.1145/3275219.3275234,
author = {Liu, Wenbin and Chen, Ningjiang and Li, Hua and Tang, Yusi and Liang, Birui},
title = {A Fair Scheduling Algorithm for Adaptive Heterogeneous Resources in Data Centers},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275234},
doi = {10.1145/3275219.3275234},
abstract = {The resource scheduling problem of data center clusters has always been a hot topic in the field of cloud computing. Existing research efforts focus on fairness, resource utilization and energy efficiency, and lack of research on heterogeneous clustering issues. To solve the problem that the traditional DRF algorithm does not consider the classification of machine performance and task type, this paper proposes a fair scheduling algorithm X-DRF that adapts to heterogeneous resources in the data center. The algorithm mainly classifies the performance of physical machines, increases the machine performance scoring factor, and increases the training and job type judgment classification of the XGBoost model. The experiments show that CPU utilization and memory usage increased by 10% and 6%, respectively. The normalized ratio is increased by about 3% compared to the original DRF system. Therefore, the presented fair scheduling algorithm for heterogeneous resources is more fair and reasonable in terms of resource allocation.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {15},
numpages = {6},
keywords = {Data center, Fairness, Heterogeneous cluster, Machine learning, Mesos, Resource scheduling},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.5555/3495724.3496497,
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {Self-paced deep reinforcement learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {773},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Feature-oriented software development, Measurement and optimization, Non-functional properties, SPL Conqueror, Software product lines}
}

@article{10.1007/s11280-018-0622-x,
author = {Wen, Guoqiu and Zhu, Yonghua and Cai, Zhiguo and Zheng, Wei},
title = {Self-tuning clustering for high-dimensional data},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0622-x},
doi = {10.1007/s11280-018-0622-x},
abstract = {Spectral clustering is an important component of clustering method, via tightly relying on the affinity matrix. However, conventional spectral clustering methods 1). equally treat each data point, so that easily affected by the outliers; 2). are sensitive to the initialization; 3). need to specify the number of cluster. To conquer these problems, we have proposed a novel spectral clustering algorithm, via employing an affinity matrix learning to learn an intrinsic affinity matrix, using the local PCA to resolve the intersections; and further taking advantage of a robust clustering that is insensitive to initialization to automatically generate clusters without an input of number of cluster. Experimental results on both artificial and real high-dimensional datasets have exhibited our proposed method outperforms the clustering methods under comparison in term of four clustering metrics.},
journal = {World Wide Web},
month = nov,
pages = {1563–1573},
numpages = {11},
keywords = {High-dimensional data, Local PCA, Multi-manifold clustering, Spectral clustering}
}

@inproceedings{10.23919/ICCAS50221.2020.9268247,
author = {Yoo, Hwiyeon and Kim, Nuri and Park, Jeongho and Oh, Songhwai},
title = {Path-Following Navigation Network Using Sparse Visual Memory},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.23919/ICCAS50221.2020.9268247},
doi = {10.23919/ICCAS50221.2020.9268247},
abstract = {Following a demonstration path without observing exact location of an agent is a challenging navigation problem. Especially, considering the probabilistic transition function of the agent makes the problem hard to solve with an exact action decision, so learning-based approaches have been used to solve this task. For example, a previous method by Kumar and Gupta et al., robust path following network (RPF), is a neural-network-based method using visual memories of the demonstration. Although the RPF shows good performances on the path-following task, it does not consider the efficiency of the visual memory since it requires the entire visual memory of the demonstration. In this paper, we propose a path-following network using sparse memory of the demonstration path that can deal with various sparsity of the visual memory. For each time step, the proposed network makes soft attention on the sparse memory to control the agent. We test the proposed model on the Habitat simulator using MatterPort3D dataset with various sparsity of memory. The experimental results show that the proposed method achieves 81.9% of success rate and 73.7% of SPL on a model with 0.8 memory sparsity, and also the results of the models with other memory sparsity achieve reasonable performances compare to the baseline methods.},
booktitle = {2020 20th International Conference on Control, Automation and Systems (ICCAS)},
pages = {883–886},
numpages = {4},
location = {Busan, Korea (South)}
}

@inproceedings{10.5555/3540261.3542303,
author = {Hahn, Meera and Chaplot, Devendra and Tulsiani, Shubham and Mukadam, Mustafa and Rehg, James M. and Gupta, Abhinav},
title = {No RL, no simulation: learning to navigate without navigating},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future image-based navigation tasks that use RL or Simulation.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2042},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.5555/3505326.3505356,
author = {Ravari, Yaser Norouzzadeh and Spronck, Pieter and Sifa, Rafet and Drachen, Anders},
title = {Predicting victory in a hybrid online competitive game: the case of Destiny},
year = {2017},
isbn = {978-1-57735-791-9},
publisher = {AAAI Press},
abstract = {Competitive multi-player game play is a common feature in major commercial titles, and has formed the foundation for esports. In this paper, the question whether it is possible to predict match outcomes in First Person Shooter-type multiplayer competitive games with mixed genres is addressed. The case employed is Destiny, which forms a hybrid title combining Massively Multi-player Online Role-Playing game features and First-Person Shooter games. Destiny provides the opportunity to investigate prediction of the match outcome, as well as the influence of performance metrics on the match results in a hybrid multi-player major commercial title. Two groups of models are presented for predicting match results: One group predicts match results for each individual game mode and the other group predicts match results in general, without considering specific game modes. Models achieve a performance between 63% and 99% in terms of average precision, with a higher performance recorded for the models trained on specific multi-player game modes, of which Destiny has several. We also analyzed performance metrics and their influence for each model. The results show that many key shooter performance metrics such as Kill/Death ratio are relevant across game modes, but also that some performance metrics are mainly important for specific competitive game modes. The results indicate that reliable match prediction is possible in FPS-type esports games.},
booktitle = {Proceedings of the Thirteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
articleno = {30},
numpages = {7},
location = {Little Cottonwood Canyon, Utah, USA},
series = {AIIDE'17}
}

@inproceedings{10.1145/3324884.3416573,
author = {M\"{u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
title = {Identifying software performance changes across variants and versions},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416573},
doi = {10.1145/3324884.3416573},
abstract = {We address the problem of identifying performance changes in the evolution of configurable software systems. Finding optimal configurations and configuration options that influence performance is already difficult, but in the light of software evolution, configuration-dependent performance changes may lurk in a potentially large number of different versions of the system.In this work, we combine two perspectives---variability and time---into a novel perspective. We propose an approach to identify configuration-dependent performance changes retrospectively across the software variants and versions of a software system. In a nutshell, we iteratively sample pairs of configurations and versions and measure the respective performance, which we use to update a model of likelihoods for performance changes. Pursuing a search strategy with the goal of measuring selectively and incrementally further pairs, we increase the accuracy of identified change points related to configuration options and interactions.We have conducted a number of experiments both on controlled synthetic data sets as well as in real-world scenarios with different software systems. Our evaluation demonstrates that we can pinpoint performance shifts to individual configuration options and interactions as well as commits introducing change points with high accuracy and at scale. Experiments on three real-world systems explore the effectiveness and practicality of our approach.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {611–622},
numpages = {12},
keywords = {active learning, configurable software systems, machine learning, software evolution, software performance},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1007/s11192-019-03307-5,
author = {Tattershall, E. and Nenadic, G. and Stevens, R. D.},
title = {Detecting bursty terms in computer science research},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {122},
number = {1},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-019-03307-5},
doi = {10.1007/s11192-019-03307-5},
abstract = {Research topics rise and fall in popularity over time, some more swiftly than others. The fastest rising topics are typically called bursts; for example “deep learning”, “internet of things” and “big data”. Being able to automatically detect and track bursty terms in the literature could give insight into how scientific thought evolves over time. In this paper, we take a trend detection algorithm from stock market analysis and apply it to over 30&nbsp;years of computer science research abstracts, treating the prevalence of each term in the dataset like the price of a stock. Unlike previous work in this domain, we use the free text of abstracts and titles, resulting in a finer-grained analysis. We report a list of bursty terms, and then use historical data to build a classifier to predict whether they will rise or fall in popularity in the future, obtaining accuracy in the region of 80%. The proposed methodology can be applied to any time-ordered collection of text to yield past and present bursty terms and predict their probable fate.},
journal = {Scientometrics},
month = jan,
pages = {681–699},
numpages = {19},
keywords = {Computer science, Bibliometrics, Term life cycles, Machine learning, DBLP, MACD}
}

@inproceedings{10.1007/978-3-030-48077-6_3,
author = {Claris\'{o}, Robert and Cabot, Jordi},
title = {Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering},
year = {2020},
isbn = {978-3-030-48076-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48077-6_3},
doi = {10.1007/978-3-030-48077-6_3},
abstract = {Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.},
booktitle = {Rigorous State-Based Methods: 7th International Conference, ABZ 2020, Ulm, Germany, May 27–29, 2020, Proceedings},
pages = {27–43},
numpages = {17},
keywords = {Model-driven engineering, Verification and validation, Testing, Graph kernels, Clustering, Diversity},
location = {Ulm, Germany}
}

@inproceedings{10.1007/978-3-030-98682-7_11,
author = {Blumenkamp, Jan and Baude, Andreas and Laue, Tim},
title = {Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_11},
doi = {10.1007/978-3-030-98682-7_11},
abstract = {Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {127–139},
numpages = {13},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.eswa.2016.01.035,
author = {Xu, Jingxin and Denman, Simon and Fookes, Clinton and Sridharan, Sridha},
title = {Detecting rare events using Kullback-Leibler divergence},
year = {2016},
issue_date = {July 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {54},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.01.035},
doi = {10.1016/j.eswa.2016.01.035},
abstract = {We present a weakly supervised approach for rare event detection.Coarse annotation, denoting only roughly when an event occurs is needed.The approach leverages the rare nature of the target events to its advantage.We demonstrate the proposed approach on the popular MIT traffic dataset.State-of-the-art performance is shown, alongside being real-time capable. Video surveillance infrastructure has been widely installed in public places for security purposes. However, live video feeds are typically monitored by human staff, making the detection of important events as they occur difficult. As such, an expert system that can automatically detect events of interest in surveillance footage is highly desirable. Although a number of approaches have been proposed, they have significant limitations: supervised approaches, which can detect a specific event, ideally require a large number of samples with the event spatially and temporally localised; while unsupervised approaches, which do not require this demanding annotation, can only detect whether an event is abnormal and not specific event types. To overcome these problems, we formulate a weakly-supervised approach using Kullback-Leibler (KL) divergence to detect rare events. The proposed approach leverages the sparse nature of the target events to its advantage, and we show that this data imbalance guarantees the existence of a decision boundary to separate samples that contain the target event from those that do not. This trait, combined with the coarse annotation used by weakly supervised learning (that only indicates approximately when an event occurs), greatly reduces the annotation burden while retaining the ability to detect specific events. Furthermore, the proposed classifier requires only a decision threshold, simplifying its use compared to other weakly supervised approaches. We show that the proposed approach outperforms state-of-the-art methods on a popular real-world traffic surveillance dataset, while preserving real time performance.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {13–28},
numpages = {16},
keywords = {Anomaly detection, Event detection, Kullback-Leibler divergence, Weakly supervised learning}
}

@article{10.1016/j.jbi.2008.12.012,
author = {Saha, Sujan Kumar and Sarkar, Sudeshna and Mitra, Pabitra},
title = {Feature selection techniques for maximum entropy based biomedical named entity recognition},
year = {2009},
issue_date = {October, 2009},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {42},
number = {5},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2008.12.012},
doi = {10.1016/j.jbi.2008.12.012},
abstract = {Named entity recognition is an extremely important and fundamental task of biomedical text mining. Biomedical named entities include mentions of proteins, genes, DNA, RNA, etc which often have complex structures, but it is challenging to identify and classify such entities. Machine learning methods like CRF, MEMM and SVM have been widely used for learning to recognize such entities from an annotated corpus. The identification of appropriate feature templates and the selection of the important feature values play a very important role in the success of these methods. In this paper, we provide a study on word clustering and selection based feature reduction approaches for named entity recognition using a maximum entropy classifier. The identification and selection of features are largely done automatically without using domain knowledge. The performance of the system is found to be superior to existing systems which do not use domain knowledge.},
journal = {J. of Biomedical Informatics},
month = oct,
pages = {905–911},
numpages = {7},
keywords = {Biomedical named entity recognition, Feature reduction, Feature selection, Machine learning, Maximum entropy classifier}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {AutoML, NAS, configuration search, neural architecture search},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1109/TPAMI.2020.2972281,
author = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
title = {Vision-Language Navigation Policy Learning and Adaptation},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Computer Society},
address = {USA},
volume = {43},
number = {12},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2020.2972281},
doi = {10.1109/TPAMI.2020.2972281},
abstract = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = dec,
pages = {4205–4216},
numpages = {12}
}

@article{10.1007/s10766-016-0417-6,
author = {Allombert, V. and Gava, F. and Tesson, J.},
title = {Multi-ML: Programming Multi-BSP Algorithms in ML},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-016-0417-6},
doi = {10.1007/s10766-016-0417-6},
abstract = {bsp is a bridging model between abstract execution and concrete parallel systems. Structure and abstraction brought by bsp allow to have portable parallel programs with scalable performance predictions, without dealing with low-level details of architectures. In the past, we designed bsml for programming bsp algorithms in ml. However, the simplicity of the bsp model does not fit the complexity of today's hierarchical architectures such as clusters of machines with multiple multi-core processors. The multi-bsp model is an extension of the bsp model which brings a tree-based view of nested components of hierarchical architectures. To program multi-bsp algorithms in ml, we propose the multi-ml language as an extension of bsml where a specific kind of recursion is used to go through a hierarchy of computing nodes. We define a formal semantics of the language and present preliminary experiments which show performance improvements with respect to bsml.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {340–361},
numpages = {22},
keywords = {Parallel programming, bsp, ml, multi-bsp}
}

@inproceedings{10.5555/3491440.3492028,
author = {Chen, Cheng and Luo, Luo and Zhang, Weinan and Yu, Yong and Lian, Yijiang},
title = {Efficient and robust high-dimensional linear contextual bandits},
year = {2021},
isbn = {9780999241165},
abstract = {The linear contextual bandits is a sequential decision-making problem where an agent decides among sequential actions given their corresponding contexts. Since large-scale data sets become more and more common, we study the linear contextual bandits in high-dimensional situations. Recent works focus on employing matrix sketching methods to accelerating contextual bandits. However, the matrix approximation error will bring additional terms to the regret bound. In this paper we first propose a novel matrix sketching method which is called Spectral Compensation Frequent Directions (SCFD). Then we propose an efficient approach for contextual bandits by adopting SCFD to approximate the covariance matrices. By maintaining and manipulating sketched matrices, our method only needs O(md) space and O(md) update time in each round, where d is the dimensionality of the data and m is the sketching size. Theoretical analysis reveals that our method has better regret bounds than previous methods in high-dimensional cases. Experimental results demonstrate the effectiveness of our algorithm and verify our theoretical guarantees.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {588},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.5555/1308171.1308193,
author = {Gruler, Alexander and Harhurin, Alexander and Hartmann, Judith},
title = {Development and Configuration of Service-based Product Lines},
year = {2007},
isbn = {0769528880},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced during the development of multi-functional system families. Addressing this trend we present an approach combining model-based development with product line techniques aiming at a consistent description of a software product family as well as supporting the configuration of its variants. We integrate the concept of variability in our framework [7] which only supported the representation of single software systems on subsequent abstraction levels so far. For the configuration of a concrete product we extend this framework by a feature-based model which allows to configure and derive single systems from a system family model. Furthermore, we explain how the complexity due to the possibly huge amount of configuration decisions can be handled by means of a staged configuration process.},
booktitle = {Proceedings of the 11th International Software Product Line Conference},
pages = {107–116},
numpages = {10},
series = {SPLC '07}
}

@inbook{10.5555/3454287.3454621,
author = {Hwang, Gunpil and Kim, Seohyeon and Bae, Hyeon-Min},
title = {Bat-G net: bat-inspired high-resolution 3D image reconstruction using ultrasonic echoes},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899, and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {334},
numpages = {12}
}

@inproceedings{10.1145/1352135.1352185,
author = {Rao, T. M. and Mitra, Sandeep},
title = {An early software engineering approach to teaching cs1, cs2 and ai},
year = {2008},
isbn = {9781595937995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1352135.1352185},
doi = {10.1145/1352135.1352185},
abstract = {We propose the use of a new design-first approach called Problem Stereotypes and Solution Frameworks, for teaching CS1 and CS2. A problem stereotype is a category of problems that can be solved using similar techniques. A solution framework is a typical solution to a problem, parts of which can be reused to solve other problems of this stereotype. Students are introduced to a stereotype through a selection of related problems, and common features among these are identified. Homework problems are selected from the same stereotype, with students expected to follow the "recipe" provided by the given examples to generate their own solutions. Using this approach reduces the stress level for beginner students, and prevents them falling prey to the "CS is HARD" myth. We present the results of our experience with this approach in two introductory classes and an upper-division Artificial Intelligence (AI) class at SUNY Brockport.},
booktitle = {Proceedings of the 39th SIGCSE Technical Symposium on Computer Science Education},
pages = {143–147},
numpages = {5},
keywords = {artificial intelligence, cs1/2, framework, game playing, puzzle solving, software engineering, stereotype, teaching},
location = {Portland, OR, USA},
series = {SIGCSE '08}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Domain Analysis, Feature Models, Product Lines},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.5555/3491440.3491754,
author = {Han, Zhongyi and Gui, Xian-Jin and Cui, Chaoran and Yin, Yilong},
title = {Towards accurate and robust domain adaptation under noisy environments},
year = {2021},
isbn = {9780999241165},
abstract = {In non-stationary environments, learning machines usually confront the domain adaptation scenario where the data distribution does change over time. Previous domain adaptation works have achieved great success in theory and practice. However, they always lose robustness in noisy environments where the labels and features of examples from the source domain become corrupted. In this paper, we report our attempt towards achieving accurate noise-robust domain adaptation. We first give a theoretical analysis that reveals how harmful noises influence unsupervised domain adaptation. To eliminate the effect of label noise, we propose an offline curriculum learning for minimizing a newly-defined empirical source risk. To reduce the impact of feature noise, we propose a proxy distribution based margin discrepancy. We seamlessly transform our methods into an adversarial network that performs efficient joint optimization for them, successfully mitigating the negative influence from both data corruption and distribution shift. A series of empirical studies show that our algorithm remarkably outperforms state of the art, over 10% accuracy improvements in some domain adaptation tasks under noisy environments.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {314},
numpages = {8},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.5555/3491440.3492078,
author = {Wang, Xintong and Wellman, Michael P.},
title = {Market manipulation: an adversarial learning framework for detection and evasion},
year = {2021},
isbn = {9780999241165},
abstract = {We propose an adversarial learning framework to capture the evolving game between a regulator who develops tools to detect market manipulation and a manipulator who obfuscates actions to evade detection. The model includes three main parts: (1) a generator that learns to adapt original manipulation order streams to resemble trading patterns of a normal trader while preserving the manipulation intent; (2) a discriminator that differentiates the adversarially adapted manipulation order streams from normal trading activities; and (3) an agent-based simulator that evaluates the manipulation effect of adapted outputs. We conduct experiments on simulated order streams associated with a manipulator and a market-making agent respectively. We show examples of adapted manipulation order streams that mimic a specified market maker's quoting patterns and appear qualitatively different from the original manipulation strategy we implemented in the simulator. These results demonstrate the possibility of automatically generating a diverse set of (unseen) manipulation strategies that can facilitate the training of more robust detection algorithms.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {638},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.1007/978-3-030-58539-6_2,
author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
title = {SoundSpaces: Audio-Visual Navigation in&nbsp;3D&nbsp;Environments},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_2},
doi = {10.1007/978-3-030-58539-6_2},
abstract = {Moving around in the world is naturally a multisensory experience, but today’s embodied agents are deaf—restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception. Project: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {17–36},
numpages = {20},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/2046684.2046686,
author = {Bannur, Sushma Nagesh and Saul, Lawrence K. and Savage, Stefan},
title = {Judging a site by its content: learning the textual, structural, and visual features of malicious web pages},
year = {2011},
isbn = {9781450310031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046684.2046686},
doi = {10.1145/2046684.2046686},
abstract = {The physical world is rife with cues that allow us to distinguish between safe and unsafe situations. By contrast, the Internet offers a much more ambiguous environment; hence many users are unable to distinguish a scam from a legitimate Web page. To help address this problem, we explore how to train classifiers that can automatically identify malicious Web pages based on clues from their textual content, structural tags, page links, visual appearance, and URLs. Using a contemporary labeled data feed from a large Web mail provider, we extract such features and demonstrate how they can be used to improve classification accuracy over previous, more constrained approaches. In particular, by analyzing the full content of individual Web pages, we more than halve the error rate obtained by a comparably trained classifier that only extracts features from URLs. By training classifiers on different sets of features, we are further able to assess the strength of clues provided by these different sources of information.},
booktitle = {Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence},
pages = {1–10},
numpages = {10},
keywords = {blacklisting, machine learning, web security},
location = {Chicago, Illinois, USA},
series = {AISec '11}
}

@inproceedings{10.1145/3297156.3297243,
author = {Volzhaninov, D. A. and Lookin, O. N. and Antsygin, I. N. and Khokhlova, A. D.},
title = {Design and Programming of the Micromanipulator Network to Study Single Cardiac Cell Mechanics},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297243},
doi = {10.1145/3297156.3297243},
abstract = {Computer-controlled biomechanical experiments on single cells require high accuracy and synchronicity of signals and stability of experimental set-ups. In this paper, we propose a digital micromanipulator network consisting of two micromanipulators to study single cardiac cell mechanics. The micromanipulation system using Ethernet for this network has a large positioning range (20 mm), covering the experimental bath with the cells and nanometer movement resolution (5 nm) for precise experiments. The software for the micromanipulator network was developed in the LabVIEW to provide sufficient synchronicity of micromanipulator movement. The testing of seven consistent steps of two micromanipulators showed that movement was synchronous and maximum loss of synchrony of coordinates was 170 nm. We believe that studies on the changes in single cardiac cell length and force under various mechanical loads can be carried out using the developed network.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {455–458},
numpages = {4},
keywords = {Biomechanics, Ethernet, LabVIEW, Micromanipulator network, Single cardiac cell},
location = {Shenzhen, China},
series = {CSAI '18}
}

@article{10.1016/j.specom.2019.09.003,
author = {Shirzhiyan, Zahra and Shamsi, Elham and Jafarpisheh, Amir Salar and Jafari, Amir Homayoun},
title = {Objective classification of auditory brainstem responses to consonant-vowel syllables using local discriminant bases},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.09.003},
doi = {10.1016/j.specom.2019.09.003},
journal = {Speech Commun.},
month = nov,
pages = {36–48},
numpages = {13},
keywords = {Speech ABR, Speech encoding, Local discriminant bases}
}

@article{10.1016/j.artint.2007.05.008,
author = {Denundefinedux, Thierry},
title = {Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {172},
number = {2–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2007.05.008},
doi = {10.1016/j.artint.2007.05.008},
abstract = {Dempster's rule plays a central role in the theory of belief functions. However, it assumes the combined bodies of evidence to be distinct, an assumption which is not always verified in practice. In this paper, a new operator, the cautious rule of combination, is introduced. This operator is commutative, associative and idempotent. This latter property makes it suitable to combine belief functions induced by reliable, but possibly overlapping bodies of evidence. A dual operator, the bold disjunctive rule, is also introduced. This operator is also commutative, associative and idempotent, and can be used to combine belief functions issues from possibly overlapping and unreliable sources. Finally, the cautious and bold rules are shown to be particular members of infinite families of conjunctive and disjunctive combination rules based on triangular norms and conorms.},
journal = {Artif. Intell.},
month = feb,
pages = {234–264},
numpages = {31},
keywords = {Dempster--Shafer theory, Distinct evidence, Evidence theory, Idempotence, Information fusion, Transferable belief model}
}

@article{10.1016/j.engappai.2012.07.010,
author = {Yang, Dong and Dong, Ming and Chang, Xiao-Kun},
title = {A dynamic constraint satisfaction approach for configuring structural products under mass customization},
year = {2012},
issue_date = {December, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {25},
number = {8},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2012.07.010},
doi = {10.1016/j.engappai.2012.07.010},
abstract = {Configuring structured products poses new challenges to the solving technologies for product configuration. This paper presents a novel and direct approach to encoding configuration models into the Dynamic Constraint Satisfaction Problems (DCSP). In the presented approach, components are encoded as DCSP variables while structural relationships are represented as DCSP activity constraints. Furthermore, the configuration constraints such as the requisition and exclusion constraints are treated as DCSP compatibility constraints, which allow a low-level component to join in the solving process only after its high-level component is selected in the configuration. The presented method allows a more compact encoding representation, compared to CSP and generative CSP. Experimental study shows that the presented DCSP encoding approach makes a significant improvement in the performance of product configuration.},
journal = {Eng. Appl. Artif. Intell.},
month = dec,
pages = {1723–1737},
numpages = {15},
keywords = {Configuration modeling, Constraint satisfaction problem, Mass customization, Product configuration}
}

@inproceedings{10.1109/ASE.2009.11,
author = {Gr\"{u}nbacher, Paul and Rabiser, Rick and Dhungana, Deepak and Lehofer, Martin},
title = {Model-Based Customization and Deployment of Eclipse-Based Tools: Industrial Experiences},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.11},
doi = {10.1109/ASE.2009.11},
abstract = {Developers of software engineering tools are facing high expectations regarding capabilities and usability. Users expect tools tailored to their specific needs and integrated in their working environment. This increases tools' complexity and complicates their customization and deployment despite available mechanisms for adaptability and extensibility. A main challenge lies in understanding and managing the dependencies between different technical mechanisms for realizing tool variability. We report on industrial experiences of applying a model-based and tool-supported product line approach for the customization and deployment of two Eclipse-based tools. We illustrate challenges of customizing these tools to different development contexts: In the first case study we developed variability models of a product line tool suite used by an industry partner and utilized these models for tool customization and deployment. In the second case study we applied the same approach to a maintenance and setup tool of our industry partner. Our experiences suggest to design software tools as product lines; to formally describe the tools' variability in models; and to provide end-user capabilities for customizing and deploying the tools.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {247–256},
numpages = {10},
keywords = {Elicpse-based tools, deployment, end-user customization, industrial experience, product line engineering},
series = {ASE '09}
}

@article{10.1016/j.neucom.2015.07.152,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng},
title = {Hessian regularization by patch alignment framework},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {204},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.07.152},
doi = {10.1016/j.neucom.2015.07.152},
abstract = {In recent years, semi-supervised learning has played a key part in large-scale image management, where usually only a few images are labeled. To address this problem, many representative works have been reported, including transductive SVM, universum SVM, co-training and graph-based methods. The prominent method is the patch alignment framework, which unifies the traditional spectral analysis methods. In this paper, we propose Hessian regression based on the patch alignment framework. In particular, we construct a Hessian using the patch alignment framework and apply it to regression problems. To the best of our knowledge, there is no report on Hessian construction from the patch alignment viewpoint. Compared with the traditional Laplacian regularization, Hessian can better match the data and then leverage the performance. To validate the effectiveness of the proposed method, we conduct human face recognition experiments on a celebrity face dataset. The experimental results demonstrate the superiority of the proposed solution in human face classification.},
journal = {Neurocomput.},
month = sep,
pages = {183–188},
numpages = {6},
keywords = {Hessian, Least squares, Patch alignment, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, \"{O}nder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Hierarchical clustering, Model comparison, Model-Driven Engineering, R, Vector space model}
}

@article{10.1016/j.dsp.2021.103106,
author = {Zhang, Hai and Xie, Qiangqiang and Lu, Bei and Gai, Shan},
title = {Dual attention residual group networks for single image deraining},
year = {2021},
issue_date = {Sep 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103106},
doi = {10.1016/j.dsp.2021.103106},
journal = {Digit. Signal Process.},
month = sep,
numpages = {11},
keywords = {Spatial attention, Residual groups, Channel attention, Single image rain removal}
}

@inproceedings{10.5555/3495724.3496081,
author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
title = {Object goal navigation using goal-oriented semantic exploration},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, 'Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {357},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/1321631.1321741,
author = {Gawley, Rachel},
title = {Automating the identification of variability realisation techniques from feature models},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321741},
doi = {10.1145/1321631.1321741},
abstract = {In Software Product Line Engineering (SPLE), feature modelling is frequently used to model commonalities and variabilities within a domain. A feature model captures an abstract view of a product line and it can serve as a starting point for software design and component implementation. Handling variability exposed within the feature model is an important problem in this context, and in this paper, we describe ongoing research aimed at automating the identification of variability realisation techniques from feature models},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {555–558},
numpages = {4},
keywords = {design patterns, feature models, software product lines, variability realisation techniques},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1007/978-3-030-98682-7_6,
author = {Bestmann, Marc and Engelke, Timon and Fiedler, Niklas and G\"{u}ldenstein, Jasper and Gutsche, Jan and Hagge, Jonas and Vahl, Florian},
title = {TORSO-21 Dataset: Typical Objects in&nbsp;RoboCup Soccer 2021},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_6},
doi = {10.1007/978-3-030-98682-7_6},
abstract = {We present a dataset specifically designed to be used as a benchmark to compare vision systems in the RoboCup Humanoid Soccer domain. The dataset is composed of a collection of images taken in various real-world locations as well as a collection of simulated images. It enables comparing vision approaches with a meaningful and expressive metric. The contributions of this paper consist of providing a comprehensive and annotated dataset, an overview of the recent approaches to vision in RoboCup, methods to generate vision training data in a simulated environment, and an approach to increase the variety of a dataset by automatically selecting a diverse set of images from a larger pool. Additionally, we provide a baseline of YOLOv4 and YOLOv4-tiny on this dataset.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {65–77},
numpages = {13},
keywords = {Computer vision, Vision dataset, Deep learning},
location = {Sydney, NSW, Australia}
}

@article{10.1145/3360512,
author = {Motamedi, Mohammad and Portillo, Felix A. and Fong, Daniel and Ghiasi, Soheil},
title = {Distill-Net: Application-Specific Distillation of Deep Convolutional Neural Networks for Resource-Constrained IoT Platforms},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3360512},
doi = {10.1145/3360512},
abstract = {Many Internet-of-Things (IoT) applications demand fast and accurate understanding of a few key events in their surrounding environment. Deep Convolutional Neural Networks (CNNs) have emerged as an effective approach to understand speech, images, and similar high-dimensional data types. Algorithmic performance of modern CNNs, however, fundamentally relies on learning class-agnostic hierarchical features that only exist in comprehensive training datasets with many classes. As a result, fast inference using CNNs trained on such datasets is prohibitive for most resource-constrained IoT platforms. To bridge this gap, we present a principled and practical methodology for distilling a complex modern CNN that is trained to effectively recognize many different classes of input data into an application-dependent essential core that not only recognizes the few classes of interest to the application accurately but also runs efficiently on platforms with limited resources. Experimental results confirm that our approach strikes a favorable balance between classification accuracy (application constraint), inference efficiency (platform constraint), and productive development of new applications (business constraint).},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {44},
numpages = {20},
keywords = {Convolutional neural network, deep learning, embedded systems, resource scalable systems}
}

@inproceedings{10.5555/3504035.3504989,
author = {Ieva, Carlo and Gotlieb, Arnaud and Kaci, Souhila and Lazaar, Nadjib},
title = {Discovering program topoi through clustering},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Understanding source code of large open-source software projects is very challenging when there is only little documentation. New developers face the task of classifying a huge number of files and functions without any help. This paper documents a novel approach to this problem, called FEAT, that automatically extracts topoi from source code by using hierarchical agglomerative clustering. Program topoi summarize the main capabilities of a software system by presenting to developers clustered lists of functions together with an index of their relevant words. The clustering method used in FEAT exploits a new hybrid distance which combines both textual and structural elements automatically extracted from source code and comments. The experimental evaluation of FEAT shows that this approach is suitable to understand open-source software projects of size approaching 2,000 functions and 150 files, which opens the door for its deployment in the open-source community.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {954},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.neucom.2019.05.009,
author = {Song, Shaoyue and Yu, Hongkai and Miao, Zhenjiang and Guo, Dazhou and Ke, Wei and Ma, Cong and Wang, Song},
title = {An easy-to-hard learning strategy for within-image co-saliency detection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {358},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.009},
doi = {10.1016/j.neucom.2019.05.009},
journal = {Neurocomput.},
month = sep,
pages = {166–176},
numpages = {11},
keywords = {Within-image co-saliency, Easy-to-hard learning, Multiple instance learning}
}

@article{10.1016/j.scico.2012.06.007,
author = {Cetina, Carlos and Giner, Pau and Fons, Joan and Pelechano, Vicente},
title = {Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.007},
doi = {10.1016/j.scico.2012.06.007},
abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2399–2413},
numpages = {15},
keywords = {Dynamic Software Product Line, Smart Hotel, Variability modeling}
}

@inproceedings{10.1007/978-3-030-58542-6_19,
author = {Wang, Hanqing and Wang, Wenguan and Shu, Tianmin and Liang, Wei and Shen, Jianbing},
title = {Active Visual Information Gathering for Vision-Language Navigation},
year = {2020},
isbn = {978-3-030-58541-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58542-6_19},
doi = {10.1007/978-3-030-58542-6_19},
abstract = {Vision-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct a robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to avoid random and inefficient actions at every step. In contrast, when humans face such a challenge, they can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make more confident navigation decisions. This work draws inspiration from human navigation behavior and endows an agent with an active information gathering ability for a more intelligent vision-language navigation policy. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides i) when and where to explore, ii) what information is worth gathering during exploration, and iii) how to adjust the navigation decision after the exploration. The experimental results show promising exploration strategies emerged from training, which leads to significant boost in navigation performance. On the R2R challenge leaderboard, our agent gets promising results all three VLN settings, i.e., single run, pre-exploration, and beam search.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII},
pages = {307–322},
numpages = {16},
keywords = {Vision-language navigation, Active exploration},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.sigpro.2020.107466,
author = {Zhu, Qi and Xu, Xiangyu and Yuan, Ning and Zhang, Zheng and Guan, Donghai and Huang, Sheng-Jun and Zhang, Daoqiang},
title = {Latent correlation embedded discriminative multi-modal data fusion},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2020.107466},
doi = {10.1016/j.sigpro.2020.107466},
journal = {Signal Process.},
month = jun,
numpages = {11},
keywords = {Multi-modal data fusion, Classification, Self-paced learning, Sparse representation}
}

@article{10.1016/j.jss.2009.10.011,
author = {Sun, Chang-ai and Rossing, Rowan and Sinnema, Marco and Bulanov, Pavel and Aiello, Marco},
title = {Modeling and managing the variability of Web service-based systems},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.10.011},
doi = {10.1016/j.jss.2009.10.011},
abstract = {Web service-based systems are built orchestrating loosely coupled, standardized, and internetworked programs. If on the one hand, Web services address the interoperability issues of modern information systems, on the other hand, they enable the development of software systems on the basis of reuse, greatly limiting the necessity for reimplementation. Techniques and methodologies to gain the maximum from this emerging computing paradigm are in great need. In particular, a way to explicitly model and manage variability would greatly facilitate the creation and customization of Web service-based systems. By variability we mean the ability of a software system to be extended, changed, customized or configured for use in a specific context. We present a framework and related tool suite for modeling and managing the variability of Web service-based systems for design and run-time, respectively. It is an extension of the COVAMOF framework for the variability management of software product families, which was developed at the University of Groningen. Among the novelties and advantages of the approach are the full modeling of variability via UML diagrams, the run-time support, and the low involvement of the user. All of which leads to a great deal of automation in the management of all kinds of variability.},
journal = {J. Syst. Softw.},
month = mar,
pages = {502–516},
numpages = {15},
keywords = {Service engineering, Variability management, Variability modeling, Web services}
}

@inproceedings{10.1145/2973839.2973852,
author = {Santos, Ismayle S. and Rocha, Lincoln S. and Neto, Pedro A. Santos and Andrade, Rossana M. C.},
title = {Model Verification of Dynamic Software Product Lines},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973852},
doi = {10.1145/2973839.2973852},
abstract = {Dynamic Software Product Lines (DSPLs) extend the concept of Software Product Lines enabling adaptation at runtime according to context changes. Such dynamic behavior is typically designed using adaptation rules, context-triggered actions responsible for features activation and deactivation at runtime. The erroneous specification and the interleaving of adaptation rules (i.e., the parallel execution of adaptation rules) can lead DSPL to reach an undesired (improperly or defective) product configuration at runtime. Thus, in order to improve the reliability of DSPL behavior, design faults must be rigorously identified and eliminated in the early stages of DSPL development. In this paper, we address this issue introducing Dynamic Feature Transition Systems (DFTSs) that allow the modeling and formal verification of the DSPLs adaptive behavior. These transition systems are derived from the adaptation rules and a Context Kripke Structure, which is a context evolution model. Furthermore, we formally define five properties that can be used to identify existing design faults in DSPL design. Aiming to assess the feasibility of our approach, a feasibility study was conducted using two DSPLs, Mobile Visit Guides and Car. In both cases, design faults were automatically detected indicating that our formalism can help in the detection of design faults in the DSPLs adaptive behavior.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {Dynamic Software Product Line, Model Checking, Software Reliability, Software Verification},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@article{10.1016/j.artint.2008.11.004,
author = {Chen, Yixin and Huang, Ruoyun and Xing, Zhao and Zhang, Weixiong},
title = {Long-distance mutual exclusion for planning},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {173},
number = {2},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2008.11.004},
doi = {10.1016/j.artint.2008.11.004},
abstract = {Mutual exclusion (mutex) is a powerful mechanism for search space pruning in planning. However, a serious limitation of mutex is that it cannot specify constraints relating actions and facts across different time steps. In this paper, we propose a new class of mutual exclusions that significantly generalizes mutex and can be efficiently computed. The proposed long-distance mutual exclusion (londex) can capture constraints over actions and facts not only at the same time step but also across multiple steps. As a generalization, londex is much stronger than mutex, and provides a general and effective tool for developing efficient planners. We propose two levels of londex. The first level, londex"1, is derived from individual domain transition graphs (DTGs), and the second level, londex"m, is derived from multiple DTGs by taking into account the interactions among them. Londex constraints provide stronger pruning power but also require a large amount of memory. To address the memory problem, we further develop a virtual realization mechanism in which only a small proportion of londex constraints are dynamically generated as needed during the search. This scheme can save a huge amount of memory without sacrificing the pruning power of londex. For evaluation purposes, we incorporate londex into SATPlan04 and SATPlan06, two efficient SAT-based planners. Our experimental results show that londex"m can significantly improve over londex"1 since the former exploits causal dependencies among DTGs. Our experimental results for various planning domains also show significant advantages of using londex constraints for reducing planning time.},
journal = {Artif. Intell.},
month = feb,
pages = {365–391},
numpages = {27},
keywords = {Constraint propagation, Mutual exclusion, Planning, Satisfiability}
}

@article{10.1016/j.jss.2014.10.037,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Galindo, Jos\'{e} A. and Parejo, Jos\'{e} A. and Benavides, David and Segura, Sergio and Egyed, Alexander},
title = {An assessment of search-based techniques for reverse engineering feature models},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.10.037},
doi = {10.1016/j.jss.2014.10.037},
abstract = {HighlightsSearch based techniques perform well for reverse engineering feature models.Different algorithms and objectives favour precision and recall differently.The F1 objective function provides a trade-off between precision and recall. Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on Fβ, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.},
journal = {J. Syst. Softw.},
month = may,
pages = {353–369},
numpages = {17},
keywords = {Feature model, Reverse engineering, Search Based Software Engineering}
}

@inproceedings{10.1145/2970276.2975938,
author = {Babur, \"{O}nder},
title = {Statistical analysis of large sets of models},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2975938},
doi = {10.1145/2970276.2975938},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {Model-driven engineering, clustering, model comparison, vector space model},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.5555/3291125.3309615,
author = {Lamprier, Sylvain and Gisselbrecht, Thibault and Gallinari, Patrick},
title = {Profile-based bandit with unknown profiles},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such proffles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2060–2099},
numpages = {40},
keywords = {profile-based exploration, stochastic linear bandits, upper confidence bounds}
}

@inproceedings{10.1007/978-3-030-64793-3_2,
author = {Shi, Zheyuan Ryan and Procaccia, Ariel D. and Chan, Kevin S. and Venkatesan, Sridhar and Ben-Asher, Noam and Leslie, Nandi O. and Kamhoua, Charles and Fang, Fei},
title = {Learning and Planning in the Feature Deception Problem},
year = {2020},
isbn = {978-3-030-64792-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64793-3_2},
doi = {10.1007/978-3-030-64793-3_2},
abstract = {Today’s high-stakes adversarial interactions feature attackers who constantly breach the ever-improving security measures. Deception mitigates the defender’s loss by misleading the attacker to make suboptimal decisions. In order to formally reason about deception, we introduce the feature deception problem (FDP), a domain-independent model and present a learning and planning framework for finding the optimal deception strategy, taking into account the adversary’s preferences which are initially unknown to the defender. We make the following contributions. (1) We show that we can uniformly learn the adversary’s preferences using data from a modest number of deception strategies. (2) We propose an approximation algorithm for finding the optimal deception strategy given the learned preferences and show that the problem is NP-hard. (3) We perform extensive experiments to validate our methods and results. In addition, we provide a case study of the credit bureau network to illustrate how FDP implements deception on a real-world problem.},
booktitle = {Decision and Game Theory for Security: 11th International Conference, GameSec 2020, College Park, MD, USA, October 28–30, 2020, Proceedings},
pages = {23–44},
numpages = {22},
location = {College Park, MD, USA}
}

@article{10.1016/j.jbi.2017.04.015,
author = {Miller, Timothy and Dligach, Dmitriy and Bethard, Steven and Lin, Chen and Savova, Guergana},
title = {Towards generalizable entity-centric clinical coreference resolution},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {69},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2017.04.015},
doi = {10.1016/j.jbi.2017.04.015},
abstract = {Display Omitted Coreference resolution is important for extracting information from clinical documents.We implement and evaluate a model that tracks entities across a document.We explore generalizability of some important coreference features. ObjectiveThis work investigates the problem of clinical coreference resolution in a model that explicitly tracks entities, and aims to measure the performance of that model in both traditional in-domain train/test splits and cross-domain experiments that measure the generalizability of learned models. MethodsThe two methods we compare are a baseline mention-pair coreference system that operates over pairs of mentions with best-first conflict resolution and a mention-synchronous system that incrementally builds coreference chains. We develop new features that incorporate distributional semantics, discourse features, and entity attributes. We use two new coreference datasets with similar annotation guidelines the THYME colon cancer dataset and the DeepPhe breast cancer dataset. ResultsThe mention-synchronous system performs similarly on in-domain data but performs much better on new data. Part of speech tag features prove superior in feature generalizability experiments over other word representations. Our methods show generalization improvement but there is still a performance gap when testing in new domains. DiscussionGeneralizability of clinical NLP systems is important and under-studied, so future work should attempt to perform cross-domain and cross-institution evaluations and explicitly develop features and training regimens that favor generalizability. A performance-optimized version of the mention-synchronous system will be included in the open source Apache cTAKES software.},
journal = {J. of Biomedical Informatics},
month = may,
pages = {251–258},
numpages = {8},
keywords = {Clinical NLP, Coreference, Generalizability, Machine learning, Portability}
}

@inproceedings{10.5555/3297863.3297936,
author = {Hanna, Josiah P. and Stone, Peter},
title = {Grounded action transformation for robot learning in simulation},
year = {2017},
publisher = {AAAI Press},
abstract = {Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. This paper proposes a new algorithm for learning in simulation - Grounded Action Transformation - and applies it to learning of humanoid bipedal locomotion. Our approach results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk.1},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4931–4932},
numpages = {2},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{10.1145/3469096.3469872,
author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
title = {On minimizing cost in legal document review workflows},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469872},
doi = {10.1145/3469096.3469872},
abstract = {Technology-assisted review (TAR) refers to human-in-the-loop machine learning workflows for document review in legal discovery and other high recall review tasks. Attorneys and legal technologists have debated whether review should be a single iterative process (one-phase TAR workflows) or whether model training and review should be separate (two-phase TAR workflows), with implications for the choice of active learning algorithm. The relative cost of manual labeling for different purposes (training vs. review) and of different documents (positive vs. negative examples) is a key and neglected factor in this debate. Using a novel cost dynamics analysis, we show analytically and empirically that these relative costs strongly impact whether a one-phase or two-phase workflow minimizes cost. We also show how category prevalence, classification task difficulty, and collection size impact the optimal choice not only of workflow type, but of active learning method and stopping point.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {30},
numpages = {10},
keywords = {active learning, cost modeling, high-recall retrieval, total recall},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@article{10.1007/s11263-019-01278-x,
author = {Ramasinghe, Sameera and Khan, Salman and Barnes, Nick and Gould, Stephen},
title = {Representation Learning on Unit Ball with 3D Roto-translational Equivariance},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {6},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01278-x},
doi = {10.1007/s11263-019-01278-x},
abstract = {Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces—such as a sphere (S2) or a unit ball (B3)—entails unique challenges. In this work, we propose a novel ‘volumetric convolution’ operation that can effectively model and convolve arbitrary functions in B3. We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {1612–1634},
numpages = {23},
keywords = {Convolution neural networks, 3D moments, Volumetric convolution, Zernike polynomials, Deep learning}
}

@inproceedings{10.1007/11908029_95,
author = {Ichihashi, Hidetomo and Honda, Katsuhiro and Notsu, Akira},
title = {Postsupervised hard c-means classifier},
year = {2006},
isbn = {3540476938},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11908029_95},
doi = {10.1007/11908029_95},
abstract = {Miyamoto et al. derived a hard clustering algorithms by defuzzifying a generalized entropy-based fuzzy c-means in which covariance matrices are introduced as decision variables. We apply the hard c-means (HCM) clustering algorithms to a postsupervised classifier to improve resubstitution error rate by choosing best clustering results from local minima of an objective function. Due to the nature of the prototype based classifier, the error rates can easily be improved by increasing the number of clusters with the cost of computer memory and CPU speed. But, with the HCM classifier, the resubstitution error rate along with the data set compression ratio is improved on several benchmark data sets by using a small number of clusters for each class.},
booktitle = {Proceedings of the 5th International Conference on Rough Sets and Current Trends in Computing},
pages = {918–927},
numpages = {10},
location = {Kobe, Japan},
series = {RSCTC'06}
}

@article{10.1016/j.dsp.2018.12.007,
author = {Yue, Guanghui and Hou, Chunping and Yan, Weiqing and Choi, Lark Kwon and Zhou, Tianwei and Hou, Yonghong},
title = {Blind quality assessment for screen content images via convolutional neural network},
year = {2019},
issue_date = {Aug 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2018.12.007},
doi = {10.1016/j.dsp.2018.12.007},
journal = {Digit. Signal Process.},
month = aug,
pages = {21–30},
numpages = {10},
keywords = {Screen content image (SCI), Blind/no reference (NR), Image quality assessment (IQA), Convolutional neural network (CNN)}
}

@inproceedings{10.1007/978-3-642-30829-1_6,
author = {Khosravi, Ramtin and Sabouri, Hamideh},
title = {Using coordinated actors to model families of distributed systems},
year = {2012},
isbn = {9783642308284},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30829-1_6},
doi = {10.1007/978-3-642-30829-1_6},
abstract = {Software product line engineering enables strategic reuse in development of families of related products. In a component-based approach to product line development, components capture functionalities appearing in one or more products in the family and different assemblies of components yield to various products or configurations. In this approach, an interaction model which effectively factors out the logic handling variability from the functionality of the system greatly enhances the reusability of components. We study the problem of variability modeling for a family of distributed systems expressed in actor model. We define a special type of actors called coordinators whose behavior is described as Reo circuits with the aim of encapsulating the variability logic. We have the benefits of Reo language for expressing coordination logic, while modeling the entire system as an actor-based distributed model. We have applied this model to a case study extracted from an industrial software family in the domain of interactive TV.},
booktitle = {Proceedings of the 14th International Conference on Coordination Models and Languages},
pages = {74–88},
numpages = {15},
location = {Stockholm, Sweden},
series = {COORDINATION'12}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Search-based Software Testing, Hyper-heuristics, Systematic Mapping, Evolutionary Algorithms, Genetic Algorithms, Meta-heuristics}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {machine learning, uncertainty},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.5555/1158337.1158678,
author = {Czarnecki, Krzysztof and Peter Kim, Chang Hwan and Kalleberg, Karl Trygve},
title = {Feature Models are Views on Ontologies},
year = {2006},
isbn = {0769525997},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Feature modeling has been proposed as an approach for describing variable requirements for software product lines. In this paper, we explore the relationship between feature models and ontologies. First, we examine how previous extensions to basic feature modeling move it closer to richer formalisms for specifying ontologies such as MOF and OWL. Then, we explore the idea of feature models as views on ontologies. Based on that idea, we propose two approaches for the combined use of feature models and ontologies: view derivation and view integration. Finally, we give some ideas about tool support for these approaches.},
booktitle = {Proceedings of the 10th International on Software Product Line Conference},
pages = {41–51},
numpages = {11},
series = {SPLC '06}
}

@article{10.1016/j.patcog.2015.02.027,
author = {Shen, Jialie and Deng, Robert H. and Cheng, Zhiyong and Nie, Liqiang and Yan, Shuicheng},
title = {On robust image spam filtering via comprehensive visual modeling},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {10},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.02.027},
doi = {10.1016/j.patcog.2015.02.027},
abstract = {The Internet has brought about fundamental changes in the way peoples generate and exchange media information. Over the last decade, unsolicited message images (image spams) have become one of the most serious problems for Internet service providers (ISPs), business firms and general end users. In this paper, we report a novel system called RoBoTs (Robust BoosTrap based spam detector) to support accurate and robust image spam filtering. The system is developed based on multiple visual properties extracted from different levels of granularity, aiming to capture more discriminative contents for effective spam image identification. In addition, a resampling based learning framework is developed to effectively integrate random forest and linear discriminative analysis (LDA) to generate comprehensive signature of spam images. It can facilitate more accurate and robust spam classification process with very limited amount of initial training examples. Using three public available test collections, the proposed system is empirically compared with the state-of-the-art techniques. Our results demonstrate its significantly higher performance from different perspectives. Author-HighlightsWe develop a novel scheme to model contents of spam image and compute comprehensive signatures.A hybrid framework is developed to detect spam images effectively.Our approach achieves substantial performance improvement on spam detection in terms of effectiveness and robustness.},
journal = {Pattern Recogn.},
month = oct,
pages = {3227–3238},
numpages = {12},
keywords = {Algorithm, Experimentation, Security, Spam}
}

@article{10.1016/j.compag.2019.105023,
author = {Moon, Taewon and Hong, Seojung and Choi, Ha Young and Jung, Dae Ho and Chang, Se Hong and Son, Jung Eek},
title = {Interpolation of greenhouse environment data using multilayer perceptron},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105023},
doi = {10.1016/j.compag.2019.105023},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {8},
keywords = {Data loss, Linear, Multivariate regression, Random forest, Spline}
}

@article{10.1016/j.image.2019.04.017,
author = {Wu, Hehe and Wang, Anhong and Liang, Jie and Li, Suyue and Li, Peihao},
title = {DCSN-Cast: Deep compressed sensing network for wireless video multicast},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2019.04.017},
doi = {10.1016/j.image.2019.04.017},
journal = {Image Commun.},
month = aug,
pages = {56–67},
numpages = {12},
keywords = {DCSN-cast, DCSRN-cast, DCSFCN-cast, Fully connected network, Compressed sensing, Deep residual network}
}

@inbook{10.5555/3454287.3454321,
author = {Anderson, Peter and Shrivastava, Ayush and Parikh, Devi and Batra, Dhruv and Lee, Stefan},
title = {Chasing ghosts: instruction following as bayesian state tracking},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) [1] within the framework of Bayesian state tracking – learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet [2] baseline when predicting the goal location on the map. On the full VLN task, i.e., navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {34},
numpages = {11}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Classification, Data mining, Feature ranking, Feature selection}
}

@article{10.1016/j.asoc.2015.03.045,
author = {Fahad, Labiba Gillani and Rajarajan, Muttukrishnan},
title = {Integration of discriminative and generative models for activity recognition in smart homes},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.045},
doi = {10.1016/j.asoc.2015.03.045},
abstract = {Graphical abstractDisplay Omitted HighlightsA hybrid activity recognition approach that combines DM with PE using SVM.DM is suitable for imbalanced number of activity instances.PE has better generalization ability in activity recognition.Evaluation on five smart home datasets validates an improved performance of the approach. Activity recognition in smart homes enables the remote monitoring of elderly and patients. In healthcare systems, reliability of a recognition model is of high importance. Limited amount of training data and imbalanced number of activity instances result in over-fitting thus making recognition models inconsistent. In this paper, we propose an activity recognition approach that integrates the distance minimization (DM) and probability estimation (PE) approaches to improve the reliability of recognitions. DM uses distances of instances from the mean representation of each activity class for label assignment. DM is useful in avoiding decision biasing towards the activity class with majority instances; however, DM can result in over-fitting. PE on the other hand has good generalization abilities. PE measures the probability of correct assignments from the obtained distances, while it requires a large amount of data for training. We apply data oversampling to improve the representation of classes with less number of instances. Support vector machine (SVM) is applied to combine the outputs of both DM and PE, since SVM performs better with imbalanced data and further improves the generalization ability of the approach. The proposed approach is evaluated using five publicly available smart home datasets. The results demonstrate better performance of the proposed approach compared to the state-of-the-art activity recognition approaches.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {992–1001},
numpages = {10},
keywords = {Activity recognition, Distance minimization, Pervasive healthcare, Probability estimation, Smart homes, Support vector machine}
}

@article{10.1016/j.scico.2013.06.009,
author = {Sabouri, Hamideh and Khosravi, Ramtin},
title = {Reducing the verification cost of evolving product families using static analysis techniques},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {83},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.06.009},
doi = {10.1016/j.scico.2013.06.009},
abstract = {Software product line engineering enables proactive reuse among a set of related products through explicit modeling of commonalities and differences among them. Software product lines are intended to be used in a long period of time. As a result, they evolve over time, due to the changes in the requirements. Having several individual products in a software family, verification of the entire family may take a considerable effort. In this paper we aim to decrease this cost by reducing the number of verified products using static analysis techniques. Furthermore, to reduce model checking costs after product line evolution, we restrict the number of products that should be re-verified by reusing the previous verification result. All proposed techniques are based on static analysis of the product family model with respect to the property and can be automated. To show the effectiveness of these techniques we apply them on a set of case studies and present the results.},
journal = {Sci. Comput. Program.},
month = apr,
pages = {35–55},
numpages = {21},
keywords = {Model checking, Program slicing, Reduction techniques, Software product lines, Static analysis}
}

@article{10.1007/s10916-015-0219-1,
author = {Ayd\i{}n, Serap and Tunga, M. Alper and Yetkin, Sinan},
title = {Mutual Information Analysis of Sleep EEG in Detecting Psycho-Physiological Insomnia},
year = {2015},
issue_date = {May       2015},
publisher = {Plenum Press},
address = {USA},
volume = {39},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-015-0219-1},
doi = {10.1007/s10916-015-0219-1},
abstract = {The primary goal of this study is to state the clear changes in functional brain connectivity during all night sleep in psycho-physiological insomnia (PPI). The secondary goal is to investigate the usefulness of Mutual Information (MI) analysis in estimating cortical sleep EEG arousals for detection of PPI. For these purposes, healthy controls and patients were compared to each other with respect to both linear (Pearson correlation coefficient and coherence) and nonlinear quantifiers (MI) in addition to phase locking quantification for six sleep stages (stage.1---4, rem, wake) by means of interhemispheric dependency between two central sleep EEG derivations. In test, each connectivity estimation calculated for each couple of epoches (C3-A2 and C4-A1) was identified by the vector norm of estimation. Then, patients and controls were classified by using 10 different types of data mining classifiers for five error criteria such as accuracy, root mean squared error, sensitivity, specificity and precision. High performance in a classification through a measure will validate high contribution of that measure to detecting PPI. The MI was found to be the best method in detecting PPI. In particular, the patients had lower MI, higher PCC for all sleep stages. In other words, the lower sleep EEG synchronization suffering from PPI was observed. These results probably stand for the loss of neurons that then contribute to less complex dynamical processing within the neural networks in sleep disorders an the functional central brain connectivity is nonlinear during night sleep. In conclusion, the level of cortical hemispheric connectivity is strongly associated with sleep disorder. Thus, cortical communication quantified in all existence sleep stages might be a potential marker for sleep disorder induced by PPI.},
journal = {J. Med. Syst.},
month = may,
pages = {1–10},
numpages = {10},
keywords = {Brain connectivity, Classification, Data mining, Mutual information, Sleep EEG}
}

@article{10.1145/1183236.1183264,
author = {Batory, Don and Benavides, David and Ruiz-Cortes, Antonio},
title = {Automated analysis of feature models: challenges ahead},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183264},
doi = {10.1145/1183236.1183264},
journal = {Commun. ACM},
month = dec,
pages = {45–47},
numpages = {3}
}

@inproceedings{10.1145/1865875.1865876,
author = {Azevedo, Sofia and Machado, Ricardo J. and Bragan\c{c}a, Alexandre and Ribeiro, Hugo},
title = {Support for variability in use case modeling with refinement},
year = {2010},
isbn = {9781450301237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1865875.1865876},
doi = {10.1145/1865875.1865876},
abstract = {The development of software product lines with model-driven approaches involves dealing with diverse modeling artifacts such as use case diagrams, component diagrams, class diagrams, activity diagrams, sequence diagrams and others. In this paper we focus on use cases for product line development and we analyze them from the perspective of variability. In that context we explore the UML (Unified Modeling Language) «extend» relationship. We also explore the functional refinement of use cases with «extend» relationships between them. This work allows understanding the activities of use case modeling with support for variability and of use case modeling with functional refinement when variability is present.},
booktitle = {Proceedings of the 7th International Workshop on Model-Based Methodologies for Pervasive and Embedded Software},
pages = {1–8},
numpages = {8},
keywords = {«extend», alternative, option, refinement, software product line, specialization, use case, variability},
location = {Antwerpen, Belgium},
series = {MOMPES '10}
}

@article{10.1016/j.future.2015.05.017,
title = {Allocating resources for customizable multi-tenant applications in clouds using dynamic feature placement},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.05.017},
doi = {10.1016/j.future.2015.05.017},
abstract = {Multi-tenancy, where multiple end users make use of the same application instance, is often used in clouds to reduce hosting costs. A disadvantage of multi-tenancy is however that it makes it difficult to create customizable applications, as all end users use the same application instance. In this article, we describe an approach for the development and management of highly customizable multi-tenant cloud applications. We apply software product line engineering techniques to cloud applications, and use an approach where applications are composed of multiple interacting components, referred to as application features. Using this approach, multiple features can be shared between different applications. Allocating resources for these feature-based applications is complex, as relations between components must be taken into account, and is referred to as the feature placement problem.In this article, we describe dynamic feature placement algorithms that minimize migrations between subsequent invocations, and evaluate them in dynamic scenarios where applications are added and removed throughout the evaluation scenario. We find that the developed algorithm achieves a low cost, while resulting in few resource migrations. In our evaluations, we observe that adding migration-awareness to the management algorithms reduces the number of instance migrations by more than 77 % and reduces the load moved between instances by more than 96 % when compared to a static management approach. Despite this reduction in number of migrations, a cost that is on average less than 3 % more than the optimal cost is achieved. We model customizable SaaS applications using feature modeling.A dynamic, migration-aware management approach is presented.Two ILP-based algorithms and a heuristic algorithm are compared.The dynamic algorithms reduce migrations and remain within 3% of the optimal cost.},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {63–76},
numpages = {14}
}

@article{10.1007/s10515-019-00253-7,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Change impact analysis for maintenance and evolution of variable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00253-7},
doi = {10.1007/s10515-019-00253-7},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This article presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining the possibly impacted source code elements when changing the source code of a product family. The approach also supports engineers, who are adapting the code of specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanisms, it is more precise than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. We report evaluation results on the benefit and performance of the approach using industrial product lines.},
journal = {Automated Software Engg.},
month = jun,
pages = {417–461},
numpages = {45},
keywords = {Change impact analysis, Maintenance, Program analysis, Variability}
}

@article{10.1016/j.ijar.2021.07.015,
author = {Bodewes, Tjebbe and Scutari, Marco},
title = {Learning Bayesian networks from incomplete data with the node-average likelihood},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {138},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.07.015},
doi = {10.1016/j.ijar.2021.07.015},
journal = {Int. J. Approx. Reasoning},
month = nov,
pages = {145–160},
numpages = {16},
keywords = {Bayesian networks, Score-based structure learning, Incomplete data}
}

@article{10.1007/s10489-018-01399-9,
author = {Abolpour Mofrad, Asieh and Yazidi, Anis and Lewi Hammer, Hugo},
title = {On solving the SPL problem using the concept of probability flux},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-01399-9},
doi = {10.1007/s10489-018-01399-9},
abstract = {The Stochastic Point Location (SPL) problem Oommen is a fundamental learning problem that has recently found a lot of research attention. SPL can be summarized as searching for an unknown point in an interval under faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic Environment which in turn informs it about the direction of the search. Since the Environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem, which was pioneered two decades ago by Oommen, relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution, i.e., infinite memory. However, this strategy yields rather poor accuracy for low discretization resolutions. In this paper, we present two major contributions to the SPL problem. First, we demonstrate that the estimation of the point location can significantly be improved by resorting to the concept of mutual probability flux between neighboring states along the line. Second, we are able to accurately track the position of the optimal point and simultaneously show a method by which we can estimate the error probability characterizing the Environment. Interestingly, learning this error probability of the Environment takes place in tandem with the unknown location estimation. We present and analyze several experiments discussing the weaknesses and strengths of the different methods.},
journal = {Applied Intelligence},
month = jul,
pages = {2699–2722},
numpages = {24},
keywords = {Estimating environment effectiveness, Flux-based Estimation Solution (FES), Last Transition-based Estimation Solution (LTES), Mutual probability flux, Stochastic Learning Weak Estimation (SLWE), Stochastic Point Location (SPL)}
}

@article{10.5555/3122009.3176858,
author = {Fenn, Shannon and Moscato, Pablo},
title = {Target curricula via selection of minimum feature sets: a case study in Boolean networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4070–4095},
numpages = {26},
keywords = {Boolean betworks, k-feature Set, multi-label classification, target curriculum}
}

@article{10.1016/j.is.2012.11.010,
author = {Gr\"{o}Ner, Gerd and Bo\v{s}Kovi\'{c}, Marko and Silva Parreiras, Fernando and Ga\v{s}Evi\'{c}, Dragan},
title = {Modeling and validation of business process families},
year = {2013},
issue_date = {July, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {38},
number = {5},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2012.11.010},
doi = {10.1016/j.is.2012.11.010},
abstract = {Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline-software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach.},
journal = {Inf. Syst.},
month = jul,
pages = {709–726},
numpages = {18},
keywords = {Business process families, Control flow relations, Process model configuration, Process model variability, Validation}
}

@article{10.1016/j.ins.2021.06.013,
author = {Yang, Guoli and Kang, Yuanji and Zhu, Xianqiang and Zhu, Cheng and Xiao, Gaoxi},
title = {Info2vec: An aggregative representation method in multi-layer and heterogeneous networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.013},
doi = {10.1016/j.ins.2021.06.013},
journal = {Inf. Sci.},
month = oct,
pages = {444–460},
numpages = {17},
keywords = {Multi-layer networks, Representation learning, Cyberspace}
}

@phdthesis{10.5555/AAI28713793,
author = {Palaparthi, Anil Kumar Reddy and A., Weiss, Jeffrey and D., Rabbitt, Richard and II, Dorval, Alan D., and M, Barkmeier-Kraemer, Julie},
advisor = {R, Titze, Ingo},
title = {Computational Motor Learning and Control of the Vocal Source for Voice Production},
year = {2021},
isbn = {9798780644439},
publisher = {The University of Utah},
abstract = {Voice production is a motor skill and requires the coordinated function of many brain regions, namely the brainstem, cerebellum, basal ganglia, diencephalon, and cerebral hemispheres. The vocal system can be subdivided into three major components: lungs, larynx, and vocal tract. Lung pressure drives the airflow in the trachea towards the larynx. The airflow causes the vocal folds in the larynx (vocal source) to oscillate under certain prephonatory conditions, generating audible pulses of airflow into the vocal tract. The vocal tract filters these pulses and radiates the sound into the air. The intrinsic laryngeal muscles play a significant role in voice production. They position the glottis, the space between the vocal folds in several pre-phonatory positions that facilitate vocal fold vibration. The resulting glottal flow is the vocal source. This project aims to develop a control system that controls the vocal source based on four acoustic and four somatosensory features. Nonlinear control theory and artificial neural networks were used to develop the controllers. A voice simulator with a biomechanical model of the vocal system, LeTalker, was used to model the voice production mechanism. In Aim 1, interrelationships between the intrinsic laryngeal muscles and lung pressure in producing various acoustic and somatosensory features during phonation were obtained. In Aim 2, feedforward and feedback controllers based on acoustic and somatosensory features to control the vocal source were developed. In Aim 3, the controllers' sensitivity and performance were assessed using perturbation analysis. The results demonstrated that the control system was able to generate the lung pressure and muscle activations such that the four acoustic and four somatosensory targets were reached with high accuracy. It was observed that for most of the test cases, the control system produces lung pressure and muscle activations that result in phonation that is within ±15 Hz of the targeted fo and ±2 dB of the targeted SPL. The three studies conducted in this dissertation can be a stepping stone to simulate and study various motor disorders related to voice.},
note = {AAI28713793}
}

@inproceedings{10.5555/3172077.3172239,
author = {Murugesan, Keerthiram and Carbonell, aime},
title = {Self-paced multitask learning with shared knowledge},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {This paper introduces self-paced task selection to multitask learning, where instances from more closely related tasks are selected in a progression of easier-to-harder tasks, to emulate an effective human education strategy, but applied to multitask machine learning. We develop the mathematical foundation for the approach based on iterative selection of the most appropriate task, learning the task parameters, and updating the shared knowledge, optimizing a new bi-convex loss function. This proposed method applies quite generally, including to multitask feature learning, multitask learning with alternating structure optimization, etc. Results show that in each of the above formulations self-paced (easier-to-harder) task selection outperforms the baseline version of these methods in all the experiments.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2522–2528},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {gentic algorithms, media arts, software product lines},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3485114.3485122,
author = {Huang, Yijiang and Leung, Pok Yin Victor and Garrett, Caelan and Gramazio, Fabio and Kohler, Matthias and Mueller, Caitlin},
title = {The new analog: A protocol for linking design and construction intent with algorithmic planning for robotic assembly of complex structures},
year = {2021},
isbn = {9781450390903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485114.3485122},
doi = {10.1145/3485114.3485122},
abstract = {Construction robotics are increasingly popular in the architectural fabrication community due to their accuracy and flexibility. Because of their high degree of motion freedom, these tools are able to assemble complex structures with irregular designs, which advances architectural aesthetics and structural performance. However, automated task and motion planning (TAMP) for a robot to assemble non-repetitive objects can be challenging due to (1) a non-repetitive assembly pattern (2) the need for a continuous robotic motion throughout a sequence of movement (3) a congested construction scene and (4) occasional robot configuration constraints due to taught positions. Recent work has already begun to address these challenges for repetitive assembly processes, where the robot repeats a pattern of primitive behaviors (e.g. brick stacking or spatial extrusion). Yet, there are many assembly processes that can benefit from a non-repetitive pattern. For example, processes can change tools on an element-by-element level to accommodate a wider range of geometry. Our work is motivated by the necessity of robotic modeling and planning for a recently published timber assembly process which utilizes distributed robotic clamps to press together interlocking joints. In addition to pick-and-place operations, the robot needs to move numerous tools within the construction scene, similar to a tool-change operation. In order to facilitate an agile process for architectural design, construction process design, and TAMP, we introduce a flowchart-based specification language which allows various designers to describe their design and construction intent and knowledge. A compiler can then translate the assembly description, sequence, process flowchart, and robotic setup into a plan skeleton. Additionally, we present a linear and a non-linear solving algorithm that can solve the plan skeleton for a full sequence of robot motions. This algorithm can be customized to take into account designer intuition, which can speed up the planning process. We provide a comparison of the two algorithms using the timber assembly process as our case study. We validate our results by robotically executing and constructing a large-scale real-world timber structure. Finally, we demonstrate the flexibility of our flowchart by showing how custom assembly actions are modeled in our case study. We also demonstrate how other recently published robotic assembly processes can be formulated using our flowcharts to demonstrate generalizability.},
booktitle = {Proceedings of the 6th Annual ACM Symposium on Computational Fabrication},
articleno = {9},
numpages = {17},
keywords = {Digital Fabrication, Distributed Robotic Tools, Robotic Assembly, Spatial Timber Structure, Task and Motion Planning},
location = {Virtual Event, USA},
series = {SCF '21}
}

@inproceedings{10.1007/978-3-030-58604-1_7,
author = {Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
title = {Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments},
year = {2020},
isbn = {978-3-030-58603-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58604-1_7},
doi = {10.1007/978-3-030-58604-1_7},
abstract = {We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some transfer, we find significantly lower absolute performance in the continuous setting – suggesting that performance in prior ‘navigation-graph’ settings may be inflated by the strong implicit assumptions. Code at 
.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII},
pages = {104–120},
numpages = {17},
keywords = {Vision-and-Language Navigation, Embodied agents},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1007/978-3-030-98682-7_5,
author = {Fernandes, Roberto and Rodrigues, Walber M. and Barros, Edna},
title = {Dataset and&nbsp;Benchmarking of&nbsp;Real-Time Embedded Object Detection for&nbsp;RoboCup SSL},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_5},
doi = {10.1007/978-3-030-98682-7_5},
abstract = {When producing a model to object detection in a specific context, the first obstacle is to have a dataset labeling the desired classes. In RoboCup, some leagues already have more than one dataset to train and evaluate a model. However, in the Small Size League (SSL), there is not such dataset available yet. This paper presents an open-source dataset to be used as a benchmark for real-time object detection in SSL. This work also presented a pipeline to train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a low-power embedded system. This pipeline is used to evaluate the proposed dataset with state-of-art optimized models. In this dataset, the MobileNet SSD v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS), while running on an SSL robot.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {53–64},
numpages = {12},
keywords = {Dataset, Benchmark, Deep learning, Object detection},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1109/IROS51168.2021.9636743,
author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv},
title = {Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636743},
doi = {10.1109/IROS51168.2021.9636743},
abstract = {We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {1562–1569},
numpages = {8},
location = {Prague, Czech Republic}
}

@article{10.1007/s10472-019-09618-w,
author = {Gerevini, Alfonso Emilio and Schubert, Lenhart},
title = {Discovering state constraints for planning with conditional effects in Discoplan (part I)},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {88},
number = {5–6},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-019-09618-w},
doi = {10.1007/s10472-019-09618-w},
abstract = {Discoplan is a durable and efficient system for inferring state constraints (invariants) in planning domains, specified in the PDDL language. It is exceptional in the range of constraint types it can discover and verify, and it directly allows for conditional effects in action operators. However, although various aspects of Discoplan have been previously described and its utility in planning demonstrated, the underlying methodology, the algorithms for the discovery and inductive verification of constraints, and the proofs of correctness of the algorithms and their complexity analysis have never been laid out in adequate detail. The purpose of this paper is to remedy these lacunae.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = jun,
pages = {641–686},
numpages = {46},
keywords = {Automated planning, Inference of state constraints for planning, Planning domain analysis, State invariants in planning, Planning with conditional effects, Knowledge discovery for planning, Automatic inductive proofs, 68T01, 68T27, 68T35, 68T30, 68T20}
}

@article{10.1016/j.specom.2021.06.001,
author = {Akbarzadeh, Sara and Lee, Sungmin and Chen, Fei and Tan, Chin-Tuan},
title = {The effect of speech and noise levels on the quality perceived by cochlear implant and normal hearing listeners},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.06.001},
doi = {10.1016/j.specom.2021.06.001},
journal = {Speech Commun.},
month = sep,
pages = {106–113},
numpages = {8},
keywords = {Cochlear implant, Speech level, Noise level, Sound quality perception, NH, CI, SPL, NR, SNR}
}

@article{10.1016/j.artmed.2006.06.003,
author = {Botros, Andrew and van Dijk, Bas and Killian, Matthijs},
title = {AutoNRTTM: An automated system that measures ECAP thresholds with the Nucleus® FreedomTM cochlear implant via machine intelligence},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {40},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2006.06.003},
doi = {10.1016/j.artmed.2006.06.003},
abstract = {Objective: AutoNRT(TM) is an automated system that measures electrically evoked compound action potential (ECAP) thresholds from the auditory nerve with the Nucleus^(R) Freedom(TM) cochlear implant. ECAP thresholds along the electrode array are useful in objectively fitting cochlear implant systems for individual use. This paper provides the first detailed description of the AutoNRT algorithm and its expert systems, and reports the clinical success of AutoNRT to date. Methods: AutoNRT determines thresholds by visual detection, using two decision tree expert systems that automatically recognise ECAPs. The expert systems are guided by a dataset of 5393 neural response measurements. The algorithm approaches threshold from lower stimulus levels, ensuring recipient safety during postoperative measurements. Intraoperative measurements use the same algorithm but proceed faster by beginning at stimulus levels much closer to threshold. When searching for ECAPs, AutoNRT uses a highly specific expert system (specificity of 99% during training, 96% during testing; sensitivity of 91% during training, 89% during testing). Once ECAPs are established, AutoNRT uses an unbiased expert system to determine an accurate threshold. Throughout the execution of the algorithm, recording parameters (such as implant amplifier gain) are automatically optimised when needed. Results: In a study that included 29 intraoperative and 29 postoperative subjects (a total of 418 electrodes), AutoNRT determined a threshold in 93% of cases where a human expert also determined a threshold. When compared to the median threshold of multiple human observers on 77 randomly selected electrodes, AutoNRT performed as accurately as the 'average' clinician. Conclusions: AutoNRT has demonstrated a high success rate and a level of performance that is comparable with human experts. It has been used in many clinics worldwide throughout the clinical trial and commercial launch of Nucleus Custom Sound(TM) Suite, significantly streamlining the clinical procedures associated with cochlear implant use.},
journal = {Artif. Intell. Med.},
month = may,
pages = {15–28},
numpages = {14},
keywords = {Automated systems, Cochlear implants, Decision trees, Electrically evoked compound action potential, Machine learning, Neural response telemetry, Pattern recognition, Threshold estimation}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {Deep learning, active learning, deep active learning}
}

@article{10.1504/IJAISC.2013.056838,
author = {Paulraj, M. P. and Andrew, Allan Melvin},
title = {Classification of interior noise comfort level of Proton model cars using feedforward neural network},
year = {2013},
issue_date = {September 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {3},
number = {4},
issn = {1755-4950},
url = {https://doi.org/10.1504/IJAISC.2013.056838},
doi = {10.1504/IJAISC.2013.056838},
abstract = {In this research, a Proton model cars noise comfort level classification system has been developed to detect the noise comfort level in cars using artificial neural network. This research focuses on developing a database consisting of car sound samples measured from different Proton make models in stationary and moving state. In the stationary condition, the sound pressure level is measured at 1,300 RPM, 2,000 RPM and 3,000 RPM while in moving condition, the sound is recorded using dB Orchestra while the car is moving at constant speed from 30 km/h up to 110 km/h. Subjective test is conducted to find the jury's evaluation for the specific sound sample. The feature set is then feed to the neural network model to classify the comfort level. The spectral power feature gives the highest classification accuracy of 88.42%.},
journal = {Int. J. Artif. Intell. Soft Comput.},
month = sep,
pages = {344–359},
numpages = {16}
}

@inproceedings{10.1007/978-3-319-47157-0_11,
author = {Wang, Yan and Wu, Xi and Ma, Guangkai and Ma, Zongqing and Fu, Ying and Zhou, Jiliu},
title = {Patch-Based Hippocampus Segmentation Using a Local Subspace Learning Method},
year = {2016},
isbn = {978-3-319-47156-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47157-0_11},
doi = {10.1007/978-3-319-47157-0_11},
abstract = {Patch-based segmentation methods utilizing multiple atlases have been widely studied for alleviating some misalignments when registering atlases to the target image. However, weights assigned to the fused labels are typically computed based on predefined features (e.g. simple patch intensities), thus being not necessarily optimal. Due to lack of discriminating features for different regions of an anatomical structure, the original feature space defined by image intensities may limit the segmentation accuracy. To address these problems, we propose a novel local subspace learning based patch-wise label propagation method to estimate a voxel-wise segmentation of the target image. Specifically, multi-scale patch intensities and texture features are first extracted from the image patch in order to acquire the abundant appearance information. Then, margin fisher analysis (MFA) is applied to neighboring samples of each voxel to be segmented from the aligned atlases, in order to extract discriminant features. This process can enhance discrimination of features for different local regions in the anatomical structure. Finally, based on extracted discriminant features, the k-nearest neighbor (kNN) classifier is used to determine the final label for the target voxel. Moreover, for the patch-wise label propagation, we first translate label patches into several discrete class labels by using the k-means clustering method, and then apply MFA to ensure that samples with similar label patches achieve a higher similarity and those with dissimilar label patches achieve a lower similarity. To demonstrate segmentation performance, we comprehensively evaluated the proposed method on the ADNI dataset for hippocampus segmentation. Experimental results show that the proposed method outperforms several conventional multi-atlas based segmentation methods.},
booktitle = {Machine Learning in Medical Imaging: 7th International Workshop, MLMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 17, 2016, Proceedings},
pages = {86–94},
numpages = {9},
keywords = {Target Image, Image Patch, Label Propagation, Spatial Neighborhood, Deformable Image Registration},
location = {Athens, Greece}
}

@article{10.1007/s10845-016-1240-z,
author = {Jiao, Roger J. and Zhou, Feng and Chu, Chih-Hsing},
title = {Decision theoretic modeling of affective and cognitive needs for product experience engineering: key issues and a conceptual framework},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {7},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-016-1240-z},
doi = {10.1007/s10845-016-1240-z},
abstract = {User experience (UX) design plays a critical role in product experience engineering. To create a theoretical foundation of UX design, it is imperative to develop mathematical and computational models for elicitation, quantification, evaluation and reasoning of affective---cognitive needs that are inherent in the fulfillment of user experience. This paper explores the key research issues for understanding how human users' subjective experience and affective prediction impact their choice behavior under uncertainty. A conceptual framework is envisioned by extending prospect theory in the field of behavioral economics to the modeling of user experience choice behavior, in which inference of affective influence is enacted through the shape parameters of prospect value functions.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1755–1767},
numpages = {13},
keywords = {Affective design, Cognitive engineering, Product design, User experience, User modeling}
}

@article{10.1016/j.pmcj.2019.02.001,
author = {Lee, Hyungu and Hwang, Jung Yeon and Lee, Shincheol and Kim, Dong In and Lee, Sung-Hoon and Lee, Jaehwan and Shin, Ji Sun},
title = {A parameterized model to select discriminating features on keystroke dynamics authentication on smartphones},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1574-1192},
url = {https://doi.org/10.1016/j.pmcj.2019.02.001},
doi = {10.1016/j.pmcj.2019.02.001},
journal = {Pervasive Mob. Comput.},
month = mar,
pages = {45–57},
numpages = {13},
keywords = {Keystroke dynamics authentication, Edge devices, Smartphones, IoT, Machine learning}
}

@inproceedings{10.1109/WI-IAT.2010.25,
author = {Jaroucheh, Zakwan and Liu, Xiaodong and Smith, Sally},
title = {Mapping Features to Context Information: Supporting Context Variability for Context-Aware Pervasive Applications},
year = {2010},
isbn = {9780769541914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2010.25},
doi = {10.1109/WI-IAT.2010.25},
abstract = {Context-aware computing is widely accepted as a promising paradigm to enable seamless computing. Several middlewares and ontology-based models for describing context information have been developed in order to support context-aware applications. However, the context variability, which refers to the possibility to infer or interpret different context information from different perspectives, has been neglected in the existing context modeling approaches. This paper presents an approach for context-aware software development based on a flexible product line based context model which significantly enhances reusability of context information by providing context variability constructs to satisfy different application needs.},
booktitle = {Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {611–614},
numpages = {4},
keywords = {context variability, context-awareness, feature model, pervasive applications, software product line},
series = {WI-IAT '10}
}

@inproceedings{10.1109/MSR.2017.18,
author = {Ghotra, Baljinder and Mcintosh, Shane and Hassan, Ahmed E.},
title = {A large-scale study of the impact of feature selection techniques on defect classification models},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.18},
doi = {10.1109/MSR.2017.18},
abstract = {The performance of a defect classification model depends on the features that are used to train it. Feature redundancy, correlation, and irrelevance can hinder the performance of a classification model. To mitigate this risk, researchers often use feature selection techniques, which transform or select a subset of the features in order to improve the performance of a classification model. Recent studies compare the impact of different feature selection techniques on the performance of defect classification models. However, these studies compare a limited number of classification techniques and have arrived at contradictory conclusions about the impact of feature selection techniques. To address this limitation, we study 30 feature selection techniques (11 filter-based ranking techniques, six filter-based subset techniques, 12 wrapper-based subset techniques, and a no feature selection configuration) and 21 classification techniques when applied to 18 datasets from the NASA and PROMISE corpora. Our results show that a correlation-based filter-subset feature selection technique with a BestFirst search method outperforms other feature selection techniques across the studied datasets (it outperforms in 70%--87% of the PROMISE-NASA data sets) and across the studied classification techniques (it outperforms for 90% of the techniques). Hence, we recommend the application of such a selection technique when building defect classification models.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {146–157},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.1109/TCBB.2007.1014,
author = {Bontempi, Gianluca},
title = {A Blocking Strategy to Improve Gene Selection for Classification of Gene Expression Data},
year = {2007},
issue_date = {April 2007},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {4},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2007.1014},
doi = {10.1109/TCBB.2007.1014},
abstract = {Because of high dimensionality, machine learning algorithms typically rely on feature selection techniques in order to perform effective classification in microarray gene expression data sets. However, the large number of features compared to the number of samples makes the task of feature selection computationally hard and prone to errors. This paper interprets feature selection as a task of stochastic optimization, where the goal is to select among an exponential number of alternative gene subsets the one expected to return the highest generalization in classification. Blocking is an experimental design strategy which produces similar experimental conditions to compare alternative stochastic configurations in order to be confident that observed differences in accuracy are due to actual differences rather than to fluctuations and noise effects. We propose an original blocking strategy for improving feature selection which aggregates in a paired way the validation outcomes of several learning algorithms to assess a gene subset and compare it to others. This is a novelty with respect to conventional wrappers, which commonly adopt a sole learning algorithm to evaluate the relevance of a given set of variables. The rationale of the approach is that, by increasing the amount of experimental conditions under which we validate a feature subset, we can lessen the problems related to the scarcity of samples and consequently come up with a better selection. The paper shows that the blocking strategy significantly improves the performance of a conventional forward selection for a set of 16 publicly available cancer expression data sets. The experiments involve six different classifiers and show that improvements take place independent of the classification algorithm used after the selection step. Two further validations based on available biological annotation support the claim that blocking strategies in feature selection may improve the accuracy and the quality of the solution. The first validation is based on retrieving PubMEd abstracts associated to the selected genes and matching them to regular expressions describing the biological phenomenon underlying the expression data sets. The biological validation that follows is based on the use of the Bioconductor package GoStats in order to perform Gene Ontology statistical analysis.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = apr,
pages = {293–300},
numpages = {8},
keywords = {Bioinformatics (genome or protein) databases, data mining, feature evaluation and selection., machine learning}
}

@inproceedings{10.1007/978-3-030-29551-6_44,
author = {Lin, Jiping and Zhou, Yu and Kang, Junhao},
title = {Low-Sampling Imagery Data Recovery by Deep Learning Inference and Iterative Approach},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_44},
doi = {10.1007/978-3-030-29551-6_44},
abstract = {Block-based compressed sensing (CS) recovery aims to reconstruct the high quality image from only a small number of observations in a block-wise manner. However, when the sampling rate is very low and the existence of additive noise, there are usually some block artifacts and detail blurs which degrades the reconstructed quality. In this paper, we propose an efficient method which takes both the advantages of deep learning (DL) framework and iterative approaches. First, a deep multi-layer perceptron (DMLP) is constructed to obtain the initial reconstructed image. Then, an efficient iterative approach is applied to keep the consistence and smoothness between the adjacent blocks. The proposed method demonstrates its efficacy on benchmark datasets.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {488–493},
numpages = {6},
keywords = {Compressed sensing, Deep learning, Iterative approach},
location = {Athens, Greece}
}

@article{10.1007/s11263-007-0095-3,
author = {Leibe, Bastian and Leonardis, Ale\v{s} and Schiele, Bernt},
title = {Robust Object Detection with Interleaved Categorization and Segmentation},
year = {2008},
issue_date = {May       2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {1–3},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-007-0095-3},
doi = {10.1007/s11263-007-0095-3},
abstract = {This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance.

The core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to focus its efforts on object pixels and to discard misleading influences from the background. Moreover, the information from where in the image a hypothesis draws its support is employed in an MDL based hypothesis verification stage to resolve ambiguities between overlapping hypotheses and factor out the effects of partial occlusion.

An extensive evaluation on several large data sets shows that the proposed system is applicable to a range of different object categories, including both rigid and articulated objects. In addition, its flexible representation allows it to achieve competitive object detection performance already from training sets that are between one and two orders of magnitude smaller than those used in comparable systems.},
journal = {Int. J. Comput. Vision},
month = may,
pages = {259–289},
numpages = {31},
keywords = {Clustering, Hough transform, Hypothesis selection, MDL, Object categorization, Object detection, Segmentation}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {Silent stores, code optimization, machine learning, nonvolatile memory, static analysis}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Numerical variability model, Feature, Non-functional requirement, Quality attribute, Category theory},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1007/978-3-030-26142-9_9,
author = {Wang, Yunyun and Zhao, Dan and Li, Yun and Chen, Kejia and Xue, Hui},
title = {The Most Related Knowledge First: A Progressive Domain Adaptation Method},
year = {2019},
isbn = {978-3-030-26141-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26142-9_9},
doi = {10.1007/978-3-030-26142-9_9},
abstract = {In domain adaptation, how to select and transfer related knowledge is critical for learning. Inspired by the fact that human usually transfer from the more related experience to the less related one, in this paper, we propose a novel progressive domain adaptation (PDA) model, which attempts to transfer source knowledge by considering the transfer order based on relevance. Specifically, PDA transfers source instances iteratively from the most related ones to the least related ones, until all related source instances have been adopted. It is an iterative learning process, source instances adopted in each iteration are determined by a gradually annealed weight such that the later iteration will introduce more source instances. Further, a reverse classification performance is used to set the termination of iteration. Experiments on real datasets demonstrate the competiveness of PDA compared with the state-of-arts.},
booktitle = {Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2019 Workshops, BDM, DLKT, LDRC, PAISI, WeL, Macau, China, April 14–17, 2019, Revised Selected Papers},
pages = {90–102},
numpages = {13},
keywords = {Domain adaptation, Progressive transfer, Iteration, Reverse classification},
location = {Macau, China}
}

@inproceedings{10.5555/3299905.3299978,
author = {Safavi, Saeid and Wang, Wenwu and Plumbley, Mark and Choobbasti, Ali Janalizadeh and Fazekas, George},
title = {Predicting the Perceived Level of Reverberation using Features from Nonlinear Auditory Model},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Perceptual measurements have typically been recognized as the most reliable measurements in assessing perceived levels of reverberation. In this paper, a combination of blind RT60 estimation method and a binaural, nonlinear auditory model is employed to derive signal-based measures (features) that are then utilized in predicting the perceived level of rever- beration. Such measures lack the excess of effort necessary for calculating perceptual measures; not to mention the variations in either stimuli or assessors that may cause such measures to be statistically insigni?cant. As a result, the automatic extraction of objective measurements that can be applied to predict the perceived level of reverberation become of vital signi?cance. Consequently, this work is aimed at discovering measurements such as clarity, reverberance, and RT60 which can automatically be derived directly from audio data. These measurements along with labels from human listening tests are then forwarded to a machine learning system seeking to build a model to estimate the perceived level of reverberation, which is labeled by an expert, autonomously. The data has been labeled by an expert human listener for a unilateral set of ?les from arbitrary audio source types. By examining the results, it can be observed that the automatically extracted features can aid in estimating the perceptual rates.},
booktitle = {Proceedings of the 23rd Conference of Open Innovations Association FRUCT},
articleno = {73},
numpages = {5},
keywords = {Audio signal processing, Feature extraction, Human experiments, Reverberation, machine learning},
location = {Bologna, Italy},
series = {FRUCT'23}
}

@article{10.5555/3122009.3242055,
author = {Patrascu, Andrei and Necoara, Ion},
title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k1/2), where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7204–7245},
numpages = {42},
keywords = {intersection of convex constraints, nonasymptotic convergence analysis, rates of convergence, stochastic convex optimization, stochastic proximal point}
}

@inproceedings{10.1007/978-3-030-98682-7_9,
author = {Antonioni, Emanuele and Riccio, Francesco and Nardi, Daniele},
title = {Improving Sample Efficiency in&nbsp;Behavior Learning by&nbsp;Using Sub-optimal Planners for&nbsp;Robots},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_9},
doi = {10.1007/978-3-030-98682-7_9},
abstract = {The design and implementation of behaviors for robots operating in dynamic and complex environments are becoming mandatory in nowadays applications. Reinforcement learning is consistently showing remarkable results in learning effective action policies and in achieving super-human performance in various tasks – without exploiting prior knowledge. However, in robotics, the use of purely learning-based techniques is still subject to strong limitations. Foremost, sample efficiency. Such techniques, in fact, are known to require large training datasets, and long training sessions, in order to develop effective action policies. Hence in this paper, to alleviate such constraint, and to allow learning in such robotic scenarios, we introduce SErP (Sample Efficient robot Policies), an iterative algorithm to improve the sample-efficiency of learning algorithms. SErP exploits a sub-optimal planner (here implemented with a monitor-replanning algorithm) to lead the exploration of the learning agent through its initial iterations. Intuitively, SErP exploits the planner as an expert in order to enable focused exploration and to avoid portions of the search space that are not effective to solve the task of the robot. Finally, to confirm our insights and to show the improvements that SErP carries with, we report the results obtained in two different robotic scenarios: (1) a cartpole scenario and (2) a soccer-robots scenario within the RoboCup@Soccer SPL environment.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {103–114},
numpages = {12},
keywords = {Automated planning, Reinforcement learning, Decision-making},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.eswa.2014.04.046,
author = {Chin, Kwai-Sang and Fu, Chao},
title = {Integrated evidential reasoning approach in the presence of cardinal and ordinal preferences and its applications in software selection},
year = {2014},
issue_date = {November, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {15},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.04.046},
doi = {10.1016/j.eswa.2014.04.046},
abstract = {A combination of cardinal and ordinal preferences in multiple-attribute decision making (MADM) demonstrates more reliability and flexibility compared with sole cardinal or ordinal preferences derived from a decision maker. This situation occurs particularly when the knowledge and experience of the decision maker, as well as the data regarding specific alternatives on certain attributes, are insufficient or incomplete. This paper proposes an integrated evidential reasoning (IER) approach to analyze uncertain MADM problems in the presence of cardinal and ordinal preferences. The decision maker provides complete or incomplete cardinal and ordinal preferences of each alternative on each attribute. Ordinal preferences are expressed as unknown distributed assessment vectors and integrated with cardinal preferences to form aggregated preferences of alternatives. Three optimization models considering cardinal and ordinal preferences are constructed to determine the minimum and maximum minimal satisfaction of alternatives, simultaneous maximum minimal satisfaction of alternatives, and simultaneous minimum minimal satisfaction of alternatives. The minimax regret rule, the maximax rule, and the maximin rule are employed respectively in the three models to generate three kinds of value functions of alternatives, which are aggregated to find solutions. The attribute weights in the three models can be precise or imprecise (i.e., characterized by six types of constraints). The IER approach is used to select the optimum software for product lifecycle management of a famous Chinese automobile manufacturing enterprise.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6718–6727},
numpages = {10},
keywords = {Cardinal and ordinal preferences, Decision analysis, Evidential reasoning approach, Integrated decision, Multiple-attribute decision making}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {defect prediction, features, software product lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.dss.2012.10.005,
author = {Van Valkenhoef, Gert and Tervonen, Tommi and Zwinkels, Tijs and De Brock, Bert and Hillege, Hans},
title = {ADDIS: A decision support system for evidence-based medicine},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {2},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.10.005},
doi = {10.1016/j.dss.2012.10.005},
abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results.},
journal = {Decis. Support Syst.},
month = may,
pages = {459–475},
numpages = {17},
keywords = {ADE, ADR, ADaM, AMIA, ANSI, ATC, BRIDG, CDASH, CDISC, CDMS, CHMP, CPOE, CRF, CRO, CTIS, CTMS, Clinical trial, DB, DED, DIS, DOI, DSS, Data model, Decision analysis, EAV, EBM, EDC, EHR, EMA, EPAR, Evidence synthesis, Evidence-based medicine, FDA, FDAAA, GCP, GUI, HL7, HSDB, ICD, ICMJE, ICTRP, JAMA, LAB, MCDA, MeSH, MedDRA, NCI, NDA, NIHUS, OBX, OCRe, ODM, OWL, PIM, PMDA, PRM, PhRMA, QRD, RIM, SDTM, SEND, SMAA, SNOMEDCT, SPL, SmPC, TDM, UMLS, WHO, caBIG, eCRF, eLab, ePRO}
}

@inproceedings{10.1145/3167132.3167162,
author = {Hielscher, Tommy and V\"{o}lzke, Henry and Papapetrou, Panagiotis and Spiliopoulou, Myra},
title = {Discovering, selecting and exploiting feature sequence records of study participants for the classification of epidemiological data on hepatic steatosis},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167162},
doi = {10.1145/3167132.3167162},
abstract = {In longitudinal epidemiological studies, participants undergo repeated medical examinations and are thus represented by a potentially large number of short examination outcome sequences. Some of those sequences may contain important information in various forms, such as patterns, with respect to the disease under study, while others may be on features of little relevance to the outcome. In this work, we propose a framework for Discovery, Selection and Exploitation (DiSelEx) of longitudinal epidemiological data, aiming to identify informative patterns among these sequences. DiSelEx combines sequence clustering with supervised learning to identify sequence groups that contribute to class separation. Newly derived and old features are evaluated and selected according to their redundancy and informativeness regarding the target variable. The selected feature set is then used to learn a classification model on the study data. We evaluate DiSelEx on cohort participants for the disorder "hepatic steatosis" and report on the impact on predictive performance when using sequential data in comparison to utilizing only the basic classifier.1},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {6–13},
numpages = {8},
keywords = {classification, epidemiological studies, feature selection, hepatic steatosis, medical data mining, patient similarity, time-series clustering},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@inproceedings{10.1109/ASE.2019.00120,
author = {Reuling, Dennis and Kelter, Udo and Ruland, Sebastian and Lochau, Malte},
title = {SiMPOSE: configurable N-way program merging strategies for superimposition-based analysis of variant-rich software},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00120},
doi = {10.1109/ASE.2019.00120},
abstract = {Modern software often exists in many different, yet similar versions and/or variants, usually derived from a common code base (e.g., via clone-and-own). In the context of product-line engineering, family-based analysis has shown very promising potential for improving efficiency in applying quality-assurance techniques to variant-rich software, as compared to a variant-by-variant approach. Unfortunately, these strategies rely on a product-line representation superimposing all program variants in a syntactically well-formed, semantically sound and variant-preserving manner, which is manually hard to obtain in practice. We demonstrate the SiMPOSE methodology for automatically generating superimpositions of N given program versions and/or variants facilitating family-based analysis of variant-rich software. SiMPOSE is based on a novel N-way model-merging technique operating at the level of control-flow automata (CFA) representations of C programs. CFAs constitute a unified program abstraction utilized by many recent software-analysis tools. We illustrate different merging strategies supported by SiMPOSE, namely variant-by-variant, N-way merging, incremental 2-way merging, and partition-based N/2-way merging, and demonstrate how SiMPOSE can be used to systematically compare their impact on efficiency and effectiveness of family-based unit-test generation. The SiMPOSE tool, the demonstration of its usage as well as related artifacts and documentation can be found at http://pi.informatik.uni-siegen.de/projects/variance/simpose.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1134–1137},
numpages = {4},
keywords = {family-based analyses, model merging, program merging, software testing},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1016/j.datak.2010.01.002,
author = {Reinhartz-Berger, Iris},
title = {Towards automatization of domain modeling},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {5},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2010.01.002},
doi = {10.1016/j.datak.2010.01.002},
abstract = {A domain model, which captures the common knowledge and the possible variability allowed among applications in a domain, may assist in the creation of other valid applications in that domain. However, to create such domain models is not a trivial task: it requires expertise in the domain, reaching a very high level of abstraction, and providing flexible, yet formal, artifacts. In this paper an approach, called Semi-automated Domain Modeling (SDM), to create draft domain models from applications in those domains, is presented. SDM takes a repository of application models in a domain and matches, merges, and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains. The similarity of the different elements is measured, with consideration of syntactic, semantic, and structural aspects. Unlike ontology and schema integration, these models capture both structural and behavioral aspects of the domain. Running SDM on small repositories of project management applications and scheduling systems, we found that the approach may provide reasonable draft domain models, whose comprehensibility, correctness, completeness, and consistency levels are satisfactory.},
journal = {Data Knowl. Eng.},
month = may,
pages = {491–515},
numpages = {25},
keywords = {DSL, Domain analysis, Domain engineering, Metamodeling, Product line engineering, UML}
}

@inproceedings{10.1145/3292500.3330990,
author = {Chen, Yu-Chia and Bijral, Avleen S. and Ferres, Juan Lavista},
title = {On Dynamic Network Models and Application to Causal Impact},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330990},
doi = {10.1145/3292500.3330990},
abstract = {Dynamic extensions of Stochastic block model (SBM) are of importance in several fields that generate temporal interaction data. These models, besides producing compact and interpretable network representations, can be useful in applications such as link prediction or network forecasting. In this paper we present a conditional pseudo-likelihood based extension to dynamic SBM that can be efficiently estimated by optimizing a regularized objective. Our formulation leads to a highly scalable approach that can handle very large networks, even with millions of nodes. We also extend our formalism to causal impact for networks that allows us to quantify the impact of external events on a time dependent sequence of networks. We support our work with extensive results on both synthetic and real networks.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1194–1204},
numpages = {11},
keywords = {causal impact, clustering, dynamic networks, stochastic block model},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/3169795,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas and Ruan, Wenjie},
title = {Duplicate Detection in Programming Question Answering Communities},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3169795},
doi = {10.1145/3169795},
abstract = {Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage “ranking-classification” problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {37},
numpages = {21},
keywords = {Community-based question answering, association rules, classification, latent semantics, question quality}
}

@article{10.1016/j.patcog.2019.106972,
author = {Dong, Ganggang and Liu, Hongwei and Kuang, Gangyao and Chanussot, Jocelyn},
title = {Target recognition in SAR images via sparse representation in the frequency domain},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {96},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.106972},
doi = {10.1016/j.patcog.2019.106972},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Sparse representation, Transformed domain, Target recognition}
}

@inproceedings{10.5555/3540261.3542304,
author = {Zhang, Jiangning and Xu, Chao and Li, Jian and Chen, Wenzhou and Wang, Yabiao and Tai, Ying and Chen, Shuo and Wang, Chengjie and Huang, Feiyue and Liu, Yong},
title = {Analogous to evolutionary algorithm: designing a unified sequence model},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2043},
numpages = {15},
series = {NIPS '21}
}

@article{10.1016/j.patrec.2021.06.029,
author = {Chaudhary, Sachin and Dudhane, Akshay and Patil, Prashant W. and Murala, Subrahmanyam and Talbar, Sanjay},
title = {Motion estimation in hazy videos},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.06.029},
doi = {10.1016/j.patrec.2021.06.029},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {130–138},
numpages = {9},
keywords = {41A05, 41A10, 65D05, 65D17, Scene understanding, Motion estimation}
}

@article{10.5555/2598944.2599210,
author = {Maldonado, Sebasti\'{a}n and L\'{o}pez, Julio},
title = {Alternative second-order cone programming formulations for support vector classification},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {268},
issn = {0020-0255},
abstract = {This paper presents two novel second-order cone programming (SOCP) formulations that determine a linear predictor using Support Vector Machines (SVMs). Inspired by the soft-margin SVM formulation, our first approach (@x-SOCP-SVM) proposes a relaxation of the conic constraints via a slack variable, penalizing it in the objective function. The second formulation (r-SOCP-SVM) is based on the LP-SVM formulation principle: the bound of the VC dimension is loosened properly using the l"~-norm, and the margin is directly maximized. The proposed methods have several advantages: The first approach constructs a flexible classifier, extending the benefits of the soft-margin SVM formulation to second-order cones. The second method obtains comparable results to the SOCP-SVM formulation with less computational effort, since one conic restriction is eliminated. Experiments on well-known benchmark datasets from the UCI Repository demonstrate that our approach accomplishes the best classification performance compared to the traditional SOCP-SVM formulation, LP-SVM, and to standard linear SVM.},
journal = {Inf. Sci.},
month = jun,
pages = {328–341},
numpages = {14},
keywords = {Linear programming SVM, Second-order cone programming, Support Vector Machine}
}

@article{10.1007/s00500-014-1364-z,
author = {Baladhandapani, Arunadevi and Nachimuthu, Deepa Subramaniam},
title = {Evolutionary learning of spiking neural networks towards quantification of 3D MRI brain tumor tissues},
year = {2015},
issue_date = {July      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {7},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-014-1364-z},
doi = {10.1007/s00500-014-1364-z},
abstract = {This paper presents a new classification technique for 3D MR images, based on a third-generation network of spiking neurons. Implementation of multi-dimensional co-occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed. The results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue-specific structural patterns and achieve segmentation of tumor part. The spiking network classifier has been validated and tested for various real-time and Harvard benchmark datasets, where appreciable performance in terms of mean square error, accuracy and computational time is obtained. The spiking network employed Izhikevich neurons as nodes in a multi-layered structure. The classifier has been compared with computational power of multi-layer neural networks with sigmoidal neurons. The results on misclassified tumors are analyzed and suggestions for future work are discussed.},
journal = {Soft Comput.},
month = jul,
pages = {1803–1816},
numpages = {14},
keywords = {3D Magnetic resonance imaging, Genetic algorithm, Izhikevich neurons, Multi-dimensional co-occurrence matrices, Spiking neural networks}
}

@article{10.1016/j.artmed.2012.03.004,
author = {Mariam, Mai and Delb, Wolfgang and Schick, Bernhard and Strauss, Daniel J.},
title = {Feasibility of an objective electrophysiological loudness scaling: A kernel-based novelty detection approach},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {55},
number = {3},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2012.03.004},
doi = {10.1016/j.artmed.2012.03.004},
abstract = {Objective: The objective of our research is to structure a foundation for an electrophysiological loudness scaling measurement, in particular to estimate an uncomfortable loudness (UCL) level by using the hybrid wavelet-kernel novelty detection (HWND). Methods and materials: Late auditory evoked potentials (LAEPs) were obtained from 20 normal hearing adults. These LAEPs were stimulated by 4 intensity levels (60 decibel (dB) sound pressure level (SPL), 70dB SPL, 80dB SPL, and 90dB SPL). We have extracted the habituation correlates in LAEPs by using HWND. For this, we employed a lattice structure-based wavelet frame decompositions for feature extraction combined with a kernel-based novelty detector. Results: The group results showed that the habituation correlates degrees, i.e., relative changes within the sweep sequences, were significantly different among 60dB SPL, 70dB SPL, 80dB SPL, and 90dB SPL stimulation level, independently from the intensity related amplitude information in the averaged LAEPs. At these particular intensities, 60% of the subjects show the correlation between the novelty measures and the stimulation levels resembles a loudness scaling function, in reverse. In this paper, we have found a correlation in between the novelty measures and loudness perception as well. We have found that high ranges of loudness levels such as loud, upper level and too loud show generally 4.88% of novelty measures and comfortable ranges of loudness levels, i.e., soft, comfortable but soft, comfortable loud and comfortable but loud are generally have 12.29% of novelty measures. Additionally, we demonstrated that our sweep-to-sweep basis of post processing scheme is reliable for habituation extraction and offers an advantage of reducing experimental time as the proposed scheme need less than 20% of single sweeps in comparison to the amount that are commonly used in arithmetical average for a meaningful result. Conclusions: We assessed the feasibility of habituation correlates for an objective loudness scaling. With respect to this first feasibility study, the presented results are promising when using the described signal processing and machine learning methodology. For the group results, the novelty measures approach is able to discriminate 60dB, 70dB, 80dB and 90dB stimulated sweeps. In addition, a correlation between the novelty measures and the subjective loudness scaling is observed. However, more loudness perception and frequency specific experiments need to be conducted to determine the UCL novelty measures threshold as well as clinically oriented studies are necessary to evaluate whether this approach might be used in the objective hearing instrument fitting procedures.},
journal = {Artif. Intell. Med.},
month = jul,
pages = {185–195},
numpages = {11},
keywords = {Adapted filter banks, Event-related potentials, Habituation, Kernel machines, Loudness scaling, Uncomfortable loudness level}
}

@article{10.1016/j.patcog.2008.09.015,
author = {Liu, Manhua and Jiang, Xudong and Kot, Alex C.},
title = {A multi-prototype clustering algorithm},
year = {2009},
issue_date = {May, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.09.015},
doi = {10.1016/j.patcog.2008.09.015},
abstract = {Clustering is an important unsupervised learning technique widely used to discover the inherent structure of a given data set. Some existing clustering algorithms uses single prototype to represent each cluster, which may not adequately model the clusters of arbitrary shape and size and hence limit the clustering performance on complex data structure. This paper proposes a clustering algorithm to represent one cluster by multiple prototypes. The squared-error clustering is used to produce a number of prototypes to locate the regions of high density because of its low computational cost and yet good performance. A separation measure is proposed to evaluate how well two prototypes are separated. Multiple prototypes with small separations are grouped into a given number of clusters in the agglomerative method. New prototypes are iteratively added to improve the poor cluster separations. As a result, the proposed algorithm can discover the clusters of complex structure with robustness to initial settings. Experimental results on both synthetic and real data sets demonstrate the effectiveness of the proposed clustering algorithm.},
journal = {Pattern Recogn.},
month = may,
pages = {689–698},
numpages = {10},
keywords = {Cluster prototype, Data clustering, Separation measure, Squared-error clustering}
}

@article{10.1155/2020/7917021,
author = {Zhang, Cheng and He, Dan and Zhang, Qingchen},
title = {A Deep Multiscale Fusion Method via Low-Rank Sparse Decomposition for Object Saliency Detection Based on Urban Data in Optical Remote Sensing Images},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/7917021},
doi = {10.1155/2020/7917021},
abstract = {The urban data provides a wealth of information that can support the life and work for people. In this work, we research the object saliency detection in optical remote sensing images, which is conducive to the interpretation of urban scenes. Saliency detection selects the regions with important information in the remote sensing images, which severely imitates the human visual system. It plays a powerful role in other image processing. It has successfully made great achievements in change detection, object tracking, temperature reversal, and other tasks. The traditional method has some disadvantages such as poor robustness and high computational complexity. Therefore, this paper proposes a deep multiscale fusion method via low-rank sparse decomposition for object saliency detection in optical remote sensing images. First, we execute multiscale segmentation for remote sensing images. Then, we calculate the saliency value, and the proposal region is generated. The superpixel blocks of the remaining proposal regions of the segmentation map are input into the convolutional neural network. By extracting the depth feature, the saliency value is calculated and the proposal regions are updated. The feature transformation matrix is obtained based on the gradient descent method, and the high-level semantic prior knowledge is obtained by using the convolutional neural network. The process is iterated continuously to obtain the saliency map at each scale. The low-rank sparse decomposition of the transformed matrix is carried out by robust principal component analysis. Finally, the weight cellular automata method is utilized to fuse the multiscale saliency graphs and the saliency map calculated according to the sparse noise obtained by decomposition. Meanwhile, the object priors knowledge can filter most of the background information, reduce unnecessary depth feature extraction, and meaningfully improve the saliency detection rate. The experiment results show that the proposed method can effectively improve the detection effect compared to other deep learning methods.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-66823-5_24,
author = {Campari, Tommaso and Eccher, Paolo and Serafini, Luciano and Ballan, Lamberto},
title = {Exploiting Scene-Specific Features for Object Goal Navigation},
year = {2020},
isbn = {978-3-030-66822-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66823-5_24},
doi = {10.1007/978-3-030-66823-5_24},
abstract = {Can the intrinsic relation between an object and the room in which it is usually located help agents in the Visual Navigation Task? We study this question in the context of Object Navigation, a problem in which an agent has to reach an object of a specific class while moving in a complex domestic environment. In this paper, we introduce a new reduced dataset that speeds up the training of navigation models, a notoriously complex task. Our proposed dataset permits the training of models that do not exploit online-built maps in reasonable times even without the use of huge computational resources. Therefore, this reduced dataset guarantees a significant benchmark and it can be used to identify promising models that could be then tried on bigger and more challenging datasets. Subsequently, we propose the SMTSC model, an attention-based model capable of exploiting the correlation between scenes and objects contained in them, highlighting quantitatively how the idea is correct.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part IV},
pages = {406–421},
numpages = {16},
keywords = {Visual Navigation, ObjectGoal Navigation, Reinforcement Learning},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Autonomic computing, Constraint programming, Dynamic adaptation, Dynamic software product line, Models at runtime, Variability, Verification, Web service composition}
}

@article{10.1016/j.csl.2012.01.008,
author = {Li, Ming and Han, Kyu J. and Narayanan, Shrikanth},
title = {Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
year = {2013},
issue_date = {January, 2013},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {27},
number = {1},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.01.008},
doi = {10.1016/j.csl.2012.01.008},
abstract = {The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively.},
journal = {Comput. Speech Lang.},
month = jan,
pages = {151–167},
numpages = {17},
keywords = {Age recognition, Formant, GMM, Gender recognition, Harmonic structure, Maximum likelihood linear regression, Pitch, Polynomial expansion, Prosodic features, SVM, Score level fusion, Sparse representation, UBM weight posterior probability supervectors}
}

@article{10.1007/s10664-017-9580-7,
author = {Przyby\l{}ek, Adam},
title = {An empirical study on the impact of AspectJ on software evolvability},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9580-7},
doi = {10.1007/s10664-017-9580-7},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2018–2050},
numpages = {33},
keywords = {AOP, Aspect-oriented programming, Controlled experiment, Maintainability, Separation of concerns, Understandability}
}

@inproceedings{10.1007/978-3-030-32248-9_54,
author = {Han, Shuo and Carass, Aaron and Prince, Jerry L.},
title = {Hierarchical Parcellation of the Cerebellum},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_54},
doi = {10.1007/978-3-030-32248-9_54},
abstract = {Parcellation of the cerebellum in an MR image has been used to study regional associations with both motion and cognitive functions. Despite the fact that the division of the cerebellum is defined hierarchically—i.e., the cerebellum can be divided into lobes and the lobes can be further divided into lobules—previous automatic methods to parcellate the cerebellum do not utilize this information. In this work, we propose a method based on convolutional neural networks&nbsp;(CNNs) to explicitly incorporate the hierarchical organization of the cerebellum. The network is constructed in a tree structure with each node representing a cerebellar region and having child nodes that further subdivide the region into finer substructures. Thus, our CNN is aware of the hierarchical organization of the cerebellum. Furthermore, by selecting tree nodes to represent the hierarchical properties of a given training sample, our network can be trained with heterogeneous training data that are labeled to different hierarchical depths. The proposed method was compared with a state-of-the-art cerebellum parcellation network. Our approach shows promising results as a first parcellation method to take the cerebellar hierarchical organization into consideration.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {484–491},
numpages = {8},
location = {Shenzhen, China}
}

@inproceedings{10.5555/3386691.3386706,
author = {Lu, Sidi and Luo, Bing and Patel, Tirthak and Yao, Yongtao and Tiwari, Devesh and Shi, Weisong},
title = {Making disk failure predictions SMARTer!},
year = {2020},
isbn = {9781939133120},
publisher = {USENIX Association},
address = {USA},
abstract = {Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.},
booktitle = {Proceedings of the 18th USENIX Conference on File and Storage Technologies},
pages = {151–168},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {FAST'20}
}

@inproceedings{10.1145/3240508.3240648,
author = {Zheng, Xiaoju and Zha, Zheng-Jun and Zhuang, Liansheng},
title = {A Feature-Adaptive Semi-Supervised Framework for Co-saliency Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240648},
doi = {10.1145/3240508.3240648},
abstract = {Co-saliency detection, which refers to the discovery of common salient foreground regions in a group of relevant images, has attracted increasing attention due to its widespread applications in many vision tasks. Existing methods assemble features from multiple views toward a comprehensive representation, however overlook the efficacy disparity among various features in detecting co-saliency. This paper proposes a novel feature-adaptive semi-supervised (FASS) framework for co-saliency detection, which seamlessly integrates multi-view feature learning, graph structure optimization and co-saliency prediction in a unified solution. In particular, the FASS exploits the efficacy disparity of multi-view features at both view and element levels by a joint formulation of view-wise feature weighting and element-wise feature selection, leading to an effective representation robust to feature noise and redundancy as well as adaptive to the task at hand. It predicts co-saliency map by optimizing co-saliency label prorogation over a graph of both labeled and unlabeled image regions. The graph structure is optimized jointly with feature learning and co-saliency prediction to precisely characterize underlying correlation among regions. The FASS is thus able to produce satisfactory co-saliency map based on the effective exploration of multi-view features as well as inter-region correlation. Extensive experiments on three benchmark datasets, i.e., iCoseg, Cosal2015 and MSRC, have demonstrated that the proposed FASS outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {959–966},
numpages = {8},
keywords = {co-saliency detection, graph optimization, multi-view feature, semi-supervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1007/978-3-030-26061-3_55,
author = {Yu, Jianguo and Markov, Konstantin and Karpov, Alexey},
title = {Speaking Style Based Apparent Personality Recognition},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_55},
doi = {10.1007/978-3-030-26061-3_55},
abstract = {In this study, we investigate the problem of apparent personality recognition using person’s voice, or more precisely, the way he or she speaks. Based on the style transfer idea in deep neural net image processing, we developed a system capable of speaking style extraction from recorded speech utterances, which then uses this information to estimate the so called Big-Five personality traits. The latent speaking style space is represented by the Gram matrix of convoluted acoustic features. We used a database with labels of personality traits perceived by other people (first impression). The experimental results showed that the proposed system achieves state of the art results for the task of audio based apparent personality recognition.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {540–548},
numpages = {9},
keywords = {Automatic Apparent Personality Recognition, First impression prediction, Speaking style representation, Computational Paralinguistics},
location = {Istanbul, Turkey}
}

@article{10.1007/s10472-019-09645-7,
author = {Liu, Xudong and Truszczynski, Miroslaw},
title = {Voting-based ensemble learning for partial lexicographic preference forests over combinatorial domains},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {87},
number = {1–2},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-019-09645-7},
doi = {10.1007/s10472-019-09645-7},
abstract = {We study preference representation models based on partial lexicographic preference trees (PLP-trees). We propose to represent preference relations as forests of small PLP-trees (PLP-forests), and to use voting rules to aggregate orders represented by the individual trees into a single order to be taken as a model of the agent’s preference relation. We show that when learned from examples, PLP-forests have better accuracy than single PLP-trees. We also show that the choice of a voting rule does not have a major effect on the aggregated order, thus rendering the problem of selecting the “right” rule less critical. Next, for the proposed PLP-forest preference models, we develop methods to compute optimal and near-optimal outcomes, the tasks that appear difficult for some other common preference models. Lastly, we compare our models with those based on decision trees, which brings up questions for future research.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = oct,
pages = {137–155},
numpages = {19},
keywords = {Lexicographic preference models, Preference learning, Preference modeling and reasoning, Social choice theory, Computational complexity theory, Voting theory, Maximum satisfiability}
}

@inproceedings{10.5555/3540261.3540707,
author = {Chen, Shizhe and Guhur, Pierre-Louis and Schmid, Cordelia and Laptev, Ivan},
title = {History aware multimodal transformer for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {446},
numpages = {14},
series = {NIPS '21}
}

@article{10.1504/IJAOSE.2016.080887,
author = {Nunes, Ingrid and Faccin, Jo\~{a}o Guilherme},
title = {Modelling and implementing modularised BDI agents with capability relationships},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {5},
number = {2/3},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2016.080887},
doi = {10.1504/IJAOSE.2016.080887},
abstract = {The BDI model is the foundation for one of the most widely used architectures to develop autonomous agents. Such model provides the concepts of beliefs, desires and intentions, which comprise the internal agent structure. Although much work has been done to support BDI agent development, there is lack of approaches that focus on modularisation of intra-agent software components. Given that agents often present a complex behaviour and, consequently, complex design and implementation, modularisation is a key to make the development of large-scale enterprise applications feasible. In this paper, we extend the concept of capability, which emerged to model BDI agent modules, by adding different types of relationships between them, namely association, composition and generalisation. Such relationships allow the development of BDI agent building blocks that can be combined so as to form agents, while hiding capability information as needed. Moreover, we present a modelling tool and implementation of the proposed relationships to not only provide the conceptual foundation of our approach but also enable its practical use. We show the effectiveness of our approach by refactoring an existing software product line implemented with BDI agents using our capability relationships.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {203–231},
numpages = {29},
keywords = {BDI architecture, BDI4JADE, agent-based systems, agent-oriented programming, autonomous agents, belief desire intention, capability relationships, modelling, modularisation, modularised BDI agents, multi-agent systems}
}

@article{10.1016/j.neucom.2015.04.087,
author = {Liu, Weifeng and Li, Yang and Tao, Dapeng and Wang, Yanjiang},
title = {A general framework for co-training and its applications},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.04.087},
doi = {10.1016/j.neucom.2015.04.087},
abstract = {Co-training is one of the major semi-supervised learning paradigms in which two classifiers are alternately trained on two distinct views and they teach each other by adding the predictions of unlabeled data to the training set of the other view. Co-training can achieve promising performance, especially when there is only a small number of labeled data. Hence, co-training has received considerable attention, and many variant co-training algorithms have been developed. It is essential and informative to provide a systematic framework for a better understanding of the common properties and differences in these algorithms. In this paper, we propose a general framework for co-training according to the diverse learners constructed in co-training. Specifically, we provide three types of co-training implementations, including co-training on multiple views, co-training on multiple classifiers, and co-training on multiple manifolds. Finally, comprehensive experiments of different methods are conducted on the UCF-iPhone dataset for human action recognition and the USAA dataset for social activity recognition. The experimental results demonstrate the effectiveness of the proposed solutions.},
journal = {Neurocomput.},
month = nov,
pages = {112–121},
numpages = {10},
keywords = {Co-training, Human action recognition, Multi-view, Semi-supervised learning, Social activity recognition}
}

@article{10.1504/IJGUC.2018.091725,
title = {Cloud computing based on agent technology, super-recursive algorithms and DNA},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {2},
issn = {1741-847X},
url = {https://doi.org/10.1504/IJGUC.2018.091725},
doi = {10.1504/IJGUC.2018.091725},
abstract = {Agents and agent systems are becoming more and more important in the development of a variety of fields such as ubiquitous computing, ambient intelligence, autonomous computing, data analytics, machine learning, intelligent systems and intelligent robotics. In this paper, we examine interactions of theoretical computer science with computer and network technologies analysing how agent technology emerged, matured and progressed in mathematical models of computation. We demonstrate how these models are used in the novel distributed intelligent managed element DIME network architecture DNA, which extends the conventional computational model of information processing networks, allowing improvement of the efficiency and resiliency of computational processes. Two implementations of DNA described in the paper illustrate how the application of agent technology radically improves current cloud computing state of the art. First example demonstrates the live migration of a database from a laptop to a cloud without losing transactions and without using containers or moving virtual machine images. The second example exhibits the implementation of cloud agnostic computing over a network of public and private clouds where live computing process workflows are migrated from one cloud to another without losing transactions. Both these implementations demonstrate the power of scientific thought for dramatically extending the current state of the art of cloud and grid computing practice.},
journal = {Int. J. Grid Util. Comput.},
month = jan,
pages = {193–204},
numpages = {12}
}

@inproceedings{10.1007/978-3-642-27872-3_21,
author = {Kumar, K. M. Anil and Suresha, Suresha},
title = {Detection of web users' opinion from normal and short opinionated words},
year = {2010},
isbn = {9783642278716},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27872-3_21},
doi = {10.1007/978-3-642-27872-3_21},
abstract = {In this paper we present an approach to identify opinion of web users from an opinionated text and to classify web user's opinion into positive or negative. Web users document their opinion in opinionated sites, shopping sites, personal pages etc., to express and share their opinion with other web users. The opinion expressed by web users may be on diverse topics such as politics, sports, products, movies etc. These opinions will be very useful to others such as, leaders of political parties, selection committees of various sports, business analysts and other stake holders of products, directors and producers of movies as well as to the other concerned web users. Today web users express their opinion using normal words and short words. These short words, such as gud for good, grt8 for great etc., are very popular and are used by a large number of web users to document their opinion. We use semantic based approach to find users opinion from both normal and short words. Our approach first detects subjective phrases and uses these phrases along with intensifiers and diminishers to obtain semantic orientation scores. The semantic orientation score of these phrases is used to identify user's opinion from an opinionated text. Our approach provides better results than the other approaches on different data sets.},
booktitle = {Proceedings of the Second International Conference on Data Engineering and Management},
pages = {139–145},
numpages = {7},
keywords = {artificial intelligence, opinion mining, sentiment analysis},
location = {Tiruchirappalli, India},
series = {ICDEM'10}
}

@article{10.1007/s11042-020-09907-1,
author = {Zhong, Yuanhong and Zhang, Jing and Zhou, Zhaokun and Cheng, Xinyu and Huang, Guan and Li, Qiang},
title = {Recovery of image and video based on compressive sensing via tensor approximation and Spatio-temporal correlation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {5},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09907-1},
doi = {10.1007/s11042-020-09907-1},
abstract = {In recent years, block-based compressive sensing (BCS) has been extensively studied because it can reduce computational complexity and data storage by dividing the image into smaller patches, but the performance of the reconstruction algorithm is not satisfactory. In this paper, a new reconstruction model for image and video is proposed. The model makes full use of spatio-temporal correlation and utilizes low-rank tensor approximation to improve the quality of the reconstructed image and video. For image recovery, the proposed model obtains a low-rank approximation of a tensor formed by non-local similar patches, and improves the reconstruction quality from a spatial perspective by combining non-local similarity and low-rank property. For video recovery, the reconstruction process is divided into two phases. In the first phase, each frame of the video sequence is regarded as an independent image to be reconstructed by taking advantage of spatial property. The second phase performs tensor approximation through searching similar patches within frames near the target frame, to achieve reconstruction by putting the spatio-temporal correlation into full play. The resulting model is solved by an efficient Alternating Direction Method of Multipliers (ADMM) algorithm. A series of experiments show that the quality of the proposed model is comparable to the current state-of-the-art recovery methods.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {7433–7450},
numpages = {18},
keywords = {Block-based compressive sensing, Image and video recovery, Low-rank tensor approximation, High order singular value decomposition (HOSVD), Spatio-temporal correlation}
}

@inproceedings{10.1007/978-3-642-14052-5_9,
author = {Autexier, Serge and Dietrich, Dominik},
title = {A tactic language for declarative proofs},
year = {2010},
isbn = {3642140513},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-14052-5_9},
doi = {10.1007/978-3-642-14052-5_9},
abstract = {Influenced by the success of the Mizar system many declarative proof languages have been developed in the theorem prover community, as declarative proofs are more readable, easier to modify and to maintain than their procedural counterparts. However, despite their advantages, many users still prefer the procedural style of proof, because procedural proofs are faster to write. In this paper we show how to define a declarative tactic language on top of a declarative proof language. The language comes along with a rich facility to declaratively specify conditions on proof states in the form of sequent patterns, as well as ellipses (dot notation) to provide a limited form of iteration. As declarative tactics are specified using the declarative proof language, they offer the same advantages as declarative proof languages. At the same time, they also produce declarative justifications in the form of a declarative proof script and can thus be seen as an attempt to reduce the gap between procedural and declarative proofs.},
booktitle = {Proceedings of the First International Conference on Interactive Theorem Proving},
pages = {99–114},
numpages = {16},
location = {Edinburgh, UK},
series = {ITP'10}
}

@inproceedings{10.5555/3540261.3541282,
author = {Zhang, Jiwen and Wei, Zhongyu and Fan, Jianqing and Peng, Jiajie},
title = {Curriculum learning for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1021},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {AutoML, configurable systems, constraint solving, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s10844-017-0479-y,
author = {Cai, Yuanyuan and Zhang, Qingchuan and Lu, Wei and Che, Xiaoping},
title = {A hybrid approach for measuring semantic similarity based on IC-weighted path distance in WordNet},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {1},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-017-0479-y},
doi = {10.1007/s10844-017-0479-y},
abstract = {As a valuable tool for text understanding, semantic similarity measurement enables discriminative semantic-based applications in the fields of natural language processing, information retrieval, computational linguistics and artificial intelligence. Most of the existing studies have used structured taxonomies such as WordNet to explore the lexical semantic relationship, however, the improvement of computation accuracy is still a challenge for them. To address this problem, in this paper, we propose a hybrid WordNet-based approach CSSM-ICSP to measuring concept semantic similarity, which leverage the information content(IC) of concepts to weight the shortest path distance between concepts. To improve the performance of IC computation, we also develop a novel model of the intrinsic IC of concepts, where a variety of semantic properties involved in the structure of WordNet are taken into consideration. In addition, we summarize and classify the technical characteristics of previous WordNet-based approaches, as well as evaluate our approach against these approaches on various benchmarks. The experimental results of the proposed approaches are more correlated with human judgment of similarity in term of the correlation coefficient, which indicates that our IC model and similarity detection approach are comparable or even better for semantic similarity measurement as compared to others.},
journal = {J. Intell. Inf. Syst.},
month = aug,
pages = {23–47},
numpages = {25},
keywords = {Concept semantic similarity, Edge distance, Intrinsic information content, WordNet}
}

@inproceedings{10.1109/CEC48606.2020.9185776,
author = {Mendon\c{c}a, Willian D.F. and Assun\c{c}\~{a}o, Wesley K.G. and Estanislau, Lucas V. and Vergilio, Silvia R. and Garcia, Alessandro},
title = {Towards a Microservices-Based Product Line with Multi-Objective Evolutionary Algorithms},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC48606.2020.9185776},
doi = {10.1109/CEC48606.2020.9185776},
abstract = {Microservices are small and independently deployable services. They can be developed on different platforms and communicate via lightweight protocols, what makes them highly interoperable. The interoperability between microservices, as well as their reuse and customization needs make this kind of systems adequate to constitute a Software Product Line. However, there is no automatic approach to support the designing of Microservices-Based Product Lines (MBPLs). To move towards the development of MBPLs, this work presents an approach, named MOEA4MBPL, to extract Feature Models (FMs) from a set of microservices-based systems. These FMs intent to leverage interoperability, enabling the practitioners to reason about reuse and/or customization of functionalities. The proposed approach is based on multi-objective evolutionary algorithms, optimizing three objectives, namely precision and recall of products denoted by an FM, and conformance with existing dependencies between microservices. MOEA4MBPL was evaluated with six microservices-based systems, using the algorithms NSGA-II and SPEA2. Our approach was capable of finding FMs with good trade-off values of precision and recall, satisfying all dependencies among the microservices. SPEA2 found better fronts of solutions than NSGA-II, but the latter always executed faster and could find single solutions closer to an ideal solution than the former.},
booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s00180-012-0381-6,
author = {Schmidt, Miriam and Palm, G\"{u}nther and Schwenker, Friedhelm},
title = {Spectral graph features for the classification of graphs and graph sequences},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1–2},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-012-0381-6},
doi = {10.1007/s00180-012-0381-6},
abstract = {In this paper, the classification power of the eigenvalues of six graph-associated matrices is investigated. Each matrix contains a certain type of geometric/ spatial information, which may be important for the classification process. The performances of the different feature types is evaluated on two data sets: first a benchmark data set for optical character recognition, where the extracted eigenvalues were utilized as feature vectors for multi-class classification using support vector machines. Classification results are presented for all six feature types, as well as for classifier combinations at decision level. For the decision level combination, probabilistic output support vector machines have been applied, with a performance up to 92.4 %. To investigate the power of the spectra for time dependent tasks, too, a second data set was investigated, consisting of human activities in video streams. To model the time dependency, hidden Markov models were utilized and the classification rate reached 98.3 %.},
journal = {Comput. Stat.},
month = feb,
pages = {65–80},
numpages = {16},
keywords = {Graph classification, Graph-associated matrices, Human activity recognition, Optical character recognition, Spectrum}
}

@article{10.1016/j.cogsys.2019.09.021,
author = {Vityaev, Evgenii},
title = {Consciousness as a logically consistent and prognostic model of reality},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2019.09.021},
doi = {10.1016/j.cogsys.2019.09.021},
journal = {Cogn. Syst. Res.},
month = jan,
pages = {231–246},
numpages = {16},
keywords = {Clustering, Categorization, Natural classification, Natural concepts, Integrated information, Concepts}
}

@article{10.1007/s11280-019-00766-x,
author = {Hu, Rongyao and Zhu, Xiaofeng and Zhu, Yonghua and Gan, Jiangzhang},
title = {Robust SVM with adaptive graph learning},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-019-00766-x},
doi = {10.1007/s11280-019-00766-x},
abstract = {Support Vector Machine (SVM) has been widely applied in real application due to its efficient performance in the classification task so that a large number of SVM methods have been proposed. In this paper, we present a novel SVM method by taking the dynamic graph learning and the self-paced learning into account. To do this, we propose utilizing self-paced learning to assign important samples with large weights, learning a transformation matrix for conducting feature selection to remove redundant features, and learning a graph matrix from the low-dimensional data of original data to preserve the data structure. As a consequence, both the important samples and the useful features are used to select support vectors in the SVM framework. Experimental analysis on four synthetic and sixteen benchmark data sets demonstrated that our method outperformed state-of-the-art methods in terms of both binary classification and multi-class classification tasks.},
journal = {World Wide Web},
month = may,
pages = {1945–1968},
numpages = {24},
keywords = {Self-paced learning, Feature selection, Graph learning, SVM}
}

@inbook{10.1145/3191315.3191317,
author = {Maier, David and Tekle, K. Tuncay and Kifer, Michael and Warren, David S.},
title = {Datalog: concepts, history, and outlook},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191317},
abstract = {This chapter is a survey of the history and the main concepts of Datalog.We begin with an introduction to the language and its use for database definition and querying. We then look back at the threads from logic languages, databases, artificial intelligence, and expert systems that led to the emergence of Datalog and reminiscence about the origin of the name. We consider the interaction of recursion with other common data language features, such as negation and aggregation, and look at other extensions, such as constraints, updates, and object-oriented features.We provide an overview of the main approaches to Datalog evaluation and their variants, then recount some early implementations of Datalog and of similar deductive database systems.We speculate on the reasons for the decline in the interest in the language in the 1990s and the causes for its later resurgence in a number of application areas.We conclude with several examples of current systems based on or supporting Datalog and briefly examine the performance of some of them.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {3–100},
numpages = {98}
}

@inproceedings{10.5555/1558109.1558283,
author = {Nunes, Ingrid and Kulesza, Uir\'{a} and Nunes, Camila and Lucena, Carlos J. P.},
title = {A domain engineering process for developing multi-agent systems product lines},
year = {2009},
isbn = {9780981738178},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent Systems Product Lines (MAS-PLs) have emerged to integrate two promising trends of software engineering: agent-oriented software engineering and software product lines. In this paper, we propose a domain engineering process to develop MAS-PLs, built on top of agent-oriented and software product line approaches.},
booktitle = {Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {1339–1340},
numpages = {2},
keywords = {agent-oriented software engineering, domain engineering, multi-agent systems, process, software product lines},
location = {Budapest, Hungary},
series = {AAMAS '09}
}

@inproceedings{10.5555/3016387.3016586,
author = {Stone, Peter},
title = {What's hot at RoboCup (extended abstract)},
year = {2016},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4346–4347},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.5555/3020419.3020482,
author = {Weng, Paul},
title = {Axiomatic foundations for a class of generalized expected utility: algebraic expected utility},
year = {2006},
isbn = {0974903922},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, we provide two axiomatizations of algebraic expected utility, which is a particular generalized expected utility, in a von Neumann-Morgenstern setting, i.e. uncertainty representation is supposed to be given and here to be described by a plausibility measure valued on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility,...) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty representation, autoduality of the decision criterion and possibility of modeling decision maker's attitude toward uncertainty.},
booktitle = {Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence},
pages = {520–527},
numpages = {8},
location = {Cambridge, MA, USA},
series = {UAI'06}
}

@article{10.5555/2591248.2591263,
author = {Androutsopoulos, Ion and Lampouras, Gerasimos and Galanis, Dimitrios},
title = {Generating natural language descriptions from OWL ontologies: the natural OWL system},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We present Naturalowl, a natural language generation system that produces texts describing individuals or classes of owl ontologies. Unlike simpler owl verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like Naturalowl, one can publish information in owl on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of Naturalowl, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent linguistic resources are available, Naturalowl produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {671–715},
numpages = {45}
}

@article{10.1016/j.jss.2011.06.026,
author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
title = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.026},
doi = {10.1016/j.jss.2011.06.026},
abstract = {Abstract: Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2208–2221},
numpages = {14},
keywords = {Configuration, Feature models, Genetic algorithm, Optimization, Product derivation, Software product lines}
}

@inproceedings{10.5555/3367243.3367420,
author = {Kim, Yejin and Kim, Kwangseob and Park, Chanyoung and Yu, Hwanjo},
title = {Sequential and diverse recommendation with long tail},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Sequential recommendation is a task that learns a temporal dynamic of a user behaviour in sequential data and predicts items that a user would like afterward. However, diversity has been rarely emphasized in the context of sequential recommendation. Sequential and diverse recommendation must learn temporal preference on diverse items as well as on general items. Thus, we propose a sequential and diverse recommendation model that predicts a ranked list containing general items and also diverse items without compromising significant accuracy. To learn temporal preference on diverse items as well as on general items, we cluster and relocate consumed long tail items to make a pseudo ground truth for diverse items and learn the preference on long tail using recurrent neural network, which enables us to directly learn a ranking function. Extensive online and offline experiments deployed on a commercial platform demonstrate that our models significantly increase diversity while preserving accuracy compared to the state-of-the-art sequential recommendation model, and consequently our models improve user satisfaction.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {2740–2746},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1145/3338286.3344387,
author = {Lee, Hao-Ping and Dingler, Tilman and Lin, Chih-Heng and Chen, Kuan-Yin and Chung, Yu-Lin and Chen, Chia-Yu and Chang, Yung-Ju},
title = {Predicting Smartphone Users' General Responsiveness to IM Contacts Based on IM Behavior},
year = {2019},
isbn = {9781450368254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338286.3344387},
doi = {10.1145/3338286.3344387},
abstract = {History of conversations through instant messaging (IM) contains abundant information about the communication patterns of the dyad, including conversation partners' mutual responsiveness to messages. We have, however, not seen many examinations of using such information in modeling mobile users' responsiveness in IM communication. In this paper, we present an in-the-wild study, in which we leverage participants' IM messaging logs to build models predicting their general responsiveness. Our models based on data from 33 IM user achieved an accuracy of up to 71% (AUROC). In particular, we show that 90-day IM-communication patterns, in general, outperformed their 14-day equivalent in our prediction models, indicating better coherence between long-term IM patterns with their general communication experience.},
booktitle = {Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {40},
numpages = {6},
keywords = {ESM, Mobile notifications, machine learning, mobile receptivity},
location = {Taipei, Taiwan},
series = {MobileHCI '19}
}

@article{10.1016/j.cie.2021.107480,
author = {Cheung, W.L. and Piplani, R. and Alam, S. and Bernard-Peyre, L.},
title = {Dynamic capacity and variable runway configurations in airport slot allocation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107480},
doi = {10.1016/j.cie.2021.107480},
journal = {Comput. Ind. Eng.},
month = sep,
numpages = {12},
keywords = {Airport slot allocation, Airport demand management, Capacity, Demand capacity imbalance, Mixed mode operations}
}

@article{10.1016/j.engappai.2009.03.001,
author = {Borangiu, Theodor and Gilbert, Pascal and Ivanescu, Nick-Andrei and Rosu, Andrei},
title = {An implementing framework for holonic manufacturing control with multiple robot-vision stations},
year = {2009},
issue_date = {June, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {4–5},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.03.001},
doi = {10.1016/j.engappai.2009.03.001},
abstract = {The paper describes a holonic control architecture and implementing issues for agile job shop assembly with networked intelligent robots, based on the dynamic simulation of material processing and transportation. The holarchy was defined considering the PROSA reference architecture relative to which in-line vision-based quality control was added by help of feature-based descriptions of the material flow. Two solutions for production planning are proposed: a knowledge-based algorithm using production rules, and an OO resolved scheduling rate planner (RSRP) based on variable-timing simulation. Failure- and recovery-management are developed as generic scenarios embedding the CNP mechanism into production self-rescheduling. Aggregate Order Holon execution is realized by OPC-based PLC software integration and event-driven product transportation. The holonic control of multiple networked robot-vision stations also features tolerance to station computer- (IBM PC-type), station controller- (robot controller), quality control- (machine vision) and communication- (LAN) failure. Fault tolerance and high availability at shop-floor level are provided due to the multiple physical communication capabilities of the robot controllers, to their multiple-axis multitasking operating capability, and to hardware redundancy of single points of failure (SPOF). Implementing solutions and experiments are reported for a 6-station robot-vision assembly cell with twin-track closed-loop pallet transportation system and product-racking RD/WR devices. Future developments will consider manufacturing integration at enterprise level.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {505–521},
numpages = {17},
keywords = {Applied AI, Holonic manufacturing, Real-time vision, Robotics, Semi-heterarchical control}
}

@inproceedings{10.5555/645654.665656,
author = {Pal, Sankar K.},
title = {Soft Computing Pattern Recognition: Principles, Integrations, and Data Mining},
year = {2001},
isbn = {3540430709},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Relevance of fuzzy logic, artificial neural networks, genetic algorithms and rough sets to pattern recognition and image processing problems is described through examples. Different integrations of these soft computing tools are illustrated. Evolutionary rough fuzzy network which is based on modular principle is explained, as an example of integrating all the four tools for efficient classification and rule generation, with its various characterstics. Significance of soft computing approach in data mining and knowledge discovery is finally discussed along with the scope of future research.},
booktitle = {Proceedings of the Joint JSAI 2001 Workshop on New Frontiers in Artificial Intelligence},
pages = {261–271},
numpages = {11}
}

@inproceedings{10.1007/978-3-030-71827-5_11,
author = {Estienne, Th\'{e}o and Vakalopoulou, Maria and Battistella, Enzo and Carr\'{e}, Alexandre and Henry, Th\'{e}ophraste and Lerousseau, Marvin and Robert, Charlotte and Paragios, Nikos and Deutsch, Eric},
title = {Deep Learning Based Registration Using&nbsp;Spatial Gradients and Noisy Segmentation Labels},
year = {2020},
isbn = {978-3-030-71826-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71827-5_11},
doi = {10.1007/978-3-030-71827-5_11},
abstract = {Image registration is one of the most challenging problems in medical image analysis. In the recent years, deep learning based approaches became quite popular, providing fast and performing registration strategies. In this short paper, we summarise our work presented on Learn2Reg challenge 2020. The main contributions of our work rely on (i) a symmetric formulation, predicting the transformations from source to target and from target to source simultaneously, enforcing the trained representations to be similar and (ii) integration of variety of publicly available datasets used both for pretraining and for augmenting segmentation labels. Our method reports a mean dice of 0.64 for task 3 and 0.85 for task 4 on the test sets, taking third place on the challenge. Our code and models are publicly available at  and .},
booktitle = {Segmentation, Classification, and Registration of Multi-Modality Medical Imaging Data: MICCAI 2020 Challenges, ABCs 2020, L2R 2020, TN-SCUI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {87–93},
numpages = {7},
location = {Lima, Peru}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.5555/3495724.3496445,
author = {Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff A.},
title = {Curriculum learning by dynamic instance hardness},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A good teacher can adjust a curriculum based on students' learning history. By analogy, in this paper, we study the dynamics of a deep neural network's (DNN) performance on individual samples during its learning process. The observed properties allow us to develop an adaptive curriculum that leads to faster learning of more accurate models. We introduce dynamic instance hardness (DIH), the exponential moving average of a sample's instantaneous hardness (e.g., a loss, or a change in output) over the training history. A low DIH indicates that a model retains knowledge about a sample over time. For DNNs, we find that a sample's DIH early in training predicts its DIH in later stages. Hence, we can train a model using samples mostly with higher DIH and safely deprioritize those with lower DIH. This motivates a DIH guided curriculum learning (DIHCL) procedure. Compared to existing CL methods: (1) DIH is more stable over time than using only instantaneous hardness, which is noisy due to stochastic training and DNN's non-smoothness; (2) DIHCL is computationally inexpensive since it uses only a byproduct of back-propagation and thus does not require extra inference. On 11 datasets, DIHCL significantly outperforms random mini-batch SGD and recent CL methods in terms of efficiency and final performance.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.neucom.2018.11.060,
author = {Li, Shaoyong and Tang, Chang and Liu, Xinwang and Liu, Yaping and Chen, Jiajia},
title = {Dual graph regularized compact feature representation for unsupervised feature selection},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.060},
doi = {10.1016/j.neucom.2018.11.060},
journal = {Neurocomput.},
month = feb,
pages = {77–96},
numpages = {20},
keywords = {Unsupervised feature selection, Dictionary learning, Local geometrical structure preservation, Feature representation}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Requirements engineering, Software product lines, Systematic literature review}
}

@article{10.1016/j.cogsys.2019.10.006,
author = {Ramirez-Pedraza, Raymundo and Vargas, Natividad and Sandoval, Carlos and del Valle-Padilla, Juan Luis and Ramos, F\'{e}lix},
title = {A bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2019.10.006},
doi = {10.1016/j.cogsys.2019.10.006},
journal = {Cogn. Syst. Res.},
month = jan,
pages = {293–303},
numpages = {11},
keywords = {Brain model, Decision-making, Planning, Spatial attention, Motor system, Goal-driven}
}

@inproceedings{10.1145/3460231.3478885,
author = {Di Sipio, Claudio and Di Rocco, Juri and Di Ruscio, Davide and Nguyen, Dr. Phuong Thanh},
title = {A Low-Code Tool Supporting the Development of Recommender Systems},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3478885},
doi = {10.1145/3460231.3478885},
abstract = {The design of recommender systems (RSs) to support software development encompasses the fulfillment of different steps, including data preprocessing, choice of the most appropriate algorithms, item delivery. Though RSs can alleviate the curse of information overload, existing approaches resemble black-box systems, in which the end-user is not expected to fine-tune or personalize the overall process. In this work, we propose LEV4REC, a low-code environment to assist developers in designing, configuring, and delivering recommender systems. The first step supported by the proposed tool includes defining an initial model that allows for the configuration of the crucial components of the wanted RS. Then, a subsequent phase is performed to finalize the RS design, e.g., to specify configuration parameters. LEV4REC is eventually capable of generating source code for the desired RS. To evaluate the capabilities of the approach, we used LEV4REC to specify two existing RSs built on top of two different recommendation algorithms, i.e., collaborative filtering and supervised machine learning.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {741–744},
numpages = {4},
keywords = {lowcode, model-driven, recommender systems},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@article{10.1016/j.specom.2019.10.006,
author = {Michelsanti, Daniel and Tan, Zheng-Hua and Sigurdsson, Sigurdur and Jensen, Jesper},
title = {Deep-learning-based audio-visual speech enhancement in presence of Lombard effect},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.006},
doi = {10.1016/j.specom.2019.10.006},
journal = {Speech Commun.},
month = dec,
pages = {38–50},
numpages = {13},
keywords = {Lombard effect, Audio-visual speech enhancement, Deep learning, Speech quality, Speech intelligibility}
}

@article{10.5555/1756006.1859931,
author = {Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
title = {Spectral Regularization Algorithms for Learning Large Incomplete Matrices},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efficient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efficiently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semidefinite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 106 X 106 incomplete matrix with 107 observed entries, and fits a rank-95 approximation to the full Netflix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {2287–2322},
numpages = {36}
}

@article{10.1016/j.neucom.2017.01.093,
author = {Tareef, Afaf and Song, Yang and Huang, Heng and Wang, Yue and Feng, Dagan and Chen, Mei and Cai, Weidong},
title = {Optimizing the cervix cytological examination based on deep learning and dynamic shape modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {248},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.01.093},
doi = {10.1016/j.neucom.2017.01.093},
abstract = {The task of segmenting nuclei and cytoplasm in Papanicolau smear images is one of the most challenging tasks in automated cervix cytological analysis owing to the high degree of overlapping, the multiform shape of the cells and their complex structures resulting from inconsistent staining, poor contrast, and the presence of inflammatory cells. This article presents a robust variational segmentation framework based on superpixelwise convolutional neutral network and a learned shape prior enabling an accurate analysis of overlapping cervical mass. The cellular components of Pap image are first classified by automatic feature learning and classification model. Then, a learning shape prior model is employed to delineate the actual contour of each individual cytoplasm inside the overlapping mass. The shape prior is dynamically modeled during the segmentation process as a weighted linear combination of shape templates from an over-complete shape dictionary under sparsity constraints. We provide quantitative and qualitative assessment of the proposed method using two databases of 153 cervical cytology images, with 870 cells in total, synthesised by accumulating real isolated cervical cells to generate overlapping cellular masses with a varying number of cells and degree of overlap. The experimental results have demonstrated that our methodology can successfully segment nuclei and cytoplasm from highly overlapping mass. Our segmentation is also competitive when compared to the state-of-the-art methods.},
journal = {Neurocomput.},
month = jul,
pages = {28–40},
numpages = {13},
keywords = {Convolutional neural network, Feature learning, Level set evolution, Overlapping cell segmentation, Sparse approximation}
}

@article{10.1016/j.patcog.2011.12.019,
author = {Graves, Daniel and Noppen, Joost and Pedrycz, Witold},
title = {Clustering with proximity knowledge and relational knowledge},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {7},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.12.019},
doi = {10.1016/j.patcog.2011.12.019},
abstract = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes.},
journal = {Pattern Recogn.},
month = jul,
pages = {2633–2644},
numpages = {12},
keywords = {Fuzzy clustering, Knowledge representation, Proximity, Relational clustering, Software requirements}
}

@inproceedings{10.1007/978-3-642-34166-3_71,
author = {Hidaka, Akinori and Kurita, Takio},
title = {Sparse discriminant analysis based on the bayesian posterior probability obtained by L1 regression},
year = {2012},
isbn = {9783642341656},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34166-3_71},
doi = {10.1007/978-3-642-34166-3_71},
abstract = {Recently the kernel discriminant analysis (KDA) has been successfully applied in many applications. However, kernel functions are usually defined a priori and it is not known what the optimum kernel function for nonlinear discriminant analysis is. Otsu derived the optimum nonlinear discriminant analysis (ONDA) by assuming the underlying probabilities similar with the Bayesian decision theory. Kurita derived discriminant kernels function (DKF) as the optimum kernel functions in terms of the discriminant criterion by investigating the optimum discriminant mapping constructed by the ONDA. The derived kernel function is defined by using the Bayesian posterior probabilities. We can define a family of DKFs by changing the estimation method of the Bayesian posterior probabilities. In this paper, we propose a novel discriminant kernel function based on L1-regularized regression, called L1 DKF. L1 DKF is given by using the Bayesian posterior probabilities estimated by L1 regression. Since L1 regression yields a sparse representation for given samples, we can naturally introduce the sparseness into the discriminant kernel function. To introduce the sparseness into LDA, we use L1 DKF as the kernel function of LDA. In experiments, we show sparseness and classification performance of L1 DKF.},
booktitle = {Proceedings of the 2012 Joint IAPR International Conference on Structural, Syntactic, and Statistical Pattern Recognition},
pages = {648–656},
numpages = {9},
location = {Hiroshima, Japan},
series = {SSPR'12/SPR'12}
}

@inproceedings{10.5555/1980844.1980855,
author = {Molder, Cristian and Boscoianu, Mircea and Stanciu, Mihai I. and Vizitiu, Iulian C.},
title = {Improved automatic number plate recognition system},
year = {2008},
isbn = {9789604740222},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {One of the main applications of pattern recognition is the use of video or imaging cameras in order to detect and recognize the vehicle license plate numbers. This is important mainly for access, traffic surveillance and law enforcement. Several systems are implemented and working in situ with more or less precision. These systems are based on the use of a feature set extracted from the best video sequences and further recognized using a classifier. In this paper we present a new system based on multiple features and classifiers, The classification results are further used as inputs for a decision fusion. The classification results are proven to be significantly better that each of the method considered individually. The individual classifiers as well as the decision fusion are tested on a large real image database.},
booktitle = {Proceedings of the 1st WSEAS International Conference on Visualization, Imaging and Simulation},
pages = {49–54},
numpages = {6},
keywords = {ANPR, decision fusion, licence plate, matching templates, neural networks, pattern recognition, skeleton features},
location = {Bucharest, Romania},
series = {VIS'08}
}

@article{10.1016/j.neucom.2016.09.070,
author = {Tareef, Afaf and Song, Yang and Cai, Weidong and Huang, Heng and Chang, Hang and Wang, Yue and Fulham, Michael and Feng, Dagan and Chen, Mei},
title = {Automatic segmentation of overlapping cervical smear cells based on local distinctive features and guided shape deformation},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {221},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.070},
doi = {10.1016/j.neucom.2016.09.070},
abstract = {Automated segmentation of cells from cervical smears poses great challenge to biomedical image analysis because of the noisy and complex background, poor cytoplasmic contrast and the presence of fuzzy and overlapping cells. In this paper, we propose an automated segmentation method for the nucleus and cytoplasm in a cluster of cervical cells based on distinctive local features and guided sparse shape deformation. Our proposed approach is performed in two stages: segmentation of nuclei and cellular clusters, and segmentation of overlapping cytoplasm. In the first stage, a set of local discriminative shape and appearance cues of image superpixels is incorporated and classified by the Support Vector Machine (SVM) to segment the image into nuclei, cellular clusters, and background. In the second stage, a robust shape deformation framework is proposed, based on Sparse Coding (SC) theory and guided by representative shape features, to construct the cytoplasmic shape of each overlapping cell. Then, the obtained shape is refined by the Distance Regularized Level Set Evolution (DRLSE) model. We evaluated our approach using the ISBI 2014 challenge dataset, which has 135 synthetic cell images for a total of 810 cells. Our results show that our approach outperformed existing approaches in segmenting overlapping cells and obtaining accurate nuclear boundaries. HighlightsA fully automated segmentation method is proposed for overlapping cervical cells.Our approach is based on superpixel-based features and guided shape deformation.Our shape initialization procedure is able to work with the different cell types.The practicality of our approach in segmenting highly overlapping cells is proved.Our approach outperformed existing approaches in nuclei and cytoplasm segmentation.},
journal = {Neurocomput.},
month = jan,
pages = {94–107},
numpages = {14},
keywords = {Distance regularized level set, Feature extraction, Overlapping cervical smear cells, Shape deformation, Sparse coding}
}

@inproceedings{10.5555/3304889.3305098,
author = {Zhang, Xuchao and Zhao, Liang and Chen, Zhiqian and Lu, Chang-Tien},
title = {Distributed self-paced learning in alternating direction method of multipliers},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Self-paced learning (SPL) mimics the cognitive process of humans, who generally learn from easy samples to hard ones. One key issue in SPL is the training process required for each instance weight depends on the other samples and thus cannot easily be run in a distributed manner in a large-scale dataset. In this paper, we reformulate the self-paced learning problem into a distributed setting and propose a novel Distributed Self-Paced Learning method (DSPL) to handle large scale datasets. Specifically, both the model and instance weights can be optimized in parallel for each batch based on a consensus alternating direction method of multipliers. We also prove the convergence of our algorithm under mild conditions. Extensive experiments on both synthetic and real datasets demonstrate that our approach is superior to those of existing methods.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {3148–3154},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jul,
articleno = {15},
numpages = {26},
keywords = {Transfer reinforcement learning, autonomous driving, deep reinforcement learning, policy distillation}
}

@article{10.1016/j.patrec.2019.03.021,
author = {Ding, Songtao and Qu, Shiru and Xi, Yuling and Sangaiah, Arun Kumar and Wan, Shaohua},
title = {Image caption generation with high-level image features},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {123},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.03.021},
doi = {10.1016/j.patrec.2019.03.021},
journal = {Pattern Recogn. Lett.},
month = may,
pages = {89–95},
numpages = {7},
keywords = {Image captioning, Language model, Bottom-up attention mechanism, Faster R-CNN}
}

@article{10.1145/2581376,
author = {Behjati, Razieh and Nejati, Shiva and Briand, Lionel C.},
title = {Architecture-Level Configuration of Large-Scale Embedded Software Systems},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2581376},
doi = {10.1145/2581376},
abstract = {Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {25},
numpages = {43},
keywords = {Model-based product-line engineering, UML/OCL, consistent configuration, constraint satisfaction techniques, formal specification, product configuration}
}

@article{10.1016/j.specom.2021.05.009,
author = {Avila, Anderson R. and O’Shaughnessy, Douglas and Falk, Tiago H.},
title = {Automatic speaker verification from affective speech using Gaussian mixture model based estimation of neutral speech characteristics},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.05.009},
doi = {10.1016/j.specom.2021.05.009},
journal = {Speech Commun.},
month = sep,
pages = {21–31},
numpages = {11},
keywords = {Speaker verification, Affective speech, Intra-speaker variability, Transfer learning}
}

@article{10.4018/IJSI.2021070105,
author = {Jo, Jun-Hyuk and Lee, Jihyun and Jaffari, Aman and Kim, Eunmi},
title = {Fault Localization With Data Flow Information and an Artificial Neural Network},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {3},
issn = {2166-7160},
url = {https://doi.org/10.4018/IJSI.2021070105},
doi = {10.4018/IJSI.2021070105},
abstract = {Fault localization is a technique for identifying the exact source code line with faults. It typically requires a lot of time and cost because, to locate the fault, a developer must track the execution of the failed program line by line. To reduce the fault localization efforts, many methods have been proposed. However, their localized suspicious code range is wide, and their fault localization effect is not high. To cope with this limitation, this paper computes the degree of fault suspiciousness of statements by using an artificial neural network and information of the executed test case, such as statement coverage, execution result, and definition-use pair. Compared to the approach that uses only statement coverage as input data for training an artificial neural network, the experiment results show higher accuracy in 15 types of faults out of 29 real fault types in the approach that the definition-use pair included.},
journal = {Int. J. Softw. Innov.},
month = jul,
pages = {66–78},
numpages = {13},
keywords = {Artificial Neural Network, Data Flow Coverage, Definition-Use, Du-Pair, Fault Localization, Fault Suspiciousness, Software Testing, Software Verification}
}

@inproceedings{10.1007/978-3-030-32248-9_56,
author = {Parvathaneni, Prasanna and Bao, Shunxing and Nath, Vishwesh and Woodward, Neil D. and Claassen, Daniel O. and Cascio, Carissa J. and Zald, David H. and Huo, Yuankai and Landman, Bennett A. and Lyu, Ilwoo},
title = {Cortical Surface Parcellation Using Spherical Convolutional Neural Networks},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_56},
doi = {10.1007/978-3-030-32248-9_56},
abstract = {We present cortical surface parcellation using spherical deep convolutional neural networks. Traditional multi-atlas cortical surface parcellation requires inter-subject surface registration using geometric features with slow processing speed on a single subject (2–3&nbsp;h). Moreover, even optimal surface registration does not necessarily produce optimal cortical parcellation as parcel boundaries are not fully matched to the geometric features. In this context, a choice of training features is important for accurate cortical parcellation. To utilize the networks efficiently, we propose cortical parcellation-specific input data from an irregular and complicated structure of cortical surfaces. To this end, we align ground-truth cortical parcel boundaries and use their resulting deformation fields to generate new pairs of deformed geometric features and parcellation maps. To extend the capability of the networks, we then smoothly morph cortical geometric features and parcellation maps using the intermediate deformation fields. We validate our method on 427 adult brains for 49 labels. The experimental results show that our method outperforms traditional multi-atlas and naive spherical U-Net approaches, while achieving full cortical parcellation in less than a minute.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {501–509},
numpages = {9},
keywords = {Cortical surface parcellation, Spherical deformation, Spherical U-Net, Surface registration},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3241403.3241426,
author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
title = {Model-based support for decision-making in architecture evolution of complex software systems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241426},
doi = {10.1145/3241403.3241426},
abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {21},
numpages = {7},
keywords = {model-based decision support, software architecture evolution, software migration, software variability management, systems-of-systems composition},
location = {Madrid, Spain},
series = {ECSA '18}
}

@article{10.1007/s10618-010-0175-9,
author = {Silla, Carlos N. and Freitas, Alex A.},
title = {A survey of hierarchical classification across different application domains},
year = {2011},
issue_date = {January   2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1–2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-010-0175-9},
doi = {10.1007/s10618-010-0175-9},
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
journal = {Data Min. Knowl. Discov.},
month = jan,
pages = {31–72},
numpages = {42},
keywords = {DAG-structured class hierarchies, Hierarchical classification, Tree-structured class hierarchies}
}

@inproceedings{10.1007/978-3-030-88081-1_16,
author = {Suriani, Vincenzo and Antonioni, Emanuele and Riccio, Francesco and Nardi, Daniele},
title = {Coordination and Cooperation in Robot Soccer},
year = {2021},
isbn = {978-3-030-88080-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88081-1_16},
doi = {10.1007/978-3-030-88081-1_16},
abstract = {Aiming at improving our physical strength and expanding our knowledge, tournaments and competitions have always contributed to our personal growth. Robotics and AI are no exception, and since beginning, competitions have been exploited to improve our understanding of such research areas (e.g. Chess, VideoGames, DARPA). In fact, the research community has launched (and it is involved) in several robotics competitions that provide a two-fold benefit of (i) promoting novel approaches and (ii) valuate proposed solutions systematically and quantitatively. In this paper, we focus on a particular research area of Robotics and AI: we analyze multi-robot systems deployed in a cooperative-adversarial environment being tasked to collaborate to achieve a common goal, while competing against an opposing team. To this end, RoboCup provide the best benchmarking environment by implementing such a challenging problem in the game of soccer. Sports, in fact, represent extremely complex challenge that require a team of robots to show dexterous and fluid movements and to feature high-level cognitive capabilities. Here, we analyse methodologies and approaches to address the problem of coordination and cooperation and we discuss state-of-the-art solutions that achieve effective decision-making processes for multi-robot adversarial scenarios.},
booktitle = {Computational Collective Intelligence: 13th International Conference, ICCCI 2021, Rhodes, Greece, September 29 – October 1, 2021, Proceedings},
pages = {215–227},
numpages = {13},
keywords = {Strategies in robotic games, Robotic competition, Soccer robots RoboCup SPL},
location = {Rhodos, Greece}
}

@article{10.1134/S106423071606006X,
author = {Kayumov, O. R.},
title = {Planning the subminimum-time motion of a robotic manipulator which avoids obstacles},
year = {2016},
issue_date = {November  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {55},
number = {6},
issn = {1064-2307},
url = {https://doi.org/10.1134/S106423071606006X},
doi = {10.1134/S106423071606006X},
abstract = {A method of planning the programmed motion of a manipulator with configuration constraints due to the presence of an obstacle in the working zone is proposed. The constraints on control variables, whose number is equal to the number of freedom degrees, are assumed to be specified beforehand. The planned motions are nearly time-optimal in the same sense, in which a dynamic system can be close to a certain linear decomposed system.},
journal = {J. Comput. Syst. Sci. Int.},
month = nov,
pages = {865–877},
numpages = {13}
}

@article{10.1007/s10922-013-9265-5,
author = {Moens, Hendrik and Truyen, Eddy and Walraven, Stefan and Joosen, Wouter and Dhoedt, Bart and De Turck, Filip},
title = {Cost-Effective Feature Placement of Customizable Multi-Tenant Applications in the Cloud},
year = {2014},
issue_date = {October   2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {4},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9265-5},
doi = {10.1007/s10922-013-9265-5},
abstract = {Cloud computing technologies can be used to more flexibly provision application resources. By exploiting multi-tenancy, instances can be shared between users, lowering the cost of providing applications. A weakness of current cloud offerings however, is the difficulty of creating customizable applications that retain these advantages. In this article, we define a feature-based cloud resource management model, making use of Software Product Line Engineering techniques, where applications are composed of feature instances using a service-oriented architecture. We focus on how resources can be allocated in a cost-effective way within this model, a problem which we refer to as the feature placement problem. A formal description of this problem, that can be used to allocate resources in a cost-effective way, is provided. We take both the cost of failure to place features, and the cost of using servers into account, making it possible to take energy costs or the cost of public cloud infrastructure into consideration during the placement calculation. Four algorithms that can be used to solve the feature placement problem are defined. We evaluate the algorithm solutions, comparing them with the optimal solution determined using an integer linear problem solver, and evaluating the execution times of the algorithms, making use of both generated inputs and a use case based on three applications. We show that, using our approach a higher degree of multi-tenancy can be achieved, and that for the considered scenarios, taking the relationships between features into account and using application-oriented placement performs 25---40 % better than a purely feature-oriented placement.},
journal = {J. Netw. Syst. Manage.},
month = oct,
pages = {517–558},
numpages = {42},
keywords = {Application placement, Cloud computing, Distributed computing, SPLE}
}

@article{10.1504/ijbidm.2021.111744,
author = {Scheidler, Anne Antonia and Rabe, Markus},
title = {Integral verification and validation for knowledge discovery procedure models},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {1},
issn = {1743-8195},
url = {https://doi.org/10.1504/ijbidm.2021.111744},
doi = {10.1504/ijbidm.2021.111744},
abstract = {This paper explains why the knowledge discovery in database (KDD) procedure models lacks verification and validation (V&amp;V) mechanisms and introduces an approach for integral V&amp;V. Based on a generic model for knowledge discovery, a structure named 'KDD triangle model' is presented. This model has a modular design and can be adapted for other KDD procedure models. This has the benefit of allowing existing projects for improving their quality assurance in knowledge discovery. In this paper, the different phases of the developed triangle model for KDD are discussed. One special focus is on the phase results and related testing mechanisms. This paper also describes possible V&amp;V techniques for the developed integral V&amp;V mechanism to ensure direct applicability of the model.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {73–87},
numpages = {14},
keywords = {knowledge discovery in databases, KDD, data mining, procedure model, verification and validation, quality assurance}
}

@inproceedings{10.1007/978-3-030-27544-0_18,
author = {Matamoros, Mauricio and Harbusch, Karin and Paulus, Dietrich},
title = {From Commands to Goal-Based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_18},
doi = {10.1007/978-3-030-27544-0_18},
abstract = {On the one hand, speech is a key aspect to people’s communication. On the other, it is widely acknowledged that language proficiency is related to intelligence. Therefore, intelligent robots should be able to understand, at least, people’s orders within their application domain. These insights are not new in RoboCup@Home, but we lack of a long-term plan to evaluate this approach.In this paper we conduct a brief review of the achievements on automated speech recognition and natural language understanding in RoboCup@Home. Furthermore, we discuss main challenges to tackle in spoken human-robot interaction within the scope of this competition. Finally, we contribute by presenting a pipelined road map to engender research in the area of natural language understanding applied to domestic service robotics.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {217–229},
numpages = {13},
keywords = {Robotic competitions, Natural language understanding, Artificial intelligence and robotics},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1016/j.jss.2017.11.004,
author = {Carvalho, Michelle Larissa Luciano and da Silva, Matheus Lessa Gonalves and Gomes, Gecynalda Soares da Silva and Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and Souza, Magno Lu de Jesus and de Almeida, Eduardo Santana},
title = {On the implementation of dynamic software product lines},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.11.004},
doi = {10.1016/j.jss.2017.11.004},
abstract = {A set of criteria to characterize mechanisms suitable to implement dynamic variability.A characterization of thirteen DSPL-ready variability mechanisms.Empirical evaluation of OOP and AOP from the perspective of DSPL evolution.Evidence showing that AOP is a feasible strategy to implement DSPL projects. Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation.},
journal = {J. Syst. Softw.},
month = feb,
pages = {74–100},
numpages = {27},
keywords = {Dynamic software product lines, Evidence-based software engineering, Software evolution, Variability mechanisms}
}

@article{10.1007/s00521-018-3560-8,
author = {Anwar, Zeeshan and Afzal, Hammad and Bibi, Nazia and Abbas, Haider and Mohsin, Athar and Arif, Omar},
title = {A hybrid-adaptive neuro-fuzzy inference system for multi-objective regression test suites optimization},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3560-8},
doi = {10.1007/s00521-018-3560-8},
abstract = {Regression testing is a mandatory activity of software development life cycle, which is performed to ensure that modifications have not caused any adverse effects on the system’s functionality. With every change in software in the maintenance phase, the size of regression test suite grows as new test cases are written to validate changes. The bigger size of regression test suite makes the testing expensive and time-consuming. Optimization of regression test suite is a possible solution to cope with this problem. Various techniques of optimization have been proposed; however, there is no perfect solution for the problem and therefore, requires better solutions to improve the optimization process. This paper presents a novel technique named as hybrid-adaptive neuro-fuzzy inference system tuned with genetic algorithm and particle swarm optimization algorithm that is used to optimize the regression test suites. Evaluation of the proposed approach is performed on benchmark test suites including “previous date problem” and “Siemens print token.” Experimental results are compared with existing state-of-the-art techniques, and results show that the proposed approach is more effective for the reduction in a regression test suites with higher requirement coverage. The size of regression test suites can be reduced up to 48% using the proposed approach without reducing the fault detection rate.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {7287–7301},
numpages = {15},
keywords = {Regression test suite optimization, Genetic algorithm, Particle swarm algorithm, Adaptive neuro-fuzzy inference system}
}

@inproceedings{10.5555/3367032.3367200,
author = {Terra-Neves, Miguel and Lynce, In\^{e}s and Manquinho, Vasco},
title = {Integrating Pseudo-Boolean constraint reasoning in multi-objective evolutionary algorithms},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Constraint-based reasoning methods thrive in solving problem instances with a tight solution space. On the other hand, evolutionary algorithms are usually effective when it is not hard to satisfy the problem constraints. This dichotomy has been observed in many optimization problems. In the particular case of Multi-Objective Combinatorial Optimization (MOCO), new recently proposed constraint-based algorithms have been shown to outperform more established evolutionary approaches when a given problem instance is hard to satisfy. In this paper, we propose the integration of constraint-based procedures in evolutionary algorithms for solving MOCO. First, a new core-based smart mutation operator is applied to individuals that do not satisfy all problem constraints. Additionally, a new smart improvement operator based on Minimal Correction Subsets is used to improve the quality of the population. Experimental results clearly show that the integration of these operators greatly improves multi-objective evolutionary algorithms MOEA/D and NSGAII. Moreover, even on problem instances with a tight solution space, the newly proposed algorithms outperform the state-of-the-art constraint-based approaches for MOCO.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1184–1190},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1016/j.neucom.2017.02.098,
author = {Zubizarreta, Asier and Larrea, Mikel and Irigoyen, Eloy and Cabanes, Itziar and Portillo, Eva},
title = {Real time direct kinematic problem computation of the 3PRS robot using neural networks},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {271},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.02.098},
doi = {10.1016/j.neucom.2017.02.098},
abstract = {The reliable calculation of the Direct Kinematic Problem (DKP) is one of the main challenges for the implementation of Real-Time (RT) controllers in Parallel Robots. The DKP estimates the pose of the end effector of the robot in terms of the sensors placed on the actuators. However, this calculation requires the use of time-consuming numerical iterative procedures.Artificial Neural Networks have been proposed to implement the complex DKP equation mapping due to their universal approximator property. However, the proposals in this area do not consider the Real Time implementation of the ANN based solution, and no approximation error vs computational time analysis is carried out.In this work, a methodology that uses Artificial Neural Networks (ANNs) to approximate the DKP is proposed. Based on the 3PRS parallel robot, a comprehensive study is carried out in which several network configurations are proposed to approximate the DKP. Moreover, to demonstrate the effectiveness of the approach, the proposed networks are evaluated considering not only their approximation capabilities, but also their Real Time performance in comparison with the traditional iterative procedures used in robotics.},
journal = {Neurocomput.},
month = jan,
pages = {104–114},
numpages = {11},
keywords = {Artificial neural network, Kinematic problem, Parallel robots}
}

@article{10.1016/j.jisa.2014.03.002,
author = {Rahbarinia, Babak and Perdisci, Roberto and Lanzi, Andrea and Li, Kang},
title = {PeerRush},
year = {2014},
issue_date = {July 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {19},
number = {3},
issn = {2214-2126},
url = {https://doi.org/10.1016/j.jisa.2014.03.002},
doi = {10.1016/j.jisa.2014.03.002},
abstract = {In this paper we present PeerRush, a novel system for the identification of unwanted P2P traffic. Unlike most previous work, PeerRush goes beyond P2P traffic detection, and can accurately categorize the detected P2P traffic and attribute it to specific P2P applications, including malicious applications such as P2P botnets. PeerRush achieves these results without the need of deep packet inspection, and can accurately identify applications that use encrypted P2P traffic.We implemented a prototype version of PeerRush and performed an extensive evaluation of the system over a variety of P2P traffic datasets. Our results show that we can detect all the considered types of P2P traffic with up to 99.5% true positives and 0.1% false positives. Furthermore, PeerRush can attribute the P2P traffic to a specific P2P application with a misclassification rate of 0.68% or less.},
journal = {J. Inf. Secur. Appl.},
month = jul,
pages = {194–208},
numpages = {15},
keywords = {Botnets, P2P, Traffic classification}
}

@article{10.1017/S0269888909990051,
author = {Recio-garc\'{\i}a, Juan antonio and D\'{\i}az-agudo, Bel\'{e}n and Gonz\'{a}lez-calero, Pedro antonio},
title = {Semantic templates for case-based reasoning systems},
year = {2009},
issue_date = {September 2009},
publisher = {Cambridge University Press},
address = {USA},
volume = {24},
number = {3},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888909990051},
doi = {10.1017/S0269888909990051},
abstract = {In this paper, we present an approach to solve the drawbacks of manual composition of software components. Our approach is applied within the jcolibri framework for building case-based reasoning (CBR) applications. We propose a system design process based on reusing templates obtained from previously designed CBR systems. Templates store the control flow of the CBR applications and include semantic annotations conceptualizing its behavior and expertise. We use CBR ontology to formalize syntactical, semantical and pragmatical aspects of the reusable components of the framework. The ontology vocabulary facilitates an annotation process of the components and allows to reason about their composition, facilitating the semi-automatic configuration of complex systems from their composing pieces.},
journal = {Knowl. Eng. Rev.},
month = sep,
pages = {245–264},
numpages = {20}
}

@inproceedings{10.5555/2969033.2969059,
author = {Jiang, Lu and Meng, Deyu and Yu, Shoou-I and Lan, Zhenzhong and Shan, Shiguang and Hauptmann, Alexander G.},
title = {Self-paced learning with diversity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2078–2086},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.1007/978-3-030-58607-2_18,
author = {Qi, Yuankai and Pan, Zizheng and Zhang, Shengping and van den Hengel, Anton and Wu, Qi},
title = {Object-and-Action Aware Model for Visual Language Navigation},
year = {2020},
isbn = {978-3-030-58606-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58607-2_18},
doi = {10.1007/978-3-030-58607-2_18},
abstract = {Vision-and-Language Navigation (VLN) is unique in that it requires turning relatively general natural-language instructions into robot agent actions, on the basis of visible environments. This requires to extract value from two very different types of natural-language information. The first is object description (e.g., ‘table’, ‘door’), each presenting as a tip for the agent to determine the next action by finding the item visible in the environment, and the second is action specification (e.g., ‘go straight’, ‘turn left’) which allows the robot to directly predict the next movements without relying on visual perceptions. However, most existing methods pay few attention to distinguish these information from each other during instruction encoding and mix together the matching between textual object/action encoding and visual perception/orientation features of candidate viewpoints. In this paper, we propose an Object-and-Action Aware Model (OAAM) that processes these two different forms of natural language based instruction separately. This enables each process to match object-centered/action-centered instruction to their own counterpart visual perception/action orientation flexibly. However, one side-issue caused by above solution is that an object mentioned in instructions may be observed in the direction of two or more candidate viewpoints, thus the OAAM may not predict the viewpoint on the shortest path as the next action. To handle this problem, we design a simple but effective path loss to penalize trajectories deviating from the ground truth path. Experimental results demonstrate the effectiveness of the proposed model and path loss, and the superiority of their combination with a 50% SPL score on the R2R dataset and a 40% CLS score on the R4R dataset in unseen environments, outperforming the previous state-of-the-art.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X},
pages = {303–317},
numpages = {15},
keywords = {Vision-and-Language Navigation, Modular network, Reward shaping},
location = {Glasgow, United Kingdom}
}

@article{10.1109/TASLP.2014.2359628,
author = {Chen, Austin and Hasegawa-Johnson, Mark A.},
title = {Mixed stereo audio classification using a stereo-input mixed-to-panned level feature},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359628},
doi = {10.1109/TASLP.2014.2359628},
abstract = {Many past studies have been conducted on speech/music discrimination due to the potential applications for broadcast and other media; however, it remains possible to expand the experimental scope to include samples of speech with varying amounts of background music. This paper focuses on the development and evaluation of two measures of the ratio between speech energy and music energy: a reference measure called speech-to-music ratio (SMR), which is known objectively only prior to mixing, and a feature called the stereo-input mix-to-peripheral level feature (SIMPL), which is computed from the stereo mixed signal as an imprecise estimate of SMR. SIMPL is an objective signal measure calculated by taking advantage of broadcast mixing techniques in which vocals are typically placed at stereo center, unlike most instruments. Conversely, SMR is a hidden variable defined by the relationship between the powers of portions of audio attributed to speech and music. It is shown that SIMPL is predictive of SMR and can be combined with state-of-the-art features in order to improve performance. For evaluation, this new metric is applied in speech/music (binary) classification, speech/music/mixed (trinary) classification, and a new speech-to-music ratio estimation problem. Promising results are achieved, including 93.06% accuracy for trinary classification and 3.86 dB RMSE for estimation of the SMR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2025–2033},
numpages = {9},
keywords = {Gaussian mixture model, audio classification, audio processing, audio segmentation, classification algorithms, mel-frequency cepstral coefficients, music information retrieval, music processing, speech processing, speech/music discrimination}
}

@inproceedings{10.1109/ASE.2015.16,
author = {Kowal, Matthias and Tschaikowski, Max and Tribastone, Mirco and Schaefer, Ina},
title = {Scaling size and parameter spaces in variability-aware software performance models},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.16},
doi = {10.1109/ASE.2015.16},
abstract = {In software performance engineering, what-if scenarios, architecture optimization, capacity planning, run-time adaptation, and uncertainty management of realistic models typically require the evaluation of many instances. Effective analysis is however hindered by two orthogonal sources of complexity. The first is the infamous problem of state space explosion---the analysis of a single model becomes intractable with its size. The second is due to massive parameter spaces to be explored, but such that computations cannot be reused across model instances. In this paper, we efficiently analyze many queuing models with the distinctive feature of more accurately capturing variability and uncertainty of execution rates by incorporating general (i.e., non-exponential) distributions. Applying product-line engineering methods, we consider a family of models generated by a core that evolves into concrete instances by applying simple delta operations affecting both the topology and the model's parameters. State explosion is tackled by turning to a scalable approximation based on ordinary differential equations. The entire model space is analyzed in a family-based fashion, i.e., at once using an efficient symbolic solution of a super-model that subsumes every concrete instance. Extensive numerical tests show that this is orders of magnitude faster than a naive instance-by-instance analysis.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {407–417},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1016/j.robot.2009.03.006,
author = {Cherubini, A. and Giannone, F. and Iocchi, L. and Lombardo, M. and Oriolo, G.},
title = {Policy gradient learning for a humanoid soccer robot},
year = {2009},
issue_date = {July, 2009},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {57},
number = {8},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2009.03.006},
doi = {10.1016/j.robot.2009.03.006},
abstract = {In humanoid robotic soccer, many factors, both at low-level (e.g., vision and motion control) and at high-level (e.g., behaviors and game strategies), determine the quality of the robot performance. In particular, the speed of individual robots, the precision of the trajectory, and the stability of the walking gaits, have a high impact on the success of a team. Consequently, humanoid soccer robots require fine tuning, especially for the basic behaviors. In recent years, machine learning techniques have been used to find optimal parameter sets for various humanoid robot behaviors. However, a drawback of learning techniques is time consumption: a practical learning method for robotic applications must be effective with a small amount of data. In this article, we compare two learning methods for humanoid walking gaits based on the Policy Gradient algorithm. We demonstrate that an extension of the classic Policy Gradient algorithm that takes into account parameter relevance allows for better solutions when only a few experiments are available. The results of our experimental work show the effectiveness of the policy gradient learning method, as well as its higher convergence rate, when the relevance of parameters is taken into account during learning.},
journal = {Robot. Auton. Syst.},
month = jul,
pages = {808–818},
numpages = {11},
keywords = {Humanoid robotics, Machine learning, Motion control}
}

@inproceedings{10.1007/978-3-642-12439-6_14,
author = {Brock, Derek and McClimens, Brian and Wasylyshyn, Christina and Trafton, J. Gregory and McCurry, Malcolm},
title = {Evaluating the utility of auditory perspective-taking in robot speech presentations},
year = {2009},
isbn = {3642124380},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12439-6_14},
doi = {10.1007/978-3-642-12439-6_14},
abstract = {In speech interactions, people routinely reason about each other's auditory perspective and change their manner of speaking accordingly, by adjusting their voice to overcome noise or distance, or by pausing for especially loud sounds and resuming when conditions are more favorable for the listener. In this paper we report the findings of a listening study motivated both by this observation and a prototype auditory interface for a mobile robot that monitors the aural parameters of its environment and infers its user's listening requirements. The results provide significant empirical evidence of the utility of simulated auditory perspective taking and the inferred use of loudness and/or pauses to overcome the potential of ambient noise to mask synthetic speech.},
booktitle = {Proceedings of the 6th International Conference on Auditory Display},
pages = {266–286},
numpages = {21},
keywords = {adaptive auditory display, auditory interaction, auditory perspective-taking, human-robot interaction, listening performance, synthetic speech},
location = {Copenhagen, Denmark},
series = {CMMR/ICAD'09}
}

@inproceedings{10.1007/978-3-642-40270-8_4,
author = {Bari, A. T. and Reaz, Mst. Rokeya and Choi, Ho-Jin and Jeong, Byeong-Soo},
title = {DNA Encoding for Splice Site Prediction in Large DNA Sequence},
year = {2013},
isbn = {9783642402692},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40270-8_4},
doi = {10.1007/978-3-642-40270-8_4},
abstract = {Splice site prediction in the pre-mRNA is a very important task for understanding gene structure and its function. To predict splice sites, SVM support vector machine based classification technique is frequently used because of its classification accuracy. High classification accuracy of SVM largely depends on DNA encoding method for feature extraction of DNA sequences. However, existing encoding approaches do not reveal the characteristics of DNA sequence very well enough to provide as much information as DNA sequences have. In this paper, we propose new effective DNA encoding method which can give more information of DNA sequence. Our encoding method can provide density information of each nucleotide along with positional information and chemical property. Extensive performance study shows that our method can provide better performance than existing encoding methods based on several performance criteria such as classification accuracy, sensitivity, specificity and area under receiver operating characteristics curve ROC.},
booktitle = {Proceedings of the 18th International Conference on Database Systems for Advanced Applications - Volume 7827},
pages = {46–58},
numpages = {13},
keywords = {DNA sequence, ROC, gene prediction, nucleotide density, orthogonal encoding, splice site, support vector machine}
}

@inproceedings{10.1007/978-3-642-25085-9_62,
author = {Ribeiro, Bernardete and Gon\c{c}alves, Ivo and Santos, S\'{e}rgio and Kovacec, Alexander},
title = {Deep learning networks for off-line handwritten signature recognition},
year = {2011},
isbn = {9783642250842},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25085-9_62},
doi = {10.1007/978-3-642-25085-9_62},
abstract = {Reliable identification and verification of off-line handwritten signatures from images is a difficult problem with many practical applications. This task is a difficult vision problem within the field of biometrics because a signature may change depending on psychological factors of the individual. Motivated by advances in brain science which describe how objects are represented in the visual cortex, advanced research on deep neural networks has been shown to work reliably on large image data sets. In this paper, we present a deep learning model for off-line handwritten signature recognition which is able to extract high-level representations. We also propose a two-step hybrid model for signature identification and verification improving the misclassification rate in the well-known GPDS database.},
booktitle = {Proceedings of the 16th Iberoamerican Congress Conference on Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
pages = {523–532},
numpages = {10},
keywords = {deep learning, generative models, signature recognition},
location = {Puc\'{o}n, Chile},
series = {CIARP'11}
}

@inproceedings{10.5555/1776814.1776823,
author = {Lappas, Georgios},
title = {Estimating the size of neural networks from the number of available training data},
year = {2007},
isbn = {3540746897},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Estimating a priori the size of neural networks for achieving high classification accuracy is a hard problem. Existing studies provide theoretical upper bounds on the size of neural networks that are unrealistic to implement. This work provides a computational study for estimating the size of neural networks using as an estimation parameter the size of available training data. We will also show that the size of a neural network is problem dependent and that one only needs the number of available training data to determine the size of the required network for achieving high classification rate. We use for our experiments a threshold neural network that combines the perceptron algorithm with simulated annealing and we tested our results on datasets from the UCI Machine Learning Repository. Based on our experimental results, we propose a formula to estimate the number of perceptrons that have to be trained in order to achieve a high classification accuracy.},
booktitle = {Proceedings of the 17th International Conference on Artificial Neural Networks},
pages = {68–77},
numpages = {10},
location = {Porto, Portugal},
series = {ICANN'07}
}

@inproceedings{10.1007/978-3-642-35101-3_70,
author = {Budden, David and Fenn, Shannon and Walker, Josiah and Mendes, Alexandre},
title = {A novel approach to ball detection for humanoid robot soccer},
year = {2012},
isbn = {9783642351006},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35101-3_70},
doi = {10.1007/978-3-642-35101-3_70},
abstract = {The ability to accurately track a ball is a critical issue in humanoid robot soccer, made difficult by processor limitations and resultant inability to process all available data from a high-definition image. This paper proposes a computationally efficient method of determining position and size of balls in a RoboCup environment, and compares the performance to two common methods: one utilising Levenberg-Marquardt least squares circle fitting, and the other utilising a circular Hough transform. The proposed method is able to determine the position of a non-occluded tennis ball with less than 10% error at a distance of 5 meters, and a half-occluded ball with less than 20% error, overall outperforming both compared methods whilst executing 300 times faster than the circular Hough transform method. The proposed method is described fully in the context of a colour based vision system, with an explanation of how it may be implemented independent of system paradigm. An extension to allow tracking of multiple balls utilising unsupervised learning and internal cluster validation is described.},
booktitle = {Proceedings of the 25th Australasian Joint Conference on Advances in Artificial Intelligence},
pages = {827–838},
numpages = {12},
keywords = {Robotics, clustering, computer vision, feature extraction, object recognition, robotic soccer},
location = {Sydney, Australia},
series = {AI'12}
}

@inproceedings{10.1007/978-3-030-00308-1_5,
author = {Rizzi, Caroline and Johnson, Colin G. and Vargas, Patricia A.},
title = {Fear Learning for Flexible Decision Making in RoboCup: A Discussion},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_5},
doi = {10.1007/978-3-030-00308-1_5},
abstract = {In this paper, we address the stagnation of RoboCup competitions in the fields of contextual perception, real-time adaptation and flexible decision-making, mainly in regards to the Standard Platform League (SPL). We argue that our Situation-Aware FEar Learning (SAFEL) model has the necessary tools to leverage the SPL competition in these fields of research, by allowing robot players to learn the behaviour profile of the opponent team at runtime. Later, players can use this knowledge to predict when an undesirable outcome is imminent, thus having the chance to act towards preventing it. We discuss specific scenarios where SAFEL’s associative learning could help to increase the positive outcomes of a team during a soccer match by means of contextual adaptation.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {59–70},
numpages = {12},
keywords = {RoboCup, Cognitive learning, Contextual fear conditioning, Brain emotional model, Affective computing},
location = {Nagoya, Japan}
}

@inproceedings{10.5555/2772879.2773415,
author = {Genter, Katie and Laue, Tim and Stone, Peter},
title = {The RoboCup 2014 SPL Drop-in Player Competition: Encouraging Teamwork without Pre-coordination},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The Standard Platform League is a soccer league at the annual RoboCup world championships in which teams of five humanoid robots play against each other. In 2014, the Drop-in Player Competition was added to the league to serve as a testbed for cooperation without pre-coordination. Instead of homogeneous robot teams that are programmed by each team to implicitly work together, this competition features ad hoc teams, i.e. teams that consist of robots originating from different RoboCup teams and that are each running different software. In this extended abstract, we provide an overview of this competition, including its motivation and rules.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1745–1746},
numpages = {2},
keywords = {ad hoc teamwork, cooperation, robot soccer},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@article{10.3233/KES-170356,
author = {Alidra, Abdelghani and Kimour, Mohamed Tahar},
title = {Adapting large pervasive and context-aware systems. A new evolutionary-based approach},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {2},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-170356},
doi = {10.3233/KES-170356},
abstract = {In order to enable ``anywhere, anytime'' computing, pervasive
systems must autonomously adapt at runtime. The use of dynamic software
product lines has emerged as a promising paradigm where well established
variability management techniques are leveraged at runtime to describe
evolution strategies and adaptation scenarios in terms of combinations of
features. In order to identify the optimal target configuration of the
system under certain circumstances, most existing approaches generate the
set of valid combinations of features and return the best one. Obviously,
while such approaches are well suited to small systems with a reduced number
of configurations, they fail in the case of large modern pervasive systems
because the generation/evaluation of all valid combinations is very costly
in terms of resources and time consumption. In the present article, we
introduce a new scalable, evolutionary-based approach to runtime adaptation
of pervasive systems. To this end, we define the concept of transitive
dependency between features and we exploit it to fasten the generation of
the optimal configuration of the system. We evaluate the scalability of our
proposal by reporting experimental results that show that our genetic
algorithm converges in up to 90% less time than the one from the
literature while preserving the exploration capabilities and solutions
quality. Finally, we illustrate our proposal on the smart homes use case.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {103–121},
numpages = {19},
keywords = {Context-awareness, pervasive systems, online adaptation, dynamic software product lines, features transitive dependencies, genetic algorithms}
}

@article{10.1016/j.patcog.2018.11.028,
author = {Zhao, Lijun and Bai, Huihui and Liang, Jie and Zeng, Bing and Wang, Anhong and Zhao, Yao},
title = {Simultaneous color-depth super-resolution with conditional generative adversarial networks},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.028},
doi = {10.1016/j.patcog.2018.11.028},
journal = {Pattern Recogn.},
month = apr,
pages = {356–369},
numpages = {14},
keywords = {Generative adversarial networks, Super-resolution, Image smoothing, Edge detection}
}

@inproceedings{10.5555/2887007.2887097,
author = {Ermon, Stefano and Xue, Yexiang and Toth, Russell and Dilkina, Bistra and Bernstein, Richard and Damoulas, Theodoros and Clark, Patrick and DeGloria, Steve and Mude, Andrew and Barrett, Christopher and Gomes, Carla P.},
title = {Learning large-scale dynamic discrete choice models of spatio-temporal preferences with application to migratory pastoralism in East Africa},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Understanding spatio-temporal resource preferences is paramount in the design of policies for sustainable development. Unfortunately, resource preferences are often unknown to policy-makers and have to be inferred from data. In this paper we consider the problem of inferring agents' preferences from observed movement trajectories, and formulate it as an Inverse Reinforcement Learning (IRL) problem. With the goal of informing policy-making, we take a probabilistic approach and consider generative models that can be used to simulate behavior under new circumstances such as changes in resource availability, access policies, or climate. We study the Dynamic Discrete Choice (DDC) models from econometrics and prove that they generalize the Max-Entropy IRL model, a widely used probabilistic approach from the machine learning literature. Furthermore, we develop SPL-GD, a new learning algorithm for DDC models that is considerably faster than the state of the art and scales to very large datasets.We consider an application in the context of pastoralism in the arid and semi-arid regions of Africa, where migratory pastoralists face regular risks due to resource availability, droughts, and resource degradation from climate change and development. We show how our approach based on satellite and survey data can accurately model migratory pastoralism in East Africa and that it considerably outperforms other approaches on a large-scale real-world dataset of pastoralists' movements in Ethiopia collected over 3 years.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {644–650},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1007/978-3-642-34327-8_33,
author = {Brugali, Davide and Gherardi, Luca and Biziak, A. and Luzzana, Andrea and Zakharov, Alexey},
title = {A reuse-oriented development process for component-based robotic systems},
year = {2012},
isbn = {9783642343261},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34327-8_33},
doi = {10.1007/978-3-642-34327-8_33},
abstract = {State of the art in robot software development mostly relies on class library reuse and only to a limited extent to component-based design. In the BRICS project we have defined a software development process that is based on the two most recent and promising approaches to software reuse, i.e. Software Product Line (SPL) and Model-Driven Engineering (MDE). The aim of this paper is to illustrate the whole software development process that we have defined for developing flexible and reusable component-based robotics libraries, to exemplify it with the case study of robust navigation functionality, and to present the software tools that we have developed for supporting the proposed process.},
booktitle = {Proceedings of the Third International Conference on Simulation, Modeling, and Programming for Autonomous Robots},
pages = {361–374},
numpages = {14},
location = {Tsukuba, Japan},
series = {SIMPAR'12}
}

@article{10.1007/s11263-018-1134-y,
author = {Deng, Jiankang and Roussos, Anastasios and Chrysos, Grigorios and Ververas, Evangelos and Kotsia, Irene and Shen, Jie and Zafeiriou, Stefanos},
title = {The Menpo Benchmark for Multi-pose 2D and 3D Facial Landmark Localisation and Tracking},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {6–7},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1134-y},
doi = {10.1007/s11263-018-1134-y},
abstract = {In this article, we present the Menpo 2D and Menpo 3D benchmarks, two new datasets for multi-pose 2D and 3D facial landmark localisation and tracking. In contrast to the previous benchmarks such as 300W and 300VW, the proposed benchmarks contain facial images in both semi-frontal and profile pose. We introduce an elaborate semi-automatic methodology for providing high-quality annotations for both the Menpo 2D and Menpo 3D benchmarks. In Menpo 2D benchmark, different visible landmark configurations are designed for semi-frontal and profile faces, thus making the 2D face alignment full-pose. In Menpo 3D benchmark, a united landmark configuration is designed for both semi-frontal and profile faces based on the correspondence with a 3D face model, thus making face alignment not only full-pose but also corresponding to the real-world 3D space. Based on the considerable number of annotated images, we organised Menpo 2D Challenge and Menpo 3D Challenge for face alignment under large pose variations in conjunction with CVPR 2017 and ICCV 2017, respectively. The results of these challenges demonstrate that recent deep learning architectures, when trained with the abundant data, lead to excellent results. We also provide a very simple, yet effective solution, named Cascade Multi-view Hourglass Model, to 2D and 3D face alignment. In our method, we take advantage of all 2D and 3D facial landmark annotations in a joint way. We not only capitalise on the correspondences between the semi-frontal and profile 2D facial landmarks but also employ joint supervision from both 2D and 3D facial landmarks. Finally, we discuss future directions on the topic of face alignment.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {599–624},
numpages = {26},
keywords = {2D face alignment, 3D face alignment, Menpo challenge}
}

@inproceedings{10.5555/1860631.1860647,
author = {Arora, Shilpa and Mayfield, Elijah and Penstein-Ros\'{e}, Carolyn and Nyberg, Eric},
title = {Sentiment classification using automatically extracted subgraph features},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this work, we propose a novel representation of text based on patterns derived from linguistic annotation graphs. We use a subgraph mining algorithm to automatically derive features as frequent subgraphs from the annotation graph. This process generates a very large number of features, many of which are highly correlated. We propose a genetic programming based approach to feature construction which creates a fixed number of strong classification predictors from these subgraphs. We evaluate the benefit gained from evolved structured features, when used in addition to the bag-of-words features, for a sentiment classification task.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text},
pages = {131–139},
numpages = {9},
location = {Los Angeles, California},
series = {CAAGET '10}
}

@article{10.5555/3288443.3288514,
author = {L\"{u}si, Iiris and Bolotnikova, Anastasia and Daneshmand, Morteza and Ozcinar, Cagri and Anbarjafari, Gholamreza},
title = {Optimal image compression via block-based adaptive colour reduction with minimal contour effect},
year = {2018},
issue_date = {Dec 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {23},
issn = {1380-7501},
abstract = {Current image acquisition devices require tremendous amounts of storage for saving the data returned. This paper overcomes the latter drawback through proposing a colour reduction technique which first subdivides the image into patches, and then makes use of fuzzy c-means and fuzzy-logic-based inference systems, in order to cluster and reduce the number of the unique colours present in each patch, iteratively. The colours available in each patch are quantised, and the emergence of false edges is checked for, by means of the Sobel edge detection algorithm, so as to minimise the contour effect. At the compression stage, a methodology taking advantage of block-based singular value decomposition and wavelet difference reduction is adopted. Considering 35000 sample images from various databases, the proposed method outperforms centre cut, moment-preserving threshold, inter-colour correlation, generic K-means and quantisation by dimensionality reduction.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {30939–30968},
numpages = {30},
keywords = {Adaptive colour reduction, Block processing, Colour image processing, Image compression}
}

@inproceedings{10.1145/2911451.2914709,
author = {Roegiest, Adam and Cormack, Gordon V.},
title = {Impact of Review-Set Selection on Human Assessment for Text Classification},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914709},
doi = {10.1145/2911451.2914709},
abstract = {In a laboratory study, human assessors were significantly more likely to judge the same documents as relevant when they were presented for assessment within the context of documents selected using random or uncertainty sampling, as compared to relevance sampling. The effect is substantial and significant [0.54 vs. 0.42, p&lt;0.0002] across a population of documents including both relevant and non-relevant documents, for several definitions of ground truth. This result is in accord with Smucker and Jethani's SIGIR 2010 finding that documents were more likely to be judged relevant when assessed within low-precision versus high-precision ranked lists. Our study supports the notion that relevance is malleable, and that one should take care in assuming any labeling to be ground truth, whether for training, tuning, or evaluating text classifiers.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {861–864},
numpages = {4},
keywords = {assessor error, ediscovery, electronic discovery, evaluation, recall, supervised learning, user study},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1007/978-3-642-02481-8_129,
author = {Delicato, Fl\'{a}via C. and Fuentes, Lidia and G\'{a}mez, Nadia and Pires, Paulo F.},
title = {Variabilities of Wireless and Actuators Sensor Network Middleware for Ambient Assisted Living},
year = {2009},
isbn = {9783642024801},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-02481-8_129},
doi = {10.1007/978-3-642-02481-8_129},
abstract = {Wireless and Actuators Sensor Networks (WSANs) are one of the key technologies for supporting many Ambient Assisted Living applications. WSAN applications development poses new challenges like dealing with diverse low-level programming abstractions and the heterogeneity of nodes with critical resource limitations. Middleware platforms can hide from final developers the complexity of managing different types of hardware and software variability by applying a Software Product Line approach. This paper proposes a &lt;em&gt;family&lt;/em&gt; of middleware for WSANs that can be customized according to the constraints imposed by the particular device, network and applications.},
booktitle = {Proceedings of the 10th International Work-Conference on Artificial Neural Networks: Part II: Distributed Computing, Artificial Intelligence, Bioinformatics, Soft Computing, and Ambient Assisted Living},
pages = {851–858},
numpages = {8},
keywords = {AAL, Middleware, SPL, WSANs},
location = {Salamanca, Spain},
series = {IWANN '09}
}

@article{10.1016/j.cad.2008.05.004,
author = {Yang, Dong and Dong, Ming and Miao, Rui},
title = {Development of a product configuration system with an ontology-based approach},
year = {2008},
issue_date = {August, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {40},
number = {8},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2008.05.004},
doi = {10.1016/j.cad.2008.05.004},
abstract = {Product configuration is a crucial means to implement the mass customization paradigm by assembling a set of customizable components to satisfy both customers' needs and technical constraints. With the aim of enabling efficient and effective development of product configuration systems by reusing configuration knowledge, an ontology-based approach to modeling product configuration knowledge is presented in this paper. The ontology-based product configuration models are hierarchically organized. At the lower level, a configuration meta-model is defined. Based on this meta-model, domain-specific configuration knowledge can be derived by reusing or inheriting the classes or relations in the meta-model. Configuration models are formalized using OWL (Ontology Web Language), an ontology representation language developed by W3C. As a result, configuration models have well-defined semantics due to the logic semantics of OWL, making it possible to automatically detect inconsistencies of configuration knowledge bases. Furthermore, configuration constraints are represented in SWRL, a rule language based on OWL. Finally, actual configuration processes are carried out using JESS, a rule engine for the Java platform, by mapping OWL-based configuration facts and SWRL-based configuration constraints into JESS facts and JESS rules, respectively. The proposed methodology is illustrated with an example for configuring the ranger drilling machine.},
journal = {Comput. Aided Des.},
month = aug,
pages = {863–878},
numpages = {16},
keywords = {OWL, Ontology, Product configuration, Semantic web}
}

@inbook{10.1145/3191315.3191316,
author = {Kifer, Michael and Liu, Yanhong Annie},
title = {Preface},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191316},
abstract = {The idea of this book grew out of a symposium that was held at Stony Brook in September 2012 in celebration of David S. Warren's fundamental contributions to Computer Science and the area of Logic Programming in particular.Logic Programming (LP) is at the nexus of Knowledge Representation, Artificial Intelligence, Mathematical Logic, Databases, and Programming Languages. It is fascinating and intellectually stimulating due to the fundamental interplay among theory, systems, and applications brought about by logic. Logic programs are more declarative in the sense that they strive to be logical specifications of "what" to do rather than "how" to do it, and thus they are high-level and easier to understand and maintain. Yet, without being given an actual algorithm, LP systems implement the logical specifications automatically.Several books cover the basics of LP but focus mostly on the Prolog language with its incomplete control strategy and non-logical features. At the same time, there is generally a lack of accessible yet comprehensive collections of articles covering the key aspects in declarative LP. These aspects include, among others, well-founded vs. stable model semantics for negation, constraints, object-oriented LP, updates, probabilistic LP, and evaluation methods, including top-down vs. bottom-up, and tabling.For systems, the situation is even less satisfactory, lacking accessible literature that can help train the new crop of developers, practitioners, and researchers. There are a few guides on Warren's Abstract Machine (WAM), which underlies most implementations of Prolog, but very little exists on what is needed for constructing a state-of-the-art declarative LP inference engine. Contrast this with the literature on, say, Compilers, where one can first study a book on the general principles and algorithms and then dive in the particulars of a specific compiler. Such resources greatly facilitate the ability to start making meaningful contributions quickly. There is also a dearth of articles about systems that support truly declarative languages, especially those that tie into first-order logic, mathematical programming, and constraint solving.LP helps solve challenging problems in a wide range of application areas, but in-depth analysis of their connection with LP language abstractions and LP implementation methods is lacking. Also, rare are surveys of challenging application areas of LP, such as Bioinformatics, Natural Language Processing, Verification, and Planning.The goal of this book is to help fill in the previously mentioned void in the LP literature. It offers a number of overviews on key aspects of LP that are suitable for researchers and practitioners as well as graduate students. The following chapters in theory, systems, and applications of LP are included.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {xvii–xx}
}

@article{10.1016/j.patcog.2007.06.007,
author = {Li, Yun and Wu, Zhong-Fu},
title = {Fuzzy feature selection based on min-max learning rule and extension matrix},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {1},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2007.06.007},
doi = {10.1016/j.patcog.2007.06.007},
abstract = {In many systems, such as fuzzy neural network, we often adopt the language labels (such as large, medium, small, etc.) to split the original feature into several fuzzy features. In order to reduce the computation complexity of the system after the fuzzification of features, the optimal fuzzy feature subset should be selected. In this paper, we propose a new heuristic algorithm, where the criterion is based on min-max learning rule and fuzzy extension matrix is designed as the search strategy. The algorithm is proved in theory and has shown its high performance over several real-world benchmark data sets.},
journal = {Pattern Recogn.},
month = jan,
pages = {217–226},
numpages = {10},
keywords = {Extension matrix, Feature selection, Fuzzy set theory, Min-max rule}
}

@inproceedings{10.1145/2897695.2897701,
author = {Abilio, Ramon and Vale, Gustavo and Figueiredo, Eduardo and Costa, Heitor},
title = {Metrics for feature-oriented programming},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897701},
doi = {10.1145/2897695.2897701},
abstract = {Feature-oriented programming (FOP) is a programming technique to implement software product lines based on composition mechanisms called refinements. A software product line is a set of software systems that share a common, managed set of features satisfying the specific needs of a particular market segment. The literature reports various software metrics for software product lines developed using object-oriented and aspect-oriented programming. However, after a literature review, we observed that we lack the definition of FOP-specific metrics. Based on this observation, this paper proposes a set of eight novel metrics for feature-oriented programming. These metrics were derived both from our experience in FOP and from existing software metrics. We demonstrate the applicability of the proposed metrics by applying them to a software product line.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {36–42},
numpages = {7},
keywords = {feature-oriented programming, software metrics, software product lines, software quality},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.5555/2888619.2888848,
author = {Rabe, Markus and Dross, Felix},
title = {A reinforcement learning approach for a decision support system for logistics networks},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {This paper presents the architecture and working principles of a Decision Support System (DSS) for logistics networks. The system relies on a data-driven discrete-event simulation model. A brief introduction to Reinforcement Learning (RL) and an explanation of the adoption of RL to the concepts of the DSS is given. An illustration of the realization is presented using a specific aspect of a logistics network. The logistics network is described in a data model which is represented by database tables. The tables are used to dynamically instantiate the simulation model. The authors describe how SQL queries can be used to model actions of an RL agent. A Data Warehouse can be used to measure Key Performance Indicators on the simulation output data of the simulation model, which can be used as a reward criterion for the RL agent. The paper presents a basis for the ongoing development of an RL agent.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {2020–2032},
numpages = {13},
location = {Huntington Beach, California},
series = {WSC '15}
}

@article{10.1016/j.eswa.2013.12.028,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Hierons, Robert M. and Benavides, David and Ruiz-Cort\'{e}s, Antonio},
title = {Automated generation of computationally hard feature models using evolutionary algorithms},
year = {2014},
issue_date = {June, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.12.028},
doi = {10.1016/j.eswa.2013.12.028},
abstract = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {3975–3992},
numpages = {18},
keywords = {Automated analysis, Evolutionary algorithms, Feature models, Performance testing, Search-based testing, Software product lines}
}

@article{10.5555/2608462.2608467,
author = {Przytu\l{}a, \L{}ukasz},
title = {NAO Soccer Robots Path Planning Based on Rough Mereology},
year = {2014},
issue_date = {April 2014},
publisher = {IOS Press},
address = {NLD},
volume = {131},
number = {2},
issn = {0169-2968},
abstract = {Soccer game is a good playground for testing artificial intelligence of robots and methods for spatial reasoning in real conditions. Decision making and path planning are only two of many tasks performed while playing soccer. This paper describes an application of rough mereology introduced by Polkowski and Skowron 1994 for path planning in robotic soccer game. Our path planning method was based on mereological potential fields introduced by Polkowski and O\'{s}mia\l{}owski 2008 and O\'{s}mia\l{}owski 2009 but was redesigned due to conditions of dynamic soccer environment, so an entirely new method was developed.},
journal = {Fundam. Inf.},
month = apr,
pages = {241–251},
numpages = {11},
keywords = {Mereological Potential Field, Nao, Path Planning, Robotics, Rough Mereology, Soccer}
}

@article{10.4018/IJBDCN.2019070106,
author = {Patil, Vilas K and Nagarale, P.P.},
title = {Prediction of L10 and Leq Noise Levels Due to Vehicular Traffic in Urban Area Using ANN and Adaptive Neuro-Fuzzy Interface System (ANFIS) Approach},
year = {2019},
issue_date = {Jul 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-0631},
url = {https://doi.org/10.4018/IJBDCN.2019070106},
doi = {10.4018/IJBDCN.2019070106},
abstract = {Recently in urban areas, road traffic noise is one of the primary sources of noise pollution. Variation in noise level is impacted by the synthesis of traffic and the percentage of heavy vehicles. Presentation to high noise levels may cause serious impact on the health of an individual or community residing near the roadside. Thus, predicting the vehicular traffic noise level is important. The present study aims at the formulation of regression, an artificial neural network (ANN) and an adaptive neuro-fuzzy interface system (ANFIS) model using the data of observed noise levels, traffic volume, and average speed of vehicles for the prediction of L10 and Leq. Measured noise levels are compared to the noise levels predicted by the experimental model. It is observed that the ANFIS approach is more superior when compared to output given by regression and an ANN model. Also, there exists a positive correlation between measured and predicted noise levels. The proposed ANFIS model can be utilized as a tool for traffic direction and planning of new roads in zones of similar land use pattern.},
journal = {Int. J. Bus. Data Commun. Netw.},
month = jul,
pages = {92–105},
numpages = {14},
keywords = {ANFIS, Artificial Neural Network, Modeling, Regression, Vehicular Traffic Noise Prediction}
}

@article{10.1109/MIS.2007.6,
author = {Sinz, Carsten and Haag, Albert and Narodytska, Nina and Walsh, Toby and Gelle, Esther and Sabin, Mihaela and Junker, Ulrich and O'Sullivan, Barry and Rabiser, Rick and Dhungana, Deepak and Grunbacher, Paul and Lehner, Klaus and Federspiel, Christian and Naus, Daniel},
title = {Configuration},
year = {2007},
issue_date = {January 2007},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {22},
number = {1},
issn = {1541-1672},
url = {https://doi.org/10.1109/MIS.2007.6},
doi = {10.1109/MIS.2007.6},
abstract = {Over the years, a whole sector of AI dealing with configuration problems has emerged, and since 1996, an annual configuration workshop has been held in affiliation with a major AI conference. This installment of Trends &amp; Controversies presents essays from the configuration workshop held in August 2006 as part of ECAI in Riva del Garda, Italy.},
journal = {IEEE Intelligent Systems},
month = jan,
pages = {78–90},
numpages = {13},
keywords = {business process, configuration, constraint satisfaction, decision explanation, product variability, product-line engineering}
}

@inproceedings{10.1145/3194133.3194143,
author = {Olaechea, Rafael and Atlee, Joanne and Legay, Axel and Fahrenberg, Uli},
title = {Trace checking for dynamic software product lines},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194143},
doi = {10.1145/3194133.3194143},
abstract = {A key objective of self-adaptive systems is to continue to provide optimal quality of service when the environment changes. A dynamic software product line (DSPL) can benefit from knowing how its various product variants would have performed (in terms of quality of service) with respect to the recent history of inputs. We propose a family-based analysis that simulates all the product variants of a DSPL simultaneously, at runtime, on recent environmental inputs to obtain an estimate of the quality of service that each one of the product variants would have had, provided it had been executing. We assessed the efficiency of our DSPL analysis compared to the efficiency of analyzing each product individually on three case studies. We obtained mixed results due to the explosion of quality-of-service values for the product variants of a DSPL. After introducing a simple data abstraction on the values of quality-of- service variables, our DSPL analysis is between 1.4 and 7.7 times faster than analyzing the products one at a time.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {69–75},
numpages = {7},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@article{10.1007/s10994-016-5570-z,
author = {Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
title = {A topological insight into restricted Boltzmann machines},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {104},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-016-5570-z},
doi = {10.1007/s10994-016-5570-z},
abstract = {Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.},
journal = {Mach. Learn.},
month = sep,
pages = {243–270},
numpages = {28},
keywords = {Complex networks, Deep learning, Scale-free networks, Small-world networks, Sparse restricted Boltzmann machines}
}

@article{10.1007/s11042-020-10103-4,
author = {Ansari, Mohd. Aquib and Singh, Dushyant Kumar},
title = {Human detection techniques for real time surveillance: a comprehensive survey},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10103-4},
doi = {10.1007/s11042-020-10103-4},
abstract = {Real-time detection of humans is an evolutionary research topic. It is an essential and prominent component of various vision based applications. Detection of humans in real-time video sequences is an arduous and challenging task due to various constraints like cluttered environment, occlusion, noise, etc. Many researchers are doing their research in this area and have published the number of researches so far. Determining humans in visual monitoring system is prominent for different types of applications like person detection and identification, fall detection for an elder person, abnormal surveillance, gender classification, crowd analysis, person gait characterization, etc. The main objective of this paper is to provide a comprehensive survey of the various challenges and modern developments seen for human detection methodologies in day vision. This paper consists of an overview of different human detection techniques and their classification based on various underlying factors. The algorithmic technicalities with their applicability to these techniques are deliberated in detail in the manuscript. Different humanitarian imperative factors have also been highlighted for comparative analysis of each human detection methodology. Our survey shows the difference between current research and future requirements.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {8759–8808},
numpages = {50},
keywords = {Human detection, Feature description, Deep convolutional neural networks, Recent progress, Surveillance, Computer vision}
}

@inproceedings{10.1007/978-3-030-49435-3_29,
author = {Reinhartz-Berger, Iris and Abbas, Sameh},
title = {A Variability-Driven Analysis Method for Automatic Extraction of Domain Behaviors},
year = {2020},
isbn = {978-3-030-49434-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49435-3_29},
doi = {10.1007/978-3-030-49435-3_29},
abstract = {Domain engineering focuses on modeling knowledge in an application domain for supporting systematic reuse in the context of complex and constantly evolving systems. Automatically supporting this task is challenging; most existing methods assume high similarity of variants which limits reuse of the generated domain artifacts, or provide very low-level features rather than actual domain features. As a result, these methods are limited in handling common scenarios such as similarly behaving systems developed by different teams, or merging existing products. To address this gap, we propose a method for extracting domain knowledge in the form of domain behaviors, building on a previously developed framework for behavior-based variability analysis among class operations. Machine learning techniques are applied for identifying clusters of operations that can potentially form domain behaviors. The approach is evaluated on a set of open-source video games, named apo-games.},
booktitle = {Advanced Information Systems Engineering: 32nd International Conference, CAiSE 2020, Grenoble, France, June 8–12, 2020, Proceedings},
pages = {467–481},
numpages = {15},
keywords = {Domain engineering, Systematic reuse, Variability analysis},
location = {Grenoble, France}
}

@inproceedings{10.5555/3382225.3382412,
author = {Xylogiannopoulos, Konstantinos F.},
title = {From data points to data curves: a new approach on big data curves clustering},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {In the new era of IoT, enormous real-values datasets are produced daily. Time series created by smart devices, financial data, weather analysis, medical applications, traffic control etc. become more and more important in human day life. Analyzing and clustering these time series or in general any kind of curve could be critical. In the current paper, a new methodology (BD2C) is presented, which applies text mining and pattern detection techniques in order to cluster curves according to their shape. Several experiments have been conducted on artificial and real datasets in order to present the accuracy, efficiency and rapid discovery of the best possible clustering that the proposed methodology can achieve.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {881–884},
numpages = {4},
keywords = {ARPaD, BD2C, LERP-RSA, curve clustering, multivariate data analytics, pattern detection},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1016/j.engappai.2011.09.026,
author = {Dumitrache, Alexandru and Borangiu, Theodor},
title = {IMS10-image-based milling toolpaths with tool engagement control for complex geometry},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {25},
number = {6},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2011.09.026},
doi = {10.1016/j.engappai.2011.09.026},
abstract = {This paper presents a NC toolpath generation strategy with tool engagement control for arbitrarily complex discrete part geometry, which reduces machining time and tool wear and can be used in high speed machining. The toolpath computation is based on image models for design part, raw stock and cutting tool, and involves pixel-based simulation of the milling process. Simulation results and comparison with existing methods are presented.},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {1161–1172},
numpages = {12},
keywords = {CNC milling, Image-based CNC toolpaths, NC adaptive high speed machining, Tool engagement control}
}

@inproceedings{10.5555/3540261.3540835,
author = {Yao, Huaxiu and Wang, Yu and Wei, Ying and Zhao, Peilin and Mahdavi, Mehrdad and Lian, Defu and Finn, Chelsea},
title = {Meta-learning with an adaptive task scheduler},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To benefit the learning of a new task, meta-learning has been proposed to transfer a well-generalized meta-model learned from various meta-training tasks. Existing meta-learning algorithms randomly sample meta-training tasks with a uniform probability, under the assumption that tasks are of equal importance. However, it is likely that tasks are detrimental with noise or imbalanced given a limited number of meta-training tasks. To prevent the meta-model from being corrupted by such detrimental tasks or dominated by tasks in the majority, in this paper, we propose an adaptive task scheduler (ATS) for the meta-training process. In ATS, for the first time, we design a neural scheduler to decide which meta-training tasks to use next by predicting the probability being sampled for each candidate task, and train the scheduler to optimize the generalization capacity of the meta-model to unseen tasks. We identify two meta-model-related factors as the input of the neural scheduler, which characterize the difficulty of a candidate task to the meta-model. Theoretically, we show that a scheduler taking the two factors into account improves the meta-training loss and also the optimization landscape. Under the setting of meta-learning with noise and limited budgets, ATS improves the performance on both miniImageNet and a real-world drug discovery benchmark by up to 13% and 18%, respectively, compared to state-of-the-art task schedulers.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {574},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Cloud computing, Cloud service selection, Decision-making}
}

@inproceedings{10.1109/MODELS.2017.22,
author = {Taentzer, Gabriele and Salay, Rick and Str\"{u}ber, Daniel and Chechik, Marsha},
title = {Transformations of software product lines: a generalizing framework based on category theory},
year = {2017},
isbn = {9781538634929},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS.2017.22},
doi = {10.1109/MODELS.2017.22},
abstract = {Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.},
booktitle = {Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
pages = {101–111},
numpages = {11},
location = {Austin, Texas},
series = {MODELS '17}
}

@article{10.1007/s11042-019-7381-2,
author = {Dad, Nisrine and En-nahnahi, Noureddine and El Alaoui Ouatik, Said},
title = {Quaternion Harmonic moments and extreme learning machine for color object recognition},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7381-2},
doi = {10.1007/s11042-019-7381-2},
abstract = {The quaternary orthogonal moments have been widely used as color image descriptors owe to their remarkable color and shape information encapsulation capability. Their computation, however, depends on finding the optimal value of a unit pure quaternion parameter, which is done empirically and with no warranty of optimality. We propose a 2D color object recognition method that relies on the quaternion-valued parameter-free disc-harmonic moment invariants (QHMs) fed into the quaternion extreme learning machine (QELM). The role of this latter is to maintain the correlation between the four parts, real and imaginary, of the quaternary descriptor coefficients. Several datasets are used for recognition experiments. We draw the conclusion that: (1) our quaternion-valued QHMs invariants outperform other quaternary moments, (2) the quaternion-valued moment invariants give results better than the modulus-based moment invariants and (3) the QELM yields results better than the state-of-the-art classifiers.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {20935–20959},
numpages = {25},
keywords = {Quaternion algebra, Color image feature extraction, Disc-Harmonic moments, Zernike moments, Spherical harmonics, Color object recognition, Back-propagation neural networks, Extreme learning machine}
}

@article{10.1007/s11265-021-01676-w,
author = {Ting, Yu-Ching and Lo, Fang-Wen and Tsai, Pei-Yun},
title = {Implementation for Fetal ECG Detection from Multi-channel Abdominal Recordings with 2D Convolutional Neural Network},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {9},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01676-w},
doi = {10.1007/s11265-021-01676-w},
abstract = {A convolutional neural network (CNN)-based approach for fetal ECG detection from the abdominal ECG recording is proposed. The flow contains a pre-processing phase and a classification phase. In the pre-processing phase, short-time Fourier transform is applied to obtain the spectrogram, which is sent to 2D CNN for classification. The classified results from multiple channels are then fused and high detection accuracy up to 95.2% is achieved and the CNN-based approach outperforms the conventional algorithm. The hardware of this fetal ECG detector composed of the spectrogram processor and 2D CNN classifier is then implemented on the FPGA platform. Because the two dimensions of the spectrogram and the kernel are asymmetric, a pre-fetch mechanism is designed to eliminate the long latency resulted from data buffering for large-size convolution. From the implementation results, it takes 20258 clock cycles for inference and almost 50% computation cycles are reduced. The power consumption is 12.33mW at 324KHz and 1V for real-time operations. The implementation demonstrates the feasibility of real-time applications in wearable devices.},
journal = {J. Signal Process. Syst.},
month = sep,
pages = {1101–1113},
numpages = {13},
keywords = {Fetal electrocardiogram, Convolutional neural network, Short-time Fourier transform}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@article{10.1016/j.knosys.2020.106660,
author = {Liu, Zhen and Feng, Xiaodong and Wang, Yecheng and Zuo, Wenbo},
title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106660},
doi = {10.1016/j.knosys.2020.106660},
journal = {Know.-Based Syst.},
month = feb,
numpages = {12},
keywords = {Recommendation, Deep learning, Noisy and outlier corruption, Instance weighting, Self-paced learning}
}

@inproceedings{10.5555/3304222.3304357,
author = {Liu, Cao and He, Shizhu and Liu, Kang and Zhao, Jun},
title = {Curriculum learning for natural answer generation},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpora. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpora could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-art, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4223–4229},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1016/j.jbi.2013.05.005,
author = {Zhu, Qian and Freimuth, Robert R. and Pathak, Jyotishman and Durski, Matthew J. and Chute, Christopher G.},
title = {Disambiguation of PharmGKB drug-disease relations with NDF-RT and SPL},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {46},
number = {4},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2013.05.005},
doi = {10.1016/j.jbi.2013.05.005},
abstract = {We disambiguate PharmGKB drug and disease associations by NDF-RT and SPL.Detailed clinical associations are clearly represented in PharmGKB.The work helps to understand drug and disease relations in details from PharmGKB.Reveals standardized drug information will accelerate clinical drug integration. PharmGKB is a leading resource of high quality pharmacogenomics data that provides information about how genetic variations modulate an individual's response to drugs. PharmGKB contains information about genetic variations, pharmacokinetic and pharmacodynamic pathways, and the effect of variations on drug-related phenotypes. These relationships are represented using very general terms, however, and the precise semantic relationships among drugs, and diseases are not often captured. In this paper we develop a protocol to detect and disambiguate general clinical associations between drugs and diseases using more precise annotation terms from other data sources. PharmGKB provides very detailed clinical associations between genetic variants and drug response, including genotype-specific drug dosing guidelines, and this procedure will armGKB. The availability of more detailed data will help investigators to conduct more precise queries, such as finding particular diseases caused or treated by a specific drug.We first mapped drugs extracted from PharmGKB drug-disease relationships to those in the National Drug File Reference Terminology (NDF-RT) and to Structured Product Labels (SPLs). Specifically, we retrieved drug and disease role relationships describing and defining concepts according to their relationships with other concepts from NDF-RT. We also used the NCBO (National Center for Biomedical Ontology) annotator to annotate disease terms from the free text extracted from five SPL sections (indication, contraindication, ADE, precaution, and warning). Finally, we used the detailed drug and disease relationship information from NDF-RT and the SPLs to annotate and disambiguate the more general PharmGKB drug and disease associations.},
journal = {J. of Biomedical Informatics},
month = aug,
pages = {690–696},
numpages = {7},
keywords = {Clinical associations, NDF-RT, PharmGKB, Pharmacogenomics, SPL}
}

@inproceedings{10.1145/3474085.3475471,
author = {Huang, Zongmo and Ren, Yazhou and Pu, Xiaorong and He, Lifang},
title = {Non-Linear Fusion for Self-Paced Multi-View Clustering},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475471},
doi = {10.1145/3474085.3475471},
abstract = {With the advance of the multi-media and multi-modal data, multi-view clustering (MVC) has drawn increasing attentions recently. In this field, one of the most crucial challenges is that the characteristics and qualities of different views usually vary extensively. Therefore, it is essential for MVC methods to find an effective approach that handles the diversity of multiple views appropriately. To this end, a series of MVC methods focusing on how to integrate the loss from each view have been proposed in the past few years. Among these methods, the mainstream idea is assigning weights to each view and then combining them linearly. In this paper, inspired by the effectiveness of non-linear combination in instance learning and the auto-weighted approaches, we propose Non-Linear Fusion for Self-Paced Multi-View Clustering (NSMVC), which is totally different from the the conventional linear-weighting algorithms. In NSMVC, we directly assign different exponents to different views according to their qualities. By this way, the negative impact from the corrupt views can be significantly reduced. Meanwhile, to address the non-convex issue of the MVC model, we further define a novel regularizer-free modality of Self-Paced Learning (SPL), which fits the proposed non-linear model perfectly. Experimental results on various real-world data sets demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3211–3219},
numpages = {9},
keywords = {multi-view clustering, non-linear fusion, self-paced learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1016/j.patcog.2016.11.018,
author = {Ma, Xiaolong and Zhu, Xiatian and Gong, Shaogang and Xie, Xudong and Hu, Jianming and Lam, Kin-Man and Zhong, Yisheng},
title = {Person re-identification by unsupervised video matching},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {65},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.018},
doi = {10.1016/j.patcog.2016.11.018},
journal = {Pattern Recogn.},
month = may,
pages = {197–210},
numpages = {14},
keywords = {Person re-identification, Action recognition, Gait recognition, Video matching, Temporal sequence matching, Spatio-temporal pyramids, Time shift}
}

@article{10.1007/s11042-017-5172-1,
author = {Ma, Xueqi and Tao, Dapeng and Liu, Weifeng},
title = {Effective human action recognition by combining manifold regularization and pairwise constraints},
year = {2019},
issue_date = {May       2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {10},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5172-1},
doi = {10.1007/s11042-017-5172-1},
abstract = {The ever-growing popularity of mobile networks and electronics has prompted intensive research on multimedia data (e.g. text, image, video, audio, etc.) management. This leads to the researches of semi-supervised learning that can incorporate a small number of labeled and a large number of unlabeled data by exploiting the local structure of data distribution. Manifold regularization and pairwise constraints are representative semi-supervised learning methods. In this paper, we introduce a novel local structure preserving approach by considering both manifold regularization and pairwise constraints. Specifically, we construct a new graph Laplacian that takes advantage of pairwise constraints compared with the traditional Laplacian. The proposed graph Laplacian can better preserve the local geometry of data distribution and achieve the effective recognition. Upon this, we build the graph regularized classifiers including support vector machines and kernel least squares as special cases for action recognition. Experimental results on a multimodal human action database (CAS-YNU-MHAD) show that our proposed algorithms outperform the general algorithms.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {13313–13329},
numpages = {17},
keywords = {Action recognition, Local structure preserving, Manifold regularization, Pairwise constraints}
}

@inproceedings{10.1109/CEC.2017.7969429,
author = {Oliveira, Mariana and Borguesan, Bruno and Dorn, Marcio},
title = {SADE-SPL: A Self-Adapting Differential Evolution algorithm with a loop Structure Pattern Library for the PSP problem},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC.2017.7969429},
doi = {10.1109/CEC.2017.7969429},
abstract = {The knowledge about the conformation of a protein molecule allows the inference and study of its biological function. Because protein function is determined by its shape and the physio-chemical properties of its exposed surface, it is extremely important to predict accurate protein models. One of the hardest problems in Structural Bioinformatics is associated with the prediction of the three-dimensional structure of a protein only from its amino acid sequence (primary structure). Coils and turns are both elements of secondary structure in proteins where the polypeptide chain reverses its overall direction; These structures are considered the most difficult secondary structure to be predicted. In this paper, we propose a loop Structure Pattern Library (SPL) which was created using experimental information extracted from Protein Data Bank aiming to constrain the conformational search space of proteins. The Self-Adapting Differential Evolution (SADE) meta-heuristic was implemented for the tertiary protein structure prediction problem using the Structure Pattern Library as knowledge. The SADE algorithm was tested with (SADE-SPL) and without the Structure Pattern Library. Archived results show that the lowest Root Mean Square Deviation values were obtained when the Structure Pattern Library was employed. Average GDT TS were higher in all SADE-SPL cases. Thereby, our results allow us to state that SPL application knowledge in SADE meta-heuristic is capable of predicting three-dimensional protein structures closer to experimental structures than SADE application without SPL.},
booktitle = {2017 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1095–1102},
numpages = {8},
location = {Donostia, San Sebasti\'{a}n, Spain}
}

@inproceedings{10.5555/3297863.3298017,
author = {Joshi, Aditya and Kanojia, Diptesh and Bhattacharyya, Pushpak and Carman, Mark},
title = {Sarcasm suite: a browser-based engine for sarcasm detection and generation},
year = {2017},
publisher = {AAAI Press},
abstract = {Sarcasm Suite is a browser-based engine that deploys five of our past papers in sarcasm detection and generation. The sarcasm detection modules use four kinds of incongruity: sentiment incongruity, semantic incongruity, historical context incongruity and conversational context incongruity. The sarcasm generation module is a chatbot that responds sarcastically to user input. With a visually appealing interface that indicates predictions using 'faces' of our co-authors from our past papers, Sarcasm Suite is our first demonstration of our work in computational sarcasm.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {5095–5096},
numpages = {2},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1016/j.eswa.2014.10.026,
author = {Arsene, Octavian and Dumitrache, Ioan and Mihu, Ioana},
title = {Expert system for medicine diagnosis using software agents},
year = {2015},
issue_date = {March 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.026},
doi = {10.1016/j.eswa.2014.10.026},
abstract = {In order to simplify the information exchange within the medical diagnosis process, a collaborative software agents framework is presented.The human body systems (e.g. respiratory, cardiovascular) are embedded into distinct software agents.The automated information exchange between different medicine specialists from different areas of expertise.The framework has three key components: knowledge management, uncertainty reasoning and software agents. In order to simplify the information exchange within the medical diagnosis process, a collaborative software agents framework is presented. The human body systems (e.g. respiratory, cardiovascular) are embedded into distinct software agents. The holistic perspective is given by the all connected agents exchanging information. The purpose of the framework is to allow the automated information exchange between different medicine specialists. The key factor of the exchange is sharing concepts between the areas of expertise. Each human body system expert will act over his concepts (evidences, causes, effects), however the information from other systems will be assimilated as well. The framework has three key components: knowledge management, uncertainty reasoning and software agents. The ontology is chosen to address the management of human body systems knowledge. The Bayesian Network is the graphical model for probabilistic knowledge representation and reasoning about partial beliefs under uncertainty. The software agents, as collaboration framework, are in charge of the belief propagation between system instances.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1825–1834},
numpages = {10},
keywords = {Bayesian Network, Ontology, Software agents}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {automated testing, e-commerce, experience report, feature modelling, variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1016/j.neucom.2018.04.001,
author = {Lu, Quanmao and Li, Xuelong and Dong, Yongsheng},
title = {Structure preserving unsupervised feature selection},
year = {2018},
issue_date = {August 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {301},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.001},
doi = {10.1016/j.neucom.2018.04.001},
abstract = {Spectral analysis was usually used to guide unsupervised feature selection. However, the performances of these methods are not always satisfactory due to that they may generate continuous pseudo labels to approximate the discrete real labels. In this paper, a novel unsupervised feature selection method is proposed based on self-expression model. Unlike existing spectral analysis based methods, we utilize self-expression model to capture the relationships between the features without learning the cluster labels. Specifically, each feature can be reconstructed by using a linear combination of all the features in the original feature space, and a representative feature should give a large weight to reconstruct other features. Besides, a structure preserved constraint is incorporated into our model for keeping the local manifold structure of the data. Then an efficient alternative iterative algorithm is utilized to solve our proposed model with the theoretical analysis on its convergence. The experimental results on different datasets show the effectiveness of our method.},
journal = {Neurocomput.},
month = aug,
pages = {36–45},
numpages = {10},
keywords = {Self-expression model, Structure preserving, Unsupervised feature selection}
}

@article{10.1016/j.specom.2015.01.003,
author = {Sadeghian, Amir and Dajani, Hilmi R. and Chan, Adrian D.C.},
title = {Classification of speech-evoked brainstem responses to English vowels},
year = {2015},
issue_date = {April 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2015.01.003},
doi = {10.1016/j.specom.2015.01.003},
abstract = {We investigated the automatic classification of speech-evoked brainstem responses.Responses to five vowels were classified based on onset and sustained features.Combined sustained features gave a classification accuracy of 83.33%.Classification accuracy with onset response features was better than chance.Vowel-specific information in responses may be useful for fitting hearing aids. This study investigated whether speech-evoked auditory brainstem responses (speech ABRs) can be automatically separated into distinct classes. With five English synthetic vowels, the speech ABRs were classified using linear discriminant analysis based on features contained in the transient onset response, the sustained envelope following response (EFR), and the sustained frequency following response (FFR). EFR contains components mainly at frequencies well below the first formant, while the FFR has more energy around the first formant. Accuracies of 83.33% were obtained for combined EFR and FFR features and 38.33% were obtained for transient response features. The EFR features performed relatively well with a classification accuracy of 70.83% despite the belief that vowel discrimination is primarily dependent on the formants. The FFR features obtained a lower accuracy of 59.58% possibly because the second formant is not well represented in all the responses. Moreover, the classification accuracy based on the transient features exceeded chance level which indicates that the initial response transients contain vowel specific information. The results of this study will be useful in a proposed application of speech ABR to objective hearing aid fitting, if the separation of the brain's responses to different vowels is found to be correlated with perceptual discrimination.},
journal = {Speech Commun.},
month = apr,
pages = {69–84},
numpages = {16},
keywords = {Auditory processing of speech, Classification of evoked responses, Envelope following response, Fitting hearing aids, Frequency following response, Speech-evoked auditory brainstem response}
}

@article{10.5555/1644520.1644705,
author = {Boutilier, Craig and Patrascu, Relu and Poupart, Pascal and Schuurmans, Dale},
title = {Constraint-based optimization and utility elicitation using the minimax decision criterion},
year = {2006},
issue_date = {June, 2006},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {170},
number = {8–9},
issn = {0004-3702},
abstract = {In many situations, a set of hard constraints encodes the feasible configurations of some system or product over which multiple users have distinct preferences. However, making suitable decisions requires that the preferences of a specific user for different configurations be articulated or elicited, something generally acknowledged to be onerous. We address two problems associated with preference elicitation: computing a best feasible solution when the user's utilities are imprecisely specified; and developing useful elicitation procedures that reduce utility uncertainty, with minimal user interaction, to a point where (approximately) optimal decisions can be made. Our main contributions are threefold. First, we propose the use of minimax regret as a suitable decision criterion for decision making in the presence of such utility function uncertainty. Second, we devise several different procedures, all relying on mixed integer linear programs, that can be used to compute minimax regret and regret-optimizing solutions effectively. In particular, our methods exploit generalized additive structure in a user's utility function to ensure tractable computation. Third, we propose various elicitation methods that can be used to refine utility uncertainty in such a way as to quickly (i.e., with as few questions as possible) reduce minimax regret. Empirical study suggests that several of these methods are quite successful in minimizing the number of user queries, while remaining computationally practical so as to admit real-time user interaction.},
journal = {Artif. Intell.},
month = jun,
pages = {686–713},
numpages = {28},
keywords = {Constraint satisfaction, Decision theory, Imprecise utility, Minimax regret, Optimization, Preference elicitation}
}

@article{10.1007/s10772-016-9367-z,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with Parkinson's diseases using PCA and NPCA},
year = {2016},
issue_date = {December  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-016-9367-z},
doi = {10.1007/s10772-016-9367-z},
abstract = {In this study, we wanted to discriminate between two groups of people. The database used in this study contains 20 patients with Parkinson's disease and 20 healthy people. Three types of sustained vowels (/a/, /o/ and /u/) were recorded from each participant and then the analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used linear and nonlinear feature extraction techniques, principal component analysis (PCA), and nonlinear PCA. These techniques reduce the number of parameters and choose the most effective acoustic features used for classification. Support vector machine with its different kernel was used for classification. We obtained an accuracy up to 87.50 % for discrimination between PD patients and healthy people.},
journal = {Int. J. Speech Technol.},
month = dec,
pages = {743–754},
numpages = {12},
keywords = {Feature selection, NPCA, PCA, Parkinson's disease, SVM}
}

@inproceedings{10.1145/3340531.3412773,
author = {Gallagher, Luke and Mallia, Antonio and Culpepper, J. Shane and Suel, Torsten and Cambazoglu, B. Barla},
title = {Feature Extraction for Large-Scale Text Collections},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412773},
doi = {10.1145/3340531.3412773},
abstract = {Feature engineering is a fundamental but poorly documented component in Learning-to-Rank (LTR) search engines. Such features are commonly used to construct learning models for web and product search engines, recommender systems, and question-answering tasks. In each of these domains, there is a growing interest in the creation of open-access test collections that promote reproducible research. However, there are still few open-source software packages capable of extracting high-quality machine learning features from large text collections. Instead, most feature-based LTR research relies on "canned" test collections, which often do not expose critical details about the underlying collection or implementation details of the extracted features. Both of these are crucial to collection creation and deployment of a search engine into production. So in this regard, the experiments are rarely reproducible with new features or collections, or helpful for companies wishing to deploy LTR systems.  In this paper, we introduce Fxt, an open-source framework to perform efficient and scalable feature extraction. Fxt can easily be integrated into complex, high-performance software applications to help solve a wide variety of text-based machine learning problems. To demonstrate the software's utility, we build and document a reproducible feature extraction pipeline and show how to recreate several common LTR experiments using the ClueWeb09B collection. Researchers and practitioners can benefit from Fxt to extend their machine learning pipelines for various text-based retrieval tasks, and learn how some static document features and query-specific features are implemented.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3015–3022},
numpages = {8},
keywords = {clueweb, feature extraction, feature importance, feature index, feature repository, lambdamart, learning to rank, ltr},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00026,
author = {Shahin, Ramy},
title = {Towards modal software engineering},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00026},
doi = {10.1109/ICSE-NIER52604.2021.00026},
abstract = {In this paper we introduce the notion of Modal Software Engineering: automatically turning sequential, deterministic programs into semantically equivalent programs efficiently operating on inputs coming from multiple overlapping worlds. We are drawing an analogy between modal logics, and software application domains where multiple sets of inputs (multiple worlds) need to be processed efficiently. Typically those sets highly overlap, so processing them independently would involve a lot of redundancy, resulting in lower performance, and in many cases intractability. Three application domains are presented: reasoning about feature-based variability of Software Product Lines (SPLs), probabilistic programming, and approximate programming.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {86–90},
numpages = {5},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@article{10.1016/j.camwa.2013.06.027,
author = {Borges, Helyane Bronoski and Silla, Carlos N. and Nievola, J\'{u}lio Cesar},
title = {An evaluation of global-model hierarchical classification algorithms for hierarchical classification problems with single path of labels},
year = {2013},
issue_date = {December, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {10},
issn = {0898-1221},
url = {https://doi.org/10.1016/j.camwa.2013.06.027},
doi = {10.1016/j.camwa.2013.06.027},
abstract = {Several classification tasks in different application domains can be seen as hierarchical classification problems. In order to deal with hierarchical classification problems, the use of existing flat classification approaches is not appropriate. For these reason, there has been a growing number of studies focusing on the development of novel algorithms able to induce classification models for hierarchical classification problems. In this paper we study the performance of a novel algorithm called Hierarchical Classification using a Competitive Neural Network (HC-CNN) and compare its performance against the Global-Model Naive Bayes (GMNB) on eight protein function prediction datasets. Interestingly enough, the comparison of two global-model hierarchical classification algorithms for single path of labels hierarchical classification problems has never been done before.},
journal = {Comput. Math. Appl.},
month = dec,
pages = {1991–2002},
numpages = {12},
keywords = {Global approach, Hierarchical classification}
}

@article{10.1007/s00521-019-04435-y,
author = {Kolokas, Nikolaos and Drosou, Anastasios and Tzovaras, Dimitrios},
title = {Text synthesis from keywords: a comparison of recurrent-neural-network-based architectures and hybrid approaches},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04435-y},
doi = {10.1007/s00521-019-04435-y},
abstract = {This paper concerns an application of recurrent neural networks to text synthesis in the word level, with the help of keywords. First, a Parts Of Speech tagging library is employed to extract verbs and nouns from the texts used in our work, a part of which are then considered, after automatic eliminations, as the aforementioned keywords. Our ultimate aim is to train a recurrent neural network to map the keyword sequence of a text to the entire text. Successive reformulations of the keyword and full-text word sequences are performed, so that they can serve as the input and target of the network as efficiently as possible. The predicted texts are understandable enough, and the model performance depends on the problem difficulty, determined by the percentage of full-text words that are considered as keywords, that ranges from 1/3 to 1/2 approximately, the training memory cost, mainly affected by the network architecture, as well as the similarity between different texts, which determines the best architecture.},
journal = {Neural Comput. Appl.},
month = may,
pages = {4259–4274},
numpages = {16},
keywords = {Deep machine learning, Sequence modeling, Natural language processing, Text mining}
}

@article{10.1007/s00165-013-0276-5,
author = {Sampath, Prahladavaradan},
title = {An elementary theory of product-line variations},
year = {2014},
issue_date = {Jul 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-013-0276-5},
doi = {10.1007/s00165-013-0276-5},
abstract = {The primary aim of a software product-line is to maximise reuse of software components by managing the variability in component functionalities and product configurations. Feature oriented domain analysis (FODA) diagrams are a formalism for modelling the variability in a software product-line, and are used as a tool for managing a product-line and planning its evolution. This paper presents an elementary theory of variations in a product-line, leading up to a technique for extracting FODA diagrams from legacy product-lines. The theory is elementary in the sense that it is built using very simple mathematical structures, making minimal assumptions on the structure of product-lines. Examples drawn from the automotive domain are used to illustrate the theoretical developments.},
journal = {Form. Asp. Comput.},
month = jul,
pages = {695–727},
numpages = {33},
keywords = {SPLE, FODA, Formal concept analysis, Lattice theory}
}

@inproceedings{10.1007/978-3-030-34585-3_18,
author = {Serra, Angela and della Pietra, Antonio and Herdener, Marcus and Tagliaferri, Roberto and Esposito, Fabrizio},
title = {Automatic Discrimination of Auditory Stimuli Perceived by the Human Brain},
year = {2018},
isbn = {978-3-030-34584-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34585-3_18},
doi = {10.1007/978-3-030-34585-3_18},
abstract = {Humans are able to perceive small difference of sound frequency but it is still unknown how the difference in frequency information is represented at the level of the primary sensory cortex. Indeed, analysis of fMRI imaging identified tonotopic maps through the auditory pathways to the primary sensory cortex. These maps are unfortunately too coarse to show ultra-fine discrimination. Then, the hypothesis is that this small frequency differences are recognised thanks to the information coming from a large set of auditory neurons. To investigate this possibility, a multi-voxel pattern discriminating analysis of the response of BOLD-fMRI in the bilateral auditory cortex to tonal stimuli with different shift in frequency was performed. Our results suggest that small shifts in the frequency are easily classified compared with big shifts and that multiple areas of the auditory cortex are involved in the tone recognition.},
booktitle = {Computational Intelligence Methods for Bioinformatics and Biostatistics: 15th International Meeting, CIBB 2018, Caparica, Portugal, September 6–8, 2018, Revised Selected Papers},
pages = {205–211},
numpages = {7},
keywords = {Tonotonic maps, Auditory cortex, BOLD-fMRI},
location = {Caparica, Portugal}
}

@article{10.1007/s00138-013-0518-9,
author = {Leo, Marco and Mazzeo, Pier Luigi and Nitti, Massimiliano and Spagnolo, Paolo},
title = {Accurate ball detection in soccer images using probabilistic analysis of salient regions},
year = {2013},
issue_date = {November  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {8},
issn = {0932-8092},
url = {https://doi.org/10.1007/s00138-013-0518-9},
doi = {10.1007/s00138-013-0518-9},
abstract = {Automatic sport video analysis has became one of the most attractive research fields in the areas of computer vision and multimedia technologies. In particular, there has been a boom in soccer video analysis research. This paper presents a new multi-step algorithm to automatically detect the soccer ball in image sequences acquired from static cameras. In each image, candidate ball regions are selected by analyzing edge circularity and then ball patterns are extracted representing locally affine invariant regions around distinctive points which have been highlighted automatically. The effectiveness of the proposed methodologies is demonstrated through a huge number of experiments using real balls under challenging conditions, as well as a favorable comparison with some of the leading approaches from the literature.},
journal = {Mach. Vision Appl.},
month = nov,
pages = {1561–1574},
numpages = {14},
keywords = {Ball detection, Dictionary learning, Interest point detection, Naive Bayes classification, Soccer ball pattern recognition, Sparse feature representation}
}

@inproceedings{10.5555/3298483.3298512,
author = {Fan, Yanbo and He, Ran and Liang, Jian and Hu, Baogang},
title = {Self-paced learning: an implicit regularization perspective},
year = {2017},
publisher = {AAAI Press},
abstract = {Self-paced learning (SPL) mimics the cognitive mechanism of humans and animals that gradually learns from easy to hard samples. One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer function. Existing methods usually pursue this by artificially designing the explicit form of SPL regularizer. In this paper, we study a group of new regularizer (named self-paced implicit regularizer) that is deduced from robust loss function. Based on the convex conjugacy theory, the minimizer function for self-paced implicit regularizer can be directly learned from the latent loss function, while the analytic form of the regularizer can be even unknown. A general framework (named SPL-IR) for SPL is developed accordingly. We demonstrate that the learning procedure of SPL-IR is associated with latent robust loss functions, thus can provide some theoretical insights for its working mechanism. We further analyze the relation between SPL-IR and half-quadratic optimization and provide a group of self-paced implicit regularizer. Finally, we implement SPL-IR to both supervised and unsupervised tasks, and experimental results corroborate our ideas and demonstrate the correctness and effectiveness of implicit regularizers.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {1877–1883},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.4018/IJOSSP.2016010102,
author = {Chahal, Kuljit Kaur and Saini, Munish},
title = {Open Source Software Evolution: A Systematic Literature Review Part 2},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016010102},
doi = {10.4018/IJOSSP.2016010102},
abstract = {This paper presents the results of a systematic literature review conducted to understand the Open Source Software OSS development process on the basis of evidence found in the empirical research studies. The study targets the OSS project evolution research papers to understand the methods and techniques employed for analysing the OSS evolution process. Our results suggest that there is lack of a uniform approach to analyse and interpret the results. The use of prediction techniques that just extrapolate the historic trends into the future should be a conscious task as it is observed that there are no long-term correlations in data of such systems. OSS evolution as a research area is still in nascent stage. Even after a number of empirical studies, the field has failed to establish a theory. There is need to formalize the field as a systematic and formal approach can produce better software.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {28–48},
numpages = {21},
keywords = {ARIMA Modelling, Automation Support, Co-Evolution, OSS Prediction, Programming Languages, Software Evolution Theory, Software Reuse}
}

@article{10.1016/j.procs.2015.02.001,
author = {Patri, Ashutosh and Patnaik, Yugesh},
title = {Random Forest and Stochastic Gradient Tree Boosting Based Approach for the Prediction of Airfoil Self-noise},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.001},
doi = {10.1016/j.procs.2015.02.001},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {109–121},
numpages = {13},
keywords = {airfoil self-noise, prediction, Random Forest, Stochastic Gradient Boosting, CART, regression, NACA0012.}
}

@article{10.5555/2541581.2541582,
author = {Sandberg, Kristian and Bahrami, Bahador and Kanai, Ryota and Barnes, Gareth Robert and Overgaard, Morten and Rees, Geraint},
title = {Early visual responses predict conscious face perception within and between subjects during binocular rivalry},
year = {2013},
issue_date = {June 2013},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {25},
number = {6},
issn = {0898-929X},
abstract = {Previous studies indicate that conscious face perception may be related to neural activity in a large time window around 170-800 msec after stimulus presentation, yet in the majority of these studies changes in conscious experience are confounded with changes in physical stimulation. Using multivariate classification on MEG data recorded when participants reported changes in conscious perception evoked by binocular rivalry between a face and a grating, we showed that only MEG signals in the 120-320 msec time range, peaking at the M170 around 180 msec and the P2m at around 260 msec, reliably predicted conscious experience. Conscious perception could not only be decoded significantly better than chance from the sensors that showed the largest average difference, as previous studies suggest, but also from patterns of activity across groups of occipital sensors that individually were unable to predict perception better than chance. In addition, source space analyses showed that sources in the early and late visual system predicted conscious perception more accurately than frontal and parietal sites, although conscious perception could also be decoded there. Finally, the patterns of neural activity associated with conscious face perception generalized from one participant to another around the times of maximum prediction accuracy. Our work thus demonstrates that the neural correlates of particular conscious contents here, faces are highly consistent in time and space within individuals and that these correlates are shared to some extent between individuals.},
journal = {J. Cognitive Neuroscience},
month = jun,
pages = {969–985},
numpages = {17}
}

@article{10.1016/j.scico.2006.10.007,
author = {Pe\~{n}a, Joaquin and Hinchey, Michael G. and Resinas, Manuel and Sterritt, Roy and Rash, James L.},
title = {Designing and managing evolving systems using a MAS product line approach},
year = {2007},
issue_date = {April, 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {66},
number = {1},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2006.10.007},
doi = {10.1016/j.scico.2006.10.007},
abstract = {We view an evolutionary system as being a software product line. The core architecture is the unchanging part of the system, and each version of the system may be viewed as a product from the product line. Each ''product'' may be described as the core architecture with some agent-based additions. The result is a multiagent system software product line. We describe an approach to such a software product line-based approach using the MaCMAS agent-oriented methodology. The approach scales to enterprise architectures as a multiagent system is an appropriate means of representing a changing enterprise architecture and the interaction between components in it. In addition, we reduce the gap between the enterprise architecture and the software architecture.},
journal = {Sci. Comput. Program.},
month = apr,
pages = {71–86},
numpages = {16},
keywords = {Enterprise architecture evolution, Multiagent systems product lines, Swarm-based systems}
}

@inproceedings{10.1007/978-3-030-32239-7_52,
author = {Xie, Yutong and Lu, Hao and Zhang, Jianpeng and Shen, Chunhua and Xia, Yong},
title = {Deep Segmentation-Emendation Model for Gland Instance Segmentation},
year = {2019},
isbn = {978-3-030-32238-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32239-7_52},
doi = {10.1007/978-3-030-32239-7_52},
abstract = {Accurate and automated gland instance segmentation on histology microscopy images can assist pathologists to diagnose the malignancy degree of colorectal adenocarcinoma. To address this problem, many deep convolutional neural network (DCNN) based methods have been proposed, most of which aim to generate better segmentation by improving the model structure and loss function. Few of them, however, focus on further emendating the inferred predictions, thus missing a chance to refine the obtained segmentation results. In this paper, we propose the deep segmentation-emendation (DSE) model for gland instance segmentation. This model consists of a segmentation network (Seg-Net) and an emendation network (Eme-Net). The Seg-Net is dedicated to generating segmentation results, and the Eme-Net learns to predict the inconsistency between the ground truth and the segmentation results generated by Seg-Net. The predictions made by Eme-Net can in turn be used to refine the segmentation result. We evaluated our DSE model against five recent deep learning models on the 2015 MICCAI Gland Segmentation challenge (GlaS) dataset and against two deep learning models on the colorectal adenocarcinoma (CRAG) dataset. Our results indicate that using Eme-Net results in significant improvement in segmentation accuracy, and the proposed DSE model is able to substantially outperform all the rest models in gland instance segmentation on both datasets.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I},
pages = {469–477},
numpages = {9},
location = {Shenzhen, China}
}

@article{10.1016/j.jbi.2011.11.002,
author = {Forestier, Germain and Lalys, Florent and Riffaud, Laurent and Trelhu, Brivael and Jannin, Pierre},
title = {Classification of surgical processes using dynamic time warping},
year = {2012},
issue_date = {April, 2012},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {45},
number = {2},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2011.11.002},
doi = {10.1016/j.jbi.2011.11.002},
abstract = {In the creation of new computer-assisted intervention systems, Surgical Process Models (SPMs) are an emerging concept used for analyzing and assessing surgical interventions. SPMs represent Surgical Processes (SPs) which are formalized as symbolic structured descriptions of surgical interventions using a pre-defined level of granularity and a dedicated terminology. In this context, one major challenge is the creation of new metrics for the comparison and the evaluation of SPs. Thus, correlations between these metrics and pre-operative data are used to classify surgeries and highlight specific information on the surgery itself and on the surgeon, such as his/her level of expertise. In this paper, we explore the automatic classification of a set of SPs based on the Dynamic Time Warping (DTW) algorithm. DTW is used to compute a similarity measure between two SPs that focuses on the different types of activities performed during surgery and their sequencing, by minimizing time differences. Indeed, it turns out to be a complementary approach to the classical methods that only focus on differences in the time and the number of activities. Experiments were carried out on 24 lumbar disk herniation surgeries to discriminate the surgeons level of expertise according to a prior classification of SPs. Supervised and unsupervised classification experiments have shown that this approach was able to automatically identify groups of surgeons according to their level of expertise (senior and junior), and opens many perspectives for the creation of new metrics for comparing and evaluating surgeries.},
journal = {J. of Biomedical Informatics},
month = apr,
pages = {255–264},
numpages = {10},
keywords = {Classification, Clustering, Dynamic time warping, Surgery evaluation, Surgical process models}
}

@article{10.1145/3230709,
author = {Liu, Wenhe and Chang, Xiaojun and Yan, Yan and Yang, Yi and Hauptmann, Alexander G.},
title = {Few-Shot Text and Image Classification via Analogical Transfer Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230709},
doi = {10.1145/3230709},
abstract = {Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {20},
keywords = {Transfer learning, classification}
}

@inproceedings{10.1145/1244002.1244277,
author = {Ubayashi, Naoyasu and Nakajima, Shin},
title = {Context-aware feature-oriented modeling with an aspect extension of VDM},
year = {2007},
isbn = {1595934804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1244002.1244277},
doi = {10.1145/1244002.1244277},
abstract = {Separation of concerns is important to reduce the complexity of software design. This paper examines a software development method starting with the feature-oriented modeling method to have VDM-based formal design. In order to overcome the problem that a feature may be scattered over the VDM design description, the notion of the aspect is adapted to propose Aspect VDM. The identified features are concisely represented in Aspect VDM to demonstrate modular descriptions of cross-cutting concerns in VDM.},
booktitle = {Proceedings of the 2007 ACM Symposium on Applied Computing},
pages = {1269–1274},
numpages = {6},
keywords = {VDM, aspect orientation, context, feature-oriented modeling, proof obligation, software product line, weaving},
location = {Seoul, Korea},
series = {SAC '07}
}

@article{10.1016/j.dss.2010.12.009,
author = {Ribeiro, Rita A. and Moreira, Ana M. and van den Broek, Pim and Pimentel, Afonso},
title = {Hybrid assessment method for software engineering decisions},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {1},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2010.12.009},
doi = {10.1016/j.dss.2010.12.009},
abstract = {During software development, many decisions need to be made to guarantee the satisfaction of the stakeholders' requirements and goals. The full satisfaction of all of these requirements and goals may not be possible, requiring decisions over conflicting human interests as well as technological alternatives, with an impact on the quality and cost of the final solution. This work aims at assessing the suitability of multi-criteria decision making (MCDM) methods to support software engineers' decisions. To fulfil this aim, a HAM (Hybrid Assessment Method) is proposed, which gives its user the ability to perceive the influence different decisions may have on the final result. HAM is a simple and efficient method that combines one single pairwise comparison decision matrix (to determine the weights of criteria) with one classical weighted decision matrix (to prioritize the alternatives). To avoid consistency problems regarding the scale and the prioritization method, HAM uses a geometric scale for assessing the criteria and the geometric mean for determining the alternative ratings.},
journal = {Decis. Support Syst.},
month = apr,
pages = {208–219},
numpages = {12},
keywords = {Aggregation operators, Multi-criteria decision making, Non-functional software requirements, Software engineering}
}

@inproceedings{10.5555/3042094.3042398,
author = {Liotta, Giacomo and Chaudhuri, Atanu},
title = {Minimizing recall risk by collaborative digitized information sharing between OEM and suppliers: a simulation based investigation},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Many Original Equipment Manufacturers (OEMs) and their suppliers face recall and warranty risks due to complex supply chains and products. OEMs and suppliers can hardly take appropriate actions for mitigating these quality risks due to lack of product history data and understanding of their probability. In this work, the product consists of two components delivered by two Tier II suppliers. Probabilities of OEM's acceptance, rework and rejection of the assembled product by a Tier I supplier and probabilities of acceptance, warranty and recall are calculated combining Bayesian Belief Network and simulation of a digitized supply chain. Results show that sharing of incoming quality information between an OEM and Tier I supplier and decision models to estimate warranty and recall probabilities can help in assessing quality improvement benefits to minimize recall risks. Suitable quality improvement contracts between an OEM and Tier I supplier can be designed using embedded product quality data.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2454–2465},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@inproceedings{10.5555/3016100.3016151,
author = {Li, Hao and Gong, Maoguo and Meng, Deyu and Miao, Qiguang},
title = {Multi-objective self-paced learning},
year = {2016},
publisher = {AAAI Press},
abstract = {Current self-paced learning (SPL) regimes adopt the greedy strategy to obtain the solution with a gradually increasing pace parameter while where to optimally terminate this increasing process is difficult to determine. Besides, most SPL implementations are very sensitive to initialization and short of a theoretical result to clarify where SPL converges to with pace parameter increasing. In this paper, we propose a novel multi-objective self-paced learning (MOSPL) method to address these issues. Specifically, we decompose the objective functions as two terms, including the loss and the self-paced regularizer, respectively, and treat the problem as the compromise between these two objectives. This naturally reformulates the SPL problem as a standard multi-objective issue. A multi-objective evolutionary algorithm is used to optimize the two objectives simultaneously to facilitate the rational selection of a proper pace parameter. The proposed technique is capable of ameliorating a set of solutions with respect to a range of pace parameters through finely compromising these solutions inbetween, and making them perform robustly even under bad initialization. A good solution can then be naturally achieved from these solutions by making use of some off-the-shelf tools in multi-objective optimization. Experimental results on matrix factorization and action recognition demonstrate the superiority of the proposed method against the existing issues in current SPL research.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {1802–1808},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1017/S0890060403171041,
author = {Felfernig, Alexander and Friedrich, Gerhard and Jannach, Dietmar and Stumptner, Markus and Zanker, Markus},
title = {Configuration knowledge representations for Semantic Web applications},
year = {2003},
issue_date = {January 2003},
publisher = {Cambridge University Press},
address = {USA},
volume = {17},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060403171041},
doi = {10.1017/S0890060403171041},
abstract = {Today's economy exhibits a growing trend toward highly specialized solution providers cooperatively offering configurable products and services to their customers. This paradigm shift requires the extension of current standalone configuration technology with capabilities of knowledge sharing and distributed problem solving. In this context a standardized configuration knowledge representation language with formal semantics is needed in order to support knowledge interchange between different configuration environments. Languages such as Ontology Inference Layer (OIL) and DARPA Agent Markup Language (DAML+OIL) are based on such formal semantics (description logic) and are very popular for knowledge representation in the Semantic Web. In this paper we analyze the applicability of those languages with respect to configuration knowledge representation and discuss additional demands on expressivity. For joint configuration problem solving it is necessary to agree on a common problem definition. Therefore, we give a description logic based definition of a configuration problem and show its equivalence with existing consistency-based definitions, thus joining the two major streams in knowledge-based configuration (description logics and predicate logic/constraint based configuration).},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {31–50},
numpages = {20}
}

@inproceedings{10.1007/978-3-319-29339-4_24,
author = {Leottau, David L. and Ruiz-del-Solar, Javier and MacAlpine, Patrick and Stone, Peter},
title = {A Study of Layered Learning Strategies Applied to Individual Behaviors in Robot Soccer},
year = {2015},
isbn = {978-3-319-29338-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29339-4_24},
doi = {10.1007/978-3-319-29339-4_24},
abstract = {Hierarchical task decomposition strategies allow robots and agents in general to address complex decision-making tasks. Layered learning is a hierarchical machine learning paradigm where a complex behavior is learned from a series of incrementally trained sub-tasks. This paper describes how layered learning can be applied to design individual behaviors in the context of soccer robotics. Three different layered learning strategies are implemented and analyzed using a ball-dribbling behavior as a case study. Performance indices for evaluating dribbling speed and ball-control are defined and measured. Experimental results validate the usefulness of the implemented layered learning strategies showing a trade-off between performance and learning speed.},
booktitle = {RoboCup 2015: Robot World Cup XIX},
pages = {290–302},
numpages = {13},
keywords = {Reinforcement learning, Layered learning, Machine learning, Soccer robotics, Biped robot, NAO, Behavior, Dribbling, Fuzzy logic},
location = {Hefei, China}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3387940.3392089,
author = {Ahlgren, John and Berezin, Maria Eugenia and Bojarczuk, Kinga and Dulskyte, Elena and Dvortsova, Inna and George, Johann and Gucevska, Natalija and Harman, Mark and L\"{a}mmel, Ralf and Meijer, Erik and Sapora, Silvia and Spahr-Summers, Justin},
title = {WES: Agent-based User Interaction Simulation on Real Infrastructure},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392089},
doi = {10.1145/3387940.3392089},
abstract = {We introduce the Web-Enabled Simulation (WES) research agenda, and describe FACEBOOK's WW system. We describe the application of WW to reliability, integrity and privacy at FACEBOOK1, where it is used to simulate social media interactions on an infrastructure consisting of hundreds of millions of lines of code. The WES agenda draws on research from many areas of study, including Search Based Software Engineering, Machine Learning, Programming Languages, Multi Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We conclude with a set of open problems and research challenges to motivate wider investigation.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {276–284},
numpages = {9},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.5555/3060832.3060865,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Learning to detect concepts from webly-labeled video data},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Learning detectors that can recognize concepts, such as people actions, objects, etc., in video content is an interesting but challenging problem. In this paper, we study the problem of automatically learning detectors from the big video data on the web without any additional manual annotations. The contextual information available on the web provides noisy labels to the video content. To leverage the noisy web labels, we propose a novel method called WEbly-Labeled Learning (WELL). It is established on two theories called curriculum learning and self-paced learning and exhibits useful properties that can be theoretically verified. We provide compelling insights on the latent non-convex robust loss that is being minimized on the noisy data. In addition, we propose two novel techniques that not only enable WELL to be applied to big data but also lead to more accurate results. The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set. Experimental results show that WELL significantly outperforms the state-of-the-art methods. To the best of our knowledge, WELL achieves by far the best reported performance on these two webly-labeled big video datasets.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1746–1752},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.5555/3298239.3298368,
author = {Raza, Mohammad and Gulwani, Sumit},
title = {Automated data extraction using predictive program synthesis},
year = {2017},
publisher = {AAAI Press},
abstract = {In recent years there has been rising interest in the use of programming-by-example techniques to assist users in data manipulation tasks. Such techniques rely on an explicit input-output examples specification from the user to automatically synthesize programs. However, in a wide range of data extraction tasks it is easy for a human observer to predict the desired extraction by just observing the input data itself. Such predictive intelligence has not yet been explored in program synthesis research, and is what we address in this work. We describe a predictive program synthesis algorithm that infers programs in a general form of extraction DSLs (domain specific languages) given input-only examples. We describe concrete instantiations of such DSLs and the synthesis algorithm in the two practical application domains of text extraction and web extraction, and present an evaluation of our technique on a range of extraction tasks encountered in practice.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {882–890},
numpages = {9},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{10.5555/2540128.2540554,
author = {Chen, Zhengzhang and Xie, Yusheng and Cheng, Yu and Zhang, Kunpeng and Agrawal, Ankit and Liao, Wei-Keng and Samatova, Nagiza F. and Choudhary, Alok},
title = {Forecast oriented classification of spatio-temporal extreme events},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {In complex dynamic systems, accurate forecasting of extreme events, such as hurricanes, is a highly underdetermined, yet very important sustainability problem. While physics-based models deserve their own merits, they often provide unreliable predictions for variables highly related to extreme events. In this paper, we propose a new supervised machine learning problem, which we call a forecast oriented classification of spatio-temporal extreme events. We formulate three important real-world extreme event classification tasks, including seasonal forecasting of (a) tropical cyclones in Northern Hemisphere, (b) hurricanes and landfalling hurricanes in North Atlantic, and (c) North African rainfall. Corresponding predictor and predictand data sets are constructed. These data present unique characteristics and challenges that could potentially motivate future Artificial Intelligent and Data Mining research.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {2952–2954},
numpages = {3},
location = {Beijing, China},
series = {IJCAI '13}
}

@inproceedings{10.5555/3172077.3172140,
author = {Han, Longfei and Zhang, Dingwen and Huang, Dong and Chang, Xiaojun and Ren, Jun and Luo, Senlin and Han, Junwei},
title = {Self-paced mixture of regressions},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Mixture of regressions (MoR) is the well-established and effective approach to model discontinuous and heterogeneous data in regression problems. Existing MoR approaches assume smooth joint distribution for its good anlaytic properties. However, such assumption makes existing MoR very sensitive to intra-component outliers (the noisy training data residing in certain components) and the inter-component imbalance (the different amounts of training data in different components). In this paper, we make the earliest effort on Self-paced Learning (SPL) in MoR, i.e., Self-paced mixture of regressions (SPMoR) model. We propose a novel selfpaced regularizer based on the Exclusive LASSO, which improves inter-component balance of training data. As a robust learning regime, SPL pursues confidence sample reasoning. To demonstrate the effectiveness of SPMoR, we conducted experiments on both the sythetic examples and real-world applications to age estimation and glucose estimation. The results show that SPMoR outperforms the state-of-the-arts methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1816–1822},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1007/978-3-030-72914-1_11,
author = {Kariyado, Yuta and Arevalo, Camilo and Villegas, Juli\'{a}n},
title = {Auralization of Three-Dimensional Cellular Automata},
year = {2021},
isbn = {978-3-030-72913-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72914-1_11},
doi = {10.1007/978-3-030-72914-1_11},
abstract = {An auralization tool for exploring three-dimensional cellular automata is presented. This proof-of-concept allows the creation of a sound field comprising individual sound events associated with each cell in a three-dimensional grid. Each sound-event is spatialized depending on the orientation of the listener relative to the three-dimensional model. Users can listen to all cells simultaneously or in sequential slices at will. Conceived to be used as an immersive Virtual Reality (VR) scene, this software application also works as a desktop application for environments where the VR infrastructure is missing. Subjective evaluations indicate that the proposed sonification increases the perceived quality and immersability of the system with respect to a visualization-only system. No subjective differences between the sequential or simultaneous presentations were found.},
booktitle = {Artificial Intelligence in Music, Sound, Art and Design: 10th International Conference, EvoMUSART 2021, Held as Part of EvoStar 2021, Virtual Event, April 7–9, 2021, Proceedings},
pages = {161–170},
numpages = {10},
keywords = {Cellular automata, Game of life, Auralization}
}

@article{10.1016/j.artint.2008.11.001,
author = {Frisch, Alan M. and Hnich, Brahim and Kiziltan, Zeynep and Miguel, Ian and Walsh, Toby},
title = {Filtering algorithms for the multiset ordering constraint},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {173},
number = {2},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2008.11.001},
doi = {10.1016/j.artint.2008.11.001},
abstract = {Constraint programming (CP) has been used with great success to tackle a wide variety of constraint satisfaction problems which are computationally intractable in general. Global constraints are one of the important factors behind the success of CP. In this paper, we study a new global constraint, the multiset ordering constraint, which is shown to be useful in symmetry breaking and searching for leximin optimal solutions in CP. We propose efficient and effective filtering algorithms for propagating this global constraint. We show that the algorithms maintain generalised arc-consistency and we discuss possible extensions. We also consider alternative propagation methods based on existing constraints in CP toolkits. Our experimental results on a number of benchmark problems demonstrate that propagating the multiset ordering constraint via a dedicated algorithm can be very beneficial.},
journal = {Artif. Intell.},
month = feb,
pages = {299–328},
numpages = {30},
keywords = {Constraint programming, Constraint propagation, Constraint satisfaction, Global constraints, Leximin optimal solutions, Modelling, Multiset ordering, Propagation algorithms, Symmetry breaking}
}

@article{10.5555/1512690.1512704,
author = {Molder, Cristian and Boscoianu, Mircea and Vizitiu, Iulian C. and Stanciu, Mihai I.},
title = {Decision fusion for improved automatic license plate recognition},
year = {2009},
issue_date = {February 2009},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {6},
number = {2},
issn = {1790-0832},
abstract = {Automatic license plate recognition (ALPR) is a pattern recognition application of great importance for access, traffic surveillance and law enforcement. Therefore many studies are concentrated on creating new algorithms or improving their performance. Many authors have presented algorithms that are based on individual methods such as skeleton features, neural networks or template matching for recognizing the license plate symbols. In this paper we present a novel approach for decisional fusion of several recognition methods, as well as new classification features. The classification results are proven to be significantly better than those obtained for each method considered individually. For better results, syntax corrections are also considered. Several trainable and non-trainable decisional fusion rules have been taken into account, evidencing each of the classification methods at their best. Experimental results are shown, the results being very encouraging by obtaining a symbol good recognition rate (GRC) of more than 99.4% on a real license plate database.},
journal = {WSEAS Trans. Info. Sci. and App.},
month = feb,
pages = {291–300},
numpages = {10},
keywords = {ALPR, decision fusion, licence plate, neural networks, pattern recognition, skeleton features}
}

@inproceedings{10.1145/3302506.3310407,
author = {Shih, Oliver and Rowe, Anthony},
title = {Can a phone hear the shape of a room?},
year = {2019},
isbn = {9781450362849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302506.3310407},
doi = {10.1145/3302506.3310407},
abstract = {Understanding the location of acoustically reflective surfaces in a room is a critical component in advanced sound processing. For example, intelligent speakers can use a room's acoustic geometry to improve playback quality, source separation accuracy, and speech recognition. In this paper, we present Synesthesia, a system for capturing the acoustic properties of a room using a single fixed speaker and a mobile phone that records audio at multiple locations. Using the arrival time of echoes, the system is able to reconstruct the position of reflective surfaces like walls and then estimate properties like surface absorption.Previous work has shown how the acoustic room impulse response (RIR) of an environment can be used to analyze echoes within a space to reconstruct room geometry. The best current RIR-based approaches rely on high-end equipment and capturing an acoustic signal broadcast into space from a known fixed constellation of microphones. They also require the precise calibration and measurement of microphone positions. In addition, most approaches pose constraints on room geometries and limit the order of RIR to achieve accurate and consistent results. In this paper, we introduce a new approach that performs RIR imaging using a mobile phone that tracks its location with visual inertial odometry (VIO) to record a dense set of samples albeit with noise in their locations. We present a new approach that is able to relax several key assumptions on RIR and show through both experimentation and simulation that even with 20cm of uncertainty in the microphone locations provided by VIO, we are still able to reconstruct the room geometry with accurate shape and dimensions. We demonstrate this capability by prototyping a tool for acoustic engineers, that allows a user to view a room's estimated geometry and absorption overlaid on the actual sensed space with augmented reality.},
booktitle = {Proceedings of the 18th International Conference on Information Processing in Sensor Networks},
pages = {277–288},
numpages = {12},
keywords = {active acoustic sensing, room reconstruction and mapping},
location = {Montreal, Quebec, Canada},
series = {IPSN '19}
}

@article{10.1155/2021/9956244,
author = {Li, Lei and Zhu, Yuquan and Cai, Tao and Niu, Dejiao and Shi, Huaji and Zou, Tingting and Huang, Chenxi},
title = {A Temporal Pool Learning Algorithm Based on Location Awareness},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/9956244},
doi = {10.1155/2021/9956244},
abstract = {Hierarchical Temporal Memory is a new type of artificial neural network model, which imitates the structure and information processing flow of the human brain. Hierarchical Temporal Memory has strong adaptability and fast learning ability and becomes a hot spot in current research. Hierarchical Temporal Memory obtains and saves the temporal characteristics of input sequences by the temporal pool learning algorithm. However, the current algorithm has some problems such as low learning efficiency and poor learning effect when learning time series data. In this paper, a temporal pool learning algorithm based on location awareness is proposed. The cell selection rules based on location awareness and the dendritic updating rules based on adjacent inputs are designed to improve the learning efficiency and effect of the algorithm. Through the algorithm prototype, three different datasets are used to test and analyze the algorithm performance. The experimental results verify that the algorithm can quickly obtain the complete characteristics of the input sequence. No matter whether there are similar segments in the sequence, the proposed algorithm has higher prediction recall and precision than the existing algorithms.},
journal = {Sci. Program.},
month = jan,
numpages = {12}
}

@inproceedings{10.5555/2886521.2886696,
author = {Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G.},
title = {Self-paced curriculum learning},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to "instructor-student-collaborative" learning mode, as opposed to "instructor-driven" in CL or "student-driven" in SPL. Empirically, we show that the advantage of SPCL on two tasks.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2694–2700},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/2382029.2382104,
author = {Prabhakaran, Vinodkumar and Rambow, Owen and Diab, Mona},
title = {Predicting overt display of power in written dialogs},
year = {2012},
isbn = {9781937284206},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We analyze overt displays of power (ODPs) in written dialogs. We present an email corpus with utterances annotated for ODP and present a supervised learning system to predict it. We obtain a best cross validation F-measure of 65.8 using gold dialog act features and 55.6 without using them.},
booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {518–522},
numpages = {5},
location = {Montreal, Canada},
series = {NAACL HLT '12}
}

@article{10.1016/j.neucom.2015.10.046,
author = {Zhang, Peng and Zhuo, Tao and Zhang, Yanning and Tao, Dapeng and Cheng, Jun},
title = {Online tracking based on efficient transductive learning with sample matching costs},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {175},
number = {PA},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.10.046},
doi = {10.1016/j.neucom.2015.10.046},
abstract = {Visual tracking has been a popular and attractive topic in computer vision for a long time. In recent decades, many challenge problems in object tracking has been effectively resolved by using learning based tracking strategies. Number of investigations carried on learning theory found that when labeled samples are limited, the learning performance can be sufficiently improved by exploiting unlabeled ones. Therefore, one of the most important issue for semi-supervised learning is how to assign the labels to the unlabeled samples, which is also the principal focus of transductive learning. Unfortunately, considering the efficiency requirement of online tracking, the optimization scheme employed by the traditional transductive learning is hard to be applied to online tracking problems because of its large computational cost during sample labeling. In this paper, we proposed an efficient transductive learning for online tracking by utilizing the correspondences among the generated unlabeled and labeled samples. Those variational correspondences are modeled by a matching costs function to achieve more efficient learning of representative separators. With a strategy of fixed budget for support vectors, the proposed learning is updated by using a weighted accumulative average of model coefficients. We evaluated the proposed tracking on benchmark database, the experiment results have demonstrated an outstanding performance via comparing with the other state-of-the-art trackers.},
journal = {Neurocomput.},
month = jan,
pages = {166–176},
numpages = {11},
keywords = {Efficient, Matching costs, Online, Tracking, Transductive learning}
}

@inproceedings{10.5555/2907132.2907135,
author = {Viappiani, Paolo and Boutilier, Craig},
title = {Optimal set recommendations based on regret},
year = {2009},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {Current conversational recommender systems do not offer guarantees on the quality of their recommendations, either because they do not maintain a model of a user's utility function, or do so in an ad hoc fashion. In this paper, we propose an approach to recommender systems that incorporates explicit utility models into the recommendation process in a decision-theoretically sound fashion. The system maintains explicit constraints on the user's utility based on the semantics of the preferences revealed by the user's actions. In particular, we propose and investigate a new decision criterion, setwise maximum regret, for constructing optimal recommendation sets. This new criterion extends the mathematical notion of maximum regret used in decision theory and preference elicitation to sets. We develop computational procedures for computing setwise max regret. We also show that the criterion suggests choice sets for queries that are myopically optimal: that is, it refines knowledge of a user's utility function in a way that reduces max regret more quickly than any other choice set. Thus setwise max regret acts both as guarantee on the quality of our recommendations and as a driver for further utility elicitation.Our simulation results suggest that this utility-theoretically sound approach to user modeling allows much more effective navigation of a product space than traditional approaches based on, for example, heuristic utility models and product similarity measures.},
booktitle = {Proceedings of the 7th International Conference on Intelligent Techniques for Web Personalization &amp; Recommender Systems - Volume 528},
pages = {20–31},
numpages = {12},
location = {Pasadena, California},
series = {ITWP'09}
}

@inproceedings{10.1007/978-3-030-75765-6_49,
author = {Thanthriwatta, Thilina and Rosenblum, David S.},
title = {Instance Selection for Online Updating in Dynamic Recommender Environments},
year = {2021},
isbn = {978-3-030-75764-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-75765-6_49},
doi = {10.1007/978-3-030-75765-6_49},
abstract = {Online recommender systems continuously learn from user interactions that occur in a streaming manner. A fundamental challenge of online recommendation is to select important instances (i.e., user interactions) for model updates to achieve higher prediction accuracy while omitting noisy instances. In this paper, we study&nbsp;(1) how to select the best instances and&nbsp;(2) how to effectively utilize the selected instances in dynamic recommender environments. We present two instance selection strategies based on Self-Paced Learning and rating profiles. We integrate them with Factorization Machines to perform online updates. Moreover, we study the impact of contextual information in online updating. We conducted experiments on a real-world check-in dataset, which contains temporal contextual features. Empirical results demonstrate that ox ur instance selection strategies effectively balance the trade-off between prediction accuracy and efficiency.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part II},
pages = {612–624},
numpages = {13},
keywords = {Instance selection, Context-aware recommender systems, Online recommender systems}
}

@article{10.1145/3480139,
author = {Chemistruck, Mike and Allen, Andrew and Snyder, John and Raghuvanshi, Nikunj},
title = {Efficient acoustic perception for virtual AI agents},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3480139},
doi = {10.1145/3480139},
abstract = {We model acoustic perception in AI agents efficiently within complex scenes with many sound events. The key idea is to employ perceptual parameters that capture how each sound event propagates through the scene to the agent's location. This naturally conforms virtual perception to human. We propose a simplified auditory masking model that limits localization capability in the presence of distracting sounds. We show that anisotropic reflections as well as the initial sound serve as useful localization cues. Our system is simple, fast, and modular and obtains natural results in our tests, letting agents navigate through passageways and portals by sound alone, and anticipate or track occluded but audible targets. Source code is provided.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = sep,
articleno = {43},
numpages = {13},
keywords = {NPC AI, acoustics, game AI, localization, masking, perception, sound propagation, virtual agents}
}

@article{10.1023/A:1017940426216,
author = {Torrens, Marc and Faltings, Boi and Pu, Pearl},
title = {SmartClients: Constraint Satisfaction as a Paradigm for Scaleable Intelligent Information Systems},
year = {2002},
issue_date = {January 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {7},
number = {1},
issn = {1383-7133},
url = {https://doi.org/10.1023/A:1017940426216},
doi = {10.1023/A:1017940426216},
abstract = {Many information systems are used in a problem solving context. Examples are travel planning systems, catalogs in electronic commerce, or agenda planning systems. They can be made more useful by integrating problem-solving capabilities into the information systems. This poses the challenge of i&gt;scaleability: when hundreds of users access a server at the same time, it is important to avoid excessive computational load.We present the concept of i&gt;SmartClients: lightweight problem-solving agents based on constraint satisfaction which can carry out the computation- and communication-intensive tasks on the user's computer. We present an example of an air travel planning system based on this technology.},
journal = {Constraints},
month = jan,
pages = {49–69},
numpages = {21},
keywords = {configuration, constraint satisfaction, electronic catalogs, electronic commerce, information systems, software agents}
}

@inproceedings{10.1007/978-3-030-98682-7_19,
author = {Yao, ZhengBai and Douglas, Will and O’Keeffe, Simon and Villing, Rudi},
title = {Faster YOLO-LITE: Faster Object Detection on Robot and Edge Devices},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_19},
doi = {10.1007/978-3-030-98682-7_19},
abstract = {Mobile robots and many edge AI devices have a need to trade off computational power against power consumption, battery size, and time between charges. Consequently, it is common for such devices to have significantly less computational power than the powerful GPU-based systems typically used to train and evaluate deep neural networks. Object detection is a key aspect of visual perception for robots and edge devices but popular object detection architectures that run fastest on GPU based systems or that are designed to maximize mAP with large input image sizes may not scale well to edge devices. In this work we evaluate the latency and mAP of several model architectures from the YOLO and SSD families on a range of devices representative of robot and edge device capabilities. We also evaluate the effect of runtime framework and show that some unexpected large differences can be found. Based on our evaluations we propose new variations of the YOLO-LITE architecture which we show can provide increased mAP at reduced latency.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {226–237},
numpages = {12},
keywords = {Deep learning, Object detection, Convolutional neural network, Embedded system, Real-time performance, Edge AI},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.5555/3304222.3304408,
author = {Zhang, Yuanxing and Zhang, Yangbin and Bian, Kaigui and Li, Xiaoming},
title = {Towards reading comprehension for long documents},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Machine reading comprehension has gained attention from both industry and academia. It is a very challenging task that involves various domains such as language comprehension, knowledge inference, summarization, etc. Previous studies mainly focus on reading comprehension on short paragraphs, and these approaches fail to perform well on the documents. In this paper, we propose a hierarchical match attention model to instruct the machine to extract answers from a specific short span of passages for the long document reading comprehension (LDRC) task. The model takes advantages from hierarchical-LSTM to learn the paragraph-level representation, and implements the match mechanism (i.e., quantifying the relationship between two contexts) to find the most appropriate paragraph that includes the hint of answers. Then the task can be decoupled into reading comprehension task for short paragraph, such that the answer can be produced. Experiments on the modified SQuAD dataset show that our proposed model outperforms existing reading comprehension models by at least 20% regarding exact match (EM), F1 and the proportion of identified paragraphs which are exactly the short paragraphs where the original answers locate.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4588–4594},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1007/s10916-018-0978-6,
author = {Rajkumar, S. and Muttan, S. and Sapthagirivasan, V. and Jaya, V. and Vignesh, S. S.},
title = {Development of Improved Software Intelligent System for Audiological Solutions},
year = {2018},
issue_date = {July      2018},
publisher = {Plenum Press},
address = {USA},
volume = {42},
number = {7},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-018-0978-6},
doi = {10.1007/s10916-018-0978-6},
abstract = {Of late, there has been an increase in hearing impairment cases and to provide the most advantageous solutions to them is an uphill task for audiologists. Significant difficulty faced by the audiologists is in effective programming of hearing aids to provide enhanced satisfaction to the users. The main aim of our study was to develop a software intelligent system (SIS): (i) to perform the required audiological investigations for finding the degree and type of hearing loss, and (ii) to suggest appropriate values of hearing aid parameters for enhancing the speech intelligibility and the satisfaction level among the hearing aid users. In this paper, we present a Neuro-Fuzzy based SIS to automatically predict and suggest the hearing-aid parameters such as gain values, compression ratio and threshold knee point, which are needed to be fixed for different octave frequencies of sound inputs during the hearing-aid trial. The test signals for audiological investigations are generated through the standard hardware present in a personal computer system and with the aid of a software algorithm. The proposed system was validated with 243 subjects' data collected at the Government General Hospital, Chennai, India. The calculated sensitivity, specificity and accuracy of the proposed audiometer incorporated in the SIS were 98.6%, 96.4 and 98.2%, respectively, by comparing its interpretations with those of the `gold standard' audiometers. Furthermore, 91% (221 of 243) of the hearing impaired subjects attained satisfaction in the first hearing aid trials itself with the gain values as recommended by the improved SIS. The proposed system reduced around 75% of the `trial and error' time spent by audiologists for enhancing satisfactory usage of the hearing aid. Hence, the proposed SIS could be used to find the degree and type of hearing loss and to recommend hearing aid parameters to provide optimal solutions to the hearing aid users.},
journal = {J. Med. Syst.},
month = jul,
pages = {1–12},
numpages = {12},
keywords = {Artificial intelligence, Compression ratio, Computerized audiometer, Digital in medical, Expert system, Hearing aid, Hearing loss, Neuro-Fuzzy, Prescriptive procedure, Software intelligent system, Sound pressure level, Speech discrimination score, Threshold knee point}
}

@inproceedings{10.5555/3540261.3541263,
author = {Chaplot, Devendra Singh and Dalal, Murtaza and Gupta, Saurabh and Malik, Jitendra and Salakhutdinov, Ruslan},
title = {SEAL: self-supervised embodied active learning using exploration and 3D consistency},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we explore how we can build upon the data and models of Internet images and use them to adapt to robot vision without requiring any extra labels. We present a framework called Self-supervised Embodied Active Learning (SEAL). It utilizes perception models trained on internet images to learn an active exploration policy. The observations gathered by this exploration policy are labelled using 3D consistency and used to improve the perception model. We build and utilize 3D semantic maps to learn both action and perception in a completely self-supervised manner. The semantic map is used to compute an intrinsic motivation reward for training the exploration policy and for labelling the agent observations using spatio-temporal 3D consistency and label propagation. We demonstrate that the SEAL framework can be used to close the action-perception loop: it improves object detection and instance segmentation performance of a pretrained perception model by just moving around in training environments and the improved perception model can be used to improve Object Goal Navigation.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1002},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-642-42042-9_85,
author = {Abernethy, Mark and Rai, Shri M.},
title = {An Innovative Fingerprint Feature Representation Method to Facilitate Authentication Using Neural Networks},
year = {2013},
isbn = {9783642420412},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-42042-9_85},
doi = {10.1007/978-3-642-42042-9_85},
abstract = {Authentication systems enable the verification of claimed identity; on computer systems these are typically password-based. However, such systems are vulnerable to numerous attack vectors and are responsible for a large number of security breaches. Biometrics is now commonly investigated as an alternative to password-based systems. There are numerous biometric characteristics that can be used for authentication purposes, each with different levels of accuracy and positive and negative implementation factors. The objective of the current study was to investigate fingerprint recognition utilizing Artificial Neural Networks ANNs as a classifier. An innovative representation method for fingerprint features was developed to facilitate verification by ANNs. For each participant, the method required the alignment of their fingerprint samples based on extracted local features, and the selection of 8 of these aligned features common to their samples. The six attributes belonging to each of the selected features were used for ANN input. Unlike the common usage, each participant had one dedicated ANN trained to recognize only their fingerprint samples. Experimental results returned a false acceptance rate FAR of 0.0 and a false rejection rate FRR of 0.0022, which were comparable to and in some cases, slightly better than other research efforts in the field.},
booktitle = {Proceedings, Part II, of the 20th International Conference on Neural Information Processing - Volume 8227},
pages = {689–696},
numpages = {8},
keywords = {Artificial Neural Networks, Authentication, Biometrics, Feature Representation, Fingerprint Recognition, Pattern Recognition, Verification},
location = {Daegu, Korea},
series = {ICONIP 2013}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {Software defect prediction, Deep feature representation, Triplet loss, Weighted cross-entropy loss, Deep neural network, 00-01, 99-00}
}

@inproceedings{10.5555/3155562.3155664,
author = {Krismayer, Thomas and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Mining constraints for event-based monitoring in systems of systems},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {826–831},
numpages = {6},
keywords = {Constraint mining, event-based monitoring, systems of systems},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@inproceedings{10.1145/3184558.3191523,
author = {Dalmia, Ayushi and J, Ganesh and Gupta, Manish},
title = {Towards Interpretation of Node Embeddings},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191523},
doi = {10.1145/3184558.3191523},
abstract = {Recently there have been a large number of studies on embedding large-scale information networks using low-dimensional, neighborhood and community aware node representations. Though the performance of these embedding models have been better than traditional methods for graph mining applications, little is known about what these representations encode, or why a particular node representation works better for certain tasks. Our work presented here constitutes the first step in decoding the black-box of vector embeddings of nodes by evaluating their effectiveness in encoding elementary properties of a node such as page rank, degree, closeness centrality, clustering coefficient, etc. We believe that a node representation is effective for an application only if it encodes the application-specific elementary properties of nodes. To unpack the elementary properties encoded in a node representation, we evaluate the representations on the accuracy with which they can model each of these properties. Our extensive study of three state-of-the-art node representation models (DeepWalk, node2vec and LINE) on four different tasks and six diverse graphs reveal that node2vec and LINE best encode the network properties of sparse and dense graphs respectively. We correlate the model performance obtained for elementary property prediction tasks with the high-level downstream applications such as link prediction and node classification, and visualize the task performance vector of each model to understand the semantic similarity between the embeddings learned by various models. Our first study of the node embedding models for outlier detection reveals that node2vec and DeepWalk identify outliers well for sparse and dense graphs respectively. Our analysis highlights that the proposed elementary property prediction tasks help in unearthing the important features responsible for the given node embedding model to perform well for a given downstream task. This understanding would facilitate in picking the right model for a given downstream task.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {945–952},
numpages = {8},
keywords = {graph representation, model interpretability, neural networks},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.5555/3298023.3298125,
author = {Hanna, Josiah P. and Stone, Peter},
title = {Grounded action transformation for robot learning in simulation},
year = {2017},
publisher = {AAAI Press},
abstract = {Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. Grounded simulation learning (GSL) promises to address this issue by altering the simulator to better match the real world. This paper proposes a new algorithm for GSL - Grounded Action Transformation - and applies it to learning of humanoid bipedal locomotion. Our approach results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk. We further evaluate our methodology in controlled experiments using a second, higher-fidelity simulator in place of the real world. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for learning robot control policies.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3834–3840},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{10.1007/978-3-031-04673-5_8,
author = {Ibias, Alfredo and Llana, Luis and N\'{u}\~{n}ez, Manuel},
title = {Using Ant Colony Optimisation to&nbsp;Select Features Having Associated Costs},
year = {2021},
isbn = {978-3-031-04672-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-04673-5_8},
doi = {10.1007/978-3-031-04673-5_8},
abstract = {Software Product Lines (SPLs) strongly facilitate the automation of software development processes. They combine features to create programs (called products) that fulfil certain properties. Testing SPLs is an intensive process where choosing the proper products to include in the testing process can be a critical task. In fact, selecting the best combination of features from an SPL is a complex problem that is frequently addressed in the literature. In this paper we use evolutionary algorithms to find a combination of features with low testing cost that include a target feature, to facilitate the integration testing of such feature. Specifically, we use an Ant Colony Optimisation algorithm to find one of the cheapest (in terms of testing) combination of features that contains a specific feature. Our results show that our framework overcomes the limitations of both brute force and random search algorithms.},
booktitle = {Testing Software and Systems: 33rd IFIP WG 6.1 International Conference, ICTSS 2021, London, UK, November 10–12, 2021, Proceedings},
pages = {106–122},
numpages = {17},
keywords = {Software Product Lines, Integration testing, Ant Colony Optimisation, Feature selection},
location = {London, United Kingdom}
}

@article{10.1504/IJAOSE.2008.016800,
author = {Verstraete, Paul and Germain, Bart Saint and Valckenaers, Paul and Brussel, Hendrik Van and Belle, Jan Van and Hadeli},
title = {Engineering manufacturing control systems using PROSA and delegate MAS},
year = {2008},
issue_date = {January 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {1},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2008.016800},
doi = {10.1504/IJAOSE.2008.016800},
abstract = {This paper presents a systematic description of a reusable software architecture for multiagent systems in the domain of manufacturing control. The architectural description consolidates the authors' expertise in this area. Until now, the research has taken a manufacturing control perspective of multiagent systems. The research team has focused on providing benefits to the manufacturing control domain by designing a novel type of control system. This paper takes a software architectural perspective of multiagent manufacturing control. The systematic description specifies a software product line architecture for manufacturing control. The paper describes the assets of the software product line architecture and how these assets can be combined.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {62–89},
numpages = {28},
keywords = {MASs, agent-based systems, manufacturing control, multi-agent systems, software architecture, software reuse}
}

@inproceedings{10.5555/2818754.2818819,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
title = {Combining multi-objective search and constraint solving for configuring large software product lines},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p &lt; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\^{A}12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {517–528},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2463372.2463545,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Minimizing test suites in software product lines using weight-based genetic algorithms},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463545},
doi = {10.1145/2463372.2463545},
abstract = {Test minimization techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. In the context of software product line, we can save effort and cost in the selection and minimization of test cases for testing a specific product by modeling the product line. However, minimizing the test suite for a product requires addressing two potential issues: 1) the minimized test suite may not cover all test requirements compared with the original suite; 2) the minimized test suite may have less fault revealing capability than the original suite. In this paper, we apply weight-based Genetic Algorithms (GAs) to minimize the test suite for testing a product, while preserving fault detection capability and testing coverage of the original test suite. The challenge behind is to define an appropriate fitness function, which is able to preserve the coverage of complex testing criteria (e.g., Combinatorial Interaction Testing criterion). Based on the defined fitness function, we have empirically evaluated three different weight-based GAs on an industrial case study provided by Cisco Systems, Inc. Norway. We also presented our results of applying the three weight-based GAs on five existing case studies from the literature. Based on these case studies, we conclude that among the three weight-based GAs, Random-Weighted GA (RWGA) achieved significantly better performance than the other ones.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1493–1500},
numpages = {8},
keywords = {fault detection capability, feature pairwise coverage, test minimization, weight-based gas},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@inproceedings{10.5555/3298023.3298119,
author = {Schmitt, Felix and Bieg, Hans-Joachim and Herman, Michael and Rothkopf, Constantin A.},
title = {I see what you see: inferring sensor and policy models of human real-world motor behavior},
year = {2017},
publisher = {AAAI Press},
abstract = {Human motor behavior is naturally guided by sensing the environment. To predict such sensori-motor behavior, it is necessary to model what is sensed and how actions are chosen based on the obtained sensory measurements. Although several models of human sensing haven been proposed, rarely data of the assumed sensory measurements is available. This makes statistical estimation of sensor models problematic. To overcome this issue, we propose an abstract structural estimation approach building on the ideas of Herman et al.'s Simultaneous Estimation of Rewards and Dynamics (SERD). Assuming optimal fusion of sensory information and rational choice of actions the proposed method allows to infer sensor models even in absence of data of the sensory measurements. To the best of our knowledge, this work presents the first general approach for joint inference of sensor and policy models. Furthermore, we consider its concrete implementation in the important class of sensor scheduling linear quadratic Gaussian problems. Finally, the effectiveness of the approach is demonstrated for prediction of the behavior of automobile drivers. Specifically, we model the glance and steering behavior of driving in the presence of visually demanding secondary tasks. The results show, that prediction benefits from the inference of sensor models. This is the case, especially, if also information is considered, that is contained in gaze switching behavior.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3797–3803},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1007/s11219-014-9258-y,
author = {Galindo, Jos\'{e} A. and Turner, Hamilton and Benavides, David and White, Jules},
title = {Testing variability-intensive systems using automated analysis: an application to Android},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9258-y},
doi = {10.1007/s11219-014-9258-y},
abstract = {Software product lines are used to develop a set of software products that, while being different, share a common set of features. Feature models are used as a compact representation of all the products (e.g., possible configurations) of the product line. The number of products that a feature model encodes may grow exponentially with the number of features. This increases the cost of testing the products within a product line. Some proposals deal with this problem by reducing the testing space using different techniques. However, a daunting challenge is to explore how the cost and value of test cases can be modeled and optimized in order to have lower-cost testing processes. In this paper, we present TESting vAriAbiLity Intensive Systems (TESALIA), an approach that uses automated analysis of feature models to optimize the testing of variability-intensive systems. We model test value and cost as feature attributes, and then we use a constraint satisfaction solver to prune, prioritize and package product line tests complementing prior work in the software product line testing literature. A prototype implementation of TESALIA is used for validation in an Android example showing the benefits of maximizing the mobile market share (the value function) while meeting a budgetary constraint.},
journal = {Software Quality Journal},
month = jun,
pages = {365–405},
numpages = {41},
keywords = {Android, Automated analysis, Feature models, Software product lines, Testing}
}

@article{10.1007/s10472-009-9132-y,
author = {Mateescu, Robert and Dechter, Rina},
title = {Mixed deterministic and probabilistic networks},
year = {2008},
issue_date = {November  2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {1–3},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-009-9132-y},
doi = {10.1007/s10472-009-9132-y},
abstract = {The paper introduces mixed networks, a new graphical model framework for expressing and reasoning with probabilistic and deterministic information. The motivation to develop mixed networks stems from the desire to fully exploit the deterministic information (constraints) that is often present in graphical models. Several concepts and algorithms specific to belief networks and constraint networks are combined, achieving computational efficiency, semantic coherence and user-interface convenience. We define the semantics and graphical representation of mixed networks, and discuss the two main types of algorithms for processing them: inference-based and search-based. A preliminary experimental evaluation shows the benefits of the new model.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = nov,
pages = {3–51},
numpages = {49},
keywords = {62F15, 62F30, 68T20, 68T30, 68T37, AND/OR search, Automated reasoning, Deterministic information, Graphical models, Inference, Mixed network, Probabilistic information, Search}
}

@article{10.1007/s10846-018-0839-z,
author = {Celemin, Carlos and Ruiz-Del-Solar, Javier},
title = {An Interactive Framework for Learning Continuous Actions Policies Based on Corrective Feedback},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {95},
number = {1},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-018-0839-z},
doi = {10.1007/s10846-018-0839-z},
abstract = {The main goal of this article is to present COACH (COrrective Advice Communicated by Humans), a new learning framework that allows non-expert humans to advise an agent while it interacts with the environment in continuous action problems. The human feedback is given in the action domain as binary corrective signals (increase/decrease the current action magnitude), and COACH is able to adjust the amount of correction that a given action receives adaptively, taking state-dependent past feedback into consideration. COACH also manages the credit assignment problem that normally arises when actions in continuous time receive delayed corrections. The proposed framework is characterized and validated extensively using four well-known learning problems. The experimental analysis includes comparisons with other interactive learning frameworks, with classical reinforcement learning approaches, and with human teleoperators trying to solve the same learning problems by themselves. In all the reported experiments COACH outperforms the other methods in terms of learning speed and final performance. It is of interest to add that COACH has been applied successfully for addressing a complex real-world learning problem: the dribbling of the ball by humanoid soccer players.},
journal = {J. Intell. Robotics Syst.},
month = jul,
pages = {77–97},
numpages = {21},
keywords = {Decision making systems, Human feedback, Human teachers, Interactive machine learning, Learning from demonstration}
}

@article{10.1007/s11263-019-01176-2,
author = {He, Xiangteng and Peng, Yuxin and Zhao, Junjie},
title = {Which and How Many Regions to Gaze: Focus Discriminative Regions for Fine-Grained Visual Categorization},
year = {2019},
issue_date = {September 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {9},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01176-2},
doi = {10.1007/s11263-019-01176-2},
abstract = {Fine-grained visual categorization (FGVC) aims to discriminate similar subcategories that belong to the same superclass. Since the distinctions among similar subcategories are quite subtle and local, it is highly challenging to distinguish them from each other even for humans. So the localization of distinctions is essential for fine-grained visual categorization, and there are two pivotal problems: (1) Which regions are discriminative and representative to distinguish from other subcategories? (2) How many discriminative regions are necessary to achieve the best categorization performance? It is still difficult to address these two problems adaptively and intelligently. Artificial prior and experimental validation are widely used in existing mainstream methods to discover which and how many regions to gaze. However, their applications extremely restrict the usability and scalability of the methods. To address the above two problems, this paper proposes a multi-scale and multi-granularity deep reinforcement learning approach (M2DRL), which learns multi-granularity discriminative region attention and multi-scale region-based feature representation. Its main contributions are as follows: (1) Multi-granularity discriminative localization is proposed to localize the distinctions via a two-stage deep reinforcement learning approach, which discovers the discriminative regions with multiple granularities in a hierarchical manner ("which problem"), and determines the number of discriminative regions in an automatic and adaptive manner ("how many problem"). (2) Multi-scale representation learning helps to localize regions in different scales as well as encode images in different scales, boosting the fine-grained visual categorization performance. (3) Semantic reward function is proposed to drive M2DRL to fully capture the salient and conceptual visual information, via jointly considering attention and category information in the reward function. It allows the deep reinforcement learning to localize the distinctions in a weakly supervised manner or even an unsupervised manner. (4) Unsupervised discriminative localization is further explored to avoid the heavy labor consumption of annotating, and extremely strengthen the usability and scalability of our M2DRL approach. Compared with state-of-the-art methods on two widely-used fine-grained visual categorization datasets, our M2DRL approach achieves the best categorization accuracy.},
journal = {Int. J. Comput. Vision},
month = sep,
pages = {1235–1255},
numpages = {21},
keywords = {Deep reinforcement learning, Fine-grained visual categorization, Multi-granularity discriminative localization, Multi-scale representation learning, Semantic reward, Unsupervised discriminative localization}
}

@inproceedings{10.1007/978-3-319-35122-3_3,
author = {Font, Jaime and Arcega, Lorena and Haugen, \O{}ystein and Cetina, Carlos},
title = {Feature Location in Model-Based Software Product Lines Through a Genetic Algorithm},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_3},
doi = {10.1007/978-3-319-35122-3_3},
abstract = {When following an extractive approach to build a model-based Software Product Line SPL from a set of existing products, features have to be located across the product models. The approaches that produce best results combine model comparisons with the knowledge from the domain experts to locate the features. However, when the domain expert fails to provide accurate information, the semi-automated approach faces challenges. To cope with this issue we propose a genetic algorithm to feature location in model-based SPLs. We have an oracle from an industrial environment that makes it possible to evaluate the results of the approaches. As a result, the proposed approach is able to provide solutions upon inaccurate information on part of the domain expert while the compared approach fails to provide a solution when the information provided by the domain expert is not accurate enough.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {39–54},
numpages = {16},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inbook{10.5555/3454287.3454826,
author = {Zhang, Jiong and Yu, Hsiang-Fu and Dhillon, Inderjit S.},
title = {AutoAssist: a framework to accelerate training of deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep Neural Networks (DNNs) have yielded superior performance in many contemporary applications. However, the gradient computation in a deep model with millions of instances leads to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement by a stochastic gradient update varies dynamically with the choice of instances in the mini-batch. In AutoAssist, we utilize this fact and design an instance shrinking operation that is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. Specifically, we train a very lightweight Assistant model jointly with the original deep network, which we refer to as the Boss. The Assistant model is designed to gauge the importance of a given instance with respect to the current Boss model such that the shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a non-blocking and asynchronous fashion such that overhead is minimal. To demonstrate the effectiveness of AutoAssist, we conduct experiments on two contemporary applications: image classification using ResNets with varied number of layers, and neural machine translation using LSTMs, ConvS2S and Transformer models. For each application, we verify that AutoAssist leads to significant reduction in training time; in particular, 30% to 40% of the total operation count can be reduced which leads to faster convergence and a corresponding decrease in training time.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {539},
numpages = {11}
}

@article{10.1007/s10472-016-9531-9,
author = {S\'{a}nchez, Alejandro and S\'{a}nchez, C\'{e}sar},
title = {Parametrized verification diagrams: temporal verification of symmetric parametrized concurrent systems},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {3–4},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-016-9531-9},
doi = {10.1007/s10472-016-9531-9},
abstract = {This paper studies the problem of verifying temporal properties (including liveness properties) of parametrized concurrent systems executed by an unbounded number of threads. To solve this problem we introduce parametrized verification diagrams (PVDs), that extend the so-called generalized verification diagrams (GVDs) adding support for parametrized verification. Even though GVDs are known to be a sound and complete proof system for non-parametrized systems, the application of GVDs to parametrized systems requires using quantification or finding a potentially different diagram for each instantiation of the parameter (number of threads). As a consequence, the use of GVDs in parametrized verification requires discharging and proving either quantified formulas or an unbounded collection of verification conditions. Parametrized verification diagrams enable the use of asinglediagram to represent the proof that all possible instances of the parametrized concurrent system satisfy the given temporal specification. Checking the proof represented by a PVD requires proving only a finite collection of quantifier-free verification conditions. The PVDs we present here assume that the parametrized systems are symmetric, which covers a large class of concurrent and distributed systems, including concurrent data types. Our second contribution is an implementation of PVDs and its integration into Leap, our prototype theorem prover. Finally, we illustrate empirically, using Leap, the practical applicability of PVDs by building and checking proofs of liveness properties of mutual exclusion protocols and concurrent data structures. To the best of our knowledge, these are the first machine-checkable proofs of liveness properties of these concurrent data types.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = aug,
pages = {249–282},
numpages = {34},
keywords = {Concurrent data types, Deductive method, Formal methods, Formal verification, Liveness properties, Parametrized systems, Temporal logic, Verification conditions}
}

@inproceedings{10.1007/11428817_26,
author = {G\'{o}mez Hidalgo, Jos\'{e} Mar\'{\i}a and Carrero Garc\'{\i}a, Francisco and Puertas Sanz, Enrique},
title = {Named entity recognition for web content filtering},
year = {2005},
isbn = {3540260315},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11428817_26},
doi = {10.1007/11428817_26},
abstract = {Effective Web content filtering is a necessity in educational and workplace environments, but current approaches are far from perfect. We discuss a model for text-based intelligent Web content filtering, in which shallow linguistic analysis plays a key role. In order to demonstrate how this model can be realized, we have developed a lexical Named Entity Recognition system, and used it to improve the effectiveness of statistical Automated Text Categorization methods. We have performed several experiments that confirm this fact, and encourage the integration of other shallow linguistic processing techniques in intelligent Web content filtering.},
booktitle = {Proceedings of the 10th International Conference on Natural Language Processing and Information Systems},
pages = {286–297},
numpages = {12},
location = {Alicante, Spain},
series = {NLDB'05}
}

@inproceedings{10.1145/280765.280842,
author = {Andr\'{e}, Elisabeth and Rist, Thomas and M\"{u}ller, Jochen},
title = {Integrating reactive and scripted behaviors in a life-like presentation agent},
year = {1998},
isbn = {0897919831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/280765.280842},
doi = {10.1145/280765.280842},
booktitle = {Proceedings of the Second International Conference on Autonomous Agents},
pages = {261–268},
numpages = {8},
keywords = {human-like qualities of synthetic agents, life-like qualities, presentation agents},
location = {Minneapolis, Minnesota, USA},
series = {AGENTS '98}
}

@inproceedings{10.5555/645882.672394,
author = {Simon, Daniel and Eisenbarth, Thomas},
title = {Evolutionary Introduction of Software Product Lines},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines have proved to be a successful and efficient means for managing the development of software in industry. The significant benefits over traditional software architectures have the potential to convince software companies to adopt the product line approach for their existing products. In that case, the question arises how to best convert the existing products into a software product line. For several reasons, an evolutionary approach is desirable. But so far, there is little guidance on the evolutionary introduction of software product lines.In this paper, we propose a lightweight iterative process supporting the incremental introduction of product line concepts for existing software products. Starting with the analysis of the legacy code, we assess what parts of the software can be restructured for product line needs at reasonable costs. For the analysis of the products, we use feature analysis, a reengineering technique tailored to the specific needs of the initiation of software product lines.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {272–282},
numpages = {11},
series = {SPLC 2}
}

@article{10.1016/j.eswa.2009.09.003,
author = {Beg, Azam and Chandana Prasad, P. W.},
title = {Prediction of area and length complexity measures for binary decision diagrams},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.09.003},
doi = {10.1016/j.eswa.2009.09.003},
abstract = {Measuring the complexity of functions that represent digital circuits in non-uniform computation models is an important area of computer science theory. This paper presents a comprehensive set of machine learnt models for predicting the complexity properties of circuits represented by binary decision diagrams. The models are created using Monte Carlo data for a wide range of circuit inputs and number of minterms. The models predict number of nodes as representations of circuit size/area and path lengths: average path length, longest path length, and shortest path length. The models have been validated using an arbitrarily-chosen subset of ISCAS-85 and MCNC-91 benchmark circuits. The models yield reasonably low RMS errors for predictions, so they can be used to estimate complexity metrics of circuits without having to synthesize them.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2864–2873},
numpages = {10},
keywords = {Area complexity, Binary decision diagrams, Circuit complexity, Complexity prediction, Machine learning, Neural network modeling, Path length complexity}
}

@inproceedings{10.5555/2820668.2820675,
author = {Hamza, Mostafa and Walker, Robert J.},
title = {Recommending features and feature relationships from requirements documents for software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Feature models are a key element in software product lines, representing the supported features and their interrelationships within a family of software products. Recommendation systems for software engineering (RSSEs) are potentially useful in supporting the extraction, maintenance, and categorization of feature models. This paper focuses on the design and implementation of an RSSE to automatically recommend features for software product lines, the types of these features, and how they could be related to each other. Such a recommender should save time and tedium over doing the work manually. We present FFRE, a prototype recommendation tool for the extraction of features and their relationships from software requirements specification (SRS) documents. FFRE is based on natural language processing (NLP) techniques and heuristics. FFRE is evaluated qualitatively from four SRS documents and compared against other tools and approaches.},
booktitle = {Proceedings of the Fourth International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {25–31},
numpages = {7},
location = {Florence, Italy},
series = {RAISE '15}
}

@article{10.1016/j.compag.2018.04.005,
author = {Mu\~{n}oz-Benavent, P. and Andreu-Garc\'{\i}a, G. and Valiente-Gonz\'{a}lez, Jos\'{e} M. and Atienza-Vanacloig, V. and Puig-Pons, V. and Espinosa, V.},
title = {Enhanced fish bending model for automatic tuna sizing using computer vision},
year = {2018},
issue_date = {Jul 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {150},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2018.04.005},
doi = {10.1016/j.compag.2018.04.005},
journal = {Comput. Electron. Agric.},
month = jul,
pages = {52–61},
numpages = {10},
keywords = {Underwater stereo-vision, Computer vision, Fisheries management, Automatic fish sizing, Biomass estimation}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-grained test case prioritization for integration testing of delta-oriented software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Delta-Oriented Software Product Lines, Model-Based Integration Testing, Test Case Prioritization},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1007/978-3-030-27544-0_8,
author = {Szemenyei, Marton and Estivill-Castro, Vladimir},
title = {Real-Time Scene Understanding Using Deep Neural Networks for RoboCup SPL},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_8},
doi = {10.1007/978-3-030-27544-0_8},
abstract = {Convolutional neural networks (CNNs) are the state-of-the-art method for most computer vision tasks. But, the deployment of CNNs on mobile or embedded platforms is challenging because of CNNs’ excessive computational requirements. We present an end-to-end neural network solution to scene understanding for robot soccer. We compose two key neural networks: one to perform semantic segmentation on an image, and another to propagate class labels between consecutive frames. We trained our networks on synthetic datasets and fine-tuned them on a set consisting of real images from a Nao robot. Furthermore, we investigate and evaluate several practical methods for increasing the efficiency and performance of our networks. Finally, we present RoboDNN, a C++ neural network library designed for fast inference on the Nao robots.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {96–108},
numpages = {13},
keywords = {Computer vision, Deep learning, Semantic segmentation, Neural networks},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1007/11925231_36,
author = {Crawford, Broderick and Castro, Carlos and Monfroy, Eric},
title = {A hybrid ant algorithm for the airline crew pairing problem},
year = {2006},
isbn = {3540490264},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11925231_36},
doi = {10.1007/11925231_36},
abstract = {This article analyzes the performance of Ant Colony Optimization algorithms on the resolution of Crew Pairing Problem, one of the most critical processes in airline management operations. Furthermore, we explore the hybridization of Ant algorithms with Constraint Programming techniques. We show that, for the instances tested from Beasley's OR-Library, the use of this kind of hybrid algorithms obtains good results compared to the best performing metaheuristics in the literature.},
booktitle = {Proceedings of the 5th Mexican International Conference on Artificial Intelligence},
pages = {381–391},
numpages = {11},
keywords = {ant colony optimization, constraint programming, crew pairing optimization, hybrid algorithm, set partitioning problem},
location = {Apizaco, Mexico},
series = {MICAI'06}
}

@inproceedings{10.1145/2739480.2754720,
author = {Assun\c{c}\~{a}o, Wesley K.G. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Extracting Variability-Safe Feature Models from Source Code Dependencies in System Variants},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754720},
doi = {10.1145/2739480.2754720},
abstract = {To effectively cope with increasing customization demands, companies that have developed variants of software systems are faced with the challenge of consolidating all the variants into a Software Product Line, a proven development paradigm capable of handling such demands. A crucial step in this challenge is to reverse engineer feature models that capture all the required feature combinations of each system variant. Current research has explored this task using propositional logic, natural language, and search-based techniques. However, using knowledge from the implementation artifacts for the reverse engineering task has not been studied. We propose a multi-objective approach that not only uses standard precision and recall metrics for the combinations of features but that also considers variability-safety, i.e. the property that, based on structural dependencies among elements of implementation artifacts, asserts whether all feature combinations of a feature model are in fact well-formed software systems. We evaluate our approach with five case studies and highlight its benefits for the software engineer.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1303–1310},
numpages = {8},
keywords = {feature models, multi-objective evolutionary algorithms, reverse engineering},
location = {Madrid, Spain},
series = {GECCO '15}
}

@article{10.1016/j.engappai.2011.05.007,
author = {Barros, Heitor and Silva, Alan and Costa, Evandro and Bittencourt, Ig Ibert and Holanda, Olavo and Sales, Leandro},
title = {Steps, techniques, and technologies for the development of intelligent applications based on Semantic Web Services: A case study in e-learning systems},
year = {2011},
issue_date = {December, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {24},
number = {8},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2011.05.007},
doi = {10.1016/j.engappai.2011.05.007},
abstract = {Semantic Web Services domain has gained special attention in academia and industry. It has been adopted as a promise to enable automation of all aspects of Web Services provision and uses, such as service creation, selection, discovery, composition, and invocation. However, the development of intelligent systems based on Semantic Web Services (SWS) is still a complex and time-consuming task, mainly with respect to the choice and integration of technologies. In this paper, we discuss some empirical issues associated with the development process for such systems and propose a systematic way for building intelligent applications based on SWS by providing the development process with steps, techniques and technologies. In addition, one experiment concerning the implementation of a real e-learning system using the proposed approach is described. The evaluation results from this experiment showed that our approach has been effective and relevant in terms of improvements in the development process of intelligent applications based on SWS.},
journal = {Eng. Appl. Artif. Intell.},
month = dec,
pages = {1355–1367},
numpages = {13},
keywords = {Grinv Middleware, Intelligent Tutoring System, Ontology, Semantic Web Services}
}

@phdthesis{10.5555/1236863,
author = {Kosorukoff, Alexander Lvovich},
advisor = {Ray, Sylvian},
title = {Methods for cluster analysis and validation in microarray gene expression data},
year = {2006},
isbn = {9780542774423},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {Motivation. Unsupervised learning or clustering is frequently used to explore gene expression profiles for insight into both regulation and function. However, the quality of clustering results is often difficult to assess and each algorithm has tunable parameters with often no obvious way to choose appropriate values. Most algorithms also require the number of clusters to be predetermined yet this value is rarely known and, thus, is arrived at by subjective criteria. Here we present a method to systematically address these challenges using statistical evaluation. Method. The method presented compares the quality of clustering results in order to choose the most appropriate algorithm, distance metric and number of clusters for gene network discovery using objective criteria. In brief, two quality assessment metrics are used: the Consensus Share (CS) and the Feature Configuration Statistic (FCS). CS is the percentage of genes (not gene pairs) that are identically clustered in several clusterings and FCS is a measure of randomness of the observed configuration of transcription factor binding sites among clustered genes. Results. We evaluate this method using both artificial and yeast microarray data. By choosing parameters settings that minimize FCS values and maximize CS values we show major advantages over other clustering methods in particular for identifying combinatorially regulated groups of genes. The results produced provide remarkable enrichment for cis-regulatory elements in clusters of genes known to be regulated by such elements and evidence of extensive combinatorial regulation. Moreover, the method can be generalized when prior information about cis-regulatory sites is absent or it is desirable to calculate FCS values based on functional categorization.},
note = {AAI3223632}
}

@article{10.1016/j.compbiomed.2016.01.002,
author = {Bokov, Plamen and Mahut, Bruno and Flaud, Patrice and Delclaux, Christophe},
title = {Wheezing recognition algorithm using recordings of respiratory sounds at the mouth in a pediatric population},
year = {2016},
issue_date = {March 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {70},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.01.002},
doi = {10.1016/j.compbiomed.2016.01.002},
abstract = {BackgroundRespiratory diseases in children are a common reason for physician visits. A diagnostic difficulty arises when parents hear wheezing that is no longer present during the medical consultation. Thus, an outpatient objective tool for recognition of wheezing is of clinical value. MethodWe developed a wheezing recognition algorithm from recorded respiratory sounds with a Smartphone placed near the mouth. A total of 186 recordings were obtained in a pediatric emergency department, mostly in toddlers (mean age 20 months). After exclusion of recordings with artefacts and those with a single clinical operator auscultation, 95 recordings with the agreement of two operators on auscultation diagnosis (27 with wheezing and 68 without) were subjected to a two phase algorithm (signal analysis and pattern classifier using machine learning algorithms) to classify records. ResultsThe best performance (71.4% sensitivity and 88.9% specificity) was observed with a Support Vector Machine-based algorithm. We further tested the algorithm over a set of 39 recordings having a single operator and found a fair agreement (kappa=0.28, CI95% 0.12, 0.45) between the algorithm and the operator. ConclusionsThe main advantage of such an algorithm is its use in contact-free sound recording, thus valuable in the pediatric population. We recorded by Smartphone respiratory sounds at the mouth in pediatric population.Two clinical operators validated the presence or absence of wheezing in 97 toddlers.We used Short-Time Fourier Transform and SVM classifier for wheeze recognition.71.4% Sensitivity and 88.9% Specificity were observed for wheeze detection.An independent test found a fair agreement with a clinical operator.},
journal = {Comput. Biol. Med.},
month = mar,
pages = {40–50},
numpages = {11},
keywords = {Automated wheezing detection, Bronchiolitis, Childhood asthma, ROC analysis, Support vector machine}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Feature interactions, Highly configurable software systems, Machine learning, Performance prediction, Performance-influence models, Software product lines, Variability}
}

@article{10.1016/j.comnet.2021.108199,
author = {Arce, Pau and Salvo, David and Pi\~{n}ero, Gema and Gonzalez, Alberto},
title = {FIWARE based low-cost wireless acoustic sensor network for monitoring and classification of urban soundscape},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108199},
doi = {10.1016/j.comnet.2021.108199},
journal = {Comput. Netw.},
month = sep,
numpages = {10},
keywords = {Acoustic sensor networks, Urban sound classification, FIWARE, Edge computing}
}

@inproceedings{10.1109/ICTAI.2008.104,
author = {Guigang, Zhang and Wang, Shu and ChengZhi, Xu and Gong, Zhiyuan and Sheu, Phillip C-Y},
title = {A Semantic Programming Language SPL+ - A Preliminary Report},
year = {2008},
isbn = {9780769534404},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2008.104},
doi = {10.1109/ICTAI.2008.104},
abstract = {This paper is a preliminary report on the development of a "declarative" programming language SPL+. It assists non-technical people to write programs. The key idea behind SPL+ is that the "programmer" only needs to solve target problems with a standardized methodology without worrying about how to solve the problem efficiently.},
booktitle = {Proceedings of the 2008 20th IEEE International Conference on Tools with Artificial Intelligence - Volume 02},
pages = {274–281},
numpages = {8},
keywords = {semantic programming},
series = {ICTAI '08}
}

@article{10.1016/j.cogsys.2016.07.004,
author = {Gay, Simon L. and Mille, Alain and Georgeon, Olivier L. and Dutech, Alain},
title = {Autonomous construction and exploitation of a spatial memory by a self-motivated agent},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {41},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2016.07.004},
doi = {10.1016/j.cogsys.2016.07.004},
abstract = {We propose an architecture for self-motivated agents allowing them to construct their own knowledge of objects and of geometrical properties of space through interaction with their environment. Self-motivation is defined here as a tendency to experiment and to respond to behavioral opportunities afforded by the environment. Interactions have predefined valences that specify inborn behavioral preferences. The long-term goal is to design agents that construct their own knowledge of their environment through experience, rather than exploiting pre-coded knowledge. Over time, the agent learns relations between elements of the environment that afford its interactions, and its perception of these elements, in the form of data structures called signatures of interactions. These signatures allow the agent to attribute a low level semantics to elements that constitute its environment based on valences of interactions, without predefined knowledge about these elements and regardless of the number of element types. Signatures of interaction are then used to localize elements in space and to construct data structures that characterize spatial properties of space, called signatures of places and signatures of presence. Signatures of place and of presence characterize space using interactions rather than geometrical or topological properties. The agent uses these structures to maintain an egocentric representation of affordances of the surrounding environment, without any preconception about the elements that compose the environment, and without using notions of geometrical space. Experiments with simulated agents show that they learn to behave in their environment, taking into account multiple surrounding objects, reaching or avoiding objects according to the valence of the interactions that they afford.},
journal = {Cogn. Syst. Res.},
month = mar,
pages = {1–35},
numpages = {35},
keywords = {Affordance, Developmental learning, Interactionism, Intrinsic motivation, Learning (artificial intelligence), Spatial awareness}
}

@inproceedings{10.1007/11430919_8,
author = {Shima, Yoshikazu and Hirata, Kouichi and Harao, Masateru},
title = {Extraction of frequent few-overlapped monotone DNF formulas with depth-first pruning},
year = {2005},
isbn = {3540260765},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11430919_8},
doi = {10.1007/11430919_8},
abstract = {In this paper, first we introduce frequent few-overlapped monotone DNF formulas under the minimum supportσ, the minimum term supportτ and the maximum overlapλ. We say that a monotone DNF formula is frequent if the support of it is greater than σ and the support of each term (or itemset) in it is greater than τ, and few-overlapped if the overlap of it is less than λ and λ &lt; τ.Then, we design the algorithm ffo_dnf to extract them. The algorithm ffo_dnf first enumerates all of the maximal frequent itemsets under τ, and secondly connects the extracted itemsets by a disjunction ∨ until satisfying σ and λ. The first step of ffo_dnf, called a depth-first pruning, follows from the property that every pair of itemsets in a few-overlapped monotone DNF formula is incomparable under a subset relation. Furthermore, we show that the extracted formulas by ffo_dnf are representative.Finally, we apply the algorithm ffo_dnf to bacterial culture data.},
booktitle = {Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
pages = {50–60},
numpages = {11},
location = {Hanoi, Vietnam},
series = {PAKDD'05}
}

@inproceedings{10.5555/2997189.2997277,
author = {Goodman, Dan F. M. and Brette, Romain},
title = {Learning to localise sounds with spiking neural networks},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds. We present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems - Volume 1},
pages = {784–792},
numpages = {9},
keywords = {auditory perception &amp; modeling (primary), computational neural models, neuroscience, supervised learning (secondary)},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'10}
}

@article{10.1016/j.specom.2019.10.002,
author = {Howson, Phil J. and Monahan, Philip J.},
title = {Perceptual motivation for rhotics as a class},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.002},
doi = {10.1016/j.specom.2019.10.002},
journal = {Speech Commun.},
month = dec,
pages = {15–28},
numpages = {14},
keywords = {Speech Perception, Phonetics-phonology, Rhotic Typology, Rhotics, Natural, Classes}
}

@inproceedings{10.1145/3474085.3475335,
author = {Zhang, Ji and Song, Jingkuan and Yao, Yazhou and Gao, Lianli},
title = {Curriculum-Based Meta-learning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475335},
doi = {10.1145/3474085.3475335},
abstract = {Meta-learning offers an effective solution to learn new concepts with scarce supervision through an episodic training scheme: a series of target-like tasks sampled from base classes are sequentially fed into a meta-learner to extract common knowledge across tasks, which can facilitate the quick acquisition of task-specific knowledge of the target task with few samples. Despite its noticeable improvements, the episodic training strategy samples tasks randomly and uniformly, without considering their hardness and quality, which may not progressively improve the meta-leaner's generalization ability. In this paper, we present a Curriculum-Based Meta-learning (CubMeta) method to train the meta-learner using tasks from easy to hard. Specifically, the framework of CubMeta is in a progressive way, and in each step, we design a module named BrotherNet to establish harder tasks and an effective learning scheme for obtaining an ensemble of stronger meta-learners. In this way, the meta-learner's generalization ability can be progressively improved, and better performance can be obtained even with fewer training tasks. We evaluate our method for few-shot classification on two benchmarks - mini-ImageNet and tiered-ImageNet, where it achieves consistent performance improvements on various meta-learning paradigms.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1838–1846},
numpages = {9},
keywords = {curriculum learning, few-shot learning, meta-learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3382507.3418889,
author = {Yan, Shen and Huang, Di and Soleymani, Mohammad},
title = {Mitigating Biases in Multimodal Personality Assessment},
year = {2020},
isbn = {9781450375818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382507.3418889},
doi = {10.1145/3382507.3418889},
abstract = {As algorithmic decision making systems are increasingly used in high-stake scenarios, concerns have risen about the potential unfairness of these decisions to certain social groups. Despite its importance, the bias and fairness of multimodal systems are not thoroughly studied. In this work, we focus on the multimodal systems designed for apparent personality assessment and hirability prediction. We use the First Impression dataset as a case study to investigate the biases in such systems. We provide detailed analyses on the biases from different modalities and data fusion strategies. Our analyses reveal that different modalities show various patterns of biases and data fusion process also introduces additional biases to the model. To mitigate the biases, we develop and evaluate two different debiasing approaches based on data balancing and adversarial learning. Experimental results show that both approaches can reduce the biases in model outcomes without sacrificing much performance. Our debiasing strategies can be deployed in real-world multimodal systems to provide fairer outcomes.},
booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
pages = {361–369},
numpages = {9},
keywords = {bias, fairness, multimodal modeling, personality assessment},
location = {Virtual Event, Netherlands},
series = {ICMI '20}
}

@inproceedings{10.1109/SBES.2009.26,
author = {Cirilo, Elder and Nunes, Ingrid and Kulesza, Uir\'{a} and Lucena, Carlos},
title = {Automating the Product Derivation Process of Multi-agent Systems Product Lines},
year = {2009},
isbn = {9780769538440},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SBES.2009.26},
doi = {10.1109/SBES.2009.26},
abstract = {Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work explores the integration between them to allow reuse and variability management in the context of complex systems. However, the automatic product derivation process is not addressed in the current literature. In this paper, we present our approach to deal with multi-agent systems product lines (MAS-PL) variability management and automatic product derivation. Our approach is implemented as an extension of the GenArch product derivation tool. A case study illustrates how the proposed approach can be used to derive products (instances) from a MAS-PL.},
booktitle = {Proceedings of the 2009 XXIII Brazilian Symposium on Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {Application Engineering, Model-driven Development, Multi-agent Systems, Product Derivation Tool, Software Product Lines},
series = {SBES '09}
}

@article{10.1016/j.patcog.2008.04.016,
author = {Shen, Jialie},
title = {Stochastic modeling western paintings for effective classification},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {2},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.04.016},
doi = {10.1016/j.patcog.2008.04.016},
abstract = {As one of the most important cultural heritages, classical western paintings have always played a special role in human live and been applied for many different purposes. While image classification is the subject of a plethora of related publications, relatively little attention has been paid to automatic categorization of western classical paintings which could be a key technique of modern digital library, museums and art galleries. This paper studies automatic classification on large western painting image collection. We propose a novel framework to support automatic classification on large western painting image collections. With this framework, multiple visual features can be integrated effectively to improve the accuracy of identification process significantly. We also evaluate our method and its competitors based on a large image collection. A careful study on the empirical results indicates the approach enjoys great superiority over the state-of-the-art approaches in different aspects.},
journal = {Pattern Recogn.},
month = feb,
pages = {293–301},
numpages = {9},
keywords = {Classic western paintings, Cultural heritage, Identification, Image retrieval}
}

@inproceedings{10.5555/3016387.3016471,
author = {Gotlieb, Arnaud and Carlsson, Mats and Liaaen, Marius and Marijan, Dusica and P\'{e}tillon, Alexandre},
title = {Automated regression testing using constraint programming},
year = {2016},
publisher = {AAAI Press},
abstract = {In software validation, regression testing aims to check the absence of regression faults in new releases of a software system. Typically, test cases used in regression testing are executed during a limited amount of time and are selected to check a given set of user requirements. When testing large systems, the number of regression tests grows quickly over the years, and yet the available time slot stays limited. In order to overcome this problem, an approach known as test suite reduction (TSR), has been developed in software engineering to select a smallest subset of test cases, so that each requirement remains covered at least once. However solving the TSR problem is difficult as the underlying optimization problem is NP-hard, but it is also crucial for vendors interested in reducing the time to market of new software releases. In this paper, we address regression testing and TSR with Constraint Programming (CP). More specifically, we propose new CP models to solve TSR that exploit global constraints, namely NVALUE and GCC. We reuse a set of preprocessing rules to reduce a priori each instance, and we introduce a structure-aware search heuristic. We evaluated our CP models and proposed improvements against existing approaches, including a simple greedy approach and MINTS, the state-of-the-art tool of the software engineering community. Our experiments show that CP outperforms both the greedy approach and MINTS when it is interfaced with MiniSAT, in terms of percentage of reduction and execution time. When MINTS is interfaced with CPLEX, we show that our CP model performs better only on percentage of reduction. Finally, by working closely with validation engineers from Cisco Systems, Norway, we integrated our CP model into an industrial regression testing process.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4010–4015},
numpages = {6},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1109/TASLP.2020.3019646,
author = {Nguyen, Thi Ngoc Tho and Gan, Woon-Seng and Ranjan, Rishabh and Jones, Douglas L.},
title = {Robust Source Counting and DOA Estimation Using Spatial Pseudo-Spectrum and Convolutional Neural Network},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3019646},
doi = {10.1109/TASLP.2020.3019646},
abstract = {Many signal processing-based methods for sound source direction-of-arrival estimation produce a spatial pseudo-spectrum of which the local maxima strongly indicate the source directions. Due to different levels of noise, reverberation and different number of overlapping sources, the spatial pseudo-spectra are noisy even after smoothing. In addition, the number of sources is often unknown. As a result, selecting the peaks from these spectra is susceptible to error. Convolutional neural network has been successfully applied to many image processing problems in general and direction-of-arrival estimation in particular. In addition, deep learning-based methods for direction-of-arrival estimation show good generalization to different environments. We propose to use a 2D convolutional neural network with multi-task learning to robustly estimate the number of sources and the directions-of-arrival from short-time spatial pseudo-spectra, which have useful directional information from audio input signals. This approach reduces the tendency of the neural network to learn unwanted association between sound classes and directional information, and helps the network generalize to unseen sound classes. The simulation and experimental results show that the proposed methods outperform other directional-of-arrival estimation methods in different levels of noise and reverberation, and different number of sources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {2626–2637},
numpages = {12}
}

@article{10.1007/s10270-014-0414-4,
author = {Wittern, Erik and Zirpins, Christian},
title = {Service feature modeling: modeling and participatory ranking of service design alternatives},
year = {2016},
issue_date = {May       2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-014-0414-4},
doi = {10.1007/s10270-014-0414-4},
abstract = {The design of software-intensive service systems involves and affects numerous stakeholders including software engineers, legal and business experts as well as a potentially large number of consumers. In consequence, the challenge arises to adequately represent the interests of these groups with respect to service design decisions. Specifically, shared service design artifacts and participatory methods for influencing their development in consensus are required, which are not yet state of the art in software service engineering. To this end, we present service feature modeling. Using a modeling notation based on feature-oriented analysis, our approach can represent and interrelate diverse service design concerns and capture their potential combinations as service design alternatives. We further present a method that allows stakeholders to rank service design alternatives based on their preferences. The ranking can support service engineers in selecting viable alternatives for implementation. To exploit this potential, we have implemented a toolkit to enable both modeling and participative ranking of service design alternatives. It has been used to apply service feature modeling in the context of public service design and evaluate the approach in this context.},
journal = {Softw. Syst. Model.},
month = may,
pages = {553–578},
numpages = {26},
keywords = {Multi-criteria feature configuration decisions, Participatory design, Service variation modeling, Software service engineering}
}

@article{10.1007/s10994-018-5710-8,
author = {Muggleton, Stephen and Dai, Wang-Zhou and Sammut, Claude and Tamaddoni-Nezhad, Alireza and Wen, Jing and Zhou, Zhi-Hua},
title = {Meta-Interpretive Learning from noisy images},
year = {2018},
issue_date = {July      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {107},
number = {7},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-018-5710-8},
doi = {10.1007/s10994-018-5710-8},
abstract = {Statistical machine learning is widely used in image classification. However, most techniques (1) require many images to achieve high accuracy and (2) do not provide support for reasoning below the level of classification, and so are unable to support secondary reasoning, such as the existence and position of light sources and other objects outside the image. This paper describes an Inductive Logic Programming approach called Logical Vision which overcomes some of these limitations. LV uses Meta-Interpretive Learning (MIL) combined with low-level extraction of high-contrast points sampled from the image to learn recursive logic programs describing the image. In published work LV was demonstrated capable of high-accuracy prediction of classes such as regular polygon from small numbers of images where Support Vector Machines and Convolutional Neural Networks gave near random predictions in some cases. LV has so far only been applied to noise-free, artificially generated images. This paper extends LV by (a) addressing classification noise using a new noise-telerant version of the MIL system Metagol, (b) addressing attribute noise using primitive-level statistical estimators to identify sub-objects in real images, (c) using a wider class of background models representing classical 2D shapes such as circles and ellipses, (d) providing richer learnable background knowledge in the form of a simple but generic recursive theory of light reflection. In our experiments we consider noisy images in both natural science settings and in a RoboCup competition setting. The natural science settings involve identification of the position of the light source in telescopic and microscopic images, while the RoboCup setting involves identification of the position of the ball. Our results indicate that with real images the new noise-robust version of LV using a single example (i.e. one-shot LV) converges to an accuracy at least comparable to a thirty-shot statistical machine learner on both prediction of hidden light sources in the scientific settings and in the RoboCup setting. Moreover, we demonstrate that a general background recursive theory of light can itself be invented using LV and used to identify ambiguities in the convexity/concavity of objects such as craters in the scientific setting and partial obscuration of the ball in the RoboCup setting.},
journal = {Mach. Learn.},
month = jul,
pages = {1097–1118},
numpages = {22},
keywords = {High-level vision, Inductive logic programming, Meta-interpretive learning, Robocup}
}

@article{10.1016/j.cmpb.2009.03.001,
author = {Moca, Vasile V. and Scheller, Bertram and Mure\c{s}an, Raul C. and Daunderer, Michael and Pipa, Gordon},
title = {EEG under anesthesia-Feature extraction with TESPAR},
year = {2009},
issue_date = {September, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {95},
number = {3},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2009.03.001},
doi = {10.1016/j.cmpb.2009.03.001},
abstract = {We investigated the problem of automatic depth of anesthesia (DOA) estimation from electroencephalogram (EEG) recordings. We employed Time Encoded Signal Processing And Recognition (TESPAR), a time-domain signal processing technique, in combination with multi-layer perceptrons to identify DOA levels. The presented system learns to discriminate between five DOA classes assessed by human experts whose judgements were based on EEG mid-latency auditory evoked potentials (MLAEPs) and clinical observations. We found that our system closely mimicked the behavior of the human expert, thus proving the utility of the method. Further analyses on the features extracted by our technique indicated that information related to DOA is mostly distributed across frequency bands and that the presence of high frequencies (&gt;80Hz), which reflect mostly muscle activity, is beneficial for DOA detection.},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
pages = {191–202},
numpages = {12},
keywords = {Depth of anesthesia, EEG, MLAEP, MLP, TESPAR}
}

@inproceedings{10.5555/3298483.3298519,
author = {Gong, Chen},
title = {Exploring commonality and individuality for multi-modal curriculum learning},
year = {2017},
publisher = {AAAI Press},
abstract = {Curriculum Learning (CL) mimics the cognitive process of humans and favors a learning algorithm to follow the logical learning sequence from simple examples to more difficult ones. Recent studies show that selecting the simplest curriculum examples from different modalities for graph-based label propagation can yield better performance than simply leveraging single modality. However, they forcibly require the curriculums generated by all modalities to be identical to a common curriculum, which discard the individuality of every modality and produce the inaccurate curriculum for the subsequent learning. Therefore, this paper proposes a novel multi-modal CL algorithm by comprehensively investigating both the individuality and commonality of different modalities. By considering the curriculums of multiple modalities altogether, their common preference on selecting the simplest examples can be explored by a row-sparse matrix, and their distinct opinions are captured by a sparse noise matrix. As a consequence, a "soft" fusion of multiple curriculums from different modalities is achieved and the propagation quality can thus be improved. Comprehensive empirical studies reveal that our method can generate higher accuracy than the state-of-the-art multi-modal CL approach and label propagation algorithms on various image classification tasks.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {1926–1933},
numpages = {8},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1007/s11633-021-1299-7,
author = {Ramalepa, Larona Pitso and Jamisola, Rodrigo S.},
title = {A Review on Cooperative Robotic Arms with Mobile or Drones Bases},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {4},
issn = {1476-8186},
url = {https://doi.org/10.1007/s11633-021-1299-7},
doi = {10.1007/s11633-021-1299-7},
abstract = {This review paper focuses on cooperative robotic arms with mobile or drone bases performing cooperative tasks. This is because cooperative robots are often used as risk-reduction tools to human life. For example, they are used to explore dangerous places such as minefields and disarm explosives. Drones can be used to perform tasks such as aerial photography, military and defense missions, agricultural surveys, etc. The bases of the cooperative robotic arms can be stationary, mobile (ground), or drones. Cooperative manipulators allow faster performance of assigned tasks because of the available “extra hand”. Furthermore, a mobile base increases the reachable ground workspace of cooperative manipulators while a drone base drastically increases this workspace to include the aerial space. The papers in this review are chosen to extensively cover a wide variety of cooperative manipulation tasks and industries that use them. In cooperative manipulation, avoiding self-collision is one of the most important tasks to be performed. In addition, path planning and formation control can be challenging because of the increased number of components to be coordinated.},
journal = {Int. J. Autom. Comput.},
month = aug,
pages = {536–555},
numpages = {20},
keywords = {Cooperative arms, mobile manipulator, aerial manipulator, mobile base, drone base, cooperative tasks}
}

@article{10.1016/j.patrec.2008.12.005,
author = {Hu, Zhilan and Wang, Guijin and Lin, Xinggang and Yan, Hong},
title = {Recovery of upper body poses in static images based on joints detection},
year = {2009},
issue_date = {April, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {30},
number = {5},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2008.12.005},
doi = {10.1016/j.patrec.2008.12.005},
abstract = {Recovering human body poses from static images is challenging without prior knowledge of pose, appearance, background and clothing. In this paper, we propose a novel model-based upper poses recovery method via effective joints detection. In our research, three observables are firstly detected: face, skin, and torso. Then the joints are properly initialized according to the observables and some heuristic configuration constraints. Finally the sample-based Markov chain Monte Carlo (MCMC) method is employed to determine the final pose. The main contributions of this paper include a robust torso detector through maximizing a posterior estimation, effective joints initialization, and two continuous likelihood functions developed for effective pose inference. Experiments on 250 real world images show that our method can accurately recover upper body poses from images with a variety of individuals, poses, backgrounds and clothing.},
journal = {Pattern Recogn. Lett.},
month = apr,
pages = {503–512},
numpages = {10},
keywords = {Gaussian mixture model, Markov chain Monte Carlo, Pose estimation, Torso detection}
}

@article{10.1016/j.infsof.2011.09.003,
author = {Conejero, Jos\'{e} M. and Figueiredo, Eduardo and Garcia, Alessandro and Hern\'{a}ndez, Juan and Jurado, Elena},
title = {On the relationship of concern metrics and requirements maintainability},
year = {2012},
issue_date = {February, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.003},
doi = {10.1016/j.infsof.2011.09.003},
abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {212–238},
numpages = {27},
keywords = {Concern metrics, Crosscutting, Maintainability, Product lines, Requirements engineering, Stability}
}

@inbook{10.5555/2167748.2167761,
author = {Wojna, Arkadiusz},
title = {Analogy-based reasoning in classifier construction},
year = {2005},
isbn = {3540298304},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Analogy-based reasoning methods in machine learning make it possible to reason about properties of objects on the basis of similarities between objects. A specific similarity based method is the k nearest neighbors (k-nn) classification algorithm. In the k-nn algorithm, a decision about a new object x is inferred on the basis of a fixed number k of the objects most similar to x in a given set of examples. The primary contribution of the dissertation is the introduction of two new classification models based on the k-nn algorithm.The first model is a hybrid combination of the k-nn algorithm with rule induction. The proposed combination uses minimal consistent rules defined by local reducts of a set of examples. To make this combination possible the model of minimal consistent rules is generalized to a metric-dependent form. An effective polynomial algorithm implementing the classification model based on minimal consistent rules has been proposed by Bazan. We modify this algorithm in such a way that after addition of the modified algorithm to the k-nn algorithm the increase of the computation time is inconsiderable. For some tested classification problems the combined model was significantly more accurate than the classical k-nn classification algorithm.For many real-life problems it is impossible to induce relevant global mathematical models from available sets of examples. The second model proposed in the dissertation is a method for dealing with such sets based on locally induced metrics. This method adapts the notion of similarity to the properties of a given test object. It makes it possible to select the correct decision in specific fragments of the space of objects. The method with local metrics improved significantly the classification accuracy of methods with global models in the hardest tested problems.The important issues of quality and efficiency of the k-nn based methods are a similarity measure and the performance time in searching for the most similar objects in a given set of examples, respectively. In this dissertation both issues are studied in detail and some significant improvements are proposed for the similarity measures and for the search methods found in the literature.},
booktitle = {Transactions on Rough Sets IV},
pages = {277–374},
numpages = {98}
}

@article{10.1016/j.cviu.2016.02.002,
author = {Timofte, Radu and Kwon, Junseok and Van Gool, Luc},
title = {PICASO},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {153},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2016.02.002},
doi = {10.1016/j.cviu.2016.02.002},
abstract = {We introduce PICASO visual tracker based on PIxel Correspondences And SOft match selection.We analyze fast soft match selection techniques for correspondences mining.We propose efficient masks and median filtering for occlusion handling.We achieve s-o-t-a performance for rigid and non-rigid object tracking on three large benchmarks. Visual tracking is one of the computer visions longstanding challenges, with many methods as a result. While most state-of-the-art methods trade-off performance for speed, we propose PICASO, an efficient, yet strongly performing tracking scheme. The target object is modeled as a set of pixel-level templates with weak configuration constraints. The pixels of a search window are matched against those of the surrounding context and of the object model. To increase the robustness, we match also from the object to the search window, and the pairs matching in both directions are the correspondences used to localize. This localization process is robust, also against occlusions which are explicitly modeled. Another source of robustness is that the model as in several other modern trackers gets constantly updated over time with newly incoming information about the target appearance. Each pixel is described by its local neighborhood. The match of a pixel is taken to be the one with the largest contribution in its sparse decomposition over a set of pixels. For this soft match selection, we analyze both l1 and l2-regularized least squares formulations and the recently proposed l1-constrained Iterative Nearest Neighbors approach. We evaluate our tracker on standard videos for rigid and non-rigid object tracking. We obtain excellent performance at 42fps with Matlab on a CPU.},
journal = {Comput. Vis. Image Underst.},
month = dec,
pages = {151–162},
numpages = {12},
keywords = {Iterative nearest neighbors, Linear decomposition, Non-rigid tracking, Pixel correspondences, Visual tracking}
}

@article{10.1016/j.jss.2019.03.064,
author = {Pradhan, Dipesh and Wang, Shuai and Ali, Shaukat and Yue, Tao and Liaaen, Marius},
title = {Employing rule mining and multi-objective search for dynamic test case prioritization},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {153},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.064},
doi = {10.1016/j.jss.2019.03.064},
journal = {J. Syst. Softw.},
month = jul,
pages = {86–104},
numpages = {19},
keywords = {Multi-objective optimization, Rule mining, Dynamic test case prioritization, Search, Black-box regression testing}
}

@inproceedings{10.5555/3045390.3045598,
author = {Michaeli, Tomer and Wang, Weiran and Livescu, Karen},
title = {Nonparametric canonical correlation analysis},
year = {2016},
publisher = {JMLR.org},
abstract = {Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster's theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1967–1976},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.5555/3305890.3305963,
author = {Pad, Pedram and Salehi, Farnood and Celis, Elisa and Thiran, Patrick and Unser, Michael},
title = {Dictionary learning based on sparse distribution tomography},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new statistical dictionary learning algorithm for sparse signals that is based on an α-stable innovation model. The parameters of the underlying model—that is, the atoms of the dictionary, the sparsity index α and the dispersion of the transform-domain coefficients—are recovered using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm. Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily α-stable. We evaluate our algorithm by performing two types of experiments: image in-painting and image denoising. In both cases, we find that our approach is competitive with state-of-the-art dictionary learning techniques. Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of ℓp-norms that constitutes the foundation of our approach.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2731–2740},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1145/2658761.2658768,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki},
title = {Efficient testing of software product lines via centralization (short paper)},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658768},
doi = {10.1145/2658761.2658768},
abstract = {Software product line~(SPL) engineering manages families of software products that share common features. However, cost-effective test case generation for an SPL is challenging. Applying existing test case generation techniques to each product variant separately may test common code in a redundant way. Moreover, it is difficult to share the test results among multiple product variants. In this paper, we propose the use of centralization, which combines multiple product variants from the same SPL and generates test cases for the entire system. By taking into account all variants, our technique generally avoids generating redundant test cases for common software components. Our case study on three SPLs shows that compared with testing each variant independently, our technique is more efficient and achieves higher test coverage.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {49–52},
numpages = {4},
keywords = {Software Product Lines, automatic test generation, random testing},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@article{10.1504/IJCSE.2017.082882,
title = {Hierarchical regression test case selection using slicing},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {2},
issn = {1742-7185},
url = {https://doi.org/10.1504/IJCSE.2017.082882},
doi = {10.1504/IJCSE.2017.082882},
abstract = {In this paper, we propose a novel regression test case selection approach by decomposing an object-oriented OO program into packages, classes, methods and statements that are affected by some modification made to the program. This decomposition is based on the proposed hierarchical slicing of an OO program. By mapping these decompositions to the existing test suite, we select a new reduced regression test suite and add some new test cases, if necessary, to retest the modified program. We apply hierarchical slicing on a suitable intermediate graph proposed for representing an OO program. This intermediate graph representation corresponds to all the possible dependences among the different parts of an OO program. We improve the scalability of the intermediate graph to a considerable extent by identifying and removing the redundant edges from the graph and thus detect the affected program parts in less time. The average reduction in time achieved for all the ten programs under experimentation is approximately 28.1%. The test cases that cover these affected parts of the program are then selected for regression testing. The average reduction in the number of test cases selected for regression testing of experimental programs is approximately 56.3%.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {179–197},
numpages = {19}
}

@inproceedings{10.5555/2888116.2888161,
author = {Zhao, Qian and Meng, Deyu and Jiang, Lu and Xie, Qi and Xu, Zongben and Hauptmann, Alexander G.},
title = {Self-paced learning for matrix factorization},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Matrix factorization (MF) has been attracting much attention due to its wide applications. However, since MF models are generally non-convex, most of the existing methods are easily stuck into bad local minima, especially in the presence of outliers and missing data. To alleviate this deficiency, in this study we present a new MF learning methodology by gradually including matrix elements into MF training from easy to complex. This corresponds to a recently proposed learning fashion called self-paced learning (SPL), which has been demonstrated to be beneficial in avoiding bad local minima. We also generalize the conventional binary (hard) weighting scheme for SPL to a more effective real-valued (soft) weighting manner. The effectiveness of the proposed self-paced MF method is substantiated by a series of experiments on synthetic, structure from motion and background subtraction data.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {3196–3202},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@article{10.1007/s11334-006-0022-8,
author = {Pecora, Federico and Rasconi, Riccardo and Cortellessa, Gabriella and Cesta, Amedeo},
title = {User-oriented problem abstractions in scheduling},
year = {2006},
issue_date = {Mar 2006},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {1},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-006-0022-8},
doi = {10.1007/s11334-006-0022-8},
abstract = {In this paper we describe a modeling framework aimed at facilitating the customization and deployment of artificial intelligence (AI) scheduling technology in real-world contexts. Specifically, we describe an architecture aimed at facilitating software product line development in the context of scheduling systems. The framework is based on two layers of abstraction: a first layer providing an interface with the scheduling technology, on top of which we define a formalism to abstract domain-specific concepts. We show how this two-layer modeling framework provides a versatile formalism for defining user-oriented problem abstractions, which is pivotal for facilitating interaction between domain experts and technologists. Moreover, we describe a graphical user interface (GUI)-enhanced tool which allows the domain expert to interact with the underlying core scheduling technology in domain-specific terms. This is achieved by automatically instantiating an abstract GUI template on top of the second modeling layer.},
journal = {Innov. Syst. Softw. Eng.},
month = mar,
pages = {1–16},
numpages = {16},
keywords = {Domain elicitation, Scheduling Tool customization, Reuse, Fast prototyping}
}

@inproceedings{10.1007/978-3-319-07317-0_8,
author = {Lochau, Malte and Peldszus, Sven and Kowal, Matthias and Schaefer, Ina},
title = {Model-Based Testing},
year = {2014},
isbn = {9783319073163},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07317-0_8},
doi = {10.1007/978-3-319-07317-0_8},
abstract = {Software more and more pervades our everyday lives. Hence, we have high requirements towards the trustworthiness of the software. Software testing greatly contributes to the quality assurance of modern software systems. However, as today's software system get more and more complex and exist in many different variants, we need rigorous and systematic approaches towards software testing. In this tutorial, we, first, present model-based testing as an approach for systematic test case generation, test execution and test result evaluation for single system testing. The central idea of model-based testing is to base all testing activities on an executable model-based test specification. Second, we consider model-based testing for variant-rich software systems and review two model-based software product line testing techniques. Sample-based testing generates a set of representative variants for testing, and variability-aware product line testing uses a family-based test model which contains the model-based specification of all considered product variants.},
booktitle = {Advanced Lectures of the 14th International School on Formal Methods for Executable Software Models - Volume 8483},
pages = {310–342},
numpages = {33}
}

@article{10.1016/j.patcog.2008.05.019,
author = {Farhangfar, Alireza and Kurgan, Lukasz and Dy, Jennifer},
title = {Impact of imputation of missing values on classification error for discrete data},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {12},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.05.019},
doi = {10.1016/j.patcog.2008.05.019},
abstract = {Numerous industrial and research databases include missing values. It is not uncommon to encounter databases that have up to a half of the entries missing, making it very difficult to mine them using data analysis methods that can work only with complete data. A common way of dealing with this problem is to impute (fill-in) the missing values. This paper evaluates how the choice of different imputation methods affects the performance of classifiers that are subsequently used with the imputed data. The experiments here focus on discrete data. This paper studies the effect of missing data imputation using five single imputation methods (a mean method, a Hot deck method, a Nai@?ve-Bayes method, and the latter two methods with a recently proposed imputation framework) and one multiple imputation method (a polytomous regression based method) on classification accuracy for six popular classifiers (RIPPER, C4.5, K-nearest-neighbor, support vector machine with polynomial and RBF kernels, and Nai@?ve-Bayes) on 15 datasets. This experimental study shows that imputation with the tested methods on average improves classification accuracy when compared to classification without imputation. Although the results show that there is no universally best imputation method, Nai@?ve-Bayes imputation is shown to give the best results for the RIPPER classifier for datasets with high amount (i.e., 40% and 50%) of missing data, polytomous regression imputation is shown to be the best for support vector machine classifier with polynomial kernel, and the application of the imputation framework is shown to be superior for the support vector machine with RBF kernel and K-nearest-neighbor. The analysis of the quality of the imputation with respect to varying amounts of missing data (i.e., between 5% and 50%) shows that all imputation methods, except for the mean imputation, improve classification error for data with more than 10% of missing data. Finally, some classifiers such as C4.5 and Nai@?ve-Bayes were found to be missing data resistant, i.e., they can produce accurate classification in the presence of missing data, while other classifiers such as K-nearest-neighbor, SVMs and RIPPER benefit from the imputation.},
journal = {Pattern Recogn.},
month = dec,
pages = {3692–3705},
numpages = {14},
keywords = {Classification, Imputation of missing values, Missing values, Multiple imputations, Single imputation}
}

@inproceedings{10.5555/1036843.1036858,
author = {Dechter, Rina and Mateescu, Robert},
title = {Mixtures of deterministic-probabilistic networks and their AND/OR search space},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The paper introduces &lt;i&gt;mixed networks,&lt;/i&gt; a new framework for expressing and reasoning with probabilistic and deterministic information. The framework combines belief networks with constraint networks, defining the semantics and graphical representation. We also introduce the AND/OR search space for graphical models, and develop a new linear space search algorithm. This provides the basis for understanding the benefits of processing the constraint information separately, resulting in the pruning of the search space. When the constraint part is tractable or has a small number of solutions, using the mixed representation can be exponentially more effective than using pure belief networks which model constraints as conditional probability tables.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {120–129},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/2832581.2832741,
author = {Gu, Bin and Sheng, Victor S. and Li, Shuo},
title = {Bi-parameter space partition for cost-sensitive SVM},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Model selection is an important problem of costsensitive SVM (CS-SVM). Although using solution path to find global optimal parameters is a powerful method for model selection, it is a challenge to extend the framework to solve two regularization parameters of CS-SVM simultaneously. To overcome this challenge, we make three main steps in this paper. (i) A critical-regions-based biparameter space partition algorithm is proposed to present all piecewise linearities of CS-SVM. (ii) An invariant-regions-based bi-parameter space partition algorithm is further proposed to compute empirical errors for all parameter pairs. (iii) The global optimal solutions for K-fold cross validation are computed by superposing K invariant region based bi-parameter space partitions into one. The three steps constitute the model selection of CS-SVM which can find global optimal parameter pairs in K-fold cross validation. Experimental results on seven normal datsets and four imbalanced datasets, show that our proposed method has better generalization ability and than various kinds of grid search methods, however, with less running time.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3532–3539},
numpages = {8},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inbook{10.5555/3454287.3454823,
author = {Wang, Siqi and Zeng, Yijie and Liu, Xinwang and Zhu, En and Yin, Jianping and Xu, Chuanfu and Kloft, Marius},
title = {Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite the wide success of deep neural networks (DNN), little progress has been made on end-to-end unsupervised outlier detection (UOD) from high dimensional data like raw images. In this paper, we propose a framework named E3 Outlier, which can perform UOD in a both effective and end-to-end manner: First, instead of the commonly-used autoencoders in previous end-to-end UOD methods, E3 Outlier for the first time leverages a discriminative DNN for better representation learning, by using surrogate supervision to create multiple pseudo classes from original unla-belled data. Next, unlike classic UOD that utilizes data characteristics like density or proximity, we exploit a novel property named inlier priority to enable end-to-end UOD by discriminative DNN. We demonstrate theoretically and empirically that the intrinsic class imbalance of inliers/outliers will make the network prioritize minimizing inliers' loss when inliers/outliers are indiscriminately fed into the network for training, which enables us to differentiate outliers directly from DNN's outputs. Finally, based on inlier priority, we propose the negative entropy based score as a simple and effective outlierness measure. Extensive evaluations show that E3 Outlier significantly advances UOD performance by up to 30% AUROC against state-of-the-art counterparts, especially on relatively difficult benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {536},
numpages = {14}
}

@article{10.1016/j.engappai.2011.02.012,
author = {Wang, Yaonan and Ge, Ji and Zhang, Hui and Zhou, Bowen},
title = {Intelligent injection liquid particle inspection machine based on two-dimensional Tsallis Entropy with modified pulse-coupled neural networks},
year = {2011},
issue_date = {June, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {24},
number = {4},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2011.02.012},
doi = {10.1016/j.engappai.2011.02.012},
abstract = {The Automatic Liquid Particle Inspection Machine (AIM) using 2-D Tsallis Entropy with modified pulse-coupled neural networks (PCNN) is used in order to detect visible foreign particles within injection fluids. According to the motion of the particles in liquid, appropriate mechanisms are utilized which guarantees that the inspection machine will follow detection procedures: ''Rotation, Abruptly Braking, Video Tracking'' to extract tiny objects from complicated sequential images. In order to reduce the influence derived from air bubbles, improved spin/stop techniques are applied. The external capture mode of CCD cameras is used to avoid the possibility of omitting certain particles by trivial displacement. 2-D Tsallis Entropy with modified PCNN is applied in order to segment the difference images, and then to judge the existence of foreign particles according to the continuity and smoothness of their traces. Preliminary experimental results (125ml 0.9% sodium chloride solution and 10% glucose as the samples) indicate that the inspection machine, which is superior to proficient inspectors, can detect the visible foreign particles effectively and that this detection speed and accuracy, as well as the correct detection rate can also facilitate the medicinal construction.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {625–637},
numpages = {13},
keywords = {Intelligent inspection machine, Liquid particle inspection, Modified PCNN, Tsallis Entropy, Two-dimensional histogram}
}

@article{10.3233/JIFS-169433,
author = {Srivastava, Smriti and Gopal and Bhardwaj, Saurabh and Thampi, Sabu M. and El-Alfy, El-Sayed M. and Mitra, Sushmita and Trajkovic, Ljiljana},
title = {Multi-scenario dataset for speaker recognition},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {34},
number = {3},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169433},
doi = {10.3233/JIFS-169433},
abstract = {The present work describes different research techniques for collecting and organizing speech database in different scenario at the institute and successfully structuring the text independent speaker identification database in Indian context. In order to get the Multi-Scenario dataset, each speaker performed multiple sessions recording in reading style with English and Hindi language with same passages but under different conditions. This work analyzed different scenario affecting the performance of speaker recognition system when tested under dissimilar training conditions. Here four different scenarios are considered; sensor and environment, language, aging and health. To study the effect of sensor, language and environment on the performance of ASR system a database of 200 speaker was created. Under different environmental conditions, four different types of sensors in parallel configuration were used to study the sensor mismatch conditions over testing and training phase. The database contains speech samples of the individual in English and Hindi in read speech styles under two environment i.e. a controlled recording chamber and library. To study the aging effect, an aging NSIT speaker database (AG-NSIT-SD) of 53 famous personalities was collected from online source varying over a period of 10–20 years. Further to study the effect of health, a cough and cold NSIT speaker database (CC-NSIT-SD) of 38 speakers was also collected to study the performance of system. Apart from this, the effect of different noise types on the speaker identification was also studied on different sensors.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1385–1392},
numpages = {8},
keywords = {Speaker identification, speaker database, aging database, cough and cold database}
}

@article{10.1145/972374.972390,
author = {Feamster, Nick},
title = {Practical verification techniques for wide-area routing},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {0146-4833},
url = {https://doi.org/10.1145/972374.972390},
doi = {10.1145/972374.972390},
abstract = {Protocol and system designers use verification techniques to analyze a system's correctness properties. Network operators need verification techniques to ensure the "correct" operation of BGP. BGP's distributed dependencies cause small configuration mistakes or oversights to spur complex errors, which sometimes have devastating effects on global connectivity. These errors are often difficult to debug because they are sometimes only exposed by a specific message arrival pattern or failure scenario.This paper presents an approach to BGP verification that is primarily based on static analysis of router configuration. We argue that: (1) because BGP's a configuration affects its fundamental behavior, verification is a program analysis problem, (2) BGP's complex, dynamic interactions are difficult to abstract and impossible to enumerate, which precludes existing verification techniques, (3) because of BGP's flexible, policy-based configuration, some aspects of BGP configuration must be checked against a higher-level specification of intended policy, and (4) although static analysis can catch many configuration errors, simulation and emulation are also necessary to determine the precise scenarios that could expose errors at runtime. Based on these observations, we propose the design of a BGP verification tool, discuss how it could be applied in practice, and describe future research challenges.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = jan,
pages = {87–92},
numpages = {6}
}

@inproceedings{10.1007/978-3-030-58545-7_8,
author = {Wang, Hu and Wu, Qi and Shen, Chunhua},
title = {Soft Expert Reward Learning for Vision-and-Language Navigation},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_8},
doi = {10.1007/978-3-030-58545-7_8},
abstract = {Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert’s behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {126–141},
numpages = {16},
keywords = {Soft expert distillation, Self perceiving reward, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/2900423.2900654,
author = {Coltin, Brian and Veloso, Manuela},
title = {Multi-observation sensor resetting localization with ambiguous landmarks},
year = {2011},
publisher = {AAAI Press},
abstract = {Successful approaches to the robot localization problem include Monte Carlo particle filters, which estimate non-parametric localization belief distributions. However, particle filters fare poorly at determining the robot's position without a good initial hypothesis. This problem has been addressed for robots that sense visual landmarks with sensor resetting, by performing sensor-based resampling when the robot is lost. For robots that make sparse, ambiguous and noisy observations, standard sensor resetting places new location hypotheses across a wide region, in positions that may be inconsistent with previous observations. We propose Multi-Observation Sensor Resetting, where observations from multiple frames are merged to generate new hypotheses more effectively. We demonstrate experimentally in the robot soccer domain on the NAO humanoid robots that Multi-Observation Sensor Resetting converges more efficiently to the robot's true position than standard sensor resetting, and is more robust to systematic vision errors.},
booktitle = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
pages = {1462–1467},
numpages = {6},
location = {San Francisco, California},
series = {AAAI'11}
}

@inproceedings{10.1145/2970276.2970327,
author = {Moonen, Leon and Di Alesio, Stefano and Binkley, David and Rolfsnes, Thomas},
title = {Practical guidelines for change recommendation using association rule mining},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970327},
doi = {10.1145/2970276.2970327},
abstract = {Association rule mining is an unsupervised learning technique that infers relationships among items in a data set. This technique has been successfully used to analyze a system's change history and uncover evolutionary coupling between system artifacts. Evolutionary coupling can, in turn, be used to recommend artifacts that are potentially affected by a given set of changes to the system. In general, the quality of such recommendations is affected by (1) the values selected for various parameters of the mining algorithm, (2) characteristics of the set of changes used to derive a recommendation, and (3) characteristics of the system's change history for which recommendations are generated.In this paper, we empirically investigate the extent to which certain choices for these factors affect change recommendation. Specifically, we conduct a series of systematic experiments on the change histories of two large industrial systems and eight large open source systems, in which we control the size of the change set for which to derive a recommendation, the measure used to assess the strength of the evolutionary coupling, and the maximum size of historical changes taken into account when inferring these couplings. We use the results from our study to derive a number of practical guidelines for applying association rule mining for change recommendation.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {732–743},
numpages = {12},
keywords = {Evolutionary coupling, association rule mining, change impact analysis, change recommendations, parameter tuning},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3366428.3380771,
author = {Mogers, Naums and Radu, Valentin and Li, Lu and Turner, Jack and O'Boyle, Michael and Dubach, Christophe},
title = {Automatic generation of specialized direct convolutions for mobile GPUs},
year = {2020},
isbn = {9781450370257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366428.3380771},
doi = {10.1145/3366428.3380771},
abstract = {Convolutional Neural Networks (CNNs) are a powerful and versatile tool for performing computer vision tasks in both resource constrained settings and server-side applications. Most GPU hardware vendors provide highly tuned libraries for CNNs such as Nvidia's cuDNN or ARM Compute Library. Such libraries are the basis for higher-level, commonly-used, machine-learning frameworks such as PyTorch or Caffe, abstracting them away from vendor-specific implementation details. However, writing optimized parallel code for GPUs is far from trivial. This places a significant burden on hardware-specific library writers which have to continually play catch-up with rapid hardware and network evolution.To reduce effort and reduce time to market, new approaches are needed based on automatic code generation, rather than manual implementation. This paper describes such an approach for direct convolutions using Lift, a new data-parallel intermediate language and compiler. Lift uses a high-level intermediate language to express algorithms which are then automatically optimized using a system of rewrite-rules. Direct convolution, as opposed to the matrix multiplication approach used commonly by machine-learning frameworks, uses an order of magnitude less memory, which is critical for mobile devices. Using Lift, we show that it is possible to generate automatically code that is X10 faster than the direct convolution while using X3.6 less space than the GEMM-based convolution of the very specialized ARM Compute Library on the latest generation of ARM Mali GPU.},
booktitle = {Proceedings of the 13th Annual Workshop on General Purpose Processing Using Graphics Processing Unit},
pages = {41–50},
numpages = {10},
keywords = {code generation, convolution, mobile GPU, parallelism},
location = {San Diego, California},
series = {GPGPU '20}
}

@article{10.1016/j.jss.2011.04.066,
author = {Cirilo, Elder and Nunes, Ingrid and Kulesza, Uir\'{a} and Lucena, Carlos},
title = {Automating the product derivation process of multi-agent systems product lines},
year = {2012},
issue_date = {February, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.04.066},
doi = {10.1016/j.jss.2011.04.066},
abstract = {Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work has been exploring their integration, namely multi-agent systems product lines (MAS-PLs), to promote reuse and variability management in the context of complex software systems. However, current product derivation approaches do not provide specific mechanisms to deal with MAS-PLs. This is essential because they typically encompass several concerns (e.g., trust, coordination, transaction, state persistence) that are constructed on the basis of heterogeneous technologies (e.g., object-oriented frameworks and platforms). In this paper, we propose the use of multi-level models to support the configuration knowledge specification and automatic product derivation of MAS-PLs. Our approach provides an agent-specific architecture model that uses abstractions and instantiation rules that are relevant to this application domain. In order to evaluate the feasibility and effectiveness of the proposed approach, we have implemented it as an extension of an existing product derivation tool, called GenArch. The approach has also been evaluated through the automatic instantiation of two MAS-PLs, demonstrating its potential and benefits to product derivation and configuration knowledge specification.},
journal = {J. Syst. Softw.},
month = feb,
pages = {258–276},
numpages = {19},
keywords = {Application engineering, Model-driven development, Multi-agent systems, Product derivation tool, Software product lines}
}

@inbook{10.1007/978-3-642-01338-6_17,
author = {Nunes, Ingrid and Nunes, Camila and Kulesza, Uir\'{a} and Lucena, Carlos},
title = {Developing and Evolving a Multi-agent System Product Line: An Exploratory Study},
year = {2009},
isbn = {9783642013379},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-01338-6_17},
abstract = {Software Product Line (SPL) approaches motivate the development and implementation of a flexible and adaptable architecture to enable software reuse in organizations. The SPL architecture addresses a set of common and variable features of a family of products. Based on this architecture, products can be derived in a systematic way. A multi-agent system product line (MAS-PL) defines a SPL architecture, whose design and implementation is accomplished using software agents to address its common and variable features. This paper presents the evolutionary development of a MAS-PL from an existing web-based system. The MAS-PL architecture developed is composed of: (i) the core architecture represented by the web-based system that addresses the main mandatory features; and (ii) a set of software agents that extends the core architecture to introduce in the web system new optional and alternative autonomous behavior features. We report several lessons learned from this exploratory study of definition of a MAS-PL.},
booktitle = {Agent-Oriented Software Engineering IX: 9th International Workshop, AOSE 2008 Estoril, Portugal, May 12-13, 2008 Revised Selected Papers},
pages = {228–242},
numpages = {15}
}

@inproceedings{10.3115/993268.993299,
author = {Matiasek, Johannes and Trost, Harald},
title = {An HPSG-based generator for German: an experiment in the reusability of linguistic resources},
year = {1996},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/993268.993299},
doi = {10.3115/993268.993299},
abstract = {We describe the development of a generator for German built by reusing and adapting existing linguistic data and software. Reusability is crucial for the successful application of NLP techniques to real-life problems since it helps to cut down on both development and adaptation effort. However, combining resources not designed to work together is not trivial. We describe the problems arising when integrating three preexisting resources (FUF, a unification-based generator, an HPSG Grammar for German, and X2MorF, a two-level morphology component) and the adaptations necessary to come up with a wide coverage tactical generator for German.},
booktitle = {Proceedings of the 16th Conference on Computational Linguistics - Volume 2},
pages = {752–757},
numpages = {6},
location = {Copenhagen, Denmark},
series = {COLING '96}
}

@inproceedings{10.1007/978-3-030-79382-1_6,
author = {Burgue\~{n}o, Loli and Claris\'{o}, Robert and G\'{e}rard, S\'{e}bastien and Li, Shuai and Cabot, Jordi},
title = {An NLP-Based Architecture for&nbsp;the&nbsp;Autocompletion of Partial Domain Models},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_6},
doi = {10.1007/978-3-030-79382-1_6},
abstract = {Domain models capture the key concepts and relationships of a business domain. Typically, domain models are manually defined by software designers in the initial phases of a software development cycle, based on their interactions with the client and their own domain expertise. Given the key role of domain models in the quality of the final system, it is important that they properly reflect the reality of the business.To facilitate the definition of domain models and improve their quality, we propose to move towards a more assisted domain modeling building process where an NLP-based assistant will provide autocomplete suggestions for the partial model under construction based on the automatic analysis of the textual information available for the project (contextual knowledge) and/or its related business domain (general knowledge). The process will also take into account the feedback collected from the designer’s interaction with the assistant. We have developed a proof-of-concept tool and have performed a preliminary evaluation that shows promising results.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {91–106},
numpages = {16},
keywords = {Domain model, Autocomplete, Modeling recommendations, Assistant, Natural language processing},
location = {Melbourne, VIC, Australia}
}

@article{10.4018/IJEIS.2019040104,
author = {Sbai, Hanae and El Faquih, Loubna and Fredj, Mounia},
title = {A Novel Tool for Configurable Process Evolution and Service Derivation},
year = {2019},
issue_date = {Apr 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-1115},
url = {https://doi.org/10.4018/IJEIS.2019040104},
doi = {10.4018/IJEIS.2019040104},
abstract = {In recent years, variability management in business processes is considered a key of reuse. Research works in this field focused mainly on variability modeling and resolution; whereas, evolution has been somehow neglected. In fact, new business requirements may occur, and business processes must evolve in order to meet the new needs. Furthermore, the evolution at business layer represented by configurable processes impact the IT layer represented by services. In this case, it is necessary to synchronize the changes between these two layers. In other words, the alignment of configurable processes and configurable services must occur to maintain an integrated view of an organization. This can be reached by the concept of service-based configurable processes. The study of existing tools in this domain shows the lack of solutions integrating both the evolution management, and the change propagation with respect to the variability. This article aims to represent the CPMEv, a novel tool for evolution management of service-based configurable processes.},
journal = {Int. J. Enterp. Inf. Syst.},
month = apr,
pages = {58–75},
numpages = {18},
keywords = {Alignment, Change Propagation, Configurable Services, Evolution, Variability}
}

@inproceedings{10.5555/3044805.3044969,
author = {Cuturi, Marco and Doucet, Arnaud},
title = {Fast computation of wasserstein barycenters},
year = {2014},
publisher = {JMLR.org},
abstract = {We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–685–II–693},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.5555/1153922.1154326,
author = {Qi, Yanjun and Hauptmann, A. and Liu, Ting},
title = {Supervised classification for video shot segmentation},
year = {2003},
isbn = {0780379659},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we explore supervised classification methods for video shot segmentation. We transform the temporal segmentation problem into a multi-class categorization issue. This approach provides a uniform framework for using different kinds of features extracted from the video and for detecting various types of shot boundaries. The approach utilizes manual labeled training data and a simple classification structure, which eliminates arbitrary thresholds and achieves more reliable estimation than previous threshold-based methods. Contrastive experiments on 13 videos (/spl sim/4 hours) show excellent performance on the 2001 TREC video track shot classification task in terms of precision and recall.},
booktitle = {Proceedings of the 2003 International Conference on Multimedia and Expo - Volume 1},
pages = {689–692},
numpages = {4},
series = {ICME '03}
}

@inproceedings{10.1109/ASE.2011.6100075,
author = {Apel, Sven and Speidel, Hendrik and Wendler, Philipp and von Rhein, Alexander and Beyer, Dirk},
title = {Detection of feature interactions using feature-aware verification},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100075},
doi = {10.1109/ASE.2011.6100075},
abstract = {A software product line is a set of software products that are distinguished in terms of features (i.e., end-user-visible units of behavior). Feature interactions-- situations in which the combination of features leads to emergent and possibly critical behavior --are a major source of failures in software product lines. We explore how feature-aware verification can improve the automatic detection of feature interactions in software product lines. Feature-aware verification uses product-line-verification techniques and supports the specification of feature properties along with the features in separate and composable units. It integrates the technique of variability encoding to verify a product line without generating and checking a possibly exponential number of feature combinations. We developed the tool suite SPLVERIFIER for feature-aware verification, which is based on standard model-checking technology. We applied it to an e-mail system that incorporates domain knowledge of AT&amp;T. We found that feature interactions can be detected automatically based on specifications that have only local knowledge.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {372–375},
numpages = {4},
series = {ASE '11}
}

@article{10.1007/s00521-020-04731-y,
author = {Helali, Abdelhamid and Ameur, Haythem and G\'{o}rriz, J. M. and Ram\'{\i}rez, J. and Maaref, Hassen},
title = {Hardware implementation of real-time pedestrian detection system},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {16},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04731-y},
doi = {10.1007/s00521-020-04731-y},
abstract = {Histogram of oriented gradients (HOG) descriptor and support vector machine (SVM) classifier have been considered as a very efficient and robust tools used in the development of pedestrian detection systems. However, the enormous calculations make it difficult to meet real-time requirement specifically for advanced driver assistance system applications. The latter requires higher frame rate and resolution. In the present work, efficient pipelined architecture founded on cell-based scanning and simultaneous linear SVM classification is highlighted. The proposed real-time pedestrian detection architecture performs without external memories, and only 3 \texttimes{} 3 pixels are processed using two line buffers. Indeed, intermediate results will contribute directly to build the cell histograms. Consequently, it allows to reduce the required memory and to speed up the HOG feature extraction procedure. The implementation results on a heterogeneous platform show that the proposed architecture achieves an efficient pedestrian detection for full-HD video recordings (1080 \texttimes{} 1920 pixels) at 60&nbsp;fps with reduced hardware resource consumption. Using the same hardware resources at 444&nbsp;MHz, the proposed system turns out to handle a HD video at 180&nbsp;fps.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {12859–12871},
numpages = {13},
keywords = {ADAS, Pedestrian detection, Embedded system, Real-time, Hardware architecture}
}

@article{10.1017/S0890060405050043,
author = {Yeh, Jinn-Yi and Wu, Tai-Hsi},
title = {Solutions for product configuration management: An empirical study},
year = {2005},
issue_date = {January 2005},
publisher = {Cambridge University Press},
address = {USA},
volume = {19},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060405050043},
doi = {10.1017/S0890060405050043},
abstract = {Customers can directly express their preferences on many options when ordering products today. Mass customization manufacturing thus has emerged as a new trend for its aiming to satisfy the needs of individual customers. This process of offering a wide product variety often induces an exponential growth in the volume of information and redundancy for data storage. Thus, a technique for managing product configuration is necessary, on the one hand, to provide customers faster configured and lower priced products, and on the other hand, to translate customers' needs into the product information needed for tendering and manufacturing. This paper presents a decision-making scheme through constructing a product family model (PFM) first, in which the relationship between product, modules, and components are defined. The PFM is then transformed into a product configuration network. A product configuration problem assuming that customers would like to have a minimum-cost and customized product can be easily solved by finding the shortest path in the corresponding product configuration network. Genetic algorithms (GAs), mathematical programming, and tree-searching methods such as uniform-cost search and iterative deepening A* are applied to obtain solutions to this problem. An empirical case is studied in this work as an example. Computational results show that the solution quality of GAs retains 93.89% for a complicated configuration problem. However, the running time of GAs outperforms the running time of other methods with a minimum speed factor of 25. This feature is very useful for a real-time system.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {39–47},
numpages = {9},
keywords = {Configuration, Genetic Algorithm, Mathematical Programming, Product Family Model, Tree Searching}
}

@article{10.1016/j.specom.2007.07.006,
author = {Yapanel, Umit H. and Hansen, John H. L.},
title = {A new perceptually motivated MVDR-based acoustic front-end (PMVDR) for robust automatic speech recognition},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {2},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2007.07.006},
doi = {10.1016/j.specom.2007.07.006},
abstract = {Acoustic feature extraction from speech constitutes a fundamental component of automatic speech recognition (ASR) systems. In this paper, we propose a novel feature extraction algorithm, perceptual-MVDR (PMVDR), which computes cepstral coefficients from the speech signal. This new feature representation is shown to better model the speech spectrum compared to traditional feature extraction approaches. Experimental results for small (40-word digits) to medium (5k-word dictation) size vocabulary tasks show varying degree of consistent improvements across different experiments; however, the new front-end is most effective in noisy car environments. The PMVDR front-end uses the minimum variance distortionless response (MVDR) spectral estimator to represent the upper envelope of the speech signal. Unlike Mel frequency cepstral coefficients (MFCCs), the proposed front-end does not utilize a filterbank. The effectiveness of the PMVDR approach is demonstrated by comparing speech recognition accuracies with the traditional MFCC front-end and recently proposed PMCC front-end in both noise-free and real adverse environments. For speech recognition in noisy car environments, a 40-word vocabulary task, PMVDR front-end provides a 36% relative decrease in word error rate (WER) over the MFCC front-end. Under simulated speaker stress conditions, a 35-word vocabulary task, the PMVDR front-end yields a 27% relative decrease in the WER. For a noise-free dictation task, a 5k-word vocabulary task, again a relative 8% reduction in the WER is reported. Finally, a novel analysis technique is proposed to quantify noise robustness of an acoustic front-end. This analysis is conducted for the acoustic front-ends analyzed in the paper and results are presented.},
journal = {Speech Commun.},
month = feb,
pages = {142–152},
numpages = {11},
keywords = {Acoustic feature extraction, Noise-robustness analysis, Robust speech recognition}
}

@article{10.1016/j.jss.2019.01.044,
author = {Th\"{u}m, Thomas and Kn\"{u}ppel, Alexander and Kr\"{u}ger, Stefan and Bolle, Stefanie and Schaefer, Ina},
title = {Feature-oriented contract composition},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.044},
doi = {10.1016/j.jss.2019.01.044},
journal = {J. Syst. Softw.},
month = jun,
pages = {83–107},
numpages = {25},
keywords = {Feature-oriented programming, Software product lines, Design by contract, Deductive verification, Formal methods}
}

@inproceedings{10.1007/978-3-030-00308-1_25,
author = {Mattamala, Mat\'{\i}as and Olave, Gonzalo and Gonz\'{a}lez, Clayder and Hasb\'{u}n, Nicol\'{a}s and Ruiz-del-Solar, Javier},
title = {The NAO Backpack: An Open-Hardware Add-on for Fast Software Development with the NAO Robot},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_25},
doi = {10.1007/978-3-030-00308-1_25},
abstract = {We present an open-source accessory for the NAO robot, which enables to test computationally demanding algorithms in an external platform while preserving robot’s autonomy and mobility. The platform has the form of a backpack, which can be 3D printed and replicated, and holds an ODROID XU4 board to process algorithms externally with ROS compatibility. We provide also a software bridge between the B-Human’s framework and ROS to have access to the robot’s sensors close to real-time. We tested the platform in several robotics applications such as data logging, visual SLAM, and robot vision with deep learning techniques. The CAD model, hardware specifications and software are available online for the benefit of the community.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {302–311},
numpages = {10},
keywords = {NAO robot, SPL, ODROID XU-4, ROS},
location = {Nagoya, Japan}
}

@article{10.1016/j.neucom.2017.07.037,
author = {Li, Honggui and Trocan, Maria},
title = {Deep neural network based single pixel prediction for unified video coding},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {272},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.07.037},
doi = {10.1016/j.neucom.2017.07.037},
abstract = {Classical video prediction methods exploit directly and shallowly the intra-frame, inter-frame and multi-view similarities within the video sequences; the proposed video prediction methods indirectly and intensively transform the frame correlations into nonlinear mappings by using a general deep neural network (DNN) with single output node. Traditional DNN based video prediction algorithms wholly and coarsely forecast the next frame, but the proposed video prediction algorithms severally and precisely anticipate single pixel of future frame in order to achieve high prediction accuracy and low computation cost. First of all, general DNN based prediction algorithms for intra-frame coding, inter-frame coding and multi-view coding are presented respectively. Then, general DNN based prediction algorithm for unified video coding is raised, which relies on the preceding three prediction algorithms. It is evaluated by simulation experiments that the proposed methods hold better performance than state of the art High Efficiency Video Coding (HEVC) in peak signal to noise ratio (PSNR) and bit per pixel (BPP) in the situation of low bitrate transmission. It is also verified by experimental results that the proposed general DNN architecture possesses higher prediction accuracy and lower computation load than those of conventional DNN architectures. It is further testified by experimental results that the proposed methods are very suitable for multi-view videos with small correlations and big disparities.},
journal = {Neurocomput.},
month = jan,
pages = {558–570},
numpages = {13},
keywords = {Deep neural network, Inter-frame coding, Intra-frame coding, Multi-view coding, Unified video coding, Video prediction}
}

@inproceedings{10.1007/978-3-642-33666-9_34,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Heider, Wolfgang and Holl, Gerald and Lettner, Daniela},
title = {Applying a consistency checking framework for heterogeneous models and artifacts in industrial product lines},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_34},
doi = {10.1007/978-3-642-33666-9_34},
abstract = {Product line engineering relies on heterogeneous models and artifacts to define and implement the product line's reusable assets. The complexity and heterogeneity of product line artifacts as well as their interdependencies make it hard to maintain consistency during development and evolution, regardless of the modeling approaches used. Engineers thus need support for detecting and resolving inconsistencies within and between the various artifacts. In this paper we present a framework for checking and maintaining consistency of arbitrary product line artifacts. Our approach is flexible and extensible regarding the supported artifact types and the definition of constraints. We discuss tool support developed for the DOPLER product line tool suite. We report the results of applying the approach to sales support applications of industrial product lines.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {531–545},
numpages = {15},
keywords = {consistency checking, model-based product lines, sales support},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/1321631.1321711,
author = {Botterweck, Goetz and O'Brien, Liam and Thiel, Steffen},
title = {Model-driven derivation of product architectures},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321711},
doi = {10.1145/1321631.1321711},
abstract = {Product Derivation is one of the central activities in Software Product Lines (SPL). One of the main challenges of the process of product derivation is dealing with complexity, which is caused by the large number of artifacts and dependencies between them. Another major challenge is maximizing development efficiency and reducing time-to-market, while at the same time producing high quality products. One approach to overcome these challenges is to automate the derivation process. To this end, this paper focuses on one particular activity of the derivation process; the derivation of the product-specific architecture and describes how this activity can be automated using a model-driven approach. The approach derives the product-specific architecture by selectively copying elements from the product-line architecture. The decision, which elements are included in the derived architecture, is based on a product-specific feature configuration. We present a prototype that implements the derivation as a model transformation described in the Atlas Transformation Language (ATL). We conclude with a short overview of related work and directions for future research},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {469–472},
numpages = {4},
keywords = {ATL, model transformation, model-driven approaches, product derivation, software architectures, software product lines},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.5555/2898607.2898861,
author = {Meri\c{c}li, \c{C}etin and Veloso, Manuela},
title = {Biped walk learning through playback and corrective demonstration},
year = {2010},
publisher = {AAAI Press},
abstract = {Developing a robust, flexible, closed-loop walking algorithm for a humanoid robot is a challenging task due to the complex dynamics of the general biped walk. Common analytical approaches to biped walk use simplified models of the physical reality. Such approaches are partially successful as they lead to failures of the robot walk in terms of unavoidable falls. Instead of further refining the analytical models, in this work we investigate the use of human corrective demonstrations, as we realize that a human can visually detect when the robot may be falling. We contribute a two-phase biped walk learning approach, which we experiment on the Aldebaran NAO humanoid robot. In the first phase, the robot walks following an analytical simplified walk algorithm, which is used as a black box, and we identify and save a walk cycle as joint motion commands. We then show how the robot can repeatedly and successfully play back the recorded motion cycle, even if in open-loop. In the second phase, we create a closed-loop walk by modifying the recorded walk cycle to respond to sensory data. The algorithm learns joint movement corrections to the open-loop walk based on the corrective feedback provided by a human, and on the sensory data, while walking autonomously. In our experimental results, we show that the learned closed-loop walking policy outperforms a hand-tuned closed-loop policy and the open-loop playback walk, in terms of the distance traveled by the robot without falling.},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {1594–1599},
numpages = {6},
location = {Atlanta, Georgia},
series = {AAAI'10}
}

@inproceedings{10.1007/978-3-030-27544-0_11,
author = {A\c{s}\i{}k, Okan and G\"{o}rer, Binnur and Ak\i{}n, H. Levent},
title = {End-to-End Deep Imitation Learning: Robot Soccer Case Study},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_11},
doi = {10.1007/978-3-030-27544-0_11},
abstract = {In imitation learning, behavior learning is generally done using the features extracted from the demonstration data. Recent deep learning algorithms enable the development of machine learning methods that can get high dimensional data as an input. In this work, we use imitation learning to teach the robot to dribble the ball to the goal. We use B-Human robot software to collect demonstration data and a deep convolutional network to represent the policies. We use top and bottom camera images of the robot as input and speed commands as outputs. The CNN policy learns the mapping between the series of images and speed commands. In 3D realistic robotics simulator experiments, we show that the robot is able to learn to search the ball and dribble the ball, but it struggles to align to the goal. The best-proposed policy model learns to score 4 goals out of 20 test episodes.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {137–149},
numpages = {13},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1111/cgf.14116,
author = {Ghorbani, S. and Wloka, C. and Etemad, A. and Brubaker, M. A. and Troje, N. F.},
title = {Probabilistic character motion synthesis using a hierarchical deep latent variable model},
year = {2020},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.1111/cgf.14116},
doi = {10.1111/cgf.14116},
abstract = {We present a probabilistic framework to generate character animations based on weak control signals, such that the synthesized motions are realistic while retaining the stochastic nature of human movement. The proposed architecture, which is designed as a hierarchical recurrent model, maps each sub-sequence of motions into a stochastic latent code using a variational autoencoder extended over the temporal domain. We also propose an objective function which respects the impact of each joint on the pose and compares the joint angles based on angular distance. We use two novel quantitative protocols and human qualitative assessment to demonstrate the ability of our model to generate convincing and diverse periodic and non-periodic motion sequences without the need for strong control signals.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
articleno = {21},
numpages = {15},
location = {Virtual Event, Canada},
series = {SCA '20}
}

@article{10.1016/j.engappai.2009.08.004,
author = {Fuentes-Fern\'{a}ndez, Rub\'{e}n and Garc\'{\i}a-Magari\~{n}o, Iv\'{a}n and G\'{o}mez-Rodr\'{\i}guez, Alma Mar\'{\i}a and Gonz\'{a}lez-Moreno, Juan Carlos},
title = {A technique for defining agent-oriented engineering processes with tool support},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {23},
number = {3},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.08.004},
doi = {10.1016/j.engappai.2009.08.004},
abstract = {The formalization of engineering processes is necessary for guaranteeing the quality and cost of the products involved. Agent-oriented engineering has already proposed development processes that still need to be further formalized to be applicable by non-researchers. This paper proposes a technique to instantiate processes for specific agent-oriented methodologies. This technique is based on three orthogonal views that are respectively related with lifecycles, disciplines and guidances. In addition, processes are modeled with a tool, which is automatically generated from a process metamodel inspired by the software &amp; systems process engineering metamodel. Accordingly, engineers can choose the methodology-process pair best-suited for the characteristics of their project. The paper illustrates the approach based on the unified development process and the scrum process for the INGENIAS methodology and compares the results with other existing alternatives.},
journal = {Eng. Appl. Artif. Intell.},
month = apr,
pages = {432–444},
numpages = {13},
keywords = {Agent-oriented engineering, Model-driven development, Multi-agent system, Process engineering, Software &amp; systems process engineering metamodel}
}

@inproceedings{10.5555/3006433.3006482,
author = {Melis, Erica and Zimmer, J\"{u}rgen and M\"{u}ller, Tobias},
title = {Extensions of constraint solving for proof planning},
year = {2000},
publisher = {IOS Press},
address = {NLD},
abstract = {The integration of constraint solvers into proof planning has pushed the problem solving horizon. Proof planning benefits from the general functionalities of a constraint solver such as consistency check, constraint inference, as well as the search for instantiations. However, off-the-shelf constraint solvers need to be extended in order to be be integrated appropriately: In particular, for correctness, the context of constraints and Eigenvariable-conditions have to be taken into account. Moreover, symbolic and numeric constraint inference are combined. This paper discusses the extensions to constraint solving for proof planning, namely the combination of symbolic and numeric inference, first-class constraints, and context trees. We also describe the implementation of these extensions in the constraint solver CoSIℇ.},
booktitle = {Proceedings of the 14th European Conference on Artificial Intelligence},
pages = {229–233},
numpages = {5},
keywords = {constraint solving, theorem proving},
location = {Berlin, Germany},
series = {ECAI'00}
}

@article{10.1007/s10270-017-0641-6,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Enabling automated requirements reuse and configuration},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0641-6},
doi = {10.1007/s10270-017-0641-6},
abstract = {A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2177–2211},
numpages = {35},
keywords = {Configuration, Feature model, Product line, Requirements, Reuse}
}

@inproceedings{10.1007/978-3-030-89128-2_5,
author = {Bigazzi, Roberto and Landi, Federico and Cornia, Marcella and Cascianelli, Silvia and Baraldi, Lorenzo and Cucchiara, Rita},
title = {Out of the Box: Embodied Navigation in the Real World},
year = {2021},
isbn = {978-3-030-89127-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89128-2_5},
doi = {10.1007/978-3-030-89128-2_5},
abstract = {The research field of Embodied AI has witnessed substantial progress in visual navigation and exploration thanks to powerful simulating platforms and the availability of 3D data of indoor and photorealistic environments. These two factors have opened the doors to a new generation of intelligent agents capable of achieving nearly perfect PointGoal Navigation. However, such architectures are commonly trained with millions, if not billions, of frames and tested in simulation. Together with great enthusiasm, these results yield a question: how many researchers will effectively benefit from these advances? In this work, we detail how to transfer the knowledge acquired in simulation into the real world. To that end, we describe the architectural discrepancies that damage the Sim2Real adaptation ability of models trained on the Habitat simulator and propose a novel solution tailored towards the deployment in real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot equipped with a single Intel RealSense camera. Different from previous work, our testing scene is unavailable to the agent in simulation. The environment is also inaccessible to the agent beforehand, so it cannot count on scene-specific semantic priors. In this way, we reproduce a setting in which a research group (potentially from other fields) needs to employ the agent visual navigation capabilities as-a-Service. Our experiments indicate that it is possible to achieve satisfying results when deploying the obtained model in the real world. Our code and models are available at .},
booktitle = {Computer Analysis of Images and Patterns: 19th International Conference, CAIP 2021, Virtual Event, September 28–30, 2021, Proceedings, Part I},
pages = {47–57},
numpages = {11},
keywords = {Embodied AI, Sim2Real, Visual navigation}
}

@article{10.1016/j.infsof.2011.06.002,
author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
title = {A systematic review of software architecture evolution research},
year = {2012},
issue_date = {January, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.002},
doi = {10.1016/j.infsof.2011.06.002},
abstract = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–40},
numpages = {25},
keywords = {Architecture analysis, Architecture evolution, Evolvability analysis, Software architecture, Software evolvability, Systematic review}
}

@inproceedings{10.5555/3298483.3298553,
author = {Li, Changsheng and Yan, Junchi and Wei, Fan and Dong, Weishan and Liu, Qingshan and Zha, Hongyuan},
title = {Self-paced multi-task learning},
year = {2017},
publisher = {AAAI Press},
abstract = {Multi-task learning is a paradigm, where multiple tasks are jointly learnt. Previous multi-task learning models usually treat all tasks and instances per task equally during learning. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, in this paper, we propose a novel multi-task learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task. We propose a novel formulation by presenting a new task-oriented regularizer that can jointly prioritize tasks and instances. Thus it can be interpreted as a self-paced learner for multi-task learning. An efficient block coordinate descent algorithm is developed to solve the proposed objective function, and the convergence of the algorithm can be guaranteed. Experimental results on the toy and real-world datasets demonstrate the effectiveness of the proposed approach, compared to the state-of-the-arts.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {2175–2181},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1109/TASLP.2016.2536478,
author = {Zhang, Xiao-Lei and Wang, DeLiang},
title = {A deep ensemble learning method for monaural speech separation},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2536478},
doi = {10.1109/TASLP.2016.2536478},
abstract = {Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {967–977},
numpages = {11},
keywords = {deep neural networks, ensemble learning, mapping-based separation, masking-based separation, monaural speech separation, multicontext networks}
}

@article{10.1016/j.neunet.2006.01.018,
author = {Raudys, \v{S}arnas},
title = {Trainable fusion rules. I. Large sample size case},
year = {2006},
issue_date = {December, 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {19},
number = {10},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2006.01.018},
doi = {10.1016/j.neunet.2006.01.018},
abstract = {A wide selection of standard statistical pattern classification algorithms can be applied as trainable fusion rules while designing neural network ensembles. A focus of the present two-part paper is finite sample effects: the complexity of base classifiers and fusion rules; the type of outputs provided by experts to the fusion rule; non-linearity of the fusion rule; degradation of experts and the fusion rule due to the lack of information in the design set; the adaptation of base classifiers to training set size, etc. In the first part of this paper, we consider arguments for utilizing continuous outputs of base classifiers versus categorical outputs and conclude: if one succeeds in having a small number of expert networks working perfectly in different parts of the input feature space, then crisp outputs may be preferable over continuous outputs. Afterwards, we oppose fixed fusion rules versus trainable ones and demonstrate situations where weighted average fusion can outperform simple average fusion. We present a review of statistical classification rules, paying special attention to these linear and non-linear rules, which are employed rarely but, according to our opinion, could be useful in neural network ensembles. We consider ideal and sample-based oracle decision rules and illustrate characteristic features of diverse fusion rules by considering an artificial two-dimensional (2D) example where the base classifiers perform well in different regions of input feature space.},
journal = {Neural Netw.},
month = dec,
pages = {1506–1516},
numpages = {11},
keywords = {Classifier combination, Complexity, Fusion, Generalization error, Learning set, Multiple classifiers systems, Neural network ensembles}
}

@article{10.1016/j.neucom.2019.04.017,
author = {Lin, Jiatai and Liu, Zhi and Chen, C.L. Philip and Zhang, Yun},
title = {A wavelet broad learning adaptive filter for forecasting and cancelling the physiological tremor in teleoperation},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {356},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.017},
doi = {10.1016/j.neucom.2019.04.017},
journal = {Neurocomput.},
month = sep,
pages = {170–183},
numpages = {14},
keywords = {Wavelet broad learning adaptive filter(WBLAF), Self-paced wavelet auto-encoder(SPWAE), Incremental learning}
}

@article{10.1109/TCBB.2019.2961667,
author = {Huang, Hai-Hui and Liang, Yong},
title = {A Novel Cox Proportional Hazards Model for High-Dimensional Genomic Data in Cancer Prognosis},
year = {2019},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2961667},
doi = {10.1109/TCBB.2019.2961667},
abstract = {The Cox proportional hazards model is a popular method to study the connection between feature and survival time. Because of the high-dimensionality of genomic data, existing Cox models trained on any specific dataset often generalize poorly to other independent datasets. In this paper, we suggest a novel strategy for the Cox model. This strategy is included a new learning technique, self-paced learning (SPL), and a new gene selection method, SCAD-Net penalty. The SPL method is adopted to aid to build a more accurate prediction with its built-in mechanism of learning from easy samples first and adaptively learning from hard samples. The SCAD-Net penalty has fixed the problem of the SCAD method without an inherent mechanism to fuse the prior graphical information. We combined the SPL with the SCAD-Net penalty to the Cox model (SSNC). The simulation shows that the SSNC outperforms the benchmark in terms of prediction and gene selection. The analysis of a large-scale experiment across several cancer datasets shows that the SSNC method not only results in higher prediction accuracies but also identifies markers that satisfactory stability across another validation dataset. The demo code for the proposed method is provided in supplemental file.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {1821–1830},
numpages = {10}
}

@article{10.1007/s10472-014-9418-6,
author = {Berry, Anne and Gutierrez, Alain and Huchard, Marianne and Napoli, Amedeo and Sigayret, Alain},
title = {Hermes: a simple and efficient algorithm for building the AOC-poset of a binary relation},
year = {2014},
issue_date = {October   2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {72},
number = {1–2},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-014-9418-6},
doi = {10.1007/s10472-014-9418-6},
abstract = {Given a relation 𝓡 ⊆ 𝓞 𝓐 on a set 𝓞 of objects and a set 𝓐 of attributes, the AOC-poset (Attribute/Object Concept poset), is the partial order defined on the "introducers" of objects and attributes in the corresponding concept lattice. In this paper, we present Hermes, a simple and efficient algorithm for building an AOC-poset which runs in  O (  m   i   n {  n   m ,  n     }), where  n  is the number of objects plus the number of attributes,  m  is the size of the relation, and  n      is the time required to perform matrix multiplication (currently   = 2.376). Finally, we compare the runtime of Hermes with the runtime of other algorithms computing the AOC-poset: Ares, Ceres and Pluton. We characterize the cases where each algorithm is the more relevant.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = oct,
pages = {45–71},
numpages = {27}
}

@article{10.1016/j.engappai.2009.02.002,
author = {Jiao, Jianxin (Roger) and Xu, Qianli and Wu, Zhang and Ng, Ngai-Kheong},
title = {Coordinating product, process, and supply chain decisions: A constraint satisfaction approach},
year = {2009},
issue_date = {October, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {7},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.02.002},
doi = {10.1016/j.engappai.2009.02.002},
abstract = {The importance of coordinating product, process, and supply chain (PPSC) decisions has received much attention and popularity in academia and industry alike. This paper formulates PPSC coordination as a factory loading allocation problem (FLAP) from a constraint satisfaction perspective. A domain-based FLAP reference model is proposed for the conceptualization of a multi-site manufacturing supply chain, considering multiple domains, network structures, product characteristics, decision variables, along with various constraints. A decision propagation structure (DPS) incorporating with a connectionist approach is developed based on the concept of constraint heuristic search to facilitate the exploration of solution spaces. A case study in a multi-national company is presented to illustrate the FLAP framework, which implies practical insights into PPSC coordination.},
journal = {Eng. Appl. Artif. Intell.},
month = oct,
pages = {992–1004},
numpages = {13},
keywords = {Constraint satisfaction, Coordination, Global manufacturing, Multi-site manufacturing, Product families, Supply chain}
}

@article{10.1016/j.patrec.2017.06.026,
author = {Hendricks, Dieter},
title = {Using real-time cluster configurations of streaming asynchronous features as online state descriptors in financial markets},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {97},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2017.06.026},
doi = {10.1016/j.patrec.2017.06.026},
abstract = {Propose learning algorithm where the state space is discovered online.A purposeful agent can detect exploitable structure in streaming asynchronous data.Removes need for human pre-processing for enumerating the state space.Efficacy of algorithm demonstrated using streaming tick data in financial markets.Proposal is shown to outperform random policy and buy-and-hold strategies. We present a scheme for online, unsupervised state discovery and detection from streaming, multi-featured, asynchronous data in high-frequency financial markets. Online feature correlations are computed using an unbiased, lossless Fourier estimator. A high-speed maximum likelihood clustering algorithm is then used to find the feature cluster configuration which best explains the structure in the correlation matrix. We conjecture that this feature configuration is a candidate descriptor for the temporal state of the system. Using a simple cluster configuration similarity metric, we are able to enumerate the state space based on prevailing feature configurations. The proposed state representation removes the need for human-driven data pre-processing for state attribute specification, allowing a learning agent to find structure in streaming data, discern changes in the system, enumerate its perceived state space and learn suitable action-selection policies.},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {21–28},
numpages = {8},
keywords = {41A05, 41A10, 65D05, 65D17, Asynchronous data, Financial markets, Online learning, State space discovery}
}

@inbook{10.5555/3454287.3455259,
author = {Cao, Yuan and Gu, Quanquan},
title = {Generalization bounds of stochastic gradient descent for wide and deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent random feature (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of \~{O}(n-1/2) that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {972},
numpages = {11}
}

@inproceedings{10.1145/2911451.2911510,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Engineering Quality and Reliability in Technology-Assisted Review},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911510},
doi = {10.1145/2911451.2911510},
abstract = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {75–84},
numpages = {10},
keywords = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@article{10.1016/j.ins.2019.12.046,
author = {Chen, Dongzi and Yang, Qinli and Liu, Jiaming and Zeng, Zhu},
title = {Selective prototype-based learning on concept-drifting data streams},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {516},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.046},
doi = {10.1016/j.ins.2019.12.046},
journal = {Inf. Sci.},
month = apr,
pages = {20–32},
numpages = {13},
keywords = {Data stream, Concept drift, Classification, Prototype, 00-01, 99-00}
}

@article{10.1016/j.neucom.2019.10.024,
author = {Zhang, Ping and Liu, Jingwen and Wang, Xiaoyang and Pu, Tian and Fei, Chun and Guo, Zhengkui},
title = {Stereoscopic video saliency detection based on spatiotemporal correlation and depth confidence optimization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {377},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.024},
doi = {10.1016/j.neucom.2019.10.024},
journal = {Neurocomput.},
month = feb,
pages = {256–268},
numpages = {13},
keywords = {Stereoscopic video, Saliency detection, Spatiotemporal correlation, Cepth confidence optimization}
}

@article{10.1016/j.jss.2019.03.027,
author = {Xu, Zhou and Li, Shuai and Luo, Xiapu and Liu, Jin and Zhang, Tao and Tang, Yutian and Xu, Jun and Yuan, Peipei and Keung, Jacky},
title = {TSTSS: A two-stage training subset selection framework for cross version defect prediction},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.027},
doi = {10.1016/j.jss.2019.03.027},
journal = {J. Syst. Softw.},
month = aug,
pages = {59–78},
numpages = {20},
keywords = {Cross version defect prediction, Spare modeling, Training subset selection, Weighted extreme learning machine, 00–01, 99-00}
}

@article{10.1016/j.cl.2018.04.004,
author = {Zhao, Tian and Huang, Xiaobing},
title = {Design and implementation of DeepDSL: A DSL for deep learning},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.04.004},
doi = {10.1016/j.cl.2018.04.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {39–70},
numpages = {32}
}

@article{10.1016/j.patcog.2017.04.024,
author = {Ren, Jianfeng and Jiang, Xudong},
title = {Regularized 2-D complex-log spectral analysis and subspace reliability analysis of micro-Doppler signature for UAV detection},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {69},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.04.024},
doi = {10.1016/j.patcog.2017.04.024},
abstract = {The proposed 2-D regularized complex-log-Fourier transform better represents mDS.The proposed subspace reliability analysis better removes unreliable dimensions.The proposed approach demonstrates superior performance for UAV detection. Unmanned aerial vehicle (UAV) has become an important radar target recently because of its wide applications and potential security threats. Traditionally, visual features such as spectrogram were often extracted for human operators to identify the micro-Doppler signature (mDS) of UAVs, i.e. sinusoidal modulation. In this paper, the authors aim to design a system for machine automatic classification of UAVs from other targets, particularly from birds as both UAVs and birds are small and slow-moving radar targets. Most existing mDS representations such as spectrogram, cepstrogram and cadence velocity diagram discard the phase spectrum, and only make use of the magnitude spectrum. Whats more, people often take the logarithm of the spectrum to enlarge the weak mDS but without sufficient care, as noise may be enlarged at the same time. The authors thus propose a regularized 2-D complex-log-Fourier transform to address these problems. Furthermore, the authors propose an object-oriented dimension-reduction technique: subspace reliability analysis, which directly removes the unreliable feature dimensions of two class-conditional covariance matrices in two separate subspaces. On the benchmark dataset, the proposed approach demonstrates better performance than the state-of-the-art approaches. More specifically, the proposed approach significantly reduces the equal error rate of the second best approach, cadence velocity diagram, from 6.68% to 3.27%.},
journal = {Pattern Recogn.},
month = sep,
pages = {225–237},
numpages = {13},
keywords = {2-D regularized complex-log-Fourier transform, Micro-Doppler signature, Radar, Subspace reliability analysis, UAV detection}
}

@article{10.5555/1460332.1460337,
author = {Vityaev, E. E. and Lapardin, K. A. and Khomicheva, I. V. and Proskura, A. L.},
title = {Transcription factor binding site recognition by regularity matrices based on the natural classification method},
year = {2008},
issue_date = {December 2008},
publisher = {IOS Press},
address = {NLD},
volume = {12},
number = {5},
issn = {1088-467X},
abstract = {A principally new approach to the classifications of nucleotide sequences based on the "natural" classification concept is proposed. As a result of "natural" classification of the nucleotide sequences, we obtain regularity matrices, where nucleotides are interconnected by regularities. Method, algorithm and software system DNANatClass for performing the "natural" classification have been developed. Experimental results comparing weight matrices with regularity matrices are presented. In this experiment, site recognition by regularity matrices appears to be more accurate than by weight matrixes.},
journal = {Intell. Data Anal.},
month = nov,
pages = {495–512},
numpages = {18}
}

@article{10.1016/j.cie.2019.03.051,
author = {B\'{a}ez, Sarah\'{\i} and Angel-Bello, Francisco and Alvarez, Ada and Meli\'{a}n-Batista, Bel\'{e}n},
title = {A hybrid metaheuristic algorithm for a parallel machine scheduling problem with dependent setup times},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.03.051},
doi = {10.1016/j.cie.2019.03.051},
journal = {Comput. Ind. Eng.},
month = may,
pages = {295–305},
numpages = {11},
keywords = {Parallel machine scheduling, Total completion time, Setup time, GRASP, Variable neighborhood search}
}

@article{10.1007/s11276-020-02382-4,
author = {Deng, Jiaxin and Ou, Weihua and Gou, Jianping and Song, Heping and Wang, Anzhi and Xu, Xing},
title = {Representation separation adversarial networks for cross-modal retrieval},
year = {2020},
issue_date = {Jul 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-020-02382-4},
doi = {10.1007/s11276-020-02382-4},
abstract = {Cross-modal retrieval aims to search the semantically similar instances from the other modalities by giving a query from one modality. Recently, generative adversarial networks (GANs) has been proposed to model the joint distribution over the data from different modalities and to learn the common representations for cross-modal retrieval. However, most of existing GANs-based methods simply project original representations of different modalities into a common representation space, and ignore the fact that different modalities share the common characteristics and on the other side each modality has the individual characteristics. To address this problem, in this paper, we propose a novel cross-modal retrieval method, called representation separation adversarial networks, which explicitly separates the original representations into common latent representations and private representations. Specifically, we minimize the correlation between the common representations and private representations to ensure independence of them. Then, we reconstruct the original representations via exchanging the common representations of different modalities to encourage the information swap. Finally, the labels are utilized to increase the discriminant of common representations. Comprehensive experimental results on two widely used datasets show that the proposed method achieved better performance than many existing GANs-based methods, and demonstrate that explicitly modeling the private representation for each modality can improve the model to extract common latent representations.},
journal = {Wirel. Netw.},
month = jun,
pages = {3469–3481},
numpages = {13},
keywords = {Cross-modal retrieval, Adversarial learning, Common representation, Private representation, Representation separation}
}

@inproceedings{10.1145/3132498.3133834,
author = {Santos, Marcelo C. B. and Colanzi, Thelma E. and Amaral, Aline M. M. M. and OliveiraJr, Edson},
title = {Preliminary study on the correlation of objective functions to optimize product-line architectures},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3133834},
doi = {10.1145/3132498.3133834},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). The Multi-Objective Approach for PLA Design (MOA4PLA) aims at optimizing the PLA design by using search algorithms easing the design activity. From an original PLA, MOA4PLA automatically obtains alternative designs to improve the original one in terms of the objectives selected for optimization. The use of search algorithms is an incipient research topic, which includes several open research questions. The evaluation model of MOA4PLA is composed of various objective functions, which use software metrics to evaluate different factors that influence on the PLA design. However, the simultaneous optimization of all objective functions is a computationally complex task. In this sense, it is worthwhile to investigate the possible correlation between objective functions because the discovery of correlated functions allows to reduce the number of objectives to be optimized by the search algorithm. Hence, in this paper we perform a preliminary study to investigate the correlation among five objective functions related to metrics that provide indicators on conventional architectural properties, such as coupling, cohesion and size. To accomplish the objective of this paper, four controlled experiments were carried out with four different PLA designs. Empirical results provide preliminary evidence that two pairs of functions are positively correlated and two other pairs of functions are negatively correlated. From such findings, several guidelines were derived to help architects to both reduce and select the objectives related to conventional architectural properties to be tackled during the PLA design optimization.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {11},
numpages = {10},
keywords = {correlation study, product-line architecture, search-based software engineering},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@article{10.1016/j.knosys.2010.10.004,
author = {Li, Jingpeng and Burke, Edmund K. and Qu, Rong},
title = {Integrating neural networks and logistic regression to underpin hyper-heuristic search},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {24},
number = {2},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2010.10.004},
doi = {10.1016/j.knosys.2010.10.004},
abstract = {A hyper-heuristic often represents a heuristic search method that operates over a space of heuristic rules. It can be thought of as a high level search methodology to choose lower level heuristics. Nearly 200 papers on hyper-heuristics have recently appeared in the literature. A common theme in this body of literature is an attempt to solve the problems in hand in the following way: at each decision point, first employ the chosen heuristic(s) to generate a solution, then calculate the objective value of the solution by taking into account all the constraints involved. However, empirical studies from our previous research have revealed that, under many circumstances, there is no need to carry out this costly 2-stage determination and evaluation at all times. This is because many problems in the real world are highly constrained with the characteristic that the regions of feasible solutions are rather scattered and small. Motivated by this observation and with the aim of making the hyper-heuristic search more efficient and more effective, this paper investigates two fundamentally different data mining techniques, namely artificial neural networks and binary logistic regression. By learning from examples, these techniques are able to find global patterns hidden in large data sets and achieve the goal of appropriately classifying the data. With the trained classification rules or estimated parameters, the performance (i.e. the worth of acceptance or rejection) of a resulting solution during the hyper-heuristic search can be predicted without the need to undertake the computationally expensive 2-stage of determination and calculation. We evaluate our approaches on the solutions (i.e. the sequences of heuristic rules) generated by a graph-based hyper-heuristic proposed for exam timetabling problems. Time complexity analysis demonstrates that the neural network and the logistic regression method can speed up the search significantly. We believe that our work sheds light on the development of more advanced knowledge-based decision support systems.},
journal = {Know.-Based Syst.},
month = mar,
pages = {322–330},
numpages = {9},
keywords = {Data mining, Educational timetabling, Hyper-heuristic, Logistic regression, Neural network}
}

@article{10.1016/j.cviu.2011.09.007,
author = {Thi, Tuan Hue and Cheng, Li and Zhang, Jian and Wang, Li and Satoh, Shinichi},
title = {Integrating local action elements for action analysis},
year = {2012},
issue_date = {March, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {116},
number = {3},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2011.09.007},
doi = {10.1016/j.cviu.2011.09.007},
abstract = {In this paper, we propose a framework for human action analysis from video footage. A video action sequence in our perspective is a dynamic structure of sparse local spatial-temporal patches termed action elements, so the problems of action analysis in video are carried out here based on the set of local characteristics as well as global shape of a prescribed action. We first detect a set of action elements that are the most compact entities of an action, then we extend the idea of Implicit Shape Model to space time, in order to properly integrate the spatial and temporal properties of these action elements. In particular, we consider two different recipes to construct action elements: one is to use a Sparse Bayesian Feature Classifier to choose action elements from all detected Spatial Temporal Interest Points, and is termed discriminative action elements. The other one detects affine invariant local features from the holistic Motion History Images, and picks up action elements according to their compactness scores, and is called generative action elements. Action elements detected from either way are then used to construct a voting space based on their local feature representations as well as their global configuration constraints. Our approach is evaluated in the two main contexts of current human action analysis challenges, action retrieval and action classification. Comprehensive experimental results show that our proposed framework marginally outperforms all existing state-of-the-arts techniques on a range of different datasets.},
journal = {Comput. Vis. Image Underst.},
month = mar,
pages = {378–395},
numpages = {18},
keywords = {Action classification, Action distance function, Action matching, Action retrieval, Generalized Hough Transform, Implicit Shape Model, Multi-dimensional density estimation, Sparse Bayesian classifier, Spatial temporal feature}
}

@inproceedings{10.1109/COMPSAC.2012.95,
author = {Sabouri, Hamideh and Jaghoori, Mohammad Mahdi and Boer, Frank de and Khosravi, Ramtin},
title = {Scheduling and Analysis of Real-Time Software Families},
year = {2012},
isbn = {9780769547367},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2012.95},
doi = {10.1109/COMPSAC.2012.95},
abstract = {A software product line describes explicitly the commonalities of and differences between different products in a family of (software) systems. A formalization of these commonalities and differences amounts to reduced development, analysis and maintenance costs in the practice of software engineering. An important feature common to next-generation real-time software systems is the need of application-level control over scheduling for optimized utilization of resources provided by for example many-core and cloud infrastructures. In this paper, we introduce a formal model of real-time software product lines which supports variability in scheduling policies and rigorous and efficient techniques for modular schedulability analysis.},
booktitle = {Proceedings of the 2012 IEEE 36th Annual Computer Software and Applications Conference},
pages = {680–689},
numpages = {10},
keywords = {Application-level Scheduling, Automata Theory, Formal Methods, Real-Time, Software Product Lines},
series = {COMPSAC '12}
}

@article{10.1016/j.compbiomed.2019.103380,
author = {Kakati, Tulika and Bhattacharyya, Dhruba K. and Barah, Pankaj and Kalita, Jugal K.},
title = {Comparison of Methods for Differential Co-expression Analysis for Disease Biomarker Prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.103380},
doi = {10.1016/j.compbiomed.2019.103380},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {12},
keywords = {Differential co-expression analysis, Empirical study, Gene expression, miRNA expression, Alzheimer's disease, Parkinson's disease, Disease biomarkers}
}

@article{10.1007/s11263-020-01374-3,
author = {Vasudevan, Arun Balajee and Dai, Dengxin and Van Gool, Luc},
title = {Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {129},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-020-01374-3},
doi = {10.1007/s11263-020-01374-3},
abstract = {The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with 10,&nbsp;714 routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions—one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our .},
journal = {Int. J. Comput. Vision},
month = jan,
pages = {246–266},
numpages = {21},
keywords = {Vision-and-language navigation, Long-range navigation, Spatial memory, Dual attention}
}

@inproceedings{10.5555/2050167.2050171,
author = {Nunes, Ingrid and Cowan, Donald and Cirilo, Elder and De Lucena, Carlos J. P.},
title = {A case for new directions in agent-oriented software engineering},
year = {2010},
isbn = {9783642226359},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The state-of-the-art of Agent-oriented Software Engineering (AOSE) is insufficiently reflected in the state-of-practice in developing complex distributed systems. This paper discusses software engineering (SE) areas that have not been widely addressed in the context of AOSE, leading to a lack of mechanisms that support the development of Multiagent Systems (MASs) based on traditional SE principles, such as modularity, reusability and maintainability. This discussion is based on an exploratory study of the development of a family of buyer agents following the belief-desire-intention model and using a Software Product Line architecture. Based on the discussion presented in this paper, we hope to encourage the AOSE community to address particular SE issues on the development of MAS that have not yet been (widely) considered.},
booktitle = {Proceedings of the 11th International Conference on Agent-Oriented Software Engineering},
pages = {37–61},
numpages = {25},
keywords = {agent-oriented software engineering, multi-agent systems, software architectures, software product lines, software reuse},
location = {Toronto, Canada},
series = {AOSE'10}
}

@article{10.1007/s00779-011-0468-z,
author = {Broek, Egon L. and Sluis, Frans and Dijkstra, Ton},
title = {Cross-validation of bimodal health-related stress assessment},
year = {2013},
issue_date = {February  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {2},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-011-0468-z},
doi = {10.1007/s00779-011-0468-z},
abstract = {This study explores the feasibility of objective and ubiquitous stress assessment. 25 post-traumatic stress disorder patients participated in a controlled storytelling (ST) study and an ecologically valid reliving (RL) study. The two studies were meant to represent an early and a late therapy session, and each consisted of a "happy" and a "stress triggering" part. Two instruments were chosen to assess the stress level of the patients at various point in time during therapy: (i) speech, used as an objective and ubiquitous stress indicator and (ii) the subjective unit of distress (SUD), a clinically validated Likert scale. In total, 13 statistical parameters were derived from each of five speech features: amplitude, zero-crossings, power, high-frequency power, and pitch. To model the emotional state of the patients, 28 parameters were selected from this set by means of a linear regression model and, subsequently, compressed into 11 principal components. The SUD and speech model were cross-validated, using 3 machine learning algorithms. Between 90% (2 SUD levels) and 39% (10 SUD levels) correct classification was achieved. The two sessions could be discriminated in 89% (for ST) and 77% (for RL) of the cases. This report fills a gap between laboratory and clinical studies, and its results emphasize the usefulness of Computer Aided Diagnostics (CAD) for mental health care.},
journal = {Personal Ubiquitous Comput.},
month = feb,
pages = {215–227},
numpages = {13},
keywords = {Computer aided diagnostics (CAD), Machine learning, Post-traumatic stress disorder (PTSD), Speech, Stress, Validity}
}

@inproceedings{10.5555/646014.677489,
author = {Dampos, Te\'{o}filo Em\'{\i}dio de and Feris, Rog\'{e}rio Schimidt and Junior, Roberto Marcondes Cesar},
title = {Improved Face/spl times/Non-Face Discrimination using Fourier Descriptors through Feature Selection},
year = {2000},
isbn = {0769508782},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This work presents a new method to discriminate face from non-face images using Fourier descriptors. The first step of our approach consists in applying a horizontal edge detection filter in the input image, followed by the extraction of a 1D signal from the computed edge map. Then we calculate the Fourier descriptors from this signal and classify the image using statistical classifiers. In order to improve our results, we applied a feature selection algorithm. Preliminary performance assessment results have shown that this approach is superior than traditional transform based methods. Besides, these results showed that our method might be used to develop a fast face detection system.},
booktitle = {Proceedings of the 13th Brazilian Symposium on Computer Graphics and Image Processing},
pages = {28–35},
numpages = {8},
keywords = {1D signal extraction, Fourier descriptors, face detection, face discrimination, face recognition, feature selection, horizontal edge detection filter, image classification, performance assessment, statistical classifiers, transform based methods},
series = {SIBGRAPI '00}
}

@inproceedings{10.1145/3350546.3352496,
author = {Rafailidis, Dimitrios},
title = {Bayesian Deep Learning with Trust and Distrust in Recommendation Systems},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352496},
doi = {10.1145/3350546.3352496},
abstract = {Exploiting the selections of social friends and foes can efficiently face the data scarcity of user preferences and the cold-start problem. In this paper, we present a Social Deep Pairwise Learning model, namely SDPL. According to the Bayesian Pairwise Ranking criterion, we design a loss function with multiple ranking criteria based on the selections of users, and those in their friends and foes to improve the accuracy in the top-k recommendation task. We capture the nonlinearity in user preferences and the social information of trust and distrust relationships by designing a deep learning architecture. In each backpropagation step, we perform social negative sampling to meet the multiple ranking criteria of our loss function. Our experiments on a benchmark dataset from Epinions, among the largest publicly available that has been reported in the relevant literature, demonstrate the effectiveness of the proposed approach, outperforming other state-of-the art methods. In addition, we show that our deep learning strategy plays an important role in capturing the nonlinear associations between user preferences and the social information of trust and distrust relationships, and demonstrate that our social negative sampling strategy is a key factor in SDPL.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {18–25},
numpages = {8},
keywords = {Pairwise learning, collaborative filtering, deep learning, social relationships},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@article{10.1145/2639475.2655756,
author = {Stone, Peter and MacAlpine, Patrick and Genter, Katie and Barrett, Sam},
title = {Drop-in games at RoboCup},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/2639475.2655756},
doi = {10.1145/2639475.2655756},
journal = {AI Matters},
month = sep,
pages = {20–22},
numpages = {3}
}

@article{10.1016/j.cmpb.2011.07.015,
author = {Cabezas, Mariano and Oliver, Arnau and Llad\'{o}, Xavier and Freixenet, Jordi and Bach Cuadra, Meritxell},
title = {A review of atlas-based segmentation for magnetic resonance brain images},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {104},
number = {3},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2011.07.015},
doi = {10.1016/j.cmpb.2011.07.015},
abstract = {Abstract: Normal and abnormal brains can be segmented by registering the target image with an atlas. Here, an atlas is defined as the combination of an intensity image (template) and its segmented image (the atlas labels). After registering the atlas template and the target image, the atlas labels are propagated to the target image. We define this process as atlas-based segmentation. In recent years, researchers have investigated registration algorithms to match atlases to query subjects and also strategies for atlas construction. In this paper we present a review of the automated approaches for atlas-based segmentation of magnetic resonance brain images. We aim to point out the strengths and weaknesses of atlas-based methods and suggest new research directions. We use two different criteria to present the methods. First, we refer to the algorithms according to their atlas-based strategy: label propagation, multi-atlas methods, and probabilistic techniques. Subsequently, we classify the methods according to their medical target: the brain and its internal structures, tissue segmentation in healthy subjects, tissue segmentation in fetus, neonates and elderly subjects, and segmentation of damaged brains. A quantitative comparison of the results reported in the literature is also presented.},
journal = {Comput. Methods Prog. Biomed.},
month = dec,
pages = {e158–e177},
numpages = {20},
keywords = {Atlas, Automated methods, Brain, Magnetic resonance imaging, Segmentation}
}

@article{10.1080/00207721.2013.860636,
author = {Liu, Gia-Shie},
title = {Combination methods to solve the availability–redundancy optimisation problem for repairable&nbsp;parallel–series systems},
year = {2015},
issue_date = {September 2015},
publisher = {Taylor &amp; Francis, Inc.},
address = {USA},
volume = {46},
number = {12},
issn = {0020-7721},
url = {https://doi.org/10.1080/00207721.2013.860636},
doi = {10.1080/00207721.2013.860636},
abstract = {This study combines a new developed redundancy allocation heuristic approach, tabu search method, simulated annealing method and non-equilibrium simulated annealing method with a genetic algorithm to solve the system availability optimisation problem. Through four proposed combination methods applied in the initial system development period, the optimal allocations of component redundancy number, reliability level and maintenance rate can be obtained to minimise the total system cost under different configuration constraints. The sensitivity analysis is also conducted based on system weight, system volume, subsystem reliability requirement levels, the cost parameters associated with the reliability level and maintenance rate to provide very helpful information for the system design and development process. Finally, the performance comparison between four proposed combination availability optimisation methods is also implemented and the results clearly show that the combination method combining the new developed redundancy allocation heuristic approach with the genetic algorithm performs better than the other three combination methods in many aspects.},
journal = {Intern. J. Syst. Sci.},
month = sep,
pages = {2240–2257},
numpages = {18},
keywords = {availability, genetic algorithm, non-equilibrium simulated annealing, redundancy allocation heuristic approach, simulated annealing, tabu search}
}

@inproceedings{10.1145/3460418.3479281,
author = {Adhikari, Aakriti and Sur, Sanjib},
title = {MilliPose: Facilitating Full Body Silhouette Imaging from Millimeter-Wave Device},
year = {2021},
isbn = {9781450384612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460418.3479281},
doi = {10.1145/3460418.3479281},
abstract = {This paper proposes MilliPose, a system that facilitates full human body silhouette imaging and 3D pose estimation from millimeter-wave (mmWave) devices. Unlike existing vision-based motion capture systems, MilliPose is not privacy-invasive and is capable of working under obstructions, poor visibility, and low light conditions. MilliPose leverages machine-learning models based on conditional Generative Adversarial Networks and Recurrent Neural Network to solve the challenges of poor resolution, specularity, and variable reflectivity with existing mmWave imaging systems. Our preliminary results show the efficacy of MilliPose in accurately predicting body joint locations under natural human movement.},
booktitle = {Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers},
pages = {1–3},
numpages = {3},
keywords = {Conditional Generative Adversarial Networks, Millimeter-Wave, Recurrent Neural Network},
location = {Virtual, USA},
series = {UbiComp/ISWC '21 Adjunct}
}

@article{10.5555/3135535.3135553,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with neurological diseases using PCA and NPCA},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
abstract = {In this study, we wanted to discriminate between 30 patients who suffer from Parkinson's disease (PD) and 20 patients with other neurological diseases (ND). All participants were asked to pronounce sustained vowel /a/ hold as long as possible at comfortable level. The analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used principal component analysis (PCA) and nonlinear PCA (NPCA). These techniques reduce the number of parameters and select the most effective ones to be used for classification. Support vector machine and k-nearest neighbor with different kernels was used for classification. We obtained accuracy up to 88% for discrimination between PD patients ND patients using KNN with k equal to three and five.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {673–683},
numpages = {11},
keywords = {Feature selection, KNN, NPCA, Neurological diseases, PCA, SVM}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Run-time architecture evaluation, Runtime architecture evaluation, Software architectures for dynamic environments, Internet of things, IoT, Design diversity}
}

@article{10.1016/j.compind.2020.103189,
author = {Huang, Zhuoyu and Jowers, Casey and Dehghan-Manshadi, Ali and Dargusch, Matthew S.},
title = {Smart manufacturing and DVSM based on an Ontological approach},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {117},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2020.103189},
doi = {10.1016/j.compind.2020.103189},
journal = {Comput. Ind.},
month = may,
numpages = {15},
keywords = {Neo4j, Semantic ontology, CHAMP, CPS, DVSM}
}

@article{10.1145/264645.264658,
author = {Kieras, David E. and Wood, Scott D. and Meyer, David E.},
title = {Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/264645.264658},
doi = {10.1145/264645.264658},
abstract = {Engineering models of human performance permit some aspects of usability of interface designs to be predicted from an analysis of the task, and thus they can replace to some extent expensive user-testing data. We successfully predicted human performance in telephone operator tasks with engineering models constructed in the EPIC (Executive Process-Interactive Control) architecture for human information processing, which is especially suited for modeling multimodal, complex tasks, and has demonstrated success in other task domains. Several models were constructed on an a priori basis to represent different hypotheses about how operators coordinate their activities to produce rapid task performance. The models predicted  the total time with useful accuracy and clarified  some important properties of the task. The best model was based directly on the GOMS analysis of the task and made simple assumptions about the operator's task strategy, suggesting that EPIC models are a feasible approach to predicting performance in multimodal high-performance tasks.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
pages = {230–275},
numpages = {46},
keywords = {cognitive models, usability engineering}
}

@article{10.4018/jsse.2010102004,
author = {Nhlabatsi, Armstrong and Nuseibeh, Bashar and Yu, Yijun},
title = {Security Requirements Engineering for Evolving Software Systems: A Survey},
year = {2010},
issue_date = {January 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {1},
issn = {1947-3036},
url = {https://doi.org/10.4018/jsse.2010102004},
doi = {10.4018/jsse.2010102004},
abstract = {Long-lived software systems often undergo evolution over an extended period. Evolution of these systems is inevitable as they need to continue to satisfy changing business needs, new regulations and standards, and introduction of novel technologies. Such evolution may involve changes that add, remove, or modify features; or that migrate the system from one operating platform to another. These changes may result in requirements that were satisfied in a previous release of a system not being satisfied in subsequent versions. When evolutionary changes violate security requirements, a system may be left vulnerable to attacks. In this article we review current approaches to security requirements engineering and conclude that they lack explicit support for managing the effects of software evolution. We then suggest that a cross fertilization of the areas of software evolution and security engineering would address the problem of maintaining compliance to security requirements of software systems as they evolve.},
journal = {Int. J. Secur. Softw. Eng.},
month = jan,
pages = {54–73},
numpages = {20},
keywords = {Entailment Relation, Security Requirements Engineering, Software Evolution}
}

@article{10.1007/s10844-021-00675-4,
author = {Vidal, Cristian and Felfernig, Alexander and Galindo, Jos\'{e} and Atas, M\"{u}sl\"{u}m and Benavides, David},
title = {Explanations for over-constrained problems using QuickXPlain with speculative executions},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-021-00675-4},
doi = {10.1007/s10844-021-00675-4},
abstract = {Conflict detection is used in various scenarios ranging from interactive decision making (e.g., knowledge-based configuration) to the diagnosis of potentially faulty models (e.g., using knowledge base analysis operations). Conflicts can be regarded as sets of restrictions (constraints) causing an inconsistency. Junker’s QuickXPlain is a divide-and-conquer based algorithm for the detection of preferred minimal conflicts. In this article, we present a novel approach to the detection of such conflicts which is based on speculative programming. We introduce a parallelization of QuickXPlain and empirically evaluate this approach on the basis of synthesized knowledge bases representing feature models. The results of this evaluation show significant performance improvements in the parallelized QuickXPlain version.},
journal = {J. Intell. Inf. Syst.},
month = dec,
pages = {491–508},
numpages = {18},
keywords = {Speculative programming, Conflict detection, Explanations, Constraint solving, Configuration, Diagnosis, Feature models}
}

@inproceedings{10.1145/2516821.2516850,
author = {Porter, Joseph and Szab\'{o}, Csan\'{a}d},
title = {Partition configuration for real-time systems with dependencies},
year = {2013},
isbn = {9781450320580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2516821.2516850},
doi = {10.1145/2516821.2516850},
abstract = {We present an analytical framework for formulating partition configuration problems in real-time systems with dependencies, particularly applicable to modeling systems with multiple criticality or security levels. Partition configuration constraints for real-time tasks include affinity and conflict. We also discuss the application of the framework to arbitrary partition schedulers, harmonic partition execution, and round robin partition execution (which is particularly problematic). Our interest is in minimizing end-to-end latency, though the computational complexity of the problem prevents us from finding optimal results. We conclude with some open problems.},
booktitle = {Proceedings of the 21st International Conference on Real-Time Networks and Systems},
pages = {87–96},
numpages = {10},
keywords = {partitioned systems, safety, security},
location = {Sophia Antipolis, France},
series = {RTNS '13}
}

@article{10.1016/j.engappai.2009.08.001,
title = {Publishers note: An Implementing Framework for Holonic Manufacturing Control with Multiple Robot-Vision Stations},
year = {2009},
issue_date = {October, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {7},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.08.001},
doi = {10.1016/j.engappai.2009.08.001},
journal = {Eng. Appl. Artif. Intell.},
month = oct,
pages = {1139},
numpages = {1}
}

@article{10.1016/S1877-0509(20)30921-2,
title = {Contents},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(20)30921-2},
doi = {10.1016/S1877-0509(20)30921-2},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–xvi},
numpages = {14}
}

@inproceedings{10.1145/3357384.3358163,
author = {Huang, Chao and Shi, Baoxu and Zhang, Xuchao and Wu, Xian and Chawla, Nitesh V.},
title = {Similarity-Aware Network Embedding with Self-Paced Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358163},
doi = {10.1145/3357384.3358163},
abstract = {Network embedding, which aims to learn low-dimensional vector representations for nodes in a network, has shown promising performance for many real-world applications, such as node classification and clustering. While various embedding methods have been developed for network data, they are limited in their assumption that nodes are correlated with their neighboring nodes with the same similarity degree. As such, these methods can be suboptimal for embedding network data. In this paper, we propose a new method named SANE, short for Similarity-Aware Network Embedding, to learn node representations by explicitly considering different similarity degrees between connected nodes in a network. In particular, we develop a new framework based on self-paced learning by accounting for both the explicit relations (i.e., observed links) and implicit relations (i.e., unobserved node similarities) in network representation learning. To justify our proposed model, we perform experiments on two real-world network data. Experiments results show that SNAE outperforms state-of-the-art embedding models on the tasks of node classification and node clustering.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2113–2116},
numpages = {4},
keywords = {deep neural network, network embedding, self-paced learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/1027527.1027748,
author = {Feng, Huamin and Shi, Rui and Chua, Tat-Seng},
title = {A bootstrapping framework for annotating and retrieving WWW images},
year = {2004},
isbn = {1581138938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1027527.1027748},
doi = {10.1145/1027527.1027748},
abstract = {Most current image retrieval systems and commercial search engines use mainly text annotations to index and retrieve WWW images. This research explores the use of machine learning approaches to automatically annotate WWW images based on a predefined list of concepts by fusing evidences from image contents and their associated HTML text. One major practical limitation of employing supervised machine learning approaches is that for effective learning, a large set of labeled training samples is needed. This is tedious and severely impedes the practical development of effective search techniques for WWW images, which are dynamic and fast-changing. As web-based images possess both intrinsic visual contents and text annotations, they provide a strong basis to bootstrap the learning process by adopting a co-training approach involving classifiers based on two orthogonal set of features -- visual and text. The idea of co-training is to start from a small set of labeled training samples, and successively annotate a larger set of unlabeled samples using the two orthogonal classifiers. We carry out experiments using a set of over 5,000 images acquired from the Web. We explore the use of different combinations of HTML text and visual representations. We find that our bootstrapping approach can achieve a performance comparable to that of the supervised learning approach with an F1 measure of over 54%. At the same time, it offers the added advantage of requiring only a small initial set of training samples.},
booktitle = {Proceedings of the 12th Annual ACM International Conference on Multimedia},
pages = {960–967},
numpages = {8},
keywords = {WWW images, bootstrapping, co-training, image annotation},
location = {New York, NY, USA},
series = {MULTIMEDIA '04}
}

@article{10.1145/3337798,
author = {Jiang, Zhe and Sainju, Arpan Man and Li, Yan and Shekhar, Shashi and Knight, Joseph},
title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337798},
doi = {10.1145/3337798},
abstract = {Class ambiguity refers to the phenomenon whereby similar features correspond to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. The problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost. Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble) and thus cannot effectively minimize class ambiguity. In contrast, we propose a spatial ensemble framework that explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. We further extend our spatial ensemble learning framework with spatial dependency between nearby zones based on the spatial autocorrelation effect. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {43},
numpages = {25},
keywords = {Spatial classification, class ambiguity, local models, spatial ensemble, spatial heterogeneity}
}

@inbook{10.5555/2554542.2554558,
author = {Metzler, Saskia and Nieuwenhuisen, Matthias and Behnke, Sven},
title = {Learning visual obstacle detection using color histogram features},
year = {2012},
isbn = {9783642320590},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Perception of the environment is crucial in terms of successfully playing soccer. Especially the detection of other players improves game play skills, such as obstacle avoidance and path planning. Such information can help refine reactive behavioral strategies, and is conducive to team play capabilities. Robot detection in the RoboCup Standard Platform League is particularly challenging as the Nao robots are limited in computing resources and their appearance is predominantly white in color like the field lines. This paper describes a vision-based multilevel approach which is integrated into the B-Human Software Framework and evaluated in terms of speed and accuracy. On the basis of color segmented images, a feed-forward neural network is trained to discriminate between robots and non-robots. The presented algorithm initially extracts image regions which potentially depict robots and prepares them for classification. Preparation comprises calculation of color histograms as well as linear interpolation in order to obtain network inputs of a specific size. After classification by the neural network, a position hypothesis is generated.},
booktitle = {Robot Soccer World Cup XV},
pages = {149–161},
numpages = {13}
}

@inproceedings{10.1007/978-3-642-31087-4_78,
author = {Yazidi, Anis and Granmo, Ole-Christoffer and Oommen, B. John and Goodwin, Morten},
title = {A hierarchical learning scheme for solving the stochastic point location problem},
year = {2012},
isbn = {9783642310867},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31087-4_78},
doi = {10.1007/978-3-642-31087-4_78},
abstract = {This paper deals with the Stochastic-Point Location (SPL) problem. It presents a solution which is novel in both philosophy and strategy to all the reported related learning algorithms. The SPL problem concerns the task of a Learning Mechanism attempting to locate a point on a line. The mechanism interacts with a random environment which essentially informs it, possibly erroneously, if the unknown parameter is on the left or the right of a given point which also is the current guess. The first pioneering work [6] on the SPL problem presented a solution which operates a one-dimensional controlled Random Walk (RW) in a discretized space to locate the unknown parameter. The primary drawback of the latter scheme is the fact that the steps made are always very conservative. If the step size is decreased the scheme yields a higher accuracy, but the convergence speed is correspondingly decreased.In this paper we introduce the Hierarchical Stochastic Searching on the Line (HSSL) solution. The HSSL solution is shown to provide orders of magnitude faster convergence when compared to the original SPL solution reported in [6]. The heart of the HSSL strategy involves performing a controlled RW on a discretized space, which unlike the traditional RWs, is not structured on the line per se, but rather on a binary tree described by intervals on the line. The overall learning scheme is shown to be optimal if the effectiveness of the environment, p, is greater than the golden ratio conjugate [4] --- which, in itself, is a very intriguing phenomenon. The solution has been both analytically analyzed and simulated, with extremely fascinating results. The strategy presented here can be utilized to determine the best parameter to be used in any optimization problem, and also in any application where the SPL can be applied [6].},
booktitle = {Proceedings of the 25th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems: Advanced Research in Applied Artificial Intelligence},
pages = {774–783},
numpages = {10},
keywords = {controlled random walk, discretized learning, learning automata, stochastic-point problem},
location = {Dalian, China},
series = {IEA/AIE'12}
}

@inproceedings{10.1145/2984751.2985721,
author = {Yokota, Tomohiro and Hashida, Tomoko},
title = {Hand Gesture and On-body Touch Recognition by Active Acoustic Sensing throughout the Human Body},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985721},
doi = {10.1145/2984751.2985721},
abstract = {In this paper, we present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around the periphery of the wrist. Our approach can recognize hand gestures and on-body touch concurrently in real-time and is expected to obtain rich input variations by combining them. We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on the forearm, and touches on the back of the hand.},
booktitle = {Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},
pages = {113–115},
numpages = {3},
keywords = {acoustic sensing, combined input, hand gestures, machine learning, on-body touch},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@article{10.1007/s10664-018-9597-6,
author = {Quirchmayr, Thomas and Paech, Barbara and Kohl, Roland and Karey, Hannes and Kasdepke, Gunar},
title = {Semi-automatic rule-based domain terminology and software feature-relevant information extraction from natural language user manuals},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9597-6},
doi = {10.1007/s10664-018-9597-6},
abstract = {Mature software systems comprise a vast number of heterogeneous system capabilities which are usually requested by different groups of stakeholders and which evolve over time. Software features describe and bundle low level capabilities logically on an abstract level and thus provide a structured and comprehensive overview of the entire capabilities of a software system. Software features are often not explicitly managed. Quite the contrary, feature-relevant information is often spread across several software engineering artifacts (e.g., user manual, issue tracking systems). It requires huge manual effort to identify and extract feature-relevant information from these artifacts in order to make feature knowledge explicit. In this paper we present a two-step-approach to extract feature-relevant information from a user manual: First we semi-automatically extract a domain terminology from a natural language user manual based on linguistic patterns. Then, we apply natural language processing techniques based on the extracted domain terminology and structural sentence information. Our approach is able to extract atomic feature-relevant information with an F1-score of at least 92.00%. We describe the implementation of the approach as well as evaluations based on example sections of a user manual taken from industry.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {3630–3683},
numpages = {54},
keywords = {Atomic information extraction, NLP, Software feature, Terminology extraction}
}

@article{10.1007/s10664-008-9094-4,
author = {Lee, Jihyun and Kang, Sungwon and Kim, Chang-Ki},
title = {Software architecture evaluation methods based on cost benefit analysis and quantitative decision making},
year = {2009},
issue_date = {August    2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9094-4},
doi = {10.1007/s10664-008-9094-4},
abstract = {Since many parts of the architecture evaluation steps of the Cost Benefit Analysis Method (CBAM) depend on the stakeholders' empirical knowledge and intuition, it is very important that such an architecture evaluation method be able to faithfully reflect the knowledge of the experts in determining Architectural Strategy (AS). However, because CBAM requires the stakeholders to make a consensus or vote for collecting data for decision making, it is difficult to accurately reflect the stakeholders' knowledge in the process. In order to overcome this limitation of CBAM, we propose the two new CBAM-based methods for software architecture evaluation, which respectively adopt the Analytic Hierarchy Process (AHP) and the Analytic Network Process (ANP). Since AHP and ANP use pair-wise comparison they are suitable for a cost and benefit analysis technique since its purpose is not to calculate correct values of benefit and cost but to decide AS with highest return on investment. For that, we first define a generic process of CBAM and develop variations from the generic process by applying AHP and ANP to obtain what we call the CBAM+AHP and CBAM+ANP methods. These new methods not only reflect the knowledge of experts more accurately but also reduce misjudgments. A case study comparison of CBAM and the two new methods is conducted using an industry software project. Because the cost benefit analysis process that we present is generic, new cost benefit analysis techniques with capabilities and characteristics different from the three methods we examine here can be derived by adopting various different constituent techniques.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {453–475},
numpages = {23},
keywords = {AHP, ANP, CBAM, Software architecture evaluation}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Automated production systems, Automation, Evolution, Software engineering}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {Quality evaluation, evolutionary algorithms, exact method, heuristic, indicator, measure, metaheuristic, metric, multi-criteria optimisation, multobjective optimisation, performance assessment}
}

@inproceedings{10.5555/3042817.3042922,
author = {Das, Mrinal Kanti and Bhattacharya, Suparna and Bhattacharyya, Chiranjib and Gopinath, K.},
title = {Subtle topic models and discovering subtly manifested software concerns automatically},
year = {2013},
publisher = {JMLR.org},
abstract = {In a recent pioneering approach LDA was used to discover cross cutting concerns (CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed in (Williamson et al., 2010) for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models (STM). STM uses a generalized stick breaking process (GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {II–253–II–261},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{10.5555/1718098.1718104,
author = {El-Raouf, Amal Abd},
title = {Hierarchical clustering of distributed object-oriented software systems: a generic solution for software-hardware mismatch problem},
year = {2009},
issue_date = {November 2009},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {8},
number = {11},
issn = {1109-2750},
abstract = {During the software lifecycle, the software structure is subject to many changes in order to fulfill the customer's requirements. In Distributed Object Oriented systems, software engineers face many challenges to solve the software-hardware mismatch problem in which the software structure does not match the customer's underlying hardware. A major design problem of Object Oriented software systems is the efficient distribution of software classes among the different nodes in the system while maintaining two features: low-coupling and high software quality. In this paper, we present a new methodology for efficiently restructuring Distributed Object Oriented software systems to improve the overall system performance and to solve the softwarehardware mismatch problem. Our method has two main phases. In the first phase, we use the hierarchical clustering method to restructure the target software application. As a result, all the possible clustering solutions that could be applied to the target software application are generated. In the second phase, we decide on the best-fit clustering solution according to the customer hardware organization.},
journal = {W. Trans. on Comp.},
month = nov,
pages = {1780–1789},
numpages = {10},
keywords = {distributed systems, hierarchical clustering, low coupling, object oriented software, performance analysis, software restructuring}
}

@inproceedings{10.5555/1113914.1113961,
author = {Li, Hailin and Dagli, Cihan H.},
title = {Hybrid least-squares methods for reinforcement learning},
year = {2003},
isbn = {3540404554},
publisher = {Springer Springer Verlag Inc},
abstract = {Model-free Least-Squares Policy Iteration (LSPI) method has been successfully used for control problems in the context of reinforcement learning. LSPI is a promising algorithm that uses linear approximator architecture to achieve policy optimization in the spirit of Q-learning. However it faces challenging issues in terms of the selection of basis functions and training sample. Inspired by orthogonal Least-Squares regression method for selecting the centers of RBF neural network, a new hybrid learning method for LSPI is proposed in this paper. The suggested method uses simulation as a tool to guide the "feature configuration" process. The results on the learning control of Cart-Pole system illustrate the effectiveness of the presented method.},
booktitle = {Proceedings of the 16th International Conference on Developments in Applied Artificial Intelligence},
pages = {471–480},
numpages = {10},
location = {Laughborough, UK},
series = {IEA/AIE'2003}
}

@article{10.1007/s11634-016-0266-6,
author = {Hayashi, Kenichi},
title = {Asymptotic comparison of semi-supervised and supervised linear discriminant functions for heteroscedastic normal populations},
year = {2018},
issue_date = {June      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {2},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-016-0266-6},
doi = {10.1007/s11634-016-0266-6},
abstract = {It has been reported that using unlabeled data together with labeled data to construct a discriminant function works successfully in practice. However, theoretical studies have implied that unlabeled data can sometimes adversely affect the performance of discriminant functions. Therefore, it is important to know what situations call for the use of unlabeled data. In this paper, asymptotic relative efficiency is presented as the measure for comparing analyses with and without unlabeled data under the heteroscedastic normality assumption. The linear discriminant function maximizing the area under the receiver operating characteristic curve is considered. Asymptotic relative efficiency is evaluated to investigate when and how unlabeled data contribute to improving discriminant performance under several conditions. The results show that asymptotic relative efficiency depends mainly on the heteroscedasticity of the covariance matrices and the stochastic structure of observing the labels of the cases.},
journal = {Adv. Data Anal. Classif.},
month = jun,
pages = {315–339},
numpages = {25},
keywords = {62G20, 62H30, 68T10, Area under the ROC curve, Labeling mechanism, Linear discriminant function, Missing data, Receiver operating characteristic curve, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-319-27343-3_1,
author = {Braubach, Lars and Pokahr, Alexander and Kalinowski, Julian and Jander, Kai},
title = {Tailoring Agent Platforms with Software Product Lines},
year = {2015},
isbn = {9783319273426},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27343-3_1},
doi = {10.1007/978-3-319-27343-3_1},
abstract = {Agent platforms have been conceived traditionally as middleware, helping to deal with various application challenges like agent programming models, remote messaging, and coordination protocols. A\"{\i} \'{z}middleware is typically a bundle of functionalities necessary to execute multi-agent applications. In contrast to this traditional view, nowadays different use cases also for selected agent concepts have emerged requiring also different kinds of functionalities. Examples include a platform for conducting multi-agent simulations, intelligent agent behavior models for controlling non-player characters NPCs in games and a lightweight version suited for mobile devices. A one-size-fits-all software bundle often does not sufficiently match these requirements, because customers and developers want solutions specifically tailored to their needs, i.e. a small but focused solution is frequently preferred over bloated software with extraneous functionality. Software product lines are an approach suitable for creating a series of similar products from a common code base. In this paper we will show how software product line modeling and technology can help creating tailor-made products from multi-agent platforms. Concretely, the Jadex platform will be analyzed and a feature model as well as an implementation path will be presented.},
booktitle = {Revised Selected Papers of the 13th German Conference on Multiagent System Technologies - Volume 9433},
pages = {3–21},
numpages = {19},
location = {Cottbus, Germany},
series = {MATES 2015}
}

@article{10.1145/3478093,
author = {Zhang, Qian and Wang, Dong and Zhao, Run and Yu, Yinggang and Shen, Junjie},
title = {Sensing to Hear: Speech Enhancement for Mobile Devices Using Acoustic Signals},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478093},
doi = {10.1145/3478093},
abstract = {Voice interactions and voice messages on mobile phones are rapidly growing in popularity. However, the user experience of these services is still worse than desired in noisy environments, especially in multi-talker scenarios, where the phone can only provide low-quality voice recordings. Speech enhancement using only audio as the input remains a grand challenge in these scenarios. In this paper, we handle this with the help of the emerging acoustic sensing technology. The key insight is that the inaudible acoustic signals emitted by speakers of phones can capture the subtle lip movements when people speak. Instead of enabling lip reading for the classification of limited voice commands, we further unlock the potential of acoustic sensing and leverage the captured lip information to improve the voice recording quality. We propose WaveVoice, a joint audio-sensory deep learning method for end-to-end speech enhancement on mobile phones. The model of WaveVoice is structured as an encoder-decoder network, in which audio and acoustic sensing data are processed through two individual CNN branches, respectively, and then fused into a joint network to generate enhanced speech. In addition, to improve the performance on new users, a self-supervised learning methodology is developed to adapt the model to extract speaker-specific features. We construct a dataset to train and evaluate WaveVoice. We also perform online tests under various noisy conditions to show the applicability of our system in real-world scenarios. Experimental results show that WaveVoice can effectively reconstruct the target clean speech from the noisy audio signals, and yield notably superior performance compared with the audio-only encoder-decoder model and the state-of-the-art speech enhancement methods. Given its promising performance, we believe that WaveVoice has made a substantial contribution to the advancement of mobile voice input.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {137},
numpages = {30},
keywords = {acoustic sensing, deep learning, lip movement, mobile speech enhancement}
}

@article{10.1504/IJAOSE.2011.043643,
author = {Nunes, Ingrid and Lucena, Carlos J. P. De and Cowan, Donald and Kulesza, Uir\'{a} and Alencar, Paulo and Nunes, Camila},
title = {Developing multi-agent system product lines: from requirements to code},
year = {2011},
issue_date = {November 2011},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {4},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2011.043643},
doi = {10.1504/IJAOSE.2011.043643},
abstract = {Many modern software systems have autonomous, open, context-aware and highly-interactive properties. The agent abstraction with its autonomous and pro-active characteristics and the related discipline of agent-oriented software engineering (AOSE) are promising paradigms to address these types of systems. Even though agents are frequently being adopted, little effort has been directed in AOSE methodologies toward extensive software reuse techniques, which can provide both reduced time-to-market and lower development costs. Multi-agent system product lines (MAS-PLs) are the result of the integration of AOSE with software product lines (SPLs). SPLs bring many reuse benefits to the agent domain through the exploitation of common characteristics among family members. In this context, this paper presents a domain engineering process for developing MAS-PLs. It defines activities and work products, whose purposes include supporting agent variability and providing agent feature traceability, both not addressed by current SPL and AOSE approaches.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = nov,
pages = {353–389},
numpages = {37}
}

@article{10.5555/1839525.1839531,
author = {Ueta, Atsushi and Yairi, Takehisa and Kanazaki, Hirofumi and Machida, Kazuo},
title = {Map building without localization by estimation of inter-feature distances},
year = {2010},
issue_date = {December 2010},
publisher = {IOS Press},
address = {NLD},
volume = {14},
number = {4},
issn = {1088-467X},
abstract = {This paper proposes an alternative solution to a mapping problem in two different cases; when bearing measurements to features (landmarks) and odometry are measured and when bearing and range measurements to features are measured. Our approach named M-SEIFD (Mapping by Sequential Estimation of Inter-Feature Distances) first estimates inter-feature distances, then finds global position of all the features by enhanced multi-dimensional scaling (MDS). M-SEIFD is different from the conventional SLAM methods based on Bayesian filtering in that robot self-localization is not compulsory and that M-SEIFD is able to utilize prior information about relative distances among features directly. We show that M-SEIFD is able to achieve a decent map of features both in simulation and in real-world environment with a mobile robot.},
journal = {Intell. Data Anal.},
month = dec,
pages = {515–529},
numpages = {15},
keywords = {Mapping, mobile robot, multi-dimensional scaling}
}

@article{10.14778/3352063.3352112,
author = {Lu, Jiaheng and Chen, Yuxing and Herodotou, Herodotos and Babu, Shivnath},
title = {Speedup your analytics: automatic parameter tuning for databases and big data systems},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352112},
doi = {10.14778/3352063.3352112},
abstract = {Database and big data analytics systems such as Hadoop and Spark have a large number of configuration parameters that control memory distribution, I/O optimization, parallelism, and compression. Improper parameter settings can cause significant performance degradation and stability issues. However, regular users and even expert administrators struggle to understand and tune them to achieve good performance. In this tutorial, we review existing approaches on automatic parameter tuning for databases, Hadoop, and Spark, which we classify into six categories: rule-based, cost modeling, simulation-based, experiment-driven, machine learning, and adaptive tuning. We describe the foundations of different automatic parameter tuning algorithms and present pros and cons of each approach. We also highlight real-world applications and systems, and identify research challenges for handling cloud services, resource heterogeneity, and real-time analytics.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1970–1973},
numpages = {4}
}

@article{10.1155/2021/5517843,
author = {Zaiyi, Pu and Kun, Yang and Chaobin, Wang and Chen, Chi-Hua},
title = {Distributed Network Image Processing System and Transmission Control Algorithm},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5517843},
doi = {10.1155/2021/5517843},
abstract = {With the increasing use of Internet technologies, image data is spreading more and more on the Internet. Whether it is a social network or a search engine, a large amount of image data is generated. By studying the distributed network image processing system and transmission control algorithm, this paper proposes a more accurate gradient calculation method based on the SIFT algorithm. It is concluded that the performance of the proposed algorithm is slightly better than that of the original algorithm, so the system is implemented. On the basis of reducing the performance of the original algorithm, the dimension of the image features is effectively reduced. By comparing the influence of the image retrieval system in the single-machine environment and the distributed environment on the image feature extraction rate, it is proved that the system uses five distributed nodes to construct the image transmission system that achieves the best results in terms of machine cost and system performance. The random Gaussian orthogonal matrix is analyzed with good stability and performance. The OMP algorithm has good convergence and reconstruction performance. The MH-BCS-SPL reconstruction algorithm works best, and the PSNR decreases very smoothly in the process of increasing the packet loss rate from 0.1 to 0.6. The experimental results show that different orthogonal bases behave differently under different images. Overall, the BCS-SPL series algorithm has greatly improved the reconstruction effect compared with the traditional OMP algorithm.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/3341161.3343517,
author = {Xylogiannopoulos, Konstantinos F.},
title = {Exhaustive exact string matching: the analysis of the full human genome},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343517},
doi = {10.1145/3341161.3343517},
abstract = {Exact string matching has been a fundamental problem in computer science for decades because of many practical applications. Some are related to common procedures, such as searching in files and text editors, or, more recently, to more advanced problems such as pattern detection in Artificial Intelligence and Bioinformatics. Tens of algorithms and methodologies have been developed for pattern matching and several programming languages, packages, applications and online systems exist that can perform exact string matching in biological sequences. These techniques, however, are limited to searching for specific and predefined strings in a sequence. In this paper a novel methodology (called Ex2SM) is presented, which is a pipeline of execution of advanced data structures and algorithms, explicitly designed for text mining, that can detect every possible repeated string in multivariate biological sequences. In contrast to known algorithms in literature, the methodology presented here is string agnostic, i.e., it does not require an input string to search for it, rather it can detect every string that exists at least twice, regardless of its attributes such as length, frequency, alphabet, overlapping etc. The complexity of the problem solved and the potential of the proposed methodology is demonstrated with the experimental analysis performed on the entire human genome. More specifically, all repeated strings with a length of up to 50 characters have been detected, an achievement which is practically impossible using other algorithms due to the exponential number of possible arrangements [EQUATION] of such long strings.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {801–808},
numpages = {8},
keywords = {ARPaD, LERP-RSA, MLERP, exact string matching, human genome, repeated pattern detection},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@article{10.1016/j.sigpro.2019.01.018,
author = {Ahmed, Talal and Bajwa, Waheed U.},
title = {ExSIS: Extended sure independence screening for ultrahigh-dimensional linear models},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.01.018},
doi = {10.1016/j.sigpro.2019.01.018},
journal = {Signal Process.},
month = jun,
pages = {33–48},
numpages = {16},
keywords = {Variable selection, Variable screening, Sure screening, Linear models, High-dimensional statistics, Sparse signal processing}
}

@article{10.1155/2021/9063410,
author = {Zhang, Jianfeng and Lv, Zhihan},
title = {Virtual Viewpoint Film and Television Synthesis Based on the Intelligent Algorithm of Wireless Network Communication for Image Repair},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9063410},
doi = {10.1155/2021/9063410},
abstract = {With the development of the computer vision field, the acquisition of scene depth information is one of the important topics in the three-dimensional reconstruction of the computer vision field, and its significance is particularly important. The purpose of this paper is to study the virtual viewpoint video synthesis for image restoration based on the intelligent algorithm of wireless network communication. Aiming at the hole problem caused by the change of occlusion relationship, this paper proposes a hole-filling method based on background recognition. A threshold segmentation algorithm is used to reduce the filling priority of foreground pixels at the boundary of the hole and fully solve the hole problem. This paper also proposes a wireless sensor network node positioning model with swarm intelligence algorithm, which combines swarm intelligence algorithm with some key issues of wireless sensor network, speeds up the convergence, and improves the traditional intelligence algorithm. According to the experimental data in this paper, the algorithm in this paper is about 20% higher than the traditional algorithm in PSNR. On SSIM, the performance of the algorithm in this paper is 4.6% higher than the traditional algorithm at most, and the lowest is 2.2%.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {15}
}

@inproceedings{10.1109/MODELS-C.2019.00046,
author = {Alwidian, Sanaa and Amyot, Daniel},
title = {Inferring metamodel relaxations based on structural patterns to support model families},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00046},
doi = {10.1109/MODELS-C.2019.00046},
abstract = {A model family is a set of related models in a given language that results from the evolution of models over time and/or variations over the space (product) dimension. To enable a more efficient analysis of family members, all at once, we have already proposed union models to capture the union of all elements in all family members, in a compact and exact manner. However, despite having each model in a model family conforming to the same metamodel, there is still no guarantee that their union model will conform to the original metamodel of the family members. This paper aims to support the representation of union models (as valid instances of a metamodel) by inferring, from the structure of the original metamodel, a relaxed metamodel to which a union model conforms. In particular, instead of relaxing all metamodel constraints, the paper contributes a heuristic method that relaxes particular constraints (related only to multiplicities of attributes and association ends) by inferring where such relaxations are needed in the metamodel. To infer relaxation points, structural patterns are first identified in metamodels, then an evidence-based or an anticipation-based approach is applied to get the actual inference. The purpose behind inferring particular metamodel relaxation points is to be able to adapt the existing tools and analysis techniques once and minimally for all potential model families of a given modeling language.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {294–303},
numpages = {10},
keywords = {metamodel, metamodel relaxation, model, model family, relaxation point, structural pattern, union model},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@article{10.1016/j.fss.2008.05.017,
author = {Juarez, Jose M. and Guil, Francisco and Palma, Jose and Marin, Roque},
title = {Temporal similarity by measuring possibilistic uncertainty in CBR},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {160},
number = {2},
issn = {0165-0114},
url = {https://doi.org/10.1016/j.fss.2008.05.017},
doi = {10.1016/j.fss.2008.05.017},
abstract = {Similarity is an essential concept in case-based reasoning (CBR). In domains in which time plays a relevant role, CBR systems require good temporal similarity measures to compare cases. Temporal cases are traditionally represented by a set of temporal features, defining time series and temporal event sequences. In the particular situation where these features are not homogeneous (i.e. combination of qualitative and quantitative information), systems find difficulties in performing the CBR cycle. Furthermore, temporal similarity measures cannot directly apply the efficient time series techniques, requiring new approaches to deal with these heterogeneous sequences. To this end, recent proposals are focused on direct matching between pairs of features within sequences, mainly based on classical distances. However, three limitations to the traditional approaches have been identified: (1) they do not consider the implicit temporal relations amongst all features of the sequence (ignoring a large amount of temporal information); (2) they ignore the uncertainty produced in any process of analogy; (3) they are designed to compare pairs of sequences, limiting their use to basic aspects of the Retrieval step of CBR (no benefits on other CBR steps). Temporal constraint networks have proved to be useful tools for temporal representation and reasoning, and can be easily extended to manage imprecision and uncertainty. An approach to solve similarity problems could be the transformation of these heterogeneous sequences into uncertain temporal relations, obtaining a temporal constraint network. The overall uncertainty of this network can be considered as an effective indicator of the sequences similarity. Therefore, this paper proposes a non-classical approach to measure temporal similarity of cases which are heterogeneous temporal event sequences. Given two or more sequences, the temporal similarity is measured by describing a unique temporal scenario of possibilistic temporal relations and calculating the uncertainty produced.},
journal = {Fuzzy Sets Syst.},
month = jan,
pages = {214–230},
numpages = {17},
keywords = {Case-based reasoning, Possibility theory, Temporal constraint networks}
}

@article{10.1145/3432195,
author = {Mao, Wenguang and Sun, Wei and Wang, Mei and Qiu, Lili},
title = {DeepRange: Acoustic Ranging via Deep Learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432195},
doi = {10.1145/3432195},
abstract = {Acoustic ranging is a technique for estimating the distance between two objects using acoustic signals, which plays a critical role in many applications, such as motion tracking, gesture/activity recognition, and indoor localization. Although many ranging algorithms have been developed, their performance still degrades significantly under strong noise, interference and hardware limitations. To improve the robustness of the ranging system, in this paper we develop a Deep learning based Ranging system, called DeepRange. We first develop an effective mechanism to generate synthetic training data that captures noise, speaker/mic distortion, and interference in the signals and remove the need of collecting a large volume of training data. We then design a deep range neural network (DRNet) to estimate distance. Our design is inspired by signal processing that ultra-long convolution kernel sizes help to combat the noise and interference. We further apply an ensemble method to enhance the performance. Moreover, we analyze and visualize the network neurons and filters, and identify a few important findings that can be useful for improving the design of signal processing algorithms. Finally, we implement and evaluate DeepRangeusing 11 smartphones with different brands and models, 4 environments (i.e., a lab, a conference room, a corridor, and a cubic area), and 10 users. Our results show that DRNet significantly outperforms existing ranging algorithms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {143},
numpages = {23},
keywords = {Acoustic Sensing, Convolutional Neural Network, Motion Tracking, Ranging}
}

@inproceedings{10.1007/978-3-030-91825-5_3,
author = {Das, Susmoy and Sharma, Arpit},
title = {State Space Minimization Preserving Embeddings for Continuous-Time Markov Chains},
year = {2021},
isbn = {978-3-030-91824-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91825-5_3},
doi = {10.1007/978-3-030-91825-5_3},
abstract = {This paper defines embeddings which allow one to construct an action labeled continuous-time Markov chain (ACTMC) from a state labeled continuous-time Markov chain (SCTMC) and vice versa. We prove that these embeddings preserve strong forward bisimulation and strong backward bisimulation. We define weak backward bisimulation for ACTMCs and SCTMCs, and also prove that our embeddings preserve both weak forward and weak backward bisimulation. Next, we define the invertibility criteria and the inverse of these embeddings. Finally, we prove that an ACTMC can be minimized by minimizing its embedded model, i.e. SCTMC and taking the inverse of the embedding. Similarly, we prove that an SCTMC can be minimized by minimizing its embedded model, i.e. ACTMC and taking the inverse of the embedding.},
booktitle = {Performance Engineering and Stochastic Modeling: 17th European Workshop, EPEW 2021, and 26th International Conference, ASMTA 2021, Virtual Event, December 9–10 and December 13–14, 2021, Proceedings},
pages = {44–61},
numpages = {18},
keywords = {Markov chain, Behavioral equivalence, Bisimulation equivalence, Stochastic systems, Embeddings},
location = {Milan, Italy}
}

@inproceedings{10.5555/646473.692986,
author = {Slezak, Dominik and Wroblewski, Jakub},
title = {Approximate Bayesian Network Classifiers},
year = {2002},
isbn = {354044274X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Bayesian network (BN) is a directed acyclic graph encoding probabilistic independence statements between variables. BN with decision attribute as a root can be applied to classification of new cases, by synthesis of conditional probabilities propagated along the edges. We consider approximate BNs, which almost keep entropy of a decision table. They have usually less edges than classical BNs. They enable to model and extend the well-known Naive Bayes approach. Experiments show that classifiers based on approximate BNs can be very efficient.},
booktitle = {Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing},
pages = {365–372},
numpages = {8},
series = {TSCTC '02}
}

@inbook{10.1145/3191315.3191327,
title = {Index},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191327},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {548–587},
numpages = {40}
}

@inproceedings{10.5555/2887007.2887106,
author = {Schichl, Hermann and Sellmann, Meinolf},
title = {Predisaster preparation of transportation networks},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {We develop a new approach for a pre-disaster planning problem which consists in computing an optimal investment plan to strengthen a transportation network, given that a future disaster probabilistically destroys links in the network. We show how the problem can be formulated as a non-linear integer program and devise an AI algorithm to solve it. In particular, we introduce a new type of extreme resource constraint and develop a practically efficient propagation algorithm for it. Experiments show several orders of magnitude improvements over existing approaches, allowing us to close an existing real-world benchmark and to solve to optimality other, more challenging benchmarks.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {709–715},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inbook{10.1145/3191315.3191322,
author = {Borraz-S\'{a}nchez, Conrado and Klabjan, Diego and Pasalic, Emir and Aref, Molham},
title = {SolverBlox: algebraic modeling in datalog},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191322},
abstract = {Datalog is a deductive query language for relational databases. We introduce LogiQL, a language based on Datalog and show how it can be used to specify mixedinteger linear optimization models and solve them. Unlike pure algebraic modeling languages, LogiQL allows the user to both specify models, and manipulate and transform the inputs and outputs of the models. This is an advantage over conventional optimization modeling languages that rely on reading data via plug-in tools or importing data from external sources via files. In this chapter, we give a brief overview of LogiQL and describe two mixed integer programming case studies: a production-transportation model and a formulation of the traveling salesman problem.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {331–354},
numpages = {24}
}

@article{10.1023/A:1008233105405,
author = {Appelt, Douglas E.},
title = {Wiederverwendung von Pl\"{a} en in deduktiven Planungssystemen (Reuse of Plans in Deductive Planning Systems), by Jana K\"{o}hler},
year = {1997},
issue_date = {1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {7},
number = {3},
issn = {0924-1868},
url = {https://doi.org/10.1023/A:1008233105405},
doi = {10.1023/A:1008233105405},
journal = {User Modeling and User-Adapted Interaction},
month = mar,
pages = {219–222},
numpages = {4}
}

@article{10.1007/s10009-014-0341-2,
author = {Filho, Jo\~{a}o Bosco and Barais, Olivier and Acher, Mathieu and Le Noir, J\'{e}r\^{o}me and Legay, Axel and Baudry, Benoit},
title = {Generating counterexamples of model-based software product lines},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-014-0341-2},
doi = {10.1007/s10009-014-0341-2},
abstract = {In a model-based software product line (MSPL), the variability of the domain is characterized in a variability model and the core artifacts are base models conforming to a modeling language (also called metamodel). A realization model connects the features of the variability model to the base model elements, triggering operations over these elements based on a configuration. The design space of an MSPL is extremely complex to manage for the engineer, since the number of variants may be exponential and the derived product models have to be conforming to numerous well-formedness and business rules. In this paper, the objective is to provide a way to generate MSPLs, called counterexamples (also called antipatterns), that can produce invalid product models despite a valid configuration in the variability model. We describe the foundations and motivate the usefulness of counterexamples (e.g., inference of guidelines or domain-specific rules to avoid earlier the specification of incorrect mappings; testing oracles for increasing the robustness of derivation engines given a modeling language). We provide a generic process, based on the common variability language (CVL) to randomly search the space of MSPLs for a specific modeling language. We develop LineGen a tool on top of CVL and modeling technologies to support the methodology and the process. LineGen targets different scenarios and is flexible to work either with just a domain metamodel as input or also with pre-defined variability models and base models. We validate the effectiveness of this process for three formalisms at different scales (up to 247 metaclasses and 684 rules). We also apply the approach in the context of a real industrial scenario involving a large-scale metamodel.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {585–600},
numpages = {16},
keywords = {Counterexamples, Model-based engineering, Software product lines}
}

@inproceedings{10.1007/978-3-319-58068-5_1,
author = {Costabello, Luca and Vandenbussche, Pierre-Yves and Shukair, Gofran and Deliot, Corine and Wilson, Neil},
title = {Traffic Analytics for Linked Data Publishers},
year = {2017},
isbn = {978-3-319-58067-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-58068-5_1},
doi = {10.1007/978-3-319-58068-5_1},
abstract = {We present a traffic analytics platform for servers that publish Linked Data. To the best of our knowledge, this is the first system that mines access logs of registered Linked Data servers to extract traffic insights on daily basis and without human intervention. The framework extracts Linked Data-specific traffic metrics from log records of HTTP lookups and SPARQL queries, and provides insights not available in traditional web analytics tools. Among all, we detect visitor sessions with a variant of hierarchical agglomerative clustering. We also identify workload peaks of SPARQL endpoints by detecting heavy and light SPARQL queries with supervised learning. The platform has been tested on 13&nbsp;months of access logs of the British National Bibliography RDF dataset.},
booktitle = {The Semantic Web: 14th International Conference, ESWC 2017, Portoro\v{z}, Slovenia, May 28 – June 1, 2017, Proceedings, Part I},
pages = {3–18},
numpages = {16},
keywords = {Linked data, Traffic analytics, Data publication, SPARQL},
location = {Portoro\v{z}, Slovenia}
}

@article{10.1017/S0890060498124010,
author = {Darr, Timothy and Klein, Mark and McGuinness, Deborah L.},
title = {Special Issue: Configuration Design},
year = {1998},
issue_date = {September 1998},
publisher = {Cambridge University Press},
address = {USA},
volume = {12},
number = {4},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060498124010},
doi = {10.1017/S0890060498124010},
abstract = {In configuration design parts are selected and connected to meet customer specifications and engineering and physical constraints. Specifications include preferences (e.g., “prefer lower cost to higher performance, all things being equal”), bounds on various resources (e.g., “the computer should have four PCI slots”), and other information to customize a configuration. Constraints typically arise from exogenous concerns, such as the available parts, the way parts can interact, and the manufacturing plant.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = sep,
pages = {293–294},
numpages = {2}
}

@inproceedings{10.1145/2245276.2231938,
author = {Sardinha, Alberto and Yu, Yijun and Niu, Nan and Rashid, Awais},
title = {EA-tracer: identifying traceability links between code aspects and early aspects},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231938},
doi = {10.1145/2245276.2231938},
abstract = {Early aspects are crosscutting concerns that are identified and addressed at the requirements and architecture level, while code aspects are crosscutting concerns that manifest at the code level. Currently, there are many approaches to address the identification and modularization of these cross-cutting concerns at each level, but very few techniques try to analyze the relationship between early aspects and code aspects. This paper presents a tool for automating the process of identifying traceability links between requirements-level aspects and code aspects, which is a first step towards an in-depth analysis. We also present an empirical evaluation of the tool with a real-life Web-based information system and a software product line for handling data on mobile devices. The results show that we can identify traceability links between early aspects and code aspects with a high accuracy.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1035–1042},
numpages = {8},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.5555/2886521.2886533,
author = {Lierler, Yuliya and Truszczynski, Miroslaw},
title = {An abstract view on modularity in knowledge representation},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Modularity is an essential aspect of knowledge representation theory and practice. It has received substantial attention. We introduce model-based modular systems, an abstract framework for modular knowledge representation formalisms, similar in scope to multi-context systems but employing a simpler information-flow mechanism. We establish the precise relationship between the two frameworks, showing that they can simulate each other. We demonstrate that recently introduced modular knowledge representation formalisms integrating logic programming with satisfiability and, more generally, with constraint satisfaction can be cast as modular systems in our sense. These results show that our formalism offers a simple unifying framework for studies of modularity in knowledge representation.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1532–1538},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@article{10.1007/s10601-017-9269-y,
author = {Cauwelaert, Sascha and Schaus, Pierre},
title = {Efficient filtering for the Resource-Cost AllDifferent constraint},
year = {2017},
issue_date = {October   2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1383-7133},
url = {https://doi.org/10.1007/s10601-017-9269-y},
doi = {10.1007/s10601-017-9269-y},
abstract = {This paper studies a family of optimization problems where a set of items, each requiring a possibly different amount of resource, must be assigned to different slots for which the price of the resource can vary. The objective is then to assign items such that the overall resource cost is minimized. Such problems arise commonly in domains such as production scheduling in the presence of fluctuating renewable energy costs or variants of the Travelling Salesman Problem. In Constraint Programming, this can be naturally modeled in two ways: (a) with a sum of element constraints; (b) with a MinimumAssignment constraint. Unfortunately the sum of element constraints obtains a weak filtering and the MinimumAssignment constraint does not scale well on large instances. This work proposes a third approach by introducing the ResourceCostAllDifferent constraint and an associated incremental and scalable filtering algorithm, running in O(n\'{z}m)$mathcal {O}(n cdot m)$, where n is the number of unbound variables and m is the maximum domain size of unbound variables. Its goal is to compute the total cost in a scalable manner by dealing with the fact that all assignments must be different. We first evaluate the efficiency of the new filtering on a real industrial problem and then on the Product Matrix Travelling Salesman Problem, a special case of the Asymmetric Travelling Salesman Problem. The study shows experimentally that our approach generally outperforms the decomposition and the MinimumAssignment ones for the problems we considered.},
journal = {Constraints},
month = oct,
pages = {493–511},
numpages = {19},
keywords = {AllDifferent, Assignment cost, Energy, Filtering, Product matrix travelling salesman problem, Resource, Scalability, Scheduling}
}

@article{10.1155/2015/635840,
author = {Krishnamurti, Sridhar},
title = {Application of neural network modeling to identify auditory processing disorders in school-age children},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-7594},
url = {https://doi.org/10.1155/2015/635840},
doi = {10.1155/2015/635840},
abstract = {P300 Auditory Event-Related Potentials (P3AERPs) were recorded in nine school-age children with auditory processing disorders and nine age- and gender-matched controls in response to tone burst stimuli presented at varying rates (1/second or 3/second) under varying levels of competing noise (0 dB, 40 dB, or 60 dB SPL). Neural network modeling results indicated that speed of information processing and task-related demands significantly influenced P3AERP latency in children with auditory processing disorders. Competing noise and rapid stimulus rates influenced P3AERP amplitude in both groups.},
journal = {Adv. Artif. Neu. Sys.},
month = jan,
articleno = {5},
numpages = {1}
}

@article{10.1016/j.cmpb.2016.01.002,
author = {undefinedosi\'{c}, Kre\v{s}imir and Popovi\'{c}, Sini\v{s}a and Kukolja, Davor and Dropulji\'{c}, Branimir and Ivanec, Dragutin and Tonkovi\'{c}, Mirjana},
title = {Multimodal analysis of startle type responses},
year = {2016},
issue_date = {Jun 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2016.01.002},
doi = {10.1016/j.cmpb.2016.01.002},
journal = {Comput. Methods Prog. Biomed.},
month = jun,
pages = {186–202},
numpages = {17},
keywords = {Startle response, Multimodal analysis, Physiological, speech and facial features, Airblast, International Affective Picture System, International Affective Digitized Sounds}
}

@article{10.1017/S0890060404040028,
author = {Simpson, Timothy W.},
title = {Product platform design and customization: Status and promise},
year = {2004},
issue_date = {January 2004},
publisher = {Cambridge University Press},
address = {USA},
volume = {18},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060404040028},
doi = {10.1017/S0890060404040028},
abstract = {In an effort to improve customization for today's highly competitive global marketplace, many companies are utilizing product families and platform-based product development to increase variety, shorten lead times, and reduce costs. The key to a successful product family is the product platform from which it is derived either by adding, removing, or substituting one or more modules to the platform or by scaling the platform in one or more dimensions to target specific market niches. This nascent field of engineering design has matured rapidly in the past decade, and this paper provides a comprehensive review of the flurry of research activity that has occurred during that time to facilitate product family design and platform-based product development for mass customization. Techniques for identifying platform leveraging strategies within a product family are reviewed along with metrics for assessing the effectiveness of product platforms and product families. Special emphasis is placed on optimization approaches and artificial intelligence techniques to assist in the process of product family design and platform-based product development. Web-based systems for product platform customization are also discussed. Examples from both industry and academia are presented throughout the paper to highlight the benefits of product families and product platforms. The paper concludes with a discussion of potential areas of research to help bridge the gap between planning and managing families of products and designing and manufacturing them.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {3–20},
numpages = {18},
keywords = {Mass Customization, Product Family, Product Platform, Product Variety}
}

@article{10.1207/s15327051hci1204_4,
author = {Kieras, David E. and Meyer, David E.},
title = {An overview of the EPIC architecture for cognition and performance with application to human-computer interaction},
year = {1997},
issue_date = {December 1997},
publisher = {L. Erlbaum Associates Inc.},
address = {USA},
volume = {12},
number = {4},
issn = {0737-0024},
url = {https://doi.org/10.1207/s15327051hci1204_4},
doi = {10.1207/s15327051hci1204_4},
abstract = {EPIC (Executive Process-Interactive Control) is a cognitive architecture especially suited for modeling human multimodal and multiple-task performance. The EPIC architecture includes peripheral sensory-motor processors surrounding a production-rule cognitive processor and is being used to construct precise computational models for a variety of human-computer interaction situations. We briefly describe some of these models to demonstrate how EPIC clarifies basic properties of human performance and provides usefully precise accounts of performance speed.},
journal = {Hum.-Comput. Interact.},
month = dec,
pages = {391–438},
numpages = {48}
}

@inbook{10.1145/3191315.3191321,
author = {De Cat, Broes and Bogaerts, Bart and Bruynooghe, Maurice and Janssens, Gerda and Denecker, Marc},
title = {Predicate logic as a modeling language: the IDP system},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191321},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {279–323},
numpages = {45}
}

@inproceedings{10.1145/1321631.1321687,
author = {Lauenroth, Kim and Pohl, Klaus},
title = {Towards automated consistency checks of product line requirements specifications},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321687},
doi = {10.1145/1321631.1321687},
abstract = {A requirements specification for an individual software system should be consistent, i.e. free of contradictions. In product line engineering, the product line requirements specification comprises all the requirements common to all products of the product line as well as the variable requirements used to derive individual products from the product line. The set of requirements (common and all the variable ones) of a product line is typically inconsistent since variable requirements can contradict each other. This is not a problem as long as contradicting requirements are not included in a product derived from the product line. Thus, the set of requirements realized in each individual product has to be consistent. Employing techniques used in single system development to check the consistency of product line requirements will thus produce false positive results, since there can be contradiction in the product line requirements specificationIn this paper we first provide a concise definition of consistency for product line requirements specifications. Based on this definition, we define a formal framework for checking consistency of product line requirements specifications. Our framework supports consistency checks in the domain engineering process. In contrast to consistency checks in single system development, it tolerates certain types of inconsistencies caused by the variability of prod-uct line requirements.},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {373–376},
numpages = {4},
keywords = {consistency, product line, requirements engineering},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@article{10.5555/2370969.2370974,
author = {Wojna, Arkadiusz},
title = {Center-Based Indexing in Vector and Metric Spaces},
year = {2003},
issue_date = {August 2003},
publisher = {IOS Press},
address = {NLD},
volume = {56},
number = {3},
issn = {0169-2968},
abstract = {The paper addresses the problem of indexing data for k nearest neighbors (k-nn) search. Given a collection of data objects and a similarity measure the searching goal is to find quickly the k most similar objects to a given query object. We present a top-down indexing method that employs a widely used scheme of indexing algorithms. It starts with the whole set of objects at the root of an indexing tree and iteratively splits data at each level of indexing hierarchy. In the paper two different data models are considered. In the first, objects are represented by vectors from a multi-dimensional vector space. The second, more general, is based on an assumption that objects satisfy only the axioms of a metric space. We propose an iterative k-means algorithm for tree node splitting in case of a vector space and an iterative k-approximate-centers algorithm in case when only a metric space is provided. The experiments show that the iterative k-means splitting procedure accelerates significantly k-nn searching over the one-step procedure used in other indexing structures such as GNAT, SS-tree and M-tree and that the relevant representation of a tree node is an important issue for the performance of the search process. We also combine different search pruning criteria used in BST, GHT nad GNAT structures into one and show that such a combination outperforms significantly each single pruning criterion. The experiments are performed for benchmark data sets of the size up to several hundreds of thousands of objects. The indexing tree with the k-means splitting procedure and the combined search criteria is particularly effective for the largest tested data sets for which this tree accelerates searching up to several thousands times.},
journal = {Fundam. Inf.},
month = aug,
pages = {285–310},
numpages = {26}
}

@inproceedings{10.1145/3375998.3376008,
author = {Zhang, Qiqi and Fan, Bo and Song, Kexing and Huo, Hua and Hou, Wenwu},
title = {Research on Intelligent Modular Process Recombination based on Improved Genetic Algorithm},
year = {2020},
isbn = {9781450377027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375998.3376008},
doi = {10.1145/3375998.3376008},
abstract = {Aiming at the planning problem of the whole process processing route of the irreversible modular process library, a modular process intelligent reorganization method based on improved genetic algorithm was designed. Firstly, the mathematical model of product multi-process route planning problem is established, and the product attribute problem is transformed into the problem of product feasibility modular process reorganization strategy. The two-level modular process retrieval method using target feature matching and feature attribute matching is adopted to improve genetics. The evolutionary algorithm obtains the feasibility modularization process of the product to be processed, and combines the center distance theory to obtain the optimal process route of the product modular process reorganization strategy. Combined with the special application examples of aluminum/copper strip, the feasibility and convergence of the proposed method are verified by simulation analysis.},
booktitle = {Proceedings of the 2019 8th International Conference on Networks, Communication and Computing},
pages = {186–191},
numpages = {6},
keywords = {Improved genetic, Intelligent reorganization, Multi-objective optimization, Routing},
location = {Luoyang, China},
series = {ICNCC '19}
}

@article{10.1016/j.patcog.2017.10.036,
author = {Zhou, Huiling and Lam, Kin-Man},
title = {Age-invariant face recognition based on identity inference from appearance age},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.036},
doi = {10.1016/j.patcog.2017.10.036},
abstract = {An age-invariant face recognition scheme based on appearance age is proposed.Human identity and aging variables are simultaneously modeled using Probabilistic LDA.Human identity features obtained from different approaches are combined efficiently together with correlations maximized for face recognition. Face recognition across age progression remains one of the area's most challenging tasks, as the aging process affects both the shape and texture of a face. One possible solution is to apply a probabilistic model to represent a face simultaneously with its identity variable, which is stable through time, and its aging variable, which changes with time. However, as the aging process varies for different people, a person may look younger or older than another person, even though their ages are the same. Consequently, using the real age labels given by existing face datasets for age-invariant face recognition will inevitably introduce ambiguity to learning algorithms. In this paper, an identity-inference model, based on age-subspace learning from appearance-age labels, is proposed. We first model human identity and aging variables simultaneously using Probabilistic Linear Discriminant Analysis (PLDA). Then, the aging subspace is learnt independently with the appearance-age labels, and the identity subspace is then determined iteratively with the Expectation-Maximization (EM) algorithm. We found that the learned aging subspace is insensitive to the training face images used, and is independent of the identity model. Consequently, the recognition of aging faces becomes simpler as identity inference no longer needs to consider age labels. Furthermore, in our algorithm, different identity features learnt from the identity model are further combined using Canonical Correlation Analysis (CCA), where their correlations are maximized for face recognition. A thorough experimental analysis of face recognition is performed on three public domain face-aging datasets: FGNET, MORPH, and CACD. Experiment results show that the proposed framework can achieve a comparable, or even better, performance against other state-of-the-art methods, especially when the age range is large.},
journal = {Pattern Recogn.},
month = apr,
pages = {191–202},
numpages = {12},
keywords = {Age-invariant, Canonical correlation analysis, Face recognition, Face verification, Identity inference, Probabilistic LDA}
}

@article{10.1007/s10817-004-2725-6,
author = {Zimmer, J\"{u}rgen and Melis, Erica},
title = {Constraint Solving for Proof Planning},
year = {2004},
issue_date = {July      2004},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {1},
issn = {0168-7433},
url = {https://doi.org/10.1007/s10817-004-2725-6},
doi = {10.1007/s10817-004-2725-6},
abstract = {Proof planning is an application of AI planning to theorem proving that employs plan operators that encapsulate mathematical proof techniques. Many proofs require the instantiation of variables; that is, mathematical objects with certain properties have to be constructed. This is particularly difficult for automated theorem provers if the instantiations have to satisfy requirements specific for a mathematical theory, for example, for finite sets or for real numbers, because in this case unification is insufficient for finding a proper instantiation. Often, constraint solving can be employed for this task. We describe a framework for the integration of constraint solving into proof planning that combines proof planners and stand-alone constraint solvers. Proof planning has some peculiar requirements that are not met by any off-the-shelf constraint-solving system. Therefore, we extended an existing propagation-based constraint solver in a generic way. This approach generalizes previous work on tackling the problem. It provides a more principled way and employs existing AI technology.},
journal = {J. Autom. Reason.},
month = jul,
pages = {51–88},
numpages = {38},
keywords = {automated reasoning, constraint satisfaction, proof plans}
}

@inproceedings{10.5555/2893873.2893883,
author = {Hasan, Md. Kamrul and Pal, Christopher},
title = {Experiments on visual information extraction with the faces of Wikipedia},
year = {2014},
publisher = {AAAI Press},
abstract = {We present a series of visual information extraction experiments using the Faces of Wikipedia database - a new resource that we release into the public domain for both recognition and extraction research containing over 50,000 identities and 60,000 disambiguated images of faces. We compare different techniques for automatically extracting the faces corresponding to the subject of a Wikipedia biography within the images appearing on the page. Our top performing approach is based on probabilistic graphical models and uses the text of Wikipedia pages, similarities of faces as well as various other features of the document, meta-data and image files. Our method resolves the problem jointly for all detected faces on a page. While our experiments focus on extracting faces from Wikipedia biographies, our approach is easily adapted to other types of documents and multiple documents. We focus on Wikipedia because the content is a Creative Commons resource and we provide our database to the community including registered faces, hand labeled and automated disambiguations, processed captions, meta data and evaluation protocols. Our best probabilistic extraction pipeline yields an expected average accuracy of 77% compared to image only and text only baselines which yield 63% and 66% respectively.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {51–58},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1109/ASEW.2008.4686323,
author = {Brcina, Robert and Riebisch, Matthias},
title = {Architecting for evolvability by means of traceability and features},
year = {2008},
isbn = {9781424427765},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASEW.2008.4686323},
doi = {10.1109/ASEW.2008.4686323},
abstract = {The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systems' evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {III–72–III–81},
location = {L'Aquila, Italy},
series = {ASE'08}
}

@inproceedings{10.5555/3061053.3061115,
author = {Zhang, Dingwen and Meng, Deyu and Zhao, Long and Han, Junwei},
title = {Bridging saliency detection to weakly supervised object detection based on self-paced curriculum learning},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Weakly-supervised object detection (WOD) is a challenging problems in computer vision. The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors, given only the training images with weak image-level labels. Intuitively, by simulating the selective attention mechanism of human visual system, saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD. However, the way to adopt saliency detection in WOD is not trivial since the detected saliency region might be possibly highly ambiguous in complex cases. To this end, this paper first comprehensively analyzes the challenges in applying saliency detection to WOD. Then, we make one of the earliest efforts to bridge saliency detection to WOD via the self-paced curriculum learning, which can guide the learning procedure to gradually achieve faithful knowledge of multi-class objects from easy to hard. The experimental results demonstrate that the proposed approach can successfully bridge saliency detection and WOD tasks and achieve the state-of-the-art object detection results under the weak supervision.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {3538–3544},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.1016/j.asoc.2017.11.051,
author = {Zeng, Zhi-zhong and Yu, Xin-guo and He, Kun and Fu, Zhang-hua},
title = {Adaptive Tabu search and variable neighborhood descent for packing unequal circles into a square},
year = {2018},
issue_date = {Apr 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.11.051},
doi = {10.1016/j.asoc.2017.11.051},
journal = {Appl. Soft Comput.},
month = apr,
pages = {196–213},
numpages = {18},
keywords = {Metaheuristic, Tabu search, Variable neighborhood search, Cutting and packing}
}

@inproceedings{10.1145/1754288.1754295,
author = {Indukuri, Kishore Varma and Krishna, P. Radha},
title = {Mining e-contract documents to classify clauses},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754295},
doi = {10.1145/1754288.1754295},
abstract = {E-contracts begin as legal documents and end up as processes that help organizations abide by legal rules while fulfilling contract terms. As contracts are complex, their deployment is predominantly established and fulfilled with significant human involvement. One of the key difficulties with any kind of contract processing is the legal ambiguity, which makes it difficult to address any violation of the contract terms. Thus, there is a need to track clauses for the contract activities under execution and violation of clauses. This necessitates deriving clause patterns from e-contract documents and map to their respective activities for further monitoring and fulfillment of e-contracts during their enactment. In this paper, we present a classification approach to extract clause patterns from e-contract documents. This is a challenging task as activities and clauses are mostly derived from both legal and business process driven contract knowledge.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {7},
numpages = {5},
keywords = {data mining, e-contracts, text analytics},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1134285.1134336,
author = {Anvik, John and Hiew, Lyndon and Murphy, Gail C.},
title = {Who should fix this bug?},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134336},
doi = {10.1145/1134285.1134336},
abstract = {Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {361–370},
numpages = {10},
keywords = {bug report assignment, bug triage, issue tracking, machine learning, problem tracking},
location = {Shanghai, China},
series = {ICSE '06}
}

@article{10.1016/j.asoc.2020.106121,
author = {Leelathakul, Nutthanon and Rimcharoen, Sunisa},
title = {Generating Kranok patterns with an interactive evolutionary algorithm},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106121},
doi = {10.1016/j.asoc.2020.106121},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {17},
keywords = {Interactive evolutionary algorithm, Thai drawing, Kranok pattern, B\'{e}zier curve, Generated art}
}

@inproceedings{10.1007/978-3-030-58580-8_28,
author = {Beeching, Edward and Dibangoye, Jilles and Simonin, Olivier and Wolf, Christian},
title = {Learning to Plan with Uncertain Topological Maps},
year = {2020},
isbn = {978-3-030-58579-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58580-8_28},
doi = {10.1007/978-3-030-58580-8_28},
abstract = {We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy. Our main contribution is a data driven learning based approach for planning under uncertainty in&nbsp;topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure. Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map. Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm. By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III},
pages = {473–490},
numpages = {18},
keywords = {Visual navigation, Topological maps, Graph neural networks},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10458-013-9241-1,
author = {Franks, Henry and Griffiths, Nathan and Anand, Sarabjot Singh},
title = {Learning agent influence in MAS with complex social networks},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {5},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-013-9241-1},
doi = {10.1007/s10458-013-9241-1},
abstract = {In complex open multi-agent systems (MAS), where there is no centralised control and individuals have equal authority, ensuring cooperative and coordinated behaviour is challenging. Norms and conventions are useful means of supporting cooperation in an emergent decentralised manner, however it takes time for effective norms and conventions to emerge. Identifying influential individuals enables the targeted seeding of desirable norms and conventions, which can reduce the establishment time and increase efficacy. Existing research is limited with respect to considering (i) how to identify influential agents, (ii) the extent to which network location imbues influence on an agent, and (iii) the extent to which different network structures affect influence. In this paper, we propose a methodology for learning a model for predicting the network value of an agent, in terms of the extent to which it can influence the rest of the population. Applying our methodology, we show that exploiting knowledge of the network structure can significantly increase the ability of individuals to influence which convention emerges. We evaluate our methodology in the context of two agent-interaction models, namely, the language coordination domain used by Salazar et al. (AI Communications 23(4): 357---372, 2010 ) and a coordination game of the form used by Sen and Airiau (in: Proceedings of the 20th International Joint Conference on Artificial Intelligence, 2007 ) with heterogeneous agent learning mechanisms, and on a variety of synthetic and real-world networks. We further show that (i) the models resulting from our methodology are effective in predicting influential network locations, (ii) there are very few locations that can be classified as influential in typical networks, (iii) four single metrics are robustly indicative of influence across a range of network structures, and (iv) our methodology learns which single metric or combined measure is the best predictor of influence in a given network.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = sep,
pages = {836–866},
numpages = {31},
keywords = {Conventions, Influence, Learning, Norms, Social networks}
}

@article{10.4018/JECO.2017040104,
author = {El Faquih, Loubna and Fredj, Mounia},
title = {Ontology-Based Framework for Quality in Configurable Process Models},
year = {2017},
issue_date = {April 2017},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1539-2937},
url = {https://doi.org/10.4018/JECO.2017040104},
doi = {10.4018/JECO.2017040104},
abstract = {In recent years, business process modeling has increasingly drawn the attention of enterprises. As a result of the wide use of business processes, redundancy problems have arisen and researchers introduced the variability management, in order to enhance the business process reuse. The most approach used in this context is the Configurable Process Model solution, which consists in representing the variable and the fixed parts together in a unique model. Due to the increasing number of variants, the configurable models become complex and incomprehensible, and their quality is therefore impacted. Most of research work is limited to the syntactic quality of process variants. The approach presented in this paper aims at providing a novel method towards syntactic verification and semantic validation of configurable process models based on ontology languages. We define validation rules for assessing the quality of configurable process models. An example in the e-healthcare domain illustrates the main steps of our approach.},
journal = {J. Electron. Commer. Organ.},
month = apr,
pages = {48–60},
numpages = {13},
keywords = {Configurable Process Models Quality, Ontology, Semantic Validation, Syntactic Verification, Variant Rich BPMN}
}

@inproceedings{10.5555/3060621.3060791,
author = {Niveau, Alexandre and Zanuttini, Bruno},
title = {Efficient representations for the modal logic S5},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {We investigate efficient representations of subjective formulas in the modal logic of knowledge, S5, and more generally of sets of sets of propositional assignments. One motivation for this study is contingent planning, for which many approaches use operations on such formulas, and can clearly take advantage of efficient representations. We study the language S5-DNF introduced by Bienvenu  et al. , and a natural variant of it that uses Binary Decision Diagrams at the propositional level. We also introduce an alternative language, called Epistemic Splitting Diagrams, which provides more compact representations. We compare all three languages from the complexity-theoretic viewpoint of knowledge compilation and also through experiments. Our work sheds light on the pros and cons of each representation in both theory and practice.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1223–1229},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.5555/2025896.2025909,
author = {Przyby\l{}ek, Adam},
title = {Systems evolution and software reuse in object-oriented programming and aspect-oriented programming},
year = {2011},
isbn = {9783642219511},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Every new programming technique makes claims that software engineers want to hear. Such is the case with aspect-oriented programming (AOP). This paper describes a quasi-controlled experiment which compares the evolution of two functionally equivalent programs, developed in two different paradigms. The aim of the study is to explore the claims that software developed with aspect-oriented languages is easier to maintain and reuse than this developed with object-oriented languages. We have found no evidence to support these claims.},
booktitle = {Proceedings of the 49th International Conference on Objects, Models, Components, Patterns},
pages = {163–178},
numpages = {16},
keywords = {AOP, maintainability, reusability, separation of concerns},
location = {Zurich, Switzerland},
series = {TOOLS'11}
}

@inproceedings{10.1007/978-3-030-89811-3_20,
author = {Saniei, Rana},
title = {Challenges in the Implementation of Privacy Enhancing Semantic Technologies (PESTs) Supporting GDPR},
year = {2020},
isbn = {978-3-030-89810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89811-3_20},
doi = {10.1007/978-3-030-89811-3_20},
abstract = {The EU General Data Protection Regulation (GDPR) imposes different requirements for data controllers collecting personal data to protect individuals’ privacy. This fact triggered many studies and projects to investigate Privacy Enhancing Technologies (PETs) for the fulfillment of the compliance requirements. In this paper, after reviewing some of the current challenges and gaps in GDPR compliance, we argue the use of Semantic Technologies in PETs in the form of an Intelligent Compliance Agent (ICA) to support data controllers in carrying out a Data Protection Impact Assessment (DPIA). Models and ontologies representing entities involved in the DPIA process can help data controllers determine the risk of their processing activities. Additionally, an inference engine, equipped with a knowledge base of DPIA-related obligations, can effectively assist data controllers in taking specific actions when a legal fact is triggered based on met conditions.},
booktitle = {AI Approaches to the Complexity of Legal Systems XI-XII: AICOL International Workshops 2018 and 2020: AICOL-XI@JURIX 2018, AICOL-XII@JURIX 2020, XAILA@JURIX 2020, Revised Selected Papers},
pages = {283–297},
numpages = {15},
keywords = {Compliance, Semantic web, Rule-based reasoning, Data Protection Impact Assessment, Privacy Enhancing Technologies}
}

@inproceedings{10.1007/978-3-030-97457-2_7,
author = {Ferrando, Angelo and Papacchini, Fabio},
title = {StreamB: A Declarative Language for Automatically Processing Data Streams in Abstract Environments for Agent Platforms},
year = {2021},
isbn = {978-3-030-97456-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-97457-2_7},
doi = {10.1007/978-3-030-97457-2_7},
abstract = {To apply BDI agents to real-world scenarios, the reality-gap, between the low-level data (perceptions) and their high-level representation (beliefs), must be bridged. This is usually achieved by a manual mapping. There are two problems with this solution: (i) if the environment changes, the mapping has to be changed as well (by the developer); (ii) part of the mapping might end up being implemented at the agent level increasing the code complexity and reducing its generality. In this paper, we present a general approach to automate the mapping between low-level data and high-level beliefs through the use of transducers. These transducers gather information from the environment and map them to high-level beliefs according to formal temporal specifications. We present our technique and we show its applicability through a case study involving the remote inspection of a nuclear plant.},
booktitle = {Engineering Multi-Agent Systems: 9th International Workshop, EMAS 2021, Virtual Event, May 3–4, 2021, Revised Selected Papers},
pages = {114–136},
numpages = {23},
keywords = {Agent programming, Stream processing, BDI model, Abstract environment}
}

@article{10.1145/2180921.2180923,
author = {Anwikar, Vallabh and Naik, Ravindra and Contractor, Adnan and Makkapati, Hemanth},
title = {Domain-driven technique for functionality identification in source code},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2180921.2180923},
doi = {10.1145/2180921.2180923},
abstract = {While migrating existing software systems to Software Product Lines, finding out the functionalities in the software is critical. For maintenance activities like deleting or changing existing features, or adding new similar features, identifying and extracting functionalities from the software is significant. This paper describes a technique for creating mapping between the source code and functionalities implemented by it while exploiting the domain knowledge. The technique is based on the notion of function variables that are used by developers for expressing functionality in the source code. By tracking the known values of the function variables and evaluating the conditions that use them, the mapping is identified. Our technique makes use of static data ow analysis and partial evaluation, and is designed with automation perspective. After applying to few samples representing real-life code structure and programming practices, the technique identified precise mapping of the detailed program elements to functions},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–8},
numpages = {8},
keywords = {function variables, functionality identification, partial evaluation}
}

@article{10.1016/j.knosys.2016.08.027,
author = {Sang, Yongsheng and Lv, Jiancheng and Qu, Hong and Yi, Zhang},
title = {Shortest path computation using pulse-coupled neural networks with restricted autowave},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.08.027},
doi = {10.1016/j.knosys.2016.08.027},
abstract = {Finding shortest paths is an important problem in transportation and communication networks. This paper develops a Pulse-Coupled Neural Network (PCNN) model to efficiently compute a single-pair shortest path. Unlike most of the existing PCNN models, the proposed model is endowed with a special mechanism, called on-forward/off-backward; if a neuron fires, its neighboring neurons in a certain forward region will be excited, whereas the neurons in a backward region will be inhibited. As a result, the model can produce a restricted autowave that propagates at different speeds corresponding to different directions, which is different from the completely nondeterministic PCNN models. Compared with some traditional methods, the proposed PCNN model significantly reduces the computational cost of searching for the shortest path. Experimental results further confirmed the efficiency and effectiveness of the proposed model.},
journal = {Know.-Based Syst.},
month = dec,
pages = {1–11},
numpages = {11},
keywords = {On-forward/off-backward, Pulse-coupled neural networks, Restricted autowave, Shortest path}
}

@article{10.1016/j.csl.2012.01.002,
author = {Azmi, Aqil M. and Al-Thanyyan, Suha},
title = {A text summarizer for Arabic},
year = {2012},
issue_date = {August, 2012},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {26},
number = {4},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.01.002},
doi = {10.1016/j.csl.2012.01.002},
abstract = {Automatic text summarization is an essential tool in this era of information overloading. In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary. It is a direct system where no machine learning is involved. We use a two pass algorithm where in pass one, we produce a primary summary using Rhetorical Structure Theory (RST); this is followed by the second pass where we assign a score to each of the sentences in the primary summary. These scores will help us in generating the final summary. For the final output, sentences are selected with an objective of maximizing the overall score of the summary whose size should not exceed the user selected limit. We used Rouge to evaluate our system generated summaries of various lengths against those done by a (human) news editorial professional. Experiments on sample texts show our system to outperform some of the existing Arabic summarization systems including those that require machine learning.},
journal = {Comput. Speech Lang.},
month = aug,
pages = {260–273},
numpages = {14},
keywords = {0/1-Knapsack, Arabic NLP, Automatic text summarization, Rhetorical Structure Theory, Rouge}
}

@article{10.1155/2020/8826568,
author = {Asare, Sarpong Kwadwo and You, Fei and Nartey, Obed Tettey and Rakhshan, Vahid},
title = {A Semisupervised Learning Scheme with Self-Paced Learning for Classifying Breast Cancer Histopathological Images},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1687-5265},
url = {https://doi.org/10.1155/2020/8826568},
doi = {10.1155/2020/8826568},
abstract = {The unavailability of large amounts of well-labeled data poses a significant challenge in many medical imaging tasks. Even in the likelihood of having access to sufficient data, the process of accurately labeling the data is an arduous and time-consuming one, requiring expertise skills. Again, the issue of unbalanced data further compounds the abovementioned problems and presents a considerable challenge for many machine learning algorithms. In lieu of this, the ability to develop algorithms that can exploit large amounts of unlabeled data together with a small amount of labeled data, while demonstrating robustness to data imbalance, can offer promising prospects in building highly efficient classifiers. This work proposes a semisupervised learning method that integrates self-training and self-paced learning to generate and select pseudolabeled samples for classifying breast cancer histopathological images. A novel pseudolabel generation and selection algorithm is introduced in the learning scheme to generate and select highly confident pseudolabeled samples from both well-represented classes to less-represented classes. Such a learning approach improves the performance by jointly learning a model and optimizing the generation of pseudolabels on unlabeled-target data to augment the training data and retraining the model with the generated labels. A class balancing framework that normalizes the class-wise confidence scores is also proposed to prevent the model from ignoring samples from less represented classes (hard-to-learn samples), hence effectively handling the issue of data imbalance. Extensive experimental evaluation of the proposed method on the BreakHis dataset demonstrates the effectiveness of the proposed method.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {16}
}

@article{10.1016/j.jss.2017.01.031,
author = {Lucas, Edson M. and Oliveira, Toacy C. and Farias, Kleinner and Alencar, Paulo S.C.},
title = {CollabRDL},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.031},
doi = {10.1016/j.jss.2017.01.031},
abstract = {Extends the Reuse Description Language (RDL) to address collaborative reuse processesIncludes three new commands in RDL, including ROLE, PARALLEL and DOPARALLELCollabRDL can represent parallelism, synchronization, multiple-choice and roleCollabRDL is capable of representing critical workflow patterns Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort.},
journal = {J. Syst. Softw.},
month = sep,
pages = {505–527},
numpages = {23},
keywords = {Collaboration, Framework, Language, Reuse process, Software reuse}
}

@article{10.1017/S0890060498124058,
author = {Attardi, Giuseppe and Cisternino, Antonio and Simi, Maria},
title = {Web-based configuration assistants},
year = {1998},
issue_date = {September 1998},
publisher = {Cambridge University Press},
address = {USA},
volume = {12},
number = {4},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060498124058},
doi = {10.1017/S0890060498124058},
abstract = {Configuration assistants are tools for guiding the final user in simple configuration tasks, such as product assembling and customization or study plans generation. For their wide availability, web-based configuration assistants are valuable in fields such as electronic commerce and information services. We describe a general approach for building web-based configuration assistants: from a high-level description of the configuration constraints and of the basic items, given in a declarative language, the hypertext files for user guidance and the Java code for constraint checking are generated. We claim that the general approach of process-oriented configuration, where the user is guided through the configuration process by an explanatory hypertext, as opposed to product-oriented configuration, where one starts from a high-level description of the product of the configuration, is better suited for many application domains.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = sep,
pages = {321–331},
numpages = {11},
keywords = {Configuration Assistants, Electronic Commerce, Process-oriented Configuration}
}

@article{10.1109/TFUZZ.2006.876743,
author = {Chang, Ping-Teng and Hung, Kuo-Chen},
title = {/spl alpha/-cut fuzzy arithmetic: simplifying rules and a fuzzy function optimization with a decision variable},
year = {2006},
issue_date = {August 2006},
publisher = {IEEE Press},
volume = {14},
number = {4},
issn = {1063-6706},
url = {https://doi.org/10.1109/TFUZZ.2006.876743},
doi = {10.1109/TFUZZ.2006.876743},
abstract = {The problems of alpha-cut fuzzy arithmetic have been shown, like in interval arithmetic, that distinct states of fuzzy parameters (or fuzzy variable values) may be chosen and produce an overestimated fuzziness. Meanwhile, local extrema of a function may exist inside the support of fuzzy parameters and cause an underestimation of fuzziness and an illegal fuzzy number's result. Previous approaches to overcoming these problems have appeared in literature. Yet, the computational burden of these approaches became even heavier. Thus, this paper is based on the vertex method in literature and extensively proposes newly devised rules observed greatly useful for simplifying the vertex method. These rules are devised through a function partitioned into subfunctions, distinguishing the types of fuzzy parameter/variable occurrences, and types of subfunctions or functions with the various observations. The improved efficiency has been found able to significantly reduce the combination (vertex) test of the vertex method for the fuzzy parameters' alpha-cut endpoints possibly to only a few fuzzy parameters' endpoint combinations. Also as related, a procedure for the fuzzy optimization of fuzzy functions with a fuzzy blurred argument (a single variable) is examined with the vertex method as well. A proper and useful preliminary algorithm is proposed. Numerical examples with results are provided},
journal = {Trans. Fuz Sys.},
month = aug,
pages = {496–510},
numpages = {15},
keywords = {fuzzy arithmetic, fuzzy function optimization, fuzzy number, vertex method}
}

