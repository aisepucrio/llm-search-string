@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {software architecture, variability management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@phdthesis{10.5555/AAI28772972,
author = {Liu, Wuji and T., Wang, Jason and Hui, Zhao, and B., Roy, Senjuti and G, Liu, Qing},
advisor = {Q, Wu, Chase},
title = {On Resource-Efficiency and Performance Optimization in Big Data Computing and Networking Using Machine Learning},
year = {2021},
isbn = {9798780626268},
publisher = {New Jersey Institute of Technology},
address = {USA},
abstract = {Due to the rapid transition from traditional experiment-based approaches to large-scale, computational intensive simulations, next-generation scientific applications typically involve complex numerical modeling and extreme-scale simulations. Such model-based simulations oftentimes generate colossal amounts of data, which must be transferred over high-performance network (HPN) infrastructures to remote sites and analyzed against experimental or observation data on high-performance computing (HPC) facility. Optimizing the performance of both data transfer in HPN and simulation-based model development on HPC is critical to enabling and accelerating knowledge discovery and scientific innovation. However, such processes generally involve an enormous set of attributes including domain-specific model parameters, network transport properties, and computing system configurations. The vast space of model parameters, the sheer volume of generated data, the limited amount of allocatable bandwidths, and the complex settings of computing systems make it practically infeasible for domain experts to manually deploy and optimize big data transfer and computing solutions in next-generation scientific applications.The research in this dissertation identifies such attributes in networks, systems, and models, conducts in-depth exploratory analysis of their impacts on data transfer throughput, computing efficiency, and modeling accuracy, and designs and customizes various machine learning techniques to optimize the performance of big data transfer in HPN, big data computing on HPC, and model development through large-scale simulations. Particularly, unobservable latent factors such as competing loads on end hosts are investigated and an algorithm named Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is employed to eliminate their negative impacts on performance prediction using machine learning models such as Support Vector Regression (SVR). Based on such analysis results, a customized, domain-specific loss function is employed within machine learning models such as Stochastic Gradient Descent Regression for throughput prediction to advise bandwidth allocation in HPN. A Bayesian Optimization (BO)-based online computational steering framework is also designed to facilitate the process of scientific simulations and improve the accuracy of model development. The solution proposed in this dissertation provides an additional layer of intelligence in big data transfer and computing, and the resulted machine learning techniques help guide strategic provisioning of high-performance networking and computing resources to maximize the performance of next-generation scientific applications.},
note = {AAI28772972}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.2478/cait-2020-0056,
author = {Astsatryan, Hrachya and Kocharyan, Aram and Hagimont, Daniel and Lalayan, Arthur},
title = {Performance Optimization System for Hadoop and Spark Frameworks},
year = {2020},
issue_date = {Dec 2020},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {20},
number = {6},
issn = {1314-4081},
url = {https://doi.org/10.2478/cait-2020-0056},
doi = {10.2478/cait-2020-0056},
abstract = {The optimization of large-scale data sets depends on the technologies and methods used. The MapReduce model, implemented on Apache Hadoop or Spark, allows splitting large data sets into a set of blocks distributed on several machines. Data compression reduces data size and transfer time between disks and memory but requires additional processing. Therefore, finding an optimal tradeoff is a challenge, as a high compression factor may underload Input/Output but overload the processor. The paper aims to present a system enabling the selection of the compression tools and tuning the compression factor to reach the best performance in Apache Hadoop and Spark infrastructures based on simulation analyzes.},
journal = {Cybern. Inf. Technol.},
month = dec,
pages = {5–17},
numpages = {13},
keywords = {Hadoop, Spark, data compression, CPU/IO tradeoff, performance optimization}
}

@article{10.1007/s00521-019-04416-1,
author = {Ali, Rashid and Nauman, Ali and Zikria, Yousaf Bin and Kim, Byung-Seo and Kim, Sung Won},
title = {Performance optimization of QoS-supported dense WLANs using machine-learning-enabled enhanced distributed channel access (MEDCA) mechanism},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04416-1},
doi = {10.1007/s00521-019-04416-1},
abstract = {Quality of service (QoS) implementation in a wireless local area network (WLAN) enables the prediction of network performance and utilization of effective bandwidth for multimedia applications. In QoS-supported WLAN, enhanced distributed channel access (EDCA) adjusts back-off parameters to implement priority-based channel access at the medium access control (MAC) layer. Although conventional QoS-supported EDCA in WLANs can provide a certain degree of QoS guarantee, the performance of best effort data (low-priority) traffic is sacrificed owing to the blind use of a binary exponential back-off (BEB) mechanism for collision avoidance among WLAN stations (STAs). In EDCA, the BEB mechanism exponentially increases the contention window (CW[AC]) for any specific priority access category (AC) when collision occurs and resets it to its initial size after successful data transmission.
This increase and reset of CW[AC] is performed regardless of the network density inference, i.e., a scarce WLAN does not require an unnecessary exponential increase in CW[AC]. Similarly, a dense WLAN causes more collisions if CW[AC] is reset to its initial minimum size. Machine-learning algorithms can scrutinize an STA’s experience for WLAN inference. Therefore, in this study, we propose a machine-learning-enabled EDCA (MEDCA) mechanism for QoS-supported MAC layer channel access in dense WLANs. This mechanism utilizes a Q-learning algorithm, which is one of the prevailing models of machine learning, to infer the network density and adjust its back-off CW[AC] accordingly. Simulation results show that MEDCA performs better as compared to the conventional EDCA mechanism in QoS-supported dense WLANs.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {13107–13115},
numpages = {9},
keywords = {QoS-supported WLANs, MAC layer channel access, Machine learning, Dense WLANs, EDCA}
}

@phdthesis{10.5555/AAI28410383,
author = {Tian, Ruiqin and Weizhen, Mao, and Evgenia, Smirni, and Pieter, Peers, and Gokcen, Kestor,},
advisor = {Bin, Ren,},
title = {Performance Optimization with an Integrated View of Compiler and Application Knowledge},
year = {2021},
isbn = {9798516911590},
publisher = {The College of William and Mary},
abstract = {Compiler optimization is a long-standing research field that enhances program performance with a set of rigorous code analyses and transformations. Traditional compiler optimization focuses on general programs or program structures without considering too much high-level application operations or data structure knowledge. In this thesis, we claim that an integrated view of the application and compiler is helpful to further improve program performance. Particularly, we study integrated optimization opportunities for three kinds of applications: irregular tree-based query processing systems such as B+ tree, security enhancement such as buffer overflow protection, and tensor/matrix-based linear algebra computation.The performance of B+ tree query processing is important for many applications, such as file systems and databases. Latch-free B+ tree query processing is efficient since the queries are processed in batches without locks. To avoid long latency, the batch size can not be very large. However, modern processors provide opportunities to process larger batches parallel with acceptable latency. From studying real-world data, we find that there are many redundant and unnecessary queries especially when the real-world data is highly skewed. We develop a query sequence transformation framework Qtrans to reduce the redundancies in queries by applying classic dataflow analysis to queries. To further confirm the effectiveness, we integrate Qtrans into an existing BSP-based B+ tree query processing system, PALM tree. The evaluations show that the throughput can be improved up to 16X.Heap overflows are still the most common vulnerabilities in C/C++ programs. Common approaches incur high overhead since it checks every memory access. By analyzing dozens of bugs, we find that all heap overflows are related to arrays. We only need to check array-related memory accesses. We propose Prober to efficiently detect and prevent heap overflows. It contains Prober-Static to identify the array-related allocations and Prober-Dynamic to protect objects at runtime. In this thesis, our contributions lie on the Prober-Static side. The key challenge is to correctly identify the array-related allocations. We propose a hybrid method. Some objects can be identified as array-related (or not) by static analysis. For the remaining ones, we instrument the basic allocation type size statically and then determine the real allocation size at runtime. The evaluations show Prober-Static is effective.Tensor algebra is widely used in many applications, such as machine learning and data analytics. Tensors representing real-world data are usually large and sparse. There are many sparse tensor storage formats, and the kernels are different with varied formats. These different kernels make performance optimization for sparse tensor algebra challenging. We propose a tensor algebra domain-specific language and a compiler to automatically generate kernels for sparse tensor algebra computations, called SPACe. This compiler supports a wide range of sparse tensor formats. To further improve the performance, we integrate the data reordering into SPACe to improve data locality. The evaluations show that the code generated by SPACe outperforms state-of-the-art sparse tensor algebra compilers.},
note = {AAI28410383}
}

@article{10.1016/j.compbiomed.2021.105015,
author = {Zhao, Songwei and Wang, Pengjun and Heidari, Ali Asghar and Chen, Huiling and He, Wenming and Xu, Suling},
title = {Performance optimization of salp swarm algorithm for multi-threshold image segmentation: Comprehensive study of breast cancer microscopy},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.105015},
doi = {10.1016/j.compbiomed.2021.105015},
journal = {Comput. Biol. Med.},
month = dec,
numpages = {33},
keywords = {Meta-heuristic algorithms, Salp swarm algorithm, Multi-threshold image segmentation, Kapur's entropy, Breast cancer, Performance optimization}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.knosys.2020.105479,
author = {Chen, Yiyang and Zhou, Yingwei},
title = {Machine learning based decision making for time varying systems: Parameter estimation and performance optimization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.105479},
doi = {10.1016/j.knosys.2020.105479},
journal = {Know.-Based Syst.},
month = feb,
numpages = {9},
keywords = {Machine learning, Model predictive control, Time varying system}
}

@article{10.1007/s11390-020-9414-8,
author = {Huang, Lan and Li, Da-Lin and Wang, Kang-Ping and Gao, Teng and Tavares, Adriano},
title = {A Survey on Performance Optimization of High-Level Synthesis Tools},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {3},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9414-8},
doi = {10.1007/s11390-020-9414-8},
abstract = {Field-programmable gate arrays (FPGAs) have recently evolved as a valuable component of the heterogeneous computing. The register transfer level (RTL) design flows demand the designers to be experienced in hardware, resulting in a possible failure of time-to-market. High-level synthesis (HLS) permits designers to work at a higher level of abstraction through synthesizing high-level language programs to RTL descriptions. This provides a promising approach to solve these problems. However, the performance of HLS tools still has limitations. For example, designers remain exposed to various aspects of hardware design, development cycles are still time consuming, and the quality of results (QoR) of HLS tools is far behind that of RTL flows. In this paper, we survey the literature published since 2014 focusing on the performance optimization of HLS tools. Compared with previous work, we extend the scope of the performance of HLS tools, and present a set of three-level evaluation criteria, covering from ease of use of the HLS tools to promotion on specific metrics of QoR. We also propose performance evaluation equations for describing the relation between the performance optimization and the QoR. We find that it needs more efforts on the ease of use for efficient HLS tools. We suggest that it is better to draw an analogy between the HLS development process and the embedded system design process, and to provide more elastic HLS methodology which integrates FPGAs virtual machines.},
journal = {J. Comput. Sci. Technol.},
month = may,
pages = {697–720},
numpages = {24},
keywords = {evaluation criterion, field-programmable gate array (FPGA), high-level synthesis (HLS), performance optimization, quality of results (QoR)}
}

@article{10.1016/j.comnet.2021.108585,
author = {Sharma, Abhishek and Garg, Amik and Sharma, Sanjay Kumar and Sachan, Vibhav Kumar and Kumar, Parvin},
title = {Performance optimization for UWB communication network under IEEE 802.15.4a channel conditions},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108585},
doi = {10.1016/j.comnet.2021.108585},
journal = {Comput. Netw.},
month = dec,
numpages = {12},
keywords = {IEEE 802.15.4a, MAN, Time hopping BPM}
}

@phdthesis{10.5555/AAI28319278,
author = {AlQuwaiee, Haifa and Ali, Mili, and Zhi, Wei, and Jing, Li, and Dantong, Yu,},
advisor = {Chase, Wu,},
title = {On Performance Optimization and Prediction of Parallel Computing Frameworks in Big Data Systems},
year = {2021},
isbn = {9798780626510},
publisher = {New Jersey Institute of Technology},
address = {USA},
abstract = {A wide spectrum of big data applications in science, engineering, and industry generate large datasets, which must be managed and processed in a timely and reliable manner for knowledge discovery. These tasks are now commonly executed in big data computing systems exemplified by Hadoop based on parallel processing and distributed storage and management. For example, many companies and research institutions have developed and deployed big data systems on top of NoSQL databases such as HBase and MongoDB, and parallel computing frameworks such as MapReduce and Spark, to ensure timely data analyses and efficient result delivery for decision making and business intelligence. This dissertation investigates and addresses two main challenges in such big data systems: i) performance optimization for distributed information composition, and ii) performance modeling and prediction of big data applications. To address the first challenge, analytical cost models are constructed to formulate a Distributed Information Composition problem in Big Data Systems, referred to as DIC-BDS, to aggregate multiple datasets stored as data blocks in Hadoop Distributed File System (HDFS) using a composition operator of specific complexity to produce one final output. DIC-BDS is rigorously proved to be NP-complete, and two heuristic algorithms are proposed. Extensive experiments are conducted with various composition operators of commonly considered degrees of complexity, and experimental results illustrate the performance superiority of the proposed solutions over existing methods. To address the second challenge, a class of regression-based machine learning models is proposed to predict the execution performance of Spark-HBase applications in Hadoop. Accurate performance modeling and prediction are critical to optimizing application performance through strategic resource allocation with suitable parameter settings and also to providing an effective recommendation of optimal system configurations to end users. An in-depth exploratory analysis is conducted to identify an exhaustive set of system parameters across multiple technology layers including Spark and HBase, and examine their effects on the execution time of Spark-HBase applications. Based on these analysis results, a subset of critical parameters is selected to develop a performance predictor using regression-based machine learning. Experimental results show that the resulted predictor achieves high accuracy with different algorithms in comparison.},
note = {AAI28319278}
}

@inproceedings{10.1145/3372799.3394370,
author = {Wolff, Willy and Porter, Barry},
title = {Performance Optimization on big.LITTLE Architectures: A Memory-latency Aware Approach},
year = {2020},
isbn = {9781450370943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372799.3394370},
doi = {10.1145/3372799.3394370},
abstract = {The energy demands of modern mobile devices have driven a trend towards heterogeneous multi-core systems which include various types of core tuned for performance or energy efficiency, offering a rich optimization space for software. On such systems, data coherency between cores is automatically ensured by an interconnect between processors. On some chip designs the performance of this interconnect, and by extension of the entire CPU cluster, is highly dependent on the software's memory access characteristics and on the set of frequencies of each CPU core. Existing frequency scaling mechanisms in operating systems use a simple load-based heuristic to tune CPU frequencies, and so fail to achieve a holistically good configuration across such diverse clusters. We propose a new adaptive governor to solve this problem, which uses a simple trained hardware model of cache interconnect characteristics, along with real-time hardware monitors, to continually adjust core frequencies to maximize system performance. We evaluate our governor on the Exynos5422 SoC, as used in the Samsung Galaxy S5, across a range of standard benchmarks. This shows that our approach achieves a speedup of up to 40%, and a 70% energy saving, including a 30% speedup in common mobile applications such as video decoding and web browsing.},
booktitle = {The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {51–61},
numpages = {11},
keywords = {asymmetric multi-processing, dvfs governor, operating systems, snooping latency},
location = {London, United Kingdom},
series = {LCTES '20}
}

@inproceedings{10.1007/978-3-030-60245-1_20,
author = {Zhan, Ke and Lu, ZhongHua and Zhang, YunQuan},
title = {Performance Optimization for Feature Extraction Section of DeepChem},
year = {2020},
isbn = {978-3-030-60244-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60245-1_20},
doi = {10.1007/978-3-030-60245-1_20},
abstract = {Based on the popular deep learning technique, the authors at Stanford implement DeepChem as an open source methods for the research in the fields of drug discovery, biology and so on. For the performance problem of training process of DeepChem neural network, this paper rebuilds the original serial feature extraction algorithm of DeepChem and optimizes the rebuilt serial algorithm based on the multiple processes algorithm. The experiment results show that the parallel algorithm achieves 15.38\texttimes{} speedup at the best compared with the serial algorithm. For the future work, first, in addition to the multiprocessing package, the other packages such as concurrent, subprocess and so on could be considered to optimize the feature extraction algorithm; Second, the serial and parallel algorithms run slower when the data block size is 150 compared with the other block sizes, optimization of training process performance for the smaller data block size is the second direction in future work.},
booktitle = {Algorithms and Architectures for Parallel Processing: 20th International Conference, ICA3PP 2020, New York City, NY, USA, October 2–4, 2020, Proceedings, Part I},
pages = {290–304},
numpages = {15},
keywords = {DeepChem, Feature Extraction, Multiple Processes Algorithm},
location = {New York, NY, USA}
}

@inproceedings{10.1007/978-3-030-58923-3_9,
author = {Arcelli, Davide},
title = {A Multi-objective Performance Optimization Approach for Self-adaptive Architectures},
year = {2020},
isbn = {978-3-030-58922-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58923-3_9},
doi = {10.1007/978-3-030-58923-3_9},
abstract = {This paper presents an evolutionary approach for multi-objective performance optimization of Self-Adaptive Systems, represented by a specific family of Queuing Network models, namely SMAPEA QNs. The approach is based on NSGA-II genetic algorithm and it is aimed at suggesting near-optimal alternative architectures in terms of mean response times for the different available system operational modes. The evaluation is performed through a controlled experiment with respect to a realistic case study, with the aim of establishing whether meta-heuristics are worth to be investigated as a valid support to performance optimization of Self-Adaptive Systems.},
booktitle = {Software Architecture: 14th European Conference, ECSA 2020, L'Aquila, Italy, September 14–18, 2020, Proceedings},
pages = {139–147},
numpages = {9},
keywords = {Self-adaptive systems, Software architecture, Software performance engineering, Search-based software engineering, Multi-objective optimization, Genetic algorithms, Queuing networks},
location = {L'Aquila, Italy}
}

@article{10.1016/j.jksuci.2019.07.010,
author = {Lim, Marcus and Abdullah, Azween and Jhanjhi, NZ},
title = {Performance optimization of criminal network hidden link prediction model with deep reinforcement learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {10},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2019.07.010},
doi = {10.1016/j.jksuci.2019.07.010},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = dec,
pages = {1202–1210},
numpages = {9},
keywords = {Hidden link prediction, Deep reinforcement learning, Criminal network analysis, Social network analysis, GPU}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/3394171.3413814,
author = {Zhuang, Weiming and Wen, Yonggang and Zhang, Xuesen and Gan, Xin and Yin, Daiying and Zhou, Dongzhan and Zhang, Shuai and Yi, Shuai},
title = {Performance Optimization of Federated Person Re-identification via Benchmark Analysis},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413814},
doi = {10.1145/3394171.3413814},
abstract = {Federated learning is a privacy-preserving machine learning technique that learns a shared model across decentralized clients. It can alleviate privacy concerns of personal re-identification, an important computer vision task. In this work, we implement federated learning to person re-identification (FedReID) and optimize its performance affected by statistical heterogeneity in the real-world scenario. We first construct a new benchmark to investigate the performance of FedReID. This benchmark consists of (1) nine datasets with different volumes sourced from different domains to simulate the heterogeneous situation in reality, (2) two federated scenarios, and (3) an enhanced federated algorithm for FedReID. The benchmark analysis shows that the client-edge-cloud architecture, represented by the federated-by-dataset scenario, has better performance than client-server architecture in FedReID. It also reveals the bottlenecks of FedReID under the real-world scenario, including poor performance of large datasets caused by unbalanced weights in model aggregation and challenges in convergence. Then we propose two optimization methods: (1) To address the unbalanced weight problem, we propose a new method to dynamically change the weights according to the scale of model changes in clients in each training round; (2) To facilitate convergence, we adopt knowledge distillation to refine the server model with knowledge generated from client models on a public dataset. Experiment results demonstrate that our strategies can achieve much better convergence with superior performance on all datasets. We believe that our work will inspire the community to further explore the implementation of federated learning on more computer vision tasks in real-world scenarios.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {955–963},
numpages = {9},
keywords = {computer vision, federated learning, machine learning system},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{10.1007/s11276-020-02540-8,
author = {Zhang, Jiaqi and Liu, Shengli and Yin, Rui and Yu, Guanding and Jin, Xinyu},
title = {Coexistence algorithms for LTE and WiFi networks in unlicensed spectrum: performance optimization and comparison},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {3},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-020-02540-8},
doi = {10.1007/s11276-020-02540-8},
abstract = {Facing the challenges brought by the surge in the demand for mobile data traffic, many coexistence algorithms for long-term evolution (LTE) and WiFi networks in the unlicensed spectrum have been developed recently. In this article, we investigate, optimize, and compare several solutions to operate LTE in the unlicensed band, which include duty-cycle muting (DCM) mechanism, listen-before-talk (LBT) mechanism, and LTE and WLAN aggregation (LWA) algorithm. We first overview the architecture, working mechanism, coexistence problem with WiFi network, and overall cost of the three different coexistence mechanisms.
 Their advantages and disadvantages are also compared and analyzed in detail. Then, to maximize the spectrum efficiency of unlicensed band, we optimize the corresponding adjustable variables in the three algorithms. Furthermore, numerical simulations are conducted to compare the performances of DCM, LBT, and LWA explicitly. The results show that DCM can achieve the highest throughput for the LTE network while LBT and LWA have less impact on the WiFi system.},
journal = {Wirel. Netw.},
month = apr,
pages = {1875–1885},
numpages = {11},
keywords = {Network coexistence, Performance optimization, LTE in unlicensed spectrum}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10586-021-03339-8,
author = {Ghazali, Rana and Adabi, Sahar and Down, Douglas G. and Movaghar, Ali},
title = {A classification of hadoop job schedulers based on performance optimization approaches},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03339-8},
doi = {10.1007/s10586-021-03339-8},
abstract = {Job scheduling in MapReduce plays a vital role in Hadoop performance. In recent years, many researchers have presented job scheduler algorithms to improve Hadoop performance. Designing a job scheduler that minimizes job execution time with maximum resource utilization is not a straightforward task. The primary purpose of this paper is to investigate agents affecting job scheduler efficiency and present a novel classification for job schedulers based on these factors. We provide a comprehensive overview of existing job schedulers in each group, evaluating their approaches, their effects on Hadoop performance, and comparing their advantages and disadvantages. Finally, we provide recommendations on choosing a preferred job scheduler in different environments for improving Hadoop performance.},
journal = {Cluster Computing},
month = dec,
pages = {3381–3403},
numpages = {23},
keywords = {MapReduce, Job scheduler, Performance metric, Straggler, Data locality, Resource utilization}
}

@article{10.1007/s11227-019-02766-0,
author = {Park, Sanghyun and Suh, Taeweon},
title = {DQN-based OpenCL workload partition for performance optimization},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-02766-0},
doi = {10.1007/s11227-019-02766-0},
abstract = {This paper proposes a deep Q network (DQN)-based method for the workload partition problem in OpenCL. The DQN, a reinforcement learning algorithm, optimizes the workload partition for each processing unit by the self-training, based on the accumulated performance data on the computing environment. Our experiments reveal that the DQN-based partition provides the performance improvement by up to 62.2% and 6.9% in JPEG decoding, compared to the LuxMark-based and target-based partitions, respectively. The DQN is able to capture the low-level contention in slave devices such as caches and memory, and the communication bottleneck between devices, and reflect it to the workload partition ratio.},
journal = {J. Supercomput.},
month = aug,
pages = {4875–4893},
numpages = {19},
keywords = {OpenCL, DQN, Workload partition}
}

@inproceedings{10.1007/978-3-030-78612-0_47,
author = {Qi, Zhihan and Pan, YunChao and Du, Hao and Zhang, Na and Bai, ZongHan and Xu, Gang},
title = {Opportunistic Network Performance Optimization Model Based on a Combination of Neural Networks and Orthogonal Experiments},
year = {2021},
isbn = {978-3-030-78611-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78612-0_47},
doi = {10.1007/978-3-030-78612-0_47},
abstract = {Under the complex environmental conditions of remote geographic locations and fragmented network areas, sending data over traditional wired networks or wireless self. Organizing networks often results in connection interruptions, packet loss and other problems. In order to reduce the problem of data loss due to network disruptions, researchers have proposed a new network technology for cut networks, named opportunity Network. Opportunity network messages have a high rate of packet loss due to factors such as the small probability of node encounters and message lifetime limits. In order to reduce the packet loss rate, improve the success rate of message transmission and reduce the message transmission delay, the paper proposes a node message transmission performance optimization model based on the fusion of neural networks and orthogonal experiments. The experimental results show that the prediction model proposed in the paper for optimizing node message transmission performance can predict the packet loss rate more accurately, and it is found that node density, node rate, and node cache are the most important factors influencing the message packet loss rate of the opportunity network, and the optimal settings of these three factors are obtained to reduce the packet loss rate of message transmission and improve the message delivery efficiency.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part II},
pages = {588–600},
numpages = {13},
keywords = {Opportunity network, Packet loss rate, Neural network, Orthogonal test},
location = {Dublin, Ireland}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@article{10.1016/j.compbiomed.2021.104564,
author = {Tandel, Gopal S. and Tiwari, Ashish and Kakde, O.G.},
title = {Performance optimisation of deep learning models using majority voting algorithm for brain tumour classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104564},
doi = {10.1016/j.compbiomed.2021.104564},
journal = {Comput. Biol. Med.},
month = aug,
numpages = {27},
keywords = {Ensemble, Majority voting, Convolutional neural network, Machine learning, Deep learning, Transfer learning, Magnetic resonance imaging, Computer-aided diagnosis}
}

@article{10.1016/j.sysarc.2019.101697,
author = {Horga, Adrian and Chattopadhyay, Sudipta and Eles, Petru and Peng, Zebo},
title = {Genetic algorithm based estimation of non–functional properties for GPGPU programs},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2019.101697},
doi = {10.1016/j.sysarc.2019.101697},
journal = {J. Syst. Archit.},
month = feb,
numpages = {14},
keywords = {Worst case execution time, Software security, GPU, Side-channel leakage, Software safety}
}

@article{10.3103/S1060992X18040094,
author = {Wei Guan},
title = {Performance Optimization of Speech Recognition System with Deep Neural Network Model},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {4},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X18040094},
doi = {10.3103/S1060992X18040094},
journal = {Opt. Mem. Neural Netw.},
month = oct,
pages = {272–282},
numpages = {11},
keywords = {acoustic model, deep neural network, discriminative training, performance optimization, speech recognition}
}

@article{10.1007/s11554-018-0795-7,
author = {Jalil, Nauman and Smith, Scott C. and Green, Roger},
title = {Performance optimization of rotation-tolerant Viola–Jones-based blackbird detection},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {3},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-018-0795-7},
doi = {10.1007/s11554-018-0795-7},
abstract = {The research described in this paper investigates the rotational robustness of the Viola–Jones algorithm (VJA) object detection method when used for red-winged blackbird (Agelaius phoeniceus) detection. VJA has been successfully used for face detection, but can be adapted to detect a variety of objects. This work uses the histogram of oriented gradients (HOG) descriptor to train the blackbird classifier. Since VJA object detection is inherently not invariant to in-plane object rotation, additional effort is required during training and detection. The proposed method extends the object detection framework developed by Viola and Jones to efficiently handle rotated blackbirds and provide a balance between detection accuracy and computation cost.},
journal = {J. Real-Time Image Process.},
month = jun,
pages = {471–478},
numpages = {8},
keywords = {Viola–Jones algorithm, Histogram of oriented gradients (HOG), In-plane object rotation, Region of interest (ROI), Gradient orientation}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.future.2019.03.045,
author = {Qasha, Rawaa and Wen, Zhenyu and Ca\l{}a, Jacek and Watson, Paul},
title = {Sharing and performance optimization of reproducible workflows in the cloud},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {98},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.03.045},
doi = {10.1016/j.future.2019.03.045},
journal = {Future Gener. Comput. Syst.},
month = sep,
pages = {487–502},
numpages = {16},
keywords = {Provisioning optimization, Workflow reproducibility, Workflow deployment, Container-based virtualization, Cloud computing}
}

@article{10.1007/s00521-021-06178-1,
author = {Ebrahimi Mood, Sepehr and Ding, Ming and Lin, Zihuai and Javidi, Mohammad Masoud},
title = {Performance optimization of UAV-based IoT communications using a novel constrained gravitational search algorithm},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {22},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06178-1},
doi = {10.1007/s00521-021-06178-1},
abstract = {In the recent years, unmanned aerial vehicles (UAVs) because of their ability to be used as aerial base stations for collecting data from IoT devices have attracted substantial interest in Internet of Things (IoT) systems. In this paper, a novel method has been proposed to increase the quality of the uplink IoT communications and to decrease the transmission power of IoT devices. For this purpose, to compute the UAVs’ trajectory, association of device-to-UAV and IoT transmission power, a novel objective function has been defined to optimize the link quality and energy consumption in the considered UAV-based IoT system. Then, for optimizing this objective function, a novel constrained version of gravitational search algorithm is proposed, which is an NP-hard problem. In this algorithm, to handle the constraints, a multiple constraint ranking method is used. Moreover, to calculate the value of the parameter of this method, a fuzzy logic controller is used to control the exploitation and exploration abilities and improve the performance of this algorithm. To evaluate the performance of the proposed method, simulations have been performed and the results were compared with those of the other methods. the experimental results show that an increase in system throughput and a decrease in the energy consumption of the considered UAV-based IoT system can be achieved simultaneously using the proposed optimization algorithm.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {15557–15568},
numpages = {12},
keywords = {UAV, Internet of Things, Energy efficiency, Constrained GSA, Multiple constraint ranking}
}

@inproceedings{10.1145/3415958.3433101,
author = {Anisetti, Marco and Ardagna, Claudio A. and Damiani, Ernesto and Panero, Paolo G.},
title = {A Methodology for Non-Functional Property Evaluation of Machine Learning Models},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433101},
doi = {10.1145/3415958.3433101},
abstract = {The pervasive diffusion of Machine Learning (ML) in many critical domains and application scenarios has revolutionized implementation and working of modern IT systems. The behavior of modern systems often depends on the behavior of ML models, which are treated as black boxes, thus making automated decisions based on inference unpredictable. In this context, there is an increasing need of verifying the non-functional properties of ML models, such as, fairness and privacy, to the aim of providing certified ML-based applications and services. In this paper, we propose a methodology based on Multi-Armed Bandit for evaluating non-functional properties of ML models. Our methodology adopts Thompson sampling, Monte Carlo Simulation, and Value Remaining. An experimental evaluation in a real-world scenario is presented to prove the applicability of our approach in evaluating the fairness of different ML models.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {38–45},
numpages = {8},
keywords = {Machine Learning Assurance, Multiarmed bandit, Non-functional properties},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3106237.3119880,
author = {Tang, Chong},
title = {System performance optimization via design and configuration space exploration},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3119880},
doi = {10.1145/3106237.3119880},
abstract = {The runtime performance of a software system often depends on a large number of static parameters, which usually interact in complex ways to carry out system functionality and influence system performance. It's hard to understand such configuration spaces and find good combinations of parameter values to gain available levels of performance. Engineers in practice often just accept the default settings, leading such systems to significantly underperform relative to their potential. This problem, in turn, has impacts on cost, revenue, customer satisfaction, business reputation, and mission effectiveness. To improve the overall performance of the end-to-end systems, we propose to systematically explore (i) how to design new systems towards good performance through design space synthesis and evaluation, and (ii) how to auto-configure an existing system to obtain better performance through heuristic configuration space search. In addition, this research further studies execution traces of a system to predict runtime performance under new configurations.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {1046–1049},
numpages = {4},
keywords = {Configuration Space, Design Space, Performance Optimization, Performance Prediction},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1007/s11235-019-00580-w,
author = {Alsamhi, S. H. and Almalki, F. A. and Ma, Ou and Ansari, M. S. and Angelides, M. C.},
title = {Performance optimization of tethered balloon technology for public safety and emergency communications},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {2},
issn = {1018-4864},
url = {https://doi.org/10.1007/s11235-019-00580-w},
doi = {10.1007/s11235-019-00580-w},
abstract = {This paper investigates the potential of a tethered balloon network architecture deployed as part of public safety networks and emergency communications. Tethered balloon technology has been evolving as a powerful and promising technology for improving public safety and for saving people’s lives. As such, it enables accomplishment of unique and specific missions for temporary events such as natural hazardous or terrorist acts. Such acts raise significantly the potential to disrupt the entire terrestrial wireless communication infrastructure. To mitigate the effects of such catastrophic events, we propose tethered balloon technology for delivering broadband services in an area over which the communication infrastructure has been entirely or partially destroyed. The results reveal a significantly high performance in providing broadband communication services with provision of Quality of Service. This suggests that the work of rescue and relief teams can be significantly enhanced.},
journal = {Telecommun. Syst.},
month = oct,
pages = {235–244},
numpages = {10},
keywords = {Public safety networks, Emergency services, Tethered balloon, Disaster recovery}
}

@article{10.1155/2020/8868225,
author = {Wei, Mingze and Yuan, Lei and Lv, Jianhui},
title = {Performance Optimization Mechanism of Adolescent Physical Training Based on Reinforcement Learning and Markov Model},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {2020},
issn = {1574-017X},
url = {https://doi.org/10.1155/2020/8868225},
doi = {10.1155/2020/8868225},
abstract = {Upon the teenagers’ failure to obtain the plenty of physical exercises at the growth and development stage, the related central nervous system is prone to degeneration and the physical fitness starts to decline gradually. In fact, through monitoring the exercise process real-timely and quantifying the exercise data, the adolescent physical training can be effectively conducted. For such process, it involves two issues, i.e., the real-time data monitoring and data quantification evaluation. Therefore, this paper proposes a novel method based on Reinforcement Learning (RL) and Markov model to monitor and evaluate the physical training effect. Meanwhile, the RL is used to optimize the adaptive bit rate of surveillance video and help the real-time data monitoring; the Markov model is employed to evaluate the health condition on the physical training. Finally, we develop a real-time monitoring system on exercise data and compare with the state-of-the-art mechanisms based on this system platform. The experimental results indicate that the proposed performance optimization mechanism can be more efficient to conduct the physical training. Particularly the average evaluation deviation rate based on Markov model is controlled within 0.16%.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1109/FUZZ48607.2020.9177856,
author = {Naik, Nitin and Jenkins, Paul and Savage, Nick and Yang, Longzhi and Naik, Kshirasagar and Song, Jingping},
title = {Embedding Fuzzy Rules with YARA Rules for Performance Optimisation of Malware Analysis},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FUZZ48607.2020.9177856},
doi = {10.1109/FUZZ48607.2020.9177856},
abstract = {YARA rules utilises string or pattern matching to perform malware analysis and is one of the most effective methods in use today. However, its effectiveness is dependent on the quality and quantity of YARA rules employed in the analysis. This can be managed through the rule optimisation process, although, this may not necessarily guarantee effective utilisation of YARA rules and its generated findings during its execution phase, as the main focus of YARA rules is in determining whether to trigger a rule or not, for a suspect sample after examining its rule condition. YARA rule conditions are Boolean expressions, mostly focused on the binary outcome of the malware analysis, which may limit the optimised use of YARA rules and its findings despite generating significant information during the execution phase. Therefore, this paper proposes embedding fuzzy rules with YARA rules to optimise its performance during the execution phase. Fuzzy rules can manage imprecise and incomplete data and encompass a broad range of conditions, which may not be possible in Boolean logic. This embedding may be more advantageous when the YARA rules become more complex, resulting in multiple complex conditions, which may not be processed efficiently utilising Boolean expressions alone, thus compromising effective decision-making. This proposed embedded approach is applied on a collected malware corpus and is tested against the standard and enhanced YARA rules to demonstrate its success.},
booktitle = {2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
pages = {1–7},
numpages = {7},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@inproceedings{10.1109/GLOBECOM38437.2019.9014313,
author = {Duong, Trung Q. and Nguyen, Long D. and Tuan, Hoang Duong and Hanzo, Lajos},
title = {Learning-Aided Realtime Performance Optimisation of Cognitive UAV-Assisted Disaster Communication},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM38437.2019.9014313},
doi = {10.1109/GLOBECOM38437.2019.9014313},
abstract = {In this work, we propose efficient optimisation methods for relay-assisted unmanned aerial vehicles (UAVs) in cognitive radio networks (CRNs) to cope with the network destruction in the event of a natural disaster. Our model considers real- time optimisation in embedded UAV-CRN communication involved in recovering wireless communication services. Particularly, by conceiving advanced optimisation techniques and training deep neural networks, our solutions become capable of supporting real-time applications in disaster recovery scenarios. Our algorithms impose low computational complexity, hence, have a low execution time in solving real- time optimisation problems. Numerical results demonstrate the benefits of our approaches proposed for UAV-CRN.},
booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Waikoloa, HI, USA}
}

@article{10.1016/j.cad.2019.06.007,
author = {Feng, Jiawei and Fu, Jianzhong and Shang, Ce and Lin, Zhiwei and Li, Bin},
title = {Sandwich panel design and performance optimization based on triply periodic minimal surfaces},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2019.06.007},
doi = {10.1016/j.cad.2019.06.007},
journal = {Comput. Aided Des.},
month = oct,
pages = {307–322},
numpages = {16},
keywords = {Triply periodic minimal surfaces, Sandwich panel, T-spline surfaces, Additive manufacturing}
}

@inproceedings{10.1145/3375998.3376039,
author = {Adinew, Deleli Mesay and Shijie, Zhou and Liao, Yongjian},
title = {Spark Performance Optimization Analysis in Memory Tuning On GC Overhead for Big Data Analytics},
year = {2020},
isbn = {9781450377027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375998.3376039},
doi = {10.1145/3375998.3376039},
abstract = {Apache spark is one of the high speed "in-memory computing" that run over the JVM. Due to increasing data in volume, it needs performance optimization mechanism that requires management of JVM heap space. To Manage JVM heap space it needs management of garbage collector pause time that affects application performance. There are different parameters to pass to spark to control JVM heap space and GC time overhead to increase application performance. Passing appropriate heap size with appropriate types of GC as a parameter is one of performance optimization which is known as Spark Garbage collection tuning. To reduce GC overhead, an experiment was done by adjusting certain parameters for loading and dataframe creation and data retrieval process. The result shows 3.23% improvement in Latency and 1.62% improvement in Throughput as compared to default parameter configuration in garbage collection tuning approach.},
booktitle = {Proceedings of the 2019 8th International Conference on Networks, Communication and Computing},
pages = {75–78},
numpages = {4},
keywords = {GC Events, Garbage Collection Tuning, HeapRegionSize, JVM heap space},
location = {Luoyang, China},
series = {ICNCC '19}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3465332.3470875,
author = {Akgun, Ibrahim Umit and Aydin, Ali Selman and Shaikh, Aadil and Velikov, Lukas and Zadok, Erez},
title = {A Machine Learning Framework to Improve Storage System Performance},
year = {2021},
isbn = {9781450385503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465332.3470875},
doi = {10.1145/3465332.3470875},
abstract = {Storage systems and their OS components are designed to accommodate a wide variety of applications and dynamic workloads. Storage components inside the OS contain various heuristic algorithms to provide high performance and adaptability for different workloads. These heuristics may be tunable via parameters, and some system calls allow users to optimize their system performance. These parameters are often predetermined based on experiments with limited applications and hardware. Thus, storage systems often run with these predetermined and possibly suboptimal values. Tuning these parameters manually is impractical: one needs an adaptive, intelligent system to handle dynamic and complex workloads. Machine learning (ML) techniques are capable of recognizing patterns, abstracting them, and making predictions on new data. ML can be a key component to optimize and adapt storage systems. In this position paper, we propose KML, an ML framework for storage systems. We implemented a prototype and demonstrated its capabilities on the well-known problem of tuning optimal readahead values. Our results show that KML has a small memory footprint, introduces negligible overhead, and yet enhances throughput by as much as 2.3x.},
booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {94–102},
numpages = {9},
keywords = {machine learning, operating systems, storage performance optimization, storage systems},
location = {Virtual, USA},
series = {HotStorage '21}
}

@article{10.1016/j.datak.2021.101909,
author = {Maass, Wolfgang and Storey, Veda C.},
title = {Pairing conceptual modeling with machine learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {134},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101909},
doi = {10.1016/j.datak.2021.101909},
journal = {Data Knowl. Eng.},
month = jul,
numpages = {35},
keywords = {Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence}
}

@inproceedings{10.1007/978-3-030-38919-2_1,
author = {Damiani, Ernesto and Ardagna, Claudio A.},
title = {Certified Machine-Learning Models},
year = {2020},
isbn = {978-3-030-38918-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38919-2_1},
doi = {10.1007/978-3-030-38919-2_1},
abstract = {The massive adoption of Machine Learning (ML) has deeply changed the internal structure, the design and the operation of software systems. ML has shifted the focus from code to data, especially in application areas where it is easier to collect samples that embody correct solutions to individual instances of a problem, than to design and code a deterministic algorithm solving it for all instances. There is an increasing awareness of the need to verify key non-functional properties of ML-based software applications like fairness and privacy. However, the traditional approach trying to verify these properties by code inspection is pointless, since ML models’ behavior mostly depends on the data and parameters used to train them. Classic software certification techniques cannot solve the issue as well. The Artificial Intelligence (AI) community has been working on the idea of preventing undesired behavior by controlling a priori the ML models’ training sets and parameters. In this paper, we take a different, online approach to ML verification, where novel behavioral monitoring techniques based on statistical testing are used to support a dynamic certification framework enforcing the desired properties on black-box ML models in operation. Our aim is to deliver a novel framework suitable for practical certification of distributed ML-powered applications in heavily regulated domains like transport, energy, healthcare, even when the certifying authority is not privy to the model training. To achieve this goal, we rely on three key ideas: (i) use test suites to define desired non-functional properties of ML models, (ii) Use statistical monitoring of ML models’ behavior at inference time to check that the desired behavioral properties are achieved, and (iii) compose monitors’ outcome within dynamic, virtual certificates for composite software applications.},
booktitle = {SOFSEM 2020: Theory and Practice of Computer Science: 46th International Conference on Current Trends in Theory and Practice of Informatics, SOFSEM 2020, Limassol, Cyprus, January 20–24, 2020, Proceedings},
pages = {3–15},
numpages = {13},
keywords = {Intelligent systems, Machine Learning, Certification},
location = {Limassol, Cyprus}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@article{10.1007/s11277-019-06556-y,
author = {Nasir, Qassim and Mahdi, Omar},
title = {Patch Antenna Performance Optimization and Multi-band Operation Using Single Rectangular Slot},
year = {2019},
issue_date = {Nov 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {109},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-019-06556-y},
doi = {10.1007/s11277-019-06556-y},
abstract = {In this paper, an investigation is carried out to see how a single, rectangular shaped slot (cut) can be used to significantly improve the performance of a simple, impedance matched rectangular microstrip patch antenna operating in wireless local area network frequencies. Tests are also extended to show how this type of slot can be employed to achieve multi-band operation. The effects of different slot orientations are also examined. The proposed configurations are simulated using the HFSS software package, where return loss, input impedance (Z-parameter), and radiation patterns are used for the analysis of the different configurations. It is found that this type of patch modification can be used to exceed the performance of much more complex patch designs and to offer outstanding efficiency even at multiple resonant frequencies while generally maintaining the radiation properties of the original antenna it was implemented on and avoiding base frequency shifts, making using this slot much easier than other designs.},
journal = {Wirel. Pers. Commun.},
month = nov,
pages = {155–173},
numpages = {19},
keywords = {Microstrip patch antenna, Slotted antenna, High performance, Multi-band operation, WLAN}
}

@article{10.1016/j.neucom.2016.04.059,
author = {Wang, Xuesong and Huang, Fei and Cheng, Yuhu},
title = {Computational performance optimization of support vector machine based on support vectors},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {211},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.04.059},
doi = {10.1016/j.neucom.2016.04.059},
abstract = {The computational performance of support vector machine (SVM) mainly depends on the size and dimension of training sample set. Because of the importance of support vectors in the determination of SVM classification hyperplane, a kind of method for computational performance optimization of SVM based on support vectors is proposed. On one hand, at the same time of the selection of super-parameters of SVM, according to Karush-Kuhn-Tucker condition and on the precondition of no loss of potential support vectors, we eliminate non-support vectors from training sample set to reduce sample size and thereby to reduce the computation complexity of SVM. On the other hand, we propose a simple intrinsic dimension estimation method for SVM training sample set by analyzing the correlation between number of support vectors and intrinsic dimension. Comparative experimental results indicate the proposed method can effectively improve computational performance.},
journal = {Neurocomput.},
month = oct,
pages = {66–71},
numpages = {6},
keywords = {Computational performance, Intrinsic dimension, Sample size, Support vector, Support vector machine}
}

@article{10.1016/j.neucom.2019.04.048,
author = {Fang, Xiaohan and Han, Yinghua and Wang, Jinkuan and Zhao, Qiang},
title = {A cognitive control approach for microgrid performance optimization in unstable wireless communication},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.048},
doi = {10.1016/j.neucom.2019.04.048},
journal = {Neurocomput.},
month = aug,
pages = {168–182},
numpages = {15},
keywords = {Microgrid, Demand response management, Cognitive control, Unstable wireless communication, Information gap, Q-learning}
}

@inproceedings{10.1109/SMC.2018.00238,
author = {Shen, Zixiao and Chen, Xin and Garibaldi, Jon},
title = {Performance Optimization of a Fuzzy Entropy Based Feature Selection and Classification Framework},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2018.00238},
doi = {10.1109/SMC.2018.00238},
abstract = {In this paper, based on a fuzzy entropy feature selection framework, different methods have been implemented and compared to improve the key components of the framework. Those methods include the combinations of three ideal vector calculations, three maximal similarity classifiers and three fuzzy entropy functions. Different feature removal orders based on the fuzzy entropy values were also compared. The proposed method was evaluated on three publicly available biomedical datasets, including Wisconsin Breast Cancer(WBC), Wisconsin Diagnostic Breast Cancer(WDBC) and Parkinsons. From the experiments, we concluded the optimized combination of the ideal vector, similarity classifier and fuzzy entropy function for feature selection. The optimized framework was also compared with other six classical filter-based feature selection methods. The proposed method was ranked as one of the top performers together with the Correlation and ReliefF methods. The proposed method achieved classification accuracies of 96.97%, 94.85% and 78.23% for the WBC, WDBC and Parkinsons datasets respectively. More importantly, the proposed method achieved the most stable performance for all three datasets when the features being gradually removed. This indicates a better feature ranking performance than the other compared methods.},
booktitle = {2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {1361–1367},
numpages = {7},
location = {Miyazaki, Japan}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1007/s11227-020-03507-4,
author = {Sharma, Atul and Goyal, Nitin and Guleria, Kalpna},
title = {Performance optimization in delay tolerant networks using backtracking algorithm for fully credits distribution to contrast selfish nodes},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {6},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03507-4},
doi = {10.1007/s11227-020-03507-4},
abstract = {Delay Tolerant Network (DTN) can be characterized as a heterogeneous network of nodes in which nodes mobility is high and resources are limited to transmit messages. In DTN, nodes use store, carry, and forward principle for delivering messages to the destination node. To forward messages through intermediate nodes may cause security issues in the network since there may exist a few selfish nodes. These nodes can affect the network's performance as they may drop messages due to their limited resources such as energy and storage capacity. To handle this kind of hazard, it is necessary to propose a mechanism that may decrease the degree of the selfishness of nodes and improve the network's delivery ratio by fully distributing credits to nodes. In this article, a credit-based mechanism has been proposed based on Combined Trust Value (CTV) of nodes in DTN. In the proposed mechanism, an agent is used to compute each node's trust value grounded of the number of messages relayed by sensor nodes. This trust value is used to distribute credits to the nodes in a distributed manner without any partiality with nodes. Backtracking approach is used to distribute credits to boundary nodes deserving credits but didn’t get credits by agent node. The proposed mechanism is implemented using ONE simulator, and the performance of the projected scheme is analyzed in comparison to existing techniques Dynamic Trust, SMART (Secure Multilayer credit based incentive Technique) and Credit-based. The results exhibit that the suggested mechanism is superior than existing techniques with reference to various performance metrics like 25% higher delivery ratio, 41% less overhead, 21% less average message delay, and 28% less packets dropped. The proposed mechanism might be helpful in scenarios where the degree of selfishness is high, and the distribution of credit follows a fully distributed approach rather than an existing partial distribution used in existing techniques.},
journal = {J. Supercomput.},
month = jun,
pages = {6036–6055},
numpages = {20},
keywords = {Credit, DTN, Faith value, Selfish degree, Selfish node, ONE simulator}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@inproceedings{10.1145/3437802.3437828,
author = {Lu, Zhenyan and Chen, Chao and Xin, Jinhan and Yu, Zhibin},
title = {On the Auto-Tuning of Elastic-search based on Machine Learning},
year = {2021},
isbn = {9781450388054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437802.3437828},
doi = {10.1145/3437802.3437828},
abstract = {Elastic-search is a distributed search engine which is used to process large amount of data widely. It has a vast number of configuration parameters which are extremely difficult to manually tune to achieve optimal throughput and latency. This paper presents an auto-tuning method to improve the performance of Elastic-search based on random forest and gradient boosting regression trees. By analyzing the working process of Elastic-search, performance-sensitive configuration parameters are selected to establish a machine learning model with high accuracy, so as to accurately predict the performance of Elastic-search with different configurations. With the help of performance prediction, the genetic algorithm finds the optimal configuration of Elastic-search under given system conditions. Three data sets with different sizes and structures are selected for evaluation and the benchmarking tool EsRally tests the performance of index and query operation. Experimental results show that our proposed method can improve the performance by 2.73 times on average and up to 7.02 times compared to the default configuration of Elastic-search.},
booktitle = {Proceedings of the 2020 1st International Conference on Control, Robotics and Intelligent System},
pages = {150–156},
numpages = {7},
keywords = {Elastic-search, distributed search engine, machine learning, performance optimization},
location = {Xiamen, China},
series = {CCRIS '20}
}

@article{10.1016/j.compeleceng.2017.10.008,
author = {Singh, Sudhakar and Garg, Rakhi and Mishra, P K},
title = {Performance optimization of MapReduce-based Apriori algorithm on Hadoop cluster},
year = {2018},
issue_date = {Apr 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.10.008},
doi = {10.1016/j.compeleceng.2017.10.008},
journal = {Comput. Electr. Eng.},
month = apr,
pages = {348–364},
numpages = {17},
keywords = {Algorithms, Data mining, Big data, Frequent itemset, Apriori, Hadoop, MapReduce}
}

@article{10.1007/s11704-015-4584-1,
author = {Zhang, Richong and Bao, Han and Sun, Hailong and Wang, Yanghao and Liu, Xudong},
title = {Recommender systems based on ranking performance optimization},
year = {2016},
issue_date = {April     2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {2},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-015-4584-1},
doi = {10.1007/s11704-015-4584-1},
abstract = {The rapid development of online services and information overload has inspired the fast development of recommender systems, among which collaborative filtering algorithms and model-based recommendation approaches are wildly exploited. For instance, matrix factorization (MF) demonstrated successful achievements and advantages in assisting internet users in finding interested information. These existing models focus on the prediction of the users' ratings on unknown items. The performance is usually evaluated by the metric root mean square error (RMSE). However, achieving good performance in terms of RMSE does not always guarantee a good ranking performance. Therefore, in this paper, we advocate to treat the recommendation as a ranking problem. Normalized discounted cumulative gain (NDCG) is chosen as the optimization target when evaluating the ranking accuracy. Specifically, we present three ranking-oriented recommender algorithms, NSMF, AdaMF and AdaNSMF. NSMF builds a NDCG approximated loss function for Matrix Factorization. AdaMF is based on an algorithm by adaptively combining component MF recommenders with boosting method. To combine the advantages of both algorithms, we propose AdaNSMF, which is a hybird of NSMF and AdaMF, and show the superiority in both ranking accuracy and model generalization. In addition, we compare our proposed approaches with the state-of-the-art recommendation algorithms. The comparison studies confirm the advantage of our proposed approaches.},
journal = {Front. Comput. Sci.},
month = apr,
pages = {270–280},
numpages = {11},
keywords = {learning to rank, matrix factorization, recommender system}
}

@inproceedings{10.1007/978-3-030-97759-7_3,
author = {Liao, Chunhua and Wang, Anjia and Georgakoudis, Giorgis and de Supinski, Bronis R. and Yan, Yonghong and Beckingsale, David and Gamblin, Todd},
title = {Extending OpenMP for Machine Learning-Driven Adaptation},
year = {2021},
isbn = {978-3-030-97758-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-97759-7_3},
doi = {10.1007/978-3-030-97759-7_3},
abstract = {OpenMP 5.0 introduced the 
[inline-graphic not available: see fulltext]
 directive to support compile-time selection from a set of directive variants based on OpenMP context. OpenMP 5.1 extended context information to include user-defined conditions that enable user-guided runtime adaptation. However, defining conditions that capture the complex interactions between applications and hardware platforms to select an optimized variant is challenging for programmers. This paper explores a novel approach to automate runtime adaptation through machine learning. We design a new 
[inline-graphic not available: see fulltext]
 directive to describe semantics for model-driven adaptation and also develop a prototype implementation. Using the Smith-Waterman algorithm as a use-case, our experiments demonstrate that the proposed adaptive OpenMP extension automatically chooses the code variants that deliver the best performance in heterogeneous platforms that consist of CPU and GPU processing capabilities. Using decision tree models for tuning has an accuracy of up&nbsp;to 93.1% in selecting the optimal variant, with negligible runtime overhead.},
booktitle = {Accelerator Programming Using Directives: 8th International Workshop, WACCPD 2021, Virtual Event, November 14, 2021, Proceedings},
pages = {49–69},
numpages = {21},
keywords = {OpenMP, Machine Learning, Runtime Adaptation}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {MLaaS, Machine Learning Services, Machine Learning, Machine Learning Platform, Machine Learning as a Service},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
articleno = {9},
numpages = {37},
keywords = {MAPE-K, Self-adaptation, feedback loops}
}

@article{10.1016/j.jss.2019.02.007,
author = {Li, Chunlin and Zhang, Jing and Chen, Yi and Luo, Youlong},
title = {Data prefetching and file synchronizing for performance optimization in Hadoop-based hybrid cloud},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.007},
doi = {10.1016/j.jss.2019.02.007},
journal = {J. Syst. Softw.},
month = may,
pages = {133–149},
numpages = {17},
keywords = {Data prefetching, File synchronizing, Hybrid cloud}
}

@article{10.1007/s11036-019-01318-3,
author = {Usman, Sardar and Mehmood, Rashid and Katib, Iyad and Albeshri, Aiiad and Altowaijri, Saleh M.},
title = {ZAKI: A Smart Method and Tool for Automatic Performance Optimization of Parallel SpMV Computations on Distributed Memory Machines},
year = {2019},
issue_date = {Apr 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1383-469X},
url = {https://doi.org/10.1007/s11036-019-01318-3},
doi = {10.1007/s11036-019-01318-3},
abstract = {SpMV is a vital computing operation of many scientific, engineering, economic and social applications, increasingly being used to develop timely intelligence for the design and management of smart societies. Several factors affect the performance of SpMV computations, such as matrix characteristics, storage formats, software and hardware platforms. The complexity of the computer systems is on the rise with the increasing number of cores per processor, different levels of caches, processors per node and high speed interconnect. There is an ever-growing need for new optimization techniques and efficient ways of exploiting parallelism. In this paper, we propose ZAKI, a data-driven, machine-learning approach and tool, to predict the optimal number of processes for SpMV computations of an arbitrary sparse matrix on a distributed memory machine. The aim herein is to allow application scientists to automatically obtain the best configuration, and hence the best performance, for the execution of SpMV computations. We train and test the tool using nearly 2000 real world matrices obtained from 45 application domains including computational fluid dynamics (CFD), computer vision, and robotics. The tool uses three machine learning methods, decision trees, random forest, gradient boosting, and is evaluated in depth. A discussion on the applicability of our proposed tool to energy efficiency optimization of SpMV computations is given. This is the first work where the sparsity structure of matrices have been exploited to predict the optimal number of processes for a given matrix in distributed memory environments by using different base and ensemble machine learning methods.},
journal = {Mob. Netw. Appl.},
month = jul,
pages = {744–763},
numpages = {20},
keywords = {Parallel computing, Sparse matrix vector product (SpMV), Machine learning, Sparse linear equation systems, Distributed memory, MPI, Decision trees, Random forest, Gradient boosting}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3392717.3392765,
author = {S\'{a}nchez Barrera, Isaac and Black-Schaffer, David and Casas, Marc and Moret\'{o}, Miquel and Stupnikova, Anastasiia and Popov, Mihail},
title = {Modeling and optimizing NUMA effects and prefetching with machine learning},
year = {2020},
isbn = {9781450379830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3392717.3392765},
doi = {10.1145/3392717.3392765},
abstract = {Both NUMA thread/data placement and hardware prefetcher configuration have significant impacts on HPC performance. Optimizing both together leads to a large and complex design space that has previously been impractical to explore at runtime.In this work we deliver the performance benefits of optimizing both NUMA thread/data placement and prefetcher configuration at runtime through careful modeling and online profiling. To address the large design space, we propose a prediction model that reduces the amount of input information needed and the complexity of the prediction required. We do so by selecting a subset of performance counters and application configurations that provide the richest profile information as inputs, and by limiting the output predictions to a subset of configurations that cover most of the performance.Our model is robust and can choose near-optimal NUMA+Pre-fetcher configurations for applications from only two profile runs. We further demonstrate how to profile online with low overhead, resulting in a technique that delivers an average of 1.68X performance improvement over a locality-optimized NUMA baseline with all prefetchers enabled.},
booktitle = {Proceedings of the 34th ACM International Conference on Supercomputing},
articleno = {34},
numpages = {13},
keywords = {NUMA, machine learning model, page mapping, performance optimization, prefetching, thread mapping},
location = {Barcelona, Spain},
series = {ICS '20}
}

@inproceedings{10.5220/0005156003160321,
author = {Alexandre, Fr\'{e}d\'{e}ric and Carrere, Maxime and Kassab, Randa},
title = {Feature, Configuration, History},
year = {2014},
isbn = {9789897580543},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005156003160321},
doi = {10.5220/0005156003160321},
abstract = {Artificial Neural Networks are very efficient adaptive models but one of their recognized weaknesses is about information representation, often carried out in an input vector without a structure. Beyond the classical elaboration of a hierarchical representation in a series of layers, we report here inspiration from neuroscience and argue for the design of heterogenous neural networks, processing information at feature, configuration and history levels of granularity, and interacting very efficiently for high-level and complex decision making. This framework is built from known characteristics of the sensory cortex, the hippocampus and the prefrontal cortex and is exemplified here in the case of pavlovian conditioning, but we propose that it can be advantageously applied in a wider extent, to design flexible and versatile information processing with neuronal computation.},
booktitle = {Proceedings of the International Joint Conference on Computational Intelligence - Volume 3},
pages = {316–321},
numpages = {6},
keywords = {Computational Neuroscience, Information Representation, Pavlovian Conditioning.},
location = {Rome, Italy},
series = {IJCCI 2014}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@inproceedings{10.5555/1753235.1753266,
author = {Hubaux, Arnaud and Classen, Andreas and Heymans, Patrick},
title = {Formal modelling of feature configuration workflows},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {In software product line engineering, the configuration process can be a long and complex undertaking that involves many participants. When configuration is supported by feature diagrams, two challenges are to modularise the feature diagram into related chunks, and to schedule them as part of the configuration process. Existing work has only focused on the first of these challenges and, for the rest, assumes that feature diagram modules are configured sequentially. This paper addresses the second challenge. It suggests using YAWL, a state-of-the-art workflow language, to represent the configuration workflow while feature diagrams model the available configuration options. The principal contribution of the paper is a new combined formalism: feature configuration workflows. A formal semantics is provided so as to pave the way for unambiguous tool specification and safer reasoning about of the configuration process. The work is motivated and illustrated through a configuration scenario taken from the space industry.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {221–230},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1007/s00778-017-0477-2,
author = {Ali, Syed Muhammad and Wrembel, Robert},
title = {From conceptual design to performance optimization of ETL workflows: current state of research and open problems},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0477-2},
doi = {10.1007/s00778-017-0477-2},
abstract = {In this paper, we discuss the state of the art and current trends in designing and optimizing ETL workflows. We explain the existing techniques for: (1) constructing a conceptual and a logical model of an ETL workflow, (2) its corresponding physical implementation, and (3) its optimization, illustrated by examples. The discussed techniques are analyzed w.r.t. their advantages, disadvantages, and challenges in the context of metrics such as autonomous behavior, support for quality metrics, and support for ETL activities as user-defined functions. We draw conclusions on still open research and technological issues in the field of ETL. Finally, we propose a theoretical ETL framework for ETL optimization.},
journal = {The VLDB Journal},
month = dec,
pages = {777–801},
numpages = {25},
keywords = {ETL conceptual design, ETL logical design, ETL optimization, ETL physical implementation, ETL workflow}
}

@article{10.1109/TASLP.2017.2674968,
author = {Bao, Yu and Chen, Huawei and Huawei Chen},
title = {Design of Robust Broadband Beamformers Using Worst-Case Performance Optimization: A Semidefinite Programming Approach},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2674968},
doi = {10.1109/TASLP.2017.2674968},
abstract = {Broadband beamformers are known sensitive to microphone mismatches, especially for small-sized arrays. To address the problem, the worst-case performance optimization WCPO criterion has been used to design robust broadband beamformers. Recently, a circular-model-based design using the WCPO criterion was proposed, which has been shown superior to the existing second-order cone programming SOCP based design also using the WCPO criterion. In this paper, however, we find that the circular-model-based design is sensitive to stopband level constraint. If the constraint is chosen tightly, it may become less robust and degrades even worse than the SOCP-based design. To achieve a better tradeoff between robustness and beamforming performance, we propose a semidefinite programming based approach for robust broadband beamformer design using the WCPO criterion. The relations of the proposed design to its existing counterparts are also theoretically analyzed. In particular, we show that the proposed design is less conservative than the SOCP-based design, and is also less sensitive to the setting of stopband level constraint and more robust when compared to the circular model-based design. Another contribution of this paper is that the relation between the SOCP- and the circular-model-based designs is theoretically revealed. Simulation results and real experimental results are presented to show the superior of the proposed design over its existing counterparts.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {895–907},
numpages = {13}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.1007/978-3-030-40907-4_14,
author = {Mahanta, Prabal and Chouta, Suchin},
title = {Translating a Legacy Stack to Microservices Using a Modernization Facade with Performance Optimization for Container Deployments},
year = {2019},
isbn = {978-3-030-40906-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-40907-4_14},
doi = {10.1007/978-3-030-40907-4_14},
abstract = {We often find it challenging to translate a legacy system when the software business is critical. Adding to the misery of technical debt is the “Broken Window” concept which adds more complexity in exercising dynamic context resolutions for independent services alongside governance and data management. This often leads to a maze of disoriented services with high interdependency. To seriously adopt “Operate what you Build” phenomena, we need a granular facade approach to understand the business requirement and translate it to the architectural operators. The paper tries to provide an approach to establish platform independent interfaces, bounded domain contexts, eliminating non-critical legacy components and incremental quality aware methods to translate a legacy system to microservices. Along with the architectural objects, the paper will also present granular level of performance management of the translated application to consider factors like - system, container, network and application service itself.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Workshops: Confederated International Workshops: EI2N, FBM, ICSP, Meta4eS and SIAnA 2019, Rhodes, Greece, October 21–25, 2019, Revised Selected Papers},
pages = {143–154},
numpages = {12},
keywords = {Microservice, Performance, Facade, Modernization, Containers},
location = {Rhodes, Greece}
}

@article{10.1016/j.parco.2019.102545,
author = {Nagasaka, Yusuke and Matsuoka, Satoshi and Azad, Ariful and Bulu\c{c}, Ayd\i{}n},
title = {Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0167-8191},
url = {https://doi.org/10.1016/j.parco.2019.102545},
doi = {10.1016/j.parco.2019.102545},
journal = {Parallel Comput.},
month = dec,
numpages = {13},
keywords = {Sparse matrix, SpGEMM, Intel KNL}
}

@article{10.1007/s10922-020-09583-4,
author = {Awad, Mohamad Khattar and Ahmed, Marwa Hassan Hafez and Almutairi, Ali F. and Ahmad, Imtiaz},
title = {Machine Learning-Based Multipath Routing for Software Defined Networks},
year = {2021},
issue_date = {Apr 2021},
publisher = {Plenum Press},
address = {USA},
volume = {29},
number = {2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-020-09583-4},
doi = {10.1007/s10922-020-09583-4},
abstract = {Network softwarization has recently been enabled via the software-defined networking (SDN) paradigm, which separates the data plane from control plane allowing for a flexible and centralized control of networks. This separation facilitates implementation of machine learning techniques for network management and optimization. In this work, a machine learning-based multipath routing (MLMR) framework is proposed for software-defined networks with quality-of-service (QoS) constraints and flow rules space constraints. The QoS-aware multipath routing problem in SDN is modeled as multicommodity network flow problem with side constraints, that is known to be NP-hard. The proposed framework utilizes network status estimates, and their corresponding routing configurations available at the network central controller to learn a mapping function between them. Once the mapping function is learned, it is applied on live-inputs of network status and routing requests to predict a multipath routing solutions in real-time. Performance evaluations of the MLMR framework on real traces of network traffic verify its accuracy and resilience to noise in training data. Furthermore, the MLMR framework demonstrates more than 98.99% improvement in computational efficiency.},
journal = {J. Netw. Syst. Manage.},
month = apr,
numpages = {30},
keywords = {Machine learning, Software defined networks, Software defined networking, Routing}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2807591.2807601,
author = {Yu, Chenhan D. and Huang, Jianyu and Austin, Woody and Xiao, Bo and Biros, George},
title = {Performance optimization for the k-nearest neighbors kernel on x86 architectures},
year = {2015},
isbn = {9781450337236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2807591.2807601},
doi = {10.1145/2807591.2807601},
abstract = {Nearest neighbor search is a cornerstone problem in computational geometry, non-parametric statistics, and machine learning. For N points, exhaustive search requires quadratic work, but many fast algorithms reduce the complexity for exact and approximate searches. The common kernel (kNN kernel) in all these algorithms solves many small-size problems exactly using exhaustive search. We propose an efficient implementation and performance analysis for the kNN kernel on x86 architectures. By fusing the distance calculation with the neighbor selection, we are able to utilize memory throughput. We present an analysis of the algorithm and explain parameter selection. We perform an experimental study varying the size of the problem, the dimension of the dataset, and the number of nearest neighbors. Overall we observe significant speedups. For example, when searching for 16 neighbors in a point dataset with 1.6 million points in 64 dimensions, our kernel is over 4 times faster than existing methods.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {7},
numpages = {12},
keywords = {data mining, high dimensional data analysis, high-performance computing, machine learning, nearest-neighbor problems, parallel algorithms},
location = {Austin, Texas},
series = {SC '15}
}

@article{10.1007/s11704-020-9441-1,
author = {Sun, Xiaobing and Zhou, Tianchi and Wang, Rongcun and Duan, Yucong and Bo, Lili and Chang, Jianming},
title = {Experience report: investigating bug fixes in machine learning frameworks/libraries},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {6},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-020-9441-1},
doi = {10.1007/s11704-020-9441-1},
abstract = {Machine learning (ML) techniques and algorithms have been successfully and widely used in various areas including software engineering tasks. Like other software projects, bugs are also common in ML projects and libraries. In order to more deeply understand the features related to bug fixing in ML projects, we conduct an empirical study with 939 bugs from five ML projects by manually examining the bug categories, fixing patterns, fixing scale, fixing duration, and types of maintenance. The results show that (1) there are commonly seven types of bugs in ML programs; (2) twelve fixing patterns are typically used to fix the bugs in ML programs; (3) 68.80% of the patches belong to micro-scale-fix and small-scale-fix; (4) 66.77% of the bugs in ML programs can be fixed within one month; (5) 45.90% of the bug fixes belong to corrective activity from the perspective of software maintenance. Moreover, we perform a questionnaire survey and send them to developers or users of ML projects to validate the results in our empirical study. The results of our empirical study are basically consistent with the feedback from developers. The findings from the empirical study provide useful guidance and insights for developers and users to effectively detect and fix bugs in ML projects.},
journal = {Front. Comput. Sci.},
month = dec,
numpages = {16},
keywords = {bug fixing, machine learning project, empirical study, questionnaire survey}
}

@inproceedings{10.1145/3461702.3462527,
author = {Hopkins, Aspen and Booth, Serena},
title = {Machine Learning Practices Outside Big Tech: How Resource Constraints Challenge Responsible Development},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462527},
doi = {10.1145/3461702.3462527},
abstract = {Practitioners from diverse occupations and backgrounds are increasingly using machine learning (ML) methods. Nonetheless, studies on ML Practitioners typically draw populations from Big Tech and academia, as researchers have easier access to these communities. Through this selection bias, past research often excludes the broader, lesser-resourced ML community---for example, practitioners working at startups, at non-tech companies, and in the public sector. These practitioners share many of the same ML development difficulties and ethical conundrums as their Big Tech counterparts; however, their experiences are subject to additional under-studied challenges stemming from deploying ML with limited resources, increased existential risk, and absent access to in-house research teams. We contribute a qualitative analysis of 17 interviews with stakeholders from organizations which are less represented in prior studies. We uncover a number of tensions which are introduced or exacerbated by these organizations' resource constraints---tensions between privacy and ubiquity, resource management and performance optimization, and access and monopolization. Increased academic focus on these practitioners can facilitate a more holistic understanding of ML limitations, and so is useful for prescribing a research agenda to facilitate responsible ML development for all.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {134–145},
numpages = {12},
keywords = {ML developers, big tech, contextual inquiry, machine learning practice},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@inproceedings{10.5555/2888619.2888705,
author = {Kanezashi, Hiroki and Suzumura, Toyotaro},
title = {Performance optimization for agent-based traffic simulation by dynamic agent assignment},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {It is indispensable to make full use of parallel and distributed systems with increasing demands for large-scale traffic simulation, but problems remain about insufficient scalability due to costs of synchronization by load unbalancing among compute nodes. To tackle this problem, we propose performance optimization method for traffic simulations to underlying road networks preprocessed by graph contraction introducing dynamic re-assignment vehicles and cross points to threads and nodes based on time-series traffic congestion. By applying the optimization and running the simulation of the real-world Dublin city on 16 compute nodes of TSUBAME 2.5, the simulation performance has improved by 4 times with the proposed graph contraction method. We compared the effect of optimizations between the agent assignment method and existing adaptive synchronization method with comparison to regular 1 synchronization per step.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {757–766},
numpages = {10},
location = {Huntington Beach, California},
series = {WSC '15}
}

@article{10.1007/s00778-021-00665-6,
author = {Wang, Jin and Wu, Jiacheng and Li, Mingda and Gu, Jiaqi and Das, Ariyam and Zaniolo, Carlo},
title = {Formal semantics and high performance in declarative machine learning using Datalog},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00665-6},
doi = {10.1007/s00778-021-00665-6},
abstract = {With an escalating arms race to adopt machine learning (ML) in diverse application domains, there is an urgent need to support declarative machine learning over distributed data platforms. Toward this goal, a new framework is needed where users can specify ML tasks in a manner where programming is decoupled from the underlying algorithmic and system concerns. In this paper, we argue that declarative abstractions based on Datalog are natural fits for machine learning and propose a purely declarative ML framework with a Datalog query interface. We show that using aggregates in recursive Datalog programs entails a concise expression of ML applications, while providing a strictly declarative formal semantics. This is achieved by introducing simple conditions under which the semantics of recursive programs is guaranteed to be equivalent to that of aggregate-stratified ones. We further provide specialized compilation and planning techniques for semi-naive fixpoint computation in the presence of aggregates and optimization strategies that are effective on diverse recursive programs and distributed data platforms. To test and demonstrate these research advances, we have developed a powerful and user-friendly system on top of Apache Spark. Extensive evaluations on large-scale datasets illustrate that this approach will achieve promising performance gains while improving both programming flexibility and ease of development and deployment for ML applications.},
journal = {The VLDB Journal},
month = may,
pages = {859–881},
numpages = {23},
keywords = {Datalog, Declarative machine learning, Apache spark, Scalability}
}

@article{10.1007/s10618-021-00781-5,
author = {Ottervanger, Gilles and Baratchi, Mitra and Hoos, Holger H.},
title = {MultiETSC: automated machine learning for early time series classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {6},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-021-00781-5},
doi = {10.1007/s10618-021-00781-5},
abstract = {Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, “na\"{\i}ve” fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36).},
journal = {Data Min. Knowl. Discov.},
month = nov,
pages = {2602–2654},
numpages = {53},
keywords = {Early classification, Time series classification, Automated machine learning}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@article{10.1007/s00500-015-1892-1,
author = {Chen, Zhenxiang and Liu, Zhusong and Peng, Lizhi and Wang, Lin and Zhang, Lei},
title = {A novel semi-supervised learning method for Internet application identification},
year = {2017},
issue_date = {April     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {8},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-1892-1},
doi = {10.1007/s00500-015-1892-1},
abstract = {Several methods based on port, payload, and transport layer features have been proposed to detect, identify, and manage Internet traffic. The diminished effectiveness of port-based identification and overheads of deep packet inspection methods motivated us to identify Internet traffic by combining distinctive flow characteristics with the machine learning method. However, the abundant ground truth Internet traffic, which is important for building a supervised classifier, is difficult to be obtained in real conditions. In this study, we propose a semi-supervised learning method that combines further division of recognition space technique with data gravitation theory. The further division of recognition space classifier is a powerful multi-classification tool that can be helpful for multi-application identification. The data gravitation may reveal the underlying data space structure from unlabeled data, and thus, it is integrated into the classification to develop a better classifier. The experimental results on the real Internet application traffic datasets demonstrate the advantages of our proposed work.},
journal = {Soft Comput.},
month = apr,
pages = {1963–1975},
numpages = {13},
keywords = {Data gravitation, Internet traffic classification, Recognition space, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-642-41278-3_64,
author = {Pauk\v{s}t\u{a}\'{z}, Andrius},
title = {Genetic Algorithm on GPU Performance Optimization Issues},
year = {2013},
isbn = {9783642412776},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41278-3_64},
doi = {10.1007/978-3-642-41278-3_64},
abstract = {The aim of this paper is to investigate genetic algorithm execution on graphics processing unit performance issues and to develop techniques on how to speed up execution by optimizing algorithm execution path and data allocation. The paper presents methods to improve genetic algorithm performance by achieving higher hardware utilization and efficient task distribution between a graphics processing unit and a central processing unit.},
booktitle = {Proceedings of the 14th International Conference on Intelligent Data Engineering and Automated Learning --- IDEAL 2013 - Volume 8206},
pages = {529–536},
numpages = {8},
keywords = {genetic algorithms, graphics processing unit, parallel computing},
location = {Hefei, China},
series = {IDEAL 2013}
}

@article{10.1007/s11277-013-1053-8,
author = {Wu, Di and Zhu, Gang and Ai, Bo},
title = {Network Performance Optimization in Constrained Queueing Systems},
year = {2013},
issue_date = {September 2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {72},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-013-1053-8},
doi = {10.1007/s11277-013-1053-8},
abstract = {Most of literature assumed infinite buffer for users, which, however, is not practical in real networks. In this paper, we investigate downlink resource scheduling with constrained queueing. We first formulate an optimization model with the objective of maximizing the system rate under a limited queue length. Then two scheduling methods are proposed to solve this problem. One scheme is based on a virtual alarming threshold; the other is a prediction-based scheme. However, they still suffer from unfairness among rate allocations. To improve performance with respect to rate fairness, the factor of rate fairness is introduced into this optimization formulation finally. Numerical results are presented to demonstrate the efficiency of the proposed scheduling methods in terms of average system rate, maximum queue length and rate fairness, compared to some existing methods.},
journal = {Wirel. Pers. Commun.},
month = sep,
pages = {1023–1042},
numpages = {20},
keywords = {Constrained optimization, Fairness, Prediction, Queue, Scheduling}
}

@inproceedings{10.1145/2593501.2593508,
author = {Ravindran, Kaliappa and Adiththan, Arun},
title = {Verification of non-functional properties of cloud-based distributed system services},
year = {2014},
isbn = {9781450328586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593501.2593508},
doi = {10.1145/2593501.2593508},
abstract = {For distributed system services implemented on a cloud, system verification assumes added importance because of third-party control of cloud resources and the attendant problems of faults, QoS degradations, and security violations. Our paper focuses on a "model-based assessment" to reason about the non-functional properties of a cloud-based distributed system using observational agents. Our approach is corroborated by measurements on system-level prototypes and simulation analysis of system models in the face of hostile environment conditions. A case study of CDN realized on cloud infrastructures is also described.},
booktitle = {Proceedings of the 9th International Workshop on Automation of Software Test},
pages = {43–49},
numpages = {7},
keywords = {Model-aided system simula tion, Probabilistic system guarantees, Service-level compliance checks, System testing &amp; certification, heuristics for scenario generation},
location = {Hyderabad, India},
series = {AST 2014}
}

@inproceedings{10.1007/978-3-030-29959-0_1,
author = {Marc, Tilen and Stopar, Miha and Hartman, Jan and Bizjak, Manca and Modic, Jolanda},
title = {Privacy-Enhanced Machine Learning with Functional Encryption},
year = {2019},
isbn = {978-3-030-29958-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29959-0_1},
doi = {10.1007/978-3-030-29959-0_1},
abstract = {Functional encryption is a generalization of public-key encryption in which possessing a secret functional key allows one to learn a function of what the ciphertext is encrypting. This paper introduces the first fully-fledged open source cryptographic libraries for functional encryption. It also presents how functional encryption can be used to build efficient privacy-enhanced machine learning models and it provides an implementation of three prediction services that can be applied on the encrypted data. Finally, the paper discusses the advantages and disadvantages of the alternative approach for building privacy-enhanced machine learning models by using homomorphic encryption.},
booktitle = {Computer Security – ESORICS 2019: 24th European Symposium on Research in Computer Security, Luxembourg, September 23–27, 2019, Proceedings, Part I},
pages = {3–21},
numpages = {19},
keywords = {Functional encryption, Cryptographic library, Machine learning, Homomorphic encryption, Privacy},
location = {Luxembourg, Luxembourg}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@article{10.1007/s10115-017-1144-z,
author = {Chen, Zhiyuan and Khoa, Le Dinh and Teoh, Ee Na and Nazir, Amril and Karuppiah, Ettikan Kandasamy and Lam, Kim Sim},
title = {Machine learning techniques for anti-money laundering (AML) solutions in suspicious transaction detection: a review},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {57},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1144-z},
doi = {10.1007/s10115-017-1144-z},
abstract = {Money laundering has been affecting the global economy for many years. Large sums of money are laundered every year, posing a threat to the global economy and its security. Money laundering encompasses illegal activities that are used to make illegally acquired funds appear legal and legitimate. This paper aims to provide a comprehensive survey of machine learning algorithms and methods applied to detect suspicious transactions. In particular, solutions of anti-money laundering typologies, link analysis, behavioural modelling, risk scoring, anomaly detection, and geographic capability have been identified and analysed. Key steps of data preparation, data transformation, and data analytics techniques have been discussed; existing machine learning algorithms and methods described in the literature have been categorised, summarised, and compared. Finally, what techniques were lacking or under-addressed in the existing research has been elaborated with the purpose of pinpointing future research directions.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {245–285},
numpages = {41},
keywords = {Anomaly detection, Anti-money laundering, Anti-money laundering typologies, Behavioural modelling, Data mining methods and algorithms, Geographic capability, Link analysis, Risk scoring, Supervised learning, Unsupervised learning}
}

@inproceedings{10.1109/ICSE43902.2021.00024,
author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Are Machine Learning Cloud APIs Used Correctly?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00024},
doi = {10.1109/ICSE43902.2021.00024},
abstract = {Machine learning (ML) cloud APIs enable developers to easily incorporate learning solutions into software systems. Unfortunately, ML APIs are challenging to use correctly and efficiently, given their unique semantics, data requirements, and accuracy-performance tradeoffs. Much prior work has studied how to develop ML APIs or ML cloud services, but not how open-source applications are using ML APIs. In this paper, we manually studied 360 representative open-source applications that use Google or AWS cloud-based ML APIs, and found 70% of these applications contain API misuses in their latest versions that degrade functional, performance, or economical quality of the software. We have generalized 8 anti-patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ML API misuses.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {125–137},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1007/978-3-030-55583-2_28,
author = {Wozniak, Ernest and C\^{a}rlan, Carmen and Acar-Celik, Esra and Putzer, Henrik J.},
title = {A Safety Case Pattern for Systems with Machine Learning Components},
year = {2020},
isbn = {978-3-030-55582-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55583-2_28},
doi = {10.1007/978-3-030-55583-2_28},
abstract = {Several standards from the domain of safety critical systems, in order to support the argumentation of the safety assurance of a system under development, recommend the construction of a safety case. This activity is guided by the objectives to be met, recommended or required by the standards along the safety lifecycle. Ongoing attempts to use Machine Learning (ML) for safety critical functionality revealed certain deficits. For instance, the widely recognized standard for functional safety of automotive systems, ISO 26262, which can be used as a basis to construct a safety case, does not reason about ML. To this end, the goal of this work is to provide a pattern for arguing about the correct implementation of safety requirements in system components based on ML. The pattern is integrated within an overall encompassing approach for safety case generation for automotive systems and its applicability is showcased on a pedestrian avoidance system.},
booktitle = {Computer Safety, Reliability, and Security. SAFECOMP 2020 Workshops: DECSoS 2020, DepDevOps 2020, USDAI 2020, and WAISE 2020, Lisbon, Portugal, September 15, 2020, Proceedings},
pages = {370–382},
numpages = {13},
keywords = {Machine learning, Safety case, ISO 26262, GSN},
location = {Lisbon, Portugal}
}

@phdthesis{10.5555/AAI28260911,
author = {Zhang, Jiyuan and Garland, Michael and Gibbons, Phil and Low, Tze Meng},
advisor = {Franz, Franchetti,},
title = {Accelerating the “Motifs” in Machine Learning on Modern Processors},
year = {2020},
isbn = {9798569966530},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {The booming growth in AI and machine learning is drastically reshaping the landscape of high performance computing. Traditional HPC addresses scientific problems that are driven by simulation, modeling and analysis in science domains. Algorithms like linear algebra and methods like differential equations are at the core of solutions to such problems. However, emerging machine learning tasks rest on a new set of algorithms and models. The computation and data movement patterns inherent in learning algorithms cannot be directly mapped to the computation motifs in the physics, chemistry, or biology simulation problems. As a result, the high performance libraries that originate in the traditional scientific domains thus cannot be straightforwardly applied to emerging ML tasks to deliver required performance.This thesis focuses on performance optimizations of computation kernels in emerging machine learning applications, spanning across a diverse range from dense, regular to sparse and irregular kernels. In this work, we demonstrate how code specialization and generation together with expert-built performance models and learned dispatch strategies can together enable ML motifs to achieve better performance on modern processors.First, we investigate the performance optimization of dense kernels with a focus on the convolutional neural networks (CNN). The computation of convolution layers in deep neural networks typically relies on high performance matrix-multiplication routines to improve performance. However, these routines are not optimized for performing convolution. Extra memory overhead is incurred due to data transformation, and the performance obtained is also less than conventionally expected. We demonstrate that direct convolution, when implemented correctly, eliminates all memory overhead, and yields performance that is between 10% to 4x better than existing high performance implementations on conventional and embedded CPU architectures. We present a model-guided optimization approach which utilizes the characteristics of system architectures to guide the optimization choices of loop ordering, blocking, and memory layout transformation. We show that a high performance direct convolution exhibits better performance scaling than expert-tuned matrix implementation, i.e. suffers less performance drop, when increasing the number of threads.Sparse kernel is an equally important computation kernel appearing in many machine learning applications such as graph analytics and genetic sequencing. One factor that prevents sparse kernels from achieving high performance on modern processors results from the prohibitively large number of different implementations and data structures for sparse problems. We start with the observation that the complicated sparse computations can be distilled into primitive set of operators such as join, merge, and difference. To accelerate those operators on modern processors with data parallelism, we propose a vectorization and code specialization approach which can eliminate the control divergences of these operators. Next, we explore the design space for vectorization on CPUs with various vector width, based on which we present the code generation algorithm that takes the data width and operations as input and generates various implementations. We then demonstrate the acceleration of the General Sparse Matrix-Matrix Multiplication (SpGEMM) on GPUs. We show how the SpGEMM implementation can leverage join/merge operators to compose a variety of implementations. Another challenge when optimizing sparse kernels is that their performance behavior is data dependent, while the input characteristics may change online during iterative updates. To leverage the different implementations offered by the code generator, we propose a low-overhead mechanism that collects the data characteristic information to learn online dispatch decisions over iterations.Overall, in this thesis, we demonstrate the interplay of code specialization and generation, together with performance modeling, learned dispatch, can enable high performance kernels for the emerging machine learning applications.},
note = {AAI28260911}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@article{10.1145/2588788,
author = {Rane, Ashay and Browne, James},
title = {Enhancing Performance Optimization of Multicore/Multichip Nodes with Data Structure Metrics},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2329-4949},
url = {https://doi.org/10.1145/2588788},
doi = {10.1145/2588788},
abstract = {Program performance optimization is usually based solely on measurements of execution behavior of code segments using hardware performance counters. However, memory access patterns are critical performance limiting factors for today's multicore chips where performance is highly memory bound. Therefore diagnoses and selection of optimizations based only on measurements of the execution behavior of code segments are incomplete because they do not incorporate knowledge of memory access patterns and behaviors. This article presents a low-overhead tool (MACPO) that captures memory traces and computes metrics for the memory access behavior of source-level (C, C++, Fortran) data structures. MACPO explicitly targets the measurement and metrics important to performance optimization for multicore chips. The article also presents a complete process for integrating measurement and analyses of code execution with measurements and analyses of memory access patterns and behaviors for performance optimization, specifically targeting multicore chips and multichip nodes of clusters. MACPO uses more realistic cache models for computation of latency metrics than those used by previous tools. Evaluation of the effectiveness of adding memory access behavior characteristics of data structures to performance optimization was done on subsets of the ASCI, NAS and Rodinia parallel benchmarks and two versions of one application program from a domain not represented in these benchmarks. Adding characteristics of the behavior of data structures enabled easier diagnoses of bottlenecks and more accurate selection of appropriate optimizations than with only code centric behavior measurements. The performance gains ranged from a few percent to 38 percent.},
journal = {ACM Trans. Parallel Comput.},
month = may,
articleno = {3},
numpages = {20},
keywords = {Performance, data structures, memory, optimization}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@inproceedings{10.1145/3299869.3314050,
author = {Agrawal, Pulkit and Arya, Rajat and Bindal, Aanchal and Bhatia, Sandeep and Gagneja, Anupriya and Godlewski, Joseph and Low, Yucheng and Muss, Timothy and Paliwal, Mudit Manu and Raman, Sethu and Shah, Vishrut and Shen, Bochao and Sugden, Laura and Zhao, Kaiyu and Wu, Ming-Chuan},
title = {Data Platform for Machine Learning},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314050},
doi = {10.1145/3299869.3314050},
abstract = {In this paper, we present a purpose-built data management system, MLdp, for all machine learning (ML) datasets. ML applications pose some unique requirements different from common conventional data processing applications, including but not limited to: data lineage and provenance tracking, rich data semantics and formats, integration with diverse ML frameworks and access patterns, trial-and-error driven data exploration and evolution, rapid experimentation, reproducibility of the model training, strict compliance and privacy regulations, etc. Current ML systems/services, often named MLaaS, to-date focus on the ML algorithms, and offer no integrated data management system. Instead, they require users to bring their own data and to manage their own data on either blob storage or on file systems. The burdens of data management tasks, such as versioning and access control, fall onto the users, and not all compliance features, such as terms of use, privacy measures, and auditing, are available. MLdp offers a minimalist and flexible data model for all varieties of data, strong version management to guarantee re-producibility of ML experiments, and integration with major ML frameworks. MLdp also maintains the data provenance to help users track lineage and dependencies among data versions and models in their ML pipelines. In addition to table-stake features, such as security, availability and scalability, MLdp's internal design choices are strongly influenced by the goal to support rapid ML experiment iterations, which cycle through data discovery, data exploration, feature engineering, model training, model evaluation, and back to data discovery. The contributions of this paper are: 1) to recognize the needs and to call out the requirements of an ML data platform, 2) to share our experiences in building MLdp by adopting existing database technologies to the new problem as well as by devising new solutions, and 3) to call for actions from our communities on future challenges.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1803–1816},
numpages = {14},
keywords = {data platform, data streaming access, data version control, dataset management for machine learning, physical data layout},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.5555/2755753.2757163,
author = {Chen, Zhuo and Marculescu, Diana},
title = {Distributed reinforcement learning for power limited many-core system performance optimization},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {As power density emerges as the main constraint for many-core systems, controlling power consumption under the Thermal Design Power (TDP) while maximizing the performance becomes increasingly critical. To dynamically save power, Dynamic Voltage Frequency Scaling (DVFS) techniques have proved to be effective and are widely available commercially. In this paper, we present an On-line Distributed Reinforcement Learning (OD-RL) based DVFS control algorithm for many-core system performance improvement under power constraints. At the finer grain, a per-core Reinforcement Learning (RL) method is used to learn the optimal control policy of the Voltage/Frequency (VF) levels in a system model-free manner. At the coarser grain, an efficient global power budget reallocation algorithm is used to maximize the overall performance. The experiments show that compared to the state-of-the-art algorithms: 1) OD-RL produces up to 98% less budget overshoot, 2) up to 44.3x better throughput per over-the-budget energy and up to 23% higher energy efficiency, and 3) two orders of magnitude speedup over state-of-the-art techniques for systems with hundreds of cores.},
booktitle = {Proceedings of the 2015 Design, Automation &amp; Test in Europe Conference &amp; Exhibition},
pages = {1521–1526},
numpages = {6},
location = {Grenoble, France},
series = {DATE '15}
}

@article{10.1145/3398020,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398020},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {82},
numpages = {47},
keywords = {IoT, deep learning, machine learning, orchestration}
}

@inproceedings{10.1145/3386263.3407649,
author = {Bavikadi, Sathwika and Sutradhar, Purab Ranjan and Khasawneh, Khaled N. and Ganguly, Amlan and Pudukotai Dinakarrao, Sai Manoj},
title = {A Review of In-Memory Computing Architectures for Machine Learning Applications},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3407649},
doi = {10.1145/3386263.3407649},
abstract = {to meet the extensive computational load presented by the rapidly growing Machine Learning (ML) and Artificial Intelligence (AI) algorithms such as Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs). In order to obtain hardware solutions to meet the low-latency and high-throughput computational demands from these algorithms, Non-Von Neumann computing architectures such as In-memory Computing (IMC)/ Processing-in-memory (PIM) are being extensively researched and experimented with. In this survey paper, we analyze and review pioneer IMC/PIM works designed to accelerate ML algorithms such as DNNs and CNNs. We investigate different architectural aspects and dimensions of these works and provide our comparative evaluations. Furthermore, we discuss challenges and limitations in IMC research and also present feasible directions based on our observations and insight.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {89–94},
numpages = {6},
keywords = {artificial intelligence, cnn, dnn, in-memory computing, machine learning, non-von-neumann architectures, processing-in-memory},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@inproceedings{10.1145/3447654.3447655,
author = {Yao, Qinglong and Wu, Yingfei and Yuan, Zhenming and Sun, Xiaoyan},
title = {OMAI Platform Based on Machine Learning},
year = {2021},
isbn = {9781450388566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447654.3447655},
doi = {10.1145/3447654.3447655},
abstract = {As far as medical data is concerned, data analysis software is an effective tool to process and analyze medical data, but it still has some problems, such as high learning cost, poor medical text processing effect, single function and so on. For this reason, this paper designs and implements the OMAI platform, which is an open)O(medical)M(data platform based on Web and combined with artificial intelligence)AI( technology. The OMAI system implements the functions of medical data collection, storage, processing, analysis and utilization. Through the application of medical examples, the platform can effectively complete the process of medical data from processing to utilization, so as to provide help for doctors.},
booktitle = {Proceedings of the 2020 9th International Conference on Networks, Communication and Computing},
pages = {1–6},
numpages = {6},
keywords = {Medical Data, OMAI, Web},
location = {Tokyo, Japan},
series = {ICNCC '20}
}

@article{10.1016/j.jss.2013.09.013,
author = {Bezemer, Cor-Paul and Zaidman, Andy},
title = {Performance optimization of deployed software-as-a-service applications},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.09.013},
doi = {10.1016/j.jss.2013.09.013},
abstract = {The goal of performance maintenance is to improve the performance of a software system after delivery. As the performance of a system is often characterized by unexpected combinations of metric values, manual analysis of performance is hard in complex systems. In this paper, we propose an approach that helps performance experts locate and analyze spots - so called performance improvement opportunities (PIOs) - for possible performance improvements. PIOs give performance experts a starting point for performance improvements, e.g., by pinpointing the bottleneck component. The technique uses a combination of association rules and performance counters to generate the rule coverage matrix, a matrix which assists with the bottleneck detection. In this paper, we evaluate our technique in two case studies. In the first one, we show that our technique is accurate in detecting the time frame during which a PIO occurs. In the second one, we show that the starting point given by our approach is indeed useful and assists a performance expert in diagnosing the bottleneck component in a system with high precision.},
journal = {J. Syst. Softw.},
month = jan,
pages = {87–103},
numpages = {17},
keywords = {Performance analysis, Performance maintenance}
}

@article{10.1145/3450494,
author = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi (Jason) and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
title = {A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3450494},
doi = {10.1145/3450494},
abstract = {The predominant paradigm for using machine learning models on a device is to train a model in the cloud and perform inference using the trained model on the device. However, with increasing numbers of smart devices and improved hardware, there is interest in performing model training on the device. Given this surge in interest, a comprehensive survey of the field from a device-agnostic perspective sets the stage for both understanding the state of the art and for identifying open challenges and future avenues of research. However, on-device learning is an expansive field with connections to a large number of related topics in AI and machine learning (including online learning, model adaptation, one/few-shot learning, etc.). Hence, covering such a large number of topics in a single survey is impractical. This survey finds a middle ground by reformulating the problem of on-device learning as resource constrained learning where the resources are compute and memory. This reformulation allows tools, techniques, and algorithms from a wide variety of research areas to be compared equitably. In addition to summarizing the state of the art, the survey also identifies a number of challenges and next steps for both the algorithmic and theoretical aspects of on-device learning.},
journal = {ACM Trans. Internet Things},
month = jul,
articleno = {15},
numpages = {49}
}

@inproceedings{10.5555/2001252.2001263,
author = {Lau, Eric and Miller, Jason E. and Choi, Inseok and Yeung, Donald and Amarasinghe, Saman and Agarwal, Anant},
title = {Multicore performance optimization using partner cores},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {As the push for parallelism continues to increase the number of cores on a chip, system design has become incredibly complex; optimizing for performance and power efficiency is now nearly impossible for the application programmer. To assist the programmer, a variety of techniques for optimizing performance and power at runtime have been developed, but many employ the use of speculative threads or performance counters. These approaches result in stolen cycles, or the use of an extra core, and such expensive penalties can greatly reduce the potential gains.At the same time that general purpose processors have grown larger and more complex, technologies for smaller embedded processors have pushed towards energy efficiency. In this paper, we combine the two and introduce the concept of Partner Cores: low-area, low-power cores paired with larger, faster compute cores. A partner core is tightly coupled to each main processing core, allowing it to perform various optimizations and functions that are impossible on a traditional chip multiprocessor. This paper demonstrates that optimization code running on a partner core can increase performance and provide a net improvement in power efficiency.},
booktitle = {Proceedings of the 3rd USENIX Conference on Hot Topic in Parallelism},
pages = {11},
numpages = {1},
location = {Berkeley, CA},
series = {HotPar'11}
}

@inproceedings{10.1145/3464298.3494884,
author = {S\'{a}nchez-Artigas, Marc and Sarroca, Pablo Gimeno},
title = {Experience Paper: Towards enhancing cost efficiency in serverless machine learning training},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3494884},
doi = {10.1145/3464298.3494884},
abstract = {Function-as-a-Service (FaaS) has raised a growing interest in how to "tame" serverless to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional "serverful" computing. To help in this task, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two key optimizations: a significance filter and a scale-in auto-tuner, and leverages them to specialize model training to the FaaS model. Our results certify that MLLess can be 15X faster than serverful ML systems [24] at a lower cost for ML models (such as sparse logistic regression and matrix factorization) that exhibit fast convergence.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {210–222},
numpages = {13},
keywords = {machine learnig, serverless computing},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@inproceedings{10.5555/2606265.2606975,
author = {Nguyen, Vu Thien Nga and Kirner, Raimund},
title = {A Heuristic Strategy for Performance Optimisation of Stream Programs},
year = {2013},
isbn = {9781479920815},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we present a design of a stream scheduler aiming at optimising throughput and latency of streaming programs with dynamic program structures. The scheduler uses heuristics based on the demand of data in communications streams. As we address dynamic structures of streaming programs, the particular challenge is that static scheduling based on formal constraints or probabilities is not applicable.},
booktitle = {Proceedings of the 2013 International Conference on Parallel and Distributed Systems},
pages = {430–431},
numpages = {2},
series = {ICPADS '13}
}

@inproceedings{10.1145/2568088.2576800,
author = {Liu, Yanbin and Dube, Parijat and Gray, Scott C.},
title = {Run-time performance optimization of a BigData query language},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2576800},
doi = {10.1145/2568088.2576800},
abstract = {JAQL is a query language for large-scale data that connects BigData analytics and MapReduce framework together. Also an IBM product, JAQL's performance is critical for IBM InfoSphere BigInsights, a BigData analytics platform. In this paper, we report our work on improving JAQL performance from multiple perspectives. We explore the parallelism of JAQL, profile JAQL for performance analysis, identify I/O as the dominant performance bottleneck, and improve JAQL performance with an emphasis on reducing I/O data size and increasing (de)serialization efficiency. With TPCH benchmark on a simple Hadoop cluster, we report up to 2x performance improvements in JAQL with our optimization fixes.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {239–246},
numpages = {8},
keywords = {biginsights, i/o optimization, jaql, java performance, mapreduce, multi-thread},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@article{10.1007/s10586-019-02912-6,
author = {Ta, Nguyen Binh Duong},
title = {
$$FC^{2}$$: cloud-based cluster provisioning for distributed machine learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-02912-6},
doi = {10.1007/s10586-019-02912-6},
abstract = {Training large, complex machine learning models such as deep neural networks with big data requires powerful computing clusters, which are costly to acquire, use and maintain. As a result, many machine learning researchers turn to cloud computing services for on-demand and elastic resource provisioning capabilities. Two issues have arisen from this trend: (1) if not configured properly, training models on cloud-based clusters could incur significant cost and time, and (2) many researchers in machine learning tend to focus more on model and algorithm development, so they may not have the time or skills to deal with system setup, resource selection and configuration. In this work, we propose and implement $$FC^{2}$$: a system for fast, convenient and cost-effective distributed machine learning over public cloud resources. Central to the effectiveness of $$FC^{2}$$ is the ability to recommend an appropriate resource configuration in terms of cost and execution time for a given model training task. Our approach differs from previous work in that it does not need to manually analyze the code and dataset of the training task in advance. The recommended resource configuration can then be deployed and managed automatically by $$FC^2$$ until the training task is completed. We have conducted extensive experiments with an implementation of $$FC^2$$, using real-world deep neural network models and datasets. The results demonstrate the effectiveness of our approach, which could produce cost saving of up to 80% while maintaining similar training performance compared to much more expensive resource configurations.},
journal = {Cluster Computing},
month = dec,
pages = {1299–1315},
numpages = {17},
keywords = {Distributed machine learning, Cloud-based clusters, Resource recommendation, Cluster deployment}
}

@article{10.1007/s00500-020-05348-y,
author = {EL Mazgualdi, Choumicha and Masrour, Tawfik and El Hassani, Ibtissam and Khdoudi, Abdelmoula},
title = {Machine learning for KPIs prediction: a case study of the overall equipment effectiveness within the automotive industry},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {4},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05348-y},
doi = {10.1007/s00500-020-05348-y},
abstract = {Key performance indicators are tools for management, decision support and forecasting; they reflect the strategy and vision of the company in terms of objectives and allow to always staying in step with the expectations of the stakeholders. Accurate forecasting of the indicators allows decisions to be reoriented to ensure performance optimization while reducing both cost and effort. This paper aims to apply different machine learning methods, namely support vector regression, optimized support vector regression (using genetic algorithm), random forest, extreme gradient boosting and deep learning to predict the overall equipment effectiveness as a case study. We will make use of several configurations of the listed models in order to provide a wide field of comparison. The data used to train our models were provided by an automotive cable production industry. The result shows that the configuration in which we used cross-validation technique, and we performed a duly splitting of data, provides predictor models with the better performances.},
journal = {Soft Comput.},
month = feb,
pages = {2891–2909},
numpages = {19},
keywords = {Machine learning, Key performance indicators, Overall equipment effectiveness, Prediction, Improvement}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@article{10.1016/j.jpdc.2013.02.009,
author = {Benoit, Anne and Dufoss\'{e}, Fanny and Girault, Alain and Robert, Yves},
title = {Reliability and performance optimization of pipelined real-time systems},
year = {2013},
issue_date = {June, 2013},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {73},
number = {6},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2013.02.009},
doi = {10.1016/j.jpdc.2013.02.009},
abstract = {We consider pipelined real-time systems that consist of a chain of tasks executing on a distributed platform. The processing of the tasks is pipelined: each processor executes only one interval of consecutive tasks. We are interested in minimizing both the input-output latency and the period of application mapping. For dependability reasons, we are also interested in maximizing the reliability of the system. We therefore assign several processors to each interval of tasks, so as to increase the reliability of the system. Both processors and communication links are unreliable and subject to transient failures. We assume that the arrival of the failures follows a constant parameter Poisson law, and that the failures are statistically independent events. We study several variants of this multiprocessor mapping problem, with several hypotheses on the target platform (homogeneous/heterogeneous speeds and/or failure rates). We provide NP-hardness complexity results, and optimal mapping algorithms for polynomial problem instances. Efficient heuristics are presented to solve the general case, and experimental results are provided.},
journal = {J. Parallel Distrib. Comput.},
month = jun,
pages = {851–865},
numpages = {15},
keywords = {Complexity results, Interval mapping, Multi-criteria (reliability, Pipelined real-time systems, latency, period) optimization}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@article{10.1007/s10845-020-01567-0,
author = {Baturynska, Ivanna and Martinsen, Kristian},
title = {Prediction of geometry deviations in additive manufactured parts: comparison of linear regression with machine learning algorithms},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01567-0},
doi = {10.1007/s10845-020-01567-0},
abstract = {Dimensional accuracy in additive manufacturing (AM) is still an issue compared with the tolerances for injection molding. In order to make AM suitable for the medical, aerospace, and automotive industries, geometry variations should be controlled and managed with a tight tolerance range. In the previously published article, the authors used statistical analysis to develop linear models for the prediction of dimensional features of laser-sintered specimens. Two identical builds with the same material, process, and build parameters were produced, resulting in 434 samples for mechanical testing (ISO 527-2 1BA). The developed linear models had low accuracy, and therefore needed an application of more advanced data analysis techniques. In this work, machine learning techniques are applied for the same data, and results are compared with the previously reported linear models. The linear regression model is the best for width. Multilayer perceptron and gradient boost regressor models have outperformed other for thickness and length. The recommendations on how the developed models can be used in the future are proposed.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {179–200},
numpages = {22},
keywords = {Additive manufacturing, PA12, Polyamide, Machine learning, Dimensional accuracy, Support vector regression, Decision tree regressor, Multilayer perceptron, Gradient boosting regressor}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00066,
author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {A replication of are machine learning cloud APIs used correctly},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00066},
doi = {10.1109/ICSE-Companion52605.2021.00066},
abstract = {This artifact aims to provide benchmark suite, data, and script used in our study "Are Machine Learning Cloud APIs Used Correctly?". We collected a suite of 360 non-trivial applications that use ML cloud APIs for manual study. We also developed checkers and tool to detect and fix API mis-uses. We hope this artifact can motivate and help future research to further tackle ML API mis-uses. All related data are available online.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {158–159},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/SEAMS.2019.00015,
author = {Jamshidi, Pooyan and C\'{a}mara, Javier and Schmerl, Bradley and K\"{a}stner, Christian and Garlan, David},
title = {Machine learning meets quantitative planning: enabling self-adaptation in autonomous robots},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00015},
doi = {10.1109/SEAMS.2019.00015},
abstract = {Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {39–50},
numpages = {12},
keywords = {artificial intelligence, machine learning, quantitative planning, robotics systems, self-adaptive systems},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@article{10.3233/JIFS-179915,
author = {Patnaik, Srikanta and Patnaik, Srikanta},
title = {Applied machine learning and management of volatility, uncertainty, complexity &amp; ambiguity (V.U.C.A)},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179915},
doi = {10.3233/JIFS-179915},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1409–1416},
numpages = {8}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@article{10.1016/j.neunet.2015.07.011,
author = {Yang, Jiachen and Ding, Zhiyong and Guo, Fei and Wang, Huogen and Hughes, Nick},
title = {A novel multivariate performance optimization method based on sparse coding and hyper-predictor learning},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {71},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2015.07.011},
doi = {10.1016/j.neunet.2015.07.011},
abstract = {In this paper, we investigate the problem of optimization of multivariate performance measures, and propose a novel algorithm for it. Different from traditional machine learning methods which optimize simple loss functions to learn prediction function, the problem studied in this paper is how to learn effective hyper-predictor for a tuple of data points, so that a complex loss function corresponding to a multivariate performance measure can be minimized. We propose to present the tuple of data points to a tuple of sparse codes via a dictionary, and then apply a linear function to compare a sparse code against a given candidate class label. To learn the dictionary, sparse codes, and parameter of the linear function, we propose a joint optimization problem. In this problem, the both the reconstruction error and sparsity of sparse code, and the upper bound of the complex loss function are minimized. Moreover, the upper bound of the loss function is approximated by the sparse codes and the linear function parameter. To optimize this problem, we develop an iterative algorithm based on descent gradient methods to learn the sparse codes and hyper-predictor parameter alternately. Experiment results on some benchmark data sets show the advantage of the proposed methods over other state-of-the-art algorithms.},
journal = {Neural Netw.},
month = nov,
pages = {45–54},
numpages = {10},
keywords = {Alternate optimization, Joint learning, Loss function, Multivariate performance measures, Pattern classification, Sparse coding}
}

@article{10.1145/3185515,
author = {Chen, Nan-Chen and Drouhard, Margaret and Kocielnik, Rafal and Suh, Jina and Aragon, Cecilia R.},
title = {Using Machine Learning to Support Qualitative Coding in Social Science: Shifting the Focus to Ambiguity},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3185515},
doi = {10.1145/3185515},
abstract = {Machine learning (ML) has become increasingly influential to human society, yet the primary advancements and applications of ML are driven by research in only a few computational disciplines. Even applications that affect or analyze human behaviors and social structures are often developed with limited input from experts outside of computational fields. Social scientists—experts trained to examine and explain the complexity of human behavior and interactions in the world—have considerable expertise to contribute to the development of ML applications for human-generated data, and their analytic practices could benefit from more human-centered ML methods. Although a few researchers have highlighted some gaps between ML and social sciences [51, 57, 70], most discussions only focus on quantitative methods. Yet many social science disciplines rely heavily on qualitative methods to distill patterns that are challenging to discover through quantitative data. One common analysis method for qualitative data is qualitative coding. In this article, we highlight three challenges of applying ML to qualitative coding. Additionally, we utilize our experience of designing a visual analytics tool for collaborative qualitative coding to demonstrate the potential in using ML to support qualitative coding by shifting the focus to identifying ambiguity. We illustrate dimensions of ambiguity and discuss the relationship between disagreement and ambiguity. Finally, we propose three research directions to ground ML applications for social science as part of the progression toward human-centered machine learning.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {9},
numpages = {20},
keywords = {Social scientists, ambiguity, computational social science, human-centered machine learning, machine learning, qualitative coding}
}

@inproceedings{10.1145/3487212.3487348,
author = {Esper, Khalil and Wildermann, Stefan and Teich, J\"{u}rgen},
title = {Enforcement FSMs: specification and verification of non-functional properties of program executions on MPSoCs},
year = {2021},
isbn = {9781450391276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487212.3487348},
doi = {10.1145/3487212.3487348},
abstract = {Many embedded system applications impose hard real-time, energy or safety requirements on corresponding programs typically concurrently executed on a given MPSoC target platform. Even when mutually isolating applications in space or time, the enforcement of such properties, e.g., by adjusting the number of processors allocated to a program or by scaling the voltage/frequency mode of involved processors, is a difficult problem to solve, particularly in view of typically largely varying environmental input (workload) per execution. In this paper, we formalize the related control problem using finite state machine models for the uncertain environment determining the workload, the system response (feedback), as well as the enforcer strategy. The contributions of this paper are as follows: a) Rather than trace-based simulation, the uncertain environment is modeled by a discrete-time Markov chain (DTMC) as a random process to characterize possible input sequences an application may experience. b) A number of important verification goals to analyze different enforcer FSMs are formulated in PCTL for the resulting stochastic verification problem, i.e., the likelihood of violating a timing or energy constraint, or the expected number of steps for a system to return to a given execution time corridor. c) Applying stochastic model checking, i.e., PRISM to analyze and compare enforcer FSMs in these properties, and finally d) proposing an approach for reducing the environment DTMC by partitioning equivalent environmental states (i.e., input states leading to an equal system response in each MPSoC mode) such that verification times can be reduced by orders of magnitude to just a few ms for real-world examples.},
booktitle = {Proceedings of the 19th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {21–31},
numpages = {11},
keywords = {MPSoC, Markov chain, PCTL, finite state machine, probabilistic model cheking, runtime requirement enforcement, verification},
location = {Virtual Event, China},
series = {MEMOCODE '21}
}

@inproceedings{10.1145/2503210.2503252,
author = {Kumar, Sidharth and Saha, Avishek and Vishwanath, Venkatram and Carns, Philip and Schmidt, John A. and Scorzelli, Giorgio and Kolla, Hemanth and Grout, Ray and Latham, Robert and Ross, Robert and Papkafa, Michael E. and Chen, Jacqueline and Pascucci, Valerio},
title = {Characterization and modeling of PIDX parallel I/O for performance optimization},
year = {2013},
isbn = {9781450323789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503210.2503252},
doi = {10.1145/2503210.2503252},
abstract = {Parallel I/O library performance can vary greatly in response to user-tunable parameter values such as aggregator count, file count, and aggregation strategy. Unfortunately, manual selection of these values is time consuming and dependent on characteristics of the target machine, the underlying file system, and the dataset itself. Some characteristics, such as the amount of memory per core, can also impose hard constraints on the range of viable parameter values. In this work we address these problems by using machine learning techniques to model the performance of the PIDX parallel I/O library and select appropriate tunable parameter values. We characterize both the network and I/O phases of PIDX on a Cray XE6 as well as an IBM Blue Gene/P system. We use the results of this study to develop a machine learning model for parameter space exploration and performance prediction.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {67},
numpages = {12},
keywords = {I/O &amp; network characterization, performance modeling},
location = {Denver, Colorado},
series = {SC '13}
}

@inproceedings{10.1007/978-3-030-41418-4_19,
author = {Khan, Muhammad Jahanzeb and Wang, Ruoyu and Sun, Daniel and Li, Guoqiang},
title = {Data Provenance Based System for&nbsp;Classification and Linear Regression in&nbsp;Distributed Machine Learning},
year = {2019},
isbn = {978-3-030-41417-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41418-4_19},
doi = {10.1007/978-3-030-41418-4_19},
abstract = {Nowadays, data provenance is widely used to increase the accuracy of machine learning models. However, facing the difficulties in information heredity, these models produce data association. Most of the studies in the field of data provenance are focused on specific domains. And there are only a few studies on a machine learning (ML) framework with distinct emphasis on the accurate partition of coherent and physical activities with implementation of ML pipelines for provenance. This paper presents a novel approach to usage of data provenance which is also called data provenance based system for classification and linear regression in distributed machine learning (DPMLR). To develop the comprehensive approach for data analysis and visualization based on a collective set of functions for various algorithms and provide the ability to run large scale graph analysis, we apply StellarGraph as our primary ML structure. The preliminary results on the complex data stream structure showed that the overall overhead is no more than 20%. It opens up opportunities for designing an integrated system which performs dynamic scheduling and network bounded synchronization based on the ML algorithm.},
booktitle = {Structured Object-Oriented Formal Language and Method: 9th International Workshop, SOFL+MSVL 2019, Shenzhen, China, November 5, 2019, Revised Selected Papers},
pages = {279–295},
numpages = {17},
keywords = {Data provenance, Machine learning, StellarGraph, Pipeline, Distributed computing},
location = {Shenzhen, China}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Feature-oriented software development, Measurement and optimization, Non-functional properties, SPL Conqueror, Software product lines}
}

@article{10.1177/1094342019842915,
author = {Dongarra, Jack and Tourancheau, Bernard and Endrei, Mark and Jin, Chao and Dinh, Minh Ngoc and Abramson, David and Poxon, Heidi and DeRose, Luiz and de Supinski, Bronis R},
title = {Statistical and machine learning models for optimizing energy in parallel applications},
year = {2019},
issue_date = {Nov 2019},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {33},
number = {6},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342019842915},
doi = {10.1177/1094342019842915},
abstract = {Rising power costs and constraints are driving a growing focus on the energy efficiency of high performance computing systems. The unique characteristics of a particular system and workload and their effect on performance and energy efficiency are typically difficult for application users to assess and to control. Settings for optimum performance and energy efficiency can also diverge, so we need to identify trade-off options that guide a suitable balance between energy use and performance. We present statistical and machine learning models that only require a small number of runs to make accurate Pareto-optimal trade-off predictions using parameters that users can control. We study model training and validation using several parallel kernels and more complex workloads, including Algebraic Multigrid (AMG), Large-scale Atomic Molecular Massively Parallel Simulator, and Livermore Unstructured Lagrangian Explicit Shock Hydrodynamics. We demonstrate that we can train the models using as few as 12 runs, with prediction error of less than 10%. Our AMG results identify trade-off options that provide up to 45% improvement in energy efficiency for around 10% performance loss. We reduce the sample measurement time required for AMG by 90%, from 13 h to 74 min.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {1079–1097},
numpages = {19},
keywords = {Energy efficiency, performance, regression modeling, machine learning, high performance computing}
}

@article{10.1007/s10723-021-09561-3,
author = {Nawrocki, Piotr and Osypanka, Patryk},
title = {Cloud Resource Demand Prediction using Machine Learning in the Context of QoS Parameters},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {1570-7873},
url = {https://doi.org/10.1007/s10723-021-09561-3},
doi = {10.1007/s10723-021-09561-3},
abstract = {Predicting demand for computing resources in any system is a vital task since it allows the optimized management of resources. To some degree, cloud computing reduces the urgency of accurate prediction as resources can be scaled on demand, which may, however, result in excessive costs. Numerous methods of optimizing cloud computing resources have been proposed, but such optimization commonly degrades system responsiveness which results in quality of service deterioration. This paper presents a novel approach, using anomaly detection and machine learning to achieve cost-optimized and QoS-constrained cloud resource configuration. The utilization of these techniques enables our solution to adapt to different system characteristics and different QoS constraints. Our solution was evaluated using a system located in Microsoft’s Azure cloud environment, and its efficiency in other providers’ computing clouds was estimated as well. Experiment results demonstrate a cost reduction ranging from 51% to 85% (for PaaS/IaaS) over the tested period.},
journal = {J. Grid Comput.},
month = jun,
numpages = {20},
keywords = {Cloud computing, Resource usage prediction, Anomaly detection, Machine learning, Quality of service, Resource cost optimization}
}

@article{10.1109/TCAD.2015.2504330,
author = {Cai, Ermao and Juan, Da-Cheng and Garg, Siddharth and Park, Jinpyo and Marculescu, Diana},
title = {Learning-Based Power/Performance Optimization for Many-Core Systems With Extended-Range Voltage/Frequency Scaling},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {35},
number = {8},
issn = {0278-0070},
url = {https://doi.org/10.1109/TCAD.2015.2504330},
doi = {10.1109/TCAD.2015.2504330},
abstract = {Near-threshold computing has emerged as a promising solution to significantly increase the energy efficiency of next-generation multicore systems. This paper evaluates and analyzes the behavior of dynamic voltage and frequency scaling for multicore systems operating under extended range: including near-threshold, nominal, and turbo modes. We adapt the model selection technique from machine learning to determine the relationship between performance and power. The theoretical results show that the resulting models satisfy convexity, which efficiently determines the optimal voltage/frequency operating points for: 1) minimizing energy consumption under throughput constraints or 2) maximizing throughput under a given power budget. We validate our models on FinFET-based chip-multiprocessors. Considering process variations (PVs), experimental results show that at 30% PV levels, our proposed method: 1) reduces energy consumption by 31.09% at iso-performance condition and 2) increases throughput by 11.46% at iso-power when compared with variation-agnostic nominal case.},
journal = {Trans. Comp.-Aided Des. Integ. Cir. Sys.},
month = aug,
pages = {1318–1331},
numpages = {14}
}

@inproceedings{10.1007/978-3-030-13342-9_15,
author = {Horovitz, Shay and Amos, Roei and Baruch, Ohad and Cohen, Tomer and Oyar, Tal and Deri, Afik},
title = {FaaStest - Machine Learning Based Cost and Performance FaaS Optimization},
year = {2018},
isbn = {978-3-030-13341-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-13342-9_15},
doi = {10.1007/978-3-030-13342-9_15},
abstract = {With the emergence of Function-as-a-Service (FaaS) in the cloud, pay-per-use pricing models became available along with the traditional fixed price model for VMs and increased the complexity of selecting the optimal platform for a given service. We present FaaStest - an autonomous solution for cost and performance optimization of FaaS services by taking a hybrid approach - learning the behavioral patterns of the service and dynamically selecting the optimal platform. Moreover, we combine a prediction based solution for reducing cold starts of FaaS services. Experiments present a reduction of over 50% in cost and over 90% in response time for FaaS calls.},
booktitle = {Economics of Grids, Clouds, Systems, and Services: 15th International Conference, GECON 2018, Pisa, Italy, September 18–20, 2018, Proceedings},
pages = {171–186},
numpages = {16},
keywords = {Function as a Service, Serverless, Machine Learning},
location = {Pisa, Italy}
}

@article{10.1007/s00607-021-01017-6,
author = {Memeti, Suejb and Pllana, Sabri},
title = {Optimization of heterogeneous systems with AI planning heuristics and machine learning: a performance and energy aware approach},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {103},
number = {12},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-021-01017-6},
doi = {10.1007/s00607-021-01017-6},
abstract = {Heterogeneous computing systems provide high performance and energy efficiency. However, to optimally utilize such systems, solutions that distribute the work across host CPUs and accelerating devices are needed. In this paper, we present a performance and energy aware approach that combines AI planning heuristics for parameter space exploration with a machine learning model for performance and energy evaluation to determine a near-optimal system configuration. For data-parallel applications our approach determines a near-optimal host-device distribution of work, number of processing units required and the corresponding scheduling strategy. We evaluate our approach for various heterogeneous systems accelerated with GPU or the Intel Xeon Phi. The experimental results demonstrate that our approach finds a near-optimal system configuration by evaluating only about 7% of reasonable configurations. Furthermore, the performance per Joule estimation of system configurations using our machine learning model is more than 1000 \texttimes{} faster compared to the system evaluation by program execution.},
journal = {Computing},
month = dec,
pages = {2943–2966},
numpages = {24},
keywords = {Heterogeneous computing, Optimization, Artificial intelligence (AI), Machine learning (ML), Planning heuristics, 90C59, 68T20, 68W10}
}

@inproceedings{10.1145/3366030.3366040,
author = {Tashkandi, Araek and Wiese, Lena},
title = {A Hybrid Machine Learning Approach for Improving Mortality Risk Prediction on Imbalanced Data},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366040},
doi = {10.1145/3366030.3366040},
abstract = {The efficiency of Machine Learning (ML) models has widely been acknowledged in the healthcare area. However, the quality of the underlying medical data is a major challenge when applying ML in medical decision making. In particular, the imbalanced class distribution problem causes the ML model to be biased towards the majority class. Furthermore, the accuracy will be biased, too, which produces the Accuracy Paradox. In this paper, we identify an optimal ML model for predicting mortality risk for Intensive Care Units (ICU) patients. We comprehensively assess an approach that leverages the efficiency of ML ensemble learning (in particular, Gradient Boosting Decision Tree) and clustering-based data sampling to handle the imbalanced data problem that this model faces. We comprehensively compare different competitors (in terms of ML models as well as clustering methods) on a big real-world ICU dataset achieving a maximum area under the curve value of 0.956.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {83–92},
numpages = {10},
keywords = {Decision Support System, Gradient Boosting Decision Tree, Imbalanced Data, Machine Learning, Risk of Mortality, Under-sampling},
location = {Munich, Germany},
series = {iiWAS2019}
}

@article{10.5555/3122009.3242010,
author = {Baydin, At\i{}l\i{}m G\"{u}nes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
title = {Automatic differentiation in machine learning: a survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5595–5637},
numpages = {43},
keywords = {backpropagation, differentiable programming}
}

@article{10.5555/3546258.3546297,
author = {Tauzin, Guillaume and Lupo, Umberto and Tunstall, Lewis and P\'{e}rez, Julian Burella and Caorsi, Matteo and Medina-Mardones, Anibal M. and Dassatti, Alberto and Hess, Kathryn},
title = {giotto-tda: a topological data analysis toolkit for machine learning and data exploration},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We introduce giotto-tda, a Python library that integrates high-performance topological data analysis with machine learning via a scikit-learn-compatible API and state-of-the-art C++ implementations. The library's ability to handle various types of data is rooted in a wide range of preprocessing techniques, and its strong focus on data exploration and interpretability is aided by an intuitive plotting API.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {39},
numpages = {6},
keywords = {topological data analysis, persistent homology, mapper, machine learning, data exploration, python}
}

@inproceedings{10.1145/2185475.2185477,
author = {Del Vento, Davide},
title = {Performance optimization on a supercomputer with cTuning and the PGI compiler},
year = {2012},
isbn = {9781450311472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2185475.2185477},
doi = {10.1145/2185475.2185477},
abstract = {In this paper we show a machine learning based implementation of autotuning, built with the cTuning CC framework. We implemented the PGI compiler in the cTuning CC framework, plugged in a few additional benchmarks and tested it on a Cray XT5m supercomputer. The main contribution of the present paper consists in combining existing autotuning techniques and using them with the PGI production compiler. Although not ready for production workflows yet, our results are encouraging.},
booktitle = {Proceedings of the 2nd International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era},
pages = {12–20},
numpages = {9},
keywords = {automatic performance tuning, benchmarks, machine learning compiler, self-optimization, self-tuning compiler},
location = {London, United Kingdom},
series = {EXADAPT '12}
}

@inproceedings{10.1145/1985441.1985458,
author = {Krishnan, Sandeep and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Empirical evaluation of reliability improvement in an evolving software product line},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985458},
doi = {10.1145/1985441.1985458},
abstract = {Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), highreuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {103–112},
numpages = {10},
keywords = {change, failures, reliability, reuse, software product lines},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.5555/2004685.2005507,
author = {Engstr\"{o}m, Emelie and Runeson, Per},
title = {Decision Support for Test Management and Scope Selection in a Software Product Line Context},
year = {2011},
isbn = {9780769543451},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In large software organizations with a product line development approach, system test planning and scope selection is a complex tasks for which tool support is needed. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of double testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. This paper discusses the need and challenges of providing decision support for test planning and test selection in a product line context, and highlights possible paths towards a pragmatic implementation of context-specific decision support of various levels of automation. With existing regression testing approaches it is possible to provide automated decision support in a few specific cases, while test management in general may be supported through visualization of test execution coverage, the testing space and the delta between the sufficiently tested system and the system under test. A better understanding of the real world context and how to map research results to the same is needed.},
booktitle = {Proceedings of the 2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
pages = {262–265},
numpages = {4},
keywords = {decision support, regression testing, software product line testing, test coverage, test selection, visualization},
series = {ICSTW '11}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Graph Laplacian, Nearest neighborhood graph, Self-paced learning, Semi-supervised learning}
}

@inproceedings{10.1145/2370816.2370838,
author = {Rane, Ashay and Browne, James},
title = {Enhancing performance optimization of multicore chips and multichip nodes with data structure metrics},
year = {2012},
isbn = {9781450311823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2370816.2370838},
doi = {10.1145/2370816.2370838},
abstract = {Program performance optimization is usually based solely on measurements of execution behavior of code segments using hardware performance counters. However, memory access patterns are critical performance limiting factors for today's multicore chips where performance is highly memory bound. Therefore diagnoses and selection of optimizations based only on measurements of the execution behavior of code segments are incomplete because they do not incorporate knowledge of memory access patterns and behaviors. This paper presents a low-overhead tool (MACPO) that captures memory traces and computes metrics for the memory access behavior of source-level (C, C++, Fortran) data structures. It also presents a complete process for integrating code segment-based and memory access pattern measurements and analyses for performance optimization specifically targeting multicore chips and multichip nodes of clusters. MACPO explicitly targets the measurement and metrics important to performance optimization for multicore chips. MACPO uses more realistic cache models for computation of latency metrics than those used by previous tools. Evaluation of the effectiveness of adding memory access behavior characteristics of data structures to performance optimization was done on subsets of the ASCI, NAS and Rodina parallel benchmarks and one application program from a domain not represented in these benchmarks. Adding memory behavior characteristics enabled easier diagnoses of bottlenecks and more accurate selection of appropriate optimizations than with only code centric behavior measurements. The performance gains ranged from a few percent to 38 percent.},
booktitle = {Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques},
pages = {147–156},
numpages = {10},
keywords = {data structures, memory, optimization, performance},
location = {Minneapolis, Minnesota, USA},
series = {PACT '12}
}

@inproceedings{10.1109/GLOBECOM38437.2019.9014187,
author = {Masood, Usama and Farooq, Hasan and Imran, Ali},
title = {A Machine Learning Based 3D Propagation Model for Intelligent Future Cellular Networks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM38437.2019.9014187},
doi = {10.1109/GLOBECOM38437.2019.9014187},
abstract = {In modern wireless communication systems, radio propagation modeling has always been a fundamental task in system design and performance optimization. These models are used in cellular networks and other radio systems to estimate the pathloss or the received signal strength (RSS) at the receiver or characterize the environment traversed by the signal. An accurate and agile estimation of pathloss is imperative for achieving desired optimization objectives. The state-of-the- art empirical propagation models are based on measurements in a specific environment and limited in their ability to capture idiosyncrasies of various propagation environments. To cope with this problem, ray-tracing based solutions are used in commercial planning tools, but they tend to be extremely time consuming and expensive. In this paper, we propose a Machine Learning (ML) based approach to complement the empirical or ray tracing-based models, for radio wave propagation modeling and RSS estimation. The proposed ML-based model leverages a pre-identified set of smart predictors, including transmitter parameters and the physical and geometric characteristics of the propagation environment, for estimating the RSS. These smart predictors are readily available at the network-side and need no further standardization. We have quantitatively compared the performance of several machine learning algorithms in their ability to capture the channel characteristics, even with sparse availability of training data. Our results show that Deep Neural Networks outperforms other ML techniques and provides a 25% increase in prediction accuracy as compared to state-of-the-art empirical models and a 12x decrease in prediction time as compared to ray tracing.},
booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Waikoloa, HI, USA}
}

@inproceedings{10.1145/2020723.2020729,
author = {Liu, Jie and Wei, Jun and Ye, Dan and Huang, Tao},
title = {A new approach to performance optimization of mashups via data flow refactoring},
year = {2010},
isbn = {9781450306942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020723.2020729},
doi = {10.1145/2020723.2020729},
abstract = {Mashup tools allow end users graphically build complex mashups using pipes to connect web data sources into a data flow. Because end users are of poor technical expertise, the designed data flows may be inefficient. This paper targets on enhancing the performance of mashups via automatically refactoring the structure of its data flows. First a set of operational semantics features are selected for annotating the operators in data flows and refactoring rules are defined to generate all candidate semantics equivalent data flows. Then a heuristic algorithm is described for accurately searching the data flow of minimal execution time by constructing a partially ordered set of data flows based on their cost estimation. This approach is applicable to general mashup data flows without knowing complete operational semantics of their operators and the efficiency improvement is demonstrated by experiments.},
booktitle = {Proceedings of the Second Asia-Pacific Symposium on Internetware},
articleno = {6},
numpages = {8},
keywords = {data flow, mashup, performance optimization, web service, web2.0},
location = {Suzhou, China},
series = {Internetware '10}
}

@inproceedings{10.1007/978-3-319-03859-9_31,
author = {Nguyen, Vu Thien Nga and Kirner, Raimund},
title = {Demand-Based Scheduling Priorities for Performance Optimisation of Stream Programs on Parallel Platforms},
year = {2013},
isbn = {978-3-319-03858-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03859-9_31},
doi = {10.1007/978-3-319-03859-9_31},
abstract = {This paper introduces a heuristic-based scheduler to optimise the throughput and latency of stream programs with dynamic network structure. The novelty is the utilisation of positive and negative demands of the stream communications. It is a centralised approach to provide load balancing for stream programs with dynamic network structures. The approach is designed for shared-memory multi-core platforms. The experiments show that our scheduler performs significantly better than the reference implementation without demand considerations.},
booktitle = {Algorithms and Architectures for Parallel Processing: 13th International Conference, ICA3PP 2013, Vietri Sul Mare, Italy, December 18-20, 2013, Proceedings, Part I},
pages = {357–369},
numpages = {13},
keywords = {Arrival Rate, Priority Function, Ready Task, Schedule Cycle, Monitoring Framework},
location = {Vietri sul Mare, Italy}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Class activation mapping, Convolutional neural network, Sentiment analysis, Weakly supervised learning, Word localization}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {complex software product line, relational concept analysis, reverse engineering, variability extraction},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10489-016-0829-4,
author = {Yang, Chunsheng and Letourneau, Sylvain and Liu, Jie and Cheng, Qiangqiang and Yang, Yubin},
title = {Machine learning-based methods for TTF estimation with application to APU prognostics},
year = {2017},
issue_date = {January   2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {46},
number = {1},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-016-0829-4},
doi = {10.1007/s10489-016-0829-4},
abstract = {Machine learning-based predictive modeling is to develop machine learning-based or data-driven models to predict failures before they occur and estimate the remaining useful life or time to failure (TTF) accurately. Accurate TTF estimation plays a vital role in predictive maintenance or PHM (Prognostic and Health Management). Despite the availability of large amounts of data and a variety of powerful data analysis methods, predictive models developed for PHM still fail to provide accurate and precise TTF estimations. This paper addresses this problem by integrating machine learning algorithms such as classification, regression and clustering. A classification system is used to determine the likelihood of component failures such that rough indications of TTF are provided. Clustering and SVM-based local regression are then introduced to refine the time to failure estimations provided by the classification system. The paper illustrates the applicability of the proposed approach through a real world aerospace application with details on data pre-processing requirements. The results demonstrate that the proposed method can reduce uncertainty in estimating time to failure, which in turn helps augment the usefulness of predictive maintenance.},
journal = {Applied Intelligence},
month = jan,
pages = {227–239},
numpages = {13},
keywords = {Classification, Clustering, Machine learning, Prognostics and health management, Regression, Support vector machines (SVMs)}
}

@inproceedings{10.1145/3386263.3407582,
author = {Mao, Wei and Xiao, Zhihua and Xu, Peng and Ren, Hongwei and Liu, Dingbang and Zhao, Shirui and An, Fengwei and Yu, Hao},
title = {Energy-Efficient Machine Learning Accelerator for Binary Neural Networks},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3407582},
doi = {10.1145/3386263.3407582},
abstract = {Binary neural network (BNN) has shown great potential to be implemented with power efficiency and high throughput. Compared with its counterpart, the convolutional neural network (CNN), BNN is trained with binary constrained weights and activations, which are more suitable for edge devices with less computing and storage resource requirements. In this paper, we introduce the BNN characteristics, basic operations and the binarized-network optimization methods. Then we summarize several accelerator designs for BNN hardware implementation by using three mainstream structures, i.e., ReRAM-based crossbar, FPGA and ASIC. Based on the BNN characteristics and hardware custom designs, all these methods achieve massively parallelized computations and highly pipelined data flow to enhance its latency and throughput performance. In addition, the intermediate data with the binary format are stored and processed on chip by constructing the computing-in-memory (CIM) architecture to reduce the off-chip communication costs, including power and latency.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {77–82},
numpages = {6},
keywords = {ReRAM, accelerator, binary neural networks (BNN), computing in memory (CIM), convolution neural networks (CNN)},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@article{10.1016/j.asoc.2016.09.006,
author = {Cordeiro, Filipe Rolim and da Silva-Filho, Abel Guilhermino},
title = {Multi-objective optimization applied to unified second level cache memory hierarchy tuning aiming at energy and performance optimization},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.09.006},
doi = {10.1016/j.asoc.2016.09.006},
abstract = {Graphical abstractDisplay Omitted HighlightsApplication of 5 multi-objective techniques to optimize cache memory hierarchy.Comparative analysis of best techniques to optimize cache memory hierarchy.PESA-II explores only 1.47% of exploration space to find near optimal solutions.PESA-II showed a better performance for all metrics analyzed.Methodology applied helps the designer to find the best configuration to the problem. Cache memory optimization has an important impact on the energy consumption of the embedded system. However, optimization is a hard task due to the large exploration space and conflicting objectives. In this work five multiobjective optimization techniques are applied to cache memory optimization. The PESA-II, NSGAII, SPEA2, PAES and NPGA approaches were applied to 18 different applications from MiBench and PowerStone benchmark suites. Results compared the quality of results in terms of the metrics of general distance, diversity, hypervolume and precision. All techniques had good performance to cache optimization, but PESA-II showed a better performance for all metrics analyzed, having better results in 83% and 88% of cases, compared with the metrics of generational distance and hypervolume, respectively. Additionally, PESA-II needs to explore only 1.47% of exploration space, finding solutions near to Pareto Optimal.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {603–610},
numpages = {8},
keywords = {Embedded systems, Energy consumption, Evolutionary algorithms, Multi-objective optimization}
}

@inproceedings{10.1145/3463274.3463325,
author = {Fu, Liming and Liang, Peng and Li, Xueying and Yang, Chen},
title = {A Machine Learning Based Ensemble Method for Automatic Multiclass Classification of Decisions},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463325},
doi = {10.1145/3463274.3463325},
abstract = {Stakeholders make various types of decisions with respect to requirements, design, management, and so on during the software development life cycle. Nevertheless, these decisions are typically not well documented and classified due to limited human resources, time, and budget. To this end, automatic approaches provide a promising way. In this paper, we aimed at automatically classifying decisions into five types to help stakeholders better document and understand decisions. First, we collected a dataset from the Hibernate developer mailing list. We then experimented and evaluated 270 configurations regarding feature selection, feature extraction techniques, and machine learning classifiers to seek the best configuration for classifying decisions. Especially, we applied an ensemble learning method and constructed ensemble classifiers to compare the performance between ensemble classifiers and base classifiers. Our experiment results show that (1) feature selection can decently improve the classification results; (2) ensemble classifiers can outperform base classifiers provided that ensemble classifiers are well constructed; (3) BoW + 50% features selected by feature selection with an ensemble classifier that combines Na\"{\i}ve Bayes (NB), Logistic Regression (LR), and Support Vector Machine (SVM) achieves the best classification result (with a weighted precision of 0.750, a weighted recall of 0.739, and a weighted F1-score of 0.727) among all the configurations. Our work can benefit various types of stakeholders in software development through providing an automatic approach for effectively classifying decisions into specific types that are relevant to their interests.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {40–49},
numpages = {10},
keywords = {Automatic Classification, Decision, Ensemble Classifier, Hibernate, Software Development},
location = {Trondheim, Norway},
series = {EASE '21}
}

@article{10.1016/j.eswa.2014.08.046,
author = {Kazemian, H.B. and Ahmed, S.},
title = {Comparisons of machine learning techniques for detecting malicious webpages},
year = {2015},
issue_date = {February 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.08.046},
doi = {10.1016/j.eswa.2014.08.046},
abstract = {3 supervised and 2 unsupervised techniques are modeled to detect malicious webpages.Supervised machine learning (ML) techniques accuracies are above 89%.Unsupervised ML techniques accuracies have at least a silhouette coefficient of 0.87.Information obtained from URLs, page links, semantics and visual features of webpages.Chrome extension, lightweight and heavyweight classifiers and online learning are used. This paper compares machine learning techniques for detecting malicious webpages. The conventional method of detecting malicious webpages is going through the black list and checking whether the webpages are listed. Black list is a list of webpages which are classified as malicious from a user's point of view. These black lists are created by trusted organizations and volunteers. They are then used by modern web browsers such as Chrome, Firefox, Internet Explorer, etc. However, black list is ineffective because of the frequent-changing nature of webpages, growing numbers of webpages that pose scalability issues and the crawlers' inability to visit intranet webpages that require computer operators to log in as authenticated users. In this paper therefore alternative and novel approaches are used by applying machine learning algorithms to detect malicious webpages. In this paper three supervised machine learning techniques such as K-Nearest Neighbor, Support Vector Machine and Naive Bayes Classifier, and two unsupervised machine learning techniques such as K-Means and Affinity Propagation are employed. Please note that K-Means and Affinity Propagation have not been applied to detection of malicious webpages by other researchers. All these machine learning techniques have been used to build predictive models to analyze large number of malicious and safe webpages. These webpages were downloaded by a concurrent crawler taking advantage of gevent. The webpages were parsed and various features such as content, URL and screenshot of webpages were extracted to feed into the machine learning models. Computer simulation results have produced an accuracy of up to 98% for the supervised techniques and silhouette coefficient of close to 0.96 for the unsupervised techniques. These predictive models have been applied in a practical context whereby Google Chrome can harness the predictive capabilities of the classifiers that have the advantages of both the lightweight and the heavyweight classifiers.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {1166–1177},
numpages = {12},
keywords = {Affinity Propagation, K-Means, K-Nearest Neighbor, Naive Bayes, Supervised and unsupervised learning, Support Vector Machine}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.compbiomed.2021.104354,
author = {Makridis, Christos A. and Zhao, David Y. and Bejan, Cosmin A. and Alterovitz, Gil},
title = {Leveraging machine learning to characterize the role of socio-economic determinants on physical health and well-being among veterans},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104354},
doi = {10.1016/j.compbiomed.2021.104354},
journal = {Comput. Biol. Med.},
month = jun,
numpages = {8},
keywords = {Health informatics, Machine learning, Subjective well-being, Socioeconomics, Veterans}
}

@article{10.1007/s00542-014-2118-7,
author = {Suja, K. J. and Kumar, G. S. and Nisanth, A. and Komaragiri, Rama},
title = {Dimension and doping concentration based noise and performance optimization of a piezoresistive MEMS pressure sensor},
year = {2015},
issue_date = {April     2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {4},
issn = {0946-7076},
url = {https://doi.org/10.1007/s00542-014-2118-7},
doi = {10.1007/s00542-014-2118-7},
abstract = {Micro electro mechanical system are highly miniaturized devices combining both electrical and mechanical components that are fabricated using integrated circuit batch processing techniques. P-type piezoresistors are diffused into the diaphragm in such a way that two of them are arranged parallel to the membrane edge and the other two are arranged perpendicular to the edge. The results reported in the literature evaluates the sensitivity by considering the change in the dimension of piezoresistors. But this work evaluates sensitivity and noise immunity of the piezoresistors by considering the change in the dimension of piezoresistors and the doping concentration when the sensor is being operated over a temperature ranging from 100 to 600 K. Various thermal effects are considered in the studies to evaluate the noise immunity. The simulation results clearly indicate that the dimension and doping concentration of piezoresistors play an important role in determining the sensitivity of the pressure sensor. It is found that the piezoresistor that senses the compressive plays an integral part in determining the sensor sensitivity. To have a better noise immunity, the doping concentration of the piezoresistor should be high if the sensor needs to operate at high temperatures else, the doping concentration should be low.},
journal = {Microsyst. Technol.},
month = apr,
pages = {831–839},
numpages = {9}
}

@article{10.5555/2871264.2871266,
author = {Bhardwaj, Kailash Chander and Sharma, R. K.},
title = {Machine learning in efficient and effective web service discovery},
year = {2015},
issue_date = {July 2015},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {14},
number = {3–4},
issn = {1540-9589},
abstract = {The web service discovery mechanism has continuously evolved during the last years. There is plethora of information available about various techniques and methods used for meeting the challenge of improving web service discovery. A tremendous effort has been reported in literature and researchers are still contributing to make the web service discovery more effective and efficient. This paper discusses various eminent researchers' work in this direction using machine learning based techniques. Machine learning is a promising area for researchers to produce accurate estimates consistently. Machine learning system effectively "learns" how to estimate from training set of completed projects. We hope that this paper would benefit researchers to carry further work discussed in this paper and provide an outlook for the future research trends.},
journal = {J. Web Eng.},
month = jul,
pages = {196–214},
numpages = {19},
keywords = {fuzzy logic, neural networks, ontology, quality of services, semantics, web ontology language, web service description language, web service modeling ontology, web services modeling language}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3317550.3321441,
author = {Barham, Paul and Isard, Michael},
title = {Machine Learning Systems are Stuck in a Rut},
year = {2019},
isbn = {9781450367271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317550.3321441},
doi = {10.1145/3317550.3321441},
abstract = {In this paper we argue that systems for numerical computing are stuck in a local basin of performance and programmability. Systems researchers are doing an excellent job improving the performance of 5-year-old benchmarks, but gradually making it harder to explore innovative machine learning research ideas.We explain how the evolution of hardware accelerators favors compiler back ends that hyper-optimize large monolithic kernels, show how this reliance on high-performance but inflexible kernels reinforces the dominant style of programming model, and argue these programming abstractions lack expressiveness, maintainability, and modularity; all of which hinders research progress.We conclude by noting promising directions in the field, and advocate steps to advance progress towards high-performance general purpose numerical computing systems on modern accelerators.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {177–183},
numpages = {7},
location = {Bertinoro, Italy},
series = {HotOS '19}
}

@article{10.1155/2019/4368036,
author = {Deli\'{c}, Vlado and Peri\'{c}, Zoran and Se\v{c}ujski, Milan and Jakovljevi\'{c}, Nik\v{s}a and Nikoli\'{c}, Jelena and Mi\v{s}kovi\'{c}, Dragi\v{s}a and Simi\'{c}, Nikola and Suzi\'{c}, Sini\v{s}a and Deli\'{c}, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Feature Extraction, Feature Terms Identification, Requirement Documents, Software Product Lines},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1007/978-3-540-88582-5_50,
author = {Jiang, Michael and Zhang, Jing and Zhao, Hong and Zhou, Yuanyuan},
title = {Enhancing Software Product Line Maintenance with Source Code Mining},
year = {2008},
isbn = {9783540885818},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-88582-5_50},
doi = {10.1007/978-3-540-88582-5_50},
abstract = {Large-scale reuse and accelerated software development have been some of the key attractions behind software product lines. Various strategies and processes have been developed to facilitate product line development, maintenance, and evolution. However, experiences with software product lines also showed that it is a rather challenging task to maintain software product lines and families over a long period of time. The time and effort needed to manage and maintain product lines increase and quality degrades as product lines evolve. Without proper methods and tools to support the evolution, the cost can outweigh the benefits.This paper describes an approach to simplifying the maintenance of software product lines and improving software quality by integrating traditional software maintenance practices with pattern-based source code mining for defect detection and correction. Our case studies were performed in an industrial setting where the evolution of multiple mobile phone models of a product line was investigated.},
booktitle = {Proceedings of the Third International Conference on Wireless Algorithms, Systems, and Applications},
pages = {538–547},
numpages = {10},
keywords = {Product Line, Reuse, Software Maintenance},
location = {Dallas, Texas},
series = {WASA '08}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3230905.3230938,
author = {Fissaa, Tarik and Guermah, Hatim and EL Hamlaoui, Mahmoud and Hafiddi, Hatim and Nassar, Mahmoud},
title = {An Intelligent Approach for Context-Aware Service Selection using Machine Learning},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230938},
doi = {10.1145/3230905.3230938},
abstract = {Service selection is a process to choose the services that best suit user functional and Non-functional Properties (NFP). With the increasing number of available services, users are offered a choice of competitively functional (or even identical) services. Therefore, this choice strongly depends on the NFPs and the user preferences (context) that differentiate between several competitive services. The service selection can be performed automatically and transparently to the user. In this paper, An extension of OWL-S service is proposed to take context information into account during the selection. Afterwards, we presents an intelligent approach for context-aware service selection based on Markov Decision Process, we show how to solve it using reinforcement learning techniques.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {46},
numpages = {6},
keywords = {Context Awareness, Machine Learning, Service Selection},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{10.1007/s10462-018-09679-z,
author = {Nguyen, Giang and Dlugolinsky, Stefan and Bob\'{a}k, Martin and Tran, Viet and L\'{o}pez Garc\'{\i}a, \'{A}lvaro and Heredia, Ignacio and Mal\'{\i}k, Peter and Hluch?, Ladislav},
title = {Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-018-09679-z},
doi = {10.1007/s10462-018-09679-z},
abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {77–124},
numpages = {48},
keywords = {Artificial Intelligence software, Deep Learning, Graphics processing unit (GPU), Intensive computing, Large-scale data mining, Machine Learning, Parallel processing}
}

@article{10.1177/1094342020978033,
author = {Wyrzykowski, Roman and Deelman, Ewa and Wyrzykowski, Roman and Deelman, Ewa},
title = {Guest editor’s note: Special issue on application performance optimization in the era of extreme heterogeneity},
year = {2021},
issue_date = {Jan 2021},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {35},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342020978033},
doi = {10.1177/1094342020978033},
journal = {Int. J. High Perform. Comput. Appl.},
month = jan,
pages = {3–4},
numpages = {2}
}

@inproceedings{10.1145/3461001.3472729,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard Paul},
title = {Requirements-driven reuse recommendation},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472729},
doi = {10.1145/3461001.3472729},
abstract = {This tutorial explores requirements-based reuse recommendation for product line assets in the context of clone-and-own product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210},
numpages = {1},
keywords = {SPL adoption, similarity, software reuse},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2020390.2020397,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Are change metrics good predictors for an evolving software product line?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020397},
doi = {10.1145/2020390.2020397},
abstract = {Background: Previous research on three years of early data for an Eclipse product identified some predictors of failure-prone files that work well for that data set. Additionally, Eclipse has been used to explore characteristics of product line software in previous research.Aims: To assess whether change metrics are good predictors of failure-prone files over time for the family of products in the evolving Eclipse product line.Method: We repeat, to the extent possible, the decision tree portion of the prior study to assess our ability to replicate the method, and then extend it by including four more recent years of data. We compare the most prominent predictors with the previous study's results. We then look at the data for three additional Eclipse products as they evolved over time. We explore whether the set of good predictors change over time for one product and whether the set differs among products.Results: We find that change metrics are consistently good and incrementally better predictors across the evolving products in Eclipse. There is also some consistency regarding which change metrics are the best predictors.Conclusion: Change metrics are good predictors for failure-prone files for the Eclipse product line. A small subset of these change metrics is fairly stable and consistent across products and releases.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {change metrics, failure-prone files, post-release defects, prediction, reuse, software product lines},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.1016/j.peva.2008.11.001,
author = {van der Weij, Wemke and Bhulai, Sandjai and van der Mei, Rob},
title = {Dynamic thread assignment in web server performance optimization},
year = {2009},
issue_date = {June, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {6},
issn = {0166-5316},
url = {https://doi.org/10.1016/j.peva.2008.11.001},
doi = {10.1016/j.peva.2008.11.001},
abstract = {Popular web sites are expected to handle huge number of requests concurrently within a reasonable time frame. The performance of these web sites is largely dependent on effective thread management of their web servers. Although the implementation of static and dynamic thread policies is common practice, remarkably little is known about the implications on performance. Moreover, the commonly used policies do not take into account the complex interaction between the threads that compete for access to a shared resource. We propose new dynamic thread-assignment policies that minimize the average response time of web servers. The web server is modeled as a two-layered tandem of multi-threading queues, where the active threads compete for access to a common resource. This type of two-layered queueing model, which occurs naturally in the performance modeling of systems with intensive software-hardware interaction, are on the one hand appealing from an application point of view, but on the other hand are challenging from a methodological point of view. Our results show that the optimal dynamic thread-assignment policies yield strong reductions in the response times. Validation on an Apache web server shows that our dynamic thread policies confirm our analytical results.},
journal = {Perform. Eval.},
month = jun,
pages = {301–310},
numpages = {10},
keywords = {Dynamic programming, Dynamic thread management, Multi-layered queueing systems, Web servers}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.5555/1326073.1326148,
author = {Bufistov, Dmitry and Cortadella, Jordi and Kishinevsky, Mike and Sapatnekar, Sachin},
title = {A general model for performance optimization of sequential systems},
year = {2007},
isbn = {1424413826},
publisher = {IEEE Press},
abstract = {Retiming, c-slow retiming and recycling are different transformations for the performance optimization of sequential circuits. For retiming and c-slow retiming, different models that provide exact solutions have already been proposed. An exact model for recycling was yet unknown. This paper presents a general formulation that covers the combination of the three schemes for performance optimization. It provides an exact model based on integer linear programming that resorts to the structural theory of marked graphs. A set of experiments has been designed to show the benefits in performance obtained by combining retiming and recycling. The results also show the applicability of the method in large circuits.},
booktitle = {Proceedings of the 2007 IEEE/ACM International Conference on Computer-Aided Design},
pages = {362–369},
numpages = {8},
location = {San Jose, California},
series = {ICCAD '07}
}

@article{10.1016/j.cmpb.2018.06.010,
author = {Akbulut, Akhan and Ertugrul, Egemen and Topcu, Varol},
title = {Fetal health status prediction based on maternal clinical history using machine learning techniques},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {163},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2018.06.010},
doi = {10.1016/j.cmpb.2018.06.010},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
pages = {87–100},
numpages = {14},
keywords = {Machine learning, Medical diagnosis, Risk prediction, Pregnancy, Fetal health, Prognosis, m-Health}
}

@inproceedings{10.1109/ICCIT.2009.268,
author = {Lee, Chong-Yen and Lee, Tsang-Yean and Wu, Homer and Tsui, Hau-Dong and Huang, Jiun-Bo},
title = {A Performance Optimization of Job Scheduling Model Based on Grid Environment},
year = {2009},
isbn = {9780769538969},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCIT.2009.268},
doi = {10.1109/ICCIT.2009.268},
abstract = {To achieve the best quality of overall optimal performance on grid environment and user services, job loads need to be balanced in grids due to the non-uniformly distributed nature of jobs submitted to grids. Jobs are transferred from a heavy loaded grid to other grids in order to achieve performance optimization. The running cost of a job is calculated at the time it is completed. In this paper, we propose a job schedule model considering job cost and performance. The algorithm is resided in all grids to schedule and arrange their own jobs by themselves to get best performance without extra user cost.},
booktitle = {Proceedings of the 2009 Fourth International Conference on Computer Sciences and Convergence Information Technology},
pages = {768–773},
numpages = {6},
series = {ICCIT '09}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {feature extraction, requirement documents, reverse engineering, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.envsoft.2019.01.003,
author = {Zaherpour, Jamal and Mount, Nick and Gosling, Simon N. and Dankers, Rutger and Eisner, Stephanie and Gerten, Dieter and Liu, Xingcai and Masaki, Yoshimitsu and M\"{u}ller Schmied, Hannes and Tang, Qiuhong and Wada, Yoshihide},
title = {Exploring the value of machine learning for weighted multi-model combination of an ensemble of global hydrological models},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {1364-8152},
url = {https://doi.org/10.1016/j.envsoft.2019.01.003},
doi = {10.1016/j.envsoft.2019.01.003},
journal = {Environ. Model. Softw.},
month = apr,
pages = {112–128},
numpages = {17},
keywords = {Machine learning, Model weighting, Gene expression programming, Global hydrological models, Optimisation}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {ArgoUML SPL benchmark, dynamic feature localization, spectrum-based localization},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.5555/1609810.1609813,
author = {Feng, Wenying and Chen, Hua},
title = {Performance optimization for web caching based on input modeling},
year = {2009},
issue_date = {April 2009},
publisher = {IOS Press},
address = {NLD},
volume = {9},
number = {1,2S2},
issn = {1472-7978},
abstract = {In this paper, the adaptive replacement policy and the pre-fetching technique for Web caching systems are studied with two user request models and their combinations. Performance results from two most commonly used replacement policies: LRU (Least Recently Used) and LFU (Least Frequently Used) are compared. It is shown that LRU performs better for one of the user request models and LFU is on the opposite. The adaptive replacement policy is intelligent in finding the best candidate policy as the user access pattern changes. The learning speed of the system is tested by different switch thresholds.},
journal = {J. Comp. Methods in Sci. and Eng.},
month = apr,
pages = {149–157},
numpages = {9},
keywords = {LFU, LRU, adaptive learning, pre-fetching, threshold, web caching}
}

@article{10.1016/j.eswa.2020.114176,
author = {AlOmar, Eman Abdullah and Peruma, Anthony and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali and Kessentini, Marouane},
title = {How we refactor and how we document it? On the use of supervised machine learning algorithms to classify refactoring documentation},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114176},
doi = {10.1016/j.eswa.2020.114176},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {26},
keywords = {Refactoring, Software quality, Software engineering, Machine learning}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/3400397.3400687,
author = {B\'{e}chard, Vincent},
title = {Robust data-driven optimization using machine learning and monte-carlo simulation},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {This paper presents a framework that was developed to achieve data-driven robust optimization of processes. The main idea is to automatically build a single and global predictive model using a machine learning technique (random forests), and then to use a derivative-free black-box optimization technique (MADS) to maximize a performance criterion. Monte-Carlo simulation is used to estimate pointwise prediction variance. This automated framework is designed to find optimal variance-stabilizing solutions while preserving the noisiness, non-smoothness, non-linearity, non-convexity and disjoint nature of real-life data. The time it takes to prepare the dataset depends on the volume of data, but this step occurs only once in the procedure. The time it takes to iterate during optimization depends on three user-specified quantities; therefore, the iterative portion is insensitive to data volume. Implementation was done using the R programming language which offers a wide variety of data processing, modelling and optimization capabilities.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3575–3586},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@article{10.1177/1094342007083801,
author = {Buttari, Alfredo and Eijkhout, Victor and Langou, Julien and Filippone, Salvatore},
title = {Performance Optimization and Modeling of Blocked Sparse Kernels},
year = {2007},
issue_date = {November  2007},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {21},
number = {4},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342007083801},
doi = {10.1177/1094342007083801},
abstract = {We present a method for automatically selecting optimal
implementations of sparse matrix-vector operations. Our software
"AcCELS" (Accelerated Compress-storage Elements for Linear Solvers)
involves a setup phase that probes machine characteristics, and a
run-time phase where stored characteristics are combined with a
measure of the actual sparse matrix to find the optimal kernel
implementation. We present a performance model that is shown to be
accurate over a large range of matrices.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {467–484},
numpages = {18},
keywords = {blocking, matrix-vector product, optimization, self-adaptivity, sparse}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {SAT, benchmark, configurable systems, sampling, software product lines, variability model},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s00500-019-03761-6,
author = {Ghorbel, Hatem and Zannini, Nicolas and Cherif, Salma and Sauser, Florian and Grunenwald, David and Droz, William and Baradji, Mahamadou and Lakehal, Djamel},
title = {Smart adaptive run parameterization (SArP): enhancement of user manual selection of running parameters in fluid dynamic simulations using bio-inspired and machine-learning techniques},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {22},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-03761-6},
doi = {10.1007/s00500-019-03761-6},
abstract = {Computational fluid dynamic (CFD) simulations present numerous challenges in the domain of artificial intelligence. Computational time, resources and cost that can reach disproportional size before leading a simulation to its fully converged solution are one of the central issues in this domain. In this paper, we propose a novel algorithm that finds optimal parameter settings for the numerical solvers of CFD software. Indeed, this research proposes an alternative approach; rather than going deeper in reducing the mathematical complexity, it suggests taking advantage of the history of previous runs in order to estimate the best parameters for numerical equation resolution. In fact, our approach is bio-inspired and based on a genetic algorithm (GA) and evolutionary strategies enhanced with surrogate functions based on machine-learning meta-models. Our research method was tested on 11 different use cases using various configurations of the GA and algorithms of machine learning such as regression trees extra trees regressors and random forest regressors. Our approach has achieved better runtime performance and higher convergence quality (an improvement varying between 8 and 40%) in all of the test cases when compared to a basic approach which requires manually selecting the parameters. Moreover, our approach outperforms in some cases manual selection of parameters by reaching convergent solutions that couldn’t otherwise be achieved manually.},
journal = {Soft Comput.},
month = nov,
pages = {12031–12047},
numpages = {17},
keywords = {Computational fluid dynamics, Genetic algorithms, Surrogate functions, Machine learning}
}

@inproceedings{10.1145/3230905.3230906,
author = {Memeti, Suejb and Pllana, Sabri and Binotto, Al\'{e}cio and Ko\l{}odziej, Joanna and Brandic, Ivona},
title = {A Review of Machine Learning and Meta-heuristic Methods for Scheduling Parallel Computing Systems},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230906},
doi = {10.1145/3230905.3230906},
abstract = {Optimized software execution on parallel computing systems demands consideration of many parameters at run-time. Determining the optimal set of parameters in a given execution context is a complex task, and therefore to address this issue researchers have proposed different approaches that use heuristic search or machine learning. In this paper, we undertake a systematic literature review to aggregate, analyze and classify the existing software optimization methods for parallel computing systems. We review approaches that use machine learning or meta-heuristics for scheduling parallel computing systems. Additionally, we discuss challenges and future research directions. The results of this study may help to better understand the state-of-the-art techniques that use machine learning and meta-heuristics to deal with the complexity of scheduling parallel computing systems. Furthermore, it may aid in understanding the limitations of existing approaches and identification of areas for improvement.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {5},
numpages = {6},
keywords = {Parallel computing, machine learning, meta-heuristics, scheduling},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s11277-018-5376-3,
author = {Kumar, Adesh and Sharma, Paawan and Gupta, Mukul Kumar and Kumar, Roushan},
title = {Machine Learning Based Resource Utilization and Pre-estimation for Network on Chip (NoC) Communication},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-018-5376-3},
doi = {10.1007/s11277-018-5376-3},
abstract = {Network on chip (NoC) is the solution to solve the problem of larger system on chip and bus based communication system. NoC provides scalable, highly reliable and modular approach for on chip communication and related problems. The wireless communication technologies such as IEEE 802.15.4 Zigbee technology follow mesh, star and cluster tree topology. The paper focuses on the development of machine learning model for design and FPGA synthesis of mesh, ring and fat tree NoC for different cluster size (N = 2, 4, 8, 16, 32, 64, 128 and 256). The fat-tree based topologies incorporate more links near the root of the tree, in order to fulfill the requirement for higher communication demand closer to the root of the tree, as compared to its leafs. It is an indirect topology in which not all routers are identical in terms of number of ports connecting to other routers or elements in the network. The research article presents the use of machine learning techniques to predict the FPGA resource utilization for NoC in advance. The present study helps in NoC chip planning before designing the chip itself by taking into account known hardware design parameters, memory utilization and timing parameters such as minimum and maximum period, frequency support etc. The machine learning is carried out based on multiple linear regression, decision tree regression and random forest regression which estimate the accuracy of the design and good performance. The interprocess communication among nodes is verified using Virtex-5 FPGA, in which data flows in packets and can vary up to `n' bit. The designs are developed in Xilinx ISE 14.2 and simulated in Modelsim 10.1b with the help of VHDL programming language. The developed model has been validated and has performed well on independent test data.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2211–2231},
numpages = {21},
keywords = {Field programmable gate array (FPGA), Interprocess communication, Machine learning, Network on chip (NoC), System on chip (SoC)}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {configuration, evaluation, feature models, recommender systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.mejo.2017.01.007,
author = {Teittinen, Jukka and Hiienkari, Markus and liobait, Indr and Hollmen, Jaakko and Berg, Heikki and Heiskala, Juha and Viitanen, Timo and Simonsson, Jesse and Koskinen, Lauri},
title = {A 5.3 pJ/op approximate TTA VLIW tailored for machine learning},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {C},
issn = {0026-2692},
url = {https://doi.org/10.1016/j.mejo.2017.01.007},
doi = {10.1016/j.mejo.2017.01.007},
abstract = {To achieve energy efficiency in the Internet-of-Things (IoT), more intelligence is required in the wireless IoT nodes. Otherwise, the energy required by the wireless communication of raw sensor data will prohibit battery lifetime, the backbone of IoT. One option to achive this intelligence is to implement a variety of machine learning algorithms on the IoT sensor instead of the cloud. Shown here is sub-milliwatt machine learning accelerator operating at the Ultra-Low Voltage Minimum-Energy Point. The accelerator is a Transport Triggered Architecture (TTA) Application-Specific Instruction-Set Processor (ASIP) targeted for running various Machine Learning algorithms. The ASIP is implemented in 28nm FDSOI (Fully Depleted Silicon On Insulator) CMOS process, with an operating voltage of 0.35V, and is capable of 5.3pJ/cycle and 1.8nJ/iteration when performing conventional machine learning algorithms. The ASIP also includes hardware and compiler support for approximate computing. With the machine learning algorithms, computing approximately brings a maximum of 4.7% energy savings.},
journal = {Microelectron. J.},
month = mar,
pages = {106–113},
numpages = {8},
keywords = {Approximate computing, Integrated circuit, Machine learning, Minimum energy point, Processor, Timing error detection}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {AutoML, NAS, configuration search, feature model, neural architecture search},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11276-018-01929-w,
author = {Sharma, Amandeep and Kakkar, Ajay},
title = {Machine learning based optimal renewable energy allocation in sustained wireless sensor networks},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {7},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-018-01929-w},
doi = {10.1007/s11276-018-01929-w},
abstract = {The environmental energy harvesting is adjudged as a reliable solution to power the wireless nodes for infinite time and assuring uninterrupted operation of deployed network nodes. But uncertain energy availability initiates an important research issue of energy management in rechargeable sensor nodes. An integrated approach of energy assignment principles with adaptive duty cycling has been proposed to efficiently utilize the available energy and to maximize the node performance. The R interface based machine learning ensemble approach has been used for solar irradiance prediction to pre-estimate the node duty cycle. Dynamic programming based optimization problem has been used for real time adaption of pre-computed node duty cycle. The effectiveness of proposed work has been validated using MATLAB interface by extensive simulations on real time solar energy profiles in terms of magnitude and stability of sensors average duty cycle. The proposed algorithm achieves an average duty cycle of 65% to 69% with a limit of 70% maximum duty cycle irrespective of irregular radiation patterns throughout the day as well as for different forecasting horizons. The results shows minimum variation in estimated and real time energy profiles in stable weather conditions and optimize the duty cycle in irregular weather conditions. The results also shows minimum variation ($$&gt;2%$$) in estimated and real time energy profiles in stable weather conditions and optimize the duty cycle in irregular weather conditions.},
journal = {Wirel. Netw.},
month = oct,
pages = {3953–3981},
numpages = {29},
keywords = {Solar forecasting, Forecasting horizons, Energy assignment principles, Adaptive duty cycling, Energy neutral state, Storage efficiency}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/1147249.1147254,
author = {Fischbein, Dario and Uchitel, Sebastian and Braberman, Victor},
title = {A foundation for behavioural conformance in software product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147254},
doi = {10.1145/1147249.1147254},
abstract = {Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {39–48},
numpages = {10},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@article{10.1155/2013/984376,
author = {Xie, Zheng and Edwards, Doug},
title = {Computational performance optimisation for statistical analysis of the effect of nano-CMOS variability on integrated circuits},
year = {2013},
issue_date = {January 2013},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2013},
issn = {1065-514X},
url = {https://doi.org/10.1155/2013/984376},
doi = {10.1155/2013/984376},
abstract = {The intrinsic variability of nanoscale VLSI technology must be taken into account when analyzing circuit designs to predict likely yield. Monte-Carlo- (MC-) and quasi-MC- (QMC-) based statistical techniques do this by analysing many randomised or quasirandomised copies of circuits. The randomisation must model forms of variability that occur in nano-CMOS technology, including "atomistic" effects without intradie correlation and effects with intradie correlation between neighbouring devices. A major problem is the computational cost of carrying out sufficient analyses to produce statistically reliable results. The use of principal components analysis, behavioural modeling, and an implementation of "Statistical Blockade" (SB) is shown to be capable of achieving significant reduction in the computational costs. A computation time reduction of 98.7% was achieved for a commonly used asynchronous circuit element. Replacing MC by QMC analysis can achieve further computation reduction, and this is illustrated for more complex circuits, with the results being compared with those of transistor-level simulations. The "yield prediction" analysis of SRAM arrays is taken as a case study, where the arrays contain up to 1536 transistors modelled using parameters appropriate to 35 nm technology. It is reported that savings of up to 99.85% in computation time were obtained.},
journal = {VLSI Des.},
month = jan,
articleno = {12},
numpages = {1}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/1454115.1454151,
author = {Curtis-Maury, Matthew and Shah, Ankur and Blagojevic, Filip and Nikolopoulos, Dimitrios S. and de Supinski, Bronis R. and Schulz, Martin},
title = {Prediction models for multi-dimensional power-performance optimization on many cores},
year = {2008},
isbn = {9781605582825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454115.1454151},
doi = {10.1145/1454115.1454151},
abstract = {Power has become a primary concern for HPC systems. Dynamic voltage and frequency scaling (DVFS) and dynamic concurrency throttling (DCT) are two software tools (or knobs) for reducing the dynamic power consumption of HPC systems. To date, few works have considered the synergistic integration of DVFS and DCT in performance-constrained systems, and, to the best of our knowledge, no prior research has developed application-aware simultaneous DVFS and DCT controllers in real systems and parallel programming frameworks. We present a multi-dimensional, online performance predictor, which we deploy to address the problem of simultaneous runtime optimization of DVFS and DCT on multi-core systems. We present results from an implementation of the predictor in a runtime library linked to the Intel OpenMP environment and running on an actual dual-processor quad-core system. We show that our predictor derives near-optimal settings of the power-aware program adaptation knobs that we consider. Our overall framework achieves significant reductions in energy (19% mean) and ED2 (40% mean), through simultaneous power savings (6% mean) and performance improvements (14% mean). We also find that our framework outperforms earlier solutions that adapt only DVFS or DCT, as well as one that sequentially applies DCT then DVFS. Further, our results indicate that prediction-based schemes for runtime adaptation compare favorably and typically improve upon heuristic search-based approaches in both performance and energy savings.},
booktitle = {Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques},
pages = {250–259},
numpages = {10},
keywords = {dynamic concurrency throttling},
location = {Toronto, Ontario, Canada},
series = {PACT '08}
}

@inproceedings{10.5555/2025951.2025954,
author = {Danylenko, Antonina and Kessler, Christoph and L\"{o}we, Welf},
title = {Comparing machine learning approaches for context-aware composition},
year = {2011},
isbn = {9783642220449},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Context-Aware Composition allows to automatically select optimal variants of algorithms, data-structures, and schedules at runtime using generalized dynamic Dispatch Tables. These tables grow exponentially with the number of significant context attributes. To make Context-Aware Composition scale, we suggest four alternative implementations to Dispatch Tables, all well-known in the field of machine learning: Decision Trees, Decision Diagrams, Naive Bayes and Support Vector Machines classifiers. We assess their decision overhead and memory consumption theoretically and practically in a number of experiments on different hardware platforms. Decision Diagrams turn out to be more compact compared to Dispatch Tables, almost as accurate, and faster in decision making. Using Decision Diagrams in Context-Aware Composition leads to a better scalability, i.e., Context-Aware Composition can be applied at more program points and regard more context attributes than before.},
booktitle = {Proceedings of the 10th International Conference on Software Composition},
pages = {18–33},
numpages = {16},
keywords = {autotuning, context-aware composition, machine learning},
location = {Zurich, Switzerland},
series = {SC'11}
}

@inproceedings{10.1145/3447818.3460370,
author = {Zamora, Yuliana and Ward, Logan and Sivaraman, Ganesh and Foster, Ian and Hoffmann, Henry},
title = {Proxima: accelerating the integration of machine learning in atomistic simulations},
year = {2021},
isbn = {9781450383356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447818.3460370},
doi = {10.1145/3447818.3460370},
abstract = {Atomistic-scale simulations are prominent scientific applications that require the repetitive execution of a computationally expensive routine to calculate a system's potential energy. Prior work shows that these expensive routines can be replaced with a machine-learned surrogate approximation to accelerate the simulation at the expense of the overall accuracy. The exact balance of speed and accuracy depends on the specific configuration of the surrogate-modeling workflow and the science itself, and prior work leaves it up to the scientist to find a configuration that delivers the required accuracy for their science problem. Unfortunately, due to the underlying system dynamics, it is rare that a single surrogate configuration presents an optimal accuracy/latency trade-off for the entire simulation. In practice, scientists must choose conservative configurations so that accuracy is always acceptable, forgoing possible acceleration. As an alternative, we propose Proxima, a systematic and automated method for dynamically tuning a surrogate-modeling configuration in response to real-time feedback from the ongoing simulation. Proxima estimates the uncertainty of applying a surrogate approximation in each step of an iterative simulation. Using this information, the specific surrogate configuration can be adjusted dynamically to ensure maximum speedup while sustaining a required accuracy metric. We evaluate Proxima using a Monte Carlo sampling application and find that Proxima respects a wide range of user-defined accuracy goals while achieving speedups of 1.02--5.5X relative to a standard},
booktitle = {Proceedings of the 35th ACM International Conference on Supercomputing},
pages = {242–253},
numpages = {12},
keywords = {control theory, machine learning, modeling and simulation},
location = {Virtual Event, USA},
series = {ICS '21}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {active automata learning, featured transition systems, model learning, software product lines, variability mining},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3401071.3401658,
author = {Woltmann, Lucas and Hartmann, Claudio and Habich, Dirk and Lehner, Wolfgang},
title = {Best of both worlds: combining traditional and machine learning models for cardinality estimation},
year = {2020},
isbn = {9781450380294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401071.3401658},
doi = {10.1145/3401071.3401658},
abstract = {Cardinality estimation is a high-profile technique in database management systems with a serious impact on query performance. Thus, a lot of traditional approaches such as histograms-based or sampling-based methods have been developed over the last decades. With the advance of Machine Learning (ML) into the database world, cardinality estimation profits from several methods improving its quality as shown in different recent papers. However, neither an ML model nor a traditional approach meets all requirements for cardinality estimation, so that a one size fits all approach is difficult to imagine. For that reason, we advocate a better interlacing of ML models and traditional approaches for cardinality estimation and thoroughly consider their potential, advantages, and disadvantages in this paper. We start by proposing a classification of different estimation techniques and their usability for cardinality estimation. Then, we motivate a novel hybrid approach as the core proof of concept of this paper which uses the best of both worlds: ML models and the proven histogram approach. For this, we show in which cases it is beneficial to use ML models or when we can trust the traditional estimators. We evaluate our hybrid approach on two real-world data sets and conclude what can be done to improve the coexistence of traditional and ML approaches in DBMS. With all our proposals, we use ML to improve DBMS without abandoning years of valuable research in cardinality estimation.},
booktitle = {Proceedings of the Third International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {4},
numpages = {8},
keywords = {cardinality estimation, hybrid, machine learning, neural networks, query optimization},
location = {Portland, Oregon},
series = {aiDM '20}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {artificial neural networks, empirical study, reusability, survey, systematic reuse},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {constraints, dynamic product line models, product line engineering, simulation, tool, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1109/TC.2010.151,
author = {Kargahi, Mehdi and Movaghar, Ali},
title = {Performance Optimization Based on Analytical Modeling in a Real-Time System with Constrained Time/Utility Functions},
year = {2011},
issue_date = {August 2011},
publisher = {IEEE Computer Society},
address = {USA},
volume = {60},
number = {8},
issn = {0018-9340},
url = {https://doi.org/10.1109/TC.2010.151},
doi = {10.1109/TC.2010.151},
abstract = {We consider a single-processor firm real-time (FRT) system with exponential interarrival and execution times for jobs with relative deadlines following a general distribution. The scheduling policy of the system is first-come first-served (FCFS) and the capacity of the system is arbitrary. This system is subject to an arbitrary-shaped time/utility function (TUF), which determines the accrued utility of each job according to its completion time. It is considered that the system power consumption at different working states is predetermined for each processor speed. We have proposed an exact analytical method for the calculation of specific performance and power-related measures of the system. The resulting analytical formulations for the performance measures are functions of the processor speed and system capacity. These measures are optimized through appropriate selections of the speed using derivatives and the capacity employing numerical search methods. Some experimental results are presented for different unimodal TUFs in systems with deterministic and exponential relative deadlines. For the latter distribution, the results are compared against similar results obtained through simulation for the nonpreemptive earliest-deadline-first (NP-EDF) scheduling policy. The comparisons show that FCFS is superior to NP-EDF for some measures and TUFs.},
journal = {IEEE Trans. Comput.},
month = aug,
pages = {1169–1181},
numpages = {13},
keywords = {Analytical modeling, firm real-time system, optimization, performance modeling, time/utility function.}
}

@inproceedings{10.1007/978-3-030-87589-3_48,
author = {Machado Reyes, Diego and Chao, Hanqing and Homayounieh, Fatemeh and Hahn, Juergen and Kalra, Mannudeep K. and Yan, Pingkun},
title = {Cardiovascular Disease Risk Improves COVID-19 Patient Outcome Prediction},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_48},
doi = {10.1007/978-3-030-87589-3_48},
abstract = {The pandemic of coronavirus disease 2019 (COVID-19) has severely impacted the world. Several studies suggest an increased risk for COVID-19 patients with underlying cardiovascular diseases (CVD). However, it is challenging to quantify such risk factors and integrate them into patient condition evaluation. This paper presents machine learning methods to assess CVD risk scores from chest computed tomography together with laboratory data, demographics, and deep learning extracted lung imaging features to increase the outcome prediction accuracy for COVID-19 patients. The experimental results demonstrate an overall increase in prediction performance when the CVD severity score was added to the feature set. The machine learning methods obtained their best performance when all categories of the features were used for the patient outcome prediction. With the best attained area under the curve of 0.888, the presented research may assist physicians in clinical decision-making process on managing COVID-19 patients.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {467–476},
numpages = {10},
keywords = {COVID-19, Machine learning, Cardiovascular disease, Chest CT, Severity score},
location = {Strasbourg, France}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@inproceedings{10.1145/2371316.2371363,
author = {Arbuckle, Tom and Hogan, Damien and Ryan, Conor},
title = {Optimising Flash non-volatile memory using machine learning: a project overview},
year = {2012},
isbn = {9781450312400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371316.2371363},
doi = {10.1145/2371316.2371363},
abstract = {While near ubiquitous, the physical principles of Flash memory mean that its performance degrades with use. During fabrication and operation, its ability to be repeatedly programmed/erased (endurance) needs to be balanced with its ability to store information over months/years (retention).This project overview describes how our modelling of data we obtain experimentally from Flash chips uniquely allows us to optimise the settings of their internal configuration registers, thereby mitigating these problems.},
booktitle = {Proceedings of the Fifth Balkan Conference in Informatics},
pages = {235–238},
numpages = {4},
keywords = {Flash memory, endurance, machine learning, memory performance optimisation, non-volatile memory, retention},
location = {Novi Sad, Serbia},
series = {BCI '12}
}

@inproceedings{10.1109/PDP.2014.24,
author = {Rughetti, Diego and Sanzo, Pierangelo Di and Ciciani, Bruno and Quaglia, Francesco},
title = {Dynamic Feature Selection for Machine-Learning Based Concurrency Regulation in STM},
year = {2014},
isbn = {9781479927296},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PDP.2014.24},
doi = {10.1109/PDP.2014.24},
abstract = {In this paper we explore machine-learning approaches for dynamically selecting the well suited amount of concurrent threads in applications relying on Software Transactional Memory (STM). Specifically, we present a solution that dynamically shrinks or enlarges the set of input features to be exploited by the machine-learner. This allows for tuning the concurrency level while also minimizing the overhead for input-features sampling, given that the cardinality of the input-feature set is always tuned to the minimum value that still guarantees reliability of workload characterization. We also present a fully heedged implementation of our proposal within the TinySTM open source framework, and provide the results of an experimental study relying on the STAMP benchmark suite, which show significant reduction of the response time with respect to proposals based on static feature selection.},
booktitle = {Proceedings of the 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing},
pages = {68–75},
numpages = {8},
keywords = {Concurrency, Performance Models, Performance Optimization, Software Transactional Memory},
series = {PDP '14}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {150% model, family model, model learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/3433701.3433792,
author = {Ali, Ahsan and Pinciroli, Riccardo and Yan, Feng and Smirni, Evgenia},
title = {Batch: machine learning inference serving on serverless platforms with adaptive batching},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Serverless computing is a new pay-per-use cloud service paradigm that automates resource scaling for stateless functions and can potentially facilitate bursty machine learning serving. Batching is critical for latency performance and cost-effectiveness of machine learning inference, but unfortunately it is not supported by existing serverless platforms due to their stateless design. Our experiments show that without batching, machine learning serving cannot reap the benefits of serverless computing. In this paper, we present BATCH, a framework for supporting efficient machine learning serving on serverless platforms. BATCH uses an optimizer to provide inference tail latency guarantees and cost optimization and to enable adaptive batching support. We prototype BATCH atop of AWS Lambda and popular machine learning inference systems. The evaluation verifies the accuracy of the analytic optimizer and demonstrates performance and cost advantages over the state-of-the-art method MArk and the state-of-the-practice tool SageMaker.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {69},
numpages = {15},
keywords = {batching, cloud, cost-effective, inference, machine-learning-as-a-service (MLaaS), modeling, optimization, prediction, serverless, service level objective (SLO), serving},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {Family-based model checking, Software Product Lines, mCRL2},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.jpdc.2012.11.001,
author = {Upadhyaya, Sujatha R.},
title = {Parallel approaches to machine learning-A comprehensive survey},
year = {2013},
issue_date = {March, 2013},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {73},
number = {3},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2012.11.001},
doi = {10.1016/j.jpdc.2012.11.001},
abstract = {Literature has always witnessed efforts that make use of parallel algorithms / parallel architecture to improve performance; machine learning space is no exception. In fact, a considerable effort has gone into this area in the past fifteen years. Our report attempts to bring together and consolidate such attempts. It tracks the development in this area since the inception of the idea in 1995, identifies different phases during the time period 1995-2011 and marks important achievements. When it comes to performance enhancement, GPU platforms have carved a special niche for themselves. The strength of these platforms comes from the capability to speed up computations exponentially by way of parallel architecture / programming methods. While it is evident that computationally complex processes like image processing, gaming etc. stand to gain much from parallel architectures; studies suggest that general purpose tasks such as machine learning, graph traversal, and finite state machines are also identified as the parallel applications of the future. Map reduce is another important technique that has evolved during this period and as the literature has it, it has been proved to be an important aid in delivering performance of machine learning algorithms on GPUs. The report summarily presents the path of developments.},
journal = {J. Parallel Distrib. Comput.},
month = mar,
pages = {284–292},
numpages = {9},
keywords = {Distributed and parallel machine learning, GPU, Map reduce}
}

@inproceedings{10.1145/3362789.3362923,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto},
title = {Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362923},
doi = {10.1145/3362789.3362923},
abstract = {Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1007–1011},
numpages = {5},
keywords = {Automatic generation, Domain engineering, High-level requirements, Information Dashboards, Meta-modeling},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1109/NCCA.2014.16,
author = {Sanzo, Pierangelo Di and Molfese, Francesco and Rughetti, Diego and Ciciani, Bruno},
title = {Providing Transaction Class-Based QoS in In-Memory Data Grids via Machine Learning},
year = {2014},
isbn = {9781479949526},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/NCCA.2014.16},
doi = {10.1109/NCCA.2014.16},
abstract = {Elastic architectures and the "pay-as-you-go" resource pricing model offered by many cloud infrastructure providers may seem the right choice for companies dealing with data centric applications characterized by high variable workload. In such a context, in-memory transactional data grids have demonstrated to be particularly suited for exploiting advantages provided by elastic computing platforms, mainly thanks to their ability to be dynamically (re-)sized and tuned. Anyway, when specific QoS requirements have to be met, this kind of architectures have revealed to be complex to be managed by humans. Particularly, their management is a very complex task without the stand of mechanisms supporting run-time automatic sizing/tuning of the data platform and the underlying (virtual) hardware resources provided by the cloud. In this paper, we present a neural network-based architecture where the system is constantly and automatically re-configured, particularly in terms of computing resources, in order to achieve transaction class-based QoS while minimizing costs of the infrastructure. We also present some results showing the effectiveness of our architecture, which has been evaluated on top of Future Grid IaaS Cloud using Red Hat Infinispan in-memory data grid and the TPC-C benchmark.},
booktitle = {Proceedings of the 2014 IEEE 3rd Symposium on Network Cloud Computing and Applications (Ncca 2014)},
pages = {46–53},
numpages = {8},
keywords = {In-memory transactional data grids, cloud computing, machine learning, neural networks, performance optimization, quality of service},
series = {NCCA '14}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {case-based reasoning, commonality and variability, feature-based representation, product lines, similarity metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1722902.1722949,
author = {AlOrainy, Abdullah},
title = {Performance optimization and long system reach for 8 x 160 Gbit/s OTDM transmission with PMD},
year = {2008},
isbn = {9780889867444},
publisher = {ACTA Press},
address = {USA},
abstract = {The use of ultra-high bit rate channels for WDM transmission is an attractive approach to increase the capacity of future networks. We perform numerical optimization of multichannel 160 Gbit/s transmission systems taking into account various system parameters including polarization mode dispersion (PMD) for different fiber types and dispersion compensation techniques. For an 8x160 Gbit/s OTDM system the maximum reach was shown to be 350 Km and 875Km for SMF and special dispersion managed fibers (DMF), respectively. Alternate compensation dispersion scheme was shown to be more robust to dispersion PMD improving substantially the performance of high-speed multichannel terrestrial systems. Furthermore, using a hybrid solution we have also shown that system reach can be greatly extended to ~1430 Km},
booktitle = {Proceedings of the Eighth IASTED International Conference on Wireless and Optical Communications},
pages = {225–230},
numpages = {6},
keywords = {160Gb/s systems, PMD, performance, simulation},
location = {Quebec City, Canada},
series = {WOC '08}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {configurations, program analysis, testing, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1504/ijhpcn.2020.110252,
author = {Tang, Zhengzhi and Zeng, Xuewen and Chen, Jun},
title = {Multi-model coupling method for imbalanced network traffic classification based on clustering},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {1},
issn = {1740-0562},
url = {https://doi.org/10.1504/ijhpcn.2020.110252},
doi = {10.1504/ijhpcn.2020.110252},
abstract = {The imbalanced category of network traffic poses a challenge to the classification methods based on machine learning, because the unbalanced data structure affects the performance of machine learning algorithms. In this paper, we propose a multi-model coupling approach to address the imbalanced data problem in network traffic classification. We process the major class to some clusters by a clustering algorithm. Then, these clusters and the minor class are used to form the training dataset for training model respectively. During the test, the test dataset is input into the previously trained models respectively, and the classification results of respective models are coupled to obtain the final result. We tested our proposed method on two well-known network traffic datasets and the results showed that it could achieve better performance and less time consumption compared with recent proposed methods in the case where the ratio of minor to major classes is very small.},
journal = {Int. J. High Perform. Comput. Netw.},
month = jan,
pages = {26–35},
numpages = {9},
keywords = {machine learning, imbalanced network traffic classification, clustering algorithm, multi-model coupling}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Feature Identification, Natural Language Documents, Reverse Engineering, Software Product Lines, Systematic Literature Review, Variability Extraction},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/CICN.2010.43,
author = {Shah, Satish K. and Vishwakarma, Dharmistha D.},
title = {Development and Simulation of Artificial Neural Network Based Decision on Parametric Values for Performance Optimization of Reactive Routing Protocol for MANET Using Qualnet},
year = {2010},
isbn = {9780769542546},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CICN.2010.43},
doi = {10.1109/CICN.2010.43},
abstract = {Collection of wireless nodes like mobile, laptops, palmtops etc. can creates a dynamic wireless network without using any existing infrastructure known as Mobile Ad-Hoc Network (MANET). MANET performs peer to peer interactions among the wireless nodes participating in the network. Each node in the network must be able to function as a router as well to relay the packets generated by other nodes. Due to the mobility of nodes network topology changes frequently and the topology changes information must update every time to all the nodes in the network. Limited bandwidth and transmission characteristic impose administrative and control information to update the dynamic charge in the network topology, due to the mobility of the wireless nodes. Information update at some fixed time interval can cause unnecessary traffic in the wireless network so to make adaptive the time interval of these messages is a technique to improve the performance of the network. In this paper a soft computing technique Artificial Neural Network based reactive AODV routing protocol is proposed to determine the frequency of hello interval to improve the performance of the network.},
booktitle = {Proceedings of the 2010 International Conference on Computational Intelligence and Communication Networks},
pages = {167–171},
numpages = {5},
keywords = {AODV, Artificial Neural Network, Mobile ad hoc networks, Qualnet, Routing algorithm},
series = {CICN '10}
}

@inproceedings{10.1145/277651.277677,
author = {Chen, Y. and Winslett, M. and Cho, Y. and Kuo, S.},
title = {Automatic parallel I/O performance optimization in Panda},
year = {1998},
isbn = {0897919890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/277651.277677},
doi = {10.1145/277651.277677},
booktitle = {Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and Architectures},
pages = {108–118},
numpages = {11},
location = {Puerto Vallarta, Mexico},
series = {SPAA '98}
}

@article{10.1007/s11277-006-8863-x,
author = {Bougeard, St\'{e}phane and H\'{e}lard, Jean-Fran\c{c}ois and Siaud, Isabelle},
title = {Performance Optimization of High Order QAM in Presence of Phase Noise and AWGN: Application to a Decision Directed Frequency Synchronization System},
year = {2006},
issue_date = {April     2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {37},
number = {1–2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-006-8863-x},
doi = {10.1007/s11277-006-8863-x},
abstract = {The paper presents an analysis of the impact of Local Oscillators (LO) phase noise on the performance of digital transmission systems. Using an efficient phase noise model, we study degradations induced by phase noise and Additive White Gaussian Noise (AWGN) on a carrier recovery system combined to high order Quadrature Amplitude Modulations (QAM). Simulation results show that an optimum trade-off between AWGN and phase noise robustness for the loop bandwidth can be determined. In order to compare simulated and analytical performance, a new general expression of the bit error probability is developped for square QAM in the presence of phase noise over Gaussian channel. Furthermore, novel decision areas related to Quadrature Amplitude Modulations are determined in order to improve the performance of the carrier recovery algorithm in the presence of phase noise and frequency offset.},
journal = {Wirel. Pers. Commun.},
month = apr,
pages = {123–138},
numpages = {16},
keywords = {QAM, phase noise, synchronization}
}

@inproceedings{10.1007/978-3-030-38651-1_17,
author = {Menouer, Tarek and C\'{e}rin, Christophe and Darmon, Patrice},
title = {Accelerated Promethee Algorithm Based on Dimensionality Reduction},
year = {2019},
isbn = {978-3-030-38650-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38651-1_17},
doi = {10.1007/978-3-030-38651-1_17},
abstract = {This paper presents an accelerated Promethee (Preference Ranking Organization METHod for Enrichment Evaluations) multi-criteria algorithm based on dimensionality reduction in large scale environments. In our context, the Promethee algorithm is used to select from a large set of objects, one or a small set of objects with a good compromise between several qualitative and quantitative criteria. The exact solution can be used by applying the exact multi-criteria Promethee algorithm. However, the drawback, with this type of exact algorithm, is the long execution time due to the combinatorial aspect of the problem. The exact Promethee computing time is linked both to the dimension of the problem (number of qualitative and quantitative criteria) and the size of the problem (number of objects). To address the previous drawback, we propose to accelerate the Promethee algorithm in combining the exact Promethee algorithm with an algorithm inherited from the Machine Learning (ML) field. The experiments demonstrate the potential of our approach under different scenarios to accelerate the respond time.},
booktitle = {Internet of Vehicles. Technologies and Services Toward Smart Cities: 6th International Conference, IOV 2019, Kaohsiung, Taiwan, November 18–21, 2019, Proceedings},
pages = {190–203},
numpages = {14},
keywords = {Performance optimization, Machine learning algorithms (K-Means), Multi-criteria algorithm},
location = {Kaohsiung, Taiwan}
}

@inproceedings{10.1109/ICSE43902.2021.00052,
author = {Nusrat, Fariha and Hassan, Foyzul and Zhong, Hao and Wang, Xiaoyin},
title = {How Developers Optimize Virtual Reality Applications: A Study of Optimization Commits in Open Source Unity Projects},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00052},
doi = {10.1109/ICSE43902.2021.00052},
abstract = {Virtual Reality (VR) is an emerging technique that provides immersive experience for users. Due to the high computation cost of rendering real-time animation twice (for both eyes) and the resource limitation of wearable devices, VR applications often face performance bottlenecks and performance optimization plays an important role in VR software development. Performance optimizations of VR applications can be very different from those in traditional software as VR involves more elements such as graphics rendering and real-time animation. In this paper, we present the first empirical study on 183 real-world performance optimizations from 45 VR software projects. In particular, we manually categorized the optimizations into 11 categories, and applied static analysis to identify how they affect different life-cycle phases of VR applications. Furthermore, we studied the complexity and design / behavior effects of performance optimizations, and how optimizations are different between large organizational software projects and smaller personal software projects. Our major findings include: (1) graphics simplification (24.0%), rendering optimization (16.9%), language / API optimization (15.3%), heap avoidance (14.8%), and value caching (12.0%) are the most common categories of performance optimization in VR applications; (2) game logic updates (30.4%) and before-scene initialization (20.0%) are the most common life-cycle phases affected by performance issues; (3) 45.9% of the optimizations have behavior and design effects and 39.3% of the optimizations are systematic changes; (4) the distributions of optimization classes are very different between organizational VR projects and personal VR projects.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {473–485},
numpages = {13},
keywords = {Empirical Study, Performance Optimization, Virtual Reality},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s10270-020-00791-9,
author = {Westfechtel, Bernhard and Greiner, Sandra},
title = {Extending single- to multi-variant model transformations by trace-based propagation of variability annotations},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00791-9},
doi = {10.1007/s10270-020-00791-9},
abstract = {Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {853–888},
numpages = {36},
keywords = {Model transformation, Software product line, Annotative variability}
}

@inproceedings{10.1145/1791314.1791349,
author = {Berral, Josep Ll. and Goiri, \'{I}\~{n}igo and Nou, Ram\'{o}n and Juli\`{a}, Ferran and Guitart, Jordi and Gavald\`{a}, Ricard and Torres, Jordi},
title = {Towards energy-aware scheduling in data centers using machine learning},
year = {2010},
isbn = {9781450300421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1791314.1791349},
doi = {10.1145/1791314.1791349},
abstract = {As energy-related costs have become a major economical factor for IT infrastructures and data-centers, companies and the research community are being challenged to find better and more efficient power-aware resource management strategies. There is a growing interest in "Green" IT and there is still a big gap in this area to be covered.In order to obtain an energy-efficient data center, we propose a framework that provides an intelligent consolidation methodology using different techniques such as turning on/off machines, power-aware consolidation algorithms, and machine learning techniques to deal with uncertain information while maximizing performance. For the machine learning approach, we use models learned from previous system behaviors in order to predict power consumption levels, CPU loads, and SLA timings, and improve scheduling decisions. Our framework is vertical, because it considers from watt consumption to workload features, and cross-disciplinary, as it uses a wide variety of techniques.We evaluate these techniques with a framework that covers the whole control cycle of a real scenario, using a simulation with representative heterogeneous workloads, and we measure the quality of the results according to a set of metrics focused toward our goals, besides traditional policies. The results obtained indicate that our approach is close to the optimal placement and behaves better when the level of uncertainty increases.},
booktitle = {Proceedings of the 1st International Conference on Energy-Efficient Computing and Networking},
pages = {215–224},
numpages = {10},
keywords = {data center, machine learning, power efficiency, scheduling, simulation},
location = {Passau, Germany},
series = {e-Energy '10}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {clone and own, families of software products, software reuse},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3385412.3386016,
author = {He, Jingxuan and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
title = {Learning fast and precise numerical analysis},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386016},
doi = {10.1145/3385412.3386016},
abstract = {Numerical abstract domains are a key component of modern static analyzers. Despite recent advances, precise analysis with highly expressive domains remains too costly for many real-world programs. To address this challenge, we introduce a new data-driven method, called LAIT, that produces a faster and more scalable numerical analysis without significant loss of precision. Our approach is based on the key insight that sequences of abstract elements produced by the analyzer contain redundancy which can be exploited to increase performance without compromising precision significantly. Concretely, we present an iterative learning algorithm that learns a neural policy that identifies and removes redundant constraints at various points in the sequence. We believe that our method is generic and can be applied to various numerical domains.  We instantiate LAIT for the widely used Polyhedra and Octagon domains. Our evaluation of LAIT on a range of real-world applications with both domains shows that while the approach is designed to be generic, it is orders of magnitude faster on the most costly benchmarks than a state-of-the-art numerical library while maintaining close-to-original analysis precision. Further, LAIT outperforms hand-crafted heuristics and a domain-specific learning approach in terms of both precision and speed.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1112–1127},
numpages = {16},
keywords = {Abstract interpretation, Machine learning, Numerical domains, Performance optimization},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3468044.3468052,
author = {Podobas, Artur and Svedin, Martin and Chien, Steven W. D. and Peng, Ivy B. and Ravichandran, Naresh Balaji and Herman, Pawel and Lansner, Anders and Markidis, Stefano},
title = {StreamBrain: An HPC Framework for Brain-like Neural Networks on CPUs, GPUs and FPGAs},
year = {2021},
isbn = {9781450385497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468044.3468052},
doi = {10.1145/3468044.3468052},
abstract = {The modern deep learning method based on backpropagation has surged in popularity and has been used in multiple domains and application areas. At the same time, there are other - less-known - machine learning algorithms with a mature and solid theoretical foundation whose performance remains unexplored. One such example is the brain-like Bayesian Confidence Propagation Neural Network (BCPNN). In this paper, we introduce StreamBrain--a framework that allows neural networks based on BCPNN to be practically deployed in High-Performance Computing systems. StreamBrain is a domain-specific language (DSL), similar in concept to existing machine learning (ML) frameworks, and supports backends for CPUs, GPUs, and even FPGAs. We empirically demonstrate that StreamBrain can train the well-known ML benchmark dataset MNIST within seconds, and we are the first to demonstrate BCPNN on STL-10 size networks. We also show how StreamBrain can be used to train with custom floating-point formats and illustrate the impact of using different bfloat variations on BCPNN using FPGAs.},
booktitle = {Proceedings of the 11th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
articleno = {8},
numpages = {6},
keywords = {AI, BCPNN, Emerging Machine Learning, FPGA, GPU, HPC, Neural networks, Representation learning, Unsupervised learning},
location = {Online, Germany},
series = {HEART '21}
}

@inproceedings{10.1145/3377929.3398109,
author = {Bokhari, Mahmoud A. and Wagner, Markus and Alexander, Brad},
title = {Genetic improvement of software efficiency: the curse of fitness estimation},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398109},
doi = {10.1145/3377929.3398109},
abstract = {Many challenges arise in the application of Genetic Improvement (GI) of Software to improve non-functional requirements of software such as energy use and run-time. These challenges are mainly centred around the complexity of the search space and the estimation of the desired fitness function. For example, such fitness function are expensive, noisy and estimating them is not a straightforward task. In this paper, we illustrate some of the challenges in computing such fitness functions and propose a synergy between in-vivo evaluation and machine learning approaches to overcome such issues.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1926–1927},
numpages = {2},
keywords = {android, energy consumption, genetic improvement, machine learning, mobile applications, non-functional properties},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {GCC, feature models, software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {e-discovery, electronic discovery, predictive coding, technology-assisted review},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Cyber-Physical Systems, Fault Detection, Highly-Configurable Systems, Product Line Testing, Search-Based Software Engineering, Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1007/978-3-030-91699-2_12,
author = {Barros, Mariana da Silva and Philippini, Igor de Moura and Silva, Ladson Gomes and Netto, Antonio Barros da Silva and Blawid, Rosana and Barros, Edna Natividade da&nbsp;Silva and Blawid, Stefan},
title = {Supervised Training of&nbsp;a&nbsp;Simple Digital Assistant for&nbsp;a&nbsp;Free Crop Clinic},
year = {2021},
isbn = {978-3-030-91698-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91699-2_12},
doi = {10.1007/978-3-030-91699-2_12},
abstract = {Family farming represents a critical segment of Brazilian agriculture, involving more than 5 million properties and generating 74% of rural jobs in the country. Yield losses caused by crop diseases and pests can be devastating for small-scale producers. However, successful disease control requires correct identification, which challenges smallholders, who often lack technical assistance. The present work proposes a system that detects disease symptoms in images of plant leaves to assist phytopathology experts. The objective is to decrease the experts’ workload and enable consulting services for free or at nominal cost. In addition, the required digital communication channel will promote the formation of a caring community ready to offer unpaid advice for family farmers. The machine learning and refinement of the assistance system are described in detail. The developed classification system achieves a recall value of 95%.},
booktitle = {Intelligent Systems: 10th Brazilian Conference, BRACIS 2021, Virtual Event, November 29 – December 3, 2021, Proceedings, Part II},
pages = {162–176},
numpages = {15},
keywords = {Disease identification, Machine learning, Family farming}
}

@inproceedings{10.1007/978-3-030-30619-9_16,
author = {Liang, Yu and Xie, Yi and Fei, Xingrui and Tan, Xincheng and Ma, Haishou},
title = {Content Recognition of Network Traffic Using Wavelet Transform and CNN},
year = {2019},
isbn = {978-3-030-30618-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30619-9_16},
doi = {10.1007/978-3-030-30619-9_16},
abstract = {With the development of the Internet, the content types of the network traffic become more and more diverse, including video, news, music, image and so on. Traffic identification plays an important role in network management, security defense and performance optimization. Traditionally, the network traffic analysis focuses on the protocol identification and application classification, which has been well studied in the past two decades. However, as a large number of existing general protocols and legal applications can be abused to hide and transmit the data of different content types, illegal content may penetrate the traffic analysis system, and lead to inefficient network management and cause potential risks for internal networks. Different from the traditional work on the identification of the protocols or applications, in this paper, we propose a new method for recognizing the content types for the network traffic. The proposed method is based on two technologies including the wavelet transform and CNN. The wavelet transform is exploited to process the time-frequency signals of the observed network traffic that is further classified by the CNN. Experiment results are presented to validate the performance of the proposed scheme.},
booktitle = {Machine Learning for Cyber Security: Second International Conference, ML4CS 2019, Xi’an, China, September 19-21, 2019, Proceedings},
pages = {224–238},
numpages = {15},
keywords = {Content recognition, Network traffic, Wavelet transform, CNN},
location = {Xi'an, China}
}

@inproceedings{10.1145/139669.139726,
author = {Olukotun, Kunle and Mudge, Trevor and Brown, Richard},
title = {Performance optimization of pipelined primary cache},
year = {1992},
isbn = {0897915097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/139669.139726},
doi = {10.1145/139669.139726},
abstract = {The CPU cycle time of a high-performance processor is usually determined by the access time of the primary cache. As processors speeds increase, designers will have to increase the number of pipeline stages used to fetch data from the cache in order to reduce the dependence of CPU cycle time on cache access time. This paper studies the performance advantages of a pipelined cache for a GaAs implementation of the MIPS based architecture using a design methodology that includes long traces of multiprogrammed applications and detailed timing analysis. The study evaluates instruction and data caches with various pipeline depths, cache sizes, block sizes, and refill penalties. The impact on CPU cycle time of these alternatives is also factored into the evaluation. Hardware-based and   software-based strategies are considered for hiding the branch and load delays which may be required to avoid pipeline hazards. The results show that software-based methods for mitigating the penalty of branch delays can be as successful as the hardware-based branch-target buffer approach, despite the code-expansion inherent in the software methods. The situation is similar for load delays; while hardware-based dynamic methods hide more delay cycles than do static approaches, they may give up the advantage by extending the cycle time. Because these methods are quite successful at hiding small numbers of branch and load delays, and because processors with pipelined caches also have shorter CPU cycle times and larger caches, a significant performance advantage is gained by   using two to three pipeline stages to fetch data from the cache.},
booktitle = {Proceedings of the 19th Annual International Symposium on Computer Architecture},
pages = {181–190},
numpages = {10},
location = {Queensland, Australia},
series = {ISCA '92}
}

@article{10.1023/A:1008244007194,
author = {Cavalieri, S.},
title = {On the Use of an Enhanced Hopfield Neural Modelto Solve FMS Performance Optimization Problem},
year = {1998},
issue_date = {March-April 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {2},
issn = {0924-669X},
url = {https://doi.org/10.1023/A:1008244007194},
doi = {10.1023/A:1008244007194},
abstract = {The performance of a Flexible Manufacturing System (FMS) is generally 
linked to its productivity and is often limited by poor use of available 
resources. One of the main goals in the automated factory environment is, 
therefore, the exploitation of resources to the full, in such a way as to 
optimize productivity. As widely documented in literature, this is a hard 
task on account of its computational complexity. For this reason a number 
of heuristic techniques are currently available, the best known of which 
are based on Event Graphs, which are a particular class of Petri Nets. The 
paper proposes a performance optimization technique which, although it is 
based on Event Graphs, applies algorithms which are different from 
traditional heuristic ones. More specifically, a novel neural model is 
used to solve the optimization problem. The neural model was obtained by 
making significant changes to a network that is well known in literature: 
the Hopfield network. The solution proposed is an original one and 
features several advantages against the most known heuristic approaches to 
the problem, the most important of which is the possibility of optimal or 
close to optimal solutions in a polynomial time, proportional to the size 
of the FMS. In addition, the possibility of 
simple, economical hardware implementation of the neural model favours its 
integration in the automated factory environment, allowing real-time 
supervision and optimization of productivity. The aim of the paper is to 
present the new neural model and its use in optimizing the performance of 
FMSs. A comparison of the neural approach with 
classical heuristic solutions and its real-time calculation capability, 
will also be treated in the paper.},
journal = {Applied Intelligence},
month = mar,
pages = {123–138},
numpages = {16},
keywords = {Hopfield neural network, flexible manufacturing system, optimization problem}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {boolean functions, feature interactions, fourier transform, performance},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3468891.3468898,
author = {Feng, Wenlong and Dong, Wei and Zhai, Shouchao and Zhang, Guohua and Sun, Xinya and Ji, Yindong},
title = {A Deep Reinforcement Learning Method for Freight Train Driving Based on Domain Knowledge and Mass Estimation Network},
year = {2021},
isbn = {9781450389402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468891.3468898},
doi = {10.1145/3468891.3468898},
abstract = {In train marshalling, the mass of the freight train will dynamically change in a wide range, which is the main difficulty in realizing its automatic driving. This paper proposes a deep reinforcement learning method that combines domain knowledge and mass estimation network (MEN). The domain knowledge of excellent drivers is utilized to accelerate the convergence speed of the algorithm and improve the driving performance. Furthermore, the MEN is introduced for estimating the mass of the entire train during driving. Finally, the deep reinforcement learning algorithm selects the output gear based on the estimated mass. The simulation results show that the proposed method has significant effects on performance optimization such as reducing parking error, improving marshalling efficiency, optimizing coupler force and reducing jerk.},
booktitle = {Proceedings of the 2021 6th International Conference on Machine Learning Technologies},
pages = {41–46},
numpages = {6},
keywords = {automatic driving, domain knowledge, freight train, reinforcement learning},
location = {Jeju Island, Republic of Korea},
series = {ICMLT '21}
}

@inproceedings{10.1145/3126858.3126880,
author = {Dilli, Renato and Filho, Huberto Kaiser and Pernas, Ana Marilza and Yamin, Adenauer},
title = {EXEHDA-RR: Machine Learning and MCDA with Semantic Web in IoT Resources Classification},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126880},
doi = {10.1145/3126858.3126880},
abstract = {Currently, a lot of resources are connected to the Internet, many simultaneously requesting and providing services. The adequate selection of resources that best meet the demands of users with a broad range of options has been a relevant and current research challenge. Based on the non-functional parameters of QoS play a significant role in the ranking of these resources according to the services they offer. This paper aims to aggregate machine learning in the pre-classification of EXEHDA middleware resources, to reduce the computational cost generated by MCDA algorithms. We presented the proposed software architecture (EXEHDA-RR), and the obtained results with the integration of machine learning in the classification process are promissing, and indicate to the research continuation.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {293–300},
numpages = {8},
keywords = {internet of things, machine learning, mcda, resource ranking},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.5555/3104635.3104716,
author = {Cavalieri, S.},
title = {Performance optimization of flexible manufacturing systems using artificial neural networks},
year = {1996},
isbn = {9056995243},
publisher = {Gordon and Breach Science Publishers, Inc.},
address = {USA},
booktitle = {Proceedings of the 9th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems},
pages = {479–486},
numpages = {8},
location = {Fukuoka, Japan},
series = {IEA/AIE'96}
}

@article{10.1016/j.neucom.2019.12.070,
author = {Dong, Zihao and Zhang, Ruixun and Shao, Xiuli and Li, Yumeng},
title = {Scale-Recursive Network with point supervision for crowd scene analysis},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {384},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.12.070},
doi = {10.1016/j.neucom.2019.12.070},
journal = {Neurocomput.},
month = apr,
pages = {314–324},
numpages = {11},
keywords = {Crowd density, Scale-Recursive, Crowd counting, Weakly supervised learning, Joint training}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@inproceedings{10.5555/1884371.1884417,
author = {S\o{}gaard, Anders and Rish\o{}j, Christian},
title = {The effect of semi-supervised learning on parsing long distance dependencies in German and Swedish},
year = {2010},
isbn = {3642147690},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper shows how the best data-driven dependency parsers available today [1] can be improved by learning from unlabeled data. We focus on German and Swedish and show that labeled attachment scores improve by 1.5%-2.5%. Error analysis shows that improvements are primarily due to better recovery of long distance dependencies.},
booktitle = {Proceedings of the 7th International Conference on Advances in Natural Language Processing},
pages = {406–417},
numpages = {12},
keywords = {dependency parsing, long distance dependencies, semi-supervised learning},
location = {Reykjavik, Iceland},
series = {IceTAL'10}
}

@inproceedings{10.5555/3540261.3541447,
author = {Sitawarin, Chawin and Kornaropoulos, Evgenios M. and Song, Dawn and Wagner, David},
title = {Adversarial examples for k-nearest neighbor classifiers based on higher-order voronoi diagrams},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Adversarial examples are a widely studied phenomenon in machine learning models. While most of the attention has been focused on neural networks, other practical models also suffer from this issue. In this work, we propose an algorithm for evaluating the adversarial robustness of k-nearest neighbor classification, i.e., finding a minimum-norm adversarial example. Diverging from previous proposals, we propose the first geometric approach by performing a search that expands outwards from a given input point. On a high level, the search radius expands to the nearby higher-order Voronoi cells until we find a cell that classifies differently from the input point. To scale the algorithm to a large k, we introduce approximation steps that find perturbation with smaller norm, compared to the baselines, in a variety of datasets. Furthermore, we analyze the structural properties of a dataset where our approach outperforms the competition.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1186},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@inproceedings{10.1145/2897937.2898060,
author = {Mirhoseini, Azalia and Rouhani, Bita Darvish and Songhori, Ebrahim M. and Koushanfar, Farinaz},
title = {Perform-ML: performance optimized machine learning by platform and content aware customization},
year = {2016},
isbn = {9781450342360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897937.2898060},
doi = {10.1145/2897937.2898060},
abstract = {We propose Perform-ML, the first Machine Learning (ML) framework for analysis of massive and dense data which customizes the algorithm to the underlying platform for the purpose of achieving optimized resource efficiency. Perform-ML creates a performance model quantifying the computational cost of iterative analysis algorithms on a pertinent platform in terms of FLOPs, communication, and memory, which characterize runtime, storage, and energy. The core of Perform-ML is a novel parametric data projection algorithm, called Elastic Dictionary (ExD), that enables versatile and sparse representations of the data which can help in minimizing performance cost. We show that Perform-ML can achieve the optimal performance objective, according to our cost model, by platform aware tuning of the ExD parameters. An accompanying API ensures automated applicability of Perform-ML to various algorithms, datasets, and platforms. Proof-of-concept evaluations of massive and dense data on different platforms demonstrate more than an order of magnitude improvements in performance compared to the state of the art, within guaranteed user-defined error bounds.},
booktitle = {Proceedings of the 53rd Annual Design Automation Conference},
articleno = {20},
numpages = {6},
location = {Austin, Texas},
series = {DAC '16}
}

@inproceedings{10.4108/eai.14-10-2015.2261719,
author = {Janidarmian, Majid and Roshan Fekr, Atena and Radecka, Katarzyna and Zilic, Zeljko and Ross, Louis},
title = {Analysis of Motion Patterns for Recognition of Human Activities},
year = {2015},
isbn = {9781631900884},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/eai.14-10-2015.2261719},
doi = {10.4108/eai.14-10-2015.2261719},
abstract = {Automatic recognition of human activity is one of the most important and challenging open areas of research in context-aware and physical training applications. The activity profiling systems normally use wearable sensors to record motion patterns over extended periods of time. The performance of these systems depends on the activity set, training data quality, extracted features and learning algorithms. In this paper, we describe the development of efficient activity recognition techniques using wearable motion sensors. An extensive evaluation is provided on three well-known classifiers with light-weight time series features to distinguish among thirty three different fitness activities. The effects of the segmentation, sensor placement as well as different sampling rates on the classification performance are discussed. The experimental results conducted with 17 subjects show an improvement of the classification accuracy compared with the previous work. In addition, the statistical analysis of the investigated models quantifies relative effects of the accelerometer axes reductions on the classification performance.},
booktitle = {Proceedings of the 5th EAI International Conference on Wireless Mobile Communication and Healthcare},
pages = {68–72},
numpages = {5},
keywords = {activity recognition, classification, machine learning, performance optimization, wearable sensors},
location = {London, Great Britain},
series = {MOBIHEALTH'15}
}

@inproceedings{10.5555/3277355.3277416,
author = {Qiao, Aurick and Aghayev, Abutalib and Yu, Weiren and Chen, Haoyang and Ho, Qirong and Gibson, Garth A. and Xing, Eric P.},
title = {Litz: elastic framework for high-performance distributed machine learning},
year = {2018},
isbn = {9781931971447},
publisher = {USENIX Association},
address = {USA},
abstract = {Machine Learning (ML) is an increasingly popular application in the cloud and data-center, inspiring new algorithmic and systems techniques that leverage unique properties of ML applications to improve their distributed performance by orders of magnitude. However, applications built using these techniques tend to be static, unable to elastically adapt to the changing resource availability that is characteristic of multi-tenant environments. Existing distributed frameworks are either inelastic, or offer programming models which are incompatible with the techniques employed by high-performance ML applications.Motivated by these trends, we present Litz, an elastic framework supporting distributed ML applications. We categorize the wide variety of techniques employed by these applications into three general themes -- stateful workers, model scheduling, and relaxed consistency -- which are collectively supported by Litz's programming model. Our implementation of Litz's execution system transparently enables elasticity and low-overhead execution.We implement several popular ML applications using Litz, and show that they can scale in and out quickly to adapt to changing resource availability, as well as how a scheduler can leverage elasticity for faster job completion and more efficient resource allocation. Lastly, we show that Litz enables elasticity without compromising performance, achieving competitive performance with state-of-the-art non-elastic ML frameworks.},
booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
pages = {631–643},
numpages = {13},
location = {Boston, MA, USA},
series = {USENIX ATC '18}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Commonality decision, Product platform scope, Software product line engineering}
}

@inproceedings{10.1007/978-3-030-87589-3_19,
author = {Yan, Yutong and Conze, Pierre-Henri and Lamard, Mathieu and Zhang, Heng and Quellec, Gwenol\'{e} and Cochener, B\'{e}atrice and Coatrieux, Gouenou},
title = {Deep Active Learning for Dual-View Mammogram Analysis},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_19},
doi = {10.1007/978-3-030-87589-3_19},
abstract = {Supervised deep learning on medical imaging requires massive manual annotations, which are expertise-needed and time-consuming to perform. Active learning aims at reducing annotation efforts by adaptively selecting the most informative samples for labeling. We propose in this paper a novel deep active learning approach for dual-view mammogram analysis, especially for breast mass segmentation and detection, where the necessity of labeling is estimated by exploiting the consistency of predictions arising from craniocaudal (CC) and mediolateral-oblique (MLO) views. Intuitively, if mass segmentation or detection is robustly performed, prediction results achieved on CC and MLO views should be consistent. Exploiting the inter-view consistency is hence a good way to guide the sampling mechanism which iteratively selects the next image pairs to be labeled by an oracle. Experiments on public DDSM-CBIS and INbreast datasets demonstrate that comparable performance with respect to fully-supervised models can be reached using only 6.83% (9.56%) of labeled data for segmentation (detection). This suggests that combining dual-view mammogram analysis and active learning can strongly contribute to the development of computer-aided diagnosis systems.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {180–189},
numpages = {10},
keywords = {Breast cancer, Mass segmentation, Mass detection, Dual-view mammogram analysis, Active learning, Computer-aided diagnosis},
location = {Strasbourg, France}
}

@inproceedings{10.1145/2499777.2500714,
author = {Huang, Changyun and Kamei, Yasutaka and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
title = {Using alloy to support feature-based DSL construction for mining software repositories},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500714},
doi = {10.1145/2499777.2500714},
abstract = {The Mining Software Repositories (MSR) field reveals knowledge for software development by analyzing data stored in repositories such as source control and bug trace systems. In order to reveal the knowledge, MSR researchers need to perform complicated procedures iteratively. To help the complex work of MSR practitioners, we study the construction of domain specific languages (DSLs) for MSR. We have conducted feature-oriented domain analysis (FODA) on MSR and developed a DSL based on the feature model. In this paper, we expand our previous work and propose to construct not a single DSL but a DSL family. A DSL family consists of a series of DSLs with commonality in their domain but suitable to specific applications of MSR. To readily construct these DSLs, we use Alloy to encode the feature model. Our encoding includes not only the DSL features and their relations but also some composition rules that can be used to generate the syntax of DSLs. Based on this, we can automatically derive the language elements to construct DSLs suitable to specific purposes of MSR.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {86–89},
numpages = {4},
keywords = {DSL, FODA, SPL, mining software repositories},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3479242.3487320,
author = {Felemban, Noor and Mehmeti, Fidan and La Porta, Thomas},
title = {VidQ: Video Query Using Optimized Audio-Visual Processing},
year = {2021},
isbn = {9781450390804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479242.3487320},
doi = {10.1145/3479242.3487320},
abstract = {As the amount of recorded and stored videos on mobile devices increase, efficient techniques for searching video content become more and more important, especially for applications like searching for the moment of crime or other specific actions. When a user sends a query searching for a specific action in a large amount of data, the goal is to respond to the query accurately and fast. In this paper, we address the problem of responding to queries which search for specific actions in mobile devices in a timely manner by utilizing both visual and audio content processing approaches. We build a system, called VidQ, which consists of several stages and uses various Convolutional Neural Networks (CNNs) and Speech APIs to respond to such queries. As the state-of-the-art computer vision and speech algorithms are computationally intensive, we use servers with GPUs to assist mobile users in the process. After a query has been issued, we identify the possible different stages of processing that will take place. This is followed by identifying the order of these stages that build up the system. Finally, we distribute the process among the available network resources to further improve the performance by minimizing the processing time. Results show that VidQ reduces the completion time by at least 50% compared to other approaches.},
booktitle = {Proceedings of the 17th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {51–60},
numpages = {10},
keywords = {deep learning, mobile networks, performance optimization},
location = {Alicante, Spain},
series = {Q2SWinet '21}
}

@inproceedings{10.1007/978-3-030-21290-2_42,
author = {Reinhartz-Berger, Iris and Shimshoni, Ilan and Abdal, Aviva},
title = {Behavior-Derived Variability Analysis: Mining Views for Comparison and Evaluation},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_42},
doi = {10.1007/978-3-030-21290-2_42},
abstract = {The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {675–690},
numpages = {16},
keywords = {Software Product Line Engineering, Variability analysis, Requirements specifications, Feature diagrams},
location = {Rome, Italy}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.5555/1855591.1855592,
author = {Ganapathi, Archana and Datta, Kaushik and Fox, Armando and Patterson, David},
title = {A case for machine learning to optimize multicore performance},
year = {2009},
publisher = {USENIX Association},
address = {USA},
abstract = {Multicore architectures have become so complex and diverse that there is no obvious path to achieving good performance. Hundreds of code transformations, compiler flags, architectural features and optimization parameters result in a search space that can take many machinemonths to explore exhaustively. Inspired by successes in the systems community, we apply state-of-the-art machine learning techniques to explore this space more intelligently. On 7-point and 27-point stencil code, our technique takes about two hours to discover a configuration whose performance is within 1% of and up to 18% better than that achieved by a human expert. This factor of 2000 speedup over manual exploration of the auto-tuning parameter space enables us to explore optimizations that were previously off-limits. We believe the opportunity for using machine learning in multicore autotuning is even more promising than the successes to date in the systems literature.},
booktitle = {Proceedings of the First USENIX Conference on Hot Topics in Parallelism},
pages = {1},
numpages = {1},
location = {Berkeley, California},
series = {HotPar'09}
}

@inproceedings{10.1145/2024724.2024746,
author = {Ge, Yang and Qiu, Qinru},
title = {Dynamic thermal management for multimedia applications using machine learning},
year = {2011},
isbn = {9781450306362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024724.2024746},
doi = {10.1145/2024724.2024746},
abstract = {Multimedia applications are expected to form the largest portion of workload in general purpose PC and portable devices. The ever-increasing computation intensity of multimedia applications elevates the processor temperature and consequently impairs the reliability and performance of the system. In this paper, we propose to perform dynamic thermal management using reinforcement learning algorithm for multimedia applications. The proposed learning model does not need any prior knowledge of the workload information or the system thermal and power characteristics. It learns the temperature change and workload switching patterns by observing the temperature sensor and event counters on the processor, and finds the management policy that provides good performance-thermal tradeoff during the runtime. We validated our model on a Dell personal computer with Intel Core 2 processor. Experimental results show that our approach provides considerable performance improvements with marginal increase in the percentage of thermal hotspot comparing to existing workload phase detection approach.},
booktitle = {Proceedings of the 48th Design Automation Conference},
pages = {95–100},
numpages = {6},
keywords = {dynamic, multimedia application, reinforcement learning, thermal management},
location = {San Diego, California},
series = {DAC '11}
}

@inproceedings{10.1145/1654059.1654116,
author = {Liao, Shih-wei and Hung, Tzu-Han and Nguyen, Donald and Chou, Chinyen and Tu, Chiaheng and Zhou, Hucheng},
title = {Machine learning-based prefetch optimization for data center applications},
year = {2009},
isbn = {9781605587448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1654059.1654116},
doi = {10.1145/1654059.1654116},
abstract = {Performance tuning for data centers is essential and complicated. It is important since a data center comprises thousands of machines and thus a single-digit performance improvement can significantly reduce cost and power consumption. Unfortunately, it is extremely difficult as data centers are dynamic environments where applications are frequently released and servers are continually upgraded.In this paper, we study the effectiveness of different processor prefetch configurations, which can greatly influence the performance of memory system and the overall data center. We observe a wide performance gap when comparing the worst and best configurations, from 1.4% to 75.1%, for 11 important data center applications. We then develop a tuning framework which attempts to predict the optimal configuration based on hardware performance counters. The framework achieves performance within 1% of the best performance of any single configuration for the same set of applications.},
booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
articleno = {56},
numpages = {10},
location = {Portland, Oregon},
series = {SC '09}
}

@article{10.1016/j.neucom.2004.11.025,
author = {Valova, Iren and Szer, Daniel and Gueorguieva, Natacha and Buer, Alexandre},
title = {A parallel growing architecture for self-organizing maps with unsupervised learning},
year = {2005},
issue_date = {October, 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2004.11.025},
doi = {10.1016/j.neucom.2004.11.025},
abstract = {Self-organizing maps (SOMs) have become popular for tasks in data visualization, pattern classification or natural language processing and can be seen as one of the major concepts for artificial neural networks of today. Their general idea is to approximate a high dimensional and previously unknown input distribution by a lower-dimensional neural network structure with the goal to model the topology of the input space as close as possible. Classical SOMs read the input values in random but sequential order one by one and thus adjust the network structure over space: the network will be built while reading larger and larger parts of the input. In contrast to this approach, we present a SOM that processes the whole input in parallel and organizes itself over time. The main reason for parallel input processing lies in the fact that knowledge can be used to recognize parts of patterns in the input space that have already been learned. This way, networks can be developed that do not reorganize their structure from scratch every time a new set of input vectors is presented, but rather adjust their internal architecture in accordance with previous mappings. One basic application could be a modeling of the whole-part relationship through layered architectures.},
journal = {Neurocomput.},
month = oct,
pages = {177–195},
numpages = {19},
keywords = {Growing architectures, Parallel learning algorithms, Self-organizing map}
}

@inproceedings{10.1007/978-3-030-78713-4_16,
author = {Burchard, Luk and Moe, Johannes and Schroeder, Daniel Thilo and Pogorelov, Konstantin and Langguth, Johannes},
title = {iPUG: Accelerating Breadth-First Graph&nbsp;Traversals Using Manycore Graphcore&nbsp;IPUs},
year = {2021},
isbn = {978-3-030-78712-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78713-4_16},
doi = {10.1007/978-3-030-78713-4_16},
abstract = {The Graphcore Intelligence Processing Unit (IPU) is a newly developed processor type whose architecture does not rely on the traditional caching hierarchies. Developed to meet the need for more and more data-centric applications, such as machine learning, IPUs combine a dedicated portion of SRAM with each of its numerous cores, resulting in high memory bandwidth at the price of capacity. The proximity of processor cores and memory makes the IPU a promising field of experimentation for graph algorithms since it is the unpredictable, irregular memory accesses that lead to performance losses in traditional processors with pre-caching.This paper aims to test the IPU’s suitability for algorithms with hard-to-predict memory accesses by implementing a breadth-first search (BFS) that complies with the Graph500 specifications. Precisely because of its apparent simplicity, BFS is an established benchmark that is not only subroutine for a variety of more complex graph algorithms, but also allows comparability across a wide range of architectures.We benchmark our IPU code on a wide range of instances and compare its performance to state-of-the-art CPU and GPU codes. The results indicate that the IPU delivers speedups of up&nbsp;to 4\texttimes{} over the fastest competing result on an NVIDIA V100 GPU, with typical speedups of about 1.5\texttimes{} on most test instances.},
booktitle = {High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24 – July 2, 2021, Proceedings},
pages = {291–309},
numpages = {19},
keywords = {IPU, Graph500, BFS, Performance optimization}
}

@article{10.1145/3456873,
author = {Tao, Yida and Tang, Shan and Liu, Yepang and Xu, Zhiwu and Qin, Shengchao},
title = {Speeding Up Data Manipulation Tasks with Alternative Implementations: An Exploratory Study},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3456873},
doi = {10.1145/3456873},
abstract = {As data volume and complexity grow at an unprecedented rate, the performance of data manipulation programs is becoming a major concern for developers. In this article, we study how alternative API choices could improve data manipulation performance while preserving task-specific input/output equivalence. We propose a lightweight approach that leverages the comparative structures in Q&amp;A sites to extracting alternative implementations. On a large dataset of Stack Overflow posts, our approach extracts 5,080&nbsp;pairs of alternative implementations that invoke different data manipulation APIs to solve the same tasks, with an accuracy of 86%. Experiments show that for 15% of the extracted pairs, the faster implementation achieved &gt;10x speedup over its slower alternative. We also characterize 68&nbsp;recurring alternative API pairs from the extraction results to understand the type of APIs that can be used alternatively. To put these findings into practice, we implement a tool, AlterApi7, to automatically optimize real-world data manipulation programs. In the 1,267&nbsp;optimization attempts on the Kaggle dataset, 76% achieved desirable performance improvements with up to orders-of-magnitude speedup. Finally, we discuss notable challenges of using alternative APIs for optimizing data manipulation programs. We hope that our study offers a new perspective on API recommendation and automatic performance optimization.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {49},
numpages = {28},
keywords = {API selection, data manipulation, empirical study, mining software repository, performance optimization}
}

@inproceedings{10.1007/978-3-030-48842-0_5,
author = {Yin, Bohong and Li, Yunchun and Dun, Ming and You, Xin and Yang, Hailong and Luan, Zhongzhi and Qian, Depei},
title = {swGBDT: Efficient Gradient Boosted Decision Tree on Sunway Many-Core Processor},
year = {2020},
isbn = {978-3-030-48841-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48842-0_5},
doi = {10.1007/978-3-030-48842-0_5},
abstract = {Gradient Boosted Decision Trees (GBDT) is a practical machine learning method, which has been widely used in various application fields such as recommendation system. Optimizing the performance of GBDT on heterogeneous many-core processors exposes several challenges such as designing efficient parallelization scheme and mitigating the latency of irregular memory access. In this paper, we propose swGBDT, an efficient GBDT implementation on Sunway processor. In swGBDT, we divide the 64 CPEs in a core group into multiple roles such as loader, saver and worker in order to hide the latency of irregular global memory access. In addition, we partition the data into two granularities such as block and tile to better utilize the LDM on each CPE for data caching. Moreover, we utilize register communication for collaboration among CPEs. Our evaluation with representative datasets shows that swGBDT achieves 4.6 and 2 performance speedup on average compared to the serial implementation on MPE and parallel XGBoost on CPEs respectively.},
booktitle = {Supercomputing Frontiers: 6th Asian Conference, SCFA 2020, Singapore, February 24–27, 2020, Proceedings},
pages = {67–86},
numpages = {20},
keywords = {Gradient Boosted Decision Tree, Many-core processor, Performance optimization},
location = {Singapore, Singapore}
}

@inproceedings{10.1007/978-3-030-64583-0_31,
author = {Galuzzi, Bruno G. and Messina, Enza and Candelieri, Antonio and Archetti, Francesco},
title = {Optimal Scenario-Tree Selection for Multistage Stochastic Programming},
year = {2020},
isbn = {978-3-030-64582-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64583-0_31},
doi = {10.1007/978-3-030-64583-0_31},
abstract = {We propose an algorithmic strategy for Multistage Stochastic Optimization, to learn a decision policy able to provide feasible and optimal decisions for every possible value of the random variables of the problem. The decision policy is built using a scenario-tree based solution combined with a regression model able to provide a decision also for those scenarios not included in the tree. For building an optimal policy, an iterative scenario generation procedure is used which selects through a Bayesian Optimization process the more informative scenario-tree. Some preliminary numerical tests show the validity of such an approach.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part I},
pages = {335–346},
numpages = {12},
keywords = {Multistage stochastic programming, Scenario-tree generation, Supervised learning},
location = {Siena, Italy}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@inproceedings{10.1145/3394486.3403265,
author = {Ueno, Yuichiro and Osawa, Kazuki and Tsuji, Yohei and Naruse, Akira and Yokota, Rio},
title = {Rich Information is Affordable: A Systematic Performance Analysis of Second-order Optimization Using K-FAC},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403265},
doi = {10.1145/3394486.3403265},
abstract = {Rich information matrices from first and second-order derivatives have many potential applications in both theoretical and practical problems in deep learning. However, computing these information matrices is extremely expensive and this enormous cost is currently limiting its application to important problems regarding generalization, hyperparameter tuning, and optimization of deep neural networks. One of the most challenging use cases of information matrices is their use as a preconditioner for the optimizers, since the information matrices need to be updated every step. In this work, we conduct a step-by-step performance analysis when computing the Fisher information matrix during training of ResNet-50 on ImageNet, and show that the overhead can be reduced to the same amount as the cost of performing a single SGD step. We also show that the resulting Fisher preconditioned optimizer can converge in 1/3 the number of epochs compared to SGD, while achieving the same Top-1 validation accuracy. This is the first work to achieve such accuracy with K-FAC while reducing the training time to match that of SGD.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2145–2153},
numpages = {9},
keywords = {distributed training, information matrix, performance optimization},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1016/j.adhoc.2019.101881,
author = {Fontaine, Jaron and Fonseca, Erika and Shahid, Adnan and Kist, Maicon and A. DaSilva, Luiz and Moerman, Ingrid and De Poorter, Eli},
title = {Towards low-complexity wireless technology classification across multiple environments},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2019.101881},
doi = {10.1016/j.adhoc.2019.101881},
journal = {Ad Hoc Netw.},
month = aug,
numpages = {12},
keywords = {Manual feature extraction, Automatic feature learning, Wireless technology classification, Machine learning, CNN}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Semi-supervised learning, Speech emotion recognition, Domain adaptation, Deep neural networks}
}

@inproceedings{10.1145/3472456.3472515,
author = {Ye, Qianwen and Liu, Wuji and Wu, Chase Q.},
title = {NoStop: A Novel Configuration Optimization Scheme for Spark Streaming},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472515},
doi = {10.1145/3472456.3472515},
abstract = {An increasing number of big data applications in various domains generate datasets continuously, which must be processed for various purposes in a timely manner. As one of the most popular streaming data processing systems, Spark Streaming applies a batch-based mechanism, which receives real-time input data streams and divides the data into multiple batches before passing them to Spark processing engine. As such, inappropriate system configurations including batch interval and executor count may lead to unstable states, hence undermining the capability and efficiency of real-time computing. Hence, determining suitable configurations is crucial to the performance of such systems. Many machine learning- and search-based algorithms have been proposed to provide configuration recommendations for streaming applications where input data streams are fed at a constant speed, which, however, is extremely rare in practice. Most real-life streaming applications process data streams arriving at a time-varying rate and hence require real-time system monitoring and continuous configuration adjustment, which still remains largely unexplored. We propose a novel streaming optimization scheme based on Simultaneous Perturbation Stochastic Approximation (SPSA), referred to as NoStop, which dynamically tunes system configurations to optimize real-time system performance with negligible overhead and proved convergence. The performance superiority of NoStop is illustrated by real-life experiments in comparison with Bayesian Optimization and Spark Back Pressure solutions. Extensive experimental results show that NoStop is able to keep track of the changing pattern of input data in real time and provide optimal configuration settings to achieve the best system performance. This optimization scheme could also be applied to other streaming data processing engines with tunable parameters.},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {59},
numpages = {10},
keywords = {Big Data Systems., Performance Optimization, Spark Streaming, Stochastic Approximation},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@article{10.1007/s10270-011-0220-1,
author = {Hubaux, Arnaud and Heymans, Patrick and Schobbens, Pierre-Yves and Deridder, Dirk and Abbasi, Ebrahim Khalil},
title = {Supporting multiple perspectives in feature-based configuration},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0220-1},
doi = {10.1007/s10270-011-0220-1},
abstract = {Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {641–663},
numpages = {23},
keywords = {Feature diagram, Feature-based configuration, Multi-view, Separation of concerns, Software product line engineering}
}

@inproceedings{10.1007/978-3-030-61616-8_34,
author = {Sedlmeier, Andreas and M\"{u}ller, Robert and Illium, Steffen and Linnhoff-Popien, Claudia},
title = {Policy Entropy for Out-of-Distribution Classification},
year = {2020},
isbn = {978-3-030-61615-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61616-8_34},
doi = {10.1007/978-3-030-61616-8_34},
abstract = {One critical prerequisite for the deployment of reinforcement learning systems in the real world is the ability to reliably detect situations on which the agent was not trained. Such situations could lead to potential safety risks when wrong predictions lead to the execution of harmful actions. In this work, we propose PEOC, a new policy entropy based out-of-distribution classifier that reliably detects unencountered states in deep reinforcement learning. It is based on using the entropy of an agent’s policy as the classification score of a one-class classifier. We evaluate our approach using a procedural environment generator. Results show that PEOC is highly competitive against state-of-the-art one-class classification algorithms on the evaluated environments. Furthermore, we present a structured process for benchmarking out-of-distribution classification in reinforcement learning.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part II},
pages = {420–431},
numpages = {12},
keywords = {Out-of-distribution classification, Policy entropy, Deep reinforcement&nbsp;learning},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.infsof.2009.10.010,
author = {Casamayor, Agustin and Godoy, Daniela and Campo, Marcelo},
title = {Identification of non-functional requirements in textual specifications: A semi-supervised learning approach},
year = {2010},
issue_date = {April, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.10.010},
doi = {10.1016/j.infsof.2009.10.010},
abstract = {Context: Early detection of non-functional requirements (NFRs) is crucial in the evaluation of architectural alternatives starting from initial design decisions. The application of supervised text categorization strategies for requirements expressed in natural language has been proposed in several works as a method to help analysts in the detection and classification of NFRs concerning different aspects of software. However, a significant number of pre-categorized requirements are needed to train supervised text classifiers, which implies that analysts have to manually assign categories to numerous requirements before being able of accurately classifying the remaining ones. Objective: We propose a semi-supervised text categorization approach for the automatic identification and classification of non-functional requirements. Therefore, a small number of requirements, possibly identified by the requirement team during the elicitation process, enable learning an initial classifier for NFRs, which could successively identify the type of further requirements in an iterative process. The goal of the approach is the integration into a recommender system to assist requirement analysts and software designers in the architectural design process. Method: Detection and classification of NFRs is performed using semi-supervised learning techniques. Classification is based on a reduced number of categorized requirements by taking advantage of the knowledge provided by uncategorized ones, as well as certain properties of text. The learning method also exploits feedback from users to enhance classification performance. Results: The semi-supervised approach resulted in accuracy rates above 70%, considerably higher than the results obtained with supervised methods using standard collections of documents. Conclusion: Empirical evidence showed that semi-supervision requires less human effort in labeling requirements than fully supervised methods, and can be further improved based on feedback provided by analysts. Our approach outperforms previous supervised classification proposals and can be further enhanced by exploiting feedback provided by analysts.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {436–445},
numpages = {10},
keywords = {Non-functional requirements, Requirement classification, Semi-supervised text learning}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.3103/S1060992X19020048,
author = {Yakovenko, A. A.},
title = {A Hybrid Learning Approach for Adaptive Classification of Acoustic Signals Using the Simulated Responses of Auditory Nerve Fibers},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X19020048},
doi = {10.3103/S1060992X19020048},
journal = {Opt. Mem. Neural Netw.},
month = apr,
pages = {118–128},
numpages = {11},
keywords = {adaptive pattern classification, auditory periphery model, machine perception, neural responses, radial basis functions, self-organizing maps, unsupervised learning}
}

@inproceedings{10.1145/1998582.1998587,
author = {Eastep, Jonathan and Wingate, David and Agarwal, Anant},
title = {Smart data structures: an online machine learning approach to multicore data structures},
year = {2011},
isbn = {9781450306072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1998582.1998587},
doi = {10.1145/1998582.1998587},
abstract = {As multicores become prevalent, the complexity of programming is skyrocketing. One major difficulty is efficiently orchestrating collaboration among threads through shared data structures. Unfortunately, choosing and hand-tuning data structure algorithms to get good performance across a variety of machines and inputs is a herculean task to add to the fundamental difficulty of getting a parallel program correct. To help mitigate these complexities, this work develops a new class of parallel data structures called Smart Data Structures that leverage online machine learning to adapt automatically. We prototype and evaluate an open source library of Smart Data Structures for common parallel programming needs and demonstrate significant improvements over the best existing algorithms under a variety of conditions. Our results indicate that learning is a promising technique for balancing and adapting to complex, time-varying tradeoffs and achieving the best performance available.},
booktitle = {Proceedings of the 8th ACM International Conference on Autonomic Computing},
pages = {11–20},
numpages = {10},
keywords = {auto-tuning, autonomic, concurrent data structures, performance optimization, self-aware, synchronization},
location = {Karlsruhe, Germany},
series = {ICAC '11}
}

@inproceedings{10.1007/978-3-030-64694-3_17,
author = {Benmerzoug, Amine and Yessad, Lamia and Ziadi, Tewfik},
title = {Analyzing the Impact of Refactoring Variants on Feature Location},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_17},
doi = {10.1007/978-3-030-64694-3_17},
abstract = {Due to the increasing importance of feature location process, several studies evaluate the performance of different techniques based on IR strategies and a set of software variants as input artifacts. The proposed techniques attempt to improve the results obtained but it is often a difficult task. None of the existing feature location techniques considers the changing nature of the input artifacts, which may undergo series of refactoring changes. In this paper, we investigate the impact of refactoring variants on the feature location techniques. We first evaluate the performance of two techniques through the ArgoUML SPL benchmark when the variants are refactored. We then discuss the degraded results and the possibility of restoring them. Finally, we outline a process of variant alignment that aims to preserve the performance of the feature location.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {279–291},
numpages = {13},
keywords = {Software Product Line, Feature location, Refactoring},
location = {Hammamet, Tunisia}
}

@article{10.1016/j.asoc.2016.07.048,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
title = {Extracting features from online software reviews to aid requirements reuse},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.048},
doi = {10.1016/j.asoc.2016.07.048},
abstract = {Display Omitted The extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have the access.Online reviews for software products can be used as input for features extraction to assist requirements reuse.Techniques from unsupervised learning and Natural Language Processing is employed as a propose solutions to Requirements Reuse problem.The approach obtained a precision of 87% (62% average) and a recall of 86% (82% average), when evaluated against the truth data set created manually. Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1297–1315},
numpages = {19},
keywords = {Latent semantic analysis, Natural language processing, Requirements reuse, Software engineering, Unsupervised learning}
}

@inproceedings{10.1145/127601.122882,
author = {Tsay, Ren-Song and Koehl, Juergen},
title = {An analytic net weighting approach for performance optimization in circuit placement},
year = {1991},
isbn = {0897913957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/127601.122882},
doi = {10.1145/127601.122882},
booktitle = {Proceedings of the 28th ACM/IEEE Design Automation Conference},
pages = {620–625},
numpages = {6},
location = {San Francisco, California, USA},
series = {DAC '91}
}

@inproceedings{10.1145/3404397.3404426,
author = {Kruli\v{s}, Martin and Kratochv\'{\i}l, Miroslav},
title = {Detailed Analysis and Optimization of CUDA K-means Algorithm},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404426},
doi = {10.1145/3404397.3404426},
abstract = {K-means is one of the most frequently used algorithms for unsupervised clustering data analysis. Individual steps of the k-means algorithm include nearest neighbor finding, efficient distance computation, and cluster-wise reduction, which may be generalized to many other purposes in data analysis, visualization, and machine learning. Efficiency of the available implementations of k-means computation steps therefore directly affect many other applications. In this work, we examine the performance limits in the context of modern massively parallel GPU accelerators. Despite the existence of many published papers on this topic, we have found that crucial performance aspects of the GPU implementations remain unaddressed, including the optimizations for memory bandwidth, cache limits, and workload dispatching on problem instances of varying cluster count, dataset size, and dimensionality. We present a detailed analysis of individual computation steps and propose several optimizations that improve the overall performance on contemporary GPU architectures. Our open-source prototype exhibits significant speedup over the current state-of-the-art implementations in virtually all practical scenarios.},
booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
articleno = {69},
numpages = {11},
keywords = {clustering, cuda, k-means, performance optimization},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@article{10.1007/s10664-021-09940-0,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {An empirical investigation of organic software product lines},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09940-0},
doi = {10.1007/s10664-021-09940-0},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems that is increasingly being applied to novel and emerging domains. In this work we investigate the use of software product line engineering in one of these new domains, synthetic biology. In synthetic biology living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse DNA parts. In this paper we perform an investigation of domain engineering that leverages an open-source repository of more than 45,000 reusable DNA parts. We show the feasibility of these new types of product line models by identifying features and related artifacts in up to 93.5% of products, and that there is indeed both commonality and variability. We then construct feature models for four commonly engineered functions leading to product lines ranging from 10 to 7.5 \texttimes{} 1020 products. In a case study we demonstrate how we can use the feature models to help guide new experimentation in aspects of application engineering. Finally, in an empirical study we demonstrate the effectiveness and efficiency of automated reverse engineering on both complete and incomplete sets of products. In the process of these studies, we highlight key challenges and uncovered limitations of existing SPL techniques and tools which provide a roadmap for making SPL engineering applicable to new and emerging domains.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {43},
keywords = {Software product lines, Synthetic biology, Reverse engineering, BioBricks}
}

@article{10.1016/j.patcog.2021.108164,
author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
title = {Training object detectors from few weakly-labeled and many unlabeled images},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108164},
doi = {10.1016/j.patcog.2021.108164},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Object detection, Weakly-supervised learning, Semi-supervised learning, Unlabelled set}
}

@inproceedings{10.1007/978-3-030-87196-3_28,
author = {Wu, Yicheng and Xu, Minfeng and Ge, Zongyuan and Cai, Jianfei and Zhang, Lei},
title = {Semi-supervised Left Atrium Segmentation with Mutual Consistency&nbsp;Training},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_28},
doi = {10.1007/978-3-030-87196-3_28},
abstract = {Semi-supervised learning has attracted great attention in the field of machine learning, especially for medical image segmentation tasks, since it alleviates the heavy burden of collecting abundant densely annotated data for training. However, most of existing methods underestimate the importance of challenging regions (e.g. small branches or blurred edges) during training. We believe that these unlabeled regions may contain more crucial information to minimize the uncertainty prediction for the model and should be emphasized in the training process. Therefore, in this paper, we propose a novel Mutual Consistency Network (MC-Net) for semi-supervised left atrium segmentation from 3D MR images. Particularly, our MC-Net consists of one encoder and two slightly different decoders, and the prediction discrepancies of two decoders are transformed as an unsupervised loss by our designed cycled pseudo label scheme to encourage mutual consistency. Such mutual consistency encourages the two decoders to have consistent and low-entropy predictions and enables the model to gradually capture generalized features from these unlabeled challenging regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it obtains impressive performance gains by exploiting the unlabeled data effectively. Our MC-Net outperforms six recent semi-supervised methods for left atrium segmentation, and sets the new state-of-the-art performance on the LA database.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {297–306},
numpages = {10},
keywords = {Semi-supervised learning, Mutual consistency, Cycled pseudo label},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3452296.3472935,
author = {Zhang, Xiao and Sen, Tanmoy and Zhang, Zheyuan and April, Tim and Chandrasekaran, Balakrishnan and Choffnes, David and Maggs, Bruce M. and Shen, Haiying and Sitaraman, Ramesh K. and Yang, Xiaowei},
title = {AnyOpt: predicting and optimizing IP Anycast performance},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472935},
doi = {10.1145/3452296.3472935},
abstract = {The key to optimizing the performance of an anycast-based system (e.g., the root DNS or a CDN) is choosing the right set of sites to announce the anycast prefix. One challenge here is predicting catchments. A na\"{\i}ve approach is to advertise the prefix from all subsets of available sites and choose the best-performing subset, but this does not scale well. We demonstrate that by conducting pairwise experiments between sites peering with tier-1 networks, we can predict the catchments that would result if we announce to any subset of the sites. We prove that our method is effective in a simplified model of BGP, consistent with common BGP routing policies, and evaluate it in a real-world testbed. We then present AnyOpt, a system that predicts anycast catchments. Using AnyOpt, a network operator can find a subset of anycast sites that minimizes client latency without using the na\"{\i}ve approach. In an experiment using 15 sites, each peering with one of six transit providers, AnyOpt predicted site catchments of 15,300 clients with 94.7% accuracy and client RTTs with a mean error of 4.6%. AnyOpt identified a subset of 12 sites, announcing to which lowers the mean RTT to clients by 33ms compared to a greedy approach that enables the same number of sites with the lowest average unicast latency.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {447–462},
numpages = {16},
keywords = {Anycast, BGP, performance optimization, routing},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {curriculum learning, reinforcement learning, self-paced learning, tempered inference, rl-as-inference}
}

@article{10.1016/j.cl.2018.07.005,
author = {Zhou, Wenbo and Liu, Lei and Zhang, Peng and L\"{u}, Shuai and Li, Jingyao},
title = {SDAC: A model for analysis of the execution semantics of data processing framework in cloud},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.07.005},
doi = {10.1016/j.cl.2018.07.005},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {406–426},
numpages = {21},
keywords = {Cloud computing, Data processing framework, Execution semantics, Fault tolerance, Performance optimization}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {co-training, multi-modal, once and for all, self-supervised learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1016/j.infsof.2009.11.001,
author = {Rabiser, Rick and Gr\"{u}nbacher, Paul and Dhungana, Deepak},
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
year = {2010},
issue_date = {March, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.001},
doi = {10.1016/j.infsof.2009.11.001},
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {324–346},
numpages = {23},
keywords = {Product derivation, Product line engineering, Software product line, Systematic literature review}
}

@inproceedings{10.1007/978-3-030-49556-5_4,
author = {Li, Guangli and Wang, Xueying and Ma, Xiu and Liu, Lei and Feng, Xiaobing},
title = {XDN: Towards Efficient Inference of Residual Neural Networks on Cambricon Chips},
year = {2019},
isbn = {978-3-030-49555-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49556-5_4},
doi = {10.1007/978-3-030-49556-5_4},
abstract = {In this paper, we present XDN, an optimization and inference engine for accelerating residual neural networks on Cambricon chips. We leverage a channel pruning method to compress the weights of ResNet-50. By exploring the optimization opportunities in computational graphs, we propose a layer fusion strategy, which dramatically decreases the number of scalar computation layers, such as Batch Normalization, Scale. Furthermore, we design an efficient implementation of XDN, including data preprocessing, hyper-parameter auto-tuning, etc. The experimental results show that the ResNet-50 model can achieve significant speedup without accuracy loss by using our XDN engine.},
booktitle = {Benchmarking, Measuring, and Optimizing: Second BenchCouncil International Symposium, Bench 2019, Denver, CO, USA, November 14–16, 2019, Revised Selected Papers},
pages = {51–56},
numpages = {6},
keywords = {Artificial intelligence systems, Residual neural networks, Cambricon chips, Performance optimization},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/3237009.3237024,
author = {Krauss, Oliver},
title = {Towards a framework for stochastic performance optimizations in compilers and interpreters: an architecture overview},
year = {2018},
isbn = {9781450364249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3237009.3237024},
doi = {10.1145/3237009.3237024},
abstract = {Modern compilers and interpreters provide code optimizations before and during run-time to stay competitive with alternative execution environments, thus moving required domain knowledge about the compilation process away from the developer and speeding up resulting software. These optimizations are often based on formal proof, or alternatively have recovery paths as backup.This publication proposes an architecture utilizing abstract syntax trees (ASTs) to optimize the runtime performance of code with stochastic - search based - machine learning techniques. From these AST modifying optimizations a pattern mining approach attempts to find transformation patterns which are applicable to a software language. The application of these patterns happens during the parsing process or the programs run-time.Future work consists of implementing and extending the presented architecture, with a considerable focus on the mining of transformation patterns.},
booktitle = {Proceedings of the 15th International Conference on Managed Languages &amp; Runtimes},
articleno = {9},
numpages = {7},
keywords = {AST transformation, pattern mining, performance optimization, stochastic optimization},
location = {Linz, Austria},
series = {ManLang '18}
}

@inproceedings{10.1145/3380446.3430631,
author = {Werner, Michael and Servadei, Lorenzo and Wille, Robert and Ecker, Wolfgang},
title = {Automatic compiler optimization on embedded software through k-means clustering},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430631},
doi = {10.1145/3380446.3430631},
abstract = {Generating instead of implementing variable design platforms is becoming increasingly popular in the development of System on Chips. This shift also poses the challenge of rapid compiler optimization that adapts to each newly generated platform. In this paper, we evaluate the impact of 104 compiler flags on memory usage and core execution time against standard optimization levels. Each flag has a different influence on these costs, which is difficult to predict. In this work, we apply cost estimation methods to predict the impact of each flag on the generated core using unsupervised Machine Learning, in the form of k-means clustering. The key strengths of the approach are the low need for data, the adaptability to new cores, and the ease of use. This helps the designer to understand the impact of flags on related applications, showing which combination is optimizing the most. As a result, we can obtain 20,93% optimization on the software size, 3,10% on the performance, and 1,75% on their trade-off beyond the -O3 optimization.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {157–162},
numpages = {6},
keywords = {compiler exploration, embedded systems, kmean-clustering, model driven architecture},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@article{10.1007/s11227-018-2721-y,
author = {Khan, Minhaj Ahmad},
title = {Towards efficient XML parsing through minimization of JVM parameter space},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {7},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2721-y},
doi = {10.1007/s11227-018-2721-y},
abstract = {A significant increase in the usage of Extensible Markup Language (XML) data for various protocols and standards emphasizes the development of efficient XML parsers. For the Java language, the XML DOM parser despite performing in-memory operations is unable to achieve peak execution performance on modern systems, especially for parsing large XML files. The issue of inefficient execution may be mitigated by selecting appropriate runtime parameters for the Java Virtual Machine (JVM). This entails to exploring parameter space in an exhaustive manner that is not practically feasible for rapid application development. This paper aims at performance enhancement of XML parsing through selection of optimal set of JVM runtime parameters. The proposed approach works independent of parser design. It reduces JVM parameter space through machine learning-based models which are trained using profile data. The impact of parameters is determined using linear regression and artificial neural network-based models. The subsequent computation of a location-based weight vector along with a threshold value for filtration of parameters generates a set of optimal parameters for performance enhancement. The XML parsing code using the optimal parameters achieves average speedups of 13.18% and 21.42% over the standard code on Intel Xeon and Intel Core i7-based systems, respectively.},
journal = {J. Supercomput.},
month = jul,
pages = {3693–3711},
numpages = {19},
keywords = {Machine learning, Parameter space exploration, Performance optimization performance estimation, XML parsing}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {Software product line testing, dimensionality reduction, many-objective problems},
location = {Natal, Brazil},
series = {SAST '20}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Software product line, Feature extraction, Information retrieval, Community detection}
}

@inproceedings{10.1007/978-3-030-57675-2_14,
author = {Wang, Xueying and Li, Guangli and Dong, Xiao and Li, Jiansong and Liu, Lei and Feng, Xiaobing},
title = {Accelerating Deep Learning Inference with Cross-Layer Data Reuse on GPUs},
year = {2020},
isbn = {978-3-030-57674-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-57675-2_14},
doi = {10.1007/978-3-030-57675-2_14},
abstract = {Accelerating the deep learning inference is very important for real-time applications. In this paper, we propose a novel method to fuse the layers of convolutional neural networks (CNNs) on Graphics Processing Units (GPUs), which applies data reuse analysis and access optimization in different levels of the memory hierarchy. To achieve the balance between computation and memory access, we explore the fusion opportunities in the CNN computation graph and propose three fusion modes of convolutional neural networks: straight, merge and split. Then, an approach for generating efficient fused code is designed, which goes deeper in multi-level memory usage for cross-layer data reuse. The effectiveness of our method is evaluated with the network layers from state-of-the-art CNNs on two different GPU platforms, NVIDIA TITAN Xp and Tesla P4. The experiments show that the average speedup is 2.02  on representative structures of CNNs, and 1.57 on end-to-end inference of SqueezeNet.},
booktitle = {Euro-Par 2020: Parallel Processing: 26th International Conference on Parallel and Distributed Computing, Warsaw, Poland, August 24–28, 2020, Proceedings},
pages = {219–233},
numpages = {15},
keywords = {Deep learning, Layer fusion, Performance optimization},
location = {Warsaw, Poland}
}

@article{10.1007/s10664-020-09853-4,
author = {Hajri, Ines and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
title = {Automating system test case classification and prioritization for use case-driven testing in product lines},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09853-4},
doi = {10.1007/s10664-020-09853-4},
abstract = {Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation was performed in the context of an industrial product family in the automotive domain. Results provide empirical evidence that we propose a practical and beneficial way to classify and prioritize system test cases for industrial product lines.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3711–3769},
numpages = {59},
keywords = {Product Line Engineering, Use case driven development, Regression testing, Test case selection and prioritization, Automotive, Requirements engineering}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Energy Aware, Machine Learning, Software Product Line, Web System},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/3412841.3441894,
author = {Gartziandia, Aitor and Arrieta, Aitor and Agirre, Aitor and Sagardui, Goiuria and Arratibel, Maite},
title = {Using regression learners to predict performance problems on software updates: a case study on elevators dispatching algorithms},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441894},
doi = {10.1145/3412841.3441894},
abstract = {Remote software deployment and updating has long been commonplace in many different fields, but now, the increasing expansion of IoT and CPSoS (Cyber-Physcal System of Systems) has highlighted the need for additional mechanisms in these systems, to ensure the correct behaviour of the deployed software version after deployment. In this sense, this paper investigates the use of Machine Learning algorithms to predict acceptable behaviour in system performance of a new software release. By monitoring the real performance, eventual unexpected problems can be identified. Based on previous knowledge and actual run-time information, the proposed approach predicts the response time that can be considered acceptable for the new software release, and this information is used to identify problematic releases. The mechanism has been applied to the post-deployment monitoring of traffic algorithms in elevator systems. To evaluate the approach, we have used performance mutation testing, obtaining good results. This paper makes two contributions. First, it proposes several regression learners that have been trained with different types of traffic profiles to efficiently predict response time of the traffic dispatching algorithm. This prediction is then compared with the actual response time of the new algorithm release, and provides a verdict about its performance. Secondly, a comparison of the different learners is performed.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {135–144},
numpages = {10},
keywords = {cyber-physical systems, machine learning, performance bugs},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3377929.3398139,
author = {Liou, Jhe-Yu and Wang, Xiaodong and Forrest, Stephanie and Wu, Carole-Jean},
title = {GEVO-ML: a proposal for optimizing ML code with evolutionary computation},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398139},
doi = {10.1145/3377929.3398139},
abstract = {Parallel accelerators, such as GPUs, are a key enabler of large-scale Machine Learning (ML) applications. However, programmers often lack detailed knowledge of the underlying architecture and fail to fully leverage their computational power. This paper proposes GEVO-ML, a tool for automatically discovering optimization opportunities and tuning the performance of ML kernels. GEVO-ML extends earlier work on GEVO (Gpu optimization using EVOlutionary computation) by focusing directly on ML frameworks, intermediate languages, and target architectures. It retains the multi-objective evolutionary search developed for GEVO, which searches for edits to GPU code compiled to LLVM-IR and improves performance on desired criteria while retaining required functionality. In earlier work, we studied some ML workloads in GPU settings and found that GEVO could improve kernel speeds by factors ranging from 1.7X to 2.9X, even with access to only a small portion of the overall ML framework. This workshop paper examines the limitations and constraints of GEVO for ML workloads and discusses our GEVO-ML design, which we are currently implementing.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1849–1856},
numpages = {8},
keywords = {genetic improvement, machine learning, multi-objective evolutionary computation},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@inproceedings{10.1145/3447818.3460692,
author = {Dun, Ming and Li, Yunchun and Yang, Hailong and Sun, Qingxiao and Luan, Zhongzhi and Qian, Depei},
title = {An optimized tensor completion library for multiple GPUs},
year = {2021},
isbn = {9781450383356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447818.3460692},
doi = {10.1145/3447818.3460692},
abstract = {Tensor computations are gaining wide adoption in big data analysis and artificial intelligence. Among them, tensor completion is used to predict the missing or unobserved value in tensors. The decomposition-based tensor completion algorithms have attracted significant research attention since they exhibit better parallelization and scalability. However, existing optimization techniques for tensor completion cannot sustain the increasing demand for applying tensor completion on ever larger tensor data. To address the above limitations, we develop the first tensor completion library cuTC on multiple Graphics Processing Units (GPUs) with three widely used optimization algorithms such as alternating least squares (ALS), stochastic gradient descent (SGD) and coordinate descent (CCD+). We propose a novel TB-COO format that leverages warp shuffle and shared memory on GPU to enable efficient reduction. In addition, we adopt the auto-tuning method to determine the optimal parameters for better convergence and performance. We compare cuTC with state-of-the-art tensor completion libraries on real-world datasets, and the results show cuTC achieves significant speedup with similar or even better accuracy.},
booktitle = {Proceedings of the 35th ACM International Conference on Supercomputing},
pages = {417–430},
numpages = {14},
keywords = {GPU, performance optimization, tensor completion},
location = {Virtual Event, USA},
series = {ICS '21}
}

@book{10.5555/2531590,
author = {Tian, Phil and Addicam, Sanjay and Malik, Shahzad},
title = {Building Intelligent Systems: Utilizing Computer Vision, Data Mining, and Machine Learning},
year = {2013},
isbn = {193405352X},
publisher = {Intel Press},
abstract = {Consumers are now demanding and expecting more from technology. Building intelligence into our devices is a promising way to satisfy this demand by providing more personalized experiences. In Building Intelligent Systems the authors investigate how computer vision, machine learning, and data mining can be used together to build smarter devices and systems. Additionally, they explore some of the practical considerations of using artificial intelligence in the real world, tackling issues that are often overlooked in academic circles, such as performance optimization, benchmarking, robustness, and privacy.}
}

@inproceedings{10.1145/2909437.2909443,
author = {Gu, Junli and Liu, Yibing and Gao, Yuan and Zhu, Maohua},
title = {OpenCL caffe: Accelerating and enabling a cross platform machine learning framework},
year = {2016},
isbn = {9781450343381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909437.2909443},
doi = {10.1145/2909437.2909443},
abstract = {Deep neural networks (DNN) achieved significant breakthrough in vision recognition in 2012 and quickly became the leading machine learning algorithm in Big Data based large scale object recognition applications. The successful deployment of DNN based applications pose challenges for a cross platform software framework that enable multiple user scenarios, including offline model training on HPC clusters and online recognition in embedded environments. Existing DNN frameworks are mostly focused on a closed format CUDA implementations, which is limiting of deploy breadth of DNN hardware systems.This paper presents OpenCL™ caffe, which targets in transforming the popular CUDA based framework caffe [1] into open standard OpenCL backend. The goal is to enable a heterogeneous platform compatible DNN framework and achieve competitive performance based on OpenCL tool chain. Due to DNN models' high complexity, we use a two-phase strategy. First we introduce the OpenCL porting strategies that guarantee algorithm convergence; then we analyze OpenCL's performance bottlenecks in DNN domain and propose a few optimization techniques including batched manner data layout and multiple command queues to better map the problem size into existing BLAS library, improve hardware resources utilization and boost OpenCL runtime efficiency.We verify OpenCL caffe's successful offline training and online recognition on both server-end and consumer-end GPUs. Experimental results show that the phase-two's optimized OpenCL caffe achieved a 4.5x speedup without modifying BLAS library. The user can directly run mainstream DNN models and achieves the best performance for a specific processors by choosing the optimal batch number depending on H/W properties and input data size.},
booktitle = {Proceedings of the 4th International Workshop on OpenCL},
articleno = {8},
numpages = {5},
keywords = {Deep learning frameworks, Deep neural networks (DNN), OpenCL},
location = {Vienna, Austria},
series = {IWOCL '16}
}

@article{10.1016/j.eswa.2018.04.033,
author = {Sreevani and Murthy, C.A. and Chanda, Bhabatosh},
title = {Generation of compound features based on feature interaction for classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.04.033},
doi = {10.1016/j.eswa.2018.04.033},
journal = {Expert Syst. Appl.},
month = oct,
pages = {61–73},
numpages = {13},
keywords = {Feature extraction, Feature selection, Compound features, Semi-features, Information theory, Feature interaction, Mutual information}
}

@inproceedings{10.1007/978-3-030-38961-1_2,
author = {Jin, Zehui and Dun, Ming and You, Xin and Yang, Hailong and Li, Yunchun and Lin, Yingchun and Luan, Zhongzhi and Qian, Depei},
title = {Improving the Parallelism of CESM on GPU},
year = {2019},
isbn = {978-3-030-38960-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38961-1_2},
doi = {10.1007/978-3-030-38961-1_2},
abstract = {Community Earth System Model (CESM) is one of the most popular climatology research models. However, the computation of CESM is quite expensive and usually lasts for weeks even on high-performance clusters. In this paper, we propose several optimization strategies to improve the parallelism of three hotspots in CESM on GPU. Specifically, we analyze the performance bottleneck of CESM and propose corresponding GPU accelerations. The experiment results show that after applying our GPU optimizations, the kernels of the physical model achieve significant performance speedup respectively.},
booktitle = {Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9–11, 2019, Proceedings, Part II},
pages = {11–18},
numpages = {8},
keywords = {CESM, GPU, Performance optimization},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3458744.3474053,
author = {Meyer, Bruno and Pozo, Aurora and Nunan Zola, Wagner M.},
title = {Warp-centric K-Nearest Neighbor Graphs construction on GPU},
year = {2021},
isbn = {9781450384414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458744.3474053},
doi = {10.1145/3458744.3474053},
abstract = {Recent advances and applications of machine learning algorithms are becoming more common in different fields. It is expected that some applications require the processing of large datasets with those algorithms, which leads to high computational costs. Massively parallel GPU methods can be applied to surpass this limitation and reduce the execution time of these algorithms. The construction of approximate K-Nearest Neighbor Graphs (K-NNG) is frequently required for similarity search or other applications such as the t-SNE dimensionality reduction technique. The K-NNG represents the K closest points (neighbors) for each point in a set. In this paper, we propose and analyze an all-points K-Nearest Neighbor Graph construction algorithm on GPU called Warp-centric K-NNG (w-KNNG), which is based on the Random Projection Forest method. Usually, the construction or search for k-NN sets for high dimensional points presents challenges for its implementation on many-core processing units, due to the space limitation in maintaining these sets in high speed shared memory. We present three warp-centric approaches for our algorithm that efficiently search and maintain the k-NN high dimensional point sets in global memory. In our experiments, the new methods allows the algorithm to achieve up to 639% faster execution when compared to the state-of-the-art FAISS library, considering an equivalent accuracy of approximate K-NNG. One of the new strategies (w-KNNG atomic) is more successful when applied to a smaller number of dimensions, while the tiled w-KNNG approach was successful in general scenarios for higher dimensional points.},
booktitle = {50th International Conference on Parallel Processing Workshop},
articleno = {5},
numpages = {10},
keywords = {GPU, K-Nearest Neighbors Graph, cuda, performance optimization, random projection forest},
location = {Lemont, IL, USA},
series = {ICPP Workshops '21}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {bipartite networks, community detection, graphon clustering, regularization of random graphs, spectral clustering, stochastic block model, sub-Gaussian biclustering}
}

@article{10.1007/s11227-020-03533-2,
author = {Shin, Mincheol and Park, Geunchul and Park, Chan Yeol and Lee, Jongmin and Kim, Mucheol},
title = {Application-specific feature selection and clustering approach with HPC system profiling data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {7},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03533-2},
doi = {10.1007/s11227-020-03533-2},
abstract = {Exascale computing,
 the next-generation computing environment, is expected to be applied to scientific and engineering applications. Accordingly, high-performance computing (HPC) technology is also being developed to improve the performance and high-speed parallelism of many-core processors. Previous researches on improving HPC performance have developed in the form of improving the overall system performance by analyzing the state of the system occurring in the range of the knowledge of expert. However, performance events occurring in a processor in a many-core environment have a large number of indicators, and it is difficult to analyze the correlation between them. In this paper, we propose an application-specific feature selection and clustering approach with HPC system profiling data. The proposed approach performs PCA-based feature selections for efficient performance analysis methods. In addition, the application-specific characteristics from profiling data can be analyzed by unsupervised learning. In our experiments, we evaluated highly parallel supercomputers with NAS parallel benchmark and were able to cluster applications efficiently.},
journal = {J. Supercomput.},
month = jul,
pages = {6817–6831},
numpages = {15},
keywords = {High performance computing, Performance enhancement, Feature selection, System profiling, Many-core systems, Knights Landing processor}
}

@article{10.1016/j.artmed.2021.102162,
author = {Naranjo, Lizbeth and P\'{e}rez, Carlos J. and Campos-Roca, Yolanda and Madruga, Mario},
title = {Replication-based regularization approaches to diagnose Reinke's edema by using voice recordings},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102162},
doi = {10.1016/j.artmed.2021.102162},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {10},
keywords = {Acoustic features, Classification, Reinke's edema, Regularization, Replicated measurements, Variable selection}
}

@inproceedings{10.1145/3286978.3287014,
author = {Bokhari, Mahmoud A. and Alexander, Brad and Wagner, Markus},
title = {In-vivo and offline optimisation of energy use in the presence of small energy signals: A case study on a popular Android library},
year = {2018},
isbn = {9781450360937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286978.3287014},
doi = {10.1145/3286978.3287014},
abstract = {Energy demands of applications on mobile platforms are increasing. As a result, there has been a growing interest in optimising their energy efficiency. As mobile platforms are fast-changing, diverse and complex, the optimisation of energy use is a non-trivial task.To date, most energy optimisation methods either use models or external meters to estimate energy use. Unfortunately, it becomes hard to build widely applicable energy models, and external meters are neither cheap nor easy to set up. To address this issue, we run application variants in-vivo on the phone and use a precise internal battery monitor to measure energy use. We describe a methodology for optimising a target application in-vivo and with application-specific models derived from the device's own internal meter based on jiffies and lines of code. We demonstrate that this process produces a significant improvement in energy efficiency with limited loss of accuracy.},
booktitle = {Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {207–215},
numpages = {9},
keywords = {Android, Non-functional properties, energy consumption, mobile applications, multi-objective optimisation},
location = {New York, NY, USA},
series = {MobiQuitous '18}
}

@book{10.5555/1972514,
author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
title = {Data Mining: Practical Machine Learning Tools and Techniques},
year = {2011},
isbn = {0123748569},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization}
}

@article{10.1145/3434402,
author = {Labini, Paolo Sylos and Cianfriglia, Marco and Perri, Damiano and Gervasi, Osvaldo and Fursin, Grigori and Lokhmotov, Anton and Nugteren, Cedric and Carpentieri, Bruno and Zollo, Fabiana and Vella, Flavio},
title = {On the Anatomy of Predictive Models for Accelerating GPU Convolution Kernels and Beyond},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3434402},
doi = {10.1145/3434402},
abstract = {Efficient HPC libraries often expose multiple tunable parameters, algorithmic implementations, or a combination of them, to provide optimized routines. The optimal parameters and algorithmic choices may depend on input properties such as the shapes of the matrices involved in the operation. Traditionally, these parameters are manually tuned or set by auto-tuners. In emerging applications such as deep learning, this approach is not effective across the wide range of inputs and architectures used in practice. In this work, we analyze different machine learning techniques and predictive models to accelerate the convolution operator and GEMM. Moreover, we address the problem of dataset generation, and we study the performance, accuracy, and generalization ability of the models. Our insights allow us to improve the performance of computationally expensive deep learning primitives on high-end GPUs as well as low-power embedded GPU architectures on three different libraries. Experimental results show significant improvement in the target applications from 50% up to 300% compared to auto-tuned and high-optimized vendor-based heuristics by using simple decision tree- and MLP-based models.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {16},
numpages = {24},
keywords = {GPU computing, neural networks, performance optimization, predictive models, supervised classification, tuning}
}

@inproceedings{10.1145/3368089.3409674,
author = {Zhao, Yutong and Xiao, Lu and Babvey, Pouria and Sun, Lei and Wong, Sunny and Martinez, Angel A. and Wang, Xiao},
title = {Automatically identifying performance issue reports with heuristic linguistic patterns},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409674},
doi = {10.1145/3368089.3409674},
abstract = {Performance issues compromise the response time and resource consumption of a software system. Modern software systems use issue tracking systems to manage all kinds of issue reports, including performance issues. The problem is that performance issues are often not explicitly tagged. The tagging mechanism, if exists, is completely voluntary, depending on the project’s convention and on submitters’ discipline. For example, the performance tag rate in Apache’s Jira system is below 1%. This paper contributes a hybrid classification approach that combines linguistic patterns and machine/deep learning techniques to automatically detect performance issue reports. We manually analyzed 980 real-life performance issue reports and derived 80 project-agnostic linguistic patterns that recur in the reports. Our approach uses these linguistic patterns to construct the sentence-level and issue-level learning features for training effective machine/deep learning classifiers. We test our approach on two separate datasets, each consisting of 980 unclassified issue reports, and compare the results with 31 baseline methods. Our approach can reach up to 83% precision and up to 59% recall. The only comparable baseline method is BERT, which is still 25% lower in the F1-score.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {964–975},
numpages = {12},
keywords = {performance optimization, software performance, software repositories mining},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Cyber-physical systems, Product line engineering, Automated configuration, Multi-stage and multi-step configuration process, Constraint classification, Variability modeling, Real-world case studies}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Dynamic Software Product Line, Feature Model, Modeling},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/2465351.2465371,
author = {Venkataraman, Shivaram and Bodzsar, Erik and Roy, Indrajit and AuYoung, Alvin and Schreiber, Robert S.},
title = {Presto: distributed machine learning and graph processing with sparse matrices},
year = {2013},
isbn = {9781450319942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465351.2465371},
doi = {10.1145/2465351.2465371},
abstract = {It is cumbersome to write machine learning and graph algorithms in data-parallel models such as MapReduce and Dryad. We observe that these algorithms are based on matrix computations and, hence, are inefficient to implement with the restrictive programming and communication interface of such frameworks.In this paper we show that array-based languages such as R [3] are suitable for implementing complex algorithms and can outperform current data parallel solutions. Since R is single-threaded and does not scale to large datasets, we have built Presto, a distributed system that extends R and addresses many of its limitations. Presto efficiently shares sparse structured data, can leverage multi-cores, and dynamically partitions data to mitigate load imbalance. Our results show the promise of this approach: many important machine learning and graph algorithms can be expressed in a single framework and are substantially faster than those in Hadoop and Spark.},
booktitle = {Proceedings of the 8th ACM European Conference on Computer Systems},
pages = {197–210},
numpages = {14},
location = {Prague, Czech Republic},
series = {EuroSys '13}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Graph-based semi-supervised learning, Self-paced learning, Multi-view learning, Semi-supervised feature selection}
}

@article{10.1145/3431388,
author = {Wang, Yu Emma and Wu, Carole-Jean and Wang, Xiaodong and Hazelwood, Kim and Brooks, David},
title = {Exploiting Parallelism Opportunities with Deep Learning Frameworks},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3431388},
doi = {10.1145/3431388},
abstract = {State-of-the-art machine learning frameworks support a wide variety of design features to enable a flexible machine learning programming interface and to ease the programmability burden on machine learning developers. Identifying and using a performance-optimal setting in feature-rich frameworks, however, involves a non-trivial amount of performance profiling efforts and often relies on domain-specific knowledge. This article takes a deep dive into analyzing the performance impact of key design features in a machine learning framework and quantifies the role of parallelism. The observations and insights distill into a simple set of guidelines that one can use to achieve much higher training and inference speedup. Across a diverse set of real-world deep learning models, the evaluation results show that the proposed performance tuning guidelines outperform the Intel and TensorFlow recommended settings by 1.30\texttimes{} and 1.38\texttimes{}, respectively.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {9},
numpages = {23},
keywords = {Machine learning frameworks, parallel computing, performance analysis}
}

@article{10.1007/s11063-020-10286-9,
author = {Li, Li and Zhao, Kaiyi and Li, Sicong and Sun, Ruizhi and Cai, Saihua},
title = {Extreme Learning Machine for Supervised Classification with Self-paced Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10286-9},
doi = {10.1007/s11063-020-10286-9},
abstract = {The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1723–1744},
numpages = {22},
keywords = {Classification, Extreme learning machine, Self-paced learning, Accuracy}
}

@inproceedings{10.1145/3318216.3363370,
author = {Guo, Tian and Walls, Robert J. and Ogden, Samuel S.},
title = {EdgeServe: efficient deep learning model caching at the edge},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363370},
doi = {10.1145/3318216.3363370},
abstract = {In this work, we look at how to effectively manage and utilize deep learning models at each edge location, to provide performance guarantees to inference requests. We identify challenges to use these deep learning models at resource-constrained edge locations, and propose to adapt existing cache algorithms to effectively manage these deep learning models.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {313–315},
numpages = {3},
keywords = {caching algorithm, deep learning inference, edge computing, performance optimization},
location = {Arlington, Virginia},
series = {SEC '19}
}

@article{10.1016/j.ijar.2007.03.006,
author = {Peterson, Leif E. and Coleman, Matthew A.},
title = {Machine learning-based receiver operating characteristic (ROC) curves for crisp and fuzzy classification of DNA microarrays in cancer research},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2007.03.006},
doi = {10.1016/j.ijar.2007.03.006},
abstract = {Receiver operating characteristic (ROC) curves were generated to obtain classification area under the curve (AUC) as a function of feature standardization, fuzzification, and sample size from nine large sets of cancer-related DNA microarrays. Classifiers used included k-nearest neighbor (kNN), naive Bayes classifier (NBC), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), learning vector quantization (LVQ1), logistic regression (LOG), polytomous logistic regression (PLOG), artificial neural networks (ANN), particle swarm optimization (PSO), constricted particle swarm optimization (CPSO), kernel regression (RBF), radial basis function networks (RBFN), gradient descent support vector machines (SVMGD), and least squares support vector machines (SVMLS). For each data set, AUC was determined for a number of combinations of sample size, total sum[-log(p)] of feature t-tests, with and without feature standardization and with (fuzzy) and without (crisp) fuzzification of features. Altogether, a total of 2,123,530 classification runs were made. At the greatest level of sample size, ANN resulted in a fitted AUC of 90%, while PSO resulted in the lowest fitted AUC of 72.1%. AUC values derived from 4NN were the most dependent on sample size, while PSO was the least. ANN depended the most on total statistical significance of features used based on sum[-log(p)], whereas PSO was the least dependent. Standardization of features increased AUC by 8.1% for PSO and -0.2% for QDA, while fuzzification increased AUC by 9.4% for PSO and reduced AUC by 3.8% for QDA. AUC determination in planned microarray experiments without standardization and fuzzification of features will benefit the most if CPSO is used for lower levels of feature significance (i.e., sum[-log(p)]~50) and ANN is used for greater levels of significance (i.e., sum[-log(p)]~500). When only standardization of features is performed, studies are likely to benefit most by using CPSO for low levels of feature statistical significance and LVQ1 for greater levels of significance. Studies involving only fuzzification of features should employ LVQ1 because of the substantial gain in AUC observed and low expense of LVQ1. Lastly, PSO resulted in significantly greater levels of AUC (89.5% average) when feature standardization and fuzzification were performed. In consideration of the data sets used and factors influencing AUC which were investigated, if low-expense computation is desired then LVQ1 is recommended. However, if computational expense is of less concern, then PSO or CPSO is recommended.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {17–36},
numpages = {20},
keywords = {Area under the curve (AUC), DNA microarrays, Fuzzy classification, Gene expression, Receiver operator characteristic (ROC) curve, Soft computing}
}

@article{10.1007/s00500-020-04999-1,
author = {Kim, Bubryur and Yuvaraj, N. and Sri Preethaa, K. R. and Santhosh, R. and Sabari, A.},
title = {RETRACTED ARTICLE: Enhanced pedestrian detection using optimized deep convolution neural network for smart building surveillance},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {22},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-04999-1},
doi = {10.1007/s00500-020-04999-1},
abstract = {Pedestrian detection and tracking is a critical task in the area of smart building surveillance. Due to advancements in sensors, the architects concentrate in construction of smart buildings. Pedestrian detection in smart building is greatly challenged by the image noises by various external environmental parameters. Traditional filter-based techniques for image classification like histogram of oriented gradients filters and machine learning algorithms suffer to perform well for huge volume of pedestrian input images. The advancements in deep learning algorithms perform exponentially good in handling the huge volume of image data. The current study proposes a pedestrian detection model based on deep convolution neural network (CNN) for classification of pedestrians from the input images. Proposed optimized version of VGG-16 architecture is evaluated for pedestrian detection on the INRIA benchmarking dataset consisting of 227 \texttimes{} 227 pixel images. The proposed model achieves an accuracy of 98.5%. It was found that proposed model performs better than the other pretrained CNN architectures and other machine learning models. Pedestrians are reasonably detected and the performance of the proposed algorithm is validated.},
journal = {Soft Comput.},
month = nov,
pages = {17081–17092},
numpages = {12},
keywords = {Pedestrian detection, Deep learning, Convolution neural network, Machine learning}
}

@inproceedings{10.1007/978-3-030-63486-5_23,
author = {Yuan, Fangming and Neubert, Peer and Protzel, Peter},
title = {LocalSPED: A Classification Pipeline that Can Learn Local Features for Place Recognition Using a Small Training Set},
year = {2020},
isbn = {978-3-030-63485-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63486-5_23},
doi = {10.1007/978-3-030-63486-5_23},
abstract = {Visual place recognition is a key component for visual-SLAM. The current state-of-art methods use CNNs (Convolutional Neural Networks) to extract either a holistic descriptor or local features from the images. In recent work, a holistic descriptor method with the name SPED was proposed. In this paper, SPED is extended to a local feature configuration called LocalSPED by applying several modifications and by introducing a novel feature pooling method. Several variations of SPED and LocalSPED are trained on a smaller dataset and their performances are evaluated on several benchmark datasets. In the experiments, LocalSPED handles the decreased training set size significantly better than the original SPED approach and provides better place recognition results.},
booktitle = {Towards Autonomous Robotic Systems: 21st Annual Conference, TAROS 2020, Nottingham, UK, September 16, 2020, Proceedings},
pages = {209–213},
numpages = {5},
keywords = {Place recognition, Local features, Robotics},
location = {Nottingham, United Kingdom}
}

@inproceedings{10.1145/3297280.3297430,
author = {Aiolli, Fabio and Conti, Mauro and Gangwal, Ankit and Polato, Mirko},
title = {Mind your wallet's privacy: identifying Bitcoin wallet apps and user's actions through network traffic analysis},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297430},
doi = {10.1145/3297280.3297430},
abstract = {With the surge in popularity of cryptocurrencies, Bitcoin has emerged as one of the most promising means for remittance, payments, and trading. Supplemented by the convenience offered by the smartphones, an increasing number of users are adopting Bitcoin wallet apps for different purposes.In this paper, we focus on identifying user activities on smart-phone-based Bitcoin wallet apps that are commonly used for sending, receiving, and trading Bitcoin. To accomplish our goal, we performed network traffic analysis using machine learning techniques. Since we focus on apps of the same type/functionality, it makes our classification problem even more difficult compared to classifying apps tailored for discrete purposes. Moreover, our goal is to identify user activities even in the presence of encryption. In our experiments, we considered the worldwide most downloaded Bitcoin wallet apps on both Google Play Store and Apple's App Store. For collecting network traffic traces, we used only physical hardware and omitted any emulator to build our experiment scenario as close to the real environment as possible. We process the traffic traces in several phases before extracting the features that are utilized to train our supervised learning algorithms. We deal with the classification problem in multiple stages in a hierarchical fashion. We ran a thorough set of experiments to assess the performance of our system and attained nearly 95% accuracy in user activity identification.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1484–1491},
numpages = {8},
keywords = {Android, Bitcoin, iOS, machine learning, traffic analysis},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3472634.3472652,
author = {Liu, Zhuang and Lu, Ye and Xie, Xueshuo and Fang, Yaozheng and Jian, Zhaolong and Li, Tao},
title = {Trusted-DNN: A TrustZone-based Adaptive Isolation Strategy for Deep Neural Networks},
year = {2021},
isbn = {9781450385671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472634.3472652},
doi = {10.1145/3472634.3472652},
abstract = {Deep neural network (DNN) models have been widely deployed on embedded and mobile devices in lots of application fields such as health care, face recognition, driver assistance, etc. These applications usually require privacy or trusted computing protection. However, diverse hardware resources, various transport protocols, and limited computation and storage capacity make it challenging for traditional embedded systems to provide complex security protection mechanism oriented DNN models. To meet the challenges, we propose Trusted-DNN, a TrustZone-based adaptive isolation strategy for DNN models. We first design a normal pattern to exploit TrustZone technology to provide overall protection for running DNNs. To deploy arbitrary DNN models into TrustZone, we then develop a dynamic model partition method, which makes our strategy easily adaptive to various DNN models and devices. Finally, we employ several optimization techniques to reduce the inference latency of Trusted-DNN models. We perform AlexNet on OP-TEE, which is a TrustZone-based secure operating system, based on a Raspberry Pi 3B+ board. The extensive experimental results highlight that the optimized Trusted-DNN can reduce memory footprint by up to 98% compared with the ordinary program and Trusted-DNN only increase inference latency by 22.8%. Our code is available at https://gitee.com/PaintZero/alexnet-tee.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China},
pages = {67–71},
numpages = {5},
keywords = {ARM TrustZone, Deep neural network, Embedded devices, Hardware security, Performance optimization},
location = {Hefei, China},
series = {ACM TURC '21}
}

@article{10.1016/j.patcog.2016.12.023,
author = {Wang, Jim Jing-Yan and Tsang, Ivor Wai-Hung and Cui, Xuefeng and Lu, Zhiwu and Gao, Xin},
title = {Multi-instance dictionary learning via multivariate performance measure optimization},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.12.023},
doi = {10.1016/j.patcog.2016.12.023},
abstract = {The multi-instance dictionary plays a critical role in multi-instance data representation. Meanwhile, different multi-instance learning applications are evaluated by specific multivariate performance measures. For example, multi-instance ranking reports the precision and recall. It is not difficult to see that to obtain different optimal performance measures, different dictionaries are needed. This observation motives us to learn performance-optimal dictionaries for this problem. In this paper, we propose a novel joint framework for learning the multi-instance dictionary and the classifier to optimize a given multivariate performance measure, such as the F1 score and precision at rank k. We propose to represent the bags as bag-level features via the bag-instance similarity, and learn a classifier in the bag-level feature space to optimize the given performance measure. We propose to minimize the upper bound of a multivariate loss corresponding to the performance measure, the complexity of the classifier, and the complexity of the dictionary, simultaneously, with regard to both the dictionary and the classifier parameters. In this way, the dictionary learning is regularized by the performance optimization, and a performance-optimal dictionary is obtained. We develop an iterative algorithm to solve this minimization problem efficiently using a cutting-plane algorithm and a coordinate descent method. Experiments on multi-instance benchmark data sets show its advantage over both traditional multi-instance learning and performance optimization methods. Different multivariate performance require different optimal multi-instance dictionaries.We propose the problem of learning performance-optimal multi-instance dictionary.We learn dictionary and classifier jointly to optimize a target performance measure.It outperforms both independent dictionary learning and performance optimization methods.},
journal = {Pattern Recogn.},
month = jun,
pages = {448–459},
numpages = {12},
keywords = {Cutting-plane algorithm, Dictionary, Multi-instance learning, Multivariate performance measures}
}

@article{10.1016/j.patcog.2018.12.022,
author = {Gribel, Daniel and Vidal, Thibaut},
title = {         HG-means: A scalable hybrid genetic algorithm for minimum sum-of-squares clustering},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.12.022},
doi = {10.1016/j.patcog.2018.12.022},
journal = {Pattern Recogn.},
month = apr,
pages = {569–583},
numpages = {15},
keywords = {Clustering, Minimum sum-of-squares, Global optimization, Hybrid genetic algorithm, K-means, Unsupervised learning}
}

@inproceedings{10.1109/SPLC.2008.28,
author = {Chae, Wonseok and Blume, Matthias},
title = {Building a Family of Compilers},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.28},
doi = {10.1109/SPLC.2008.28},
abstract = {We have developed and maintained a set of closely related compilers. Although much of their code is duplicated and shared, they have been maintained separately because they are treated as different compilers. Even if they were merged together, the combined code would become too complicated to serve as the base for another extension. We describe our experience to address this problem by adopting the product line engineering paradigm to build a family of compilers. This paradigm encourages developers to focus on developing a set of compilers rather than on developing one particular compiler. We show engineering activities for a family of compilers from product line analysis through product line architecture design to product line component design. Then, we present how to build particular compilers from core assets resulting from the previous activities and how to take advantage of modern programming language technology to organize this task. Our experience demonstrates that the product line engineering as a developing paradigm can ease the construction of a family of compilers.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {307–316},
numpages = {10},
keywords = {compilers, feature-oriented, module system, product line engineering, standard ml},
series = {SPLC '08}
}

@inproceedings{10.1145/3358960.3379137,
author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379137},
doi = {10.1145/3358960.3379137},
abstract = {Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single "dominant" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {277–288},
numpages = {12},
keywords = {configurable systems, machine learning, performance prediction, software product lines},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.5555/2033408.2033453,
author = {Rokos, Georgios and Gorman, Gerard and Kelly, Paul H. J.},
title = {Accelerating anisotropic mesh adaptivity on nVIDIA's CUDA using texture interpolation},
year = {2011},
isbn = {9783642233968},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Anisotropic mesh smoothing is used to generate optimised meshes for Computational Fluid Dynamics (CFD). Adapting the size and shape of elements in an unstructured mesh to a specification encoded in a metric tensor field is done by relocating mesh vertices. This computationally intensive task can be accelerated by engaging nVIDIA's CUDA-enabled GPUs. This article describes the algorithmic background, the design choices and the implementation details that led to a meshsmoothing application running in double-precision on a Tesla C2050 board. Engaging CUDA's texturing hardware to manipulate the metric tensor field accelerates execution by up to 6.2 times, leading to a total speedup of up to 148 times over the serial CPU code and up to 15 times over the 12-threaded OpenMP code.},
booktitle = {Proceedings of the 17th International Conference on Parallel Processing - Volume Part II},
pages = {387–398},
numpages = {12},
keywords = {CUDA, anisotropic mesh adaptivity, metric tensor field, parallel execution, texturing hardware, vertex smoothing},
location = {Bordeaux, France},
series = {Euro-Par'11}
}

@article{10.1007/s11227-020-03382-z,
author = {Corral-Garc\'{\i}a, Javier and Lemus-Prieto, Felipe and P\'{e}rez-Toledano, Miguel-\'{A}ngel},
title = {Efficient code development for improving execution performance in high-performance computing centers},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {4},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03382-z},
doi = {10.1007/s11227-020-03382-z},
abstract = {Thanks to high-performance computing (HPC), it is possible to solve all kinds of highly complex projects from multiple scientific disciplines that require computationally intensive tasks to be undertaken and which otherwise could not be addressed. Unfortunately, since the development of parallel codes requires highly specific knowledge, it can become a challenge for beginners and non-expert programmers, especially when it comes to making adequate and efficient use of the available computing resources. To this end, we developed a transcompiler for helping researchers and inexperienced users who do not have the necessary skills in the use of parallel programming, and aimed at improving the performance of their HPC routines and tasks. Current efforts are focused on an additional module for optimizing code fragments in order to reduce their running times. In order to achieve this, twenty-six software techniques were selected from the literature to be integrated into this new module, all of them aimed at improving execution times of HPC programs by directly writing efficient code. Their effectiveness is analyzed and discussed in the current manuscript through a complete set of tests designed and conducted to measure and evaluate benefits achieved when applying these techniques.},
journal = {J. Supercomput.},
month = apr,
pages = {3261–3288},
numpages = {28},
keywords = {High-performance computing, Efficient code, Code optimization, Performance optimization}
}

@inproceedings{10.3115/1218955.1219031,
author = {Niu, Cheng and Li, Wei and Srihari, Rohini K.},
title = {Weakly supervised learning for cross-document person name disambiguation supported by information extraction},
year = {2004},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1218955.1219031},
doi = {10.3115/1218955.1219031},
abstract = {It is fairly common that different people are associated with the same name. In tracking person entities in a large document pool, it is important to determine whether multiple mentions of the same name across documents refer to the same entity or not. Previous approach to this problem involves measuring context similarity only based on co-occurring words. This paper presents a new algorithm using information extraction support in addition to co-occurring words. A learning scheme with minimal supervision is developed within the Bayesian framework. Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features. Statistical annealing is applied to derive the final entity coreference chains by globally fitting the pairwise context similarities. Benchmarking shows that our new approach significantly outperforms the existing algorithm by 25 percentage points in overall F-measure.},
booktitle = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
pages = {597–es},
location = {Barcelona, Spain},
series = {ACL '04}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@inproceedings{10.1007/978-3-030-76352-7_38,
author = {Taktak, Hela and Boukadi, Khouloud and Gu\'{e}gan, Chirine Ghedira and Mrissa, Michael and Gargouri, Fa\"{\i}ez},
title = {Towards Knowledge-Driven Automatic Service Composition for Wildfire Prediction},
year = {2020},
isbn = {978-3-030-76351-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-76352-7_38},
doi = {10.1007/978-3-030-76352-7_38},
abstract = {Wildfire prediction from Earth Observation (EO) data has gained much attention in the past years, through the development of connected sensors and weather satellites. Nowadays, it is possible to extract knowledge from collected EO data and to learn from this knowledge without human intervention to trigger wildfire alerts. However, exploiting knowledge extracted from multiple EO data sources at run-time and predicting wildfire raise multiple challenges. One major challenge is to provide dynamic construction of service composition plans, according to the data obtained from sensors. In this paper, we present a knowledge-driven Machine Learning approach that relies on historical data related to wildfire observations to guide the collection of EO data and to automatically and dynamically compose services for triggering wildfire alerts.},
booktitle = {Service-Oriented Computing  – ICSOC 2020 Workshops: AIOps, CFTIC, STRAPS, AI-PA, AI-IOTS, and Satellite Events, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {408–420},
numpages = {13},
keywords = {Machine Learning, Fire prediction, Service composition},
location = {Dubai, United Arab Emirates}
}

@inproceedings{10.1007/978-3-030-71058-3_2,
author = {Chen, Bangduo and Li, Mingzhen and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
title = {swRodinia: A Benchmark Suite for&nbsp;Exploiting Architecture Properties of&nbsp;Sunway Processor},
year = {2020},
isbn = {978-3-030-71057-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71058-3_2},
doi = {10.1007/978-3-030-71058-3_2},
abstract = {The Sunway processor has been demonstrated with superior performance by various scientific applications, domain specific frameworks and numerical algorithms. However, the optimization techniques that can fully exploit the architecture features are usually buried deep in large code bases, which prevents average programmers to understand such optimization techniques. Thus, the existing complex software fails to provide guidance for more programs embracing the computation power of Sunway processor. In this paper, we build a benchmark suite swRodinia by porting and optimizing the well-known Rodinia benchmark on Sunway processor. Specifically, we demonstrate several optimization techniques by tailoring the benchmarks to better leverage the architecture features for higher performance. Moreover, based on the optimization experiences, we derive several useful insights from both software and hardware perspectives, that not only guide the better utilization of current Sunway processor, but also reveal the direction of hardware improvements for future Sunway processor. We open source the swRodinia benchmark suite and encourage the community to enhance the benchmark with us continuously.},
booktitle = {Benchmarking, Measuring, and Optimizing: Third BenchCouncil International Symposium, Bench 2020, Virtual Event, November 15–16, 2020, Revised Selected Papers},
pages = {22–38},
numpages = {17},
keywords = {Sunway processor, Benchmark suite, Heterogeneous manycore, Parallelization, Performance optimization}
}

@article{10.1016/j.vlsi.2021.08.006,
author = {undefinedslamo\u{g}lu, Gamze and \c{C}ak\i{}c\i{}, Tu\u{g}berk O\u{g}ulcan and G\"{u}zelhan, \c{S}eyda Nur and Afacan, Engin and D\"{u}ndar, G\"{u}nhan},
title = {Deep learning aided efficient yield analysis for multi-objective analog integrated circuit synthesis},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81},
number = {C},
issn = {0167-9260},
url = {https://doi.org/10.1016/j.vlsi.2021.08.006},
doi = {10.1016/j.vlsi.2021.08.006},
journal = {Integr. VLSI J.},
month = nov,
pages = {322–330},
numpages = {9},
keywords = {Deep neural network, Performance optimization, Process variations, IC modeling, Multi-objective optimization}
}

@article{10.1145/3476066,
author = {Kim, Seunghyun and Razi, Afsaneh and Stringhini, Gianluca and Wisniewski, Pamela J. and De Choudhury, Munmun},
title = {A Human-Centered Systematic Literature Review of Cyberbullying Detection Algorithms},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476066},
doi = {10.1145/3476066},
abstract = {Cyberbullying is a growing problem across social media platforms, inflicting short and long-lasting effects on victims. To mitigate this problem, research has looked into building automated systems, powered by machine learning, to detect cyberbullying incidents, or the involved actors like victims and perpetrators. In the past, systematic reviews have examined the approaches within this growing body of work, but with a focus on the computational aspects of the technical innovation, feature engineering, or performance optimization, without centering around the roles, beliefs, desires, or expectations of humans. In this paper, we present a human-centered systematic literature review of the past 10 years of research on automated cyberbullying detection. We analyzed 56 papers based on a three-prong human-centeredness algorithm design framework - spanning theoretical, participatory, and speculative design. We found that the past literature fell short of incorporating human-centeredness across multiple aspects, ranging from defining cyberbullying, establishing the ground truth in data annotation, evaluating the performance of the detection models, to speculating the usage and users of the models, including potential harms and negative consequences. Given the sensitivities of the cyberbullying experience and the deep ramifications cyberbullying incidents bear on the involved actors, we discuss takeaways on how incorporating human-centeredness in future research can aid with developing detection systems that are more practical, useful, and tuned to the diverse needs and contexts of the stakeholders.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {325},
numpages = {34},
keywords = {cyberbullying detection, human-centered machine learning, literature review, social media}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.5555/2946645.3053434,
author = {Szab\'{o}, Zolt\'{a}n and Sriperumbudur, Bharath K. and P\'{o}czos, Barnab\'{a}s and Gretton, Arthur},
title = {Learning theory for distribution regression},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; G\"{a}rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5272–5311},
numpages = {40},
keywords = {Kernel ridge regression, mean embedding, minimax optimality, multi-instance learning, two-Stage sampled distribution regression}
}

@article{10.5555/3455716.3455897,
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
title = {Curriculum learning for reinforcement learning domains: a framework and survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {181},
numpages = {50},
keywords = {curriculum learning, reinforcement learning, transfer learning}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, Andr\'{e} and Batista, Thais and Cacho, N\'{e}lio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {cloud computing, cloud platforms, health watcher system, services, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/3408352.3408727,
author = {Agrawal, Kunal and Baruah, Sanjoy and Burns, Alan and Singh, Abhishek},
title = {Minimizing execution duration in the presence of learning-enabled components},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Autonomous systems are increasingly using components that incorporate machine learning and other AI-based techniques in order to achieve improved performance. We address the problem of assuring correctness in safety-critical systems that use such components. We investigate an approach which formulates the problem as one in which performance is an objective function to be optimized while safety is a hard constraint that must be satisfied. We then apply heuristics and algorithmic techniques from optimization theory in order to solve the resulting constrained optimization problem.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1644–1649},
numpages = {6},
keywords = {learning-enabled components (LECs), performance optimization, run-time monitoring, safety-critical systems, typical analysis},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.1145/3297280.3299743,
author = {Paucar, Luis H. Garcia and Bencomo, Nelly and Yuen, Kevin Kam Fung},
title = {ARRoW: &lt;u&gt;a&lt;/u&gt;utomatic &lt;u&gt;r&lt;/u&gt;untime &lt;u&gt;r&lt;/u&gt;eappraisal &lt;u&gt;o&lt;/u&gt;f &lt;u&gt;w&lt;/u&gt;eights for self-adaptation},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3299743},
doi = {10.1145/3297280.3299743},
abstract = {[&lt;u&gt;Context/Motivation&lt;/u&gt;] Decision-making for self-adaptive systems (SAS) requires the runtime trade-off of multiple non-functional requirements (NFRs) and the costs-benefits analysis of the alternative solutions. Usually, it is required the specification of the weights (a.k.a. preferences) associated with the NFRs and decision-making strategies. These preferences are traditionally defined at design-time. [&lt;u&gt;Questions/Problems&lt;/u&gt;] A big challenge is the need to deal with unsuitable preferences, based on empirical evidence available at runtime, and which may not agree anymore with previous assumptions. Therefore, new techniques are needed to systematically reassess the current preferences according to empirical evidence collected at runtime. [&lt;u&gt;Principal ideas/ results&lt;/u&gt;] We present ARRoW (&lt;u&gt;A&lt;/u&gt;utomatic &lt;u&gt;R&lt;/u&gt;untime &lt;u&gt;R&lt;/u&gt;eappraisal &lt;u&gt;o&lt;/u&gt;f &lt;u&gt;W&lt;/u&gt;eights) to support the dynamic update of preferences/weights associated with the NFRs and decision-making strategies in SAS, while taking into account the current levels of satisficement that NFRs can reach during the system's operation. [&lt;u&gt;Contribution&lt;/u&gt;] To developed ARRoW, we have extended the Primitive Cognitive Network Process (P-CNP), a version of the Analytical Hierarchy Process (AHP), to enable the handling and update of weights during runtime. Specifically, in this paper, we show a formalization for the specification of the decision-making of a SAS in terms of NFRs, the design decisions and their corresponding weights as a P-CNP problem. We also report on how the P-CNP has been extended to be used at runtime. We show how the propagation of elements of P-CNP matrices is performed in such a way that the weights are updated to therefore, improve the levels of satisficement of the NFRs to better match the current environment during runtime. ARRoW leverages the Bayesian learning process underneath, which on the other hand, provides the mechanism to get access to evidence about the levels of satisficement of the NFRs. The experiments have been applied to a case study of the networking application domain where the decision-making has been improved.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1584–1591},
numpages = {8},
keywords = {AHP, bayesian evidence, decision-making, non-functional properties, runtime models, self-adaptation, uncertainty},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3468264.3468555,
author = {Chen, Tao and Li, Miqing},
title = {Multi-objectivizing software configuration tuning},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468555},
doi = {10.1145/3468264.3468555},
abstract = {Automatically tuning software configuration for optimizing a single performance attribute (e.g., minimizing latency) is not trivial, due to the nature of the configuration systems (e.g., complex landscape and expensive measurement). To deal with the problem, existing work has been focusing on developing various effective optimizers. However, a prominent issue that all these optimizers need to take care of is how to avoid the search being trapped in local optima — a hard nut to crack for software configuration tuning due to its rugged and sparse landscape, and neighboring configurations tending to behave very differently. Overcoming such in an expensive measurement setting is even more challenging. In this paper, we take a different perspective to tackle this issue. Instead of focusing on improving the optimizer, we work on the level of optimization model. We do this by proposing a meta multi-objectivization model (MMO) that considers an auxiliary performance objective (e.g., throughput in addition to latency). What makes this model unique is that we do not optimize the auxiliary performance objective, but rather use it to make similarly-performing while different configurations less comparable (i.e. Pareto nondominated to each other), thus preventing the search from being trapped in local optima.  Experiments on eight real-world software systems/environments with diverse performance attributes reveal that our MMO model is statistically more effective than state-of-the-art single-objective counterparts in overcoming local optima (up to 42% gain), while using as low as 24% of their measurements to achieve the same (or better) performance result.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {453–465},
numpages = {13},
keywords = {Configuration tuning, multi-objectivization, performance optimization, search-based software engineering},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3332305.3332322,
author = {Park, Donggeun},
title = {Research of Cyclone Optimization Based on CFD, GMDH-Type Neural Network and Genetic Algorithm},
year = {2019},
isbn = {9781450365925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332305.3332322},
doi = {10.1145/3332305.3332322},
abstract = {Gas cyclone has two main parameters for evaluating separation performance, separation efficiency and pressure drop through cyclone. They are closely influenced by the geometrical design variables of the cyclone. This study performed optimization of the cyclone performance for the cyclone shape based on computational fluid dynamics (CFD), GMDH type neural network and genetic algorithm (GA). First, CFD was used to obtain the data of the cyclone performance parameters. As result of the CFD validation, the errors of the reference model and CFD were 0.5 % and 2 % for the pressure drop and the separation efficiency. Secondly, the meta-model of the cyclone performance was derived by using GMDH algorithm based on supervised learning of machine learning. The fitness of the modelling results was shown using the correlation coefficient. As results of the GMDH, the correlation coefficients of meta models of the separation efficiency and the pressure drop were 98.9 %, 99.7 %, respectively. Finally, we performed optimization of the meta model by applying GA. When the optimal point was compared with the reference model, the performance of the optimal point was improved by 24.31 % and 8.32 % for pressure drop and the separation efficiency, respectively.},
booktitle = {Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations},
pages = {90–96},
numpages = {7},
keywords = {Artificial neural network, Computational fluid dynamics, Cyclone separator, Optimization},
location = {Perth, WN, Australia},
series = {ICVARS '19}
}

@inproceedings{10.5555/3266365.3266406,
author = {Zubok, Dmitrii and Maiatin, Aleksandr and Khegai, Maksim and Kharchenko, Tatiana},
title = {Using Alternating Decision Trees in Multi-Leveled Hierarchical Cloud Based System},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Cloud platforms are an essential part of modern world. Used in all kind of ?elds, from education to science, they became inseparable from information technologies and computing sphere. However the problem of performance optimization still exists and is not entirely solved even today. With introduction of machine learning and arti?cial intelligence this task became easier to solve. This paper presents a way to utilize decision trees to control performance of a computational system and to balance load on different nodes in attempt to increase quality of service.},
booktitle = {Proceedings of the 22st Conference of Open Innovations Association FRUCT},
articleno = {41},
numpages = {6},
keywords = {Architecture, Decision trees, Machine learning, Optimization},
location = {Jyvaskyla, Finland},
series = {FRUCT'22}
}

@inproceedings{10.1145/3314221.3314644,
author = {Khan, Tanvir Ahmed and Zhao, Yifan and Pokam, Gilles and Mozafari, Barzan and Kasikci, Baris},
title = {Huron: hybrid false sharing detection and repair},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314644},
doi = {10.1145/3314221.3314644},
abstract = {Writing efficient multithreaded code that can leverage the full parallelism of underlying hardware is difficult. A key impediment is insidious cache contention issues, such as false sharing. False sharing occurs when multiple threads from different cores access disjoint portions of the same cache line, causing it to go back and forth between the caches of different cores and leading to substantial slowdown.  Alas, existing techniques for detecting and repairing false sharing have limitations. On the one hand, in-house (i.e., offline) techniques are limited to situations where falsely-shared data can be determined statically, and are otherwise inaccurate. On the other hand, in-production (i.e., run-time) techniques incur considerable overhead, as they constantly monitor a program to detect false sharing. In-production repair techniques are also limited by the types of modifications they can perform on the fly, and are therefore less effective.  We present Huron, a hybrid in-house/in-production false sharing detection and repair system. Huron detects and repairs as much false sharing as it can in-house, and relies on its lightweight in-production mechanism for remaining cases. The key idea behind Huron's in-house false sharing repair is to group together data that is accessed by the same set of threads, to shift falsely-shared data to different cache lines. Huron's in-house repair technique can generalize to previously-unobserved inputs. Our evaluation shows that Huron can detect more false sharing bugs than all state-of-the-art techniques, and with a lower overhead. Huron improves runtime performance by 3.82\texttimes{} on average (up to 11\texttimes{}), which is 2.11-2.27\texttimes{} better than the state of the art.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {453–468},
numpages = {16},
keywords = {False sharing, Performance optimization},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3458817.3476217,
author = {Yang, Weiling and Fang, Jianbin and Dong, Dezun and Su, Xing and Wang, Zheng},
title = {LIBSHALOM: optimizing small and irregular-shaped matrix multiplications on ARMv8 multi-cores},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476217},
doi = {10.1145/3458817.3476217},
abstract = {General Matrix Multiplication (GEMM) is a key subroutine in highperformance computing. While the mainstream linear algebra libraries can deliver high performance on large and regular-shaped GEMM, they are inadequate for optimizing small and irregular-shaped GEMMs, which are commonly seen in new HPC applications. Some of the recent works in this direction have made promising progress on x86 architectures and GPUs but still leave much room for improvement on emerging HPC hardware built upon the ARMv8 architecture. We present LibShalom, an open-source library for optimizing small and irregular-shaped GEMMs, explicitly targeting the ARMv8 architecture. LibShalom builds upon the classical Goto algorithm but tailors it to minimize the expensive memory accessing overhead for data packing and processing small matrices. It uses analytic methods to determine GEMM kernel optimization parameters, enhancing the computation and parallelization efficiency of the GEMM kernels. We evaluate LibShalom by applying it to three ARMv8 multi-core architectures and comparing it against five mainstream linear algebra libraries. Experimental results show that LibShalom can consistently outperform existing solutions across GEMM workloads and hardware architectures.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {72},
numpages = {14},
keywords = {ARMv8 multi-core, matrix multiplication, performance optimization, small and irregular-shaped},
location = {St. Louis, Missouri},
series = {SC '21}
}

@article{10.1007/s10766-020-00662-2,
author = {Wang, Bo and Tang, Jie and Zhang, Rui and Liu, Jialei and Liu, Shaoshan and Qi, Deyu},
title = {A Task-Aware Fine-Grained Storage Selection Mechanism for In-Memory Big Data Computing Frameworks},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {1},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-020-00662-2},
doi = {10.1007/s10766-020-00662-2},
abstract = {In-memory big data computing, widely used in hot areas such as deep learning and artificial intelligence, can meet the demands of ultra-low latency service and real-time data analysis. However, existing in-memory computing frameworks usually use memory in an aggressive way. Memory space is quickly exhausted and leads to great performance degradation or even task failure. On the other hand, the increasing volumes of raw data and intermediate data introduce huge memory demands, which further deteriorate the short of memory. To release the pressure on memory, those in-memory frameworks provide various storage schemes options for caching data, which determines where and how data is cached. But their storage scheme selection mechanisms are simple and insufficient, always manually set by users. Besides, those coarse-grained data storage mechanisms cannot satisfy memory access patterns of each computing unit which works on only part of the data. In this paper, we proposed a novel task-aware fine-grained storage scheme auto-selection mechanism. It automatically determines the storage scheme for caching each data block, which is the smallest unit during computing. The caching decision is made by considering the future tasks, real-time resource utilization, and storage costs, including block creation costs, I/O costs, and serialization costs under each storage scenario. The experiments show that our proposed mechanism, compared with the default storage setting, can offer great performance improvement, especially in memory-constrained circumstances it can be as much as 78%.},
journal = {Int. J. Parallel Program.},
month = feb,
pages = {25–50},
numpages = {26},
keywords = {Big data, In-memory computing, Storage scheme, Performance optimization}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Feature model, Software product line, Defect, Product line model, Quality}
}

@inproceedings{10.1109/SPLC.2011.47,
author = {Chen, Sheng and Erwig, Martin},
title = {Optimizing the Product Derivation Process},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.47},
doi = {10.1109/SPLC.2011.47},
abstract = {Feature modeling is widely used in software product-line engineering to capture the commonalities and variabilities within an application domain. As feature models evolve, they can become very complex with respect to the number of features and the dependencies among them, which can cause the product derivation based on feature selection to become quite time consuming and error prone. We address this problem by presenting techniques to find good feature selection sequences that are based on the number of products that contain a particular feature and the impact of a selected feature on the selection of other features. Specifically, we identify a feature selection strategy, which brings up highly selective features early for selection. By prioritizing feature selection based on the selectivity of features our technique makes the feature selection process more efficient. Moreover, our approach helps with the problem of unexpected side effects of feature selection in later stages of the selection process, which is commonly considered a difficult problem. We have run our algorithm on the e-Shop and Berkeley DB feature models and also on some automatically generated feature models. The evaluation results demonstrate that our techniques can shorten the product derivation processes significantly.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {35–44},
numpages = {10},
keywords = {Decision Sequence, Feature Model, Feature Selection},
series = {SPLC '11}
}

@article{10.1016/j.compbiomed.2020.103991,
author = {Sreejith, S. and Khanna Nehemiah, H. and Kannan, A.},
title = {Clinical data classification using an enhanced SMOTE and chaotic evolutionary feature selection},
year = {2020},
issue_date = {Nov 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {126},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.103991},
doi = {10.1016/j.compbiomed.2020.103991},
journal = {Comput. Biol. Med.},
month = nov,
numpages = {14},
keywords = {Clinical decision support system, Class imbalance, Feature selection, Chaotic maps, Classification, SMOTE, Multi Verse Optimisation}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Apriori Algorithm, DNA, K-NN, Naive Bayes, Rough Set}
}

@inproceedings{10.1145/3472883.3487015,
author = {Desu, Anuroop and Puvvadi, Udaya and Stachecki, Tyler and Vishwakarma, Sagar and Khalili, Sadegh and Ghose, Kanad and Sammakia, Bahgat G.},
title = {Latency-Aware Dynamic Server and Cooling Capacity Provisioner for Data Centers},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3487015},
doi = {10.1145/3472883.3487015},
abstract = {Data center operators generally overprovision IT and cooling capacities to address unexpected utilization increases that can violate service quality commitments. This results in energy wastage. To reduce this wastage, we introduce HCP (Holistic Capacity Provisioner), a service latency aware management system for dynamically provisioning the server and cooling capacity. Short-term load prediction is used to adjust the online server capacity to concentrate the workload onto the smallest possible set of online servers. Idling servers are completely turned off based on a separate long-term utilization predictor. HCP targets data centers that use chilled air cooling and varies the cooling provided commensurately, using adjustable aperture tiles and speed control of the blower fans in the air handler. An HCP prototype supporting a server heterogeneity is evaluated with real-world workload traces/requests and realizes up to 32% total energy savings while limiting the 99th-percentile and average latency increases to at most 6.67% and 3.24%, respectively, against a baseline system where all servers are kept online.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {335–349},
numpages = {15},
keywords = {Energy-performance optimization, data centers, resource management},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

@article{10.1016/j.knosys.2019.105185,
author = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Xie, Shengli and Su, Chun-Yi},
title = {Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105185},
doi = {10.1016/j.knosys.2019.105185},
journal = {Know.-Based Syst.},
month = feb,
numpages = {10},
keywords = {Graph-regularization, Semi-supervised learning, Multi-view clustering, Non-negative matrix factorization}
}

@article{10.5555/2946645.2946709,
author = {Adi, Yossi and Keshet, Joseph},
title = {StructED: risk minimization in structured prediction},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents STRUCTED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from http://adiyoss.github.io/StructED/.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2282–2286},
numpages = {5},
keywords = {CRF, direct loss minimization, structural SVM, structured prediction}
}

@article{10.1016/j.asoc.2021.107164,
author = {Saeed, Farah and Khan, Muhammad Attique and Sharif, Muhammad and Mittal, Mamta and Goyal, Lalit Mohan and Roy, Sudipta},
title = {Deep neural network features fusion and selection based on PLS regression with an application for crops diseases classification},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {103},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107164},
doi = {10.1016/j.asoc.2021.107164},
journal = {Appl. Soft Comput.},
month = may,
numpages = {15},
keywords = {Crops diseases, CNN, Feature extraction, Feature fusion, PLS based selection}
}

@inproceedings{10.5555/3524938.3525259,
author = {Geng, Sinong and Nassif, Houssam and Manzanares, Carlos A. and Reppen, A. Max and Sircar, Ronnie},
title = {Deep PQR: solving inverse reinforcement learning using anchor actions},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the Q- function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {321},
numpages = {11},
series = {ICML'20}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {curriculum learning, linear regression, hinge loss minimization}
}

@inproceedings{10.1145/3437801.3441592,
author = {G\'{o}mez, Constantino and Mantovani, Filippo and Focht, Erich and Casas, Marc},
title = {Efficiently running SpMV on long vector architectures},
year = {2021},
isbn = {9781450382946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437801.3441592},
doi = {10.1145/3437801.3441592},
abstract = {Sparse Matrix-Vector multiplication (SpMV) is an essential kernel for parallel numerical applications. SpMV displays sparse and irregular data accesses, which complicate its vectorization. Such difficulties make SpMV to frequently experiment non-optimal results when run on long vector ISAs exploiting SIMD parallelism. In this context, the development of new optimizations becomes fundamental to enable high performance SpMV executions on emerging long vector architectures. In this paper, we improve the state-of-the-art SELL-C-σ sparse matrix format by proposing several new optimizations for SpMV. We target aggressive long vector architectures like the NEC Vector Engine. By combining several optimizations, we obtain an average 12% improvement over SELL-C-σ considering a heterogeneous set of 24 matrices. Our optimizations boost performance in long vector architectures since they expose a high degree of SIMD parallelism.},
booktitle = {Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {292–303},
numpages = {12},
keywords = {NEC vector engine, SpMV, long-vector architectures, performance optimization},
location = {Virtual Event, Republic of Korea},
series = {PPoPP '21}
}

@inproceedings{10.1007/978-3-030-55789-8_59,
author = {Abeyrathna, Kuruge Darshana and Granmo, Ole-Christoffer and Goodwin, Morten},
title = {Integer Weighted Regression Tsetlin Machines},
year = {2020},
isbn = {978-3-030-55788-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55789-8_59},
doi = {10.1007/978-3-030-55789-8_59},
abstract = {The Regression Tsetlin Machine (RTM) addresses the lack of interpretability impeding state-of-the-art nonlinear regression models. It does this by using conjunctive clauses in propositional logic to capture the underlying non-linear frequent patterns in the data. These, in turn, are combined into a continuous output through summation, akin to a linear regression function, however, with non-linear components and binary weights. However, the resolution of the RTM output is proportional to the number of clauses employed. This means that computation cost increases with resolution. To address this problem, we here introduce integer weighted RTM clauses. Our integer weighted clause is a compact representation of multiple clauses that capture the same sub-pattern—w repeating clauses are turned into one, with an integer weight w. This reduces computation cost w times, and increases interpretability through a sparser representation. We introduce a novel learning scheme, based on so-called stochastic searching on the line. We evaluate the potential of the integer weighted RTM empirically using two artificial datasets. The results show that the integer weighted RTM is able to acquire on par or better accuracy using significantly less computational resources compared to regular RTM and an RTM with real-valued weights.},
booktitle = {Trends in Artificial Intelligence Theory and Applications. Artificial Intelligence Practices: 33rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2020, Kitakyushu, Japan, September 22-25, 2020, Proceedings},
pages = {686–694},
numpages = {9},
keywords = {Tsetlin machines, Regression tsetlin machines, Weighted tsetlin machines, Interpretable machine learning, Stochastic searching on the line},
location = {Kitakyushu, Japan}
}

@inproceedings{10.1145/3395035.3425300,
author = {Johansen, H\r{a}vard D. and Johansen, Dag and Kupka, Tomas and Riegler, Michael A. and Halvorsen, P\r{a}l},
title = {Scalable Infrastructure for Efficient Real-Time Sports Analytics},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425300},
doi = {10.1145/3395035.3425300},
abstract = {Recent technological advances are adapted in sports to improve performance, avoid injuries, and make advantageous decisions. In this paper, we describe our ongoing efforts to develop and deploy PMSys, our smartphone-based athlete monitoring and reporting system. We describe our first attempts to gain insight into some of the data we have collected. Experiences so far are promising, both on the technical side and for athlete performance development. Our initial application of artificial-intelligence methods for prediction is encouraging and indicative.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {230–234},
numpages = {5},
keywords = {algorithmic analysis, artificial intelligence, machine learning, privacy-preserving data collection, sports performance logging},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1145/3453800.3453814,
author = {Zhu, Heng and Yang, Dongchao and Huang, Geng and Wu, Qingyuan and Li, Teng and Tao, Bo},
title = {YOLOv3 with Asymmetric Intersection over Union Based Loss Function for Human Detection},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453814},
doi = {10.1145/3453800.3453814},
abstract = {In order to improve the performance of the target detection model based on convolutional neural network, and make the bounding box representing the object location contain as complete original target information as possible, this paper improved the loss function of the bounding box regression part of the YOLOv3 network. A new metric, Asymmetric Intersection over Union (AIoU), is proposed to measure the similarity between the prediction box and the truth. Theoretical analysis shows that AIoU based loss function can better guide the direction of bounding box regression after introducing asymmetry. Experiments show that the improved YOLOv3 network achieves higher accuracy and maintains fast convergence in human target detection. The examples of bounding box regression on the test dataset show that the target information loss can be reduced by the proposed method. In addition, distinguishing between the prediction and the truth in calculation may have reference significance for the loss function design of network models for other tasks},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {70–76},
numpages = {7},
keywords = {Deep Learning, Intersection over Union, Loss Function, Target Detection},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@inproceedings{10.1007/978-3-030-61705-9_33,
author = {Santos, Mois\'{e}s R. and Mundim, Leandro R. and Carvalho, Andr\'{e} C. P. L. F.},
title = {Evaluation of Error Metrics for Meta-learning Label Definition in the Forecasting Task},
year = {2020},
isbn = {978-3-030-61704-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61705-9_33},
doi = {10.1007/978-3-030-61705-9_33},
abstract = {Meta-learning has been successfully applied to time series forecasting. For such, it uses meta-datasets created by previous machine learning applications. Each row in a meta-dataset represents a time series dataset. Each row, apart from the last, is meta-feature describing aspects of the related dataset. The last column is a target value, a meta-label. Here, the meta-label is the forecasting model with the best predictive performance for a specific error metric. In the previous studies applying meta-learning to time series forecasting, error metrics have been arbitrarily chosen. We believe that the error metric used can affect the results obtained by meta-learning. This study presents an experimental analysis of the predictive performance obtained by using different error metrics for the definition of the meta-label value. The experiments performed used 100 time series collected from the ICMC time series prediction open access repository, which has time series from a large variety of application domains. A traditional meta-learning framework for time series forecasting was used in this work. According to the experimental results, the mean absolute error can be the best metric for meta-label definition.},
booktitle = {Hybrid Artificial Intelligent Systems: 15th International Conference, HAIS 2020, Gij\'{o}n, Spain, November 11-13, 2020, Proceedings},
pages = {397–409},
numpages = {13},
keywords = {Meta-learning, Meta-label, Error metrics, Time series},
location = {Gij\'{o}n, Spain}
}

@inproceedings{10.5555/3524938.3524952,
author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
title = {Learning what to defer for maximum independent sets},
year = {2020},
publisher = {JMLR.org},
abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {14},
numpages = {11},
series = {ICML'20}
}

@article{10.1504/ijcse.2021.119989,
author = {Marulli, Fiammetta and Bellini, Emanuele and Marrone, Stefano},
title = {On managing security in smart e-health applications},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {24},
number = {6},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2021.119989},
doi = {10.1504/ijcse.2021.119989},
abstract = {Distributed machine learning can give an adaptable but strong shared condition for the design of trusted AI applications; this is mainly due to lack of privacy of centralised remote learning mechanisms. This notwithstanding, also distributed approaches have been compromised by several attack models (mainly data poisoning): in such a situation, a malicious member of the learning party may inject bad data. As such applications are growing in criticality, learning models must face with security and protection just as with versatility issues. The aim of the paper is to improve these applications by providing extra security features for distributed and federated learning mechanisms: more in the details, the paper examines specific concerns such as the utilisation of blockchain, homomorphic cryptography and meta-modelling techniques to ensure protection as well as other non-functional properties.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {639–652},
numpages = {13},
keywords = {federated learning, cloud computing, security in machine learning, adversarial attacks}
}

@article{10.1007/s11390-021-0771-8,
author = {Liu, Jason and Espina, Pedro and Sun, Xian-He},
title = {A Study on Modeling and Optimization of Memory Systems},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {1},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-021-0771-8},
doi = {10.1007/s11390-021-0771-8},
abstract = {Accesses Per Cycle (APC), Concurrent Average Memory Access Time (C-AMAT), and Layered Performance Matching (LPM) are three memory performance models that consider both data locality and memory assess concurrency. The APC model measures the throughput of a memory architecture and therefore reflects the quality of service (QoS) of a memory system. The C-AMAT model provides a recursive expression for the memory access delay and therefore can be used for identifying the potential bottlenecks in a memory hierarchy. The LPM method transforms a global memory system optimization into localized optimizations at each memory layer by matching the data access demands of the applications with the underlying memory system design. These three models have been proposed separately through prior efforts. This paper reexamines the three models under one coherent mathematical framework. More specifically, we present a new memory- centric view of data accesses. We divide the memory cycles at each memory layer into four distinct categories and use them to recursively define the memory access latency and concurrency along the memory hierarchy. This new perspective offers new insights with a clear formulation of the memory performance considering both locality and concurrency. Consequently, the performance model can be easily understood and applied in engineering practices. As such, the memory-centric approach helps establish a unified mathematical foundation for model-driven performance analysis and optimization of contemporary and future memory systems.},
journal = {J. Comput. Sci. Technol.},
month = jan,
pages = {71–89},
numpages = {19},
keywords = {performance modeling, performance optimization, memory architecture, memory hierarchy, concurrent average memory access time}
}

@inproceedings{10.1007/978-3-030-60248-2_14,
author = {Ye, Qianwen and Wu, Chase Q. and Liu, Wuji and Hou, Aiqin and Shen, Wei},
title = {Profiling-Based Big Data Workflow Optimization in a Cross-layer Coupled Design Framework},
year = {2020},
isbn = {978-3-030-60247-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60248-2_14},
doi = {10.1007/978-3-030-60248-2_14},
abstract = {Big data processing and analysis increasingly rely on workflow technologies for knowledge discovery and scientific innovation. The execution of big data workflows is now commonly supported on reliable and scalable data storage and computing platforms such as Hadoop. There are a variety of factors affecting workflow performance across multiple layers of big data systems, including the inherent properties (such as scale and topology) of the workflow, the parallel computing engine it runs on, the resource manager that orchestrates distributed resources, the file system that stores data, as well as the parameter setting of each layer. Optimizing workflow performance is challenging because the compound effects of the aforementioned layers are complex and opaque to end users. Generally, tuning their parameters requires an in-depth understanding of big data systems, and the default settings do not always yield optimal performance. We propose a profiling-based cross-layer coupled design framework to determine the best parameter setting for each layer in the entire technology stack to optimize workflow performance. To tackle the large parameter space, we reduce the number of experiments needed for profiling with two approaches: i) identify a subset of critical parameters with the most significant influence through feature selection; and ii) minimize the search process within the value range of each critical parameter using stochastic approximation. Experimental results show that the proposed optimization framework provides the most suitable parameter settings for a given workflow to achieve the best performance. This profiling-based method could be used by end users and service providers to configure and execute large-scale workflows in complex big data systems.},
booktitle = {Algorithms and Architectures for Parallel Processing: 20th International Conference, ICA3PP 2020, New York City, NY, USA, October 2–4, 2020, Proceedings, Part III},
pages = {197–217},
numpages = {21},
keywords = {Big data workflows, performance optimization, workflow profiling, stochastic approximation, coupled design},
location = {New York, NY, USA}
}

@article{10.1155/2019/8127869,
author = {Zhu, Qi and Yuan, Ning and Guan, Donghai and Deng, Ke},
title = {Cognitive Driven Multilayer Self-Paced Learning with Misclassified Samples},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1076-2787},
url = {https://doi.org/10.1155/2019/8127869},
doi = {10.1155/2019/8127869},
abstract = {In recent years, self-paced learning (SPL) has attracted much attention due to its improvement to nonconvex optimization based machine learning algorithms. As a methodology introduced from human learning, SPL dynamically evaluates the learning difficulty of each sample and provides the weighted learning model against the negative effects from hard-learning samples. In this study, we proposed a cognitive driven SPL method, i.e., retrospective robust self-paced learning (R2SPL), which is inspired by the following two issues in human learning process: the misclassified samples are more impressive in upcoming learning, and the model of the follow-up learning process based on large number of samples can be used to reduce the risk of poor generalization in initial learning phase. We simultaneously estimated the degrees of learning-difficulty and misclassified in each step of SPL and proposed a framework to construct multilevel SPL for improving the robustness of the initial learning phase of SPL. The proposed method can be viewed as a multilayer model and the output of the previous layer can guide constructing robust initialization model of the next layer. The experimental results show that the R2SPL outperforms the conventional self-paced learning models in classification task.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@article{10.1007/s10270-020-00832-3,
author = {Barquero, Gala and Troya, Javier and Vallecillo, Antonio},
title = {Improving query performance on dynamic graphs},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00832-3},
doi = {10.1007/s10270-020-00832-3},
abstract = {Querying large models efficiently often imposes high demands on system resources such as memory, processing time, disk access or network latency. The situation becomes more complicated when data are highly interconnected, e.g. in the form of graph structures, and when data sources are heterogeneous, partly coming from dynamic systems and partly stored in databases. These situations are now common in many existing social networking applications and geo-location systems, which require specialized and efficient query algorithms in order to make informed decisions on time. In this paper, we propose an algorithm to improve the memory consumption and time performance of this type of queries by reducing the amount of elements to be processed, focusing only on the information that is relevant to the query but without compromising the accuracy of its results. To this end, the reduced subset of data is selected depending on the type of query and its constituent filters. Three case studies are used to evaluate the performance of our proposal, obtaining significant speedups in all cases.},
journal = {Softw. Syst. Model.},
month = aug,
pages = {1011–1041},
numpages = {31},
keywords = {Data stream processing, Dynamic graphs, Performance optimization, Precomputing systems, Data queries}
}

@inproceedings{10.1609/aaai.v33i01.33015725,
author = {Zhang, Biqiao and Kong, Yuqing and Essl, Georg and Provost, Emily Mower},
title = {undefined-similarity preservation loss for soft labels: a demonstration on cross-corpus speech emotion recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015725},
doi = {10.1609/aaai.v33i01.33015725},
abstract = {In this paper, we propose a Deep Metric Learning (DML) approach that supports soft labels. DML seeks to learn representations that encode the similarity between examples through deep neural networks. DML generally presupposes that data can be divided into discrete classes using hard labels. However, some tasks, such as our exemplary domain of speech emotion recognition (SER), work with inherently subjective data, data for which it may not be possible to identify a single hard label. We propose a family of loss functions, undefined-Similarity Preservation Loss (undefined-SPL), based on the dual form of undefined-divergence for DML with soft labels. We show that the minimizer of undefined-SPL preserves the pairwise label similarities in the learned feature embeddings. We demonstrate the efficacy of the proposed loss function on the task of cross-corpus SER with soft labels. Our approach, which combines undefined-SPL and classification loss, significantly outperforms a baseline SER system with the same structure but trained with only classification loss in most experiments. We show that the presented techniques are more robust to over-training and can learn an embedding space in which the similarity between examples is meaningful.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {702},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1561/2000000039,
author = {Deng, Li and Yu, Dong},
title = {Deep Learning: Methods and Applications},
year = {2014},
issue_date = {Jun 2014},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {7},
number = {3–4},
issn = {1932-8346},
url = {https://doi.org/10.1561/2000000039},
doi = {10.1561/2000000039},
abstract = {This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.},
journal = {Found. Trends Signal Process.},
month = jun,
pages = {197–387},
numpages = {191},
keywords = {Deep learning, Machine learning, Artificial intelligence, Neural networks, Deep neural networks, Deep stacking networks, Autoencoders, Supervised learning, Unsupervised learning, Hybrid deep networks, Object recognition, Computer vision, Natural language processing, Language models, Multi-task learning, Multi-modal processing}
}

@inproceedings{10.1145/3123939.3123970,
author = {Hill, Parker and Jain, Animesh and Hill, Mason and Zamirai, Babak and Hsu, Chang-Hong and Laurenzano, Michael A. and Mahlke, Scott and Tang, Lingjia and Mars, Jason},
title = {DeftNN: addressing bottlenecks for DNN execution on GPUs via synapse vector elimination and near-compute data fission},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3123970},
doi = {10.1145/3123939.3123970},
abstract = {Deep neural networks (DNNs) are key computational building blocks for emerging classes of web services that interact in real time with users via voice, images and video inputs. Although GPUs have gained popularity as a key accelerator platform for deep learning workloads, the increasing demand for DNN computation leaves a significant gap between the compute capabilities of GPU-enabled datacenters and the compute needed to service demand.The state-of-the-art techniques to improve DNN performance have significant limitations in bridging the gap on real systems. Current network pruning techniques remove computation, but the resulting networks map poorly to GPU architectures, yielding no performance benefit or even slowdowns. Meanwhile, current bandwidth optimization techniques focus on reducing off-chip bandwidth while overlooking on-chip bandwidth, a key DNN bottleneck.To address these limitations, this work introduces DeftNN, a GPU DNN execution framework that targets the key architectural bottlenecks of DNNs on GPUs to automatically and transparently improve execution performance. DeftNN is composed of two novel optimization techniques - (1) synapse vector elimination, a technique that identifies non-contributing synapses in the DNN and carefully transforms data and removes the computation and data movement of these synapses while fully utilizing the GPU to improve performance, and (2) near-compute data fission, a mechanism for scaling down the on-chip data movement requirements within DNN computations. Our evaluation of DeftNN spans 6 state-of-the-art DNNs. By applying both optimizations in concert, DeftNN is able to achieve an average speedup of 2.1X on real GPU hardware. We also introduce a small additional hardware unit per GPU core to facilitate efficient data fission operations, increasing the speedup achieved by DeftNN to 2.6X.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {786–799},
numpages = {14},
keywords = {GPU architecture, deep neural networks, memory bandwidth, performance optimization},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{10.1007/978-3-319-24888-2_3,
author = {Ma, Guangkai and Gao, Yaozong and Wang, Li and Wu, Ligang and Shen, Dinggang},
title = {Soft-Split Random Forest for Anatomy Labeling},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_3},
doi = {10.1007/978-3-319-24888-2_3},
abstract = {Random Forest (RF) has been widely used in the learning-based labeling. In RF, each sample is directed from the root to each leaf based on the decisions made in the interior nodes, also called splitting nodes. The splitting nodes assign a testing sample to either left or right child based on the learned splitting function. The final prediction is determined as the average of label probability distributions stored in all arrived leaf nodes. For ambiguous testing samples, which often lie near the splitting boundaries, the conventional splitting function, also referred to as hard split function, tends to make wrong assignments, hence leading to wrong predictions. To overcome this limitation, we propose a novel soft-split random forest (SSRF) framework to improve the reliability of node splitting and finally the accuracy of classification. Specifically, a soft split function is employed to assign a testing sample into both left and right child nodes with their certain probabilities, which can effectively reduce influence of the wrong node assignment on the prediction accuracy. As a result, each testing sample can arrive at multiple leaf nodes, and their respective results can be fused to obtain the final prediction according to the weights accumulated along the path from the root node to each leaf node. Besides, considering the importance of context information, we also adopt a Haar-features based context model to iteratively refine the classification map. We have comprehensively evaluated our method on two public datasets, respectively, for labeling hippocampus in MR images and also labeling three organs in Head &amp; Neck CT images. Compared with the hard-split RF (HSRF), our method achieved a notable improvement in labeling accuracy.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {17–25},
numpages = {9},
location = {Munich, Germany}
}

@inproceedings{10.5555/3524938.3525116,
author = {Choo, Davin and Grunau, Christoph and Portmann, Julian and Rozho\v{n}, V\'{a}clav},
title = {k-means++: few more steps yield constant approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k)-approximation in expectation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant ε &gt; 0, with only εk additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {178},
numpages = {9},
series = {ICML'20}
}

@inproceedings{10.1145/3410463.3414670,
author = {Ye, Guixin and Tang, Zhanyong and Wang, Huanting and Fang, Dingyi and Fang, Jianbin and Huang, Songfang and Wang, Zheng},
title = {Deep Program Structure Modeling Through Multi-Relational Graph-based Learning},
year = {2020},
isbn = {9781450380751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410463.3414670},
doi = {10.1145/3410463.3414670},
abstract = {Deep learning is emerging as a promising technique for building predictive models to support code-related tasks like performance optimization and code vulnerability detection. One of the critical aspects of building a successful predictive model is having the right representation to characterize the model input for the given task. Existing approaches in the area typically treat the program structure as a sequential sequence but fail to capitalize on the rich semantics of data and control flow information, for which graphs are a proven representation structure.We present POEM, a novel framework that automatically learns useful code representations from graph-based program structures. At the core of POEM is a graph neural network (GNN) that is specially designed for capturing the syntax and semantic information from the program abstract syntax tree and the control and data flow graph. As a departure from existing GNN-based code modeling techniques, our network simultaneously learns over multiple relations of a program graph. This capability enables the learning framework to distinguish and reason about the diverse code relationships, be it a data or a control flow or any other relationships that may be important for the downstream processing task.We apply POEM to four representative tasks that require a strong ability to reason about the program structure: heterogeneous device mapping, parallel thread coarsening, loop vectorization and code vulnerability detection. We evaluate POEM on programs written in OpenCL, C, Java and Swift, and compare it against nine learning-based methods. Experimental results show that POEM consistently outperforms all competing methods across evaluation settings.},
booktitle = {Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
pages = {111–123},
numpages = {13},
keywords = {code optimization, machine learning, program modeling},
location = {Virtual Event, GA, USA},
series = {PACT '20}
}

@article{10.1007/s00034-021-01657-1,
author = {Pravin, Sheena Christabel and Palanivelan, M.},
title = {A Hybrid Deep Ensemble for Speech Disfluency Classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01657-1},
doi = {10.1007/s00034-021-01657-1},
abstract = {In this paper, a novel Hybrid Deep Ensemble (HDE) is proposed for automatic speech disfluency classification on a sparse speech dataset. Categorizations of speech disfluencies for diagnosis of speech disorders have so long relied on sophisticated deep learning models. Such a task can be accomplished by a straightforward approach with high accuracy by the proposed model which is an optimal combination of diverse machine learning and deep learning algorithms in a hierarchical arrangement which includes a deep autoencoder that yields the compressed latent features. The proposed model has shown considerable improvement in downgrading processing time overcoming the issues of cumbersome hyper-parameter tuning and huge data demand of the deep learning algorithms with high classification accuracy. Experimental results show that the proposed Hybrid Deep Ensemble has superior performance compared to the individual base learners, and the deep neural network as well. The proposed model and the baseline models were evaluated in terms of Cohen’s kappa coefficient, Hamming loss, Jaccard score, F-score and classification accuracy.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3968–3995},
numpages = {28},
keywords = {Hybrid Deep Ensemble, Speech disfluency classification, Sparse speech dataset, Deep autoencoder, Latent features}
}

@article{10.1016/j.asoc.2014.10.032,
author = {Liew, Wei Shiung and Seera, Manjeevan and Loo, Chu Kiong and Lim, Einly},
title = {Affect classification using genetic-optimized ensembles of fuzzy ARTMAPs},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {27},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2014.10.032},
doi = {10.1016/j.asoc.2014.10.032},
abstract = {Training neural networks in distinguishing different emotions from physiological signals frequently involves fuzzy definitions of each affective state. In addition, manual design of classification tasks often uses sub-optimum classifier parameter settings, leading to average classification performance. In this study, an attempt to create a framework for multi-layered optimization of an ensemble of classifiers to maximize the system's ability to learn and classify affect, and to minimize human involvement in setting optimum parameters for the classification system is proposed. Using fuzzy adaptive resonance theory mapping (ARTMAP) as the classifier template, genetic algorithms (GAs) were employed to perform exhaustive search for the best combination of parameter settings for individual classifier performance. Speciation was implemented using subset selection of classification data attributes, as well as using an island model genetic algorithms method. Subsequently, the generated population of optimum classifier configurations was used as candidates to form an ensemble of classifiers. Another set of GAs were used to search for the combination of classifiers that would result in the best classification ensemble accuracy. The proposed methodology was tested using two affective data sets and was able to produce relatively small ensembles of fuzzy ARTMAPs with excellent affect recognition accuracy.},
journal = {Appl. Soft Comput.},
month = feb,
pages = {53–63},
numpages = {11},
keywords = {Affect recognition, Classifier ensemble, Fuzzy ARTMAP, Genetic algorithm, Parameter optimization, Supervised learning}
}

@inproceedings{10.1145/2110147.2110161,
author = {Lienhardt, Michael and Clarke, Dave},
title = {Row types for delta-oriented programming},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110161},
doi = {10.1145/2110147.2110161},
abstract = {Delta-oriented programming (DOP) provides a technique for implementing Software Product Lines based on modifications (add, remove, modify) to a core program. Unfortunately, such modifications can introduce errors into a program, especially when type signatures of classes are modified in a non-monotonic fashion. To deal with this problem we present a type system for delta-oriented programs based on row polymorphism. This exercise elucidates the close correspondence between delta-oriented programs and row polymorphism.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {121–128},
numpages = {8},
keywords = {delta-oriented programming, software product line engineering, structural typing},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/3330345.3330377,
author = {Pfaffe, Philip and Grosser, Tobias and Tillmann, Martin},
title = {Efficient hierarchical online-autotuning: a case study on polyhedral accelerator mapping},
year = {2019},
isbn = {9781450360791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330345.3330377},
doi = {10.1145/3330345.3330377},
abstract = {Identifying the (near) optimal program variants an optimizing and parallelizing compiler should generate is known to be difficult. Autotuning is the best solution to navigate the often high-dimensional space of possible options. However, to be practical an autotuner should (a) have high convergence speed and (b) be robust in face of varying inputs. Current techniques for offline tuning, where convergence speed is less important, provide solutions only for known inputs, whereas online tuning can be input sensitive but currently lacks in convergence speed. In this paper, we present hierarchical online-autotuning, a novel technique to exploit structure in the search space and the underlying tuning problem to increase convergence speed during online tuning. By modeling symmetries and redundancies in configurations and by exploiting domain knowledge to predict performance we reduce the search space size by orders of magnitudes. Combining our tuner with a polyhedral parallelizing compiler for GPUs, we show that the performance of a GEMM GPU kernel generated with default parameters is increased by 6\texttimes{} and that the convergence speed of the tuning process is increased by a factor of up to 1.7 compared to OpenTuner. With hierarchical tuning we make the deployment of always-on online-autotuning practical.},
booktitle = {Proceedings of the ACM International Conference on Supercomputing},
pages = {354–366},
numpages = {13},
keywords = {GPGPU, online-autotuning, performance optimization, polyhedral compilation},
location = {Phoenix, Arizona},
series = {ICS '19}
}

@article{10.1109/MWC.011.2000501,
author = {Qin, Zhijin and Li, Geoffrey Ye and Ye, Hao},
title = {Federated Learning and Wireless Communications},
year = {2021},
issue_date = {October 2021},
publisher = {IEEE Press},
volume = {28},
number = {5},
issn = {1536-1284},
url = {https://doi.org/10.1109/MWC.011.2000501},
doi = {10.1109/MWC.011.2000501},
abstract = {Federated learning becomes increasingly attractive in the areas of wireless communications and machine learning due to its powerful learning ability and potential applications. In contrast to other machine learning techniques that require no communication resources, federated learning exploits communications between the central server and the distributed local clients to train and optimize a model. Therefore, how to efficiently assign limited communication resources to train a federated learning model becomes critical to performance optimization. On the other hand, federated learning, as a brand-new tool, can potentially enhance the intelligence of wireless networks. In this article, we provide a comprehensive overview of the relationship between federated learning and wireless communications, including basic principles of federated learning, efficient communications for training a federated learning model, and federated learning for intelligent wireless applications. We also identify some research challenges and directions at the end of this article.},
journal = {Wireless Commun.},
month = oct,
pages = {134–140},
numpages = {7}
}

@article{10.1613/jair.1.12562,
author = {Burashnikova, Aleksandra and Maximov, Yury and Clausel, Marianne and Laclau, Charlotte and Iutzeler, Franck and Amini, Massih-Reza},
title = {Learning over No-Preferred and Preferred Sequence of Items for Robust Recommendation},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12562},
doi = {10.1613/jair.1.12562},
abstract = {In this paper, we propose a theoretically supported sequential strategy for training a large-scale Recommender System (RS) over implicit feedback, mainly in the form of clicks. The proposed approach consists in minimizing pairwise ranking loss over blocks of consecutive items constituted by a sequence of non-clicked items followed by a clicked one for each user. We present two variants of this strategy where model parameters are updated using either the momentum method or a gradient-based approach. To prevent updating the parameters for an abnormally high number of clicks over some targeted items (mainly due to bots), we introduce an upper and a lower threshold on the number of updates for each user. These thresholds are estimated over the distribution of the number of blocks in the training set. They affect the decision of RS by shifting the distribution of items that are shown to the users. Furthermore, we provide a convergence analysis of both algorithms and demonstrate their practical efficiency over six large-scale collections with respect to various ranking measures and computational time.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {121–142},
numpages = {22},
keywords = {preferences, machine learning, theorem proving}
}

@inproceedings{10.1007/978-3-319-07518-1_16,
author = {Kunkel, Julian M. and Zimmer, Michaela and H\"{u}bbe, Nathanael and Aguilera, Alvaro and Mickler, Holger and Wang, Xuan and Chut, Andriy and B\"{o}nisch, Thomas and L\"{u}ttgau, Jakob and Michel, Roman and Weging, Johann},
title = {The SIOX Architecture --- Coupling Automatic Monitoring and Optimization of Parallel I/O},
year = {2014},
isbn = {9783319075174},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07518-1_16},
doi = {10.1007/978-3-319-07518-1_16},
abstract = {Performance analysis and optimization of high-performance I/O systems is a daunting task. Mainly, this is due to the overwhelmingly complex interplay of the involved hardware and software layers. The Scalable I/O for Extreme Performance SIOX project provides a versatile environment for monitoring I/O activities and learning from this information. The goal of SIOX is to automatically suggest and apply performance optimizations, and to assist in locating and diagnosing performance problems.In this paper, we present the current status of SIOX. Our modular architecture covers instrumentation of POSIX, MPI and other high-level I/O libraries; the monitoring data is recorded asynchronously into a global database, and recorded traces can be visualized. Furthermore, we offer a set of primitive plug-ins with additional features to demonstrate the flexibility of our architecture: A surveyor plug-in to keep track of the observed spatial access patterns; an fadvise plug-in for injecting hints to achieve read-ahead for strided access patterns; and an optimizer plug-in which monitors the performance achieved with different MPI-IO hints, automatically supplying the best known hint-set when no hints were explicitly set. The presentation of the technical status is accompanied by a demonstration of some of these features on our 20 node cluster. In additional experiments, we analyze the overhead for concurrent access, for MPI-IO's 4-levels of access, and for an instrumented climate application.While our prototype is not yet full-featured, it demonstrates the potential and feasibility of our approach.},
booktitle = {Proceedings of the 29th International Conference on Supercomputing - Volume 8488},
pages = {245–260},
numpages = {16},
keywords = {Machine Learning, Parallel I/O, Performance Optimization},
location = {Leipzig, Germany},
series = {ISC 2014}
}

@article{10.1177/10943420211010930,
author = {De Supinski, Bronis and Jacobs, Sam Ade and Moon, Tim and McLoughlin, Kevin and Jones, Derek and Hysom, David and Ahn, Dong H and Gyllenhaal, John and Watson, Pythagoras and Lightstone, Felice C and Allen, Jonathan E and Karlin, Ian and Van Essen, Brian},
title = {Enabling rapid COVID-19 small molecule drug design through scalable deep learning of generative models},
year = {2021},
issue_date = {Sep 2021},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {35},
number = {5},
issn = {1094-3420},
url = {https://doi.org/10.1177/10943420211010930},
doi = {10.1177/10943420211010930},
abstract = {We improved the quality and reduced the time to produce machine learned models for use in small molecule antiviral design. Our globally asynchronous multi-level parallel training approach strong scales to all of Sierra with up to 97.7% efficiency. We trained a novel, character-based Wasserstein autoencoder that produces a higher quality model trained on 1.613 billion compounds in 23 minutes while the previous state of the art takes a day on 1 million compounds. Reducing training time from a day to minutes shifts the model creation bottleneck from computer job turnaround time to human innovation time. Our implementation achieves 318 PFLOPs for 17.1% of half-precision peak. We will incorporate this model into our molecular design loop enabling the generation of more diverse compounds; searching for novel, candidate antiviral drugs improves and reduces the time to synthesize compounds to be tested in the lab.},
journal = {Int. J. High Perform. Comput. Appl.},
month = sep,
pages = {469–482},
numpages = {14},
keywords = {COVID 19, machine learning, scalable performance, generative models, drug design}
}

@inproceedings{10.1145/2964284.2973801,
author = {Latifi Oskouei, Seyyed Salar and Golestani, Hossein and Hashemi, Matin and Ghiasi, Soheil},
title = {CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973801},
doi = {10.1145/2964284.2973801},
abstract = {Many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep CNN-based machine learning algorithms. However, performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid [1], for execution of trained deep CNNs on Android-based mobile devices. Empirical evaluations show that CNNdroid achieves up to 60X speedup and 130X energy saving on current mobile devices. The CNNdroid open source library is available for download at https://github.com/ENCP/CNNdroid},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1201–1205},
numpages = {5},
keywords = {android, deep convolutional neural network (CNN), low energy consumption, mobile GPU, open source software, performance optimization, renderscript},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.1007/s00521-018-3478-1,
author = {Gu, Nannan and Fan, Pengying and Fan, Mingyu and Wang, Di},
title = {Structure regularized self-paced learning for robust semi-supervised pattern classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3478-1},
doi = {10.1007/s00521-018-3478-1},
abstract = {Semi-supervised classification is a hot topic in pattern recognition and machine learning. However, in presence of heavy noise and outliers, the unlabeled training data could be very challenging or even misleading for the semi-supervised classifier. In this paper, we propose a novel structure regularized self-paced learning method for semi-supervised classification problems, which can efficiently learn partially labeled training data sequentially from the simple to the complex ones. The proposed formulation consists of three components: a cost function defined by a mixture of losses, a functional complexity regularizer, and a self-paced regularizer; and the corresponding optimization algorithm involves three iterative steps: classifier updating, sample importance calculating, and pseudo-labeling. In the proposed method, the cost function for classifier updating and sample importance calculating is defined as a combination of the label fitting loss and manifold smoothness loss. Then, the importance of the pseudo-labeled and unlabeled samples is adaptively calculated by the novel cost. Unlabeled samples with high importance values are pseudo-labeled with their current predictions. In this way, labels are efficiently propagated from the labeled samples to the unlabeled ones in the robust self-paced manner. Experimental results on several benchmark data sets are provided to show the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6559–6574},
numpages = {16},
keywords = {Semi-supervised classification, Pattern classification, Self-paced learning, Manifold learning, Locally linear coding}
}

@article{10.1016/j.cie.2021.107492,
author = {Fan, Haipeng and Wu, Min and Cao, Weihua and Lai, Xuzhi and Chen, Luefeng and Lu, Chengda and Du, Sheng and She, Jinhua},
title = {An operating performance assessment strategy with multiple modes based on least squares support vector machines for drilling process},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107492},
doi = {10.1016/j.cie.2021.107492},
journal = {Comput. Ind. Eng.},
month = sep,
numpages = {11},
keywords = {Drilling process, Multiple modes, Operating performance assessment, Process capability index, Support vector machine}
}

@inproceedings{10.1145/3127479.3128605,
author = {Zhu, Yuqing and Liu, Jianxun and Guo, Mengying and Bao, Yungang and Ma, Wenlong and Liu, Zhuoyue and Song, Kunpeng and Yang, Yingchun},
title = {BestConfig: tapping the performance potential of systems via automatic configuration tuning},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3128605},
doi = {10.1145/3127479.3128605},
abstract = {An ever increasing number of configuration parameters are provided to system users. But many users have used one configuration setting across different workloads, leaving untapped the performance potential of systems. A good configuration setting can greatly improve the performance of a deployed system under certain workloads. But with tens or hundreds of parameters, it becomes a highly costly task to decide which configuration setting leads to the best performance. While such task requires the strong expertise in both the system and the application, users commonly lack such expertise.To help users tap the performance potential of systems, we present Best Config, a system for automatically finding a best configuration setting within a resource limit for a deployed system under a given application workload. BestConfig is designed with an extensible architecture to automate the configuration tuning for general systems. To tune system configurations within a resource limit, we propose the divide-and-diverge sampling method and the recursive bound-and-search algorithm. BestConfig can improve the throughput of Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce the running time of Hive join job by about 50% and that of Spark join job by about 80%, solely by configuration adjustment.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {338–350},
numpages = {13},
keywords = {ACT, automatic configuration tuning, performance optimization},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/3410220.3453911,
author = {Hon, Hsiao-Wuen},
title = {AI for System - Infusing AI into Cloud Computing Systems},
year = {2021},
isbn = {9781450380720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410220.3453911},
doi = {10.1145/3410220.3453911},
abstract = {In the past fifteen years, the most significant paradigm shift in the computing industry is the migration to cloud computing, which brings unprecedented opportunities of digital transformation to business, society, and human life. The implication of this is profound. It means that cloud computing platforms have become part of the basic infrastructure of the world. Therefore, the non-functional properties of cloud computing platforms, including availability, reliability, performance, efficiency, security, sustainability, etc., become immensely important. The distributed nature, massive scale, and high complexity of cloud computing platforms ranging from storage to networking, computing and beyond present huge challenges to achieve effective and efficient building and operation of such software systems. There is huge wealth of various types of data available throughout the entire development lifecycle of software systems. This is manifested even stronger with the paradigm shift to cloud computing as much more data are available on system runtime and workloads. Leveraging the amount of data, AI for System is to utilize AI/ML technologies to design and build high-quality cloud systems at scale. In this talk, I will first introduce the concept of AI for System and its research landscape. Then using a few projects at Microsoft as examples [1-10], I will talk about the work from Microsoft Research on AI for System and its impact. I will also discuss the research challenges and opportunities in AI for System moving forward.},
booktitle = {Abstract Proceedings of the 2021 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems},
pages = {39–40},
numpages = {2},
keywords = {ai for system, aiops, artificial intelligence, cloud computing, digital transformation, distributed system, machine learning, software analytics},
location = {Virtual Event, China},
series = {SIGMETRICS '21}
}

@article{10.1016/j.neunet.2021.03.022,
author = {Zhong, Yongjian and Du, Bo and Xu, Chang},
title = {Learning to reweight examples in multi-label classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.03.022},
doi = {10.1016/j.neunet.2021.03.022},
journal = {Neural Netw.},
month = oct,
pages = {428–436},
numpages = {9},
keywords = {Multi-label classification, Self-paced learning, Reweight instance}
}

@inproceedings{10.1145/3394486.3406703,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record.The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology.DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {distributed deep learning, machine learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3200842.3200858,
author = {Erdeniz, Seda Polat and Felfernig, Alexander},
title = {OCSH: optimized cluster specific heuristics for the university course timetabling problem},
year = {2018},
isbn = {9781450364041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3200842.3200858},
doi = {10.1145/3200842.3200858},
abstract = {The University Course Timetabling Problem (UCTP) is a search problem that allocates a given number of rooms with given courses based on their scheduled time slots. UCTP belongs to the NP-complete class and can be defined as a constraint satisfaction problem (CSP). To solve the performance issue of CSP solvers, there are various local search methods. CSP solvers often use variable and value ordering heuristics to improve their search performance. Specific variable and value ordering heuristics can be even calculated by the help of a learning algorithm. Cluster-Specific Heuristics (CSH) are variable ordering heuristics which are learned based on clusters of CSPs. In this paper, to solve UCTP with user requirements, we propose a better performing CSH which is called Optimized Cluster-Specific Heuristics (OCSH). We have tested OCSH on generated time tabling problems with various user requirements and compared the runtime performances of variations of cluster-specific heuristics with OCSH. Finally, we show that OCSH is the best performing version of cluster specific heuristics to solve UCTP with user requirements.},
booktitle = {Proceedings of the 8th International Conference on Information Systems and Technologies},
articleno = {13},
numpages = {6},
keywords = {artificial intelligence, clustering, configuration, constraint satisfaction problems, performance optimization, variable and value ordering heuristics},
location = {Istanbul, Turkey},
series = {ICIST '18}
}

@inproceedings{10.1145/3219104.3219156,
author = {Huang, Ruizhu and Xu, Weijia and Liverani, Silvia and Hiltbrand, Dave and Stapleton, Ann E.},
title = {A Case Study of R Performance Analysis and Optimization},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3219156},
doi = {10.1145/3219104.3219156},
abstract = {Although R has become an analytic platform for many scientific domains, high performance has rarely been a trait of R. The inefficiency can come from the R programming specification itself or the interpreter environment implementation. Profiling and optimizing useful R code can not only directly benefit domain science researchers but also increase the efficiency of R code to run on high performance computing resources. We use envirotyping analysis as an example. This analysis considers both genetic information and environment conditions to understand how these factors affect crop yields through multidimensional data collected from fields and simulations. The analysis has the potential to improve breeding schemes for better global crop yield. A central tool used to support this analysis is an R package, "PReMiuM: Dirichlet Process Bayesian Clustering, Profile Regression", whose computational complexity increases as numbers of observations and features grow. The package is a useful tool for Bayesian clustering and inference with broad application potentials if computational bottlenecks can be overcome. In this paper, we detail our experiences on detecting the bottlenecks and optimizing its performance. We present a general workflow for investigating general performance issues such as execution time and memory usage to understand R program behavior and thus helping the optimization of the code. The workflow can be applied to other R applications. With the approach presented here, R users can easily identify inefficient code block, search for potential optimization solutions, and efficiently utilize high performance computing resources for scientific research.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing: Seamless Creativity},
articleno = {33},
numpages = {6},
keywords = {PReMiuM, R, performance optimization, software profiling},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@article{10.1007/s10470-016-0918-7,
author = {Nasef, Ashrf and Marjanovi\'{c}-Jakovljevi\'{c}, Marina and Njegu\v{s}, Angelina},
title = {Stochastic gradient descent analysis for the evaluation of a speaker recognition},
year = {2017},
issue_date = {February  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {90},
number = {2},
issn = {0925-1030},
url = {https://doi.org/10.1007/s10470-016-0918-7},
doi = {10.1007/s10470-016-0918-7},
abstract = {Performance optimization in speaker recognition is a challenging task in the field of vocal based human-computer interaction. Many researches have shown that deep learning Neural Network methods have the best performance in comparison with other classifiers. However, those methods with many parameters require a lot of tunings in order to optimize the performance in different supervised learning tasks. In this paper, we show that picking a good combination of parameters can significantly improve the performance of Stochastic Gradient Descent deep learning Neural Network method in automatic speaker recognition even in a noisy environment. Parameters that are analyzed are learning rate, hidden and input layer dropout rate.},
journal = {Analog Integr. Circuits Signal Process.},
month = feb,
pages = {389–397},
numpages = {9},
keywords = {Deep learning Neural Network, Dropout rate, Learning rate, Pattern recognition, Speech analysis, Stochastic gradient descent}
}

@article{10.1016/j.artmed.2021.102165,
author = {de Siqueira, Vilson Soares and Borges, Mois\'{e}s Marcos and Furtado, Rog\'{e}rio Gomes and Dourado, Colandy Nunes and da Costa, Ronaldo Martins},
title = {Artificial intelligence applied to support medical decisions for the automatic analysis of echocardiogram images: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102165},
doi = {10.1016/j.artmed.2021.102165},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {19},
keywords = {Echocardiogram, Echocardiography, Machine Learning, Deep Learning}
}

@article{10.1007/s11042-021-10893-1,
author = {Xue, Tao and Hong, Yang},
title = {IX-ResNet: fragmented multi-scale feature fusion for image classification},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {18},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10893-1},
doi = {10.1007/s11042-021-10893-1},
abstract = {With the continuous in-depth study of convolutional neural network in computer vision, how to improve the performance of network structure has been the focus of current research. Recent works have shown that multi-scale feature concatenation, shortcut connection and grouping convolution can effectively train deeper networks and improve the accuracy and effectiveness of the network. In this paper, we present a novel feature transformation strategy of fragmented multi-scale feature fusion. Moreover, an efficient modularized image classification network, IX-ResNet, is proposed based on this new strategy. IX-ResNet consists of many large isomorphic modules stacked in the form of residual network while Each large module can be composed of many small heterogeneous modules. The performance of IX-ResNet is verified on cifar-10, cifar-100 and ImageNet-1&nbsp;K datasets, which indicates that IX-ResNet model using fragmented multi-scale feature fusion strategy can further improve accuracy compare to the original grouping convolution network ResNeXt with the same or even lower parameters.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {27855–27865},
numpages = {11},
keywords = {CNN, Image classification, Grouping, Multi-scale, Fragment}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Digital phenotyping, Digital medicine, Paralinguistics, Machine learning, Speech elicitation, Valence}
}

@article{10.1007/s11042-020-10443-1,
author = {Rao, Champakamala Sundar and Karunakara, K.},
title = {A comprehensive review on brain tumor segmentation and classification of MRI images},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10443-1},
doi = {10.1007/s11042-020-10443-1},
abstract = {In the analysis of medical images, one of the challenging tasks is the recognition of brain tumours via medical resonance images (MRIs). The diagnosis process is still tedious due to its complexity and considerable variety in tissues of tumor perception. Therefore, the necessities of tumor identification techniques are improving nowadays for medical applications. In the past decades, different approaches in the segmentation of various precisions and complexity degree have been accomplished, which depends on the simplicity and the benchmark of the technique. An overview of this analysis is to give out the summary of the semi-automatic techniques for brain tumor segmentation and classification utilizing MRI. An enormous amount of MRI based image data is accomplished using deep learning approaches. There are several works, dealing on the conventional approaches for MRI-based segmentation of brain tumor. Alternatively, in this review, we revealed the latest trends in the methods of deep learning. Initially, we explain the several threads in MRI pre-processing, including registration of image, rectification of bias field, and non-brain tissue dismissal. And terminally, the present state evaluation of algorithm is offered and forecasting the growths to systematise the MRI-based brain tumor into a regular cyclic routine in the clinical field are focussed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17611–17643},
numpages = {33},
keywords = {MRI, Brain tumor, Segmentation, Bias field, Tissue, Image processing}
}

@inproceedings{10.1145/3437984.3458840,
author = {Mulder, Rik and Radu, Valentin and Dubach, Christophe},
title = {Fast Optimisation of Convolutional Neural Network Inference using System Performance Models},
year = {2021},
isbn = {9781450382984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437984.3458840},
doi = {10.1145/3437984.3458840},
abstract = {The choice of convolutional routines (or primitives) for implementing the operations in a Convolutional Neural Network (CNN) has a tremendous impact over the inference time. To optimise the execution latency for a target system, a lengthy profiling stage is needed - iterating over all the implementations of convolutional primitives in the configuration of each layer to measure their execution time on that platform. Each primitive exercises the system resources in different ways, so new profiling is currently needed when optimising for another system. In this work, we replace this prohibitively expensive profiling stage with a machine learning based approach of performance modelling. Our approach drastically speeds up the optimisation by estimating the latency of convolutional primitives in any layer configuration running on a target system. We reduce the time needed for optimising the execution of large neural networks on an ARM Cortex-A73 system from hours to just seconds. Our performance model is easily transferable across target platforms. This is demonstrated by training a performance model on an Intel platform and transferring its predictive performance to AMD and ARM systems, using very few profiled samples from the target platforms for fine-tuning the performance model.},
booktitle = {Proceedings of the 1st Workshop on Machine Learning and Systems},
pages = {104–110},
numpages = {7},
keywords = {inference optimisation, neural network latency},
location = {Online, United Kingdom},
series = {EuroMLSys '21}
}

@inproceedings{10.1007/978-3-030-32692-0_49,
author = {Peng, Shiqi and Lai, Bolin and Yao, Guangyu and Zhang, Xiaoyun and Zhang, Ya and Wang, Yan-Feng and Zhao, Hui},
title = {Learning-Based Bone Quality Classification Method for Spinal Metastasis},
year = {2019},
isbn = {978-3-030-32691-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32692-0_49},
doi = {10.1007/978-3-030-32692-0_49},
abstract = {Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by +12.54%, +7.23% and +29.06% for blastic, mixed and lytic lesions, respectively, meanwhile +12.33%, +23.21% and +34.25% at vertebrae level.},
booktitle = {Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {426–434},
numpages = {9},
keywords = {Spinal metastasis, Bone quality classification, Multi-task learning, Self-paced learning},
location = {Shenzhen, China}
}

@article{10.1155/2021/4327896,
author = {Xie, Shu-Tong and He, Zong-Bao and Chen, Qiong and Chen, Rong-Xin and Kong, Qing-Zhao and Song, Cun-Ying and Huang, Jiwei},
title = {Predicting Learning Behavior Using Log Data in Blended Teaching},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4327896},
doi = {10.1155/2021/4327896},
abstract = {Online and offline blended teaching mode, the future trend of higher education, has recently been widely used in colleges around the globe. In the article, we conducted a study on students’ learning behavior analysis and student performance prediction based on the data about students’ behavior logs in three consecutive years of blended teaching in a college’s “Java Language Programming” course. Firstly, the data from diverse platforms such as MOOC, Rain Classroom, PTA, and cnBlog are integrated and preprocessed. Secondly, a novel multiclass classification framework, combining the genetic algorithm (GA) and the error correcting output codes (ECOC) method, is developed to predict the grade levels of students. In the framework, GA is designed to realize both the feature selection and binary classifier selection to fit the ECOC models. Finally, key factors affecting grades are identified in line with the optimal subset of features selected by GA, which can be analyzed for teaching significance. The results show that the multiclass classification algorithm designed in this article can effectively predict grades compared with other algorithms. In addition, the selected subset of features corresponding to learning behaviors is pedagogically instructive.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-86380-7_23,
author = {Krysi\'{n}ska, Izabela and Morzy, Miko\l{}aj and Kajdanowicz, Tomasz},
title = {Curriculum Learning Revisited: Incremental Batch Learning with Instance Typicality Ranking},
year = {2021},
isbn = {978-3-030-86379-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86380-7_23},
doi = {10.1007/978-3-030-86380-7_23},
abstract = {The technique of curriculum learning mimics cognitive mechanisms observed in human learning, where simpler concepts are presented prior to gradual introduction of more difficult concepts. Until now, the major obstacle for curriculum methods was the lack of a reliable method for estimating the difficulty of training instances. In this paper we show that, instead of trying to assess the difficulty of learning instances, a simple graph-based method of computing the typicality of instances can be used in conjunction with curriculum methods. We design new batch schedulers which organize ordered instances into batches of varying size and learning difficulty. Our method does not require any changes to the architecture of trained models, we improve the training merely by manipulating the order and frequency of instance presentation to the model.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part IV},
pages = {279–291},
numpages = {13},
keywords = {Curriculum learning, Typicality, Batch training},
location = {Bratislava, Slovakia}
}

@inproceedings{10.1007/978-3-030-90439-5_26,
author = {Lakshya},
title = {Behaviour of Sample Selection Techniques Under Explicit Regularization},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_26},
doi = {10.1007/978-3-030-90439-5_26},
abstract = {There is a multitude of sample selection-based learning strategies that have been developed for learning with noisy labels. However, It has also been indicated in the literature that perhaps early stopping is better than fully training the model for getting better performance. It leads us to wonder about the behavior of the sample selection strategies under explicit regularization. To this end, we considered four of the most fundamental sample selection-based models MentorNet, Coteaching, Coteaching-plus and JoCor. We provide empirical results of applying explicit L2 regularization to the above-mentioned approaches. We also compared the results with a baseline - a vanilla CNN model trained with just regularization. We show that under explicit regularization, the pre-conceived ranking of the approaches might change. We also show several instances where the baseline was able to outperform some or all of the existing approaches. Moreover, we show that under explicit regularization, the performance gap between the approaches can also reduce.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {331–340},
numpages = {10}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@inproceedings{10.1145/3409073.3409084,
author = {Congyi, Deng and Guangshun, Shi},
title = {Method for Detecting Android Malware Based on Ensemble Learning},
year = {2020},
isbn = {9781450377645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409073.3409084},
doi = {10.1145/3409073.3409084},
abstract = {In recent years, we have become increasingly dependent on smart devices. Android is an operating system mainly used on mobile devices, where hundreds of millions of users can download various apps through many application stores. Under these circumstances, a large number of malicious apps can be put into the application stores by developers to achieve the purpose of attacking, controlling user devices, and even stealing user information and property. Therefore, it is necessary to identify malwares in mass apps through analysis and detection to remind users. We propose an idea of detecting and discriminating Android malware based on an ensemble learning method. Firstly, a static analysis of AndroidManifest file in APK is performed to extract features such as permission calls, component calls, and intents in system. Then we use XGBoost method, an implementation of ensemble learning, to detect malicious applications. The conclusion is that this system performs very well in Android malware detection.},
booktitle = {Proceedings of the 2020 5th International Conference on Machine Learning Technologies},
pages = {28–31},
numpages = {4},
keywords = {Android Malware, Ensemble Learning, Malware Detection, Static Analysis},
location = {Beijing, China},
series = {ICMLT '20}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Adaptive Boosting, Cyber security, Ensemble Learning, Extreme Learning Machine, Fractional Polynomial Kernels, KDD Cup 1999, Kernel Selection, Machine Learning, Multiclass Classification, Multiple Kernel Learning, Network Intrusion Detection}
}

@article{10.1007/s10462-020-09889-4,
author = {Dornaika, Fadi},
title = {Joint feature and instance selection using manifold data criteria: application to image classification},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {3},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09889-4},
doi = {10.1007/s10462-020-09889-4},
abstract = {In many pattern recognition applications feature selection and instance selection can be used as two data preprocessing methods that aim at reducing the computational cost of the learning process. Moreover, in some cases, feature subset selection can improve the classification performance. Feature selection and instance selection can be interesting since the choice of features and instances greatly influence the performance of the learnt models as well as their training costs. In the past, unifying both problems was carried out by solving a global optimization problem using meta-heuristics. This paradigm not only does not exploit the manifold structure of data but can be computationally expensive. To the best of our knowledge, the joint use of sparse modeling representative and feature subset relevance have not been exploited by the joint feature and selection methods. In this paper, we target the joint feature and instance selection by adopting feature subset relevance and sparse modeling representative selection. More precisely, we propose three schemes for the joint feature and instance selection. The first is a wrapper technique while the two remaining ones are filter approaches. In the filter approaches, the search process adopts a genetic algorithm in which the evaluation is mainly given by a score that quantify the goodness of the features and instances. An efficient instance selection technique is used and integrated in the search process in order to adapt the instances to the candidate feature subset. We evaluate the performance of the proposed schemes using image classification where classifiers are the nearest neighbor classifier and support vector machine classifier. The study is conducted on five public image datasets. These experiments show the superiority of the proposed schemes over various baselines. The results confirm that the filter approaches leads to promising improvement on classification accuracy when both feature selection and instance selection are adopted.},
journal = {Artif. Intell. Rev.},
month = mar,
pages = {1735–1765},
numpages = {31},
keywords = {Feature selection, Instance selection, Feature and instance selection, Data reduction, Linear discriminant analysis (LDA), Local discriminant embedding (LDE), Classification}
}

@inproceedings{10.1145/3349341.3349504,
author = {Huang, Chunfang and Wang, Xiangrong},
title = {Financial Innovation Based on Artificial Intelligence Technologies},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349504},
doi = {10.1145/3349341.3349504},
abstract = {Nowadays, the degree of the heated topic of artificial intelligence in the world reaches a new height. Due to the breakthrough of deep learning algorithm based on neural network, the level of artificial intelligence technologies has been enhanced significantly. The global financial industry is quietly changing under the catalysis of artificial intelligence. The frontier artificial intelligence technologies, such as the technology of expert system, machine learning and knowledge discovery in database are combed to explore the financial applications of artificial intelligence. Based on these key technologies, this paper proposed three applications of artificial intelligence in the financial field, including intelligent investment adviser, transaction forecast and financial regulation, discusses the key technologies of artificial intelligence and financial innovation products based on these technologies, such as the functions of the transaction prediction system based on artificial intelligence technologies include forecast analysis, index statistics, stock analysis and information retrieval, etc. The structures of the systems are drawn and the design principles are provided. Finally, to guard the safety of the applications of artificial intelligence, the paper gives the suggestions of enhancing identity authentication, introducing monitoring measures and limiting autonomy degree.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {750–754},
numpages = {5},
keywords = {Deep Learning, Financial Regulation, Intelligent Investment Adviser, Machine Learning, Transaction Forecast},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1007/978-3-030-28061-1_8,
author = {Ma, Xiaobin and Du, Zhihui and Sun, Yankui and Tchernykh, Andrei and Wu, Chao and Wei, Jianyan},
title = {An Efficient Parallel Framework to Analyze Astronomical Sky Survey Data},
year = {2018},
isbn = {978-3-030-28060-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-28061-1_8},
doi = {10.1007/978-3-030-28061-1_8},
abstract = {Big data has been an important analysis method anywhere we turn today. We hold broad recognition of the value of data, and products obtained through analyzing it. There are multiple steps to the data analysis pipeline, which can be abstracted as a framework provides universal parallel high-performance data analysis. Based on ray, this paper proposed a parallel framework written in Python with an interface to aggregate and analyze homogeneous astronomical sky survey time series data. As such, we can achieve parallel training and analysis only by defining the customized analyze functions, decision module and I/O interfaces, while the framework is able to manage the pipeline such as data fetching, saving, parallel job scheduling and load balancing. Meanwhile, the data scientists can focus on the analysis procedure and save the time speeding this program up. We tested out the framework on synthetic data with raw files and HBase entries as data sources and result formats, reduced the analyze cost for scientists not familiar with parallel programming while needs to handle a mass of data. We integrate time series anomaly detection algorithms with our parallel dispatching module to achieve high-performance data processing frameworks. Experimental results on synthetic astronomical sky survey time series data show that our model achieves good speed up ratio in executing analysis programs.},
booktitle = {Big Scientific Data Management: First International Conference, BigSDM 2018, Beijing, China, November 30 – December 1, 2018, Revised Selected Papers},
pages = {67–77},
numpages = {11},
keywords = {Big data analysis, Deep learning algorithm, Performance optimization},
location = {Beijing, China}
}

@inproceedings{10.1145/3321408.3326675,
author = {Tang, Xudong and Xue, Chen and Wu, Tao and Wang, Tiejun},
title = {Optimization of parallel program based on lattice BGK method},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326675},
doi = {10.1145/3321408.3326675},
abstract = {Lattice Boltzmann Method (LBM) is a new method for fluid system modeling and simulation. This method is a mesoscopic model between the micro-molecular dynamics model and the macro-continuous model of fluids, and has both models' advantages. In this paper, we use the parallel methods and the extended interfaces provided by Sunway TaihuLight System to optimize the fluid solution program based on the lattice Boltzmann method (we use D3Q19 model). Based on the light of Sunway TaihuLight, we use two-level parallel mode to optimize the SWLBM program (based on LBM). The first level uses MPI parallel partition, and the second level uses master-slave accelerated parallel method to improve performance. We optimize the solution process based on lattice Boltzmann method, which greatly improves the efficiency of the fluid calculation program based on lattice Boltzmann method. Our method reduces the computational time of SWLBM program from 3127 s to 113 s.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {147},
numpages = {7},
keywords = {LBM, athread, lattice bhatnagar-gross-krook, parallel, performance optimization},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@article{10.1007/s11227-018-2252-6,
author = {B\'{a}n, D\'{e}nes and Ferenc, Rudolf and Siket, Istv\'{a}n and Kiss, \'{A}kos and Gyim\'{o}thy, Tibor},
title = {Prediction models for performance, power, and energy efficiency of software executed on heterogeneous hardware},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2252-6},
doi = {10.1007/s11227-018-2252-6},
abstract = {Heterogeneous computer environments are becoming commonplace so it is increasingly important to understand how and where we could execute a given algorithm the most efficiently. In this paper we propose a methodology that uses both static source code metrics, and dynamic execution time, power, and energy measurements to build gain ratio prediction models. These models are trained on special benchmarks that have both sequential and parallel implementations and can be executed on various computing elements, e.g., on CPUs, GPUs, or FPGAs. After they are built, however, they can be applied to a new system using only the system’s static source code metrics which are much more easily computable than any dynamic measurement. We found that while estimating a continuous gain ratio is a much harder problem, we could predict the gain category (e.g., “slight improvement” or “large deterioration”) of porting to a specific configuration significantly more accurately than a random choice, using static information alone. We also conclude based on our benchmarks that parallelized implementations are less maintainable, thereby supporting the need for automatic transformations.},
journal = {J. Supercomput.},
month = aug,
pages = {4001–4025},
numpages = {25},
keywords = {Green computing, Heterogeneous architecture, Performance optimization, Power-aware execution, Configuration selection}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {Software product line, actor, cross-domain reference architecture, multi product line, reference architecture},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.5555/3524938.3525166,
author = {Degenne, R\'{e}my and Shao, Han and Koolen, Wouter M.},
title = {Structure adaptive algorithms for stochastic bandits},
year = {2020},
publisher = {JMLR.org},
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the suboptimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {228},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.5555/3540261.3540762,
author = {Wang, Jixuan and Wang, Kuan-Chieh and Rudzicz, Frank and Brudno, Michael},
title = {Grad2Task: improved few-shot text classification using gradients for task representation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Large pretrained language models (LMs) like BERT have improved performance in many disparate natural language processing (NLP) tasks. However, fine tuning such models requires a large number of training examples for each target task. Simultaneously, many realistic NLP problems are "few shot", without a sufficiently large training set. In this work, we propose a novel conditional neural process-based approach for few-shot text classification that learns to transfer from other diverse tasks with rich annotation. Our key idea is to represent each task using gradient information from a base model and to train an adaptation network that modulates a text classifier conditioned on the task representation. While previous task-aware few-shot learners represent tasks by input encoding, our novel task representation is more powerful, as the gradient captures input-output relationships of a task. Experimental results show that our approach outperforms traditional fine-tuning, sequential transfer learning, and state-of-the-art meta learning approaches on a collection of diverse few-shot tasks. We further conducted analysis and ablations to justify our design choices.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {501},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Optimal feature selection, Software product line, Evolutionary algorithm, Multi-objective optimization}
}

@inproceedings{10.5555/3524938.3525504,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Extrapolation for large-batch training in deep learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {566},
numpages = {11},
series = {ICML'20}
}

@article{10.1016/j.neucom.2019.04.066,
author = {Zhu, Qi and Yuan, Ning and Huang, Jiashuang and Hao, Xiaoke and Zhang, Daoqiang},
title = {Multi-modal AD classification via self-paced latent correlation analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.066},
doi = {10.1016/j.neucom.2019.04.066},
journal = {Neurocomput.},
month = aug,
pages = {143–154},
numpages = {12},
keywords = {Multi-modal fusion, Feature extraction, Low-rank, Self-paced learning, Computer-aided diagnosis}
}

@inproceedings{10.1007/978-3-030-19945-6_1,
author = {Rao, Nageswara S. V. and Sen, Satyabrata and Liu, Zhengchun and Kettimuthu, Rajkumar and Foster, Ian},
title = {Learning Concave-Convex Profiles of Data Transport over Dedicated Connections},
year = {2018},
isbn = {978-3-030-19944-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-19945-6_1},
doi = {10.1007/978-3-030-19945-6_1},
abstract = {Dedicated data transport infrastructures are increasingly being deployed to support distributed big-data and high-performance computing scenarios. These infrastructures employ data transfer nodes that use sophisticated software stacks to support network transport among sites, which often house distributed file and storage systems. Throughput measurements collected over such infrastructures for a range of round trip times (RTTs) reflect the underlying complex end-to-end connections, and have revealed dichotomous throughput profiles as functions of RTT. In particular, concave regions of throughput profiles at lower RTTs indicate near-optimal performance, and convex regions at higher RTTs indicate bottlenecks due to factors such as buffer or credit limits. We present a machine learning method that explicitly infers these concave and convex regions and transitions between them using sigmoid functions. We also provide distribution-free confidence estimates for the generalization error of these concave-convex profile estimates. Throughput profiles for data transfers over 10&nbsp;Gbps connections with 0–366&nbsp;ms RTT provide important performance insights, including the near optimality of transfers performed with the XDD tool between XFS filesystems, and the performance limits of wide-area Lustre extensions using LNet routers. A direct application of generic machine learning packages does not adequately highlight these critical performance regions or provide as precise confidence estimates.},
booktitle = {Machine Learning for Networking: First International Conference, MLN 2018, Paris, France, November 27–29, 2018, Revised Selected Papers},
pages = {1–22},
numpages = {22},
keywords = {Data transport, Throughput profile, Concavity-convexity, Generalization bounds},
location = {Paris, France}
}

@inproceedings{10.1145/3458817.3476210,
author = {Bhatia, Harsh and Di Natale, Francesco and Moon, Joseph Y. and Zhang, Xiaohua and Chavez, Joseph R. and Aydin, Fikret and Stanley, Chris and Oppelstrup, Tomas and Neale, Chris and Schumacher, Sara Kokkila and Ahn, Dong H. and Herbein, Stephen and Carpenter, Timothy S. and Gnanakaran, Sandrasegaram and Bremer, Peer-Timo and Glosli, James N. and Lightstone, Felice C. and Ing\'{o}lfsson, Helgi I.},
title = {Generalizable coordination of large multiscale workflows: challenges and learnings at scale},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476210},
doi = {10.1145/3458817.3476210},
abstract = {The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98% GPU occupancy for more than 83% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {10},
numpages = {16},
keywords = {adaptive simulations, cancer research, heterogenous architecture, machine learning, massively parallel, multiscale simulations},
location = {St. Louis, Missouri},
series = {SC '21}
}

@article{10.1145/3374916,
author = {Luporini, Fabio and Louboutin, Mathias and Lange, Michael and Kukreja, Navjot and Witte, Philipp and H\"{u}ckelheim, Jan and Yount, Charles and Kelly, Paul H. J. and Herrmann, Felix J. and Gorman, Gerard J.},
title = {Architecture and Performance of Devito, a System for Automated Stencil Computation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3374916},
doi = {10.1145/3374916},
abstract = {Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process—from mathematical equations down to C++ code—is performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling, and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the backend of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications.},
journal = {ACM Trans. Math. Softw.},
month = apr,
articleno = {6},
numpages = {28},
keywords = {Finite-difference method, compiler, domain-specific language, performance optimization, stencil, structured grid, symbolic processing}
}

@inproceedings{10.1145/3419604.3419775,
author = {Robert, Sophie and Zertal, Soraya and Goret, Ga\"{e}l},
title = {SHAMan: an intelligent framework for HPC auto-tuning of I/O accelerators},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419775},
doi = {10.1145/3419604.3419775},
abstract = {Like most modern computer systems, High Performance Computing (HPC) machines integrate many highly configurable hardware devices and software components. Finding their optimal parametrization is a complex task, as the size of the parametric space and the non-linear behavior of HPC systems make hand tuning, theoretical modeling or exhaustive sampling unsuitable for most cases. Auto-tuning methods relying on black-box optimization have emerged as a promising solution for finding systems' best parametrization without making any assumption on their behaviors. In this paper, we present the architecture of an auto-tuning framework, called Smart HPC Application MANager (SHAMan), that integrates black-box optimization heuristics to find the optimal parametrization of an Input/Output (I/O) accelerator for a HPC application. We describe the conceptual and technical architecture of the framework and its native support for HPC clusters' ecosystem. We detail in depth the stand-alone optimization engine and its integration as a service provided by a Web application. We deployed and tested the framework by tuning an I/O accelerator developed by the Atos company on a HPC cluster running in production. The tuner's performance is evaluated by optimizing 90 different I/O oriented applications. We show a median improvement of 29% in speed-up compared to the default parametrization and this improvement goes up to 98% for a certain class of applications.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {31},
numpages = {6},
keywords = {Auto-tuning, High Performance Computing, I/O accelerators, Input/Output, Performance optimization, Randomized search heuristics},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.5555/3524938.3525278,
author = {Gopi, Sivakanth and Gulhane, Pankaj and Kulkarni, Janardhan and Shen, Judy Hanwen and Shokouhi, Milad and Yekhanin, Sergey},
title = {Differentially private set union},
year = {2020},
publisher = {JMLR.org},
abstract = {We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe U of items, possibly of infinite size, and a database D of users. Each user i contributes a subset Wi ⊆ U of items. We want an (ε,δ)-differentially private Algorithm which outputs a subset S ⊂ UiWi such that the size of S is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, n-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {340},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {active learning, adaptive instance selection, benchmarking, fractional factorial design, non-functional properties},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@article{10.3233/JIFS-200933,
author = {Guo, Feiyan and Tang, Bing and Zhang, Jiaming},
title = {Mobile edge server placement based on meta-heuristic algorithm},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200933},
doi = {10.3233/JIFS-200933},
abstract = {The rapid development of the Internet of Things and 5G networks have generated a large amount of data. By offloading computing tasks from mobile devices to edge servers with sufficient computing resources, network congestion and data transmission delays can be effectively reduced. The placement of edge server is the core of task offloading and is a multi-objective optimization problem with multiple resource constraints. Efficient placement approach can effectively meet the needs of mobile users to access services with low latency and high bandwidth. To this end, an optimization model of edge server placement has been established in this paper through minimizing both communication delay and load difference as the optimization goal. Then, an Edge Server placement based on meta-Heuristic alGorithM (ESH-GM) has been proposed to achieve multi-objective optimization. Firstly, the K-means algorithm is combined with the ant colony algorithm, and the pheromone feedback mechanism is introduced into the placement of edge servers by emulating the mechanism of ant colony sharing pheromone in the foraging process, and the ant colony algorithm is improved by setting the taboo table to improve the convergence speed of the algorithm. Then, the improved heuristic algorithm is used to solve the optimal placement of edge servers. Experimental results using Shanghai Telecom’s real datasets show that the proposed ESH-GM achieves an optimal balance between low latency and load balancing, while guaranteeing quality of service, which outperforms several existing representative approaches.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {8883–8897},
numpages = {15},
keywords = {Mobile edge computing, server placement, heuristic algorithm, performance optimization}
}

@mastersthesis{10.5555/AAI28028519,
author = {Obioma, Blessing and Papavasiliou, Georgia and Vaicik, Marcella and Rashid, Mudassir},
advisor = {Keigo, Kawaji,},
title = {Impact of Data Shape, Fidelity, and Inter-Observer Reproducibility on Cardiac Magnetic Resonance Image Pipelines},
year = {2020},
isbn = {9798664777208},
publisher = {Illinois Institute of Technology},
address = {USA},
abstract = {Artificial Intelligence (AI) holds a great promise in the healthcare. It provides a variety of advantages with its application in clinical diagnosis, disease prediction, and treatment, with such interests intensifying in the medical image field. AI can automate various cumbersome data processing techniques in medical imaging such as segmentation of left ventricular chambers and image-based classification of diseases. However, full clinical implementation and adaptation of emerging AI-based tools face challenges due to the inherently opaque nature of such AI algorithms based on Deep Neural Networks (DNN), for which computer-trained bias is not only difficult to detect by physician users but is also difficult to safely design in software development.In this work, we examine AI application in Cardiac Magnetic Resonance (CMR) using an automated image classification task, and thereby propose an AI quality control framework design that differentially evaluates the black-box DNN via carefully prepared input data with shape and fidelity variations to probe system responses to these variations. Two variants of the Visual Geometric Graphics with 19 neural layers (VGG19) was used for classification, with a total of 60,000 CMR images. Findings from this work provides insights on the importance of quality training data preparation and demonstrates the importance of data shape variability. It also provides gateway for computation performance optimization in training and validation time.},
note = {AAI28028519}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Curriculum learning, Self-paced learning, Learning with privileged information}
}

@inproceedings{10.1145/3078971.3079003,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079003},
doi = {10.1145/3078971.3079003},
abstract = {Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {32–40},
numpages = {9},
keywords = {big data, concept detection, noisy data, prior knowledge, video understanding, web label, webly-supervised learning},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@inproceedings{10.1007/978-3-030-44689-5_13,
author = {Chatterjee, Swagato and Dutta, Rwik Kumar and Ganguly, Debayan and Chatterjee, Kingshuk and Roy, Sudipta},
title = {Bengali Handwritten Character Classification Using Transfer Learning on Deep Convolutional Network},
year = {2019},
isbn = {978-3-030-44688-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-44689-5_13},
doi = {10.1007/978-3-030-44689-5_13},
abstract = {Bengali is the sixth most popular spoken language in the world. Computerized detection of handwritten Bengali (Bangla Lekha) character is very difficult due to the diversity and veracity of characters. In this paper, we have proposed a modified state-of-the-art deep learning to tackle the problem of Bengali handwritten character recognition. This method used the lesser number of iterations to train than other comparable methods. The transfer learning on Resnet-50 deep convolutional neural network model is used on pretrained ImageNet dataset. One cycle policy is modified with varying the input image sizes to ensure faster training. Proposed method executed on BanglaLekha-Isolated dataset for evaluation that consists of 84 classes (50 Basic, 10 Numerals and 24 Compound Characters). We have achieved 97.12% accuracy in just 47 epochs. Proposed method gives very good results in terms of epoch and accuracy compare to other recent methods by considering number of classes. Without ensembling, proposed solution achieves state-of-the-art result and shows the effectiveness of ResNet-50 for classification of Bangla HCR.},
booktitle = {Intelligent Human Computer Interaction: 11th International Conference, IHCI 2019, Allahabad, India, December 12–14, 2019, Proceedings},
pages = {138–148},
numpages = {11},
keywords = {Bengali character, Classification, Transfer learning, Deep learning, Convolutional network},
location = {Allahabad, India}
}

@article{10.1016/j.patcog.2019.107173,
author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
title = {Unsupervised domain adaptive re-identification: Theory and practice},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107173},
doi = {10.1016/j.patcog.2019.107173},
journal = {Pattern Recogn.},
month = jun,
numpages = {11},
keywords = {Person re-identification, Unsupervised domain adaptation}
}

@inproceedings{10.1145/3268866.3268889,
author = {Robinson, Carl Peter and Li, Baihua and Meng, Qinggang and Pain, Matthew},
title = {Effectiveness of Surface Electromyography in Pattern Classification for Upper Limb Amputees},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268889},
doi = {10.1145/3268866.3268889},
abstract = {This study was undertaken to explore 18 time domain (TD) and time-frequency domain (TFD) feature configurations to determine the most discriminative feature sets for classification. Features were extracted from the surface electromyography (sEMG) signal of 17 hand and wrist movements and used to perform a series of classification trials with the random forest classifier. Movement datasets for 11 intact subjects and 9 amputees from the NinaPro online database repository were used. The aim was to identify any optimum configurations that combined features from both domains and whether there was consistency across subject type for any standout features. This work built on our previous research to incorporate the TFD, using a Discrete Wavelet Transform with a Daubechies wavelet. Findings report configurations containing the same features combined from both domains perform best across subject type (TD: root mean square (RMS), waveform length, and slope sign changes; TFD: RMS, standard deviation, and energy). These mixed-domain configurations can yield optimal performance (intact subjects: 90.98%; amputee subjects: 75.16%), but with only limited improvement on single-domain configurations. This suggests there is limited scope in attempting to build a single absolute feature configuration and more focus should be put on enhancing the classification methodology for adaptivity and robustness under actual operating conditions.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {107–112},
numpages = {6},
keywords = {Classification, Feature Extraction, Machine Learning, Myoelectric Control, Surface Electromyography},
location = {Beijing, China},
series = {AIPR '18}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Cl\'{e}ment and Mann, Zolt\'{a}n \'{A}d\'{a}m and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Adaptation, Reinforcement learning, Feature model, Cloud service},
location = {Dubai, United Arab Emirates}
}

@inproceedings{10.1145/3431920.3439301,
author = {Choi, Young-kyu and Chi, Yuze and Qiao, Weikang and Samardzic, Nikola and Cong, Jason},
title = {HBM Connect: High-Performance HLS Interconnect for FPGA HBM},
year = {2021},
isbn = {9781450382182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3431920.3439301},
doi = {10.1145/3431920.3439301},
abstract = {With the recent release of High Bandwidth Memory (HBM) based FPGA boards, developers can now exploit unprecedented external memory bandwidth. This allows more memory-bounded applications to benefit from FPGA acceleration. However, fully utilizing the available bandwidth may not be an easy task. If an application requires multiple processing elements to access multiple HBM channels, we observed a significant drop in the effective bandwidth. The existing high-level synthesis (HLS) programming environment had limitation in producing an efficient communication architecture. In order to solve this problem, we propose HBM Connect, a high-performance customized interconnect for FPGA HBM board. Novel HLS-based optimization techniques are introduced to increase the throughput of AXI bus masters and switching elements. We also present a high-performance customized crossbar that may replace the built-in crossbar. The effectiveness of HBM Connect is demonstrated using Xilinx's Alveo U280 HBM board. Based on bucket sort and merge sort case studies, we explore several design spaces and find the design point with the best resource-performance trade-off. The result shows that HBM Connect improves the resource-performance metrics by 6.5X-211X.},
booktitle = {The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {116–126},
numpages = {11},
keywords = {field-programmable gate array, high bandwidth memory, high-level synthesis, on-chip network, performance optimization},
location = {Virtual Event, USA},
series = {FPGA '21}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Data stream classification, Ensemble learning, Recurring concept drift, Unlabeled data}
}

@article{10.1016/j.ins.2019.12.015,
author = {Xiao, Yanshan and Yang, Xiaozhou and Liu, Bo},
title = {A new self-paced method for multiple instance boosting learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {515},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.015},
doi = {10.1016/j.ins.2019.12.015},
journal = {Inf. Sci.},
month = apr,
pages = {80–90},
numpages = {11},
keywords = {Multiple instance learning, Multiple instance boost learning, Self-Paced learning}
}

@article{10.1016/j.eswa.2021.115218,
author = {Serrano-P\'{e}rez, Jonathan and Sucar, L. Enrique},
title = {Artificial datasets for hierarchical classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115218},
doi = {10.1016/j.eswa.2021.115218},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Artificial datasets, Hierarchical classification, Evaluation}
}

@inproceedings{10.1007/978-3-030-73197-7_29,
author = {Du, Yuntao and Chen, Yinghao and Cui, Fengli and Zhang, Xiaowen and Wang, Chongjun},
title = {Cross-Domain Error Minimization for Unsupervised Domain Adaptation},
year = {2021},
isbn = {978-3-030-73196-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73197-7_29},
doi = {10.1007/978-3-030-73197-7_29},
abstract = {Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Previous methods focus on learning domain-invariant features to decrease the discrepancy between the feature distributions as well as minimizing the source error and have made remarkable progress. However, a recently proposed theory reveals that such a strategy is not sufficient for a successful domain adaptation. It shows that besides a small source error, both the discrepancy between the feature distributions and the discrepancy between the labeling functions should be small across domains. The discrepancy between the labeling functions is essentially the cross-domain errors which are ignored by existing methods. To overcome this issue, in this paper, a novel method is proposed to integrate all the objectives into a unified optimization framework. Moreover, the incorrect pseudo labels widely used in previous methods can lead to error accumulation during learning. To alleviate this problem, the pseudo labels are obtained by utilizing structural information of the target domain besides source classifier and we propose a curriculum learning based strategy to select the target samples with more accurate pseudo-labels during training. Comprehensive experiments are conducted, and the results validate that our approach outperforms state-of-the-art methods.},
booktitle = {Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II},
pages = {429–448},
numpages = {20},
keywords = {Transfer learning, Domain adaptation, Cross-domain errors},
location = {Taipei, Taiwan}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@article{10.1145/3456630,
author = {Oikonomou, Panagiotis and Karanika, Anna and Anagnostopoulos, Christos and Kolomvatsos, Kostas},
title = {On the Use of Intelligent Models towards Meeting the Challenges of the Edge Mesh},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3456630},
doi = {10.1145/3456630},
abstract = {Nowadays, we are witnessing the advent of the Internet of Things (IoT) with numerous devices performing interactions between them or with their environment. The huge number of devices leads to huge volumes of data that demand the appropriate processing. The “legacy” approach is to rely on Cloud where increased computational resources can realize any desired processing. However, the need for supporting real-time applications requires a reduced latency in the provision of outcomes. Edge Computing (EC) comes as the “solver” of the latency problem. Various processing activities can be performed at EC nodes having direct connection with IoT devices. A number of challenges should be met before we conclude a fully automated ecosystem where nodes can cooperate or understand their status to efficiently serve applications. In this article, we perform a survey of the relevant research activities towards the vision of Edge Mesh (EM), i.e., a “cover” of intelligence upon the EC. We present the necessary hardware and discuss research outcomes in every aspect of EC/EM nodes functioning. We present technologies and theories adopted for data, tasks, and resource management while discussing how machine learning and optimization can be adopted in the domain.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {125},
numpages = {42},
keywords = {Data Management, Edge Computing, Edge Mesh, Internet of Things, Machine Learning, Resources Management, Tasks Management}
}

@article{10.1016/j.engappai.2013.06.002,
author = {Zvoianu, Alexandru-Ciprian and Bramerdorfer, Gerd and Lughofer, Edwin and Silber, Siegfried and Amrhein, Wolfgang and Peter Klement, Erich},
title = {Hybridization of multi-objective evolutionary algorithms and artificial neural networks for optimizing the performance of electrical drives},
year = {2013},
issue_date = {September, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {26},
number = {8},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2013.06.002},
doi = {10.1016/j.engappai.2013.06.002},
abstract = {Performance optimization of electrical drives implies a lot of degrees of freedom in the variation of design parameters, which in turn makes the process overly complex and sometimes impossible to handle for classical analytical optimization approaches. This, and the fact that multiple non-independent design parameter have to be optimized synchronously, makes a soft computing approach based on multi-objective evolutionary algorithms (MOEAs) a feasible alternative. In this paper, we describe the application of the well known Non-dominated Sorting Genetic Algorithm II (NSGA-II) in order to obtain high-quality Pareto-optimal solutions for three optimization scenarios. The nature of these scenarios requires the usage of fitness evaluation functions that rely on very time-intensive finite element (FE) simulations. The key and novel aspect of our optimization procedure is the on-the-fly automated creation of highly accurate and stable surrogate fitness functions based on artificial neural networks (ANNs). We employ these surrogate fitness functions in the middle and end parts of the NSGA-II run (-&gt;hybridization) in order to significantly reduce the very high computational effort required by the optimization process. The results show that by using this hybrid optimization procedure, the computation time of a single optimization run can be reduced by 46-72% while achieving Pareto-optimal solution sets with similar, or even slightly better, quality as those obtained when conducting NSGA-II runs that use FE simulations over the whole run-time of the optimization process.},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {1781–1794},
numpages = {14},
keywords = {Electrical drives, Feed-forward artificial neural networks, Hybridization, Multi-objective evolutionary algorithms, Performance optimization, Surrogate fitness evaluation}
}

@article{10.1016/j.future.2018.05.080,
author = {Cai, Lin and Qi, Yong and Wei, Wei and Wu, Jinsong and Li, Jingwei},
title = {mrMoulder: A recommendation-based adaptive parameter tuning approach for big data processing platform},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.05.080},
doi = {10.1016/j.future.2018.05.080},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {570–582},
numpages = {13},
keywords = {Big data processing, Performance optimization, Parameter tuning, Online configuration recommendation, Collaborative filtering}
}

@inproceedings{10.1007/978-3-030-23502-4_14,
author = {Sondur, Sanjeev and Kant, Krishna},
title = {Towards Automated Configuration of Cloud Storage Gateways: A Data Driven Approach},
year = {2019},
isbn = {978-3-030-23501-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23502-4_14},
doi = {10.1007/978-3-030-23502-4_14},
abstract = {Cloud storage gateways (CSGs) are an essential part of enterprises to take advantage of the scale and flexibility of cloud object store. A CSG provides clients the impression of a locally configured large size block-based storage device, which needs to be mapped to remote cloud storage which is invariably object based. Proper configuration of the cloud storage gateway is extremely challenging because of numerous parameters involved and interactions among them. In this paper, we study this problem for a commercial CSG product that is typical of offerings in the market. We explore how machine learning techniques can be exploited both for the forward problem (i.e. predicting performance from the configuration parameters) and backward problem (i.e. predicting configuration parameter values from the target performance). Based on extensive testing with real world customer workloads, we show that it is possible to achieve excellent prediction accuracy while ensuring that the model is not overfitted to the data.},
booktitle = {Cloud Computing – CLOUD 2019: 12th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings},
pages = {192–207},
numpages = {16},
keywords = {Cloud storage gateway, Object store, Performance, Configuration management, Machine learning},
location = {San Diego, CA, USA}
}

@article{10.1016/j.neucom.2019.03.062,
author = {Ren, Yazhou and Que, Xiaofan and Yao, Dezhong and Xu, Zenglin},
title = {Self-paced multi-task clustering},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {350},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.062},
doi = {10.1016/j.neucom.2019.03.062},
journal = {Neurocomput.},
month = jul,
pages = {212–220},
numpages = {9},
keywords = {Multi-task clustering, Self-paced learning, Non-convexity, Soft weighting}
}

@inproceedings{10.1145/3366750.3366762,
author = {Li, Yong and E, Fei},
title = {Intelligent Semi-Submersible Heavy Transport Vessel},
year = {2019},
isbn = {9781450372480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366750.3366762},
doi = {10.1145/3366750.3366762},
abstract = {As the major ocean transport tool of large intact cargo, the SSHTV (semi-submersible heavy transport vessel) has become more and more valued with the rapid advance of ocean engineering, polar exploration and global trade. Simultaneously, higher safety, reliability, efficiency and performance requirements for SSHTV are needed to guarantee owner and operator profit. An intelligent SSHTV concept is presented here, based on development and maturity of sensor technology, information technology, data technology and IoT (Internet of Things) technology, all aimed at satisfying these requirements. The concept combines intelligent navigation, onboard equipment and energy management, intelligent vessel and fleet operation. It embodies the integration of people, things and service, and leads the development trend.},
booktitle = {Proceedings of the 2019 2nd International Conference on Machine Learning and Machine Intelligence},
pages = {63–67},
numpages = {5},
keywords = {Marine, intelligent ship, remote diagnosis, semi-submersible heavy transport vessel, surrounding awareness},
location = {Jakarta, Indonesia},
series = {MLMI '19}
}

@article{10.1007/s00500-016-2184-0,
author = {Gu, Hui and Ren, Shaojun and Si, Fengqi and Xu, Zhigao},
title = {Evolved FCM framework for working condition classification in furnace system},
year = {2017},
issue_date = {November  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-016-2184-0},
doi = {10.1007/s00500-016-2184-0},
abstract = {In this paper, an evolved FCM-based clustering method combined with entropy theory is proposed to develop a working condition classification model for the furnace system in coal-fired power plants. To overcome the disadvantage in beforehand determination of clustering number in basic FCM method, Silhouette index is selected as a parameter to evaluate clustering number adaptively in the process. Each time the FCM runs, the selected Silhouette index evaluates the clustering results considering both close and separation degree. Six datasets from UCI machine learning repository are used to certify the effectiveness of the evolved FCM method. Furthermore, pressure sequences from a 300-MW boiler are then discussed as the industrial case study. Three kinds of entropy values, featured from pressure sequence in time---frequency domain, are obtained for further clustering analysis. The clustering results show the strong relationship between boiler's load and pressure sequences in furnace system. This method can be considered a reference method for data mining in other fluctuating and time-varying sequences.},
journal = {Soft Comput.},
month = nov,
pages = {6317–6329},
numpages = {13},
keywords = {Entropy, Evolved FCM, Pressure sequence, Silhouette index}
}

@article{10.1145/3309205,
author = {Behzad, Babak and Byna, Surendra and Prabhat and Snir, Marc},
title = {Optimizing I/O Performance of HPC Applications with Autotuning},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/3309205},
doi = {10.1145/3309205},
abstract = {Parallel Input output is an essential component of modern high-performance computing (HPC). Obtaining good I/O performance for a broad range of applications on diverse HPC platforms is a major challenge, in part, because of complex inter dependencies between I/O middleware and hardware. The parallel file system and I/O middleware layers all offer optimization parameters that can, in theory, result in better I/O performance. Unfortunately, the right combination of parameters is highly dependent on the application, HPC platform, problem size, and concurrency. Scientific application developers do not have the time or expertise to take on the substantial burden of identifying good parameters for each problem configuration. They resort to using system defaults, a choice that frequently results in poor I/O performance. We expect this problem to be compounded on exascale-class machines, which will likely have a deeper software stack with hierarchically arranged hardware resources.We present as a solution to this problem an autotuning system for optimizing I/O performance, I/O performance modeling, I/O tuning, and I/O patterns. We demonstrate the value of this framework across several HPC platforms and applications at scale.},
journal = {ACM Trans. Parallel Comput.},
month = mar,
articleno = {15},
numpages = {27},
keywords = {HPC, I/O, autotuning, parallel file systems, performance optimization, storage}
}

@article{10.1504/ijict.2021.111923,
author = {Hu, Haiyan and Su, Chang},
title = {Self-adaptive process optimisation method for SBS cloud application based on reinforcement learning},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {1},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2021.111923},
doi = {10.1504/ijict.2021.111923},
abstract = {To solve the problem of poor comprehensive performance of the traditional SBS cloud application adaptive process optimisation method, a new SBS cloud application adaptive process optimisation method based on reinforcement learning was proposed. Establish the adaptive action type selection model, realise the optimal choice of operation type, build the cloud application adaptive process optimisation model for resource cost, convert the problem into the corresponding mathematical model, solve and make the mathematical model of the algorithm, and obtain the best adaptive process optimisation scheme SBS cloud application. The simulation results show that the predicted load value of the method is the closest to the actual load value, the relative error of the prediction is less than 21.03, and the average time is less than 2.3 s, indicating that the method has good performance and high practical application value.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {105–124},
numpages = {19},
keywords = {enhanced learning, SBS cloud application, self-adaptive process optimisation, system performance optimisation}
}

@article{10.1145/3243316,
author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
title = {Unsupervised Person Re-identification: Clustering and Fine-tuning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3243316},
doi = {10.1145/3243316},
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {83},
numpages = {18},
keywords = {Large-scale person re-identification, clustering, convolutional neural network, unsupervised learning}
}

@article{10.1016/j.micpro.2021.103964,
author = {Gokilavani, N. and Bharathi, B.},
title = {Multi-Objective based test case selection and prioritization for distributed cloud environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103964},
doi = {10.1016/j.micpro.2021.103964},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {6},
keywords = {Cloud environment, Software testing, Similarity-based clustering, Test case prioritization, Test case selection, Particle swarm optimization, Software product line}
}

@inproceedings{10.5555/3060832.3060891,
author = {Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting},
title = {Self-paced boost learning for classification},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Effectiveness and robustness are two essential aspects of supervised learning studies. For effective learning, ensemble methods are developed to build a strong effective model from ensemble of weak models. For robust learning, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously enhancing the learning effectiveness and robustness, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a max-margin boosting optimization with self-paced sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1932–1938},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.1007/s10586-019-03012-1,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Ther\'{o}n, Roberto and Amo Filv\`{a}, Daniel and Fonseca Escudero, David},
title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-03012-1},
doi = {10.1007/s10586-019-03012-1},
abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm.},
journal = {Cluster Computing},
month = sep,
pages = {1803–1816},
numpages = {14},
keywords = {SPL, Domain engineering, Meta-model, Information dashboards, Feature model, Artificial intelligence, Automatic configuration}
}

@inproceedings{10.1145/3295500.3356217,
author = {Patki, Tapasya and Thiagarajan, Jayaraman J. and Ayala, Alexis and Islam, Tanzima Z.},
title = {Performance optimality or reproducibility: that is the question},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356217},
doi = {10.1145/3295500.3356217},
abstract = {The era of extremely heterogeneous supercomputing brings with itself the devil of increased performance variation and reduced reproducibility. There is a lack of understanding in the HPC community on how the simultaneous consideration of network traffic, power limits, concurrency tuning, and interference from other jobs impacts application performance.In this paper, we design a methodology that allows both HPC users and system administrators to understand the trade-off space between optimal and reproducible performance. We present a first-of-its-kind dataset that simultaneously varies multiple system- and user-level parameters on a production cluster, and introduce a new metric, called the desirability score, which enables comparison across different system configurations. We develop a novel, model-agnostic machine learning methodology based on the graph signal theory for comparing the influence of parameters on application predictability, and using a new visualization technique, make practical suggestions for best practices for multi-objective HPC environments.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {77},
numpages = {30},
keywords = {graph signal analysis, machine learning, performance reproducibility, visualization},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {adversarial machine learning, autonomous driving, sensor attack},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Business Process Model, Domain-specific rules, Model Driven Architecture, Rule Generation, Variability Model}
}

@inproceedings{10.1007/978-3-030-90888-1_8,
author = {Chai, Yanfeng and Ge, Jiake and Chai, Yunpeng and Wang, Xin and Zhao, BoXuan},
title = {XTuning: Expert Database Tuning System Based on Reinforcement Learning},
year = {2021},
isbn = {978-3-030-90887-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90888-1_8},
doi = {10.1007/978-3-030-90888-1_8},
abstract = {Database performance optimization has become a hot issue in recent years. Some works deeply reconstruct the database to achieve specified goals like throughput or latency. The others focus on the database’s configuration knobs with reinforcement learning (RL) to improve the performance without any empirical knowledge. But the exhaustive offline training process costs plenty of time and resources due to the large inefficient configuration knobs combinations with trial-and-error methods. The most time-consuming part of the process is not the RL network training, but the database performance evaluation for acquiring the reward values of target performance like throughput or latency. So we propose an expert database tuning system (XTuning) which contains a correlation knowledge model to remove unnecessary training costs and a multi-instance mechanism (MIM) to support fine-grained tuning for diverse workloads. The models define the importance and correlations among these configuration knobs for the user’s specified target. Then we implement the models as Progressive Expert Knowledge Tuning (PEKT) algorithm with an abstracted architectural optimization integrated into XTuning. Experiments show that XTuning can effectively reduce the training time and achieves extra performance promotion compared with the state-of-the-art tuning methods.},
booktitle = {Web Information Systems Engineering – WISE 2021: 22nd International Conference on Web Information Systems Engineering, WISE 2021, Melbourne, VIC, Australia, October 26–29, 2021, Proceedings, Part I},
pages = {101–110},
numpages = {10},
keywords = {Database optimization, Auto tuning, Expert knowledge rules, Reinforcement learning, Reduce training time},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/2907294.2907297,
author = {Tan, Wei and Cao, Liangliang and Fong, Liana},
title = {Faster and Cheaper: Parallelizing Large-Scale Matrix Factorization on GPUs},
year = {2016},
isbn = {9781450343145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2907294.2907297},
doi = {10.1145/2907294.2907297},
abstract = {Matrix factorization (MF) is used by many popular algorithms such as collaborative filtering. GPU with massive cores and high memory bandwidth sheds light on accelerating MF much further when appropriately exploiting its architectural characteristics.This paper presents cuMF, a CUDA-based matrix factorization library that optimizes alternate least square (ALS) method to solve very large-scale MF. CuMF uses a set of techniques to maximize the performance on single and multiple GPUs. These techniques include smart access of sparse data leveraging GPU memory hierarchy, using data parallelism in conjunction with model parallelism, minimizing the communication overhead among GPUs, and a novel topology-aware parallel reduction scheme.With only a single machine with four Nvidia GPU cards, cuMF can be 6-10 times as fast, and 33-100 times as cost-efficient, compared with the state-of-art distributed CPU solutions. Moreover, cuMF can solve the largest matrix factorization problem ever reported in current literature, with impressively good performance.},
booktitle = {Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing},
pages = {219–230},
numpages = {12},
keywords = {alternating least square (als), cuda, gpu, matrix factorization, parallel algorithms, performance optimization},
location = {Kyoto, Japan},
series = {HPDC '16}
}

@inproceedings{10.1145/3359789.3359805,
author = {Sanders, Matthew W and Yue, Chuan},
title = {Mining least privilege attribute based access control policies},
year = {2019},
isbn = {9781450376280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359789.3359805},
doi = {10.1145/3359789.3359805},
abstract = {Creating effective access control policies is a significant challenge to many organizations. Over-privilege increases security risk from compromised credentials, insider threats, and accidental misuse. Under-privilege prevents users from performing their duties. Policies must balance between these competing goals of minimizing under-privilege vs. over-privilege. The Attribute Based Access Control (ABAC) model has been gaining popularity in recent years because of its advantages in granularity, flexibility, and usability. ABAC allows administrators to create policies based on attributes of users, operations, resources, and the environment. However, in practice, it is often very difficult to create effective ABAC policies in terms of minimizing under-privilege and over-privilege especially for large and complex systems because their ABAC privilege spaces are typically gigantic. In this paper, we take a rule mining approach to mine systems' audit logs for automatically generating ABAC policies which minimize both under-privilege and over-privilege. We propose a rule mining algorithm for creating ABAC policies with rules, a policy scoring algorithm for evaluating ABAC policies from the least privilege perspective, and performance optimization methods for dealing with the challenges of large ABAC privilege spaces. Using a large dataset of 4.7 million Amazon Web Service (AWS) audit log events, we demonstrate that our automated approach can effectively generate least privilege ABAC policies, and can generate policies with less over-privilege and under-privilege than a Role Based Access Control (RBAC) approach. Overall, we hope our work can help promote a wider and faster deployment of the ABAC model, and can help unleash the advantages of ABAC to better protect large and complex computing systems.},
booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference},
pages = {404–416},
numpages = {13},
keywords = {ABAC, machine learning, principle of least privilege, rule mining},
location = {San Juan, Puerto Rico, USA},
series = {ACSAC '19}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/3097983.3098048,
author = {Tata, Sandeep and Popescul, Alexandrin and Najork, Marc and Colagrosso, Mike and Gibbons, Julian and Green, Alan and Mah, Alexandre and Smith, Michael and Garg, Divanshu and Meyer, Cayden and Kan, Reuben},
title = {Quick Access: Building a Smart Experience for Google Drive},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098048},
doi = {10.1145/3097983.3098048},
abstract = {Google Drive is a cloud storage and collaboration service used by hundreds of millions of users around the world. Quick Access is a new feature in Google Drive that surfaces the most relevant documents when a user visits the home screen. Our metrics show that users locate their documents in half the time with this feature compared to previous approaches. The development of Quick Access illustrates many general challenges and constraints associated with practical machine learning such as protecting user privacy, working with data services that are not designed with machine learning in mind, and evolving product definitions. We believe that the lessons learned from this experience will be useful to practitioners tackling a wide range of applied machine learning problems.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1643–1651},
numpages = {9},
keywords = {applied machine learning, neural networks, private data},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3371425.3371432,
author = {Song, Yoojeong and Lee, Jongwoo},
title = {Design of stock price prediction model with various configuration of input features},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371432},
doi = {10.1145/3371425.3371432},
abstract = {The interest rate of Korea has remained around 1.75 % annually for years. However, the interest rates of developed countries such as the United States of America are 2.25 ~ 2.5%. Korea is very low compared to this, so it is hard to save money on a deposit or installment saving. Therefore, many people want to use stock investment methods to gain high interest rates despite the high risk. Many people are predicting whether stock prices will rise or fall for investments on their subjective opinion. However, in the field of computer engineering, many people try to predict the stock price using artificial neural network, which has been proven to have good performance through many studies. The direction of stock forecasting research using artificial neural networks is very diverse such as model structure, composition of input feature, composition of target vector and so on. In this paper, we design three stock price prediction model with various input features that have specific characteristic. We hypothesized that, for effective stock price prediction through artificial neural networks, using implicit meaning data. We also questioned which of the implicit data would be most predictive. To prove it, we suggest three stock price prediction model. We implemented these three models and experimented to performance evaluation. Through this, we find out what kind of features would be effective for stock price prediction.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {3},
numpages = {5},
keywords = {artificial neural network, binary feature, input feature configuration, stock prediction, technical analysis},
location = {Sanya, China},
series = {AIIPCC '19}
}

@inproceedings{10.1145/3453688.3461532,
author = {Zhang, Jinshan and Jiao, Bo and Wang, Yunzhengmao and Zhu, Haozhe and Zhang, Lihua and Chen, Chixiao},
title = {ALPINE: An Agile Processing-in-Memory Macro Compilation Framework},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461532},
doi = {10.1145/3453688.3461532},
abstract = {Processing-in-Memory architectures and circuit designs are playing significant roles in the recent energy-efficient machine learning chips. This paper proposes a PIM macro compilation framework called ALPINE to speed up previously tedious and error-prone PIM design flow, paving the way towards open-source and process-portable PIM chips. Relying on an extensible PIM standard cell library, ALPINE can generate the corresponding topology according to the specification, and process placement and routing. The proposed PIM macro is compatible with different storage devices such as SRAM and RRAM, and can support various quantization bit-widths and dataflows. To verify the effectiveness, a 128\texttimes{}128 SRAM-based PIM macro instance is implemented, and the simulation results show that it can achieve an energy efficiency of 19.05TOPS/W under 65nm CMOS technology. The macro performance is not inferior to the state-of-the-art custom PIM designs.},
booktitle = {Proceedings of the 2021 Great Lakes Symposium on VLSI},
pages = {333–338},
numpages = {6},
keywords = {agile design, compilation, machine learning, processing-in-memory},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Mar\'{\i}n, Carlos Enrique Montenegro and Crespo, Rub\'{e}n Gonz\'{a}lez},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Environmental noise, Road traffic, Vehicle, Classification, Deep learning}
}

@inproceedings{10.1109/GLOBECOM46510.2021.9685627,
author = {Rivkin, Dmitriy and Meger, David and Wu, Di and Chen, Xi and Liu, Xue and Dudek, Gregory},
title = {Learning Assisted Identification of Scenarios Where Network Optimization Algorithms Under-Perform},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM46510.2021.9685627},
doi = {10.1109/GLOBECOM46510.2021.9685627},
abstract = {We present a generative adversarial method that uses deep learning to identify network load traffic conditions in which network optimization algorithms under-perform other known algorithms: the Deep Convolutional Failure Generator (DCFG). The spatial distribution of network load presents challenges for network operators for tasks such as load balancing, in which a network optimizer attempts to maintain high quality communication while at the same time abiding capacity constraints. Testing a network optimizer for all possible load distributions is challenging if not impossible. We propose a novel method that searches for load situations where a target network optimization method underperforms baseline, which are key test cases that can be used for future refinement and performance optimization. By modeling a realistic network simulator's quality assessments with a deep network and, in parallel, optimizing a load generation network, our method efficiently searches the high dimensional space of load patterns and reliably finds cases in which a target network optimization method under-performs a baseline by a significant margin.},
booktitle = {2021 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–7},
numpages = {7},
location = {Madrid, Spain}
}

@article{10.1145/605521.605524,
author = {Yu, Shengke and Winslett, Marianne and Lee, Jonghyun and Ma, Xiaosong},
title = {Automatic and portable performance modeling for parallel I/O: a machine-learning approach},
year = {2002},
issue_date = {December 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/605521.605524},
doi = {10.1145/605521.605524},
abstract = {A performance model for a parallel I/O system is essential for detailed performance analyses, automatic performance optimization of I/O request handling, and potential performance bottleneck identification. Yet how to build a portable performance model for parallel I/O system is an open problem. In this paper, we present a machine-learning approach to automatic performance modeling for parallel I/O systems. Our approach is based on the use of a platform-independent performance metamodel, which is a radial basis function neural network. Given training data, the metamodel generates a performance model automatically and efficiently for a parallel I/O system on a given platform. Experiments suggest that our goal of having the generated model provide accurate performance predictions is attainable, for the parallel I/O library that served as our experimental testbed on an IBM SP. This suggests that it is possible to model parallel I/O system performance automatically and portably, and perhaps to model a broader class of storage systems as well.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = dec,
pages = {3–5},
numpages = {3}
}

@inproceedings{10.1007/978-3-030-58577-8_17,
author = {Pan, Lili and Ai, Shijie and Ren, Yazhou and Xu, Zenglin},
title = {Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_17},
doi = {10.1007/978-3-030-58577-8_17},
abstract = {Deep discriminative models (e.g.&nbsp;deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model—self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {271–287},
numpages = {17},
keywords = {Underrepresented examples, Self-paced learning, Entropy, Deep regression forests},
location = {Glasgow, United Kingdom}
}

@article{10.1145/3024188,
author = {Filieri, Antonio and Maggio, Martina and Angelopoulos, Konstantinos and D’ippolito, Nicol\'{a}s and Gerostathopoulos, Ilias and Hempel, Andreas Berndt and Hoffmann, Henry and Jamshidi, Pooyan and Kalyvianaki, Evangelia and Klein, Cristian and Krikava, Filip and Misailovic, Sasa and Papadopoulos, Alessandro V. and Ray, Suprio and Sharifloo, Amir M. and Shevtsov, Stepan and Ujma, Mateusz and Vogel, Thomas},
title = {Control Strategies for Self-Adaptive Software Systems},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3024188},
doi = {10.1145/3024188},
abstract = {The pervasiveness and growing complexity of software systems are challenging software engineering to design systems that can adapt their behavior to withstand unpredictable, uncertain, and continuously changing execution environments. Control theoretical adaptation mechanisms have received growing interest from the software engineering community in the last few years for their mathematical grounding, allowing formal guarantees on the behavior of the controlled systems. However, most of these mechanisms are tailored to specific applications and can hardly be generalized into broadly applicable software design and development processes.This article discusses a reference control design process, from goal identification to the verification and validation of the controlled system. A taxonomy of the main control strategies is introduced, analyzing their applicability to software adaptation for both functional and nonfunctional goals. A brief extract on how to deal with uncertainty complements the discussion. Finally, the article highlights a set of open challenges, both for the software engineering and the control theory research communities.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = feb,
articleno = {24},
numpages = {31},
keywords = {Self-adaptive software, control theory, formal methods, non-functional properties}
}

@article{10.1016/j.knosys.2019.105424,
author = {Liu, Xiaoshuang and Luo, Senlin and Pan, Limin},
title = {Robust boosting via self-sampling},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105424},
doi = {10.1016/j.knosys.2019.105424},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Boosting, Loss function, Robustness, Self-sampling}
}

@article{10.1016/j.is.2017.12.005,
author = {Borkowski, Michael and Fdhila, Walid and Nardelli, Matteo and Rinderle-Ma, Stefanie and Schulte, Stefan},
title = {Event-based failure prediction in distributed business processes},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {81},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2017.12.005},
doi = {10.1016/j.is.2017.12.005},
journal = {Inf. Syst.},
month = mar,
pages = {220–235},
numpages = {16},
keywords = {Failure prediction, Event-based systems, Business process management, Machine learning}
}

@inproceedings{10.1007/978-3-030-61362-4_11,
author = {ter Beek, Maurice H. and Cleophas, Loek and Legay, Axel and Schaefer, Ina and Watson, Bruce W.},
title = {X-by-Construction: Correctness Meets Probability},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_11},
doi = {10.1007/978-3-030-61362-4_11},
abstract = {In recent years, researchers have started to investigate X-by-Construction (XbC) as a refinement approach to engineer systems that by-construction satisfy certain non-functional properties, beyond correctness as considered by the more traditional Correctness-by-Construction (CbC). In line with increasing attention for fault-tolerance and the use of machine-learning techniques in modern software systems, in which even correctness is hard to establish, this track brings together researchers and practitioners that are interested in XbC in particular in the setting of probabilistic properties.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {211–215},
numpages = {5},
location = {Rhodes, Greece}
}

@article{10.1007/s10664-019-09691-z,
author = {Li, Yangguang and Jiang, Zhen Ming},
title = {Assessing and optimizing the performance impact of the just-in-time configuration parameters - a case study on PyPy},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09691-z},
doi = {10.1007/s10664-019-09691-z},
abstract = {Many modern programming languages (e.g., Python, Java, and JavaScript) support just-in-time (JIT) compilation to speed up the execution of a software system. During runtime, the JIT compiler translates the frequently executed part of the system into efficient machine code, which can be executed much faster compared to the default interpreted mode. There are many JIT configuration parameters, which vary based on the programming languages and types of the jitting strategies (method vs. tracing-based). Although there are many existing works trying to improve various aspects of the jitting process, there are very few works which study the performance impact of the JIT configuration settings. In this paper, we performed an empirical study on the performance impact of the JIT configuration settings of PyPy. PyPy is a popular implementation of the Python programming language. Due to PyPy's efficient JIT compiler, running Python programs under PyPy is usually much faster than other alternative implementations of Python (e.g., cPython, Jython, and IronPython). To motivate the need for tuning PyPy's JIT configuration settings, we first performed an exploratory study on two microbenchmark suites. Our findings show that systems executed under PyPy's default JIT configuration setting may not yield the best performance. Optimal JIT configuration settings vary from systems to systems. Larger portions of the code being jitted do not necessarily lead to better performance. To cope with these findings, we developed an automated approach, ESM-MOGA, to tuning the JIT configuration settings. ESM-MOGA, which stands for effect-size measure-based multi-objective genetic algorithm, automatically explores the PyPy's JIT configuration settings for optimal solutions. Case studies on three open source systems show that systems running under the resulting configuration settings significantly out-perform (5% - 60% improvement in average peak performance) the default configuration settings.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2323–2363},
numpages = {41},
keywords = {Just-in-time compilation, Performance analysis, Performance optimization, Performance testing, Software configuration}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Image classification, Curriculum learning, Self-paced learning, Multi-modal}
}

@inproceedings{10.1145/2939672.2939724,
author = {Geyik, Sahin Cem and Faleev, Sergey and Shen, Jianqiang and O'Donnell, Sean and Kolay, Santanu},
title = {Joint Optimization of Multiple Performance Metrics in Online Video Advertising},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939724},
doi = {10.1145/2939672.2939724},
abstract = {The field of online advertising, in essence, deals with the problem of presenting ads to online users in the most appropriate contexts to achieve a multitude of advertiser goals. A vast amount of work in online advertising has been focused on optimizing banner display advertising campaigns where the main goal lies in direct response metrics, often as clicks or conversions. In this paper, we explore the newly popularized space of online video advertising, where brand recognition is the key focus. We propose a framework based on a feedback mechanism where we optimize multiple video specific performance indicators while making sure the delivery constraints (budget and user reach) of advertisers are satisfied. While our main focus is on improving metrics such as engagement (amount of view time), and viewability (whether a campaign is within eyesight of a user), we also discuss the possibilities of expanding to other metrics. We demonstrate the benefit of our framework via empirical results in multiple real-world advertising campaigns. To the best of our knowledge, this is the first paper that deals with the unique challenges arising from the nature of online video advertising.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {471–480},
numpages = {10},
keywords = {online advertising, performance optimization, video advertising},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Evolutionary algorithm, Metaheuristics, Search based software engineering, Software product line, Systematic mapping study}
}

@article{10.1109/TCBB.2015.2476790,
author = {Deng, Su-Ping and Zhu, Lin and Huang, De-Shuang},
title = {Predicting hub genes associated with cervical cancer through gene co-expression networks},
year = {2016},
issue_date = {January/February 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2476790},
doi = {10.1109/TCBB.2015.2476790},
abstract = {Cervical cancer is the third most common malignancy in women worldwide. It remains a leading cause of cancer-related death for women in developing countries. In order to contribute to the treatment of the cervical cancer, in our work, we try to find a few key genes resulting in the cervical cancer. Employing functions of several bioinformatics tools, we selected 143 differentially expressed genes (DEGs) associated with the cervical cancer. The results of bioinformatics analysis show that these DEGs play important roles in the development of cervical cancer. Through comparing two differential co-expression networks (DCNs) at two different states, we found a common sub-network and two differential sub-networks as well as some hub genes in three sub-networks. Moreover, some of the hub genes have been reported to be related to the cervical cancer. Those hub genes were analyzed from Gene Ontology function enrichment, pathway enrichment and protein binding three aspects. The results can help us understand the development of the cervical cancer and guide further experiments about the cervical cancer.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {27–35},
numpages = {9},
keywords = {cervical cancer, co-expression network, differentially expressed genes, hub genes}
}

@article{10.3233/THC-218026,
author = {Zhou, Zhiming and Huang, Haihui and Liang, Yong},
title = {Cancer classification and biomarker selection via a penalized logsum network-based logistic regression model},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {S1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-218026},
doi = {10.3233/THC-218026},
journal = {Technol. Health Care},
month = jan,
pages = {287–295},
numpages = {9},
keywords = {Regularization, gene selection, log-sum penalty, network-based knowledge}
}

@inproceedings{10.1109/ISCA45697.2020.00045,
author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
title = {MLPerf inference benchmark},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00045},
doi = {10.1109/ISCA45697.2020.00045},
abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {446–459},
numpages = {14},
keywords = {benchmarking, inference, machine learning},
location = {Virtual Event},
series = {ISCA '20}
}

@article{10.1007/s11219-017-9370-x,
author = {Gerndt, Michael and Benkner, Siegfried and C\'{e}sar, Eduardo and Navarrete, Carmen and Bajrovic, Enes and Dokulil, Jiri and Guill\'{e}n, Carla and Mijakovic, Robert and Sikora, Anna},
title = {A multi-aspect online tuning framework for HPC applications},
year = {2018},
issue_date = {September 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9370-x},
doi = {10.1007/s11219-017-9370-x},
abstract = {Developing software applications for high-performance computing (HPC) requires careful optimizations targeting a myriad of increasingly complex, highly interrelated software, hardware and system components. The demands placed on minimizing energy consumption on extreme-scale HPC systems and the associated shift towards hete rogeneous architectures add yet another level of complexity to program development and optimization. As a result, the software optimization process is often seen as daunting, cumbersome and time-consuming by software developers wishing to fully exploit HPC resources. To address these challenges, we have developed the Periscope Tuning Framework (PTF), an online automatic integrated tuning framework that combines both performance analysis and performance tuning with respect to the myriad of tuning parameters available to today's software developer on modern HPC systems. This work introduces the architecture, tuning model and main infrastructure components of PTF as well as the main tuning plugins of PTF and their evaluation.},
journal = {Software Quality Journal},
month = sep,
pages = {1063–1096},
numpages = {34},
keywords = {Automatic performance tuning, Energy tuning, High-performance computing, OpenCL, Parallel architectures, Performance optimization}
}

@inbook{10.5555/3454287.3454459,
author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
title = {Meta-weight-net: learning an explicit mapping for sample weighting},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {172},
numpages = {12}
}

@inproceedings{10.1145/3077981.3078031,
author = {Robinson, Carl Peter and Li, Baihua and Meng, Qinggang and Pain, Matthew T.G.},
title = {Pattern Classification of Hand Movements using Time Domain Features of Electromyography},
year = {2017},
isbn = {9781450352093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077981.3078031},
doi = {10.1145/3077981.3078031},
abstract = {Myoelectric control of prostheses is a long-established technique, using surface electromyography (sEMG) to detect the electrical signals of muscle activity and perform subsequent mechanical actions. Despite several decades' research, robust, responsive and intuitive control schemes remain elusive. Current commercial hardware advances offer a variety of movements but the control systems are unnatural, using sequential switching methods triggered by specific sEMG signals. However, recent research with pattern recognition and simultaneous and proportional control shows good promise for natural myoelectric control. This paper investigates several sEMG time domain features using a series of hand movements performed by 11 subjects, taken from a benchmark database, to determine if optimal classification accuracy is dependent on feature set size. The features were extracted from the data using a sliding window process and applied to five machine learning classifiers, of which Random Forest consistently performed best. Results suggest a few simple features such as Root Mean Square and Waveform Length achieve comparable performance to using the entire feature set, when identifying the hand movements, although further work is required for feature optimisation.},
booktitle = {Proceedings of the 4th International Conference on Movement Computing},
articleno = {27},
numpages = {6},
keywords = {Electromyography, Machine learning, Myoelectric control, Time domain features},
location = {London, United Kingdom},
series = {MOCO '17}
}

@article{10.1613/jair.1.11688,
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11688},
doi = {10.1613/jair.1.11688},
abstract = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1183–1317},
numpages = {135},
keywords = {natural language, machine learning, computer vision, deep learning}
}

@article{10.1016/j.engappai.2019.08.015,
author = {Tavasoli, Hanane and Oommen, B. John and Yazidi, Anis},
title = {On utilizing weak estimators to achieve the online classification of data streams},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.08.015},
doi = {10.1016/j.engappai.2019.08.015},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {11–31},
numpages = {21},
keywords = {Weak estimators, Learning automata, Non-stationary environments, Classification in data streams}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {adaptation, dynamic software product lines, evolution, machine learning},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/3462244.3479890,
author = {Das, Kapotaksha and Sharak, Salem and Riani, Kais and Abouelenien, Mohamed and Burzo, Mihai and Papakostas, Michalis},
title = {Multimodal Detection of Drivers Drowsiness and Distraction},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479890},
doi = {10.1145/3462244.3479890},
abstract = {Considering the ever-growing presence of automobiles around the world, ensuring the safety of those on and near roadways is of great importance. From the causes of accidents, drowsiness and distractedness are among the most consequential. In this paper, we use a multimodal dataset consisting of 11 recorded channels over 45 subjects to model driver’s drowsiness and distraction. Our work puts forward the application of this dataset by using segmented windows as features, resulting in four main contributions. We explore the performance of each individual modality and specify which signals and features have a better capability of detecting drowsiness and different kinds of distractions. In addition, we analyze the effects of early fusion on the classification of the driver’s state using multiple physiological and thermal channels. Finally, we use cascaded late fusion and test three voting strategies to evaluate the performance of our proposed approach. Our results confirm the effectiveness of utilizing a multimodal approach in detecting both drowsiness and distraction as two separate factors influencing the driver and provide guidelines on which signals are appropriate for detecting different driver’s states.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {416–424},
numpages = {9},
keywords = {action units, classification, driver alertness, machine learning, multimodal dataset, thermal imaging},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Variability, Configuration workflow, Process mining, Process discovery, Clustering}
}

@inproceedings{10.1145/2829966.2829967,
author = {Danieli, Morena and Riccardi, Giuseppe and Alam, Firoj},
title = {Emotion Unfolding and Affective Scenes: A Case Study in Spoken Conversations},
year = {2015},
isbn = {9781450339889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2829966.2829967},
doi = {10.1145/2829966.2829967},
abstract = {The manifestation of human emotions evolves over time and space. Most of the work on affective computing research is limited to the association of context-free signal segments, such as utterances and images, to basic emotions. In this paper, we discuss the hypothesis that interpreting emotions requires a conceptual description of their dynamics within the context of their manifestations. We describe the unfolding of emotions through the proposed affective scene framework. Affective scenes are defined in terms of who first expresses the variation in their emotional state in a conversation, how this affects the other speaker's emotional appraisal and response, and which modifications occur from the initial through the final state of the scene. This conceptual framework is applied and evaluated on real human-human conversations drawn from call centers. We show that the automatic classification of affective scenes achieves more than satisfactory results and it benefits from acoustic, lexical and psycholinguistic features of the speech and linguistics signals.},
booktitle = {Proceedings of the International Workshop on Emotion Representations and Modelling for Companion Technologies},
pages = {5–11},
numpages = {7},
keywords = {affective scene, computational paralinguistics, emotion, machine learning, spoken conversation},
location = {Seattle, Washington, USA},
series = {ERM4CT '15}
}

@inproceedings{10.1145/3456727.3463830,
author = {Meth, Kalman and Kumara, Indika and Quattrocchi, Giovanni},
title = {Intelligent re-deployment feedback loop for hybrid applications},
year = {2021},
isbn = {9781450383981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456727.3463830},
doi = {10.1145/3456727.3463830},
abstract = {We propose enabling continuous performance optimisation of distributed hybrid applications in heterogeneous cloud, Edge, and HPC environments by employing an intelligent re-deployment feedback loop.},
booktitle = {Proceedings of the 14th ACM International Conference on Systems and Storage},
articleno = {24},
numpages = {1},
keywords = {application optimisation, distributed computing, monitoring},
location = {Haifa, Israel},
series = {SYSTOR '21}
}

@article{10.1016/j.sysarc.2021.102116,
author = {Sachan, Akash and Ghoshal, Bibhas},
title = {Learning based compilation of embedded applications targeting minimal energy consumption▪},
year = {2021},
issue_date = {Jun 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2021.102116},
doi = {10.1016/j.sysarc.2021.102116},
journal = {J. Syst. Archit.},
month = jun,
numpages = {13},
keywords = {Embedded system, Compiler, Minimal power dissipation, Machine learning, Multi-core processor}
}

@inproceedings{10.1007/978-3-030-37334-4_16,
author = {Malykh, Valentin and Samarin, Aleksei},
title = {Combined Advertising Sign Classifier},
year = {2019},
isbn = {978-3-030-37333-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-37334-4_16},
doi = {10.1007/978-3-030-37334-4_16},
abstract = {The article describes the problem of classifying photographs of advertising signs of commercial establishments according to the type of services provided. The proposed solution is based on the sharing of textual and visual features. We provide a composite model that includes a text recognition module and an extractor of visual characteristics to improve classification accuracy. We achieve  of 0.24 exceeding strong baseline quality for 10%.},
booktitle = {Analysis of Images, Social Networks and Texts: 8th International Conference, AIST 2019, Kazan, Russia, July 17–19, 2019, Revised Selected Papers},
pages = {179–185},
numpages = {7},
keywords = {Sharing of textual and visual features, Composite model, Optical character recognition, Visual characteristics},
location = {Kazan, Russia}
}

@inproceedings{10.1109/ICASSP.2018.8462638,
author = {Vempaty, Aditya and Kailkhura, Bhavya and Varshney, Pramod K.},
title = {Human-Machine Inference Networks for Smart Decision Making: Opportunities and Challenges},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2018.8462638},
doi = {10.1109/ICASSP.2018.8462638},
abstract = {The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines complementary cognitive strengths of humans and machines in an intelligent manner to tackle various inference tasks and achieves higher performance than either humans or machines by themselves. While inference performance optimization techniques for human-only or sensor-only networks are quite mature, HuMaINs require novel signal processing and machine learning solutions. In this paper, we present an overview of the HuMaINs architecture with a focus on three main issues that include architecture design, inference algorithms including security/privacy challenges, and application areas/use cases.},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {6961–6965},
numpages = {5},
location = {Calgary, AB, Canada}
}

@article{10.1016/j.cmpb.2019.05.022,
author = {P\'{e}rez-Benito, Francisco Javier and Signol, Francois and P\'{e}rez-Cort\'{e}s, Juan-Carlos and Poll\'{a}n, Marina and P\'{e}rez-G\'{o}mez, Beatriz and Salas-Trejo, Dolores and Casals, Mar\'{\i}a and Mart\'{\i}nez, Inmaculada and LLobet, Rafael},
title = {Global parenchymal texture features based on histograms of oriented gradients improve cancer development risk estimation from healthy breasts},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {177},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2019.05.022},
doi = {10.1016/j.cmpb.2019.05.022},
journal = {Comput. Methods Prog. Biomed.},
month = aug,
pages = {123–132},
numpages = {10},
keywords = {Breast density, Texture features, Cancer development risk, Breast cancer, AUC, BI-RADS, BMI, CC, DBT, FGT, G-HOGH, GLCM, HOG, LBP, MD, MLO, MTR, NRI, PD, RF, ROC}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {benchmark, feature location, reverse engineering, software product lines, variability mining},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2802658.2802667,
author = {Gallardo, Esthela and Vienne, Jerome and Fialho, Leonardo and Teller, Patricia and Browne, James},
title = {MPI Advisor: a Minimal Overhead Tool for MPI Library Performance Tuning},
year = {2015},
isbn = {9781450337953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2802658.2802667},
doi = {10.1145/2802658.2802667},
abstract = {A majority of parallel applications executed on HPC clusters use MPI for communication between processes. Most users treat MPI as a black box, executing their programs using the cluster's default settings. While the default settings perform adequately for many cases, it is well known that optimizing the MPI environment can significantly improve application performance. Although the existing optimization tools are effective when used by performance experts, they require deep knowledge of MPI library behavior and the underlying hardware architecture in which the application will be executed. Therefore, an easy-to-use tool that provides recommendations for configuring the MPI environment to optimize application performance is highly desirable. This paper addresses this need by presenting an easy-to-use methodology and tool, named MPI Advisor, that requires just a single execution of the input application to characterize its predominant communication behavior and determine the MPI configuration that may enhance its performance on the target combination of MPI library and hardware architecture. Currently, MPI Advisor provides recommendations that address the four most commonly occurring MPI-related performance bottlenecks, which are related to the choice of: 1) point-to-point protocol (eager vs. rendezvous), 2) collective communication algorithm, 3) MPI tasks-to-cores mapping, and 4) Infiniband transport protocol. The performance gains obtained by implementing the recommended optimizations in the case studies presented in this paper range from a few percent to more than 40%. Specifically, using this tool, we were able to improve the performance of HPCG with MVAPICH2 on four nodes of the Stampede cluster from 6.9 GFLOP/s to 10.1 GFLOP/s. Since the tool provides application-specific recommendations, it also informs the user about correct usage of MPI.},
booktitle = {Proceedings of the 22nd European MPI Users' Group Meeting},
articleno = {6},
numpages = {10},
keywords = {HPC, MPI, Performance tool, performance optimization},
location = {Bordeaux, France},
series = {EuroMPI '15}
}

@inproceedings{10.1145/3368826.3377918,
author = {Shobaki, Ghassan and Kerbow, Austin and Mekhanoshin, Stanislav},
title = {Optimizing occupancy and ILP on the GPU using a combinatorial approach},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377918},
doi = {10.1145/3368826.3377918},
abstract = {This paper presents the first general solution to the problem of optimizing both occupancy and Instruction-Level Parallelism (ILP) when compiling for a Graphics Processing Unit (GPU). Exploiting ILP (minimizing schedule length) requires using more registers, but using more registers decreases occupancy (the number of thread groups that can be run in parallel). The problem of balancing these two conflicting objectives to achieve the best overall performance is a challenging open problem in code optimization. In this paper, we present a two-pass Branch-and-Bound (B&amp;B) algorithm for solving this problem by treating occupancy as a primary objective and ILP as a secondary objective. In the first pass, the algorithm searches for a maximum-occupancy schedule, while in the second pass it iteratively searches for the shortest schedule that gives the maximum occupancy found in the first pass. The proposed scheduling algorithm was implemented in the LLVM compiler and applied to an AMD GPU. The algorithm’s performance was evaluated using benchmarks from the PlaidML machine learning framework relative to LLVM’s scheduling algorithm, AMD’s production scheduling algorithm and an existing B&amp;B scheduling algorithm that uses a different approach. The results show that the proposed B&amp;B scheduling algorithm speeds up almost every benchmark by up to 35% relative to LLVM’s scheduler, up to 31% relative to AMD’s scheduler and up to 18% relative to the existing B&amp;B scheduler. The geometric-mean improvements are 16.3% relative to LLVM’s scheduler, 5.5% relative to AMD’s production scheduler and 6.2% relative to the existing B&amp;B scheduler. If more compile time can be tolerated, a geometric-mean improvement of 6.3% relative to AMD’s scheduler can be achieved.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {133–144},
numpages = {12},
keywords = {Branch-and-Bound, Compiler Optimizations, Graphics Processing Unit (GPU), Instruction Scheduling, Instruction-Level Parallelism (ILP), Performance Optimization, Register Pressure Reduction},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.1007/978-3-030-00308-1_33,
author = {O’Keeffe, Simon and Villing, Rudi},
title = {A Benchmark Data Set and Evaluation of Deep Learning Architectures for Ball Detection in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_33},
doi = {10.1007/978-3-030-00308-1_33},
abstract = {This paper presents a benchmark data set for evaluating ball detection algorithms in the RoboCup Soccer Standard Platform League. We created a labelled data set of images with and without ball derived from vision log files recorded by multiple NAO robots in various lighting conditions. The data set contains 5209 labelled ball image regions and 10924 non-ball regions. Non-ball image regions all contain features that had been classified as a potential ball candidate by an existing ball detector. The data set was used to train and evaluate 252 different Deep Convolutional Neural Network (CNN) architectures for ball detection. In order to control computational requirements, this evaluation focused on networks with 2–5 layers that could feasibly run in the vision and cognition cycle of a NAO robot using two cameras at full frame rate (2&nbsp;\texttimes{}&nbsp;30&nbsp;Hz). The results show that the classification performance of the networks is quite insensitive to the details of the network design including input image size, number of layers and number of outputs at each layer. In an effort to reduce the computational requirements of CNNs we evaluated XNOR-Net architectures which quantize the weights and activations of a neural network to binary values. We examined XNOR-Nets corresponding to the real-valued CNNs we had already tested in order to quantify the effect on classification metrics. The results indicate that ball classification performance degrades by 12% on average when changing from real-valued CNN to corresponding XNOR-Net.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {398–409},
numpages = {12},
keywords = {Convolution neural network, Deep learning, Ball detection, XNOR-Net},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3373376.3378508,
author = {Zheng, Size and Liang, Yun and Wang, Shuo and Chen, Renze and Sheng, Kaiwen},
title = {FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378508},
doi = {10.1145/3373376.3378508},
abstract = {Tensor computation plays a paramount role in a broad range of domains, including machine learning, data analytics, and scientific computing. The wide adoption of tensor computation and its huge computation cost has led to high demand for flexible, portable, and high-performance library implementation on heterogeneous hardware accelerators such as GPUs and FPGAs. However, the current tensor library implementation mainly requires programmers to manually design low-level implementation and optimize from the algorithm, architecture, and compilation perspectives. Such a manual development process often takes months or even years, which falls far behind the rapid evolution of the application algorithms.In this paper, we introduce FlexTensor, which is a schedule exploration and optimization framework for tensor computation on heterogeneous systems. FlexTensor can optimize tensor computation programs without human interference, allowing programmers to only work on high-level programming abstraction without considering the hardware platform details. FlexTensor systematically explores the optimization design spaces that are composed of many different schedules for different hardware. Then, FlexTensor combines different exploration techniques, including heuristic method and machine learning method to find the optimized schedule configuration. Finally, based on the results of exploration, customized schedules are automatically generated for different hardware. In the experiments, we test 12 different kinds of tensor computations with totally hundreds of test cases and FlexTensor achieves average 1.83x performance speedup on NVIDIA V100 GPU compared to cuDNN; 1.72x performance speedup on Intel Xeon CPU compared to MKL-DNN for 2D convolution; 1.5x performance speedup on Xilinx VU9P FPGA compared to OpenCL baselines; 2.21x speedup on NVIDIA V100 GPU compared to the state-of-the-art.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {859–873},
numpages = {15},
keywords = {code generation, compiler optimization, heterogeneous systems, machine learning},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/3318299.3318306,
author = {Xiao, Yu Xiang and Wu, Mei Min and Bi, Qian},
title = {Visual Optimization of Cluster Simulation Based on Multi Process Service and Load Balancing Agent},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318306},
doi = {10.1145/3318299.3318306},
abstract = {This article introduces the OsgEarth open source project and the establishment of three-dimensional (3D) cluster situation. On account of multiple nodes and heavy task, the simulation visual effect in the 3D situation is not smooth. Aiming at the problems mentioned above, a multi process service architecture and a dynamic load balancing agent are proposed to deal with heavy task. Simultaneously, a visual optimization scheme based on callback and multithread interpolation is proposed to settle the caton phenomenon caused by the multi nodes in the 3D situation. On this basis, we verify the cluster simulation scene of 40 and 200 nodes. The experiments demonstrates a favourable visual impact with high performance.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {494–500},
numpages = {7},
keywords = {Multi process service architecture, load balancing, multithread interpolation},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.3233/JIFS-179942,
author = {Chao, Ma and Pan, Young Hwan and Zeng, ChuYao and Patnaik, Srikanta},
title = {Intelligent interaction design research based on block chain communication technology and fuzzy system},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179942},
doi = {10.3233/JIFS-179942},
abstract = {With the increasing amount of information on the Internet, data storage management tends to be distributed. In distributed storage environment, users pay more and more attention to the timeliness of user interaction experience and the reliability of information interaction. However, the efficiency of users is often limited by the efficiency of data communication between distributed sites. One of the important goals of distributed data management is to improve the efficiency of data transmission and ensure the reliability of data transmission. Block chain technology is one of the emerging technologies supporting the development of management information system; it provides a solution for the storage, verification, transmission and communication of the distributed data. This paper focuses on solving the problem of block chain data transmission, and studies it from three aspects: improving the efficiency of data communication, ensuring the reliability of transmission, and improving the fairness of service, and different block chain data communication performance optimization strategies are proposed under the constraints of node communication capability, node trust, weight, priority of service request and other influencing factors.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1685–1691},
numpages = {7},
keywords = {Block chain, Communication technology, fuzzy system, intelligent interaction design, data transfer}
}

@phdthesis{10.5555/AAI28544034,
author = {Khoshmanesh, Seyedehzahra and Samik, Basu, and Andrew, Miner, and Hridesh, Rajan, and Karin, Dorman,},
advisor = {R, Lutz, Robyn},
title = {Learning Feature Interactions with and without Specifications},
year = {2021},
isbn = {9798544278207},
publisher = {Iowa State University},
address = {USA},
abstract = {Developers of software product lines and highly configurable systems reuse and combine features (units of functionality) to build new or customize existing products. However, features can interact in ways that are contrary to developers' intent. Predicting whether a new combination of features will produce an unwanted or even hazardous feature interaction is a continuing challenge. Current techniques to detect unwanted feature interactions are costly, slow, and inadequate. In this thesis, we investigate how to detect unwanted feature interactions early in development and that are scalable to large software product lines or highly configurable systems. First, we propose a similarity-based method to identify unwanted feature interactions much earlier in the development process for early detection. It uses knowledge of prior feature interactions stored with the software product line's feature model to help find unwanted interactions between a new feature and existing features. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.Moreover, to learn and automate the detection, we show how detecting unwanted feature interactions can be effectively represented as a link prediction problem. We investigate six link-based similarity metrics and evaluate our approach on a software product line benchmark. Results show that the best machine learning algorithms achieve an accuracy of 0.75 to 1 for classifying feature interactions.Finally, we develop a new approach based on program analysis that extracts feature-relevant learning models from the source code to obtain more semantic details of unwanted feature interactions. The method is capable of learning feature interactions whether constraints on feature combinations are specified or not. If specifications of feature constraints are unavailable, as is common in real-world systems, our approach infers the constraints using feature-related data-flow dependency information. Experimental evaluation on three software product line benchmarks and a highly configurable system shows that this approach is fast and effective.The contribution is to support developers by automatically detecting those feature combinations in a new product or version that can interact in unwanted or unrecognized ways. This enables a better understanding of hidden interactions and identifies software components that should be tested together because their features interact in some configurations.},
note = {AAI28544034}
}

@article{10.1007/s00500-019-04503-4,
author = {Abboud, Ralph and Tekli, Joe},
title = {Integration of nonparametric fuzzy classification with an evolutionary-developmental framework to perform music sentiment-based analysis and composition},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {13},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04503-4},
doi = {10.1007/s00500-019-04503-4},
abstract = {Over the past years, several approaches have been developed to create algorithmic music composers. Most existing solutions focus on composing music that appears theoretically correct or interesting to the listener. However, few methods have targeted sentiment-based music composition: generating music that expresses human emotions. The few existing methods are restricted in the spectrum of emotions they can express (usually to two dimensions: valence and arousal) as well as the level of sophistication of the music they compose (usually monophonic, following translation-based, predefined templates or heuristic textures). In this paper, we introduce a new algorithmic framework for autonomous music sentiment-based expression and composition, titled MUSEC, that perceives an extensible set of six primary human emotions (e.g., anger, fear, joy, love, sadness, and surprise) expressed by a MIDI musical file and then composes (creates) new polyphonic (pseudo) thematic, and diversified musical pieces that express these emotions. Unlike existing solutions, MUSEC is: (i) a hybrid crossover between supervised learning (SL, to learn sentiments from music) and evolutionary computation (for music composition, MC), where SL serves at the fitness function of MC to compose music that expresses target sentiments, (ii) extensible in the panel of emotions it can convey, producing pieces that reflect a target crisp sentiment (e.g., love) or a collection of fuzzy sentiments (e.g., 65% happy, 20% sad, and 15% angry), compared with crisp-only or two-dimensional (valence/arousal) sentiment models used in existing solutions, (iii) adopts the evolutionary-developmental model, using an extensive set of specially designed music-theoretic mutation operators (trille, staccato, repeat, compress, etc.), stochastically orchestrated to add atomic (individual chord-level) and thematic (chord pattern-level) variability to the composed polyphonic pieces, compared with traditional evolutionary solutions producing monophonic and non-thematic music. We conducted a large battery of tests to evaluate MUSEC’s effectiveness and efficiency in both sentiment analysis and composition. It was trained on a specially constructed set of 120 MIDI pieces, including 70 sentiment-annotated pieces: the first significant dataset of sentiment-labeled MIDI music made available online as a benchmark for future research in this area. Results are encouraging and highlight the potential of our approach in different application domains, ranging over music information retrieval, music composition, assistive music therapy, and emotional intelligence.},
journal = {Soft Comput.},
month = jul,
pages = {9875–9925},
numpages = {51},
keywords = {Music sentiment analysis, MIDI, Evolutionary algorithms, Algorithmic composition, Supervised learning, Fuzzy classification}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study}
}

@inproceedings{10.1007/978-3-030-74251-5_8,
author = {Esteves, Diego and Marcelino, Jos\'{e} and Chawla, Piyush and Fischer, Asja and Lehmann, Jens},
title = {HORUS-NER: A Multimodal Named Entity Recognition Framework for Noisy Data},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_8},
doi = {10.1007/978-3-030-74251-5_8},
abstract = {Recent work based on Deep Learning presents state-of-the-art (SOTA) performance in the named entity recognition (NER) task. However, such models still have the performance drastically reduced in noisy data (e.g., social media, search engines), when compared to the formal domain (e.g., newswire). Thus, designing and exploring new methods and architectures is highly necessary to overcome current challenges. In this paper, we shift the focus of existing solutions to an entirely different perspective. We investigate the potential of embedding word-level features extracted from images and news. We performed a very comprehensive study in order to validate the hypothesis that images and news (obtained from an external source) may boost the task on noisy data, revealing very interesting findings. When our proposed architecture is used: (1) We beat SOTA in precision with simple CRFs models (2) The overall performance of decision trees-based models can be drastically improved. (3) Our approach overcomes off-the-shelf models for this task. (4) Images and text consistently increased recall over different datasets for SOTA, but at cost of precision. All experiment configurations, data and models are publicly available to the research community at},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {89–100},
numpages = {12},
keywords = {Named entity recognition, WNUT, Noisy text, Information retrieval, Images, Text, Multi-modal},
location = {Porto, Portugal}
}

@inproceedings{10.1145/3447555.3464850,
author = {He, Fang and Deng, Yang and Xu, Yanhui and Xu, Cheng and Hong, Dezhi and Wang, Dan},
title = {Energon: A Data Acquisition System for Portable Building Analytics},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3464850},
doi = {10.1145/3447555.3464850},
abstract = {Emerging building analytics rely on data-driven machine learning algorithms. However, writing these analytics is still challenging---developers not only need to know what data are required by the analytics but also how to reach the data in each individual building, despite the existing solutions to standardizing data and resource management in buildings. To bridge the gap between analytics development and the specific details of reaching the actual data in each building, we present Energon, an open-source system that enables portable building analytics. The core of Energon is a new data organization of building data, as well as the tools that can effectively manage building data and support building analytics development. More specifically, we propose a new "logic partition" of data resources in buildings, and this abstraction universally applies to all buildings. We develop a declarative query language to find data resources in this new logic views with high-level queries, thus substantially reducing development efforts. We also develop a query engine with automatic data extraction by traversing building ontology that widely exists in buildings. In this way, Energon enables analytics requirements to be mapped to building resources in a building-agnostic manner. Using four types of real-world building analytics, we demonstrate the use of Energon as well as its effectiveness in reducing development efforts.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {15–26},
numpages = {12},
keywords = {Data analytics, Declarative query, Machine learning, Smart building},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.1109/SPLC.2008.11,
author = {Niu, Nan and Easterbrook, Steve},
title = {On-Demand Cluster Analysis for Product Line Functional Requirements},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.11},
doi = {10.1109/SPLC.2008.11},
abstract = {We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {87–96},
numpages = {10},
keywords = {functional requirements profiles, information-theoretic clustering, overlapping clustering, requirements clustering},
series = {SPLC '08}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Software product line testing, Search-Based software engineering, Preference-Based algorithms}
}

@article{10.1145/3162626,
author = {Nguyen, Khanh and Wang, Kai and Bu, Yingyi and Fang, Lu and Xu, Guoqing},
title = {Understanding and Combating Memory Bloat in Managed Data-Intensive Systems},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3162626},
doi = {10.1145/3162626},
abstract = {The past decade has witnessed increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer’s choice for implementing such applications, due to its quick development cycle and rich suite of libraries and frameworks. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets large volumes of input data, memory bloat is significantly magnified and becomes a scalability-prohibiting bottleneck.This article first studies, analytically and empirically, the impact of bloat on the performance and scalability of large-scale, real-world data-intensive systems. To combat bloat, we design a novel compiler framework, called Facade, that can generate highly efficient data manipulation code by automatically transforming the data path of an existing data-intensive application. The key treatment is that in the generated code, the number of runtime heap objects created for data classes in each thread is (almost) statically bounded, leading to significantly reduced memory management cost and improved scalability. We have implemented Facade and used it to transform seven common applications on three real-world, already well-optimized data processing frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: the generated programs have (1) achieved a 3% to 48% execution time reduction and an up to 88\texttimes{} GC time reduction, (2) consumed up to 50% less memory, and (3) scaled to much larger datasets.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {12},
numpages = {41},
keywords = {Big data, managed languages, memory management, performance optimization}
}

@article{10.1007/s10462-020-09935-1,
author = {Choubey, Sachin and Karmakar, G. P.},
title = {Artificial intelligence techniques and their application in oil and gas industry},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {5},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09935-1},
doi = {10.1007/s10462-020-09935-1},
abstract = {Data are being continuously generated from various operational steps in the oil and gas industry. The recordings of these data and their proper utilization have become a major concern for the oil and gas industry. Decision making based on predictive as well as inferential data analytics helps in making accurate decisions within a short period of time. In spite of many challenges, the use of data analytics for decision making is increasing on a large-scale in the oil and gas industry. An appreciable amount of development has been done in the above area of research. Many complex problems may now be easily solved using Artificial Intelligence (AI) and Machine Learning (ML) techniques. Historical, as well as real-time data, can be assimilated to achieve higher production by gathering data from the gas/oil wells. Various analytical modeling techniques are now widely being used by the oil and gas sector to make a decision based on data analytics. This paper reviews the recent developments via applications of AI and ML techniques for efficient exploitation of the data obtained, starting from the exploration for crude oil to the distribution of its end products. A brief account of the acceptance and future of these techniques in the oil and gas industry is also discussed. Present work may provide a technical framework for choosing relevant technologies for effectively gaining the information from the large volume of data generated by the oil and gas industry.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {3665–3683},
numpages = {19},
keywords = {Artificial intelligence, Machine learning, Big data analytics, Oil and gas industry}
}

@article{10.1145/3301489,
author = {Shobaki, Ghassan and Kerbow, Austin and Pulido, Christopher and Dobson, William},
title = {Exploring an Alternative Cost Function for Combinatorial Register-Pressure-Aware Instruction Scheduling},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3301489},
doi = {10.1145/3301489},
abstract = {Multiple combinatorial algorithms have been proposed for doing pre-allocation instruction scheduling with the objective of minimizing register pressure or balancing register pressure and instruction-level parallelism. The cost function that is minimized in most of these algorithms is the peak register pressure (or the peak excess register pressure). In this work, we explore an alternative register-pressure cost function, which is the Sum of Live Interval Lengths (SLIL). Unlike the peak cost function, which captures register pressure only at the highest pressure point in the schedule, the proposed SLIL cost function captures register pressure at all points in the schedule. Minimizing register pressure at all points is desirable in larger scheduling regions with multiple high-pressure points. This article describes a Branch-and-Bound (B8B) algorithm for minimizing the SLIL cost function. The algorithm is based on two SLIL-specific dynamic lower bounds as well as the history utilization technique proposed in our previous work. The proposed algorithm is implemented into the LLVM Compiler and evaluated experimentally relative to our previously proposed B8B algorithm for minimizing the peak excess register pressure. The experimental results show that the proposed algorithm for minimizing the SLIL cost function produces substantially less spilling than the previous algorithm that minimizes the peak cost function. Execution-time results on various processors show that the proposed B8B algorithm significantly improves the performance of many CPU2006 benchmarks by up to 49% relative to LLVM's default scheduler. The geometric-mean improvement for FP2006 on Intel Core i7 is 4.22%.},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
articleno = {1},
numpages = {30},
keywords = {register pressure reduction, performance optimization, optimal instruction scheduling, instruction-level parallelism (ILP), branch-and-bound enumeration, NP-complete problems, Compiler optimizations}
}

@inproceedings{10.1145/2988287.2989158,
author = {Imtiaz, Sahar and Ghauch, Hadi and Rahman, M. Mahboob Ur and Koudouridis, George and Gross, James},
title = {Learning-Based Resource Allocation Scheme for TDD-Based 5G CRAN System},
year = {2016},
isbn = {9781450345026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988287.2989158},
doi = {10.1145/2988287.2989158},
abstract = {Provision of high data rates with always-on connectivity to high mobility users is one of the motivations for design of fifth generation (5G) systems. High system capacity can be achieved by coordination between large number of antennas, which is done using the cloud radio access network (CRAN) design in 5G systems. In terms of baseband processing, allocation of appropriate resources to the users is necessary to achieve high system capacity, for which the state of the art uses the users' channel state information (CSI); however, they do not take into account the associated overhead, which poses a major bottleneck for the effective system performance. In contrast to this approach, this paper proposes the use of machine learning for allocating resources to high mobility users using only their position estimates. Specifically, the `random forest' algorithm, a supervised machine learning technique, is used to design a learning-based resource allocation scheme by exploiting the relationships between the system parameters and the users' position estimates. In this way, the overhead for CSI acquisition is avoided by using the position estimates instead, with better spectrum utilization. While the initial numerical investigations, with minimum number of users in the system, show that the proposed learning-based scheme achieves 86% of the efficiency achieved by the perfect CSI-based scheme, if the effect of overhead is factored in, the proposed scheme performs better than the CSI-based approach. In a realistic scenario, with multiple users in the system, the significant increase in overhead for the CSI-based scheme leads to a performance gain of 100%, or more, by using the proposed scheme, and thus proving the proposed scheme to be more efficient in terms of system performance.},
booktitle = {Proceedings of the 19th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {176–185},
numpages = {10},
keywords = {tdd, resource allocation, machine learning., cran, 5g},
location = {Malta, Malta},
series = {MSWiM '16}
}

@inproceedings{10.5555/3299905.3299920,
author = {Chervyak, Nikolay and Lyakhov, Pavel and Valueva, Maria and Valuev, Georgii and Kaplun, Dmitrii and Efimenko, George and Gnezdilov, Denis},
title = {Area-Efficient FPGA Implementation of Minimalistic Convolutional Neural Network Using Residue Number System},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Convolutional Neural Networks (CNN) is the promising tool for solving task of image recognition in computer vision systems. However, the most known implementation of CNNs require a significant amount of memory for storing weights in training and work. To reduce the resource costs of CNN implementation we propose the architecture that separated on hardware and software parts for performance optimization. Also we propose to use Residue Number System (RNS) arithmetic in the hardware part which implements the convolutional layer of CNN. Software simulation using Matlab 2017b shows that CNN with a minimum number of layers can be quickly and successfully trained. Hardware simulation using FPGA Kintex7 xc7k70tfbg484-2 demonstrates that using RNS in convolutional layer of CNN allows to reduce hardware costs by 32% compared with the traditional approach based on the binary number system.},
booktitle = {Proceedings of the 23rd Conference of Open Innovations Association FRUCT},
articleno = {15},
numpages = {7},
keywords = {pattern recognition, image processing, Residue Number System, FPGA, Convolutional Neural Network},
location = {Bologna, Italy},
series = {FRUCT'23}
}

@article{10.1016/j.neucom.2019.11.104,
author = {Ren, Yazhou and Huang, Shudong and Zhao, Peng and Han, Minghao and Xu, Zenglin},
title = {Self-paced and auto-weighted multi-view clustering},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.104},
doi = {10.1016/j.neucom.2019.11.104},
journal = {Neurocomput.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Soft weighting, Multi-view clustering, Self-paced learning}
}

@inproceedings{10.1145/3404687.3404690,
author = {Fang, Zhou and Ma, Chao and Qu, Jiaxing and Song, Xue and Zhang, Chi},
title = {A Potential Value Preferences Elicitation Approach Based on SC-VPM and KNN},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404690},
doi = {10.1145/3404687.3404690},
abstract = {Nowadays the more and more customers start to select and use the composite Web service on Internet, at the same time the services with the same functional properties but the different non-functional properties are increasingly emerging on Internet, which cause the information overload. Then the customer is not able to completely understand various composite Web services, and he/she is not able to define reasonable value preferences clearly on them. Therefore, this paper presents a potential value preference elicitation approach based on SC-VPM model and KNN algorithm, so as to support the third-party brokers to recommends top-satisfying services to customers according to the value preferences of the customers. In the approach, the inference rules based on the semantic relationships in SC-VPM model are used to preliminarily supplement the initial customer-value preference matrix firstly, so as to reduce the impact of the matrix sparsity on the following prediction. And then the KNN algorithm is used to identify the value preferences of K nearest neighbors customers, and the value preference vector of the target customer can be predicted and obtained. At last, a case is used to validate the proposed approach.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {60–64},
numpages = {5},
keywords = {Value preferences, KNN algorithm, Information overload, Inference rules},
location = {Chengdu, China},
series = {ICBDC '20}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {God class, Project context information, Software metrics, Machine learning, Design smell detection}
}

@inproceedings{10.1145/3377024.3377040,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Generating attributed variability models for transfer learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377040},
doi = {10.1145/3377024.3377040},
abstract = {Modern software systems often provide configuration options for customizing of the system's functional and non-functional properties, such as response time and energy consumption. The valid configurations of a software system are commonly documented in a variability model. Supporting the optimization of a system's non-functional properties, variability models have been extended with attributes that represent the influence of one or multiple options on a property. The concrete values of attributes are typically determined only in a single environment (e.g., for a specific software version, a certain workload, and a specific hardware setup) and are applicable only for this context. Changing the environment, attribute values need to be updated. Instead of determining all attributes from scratch with new measurements, recent approaches rely on transfer learning to reduce the effort of obtaining new attribute values. However, the development and evaluation of new transfer-learning techniques requires extensive measurements by themselves, which often is prohibitively costly. To support research in this area, we propose an approach to synthesize realistic attributed variability models from a base model. This way, we can support research and validation of novel transfer-learning techniques for configurable software systems. We use a genetic algorithm to vary attribute values. Combined with a declarative objective function, we search a changed attributed variability model that keeps some key characteristics while mimicking realistic changes of individual attribute values. We demonstrate the applicability of our approach by replicating the evaluation of an existing transfer-learning technique.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {8},
keywords = {variability modelling, transfer learning, attributed variability models, Loki},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3434780.3436640,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Theron, Roberto},
title = {Advances in the use of domain engineering to support feature identification and generation of information visualizations},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436640},
doi = {10.1145/3434780.3436640},
abstract = {Information visualization tools are widely used to better understand large and complex datasets. However, to make the most out of them, it is necessary to rely on proper designs that consider not only the data to be displayed, but also the audience and the context. There are tools that already allow users to configure their displays without requiring programming skills, but this research project aims at exploring the automatic generation of information visualizations and dashboards in order to avoid the configuration process, and select the most suitable features of these tools taking into account their contexts. To address this problem, a domain engineering, and machine learning approach is proposed.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1053–1056},
numpages = {4},
keywords = {Meta-modeling, Machine Learning, Information Dashboards, High-level requirements, Domain engineering, Automatic generation},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@article{10.1007/s11227-017-2118-3,
author = {Li, Yuxiang and Zhao, Yinliang and Sun, Liyu and Shen, Mengjuan},
title = {A hybrid sample generation approach in speculative multithreading},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-017-2118-3},
doi = {10.1007/s11227-017-2118-3},
abstract = {Speculative multithreading (SpMT) is a thread-level automatic parallelization technique to accelerate sequential programs. Machine learning has been successfully brought into SpMT to improve its performance. An appropriate sample set, which plays the role of knowledge provider, is important for machine learning-based (ML-based) thread partition. Conventionally, heuristic rules-based (HR-based) sample generation approach cannot generate adaptive samples. A hybrid sample generation approach can break this bottleneck. With this method, we firstly automatically generate samples, which are MIPS codes consisting of spawning points (SPs) and control quasi-independent points (CQIPs) by heuristic rules; secondly manually adjust the positions of SPs and CQIPs and rebuild pre-computation slice to obtain better performance for every sample; and then build model to ensure that the probability of adjusting to the optimal partition positions is increasing. During the implementation of this approach, three measures: bias weighting, preservation of optimal solutions, summary of greedy rules, are taken. In this way, we enhance the adjustment frequency for subroutines with high called time and preserve the optimal partition positions, so to achieve a stable speedup improvement. On Prophet, which is a generic SpMT processor to evaluate the performance of multithreaded programs, SPEC2000 and Olden benchmarks are used as input. Experiments show that our approach can obtain better sample sets, which deliver a better performance improvement of about 86.9% on a 16 core than the samples generated by HR-based approach. Experiment results also prove that this approach is effective to generate sample sets for ML-based thread partition.},
journal = {J. Supercomput.},
month = aug,
pages = {4193–4225},
numpages = {33},
keywords = {Sample set, Machine learning, Speculative multithreading}
}

@inproceedings{10.1109/GLOBECOM46510.2021.9685602,
author = {Wei, Yi and Zhao, Ming-Min and Zhao, Min-Jian},
title = {Model-Driven GAN-Based Channel Modeling for IRS-Aided Wireless Communication},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM46510.2021.9685602},
doi = {10.1109/GLOBECOM46510.2021.9685602},
abstract = {Intelligent reflecting surface (IRS) is a promising new technology that is able to create a favorable wireless signal propagation environment by collaboratively reconfiguring the passive reflecting elements, yet with low hardware and energy cost. In IRS-aided wireless communication systems, channel modeling is a fundamental task for communication algorithm design and performance optimization, which however is also very challenging since in-depth domain knowledge and technical expertise in radio signal propagations are required, especially for modeling the high-dimensional cascaded base station (BS)-IRS and IRS-user channels (also referred to as the reflected channels). In this paper, we propose a model-driven generative adversarial network (GAN)-based channel modeling framework to autonomously learn the reflected channel distribution, without complex theoretical analysis or data processing. The designed GAN (also named as IRS-GAN) is trained to reach the Nash equilibrium of a minimax game between a generative model and a discriminative model, where the special structure of the reflected channels is incorporated to improve the modeling accuracy. Simulation results are presented to validate the effectiveness of the proposed IRS-GAN framework for IRS-related channel modeling.},
booktitle = {2021 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Madrid, Spain}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {99-00, 00-01, Local structure preservation, Latent representation learning, Hypergraph learning, Unsupervised feature selection}
}

@phdthesis{10.5555/AAI28646699,
author = {Xu, Hao and Bin, Ren, and Weizhen, Mao, and Qun, Li, and Guoliang, Jin,},
advisor = {Xu, Liu,},
title = {Combining Performance Profiling and Modeling for Accuracy and Efficiency},
year = {2021},
isbn = {9798460422388},
publisher = {The College of William and Mary},
abstract = {Modern computer systems have evolved to employ powerful parallel architectures, including multi-core processors, multi-socket chips, large memory subsystems, and fast network communication. Given such powerful hardware, developers rely on performance profiling and modeling to guide their performance optimization. However, performance optimization is facing new challenges on efficiency and accuracy with emerging computer systems. In this dissertation, we propose approaches to address these challenges. We first study memory contention in Non-Uniform Memory Access (NUMA) architectures. We present DR-BW,  a new tool based on machine learning to identify bandwidth contention in NUMA architectures and provide optimization  guidance. DR-BW collects performance data with low overhead (&lt;10%), feeds the data into a novel machine learning model to identify contention achieving more than 96% accuracy, and associates the analysis results with both programs and significant data objects.Then, we study and fix inaccuracy measurement in modern profilers. We investigate multiple modern architectures and quantify the PMU instruction profiling inaccuracy in these architectures with mathematical modeling. Then we design a systematic framework to evaluate the impact of PMU inaccuracy to the profiling results. We propose a software-based technique to rectify the measurement inaccuracy raised by PMU and demonstrate its effectiveness. Our research reveals that profiling and modeling significantly benefit system performance improvement. In addition, modeling based profiling also help user understand the performance bottleneck and guides the performance optimization.},
note = {AAI28646699}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {product line engineering, model-based testing, behavioral variability, aspect-oriented modeling, UML state machine},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/2996758.2996760,
author = {Zhang, Hao and Yao, Danfeng (Daphne) and Ramakrishnan, Naren},
title = {Causality-based Sensemaking of Network Traffic for Android Application Security},
year = {2016},
isbn = {9781450345736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996758.2996760},
doi = {10.1145/2996758.2996760},
abstract = {Malicious Android applications pose serious threats to mobile security. They threaten the data confidentiality and system integrity on Android devices. Monitoring runtime activities serves as an important technique for analyzing dynamic app behaviors. We design a triggering relation model for dynamically analyzing network traffic on Android devices. Our model enables one to infer the dependency of outbound network requests from the device. We describe a new machine learning approach for discovering the dependency of network requests. These request-level dependence relations are used to detect stealthy malware activities. Malicious requests are identified due to the lack of dependency with legitimate triggers. Our prototype is evaluated on 14GB network traffic data and system logs collected from an Android tablet. Experimental results show that our solution achieves a high accuracy (99.1%) in detecting malicious requests sent from new malicious apps.},
booktitle = {Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security},
pages = {47–58},
numpages = {12},
keywords = {network security, mobile security, machine learning, anomaly detection},
location = {Vienna, Austria},
series = {AISec '16}
}

@article{10.1145/3369393,
author = {Ding, Yuhang and Fan, Hehe and Xu, Mingliang and Yang, Yi},
title = {Adaptive Exploration for Unsupervised Person Re-identification},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3369393},
doi = {10.1145/3369393},
abstract = {Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {3},
numpages = {19},
keywords = {unsupervised learning, domain adaptation, deep learning, Person re-identification}
}

@inproceedings{10.1109/ICPPW.2014.42,
author = {Li, Lu and Dastgeer, Usman and Kessler, Christoph},
title = {Pruning Strategies in Adaptive Off-Line Tuning for Optimized Composition of Components on Heterogeneous Systems},
year = {2014},
isbn = {9781479956159},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPPW.2014.42},
doi = {10.1109/ICPPW.2014.42},
abstract = {Adaptive program optimizations, such as automatic selection of the expected fastest implementation variant for a computation component depending on runtime context, are important especially for heterogeneous computing systems but require good performance models. Empirical performance models based on trial executions which require no or little human efforts show more practical feasibility if the sampling and training cost can be reduced to a reasonable level. In previous work we proposed an early version of adaptive pruning algorithm for efficient selection of training samples, a decision-tree based method for representing, predicting and selecting the fastest implementation variants for given run-time call context properties, and a composition tool for building the overall composed application from its components. For adaptive pruning we use a heuristic convexity assumption. In this paper we consolidate and improve the method by new pruning techniques to better support the convexity assumption and better control the trade-off between sampling time, prediction accuracy and runtime prediction overhead. Our results show that the training time can be reduced by up to 39 times without noticeable prediction accuracy decrease. Furthermore, we evaluate the effect of combinations of pruning strategies and compare our adaptive sampling method with random sampling. We also use our smart-sampling method as a preprocessor to a state-of-the-art decision tree learning algorithm and compare the result to the predictor directly calculated by our method.},
booktitle = {Proceedings of the 2014 43rd International Conference on Parallel Processing Workshops},
pages = {255–264},
numpages = {10},
keywords = {Performance optimization, Machine learning, Implementation selection, Heterogeneous computing, GPU, Autotuning, Adaptive sampling},
series = {ICPPW '14}
}

@article{10.1016/j.eswa.2007.09.057,
author = {Wu, Chia-Wei and Tsai, Richard Tzong-Han and Lee, Cheng-Wei and Hsu, Wen-Lian},
title = {Web taxonomy integration with hierarchical shrinkage algorithm and fine-grained relations},
year = {2008},
issue_date = {November, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {35},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.09.057},
doi = {10.1016/j.eswa.2007.09.057},
abstract = {We address the problem of integrating web taxonomies from different real Internet applications. Integrating web taxonomies is to transfer instances from a source to target taxonomy. Unlike the conventional text categorization problem, in taxonomy integration, the source taxonomy contains extra information that can be used to improve the categorization. The major existing methods can be divided in two types: those that use neighboring categories to smooth the document term vector and those that consider the semantic relationship between corresponding categories of the target and source taxonomies to facilitate categorization. In contrast to the first type of approach, which only uses a flattened hierarchy for smoothing, we apply a hierarchy shrinkage algorithm to smooth child documents by their parents. We also discuss the effect of using different hierarchical levels for smoothing. To extend the second type of approach, we extract fine-grain semantic relationships, which consider the relationships between lower-level categories. In addition, we use the cosine similarity to measure the semantic relationships, which achieves better performance than existing methods. Finally, we integrate the existing approaches and the proposed methods into one machine learning model to find the best feature configuration. The results of experiments on real Internet data demonstrate that our system outperforms standard text classifiers by about 10%.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {2123–2131},
numpages = {9},
keywords = {Web taxonomy integration, Text categorization, Shrinkage algorithm}
}

@inproceedings{10.1145/3332186.3332235,
author = {Mehringer, Susan and Myers, Christopher R. and Houchins, Jennifer and Rivera, Lorna},
title = {Mining Online Training Log Data},
year = {2019},
isbn = {9781450372275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332186.3332235},
doi = {10.1145/3332186.3332235},
abstract = {Online training has been growing in popularity, and offers many advantages for both trainers and learners. Assessing the usage and impact of online material can be difficult, especially if content is made available to anyone and is not part of a course requiring formal enrollment. The Cornell Virtual Workshop (CVW) first offered online training on topics in high-performance computing and computational science in 1994, and ten years ago we began logging usage. We are now performing our first in-depth analysis of those log data to identify patterns in usage, so that we can better understand how users access the material, which types of topics and materials result in the greatest impact, how topic usage changes over time, and what types of presentation format might be preferred. While the CVW is built around a cohesive, sequential narrative for each training topic, we find that many users access our content in a more targeted fashion, suggesting that we rethink how we package our material. We anticipate that ongoing analysis using data science and machine learning methods will enable us to produce more useful training materials, and provide the educational community with valuable information about patterns in online material usage.},
booktitle = {Practice and Experience in Advanced Research Computing 2019: Rise of the Machines (Learning)},
articleno = {84},
numpages = {8},
keywords = {HPC, Training, Usage Statistics, XSEDE},
location = {Chicago, IL, USA},
series = {PEARC '19}
}

@inproceedings{10.1145/3377930.3389815,
author = {Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd},
title = {Multi-objective hyperparameter tuning and feature selection using filter ensembles},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389815},
doi = {10.1145/3377930.3389815},
abstract = {Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well.We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles.While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {471–479},
numpages = {9},
keywords = {multiobjective optimization, model-based optimization, hyperparameter optimization, feature selection, evolutionary algorithms},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Text, Linear discriminant analysis, Fisher, Eigenvectors, Classification, Arabic}
}

@article{10.1007/s00165-021-00543-6,
author = {Yang, Zhibin and Bao, Yang and Yang, Yongqiang and Huang, Zhiqiu and Bodeveix, Jean-Paul and Filali, Mamoun and Gu, Zonghua},
title = {Exploiting augmented intelligence in the modeling of safety-critical autonomous systems},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {3},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00543-6},
doi = {10.1007/s00165-021-00543-6},
abstract = {Machine learning (ML) is used increasingly in safety-critical
systems to provide more complex autonomy to make the system to do
decisions by itself in uncertain environments. Using ML to learn
system features is fundamentally different from manually
implementing them in conventional components written in source code.
In this paper, we make a first step towards exploring
the architecture modeling of safety-critical autonomous systems
which are composed of conventional components and ML components,
based on natural language requirements. Firstly, augmented
intelligence for restricted natural language requirement
modeling is proposed. In that, several AI technologies such as
natural language processing and clustering are used to recommend
candidate terms to the glossary, as well as machine learning is used
to predict the category of requirements. The glossary including data
dictionary and domain glossary and the category of requirements will
be used in the restricted natural language requirement specification
method RNLReq, which is equipped with a set of restriction rules and
templates to structure and restrict the way how users document
requirements. Secondly, automatic generation of SysML architecture
models from the RNLReq requirement specifications is presented.
Thirdly, the prototype tool is implemented based on Papyrus.
Finally,  it presents the evaluation of the proposed
approach using an industrial autonomous guidance, navigation and
control case study.},
journal = {Form. Asp. Comput.},
month = jun,
pages = {343–384},
numpages = {42},
keywords = {SysML, Machine learning, Natural language processing, Restricted natural language requirements, Augmented intelligence, Safety-critical autonomous system}
}

@inproceedings{10.1145/3410463.3414648,
author = {Rumi, Masuma Akter and Ma, Xiaolong and Wang, Yanzhi and Jiang, Peng},
title = {Accelerating Sparse CNN Inference on GPUs with Performance-Aware Weight Pruning},
year = {2020},
isbn = {9781450380751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410463.3414648},
doi = {10.1145/3410463.3414648},
abstract = {Weight pruning is a popular technique to reduce the size and computation complexity of the Convolutional Neural Networks (CNNs). Despite its success in reducing the model size, weight pruning has brought limited benefit to the CNN inference performance, due to the irregularity introduced in the sparse convolution operations. In this work, we aim to improve the performance of sparse convolutions on GPUs by mitigating the irregularity. We find that the existing performance optimization techniques for sparse matrix computations fail to accelerate sparse convolutions, and we observe that the main performance bottleneck is caused by the heavy control-flow instructions. Based on the observation, we proposed a new GEMM-based implementation of sparse convolutions. Our main idea is to extract dense blocks of non-zeros in the sparse convolution kernels, and use dense matrix-matrix multiplication for these dense blocks to achieve high throughput. For cases where many non-zero weights cannot be grouped into dense blocks, we propose a performance-aware re-pruning strategy that removes the least important weights in the sparse kernels to further improve the throughput. The experimental results with five real-world pruned CNN models show that our techniques can significantly improve the layer-wise performance of sparse convolution operations as well as the end-to-end performance of CNN inference.},
booktitle = {Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
pages = {267–278},
numpages = {12},
keywords = {sparse convolution, gpus, cnn pruning},
location = {Virtual Event, GA, USA},
series = {PACT '20}
}

@article{10.1016/j.future.2019.07.013,
author = {Shen, Rongbo and Yan, Kezhou and Tian, Kuan and Jiang, Cheng and Zhou, Ke},
title = {Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.07.013},
doi = {10.1016/j.future.2019.07.013},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {668–679},
numpages = {12},
keywords = {Self-paced learning, Deep active learning, Mass detection, Mammography, Breast cancer}
}

@article{10.1007/s00521-015-2164-9,
author = {Lin, Fan and Wang, Jingbin and Zhang, Nian and Xiahou, Jianbing and Mcdonald, Nancy},
title = {Multi-kernel learning for multivariate performance measures optimization},
year = {2017},
issue_date = {August    2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {8},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-015-2164-9},
doi = {10.1007/s00521-015-2164-9},
abstract = {In this paper, we investigate the problem of optimizing complex multivariate performance measures to learn classifiers for pattern classification problems. For the first time, the multi-kernel learning is considered to construct a classifier to optimize a given nonlinear and non-smooth multivariate classifier performance measure. We estimate and optimize the upper bound of the given multivariate performance measure, instead of optimizing it directly. Moreover, to solve the problem of kernel function selection and kernel parameter tuning,
 we proposed to construct an optimal kernel by weighted linear combination of some candidate kernels. The learning of the classifier parameter and the kernel weight are unified in a single objective function considering minimizing the upper bound of the given multivariate performance measure. The objective function is optimized with regard to classifier parameter and kernel weight alternately in an iterative algorithm. The developed algorithm is evaluated on two different pattern classification methods with regard to various multivariate performance measure optimization problems. The experiment results show the proposed algorithm outperforms the competing methods.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {2075–2087},
numpages = {13},
keywords = {Pattern classification, Multivariate performance measures, Multi-kernel learning, Cutting plane algorithm, Alternate optimization}
}

@article{10.1016/j.neucom.2014.12.100,
author = {Garcia, Lu\'{\i}s P.F. and Carvalho, Andr\'{e} C.P.L.F. de and Lorena, Ana C.},
title = {Noise detection in the meta-learning level},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {176},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.12.100},
doi = {10.1016/j.neucom.2014.12.100},
abstract = {The presence of noise in real data sets can harm the predictive performance of machine learning algorithms. There are several noise filtering techniques whose goal is to improve the quality of the data in classification tasks. These techniques usually scan the data for noise identification in a preprocessing step. Nonetheless, this is a non-trivial task and some noisy data can remain unidentified, while safe data can also be removed. The bias of each filtering technique influences its performance on a particular data set. Therefore, there is no single technique that can be considered the best for all domains or data distribution and choosing a particular filter is not straightforward. Meta-learning has been largely used in the last years to support the recommendation of the most suitable machine learning algorithm(s) for a new data set. This paper presents a meta-learning recommendation system able to predict the expected performance of noise filters in noisy data identification tasks. For such, a meta-base is created, containing meta-features extracted from several corrupted data sets along with the performance of some noise filters when applied to these data sets. Next, regression models are induced from this meta-base to predict the expected performance of the investigated filters in the identification of noisy data. The experimental results show that meta-learning can provide a good recommendation of the most promising filters to be applied to new classification data sets.},
journal = {Neurocomput.},
month = feb,
pages = {14–25},
numpages = {12},
keywords = {Noise identification, Meta-learning, Complexity measures, Characterization measures}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1007/978-3-031-24337-0_43,
author = {L\^{e}, Luyundefinedn Ngundefinedc and Haralambous, Yannis},
title = {CCG Supertagging Using Morphological and&nbsp;Dependency Syntax Information},
year = {2019},
isbn = {978-3-031-24336-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-24337-0_43},
doi = {10.1007/978-3-031-24337-0_43},
abstract = {After presenting a new CCG supertagging algorithm based on morphological and dependency syntax information, we use this algorithm to create a CCG French Tree Bank corpus (20,261 sentences) based on the FTB corpus by Abeill\'{e} et al. We then use this corpus, as well as the Groningen Tree Bank corpus for the English language, to train a new BiLSTM+CRF neural architecture that uses (a)&nbsp;morphosyntactic input features and (b)&nbsp;feature correlations as input features. We show experimentally that for an inflected language like French, dependency syntax information allows significant improvement of the accuracy of the CCG supertagging task, when using deep learning techniques.},
booktitle = {Computational Linguistics and Intelligent  Text Processing: 20th International Conference, CICLing 2019, La Rochelle, France, April 7–13, 2019, Revised Selected Papers, Part I},
pages = {608–621},
numpages = {14},
keywords = {CRF, BiLSTM, FTB corpus, Dependency syntax, CCG supertagging},
location = {La Rochelle, France}
}

@inproceedings{10.1007/978-3-030-67658-2_34,
author = {Yamaguchi, Akihiro and Maya, Shigeru and Ueno, Ken},
title = {RLTS: Robust Learning Time-Series Shapelets},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_34},
doi = {10.1007/978-3-030-67658-2_34},
abstract = {Shapelets are time-series segments effective for classifying time-series instances. Joint learning of both classifiers and shapelets has been studied in recent years because such a method provides both superior classification performance and interpretable results. For robust learning, we introduce Self-Paced Learning (SPL) and adaptive robust losses into this method. The SPL method can assign latent instance weights by considering not only classification losses but also understandable shapelet discovery. Furthermore, the adaptive robustness introduced into feature vectors is jointly learned with shapelets, a classifier, and latent instance weights. We demonstrate the superiority of AUC and the validity of our approach on UCR time-series datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {595–611},
numpages = {17},
keywords = {Time-series shapelets, Self-paced learning, Robust losses},
location = {Ghent, Belgium}
}

@inproceedings{10.1145/1809049.1809079,
author = {Eastep, Jonathan and Wingate, David and Santambrogio, Marco D. and Agarwal, Anant},
title = {Smartlocks: lock acquisition scheduling for self-aware synchronization},
year = {2010},
isbn = {9781450300742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809049.1809079},
doi = {10.1145/1809049.1809079},
abstract = {As multicore processors become increasingly prevalent, system complexity is skyrocketing. The advent of the asymmetric multicore compounds this - it is no longer practical for an average programmer to balance the system constraints associated with today's multicores and worry about new problems like asymmetric partitioning and thread interference. Adaptive, or self-aware, computing has been proposed as one method to help application and system programmers confront this complexity. These systems take some of the burden off of programmers by monitoring themselves and optimizing or adapting to meet their goals.This paper introduces a self-aware synchronization library for multicores and asymmetric multicores called Smartlocks. Smartlocks is a spin-lock library that adapts its internal implementation during execution using heuristics and machine learning to optimize toward a user-defined goal, which may relate to performance or problem-specific criteria. Smartlocks builds upon adaptation techniques from prior work like reactive locks [1], but introduces a novel form of adaptation that we term lock acquisition scheduling designed specifically to address asymmetries in multicores. Lock acquisition scheduling is optimizing which waiter will get the lock next for the best long-term effect when multiple threads (or processes) are spinning for a lock.This work demonstrates that lock scheduling is important for addressing asymmetries in multicores. We study scenarios where core speeds vary both dynamically and intrinsically under thermal throttling and manufacturing variability, respectively, and we show that Smartlocks significantly outperforms conventional spin-locks and reactive locks. Based on our findings, we provide guidelines for application scenarios where Smartlocks works best versus less optimally.},
booktitle = {Proceedings of the 7th International Conference on Autonomic Computing},
pages = {215–224},
numpages = {10},
keywords = {synchronization, self-tuning, self-aware, performance optimization, heterogeneous multicore, asymmetric multicore},
location = {Washington, DC, USA},
series = {ICAC '10}
}

@inproceedings{10.1007/978-3-030-26061-3_3,
author = {Akhtiamov, Oleg and Fedotov, Dmitrii and Minker, Wolfgang},
title = {A Comparative Study of Classical and Deep Classifiers for Textual Addressee Detection in Human-Human-Machine Conversations},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_3},
doi = {10.1007/978-3-030-26061-3_3},
abstract = {The problem of addressee detection (AD) arises in multiparty conversations involving several dialogue agents. In order to maintain such conversations in a realistic manner, an automatic spoken dialogue system is supposed to distinguish between computer- and human-directed utterances since the latter utterances either need to be processed in a specific way or should be completely ignored by the system. In the present paper, we consider AD to be a text classification problem and model three aspects of users’ speech (syntactical, lexical, and semantical) that are relevant to AD in German. We compare simple classifiers operating with supervised text representations learned from in-domain data and more advanced neural network-based models operating with unsupervised text representations learned from in- and out-of-domain data. The latter models provide a small yet significant AD performance improvement over the classical ones on the Smart Video Corpus. A neural network-based semantical model determines the context of the first four words of an utterance to be the most informative for AD, significantly surpasses syntactical and lexical text classifiers and keeps up with a baseline multimodal metaclassifier that utilises acoustical information in addition to textual data. We also propose an effective approach to building representations for out-of-vocabulary words.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {20–30},
numpages = {11},
keywords = {Spoken dialogue system, Human-computer interaction, Speaking style, Text classification},
location = {Istanbul, Turkey}
}

@article{10.1007/s11042-019-7498-3,
author = {Kaur, Taranjit and Saini, Barjinder Singh and Gupta, Savita},
title = {An adaptive fuzzy K-nearest neighbor approach for MR brain tumor image classification using parameter free bat optimization algorithm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7498-3},
doi = {10.1007/s11042-019-7498-3},
abstract = {This paper presents an automatic diagnosis system for the tumor grade classification through magnetic resonance imaging (MRI). The diagnosis system involves a region of interest (ROI) delineation using intensity and edge magnitude based multilevel thresholding algorithm. Then the intensity and the texture attributes are extracted from the segregated ROI. Subsequently, a combined approach known as Fisher+ Parameter-Free BAT (PFreeBAT) optimization is employed to derive the optimal feature subset. Finally, a novel learning approach dubbed as PFree BAT enhanced fuzzy K-nearest neighbor (FKNN) is proposed by combining FKNN with PFree BAT for the classification of MR images into two categories: High and Low-Grade. In PFree BAT enhanced FKNN, the model parameters, i.e., neighborhood size k and the fuzzy strength parameter m are adaptively specified by the PFree BAT optimization approach. Integrating PFree BAT with FKNN enhances the classification capability of the FKNN. The diagnostic system is rigorously evaluated on four MR images datasets including images from BRATS 2012 database and the Harvard repository using classification performance metrics. The empirical results illustrate that the diagnostic system reached to ceiling level of accuracy on the test MR image dataset via 5-fold cross-validation mechanism. Additionally, the proposed PFree BAT enhanced FKNN is evaluated on the Parkinson dataset (PD) from the UCI repository having the pre-extracted feature space. The proposed PFree BAT enhanced FKNN reached to an average accuracy of 98% and 97.45%. with and without feature selection on PD dataset. Moreover, solely to contrast, the performance of the proposed PFree BAT enhanced FKNN with the existing FKNN variants the experimentations were also done on six other standard datasets from KEEL repository. The results indicate that the proposed learning strategy achieves the best value of accuracy in contrast to the existing FKNN variants.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21853–21890},
numpages = {38},
keywords = {Model parameters, Diagnosis system, PFree BAT optimization, Fuzzy K-nearest neighbor}
}

@article{10.1145/3463369,
author = {Grohmann, Johannes and Eismann, Simon and Bauer, Andr\'{e} and Spinner, Simon and Blum, Johannes and Herbst, Nikolas and Kounev, Samuel},
title = {SARDE: A Framework for Continuous and Self-Adaptive Resource Demand Estimation},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3463369},
doi = {10.1145/3463369},
abstract = {Resource demands are crucial parameters for modeling and predicting the performance of software systems. Currently, resource demand estimators are usually executed once for system analysis. However, the monitored system, as well as the resource demand itself, are subject to constant change in runtime environments. These changes additionally impact the applicability, the required parametrization as well as the resulting accuracy of individual estimation approaches. Over time, this leads to invalid or outdated estimates, which in turn negatively influence the decision-making of adaptive systems. In this article, we present SARDE, a framework for self-adaptive resource demand estimation in continuous environments. SARDE dynamically and continuously tunes, selects, and executes an ensemble of resource demand estimation approaches to adapt to changes in the environment. This creates an autonomous and unsupervised ensemble estimation technique, providing reliable resource demand estimations in dynamic environments. We evaluate SARDE using two realistic datasets. One set of different micro-benchmarks reflecting different possible system states and one dataset consisting of a continuously running application in a changing environment. Our results show that by continuously applying online optimization, selection and estimation, SARDE is able to efficiently adapt to the online trace and reduce the model error using the resulting ensemble technique.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jun,
articleno = {6},
numpages = {31},
keywords = {self-tuning algorithms, resource demand estimation, optimization, machine learning, Self-adaptive systems}
}

@inproceedings{10.1007/978-3-030-33246-4_45,
author = {Gonz\'{a}lez-Rojas, Oscar and Tafurth, Juan},
title = {Multi-cloud Services Configuration Based on Risk Optimization},
year = {2019},
isbn = {978-3-030-33245-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33246-4_45},
doi = {10.1007/978-3-030-33246-4_45},
abstract = {Nowadays risk analysis becomes critical in the Cloud Computing domain due to the increasing number of threats affecting applications running on cloud infrastructures. Multi-cloud environments allow connecting and migrating services from multiple cloud providers to manage risks. This paper addresses the question of how to model and configure multi-cloud services that can adapt to changes in user preferences and threats on individual and composite services. We propose an approach that combines Product Line (PL) and Machine Learning (ML) techniques to model and timely find optimal configurations of large adaptive systems such as multi-cloud services. A three-layer variability modeling on domain, user preferences, and adaptation constraints is proposed to configure multi-cloud solutions. ML regression algorithms are used to quantify the risk of resulting configurations by analyzing how a service was affected by incremental threats over time. An experimental evaluation on a real life electronic identification and trust multi-cloud service shows the applicability of the proposed approach to predict the risk for alternative re-configurations on autonomous and decentralized services that continuously change their availability and provision attributes.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&amp;TC 2019, Rhodes, Greece, October 21–25, 2019, Proceedings},
pages = {733–749},
numpages = {17},
keywords = {Machine learning, Risk optimization, Product line configuration, Variability modeling, Multi-cloud services},
location = {Rhodes, Greece}
}

@inproceedings{10.5555/3042817.3043037,
author = {Ogawa, Kohei and ImamuraI, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},
title = {Infinitesimal annealing for training semi-supervised support vector machines},
year = {2013},
publisher = {JMLR.org},
abstract = {The semi-supervised support vector machine (S3VM) is a maximum-margin classification algorithm based on both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good (local) solution of S3VM is to gradually increase the effect of unlabeled data, \`{a} la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–897–III–905},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{10.1155/2020/8845932,
author = {Xu, Hui and Cheng, Hongju},
title = {Distinguishing Hand Drawing Style Based on Multilevel Analytics Framework},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/8845932},
doi = {10.1155/2020/8845932},
abstract = {Hand drawing is an indispensable professional skill in the fields of environmental design, industrial design, architectural engineering, civil engineering, and other engineering design education. Students usually imitate masterpieces to practice basic skills, which is an important link for a beginner. A system for digital management requires a function for an automatic recommendation task of different brushwork skill expressions. Thus, the classification method for brushwork is to combine hand-crafted features generated by DCNN and then use the final features for input to a tree structure classification scheme. The method improvement of the other deep learning models has effectiveness in distinguishing art ontology attributes.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {10}
}

@article{10.1016/j.comcom.2021.06.027,
author = {Shang, Xiuhai and Che, Xusheng},
title = {Optimization of fitness data monitoring system based on Internet of Things and cloud computing},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {177},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2021.06.027},
doi = {10.1016/j.comcom.2021.06.027},
journal = {Comput. Commun.},
month = sep,
pages = {125–132},
numpages = {8},
keywords = {Data monitoring system, Isolated forest algorithm, Fitness data supervision, Cloud computing, Internet of Things}
}

@article{10.3233/HIS-2011-0140,
author = {Ahumada, Hern\'{a}n and Grinblat, Guillermo L. and Uzal, Lucas C. and Ceccatto, Alejandro and Granitto, Pablo M.},
title = {Evaluation of a new hybrid algorithm for highly imbalanced classification problems},
year = {2011},
issue_date = {October 2011},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {4},
issn = {1448-5869},
url = {https://doi.org/10.3233/HIS-2011-0140},
doi = {10.3233/HIS-2011-0140},
abstract = {Many times in classification problems, particularly in critical real world applications, one of the classes has much less samples than the others usually known as the class imbalance problem. In this work we discuss and evaluate the use of the REPMAC algorithm to solve imbalanced problems. Using a clustering method, REPMAC recursively splits the majority class in several subsets, creating a decision tree, until the resulting sub-problems are balanced or easy to solve. We use two diverse clustering methods and three different classifiers coupled with REPMAC to evaluate the new method on several benchmark datasets spanning a wide range of number of features, samples and imbalance degree. We also apply our method to a real world problem, the identification of weed seeds. We find that the good performance of REPMAC is almost independent of the classifier or the clustering method coupled to it, which suggests that its success is mostly related to the use of an appropriate strategy to cope with imbalanced problems.},
journal = {Int. J. Hybrid Intell. Syst.},
month = oct,
pages = {199–211},
numpages = {13},
keywords = {Hybrid Systems, Clustering, Classification, Class Imbalance}
}

@inproceedings{10.1109/WAINA.2008.267,
author = {Witold, Abramowicz and Konstanty, Haniewicz and Monika, Kaczmarek and Raul, Palma and Dominik, Zyskowski},
title = {NFP Ontology for Discovery and Sharing Web Services in Distributed Registries},
year = {2008},
isbn = {9780769530963},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WAINA.2008.267},
doi = {10.1109/WAINA.2008.267},
abstract = {The success of Web services technology depends heavily on the effective discovery and sharing of services in distributed environments using various registries. These tasks should take into account both functional and non-functional properties (NFP) of a service. Such properties may be for example: execution time, reliability, security, cost or provider’s reputation. However, currently known approaches focus mainly on the functional attributes of Web services and there is an ongoing discussion on the scope and methods that should be used to express the non-functional side of WS. This article presents an ontology of non-functional properties and shows, on the example of the WebSter registry, how it may be used within the WS discovery process in distributed environments.},
booktitle = {Proceedings of the 22nd International Conference on Advanced Information Networking and Applications - Workshops},
pages = {1416–1421},
numpages = {6},
keywords = {Web services, Ontology Sharing, Ontology Discovery, Non Functional Properties, Distributed Registries},
series = {AINAW '08}
}

@article{10.1177/0165551515591724,
author = {Onan, Aytu\u{g}},
title = {Classifier and feature set ensembles for web page classification},
year = {2016},
issue_date = {4 2016},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {42},
number = {2},
issn = {0165-5515},
url = {https://doi.org/10.1177/0165551515591724},
doi = {10.1177/0165551515591724},
abstract = {Web page classification is an important research direction on web mining. The abundant amount of data available on the web makes it essential to develop efficient and robust models for web mining tasks. Web page classification is the process of assigning a web page to a particular predefined category based on labelled data. It serves for several other web mining tasks, such as focused web crawling, web link analysis and contextual advertising. Machine learning and data mining methods have been successfully applied for several web mining tasks, including web page classification. Multiple classifier systems are a promising research direction in machine learning, which aims to combine several classifiers by differentiating base classifiers and/or dataset distributions so that more robust classification models can be built. This paper presents a comparative analysis of four different feature selections correlation, consistency, information gain and chi-square-based feature selection and four different ensemble learning methods Boosting, Bagging, Dagging and Random Subspace based on four different base learners naive Bayes, K-nearest neighbour algorithm, C4.5 algorithm and FURIA algorithm. The article examines the predictive performance of ensemble methods for web page classification. The experimental results indicate that feature selection and ensemble learning can enhance the predictive performance of classifiers in web page classification. For the DMOZ-50 dataset, the highest average predictive performance 88.1% is obtained with the combination of consistency-based feature selection with AdaBoost and naive Bayes algorithms, which is a promising result for web page classification. Experimental results indicate that Bagging and Random Subspace ensemble methods and correlation-based and consistency-based feature selection methods obtain better results in terms of accuracy rates.},
journal = {J. Inf. Sci.},
month = apr,
pages = {150–165},
numpages = {16},
keywords = {web page classification, multiple classifiers, Ensemble learning}
}

@inproceedings{10.1145/3295500.3356197,
author = {Di Natale, Francesco and Bhatia, Harsh and Carpenter, Timothy S. and Neale, Chris and Kokkila-Schumacher, Sara and Oppelstrup, Tomas and Stanton, Liam and Zhang, Xiaohua and Sundram, Shiv and Scogland, Thomas R. W. and Dharuman, Gautham and Surh, Michael P. and Yang, Yue and Misale, Claudia and Schneidenbach, Lars and Costa, Carlos and Kim, Changhoan and D'Amora, Bruce and Gnanakaran, Sandrasegaram and Nissley, Dwight V. and Streitz, Fred and Lightstone, Felice C. and Bremer, Peer-Timo and Glosli, James N. and Ing\'{o}lfsson, Helgi I.},
title = {A massively parallel infrastructure for adaptive multiscale simulations: modeling RAS initiation pathway for cancer},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356197},
doi = {10.1145/3295500.3356197},
abstract = {Computational models can define the functional dynamics of complex systems in exceptional detail. However, many modeling studies face seemingly incommensurate requirements: to gain meaningful insights into some phenomena requires models with high resolution (microscopic) detail that must nevertheless evolve over large (macroscopic) length- and time-scales. Multiscale modeling has become increasingly important to bridge this gap. Executing complex multiscale models on current petascale computers with high levels of parallelism and heterogeneous architectures is challenging. Many distinct types of resources need to be simultaneously managed, such as GPUs and CPUs, memory size and latencies, communication bottlenecks, and filesystem bandwidth. In addition, robustness to failure of compute nodes, network, and filesystems is critical.We introduce a first-of-its-kind, massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), which couples a macro scale model spanning micrometer length- and millisecond time-scales with a micro scale model employing high-fidelity molecular dynamics (MD) simulations. MuMMI is a cohesive and transferable infrastructure designed for scalability and efficient execution on heterogeneous resources. A central workflow manager simultaneously allocates GPUs and CPUs while robustly handling failures in compute nodes, communication networks, and filesystems. A hierarchical scheduler controls GPU-accelerated MD simulations and in situ analysis.We present the various MuMMI components, including the macro model, GPU-accelerated MD, in situ analysis of MD data, machine learning selection module, a highly scalable hierarchical scheduler, and detail the central workflow manager that ties these modules together. In addition, we present performance data from our runs on Sierra, in which we validated MuMMI by investigating an experimentally intractable biological system: the dynamic interaction between RAS proteins and a plasma membrane. We used up to 4000 nodes of the Sierra supercomputer, concurrently utilizing over 16,000 GPUs and 176,000 CPU cores, and running up to 36,000 different tasks. This multiscale simulation includes about 120,000 MD simulations aggregating over 200 milliseconds, which is orders of magnitude greater than comparable studies.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {57},
numpages = {16},
keywords = {multiscale simulations, massively parallel, machine learning, heterogenous architecture, cancer research, adaptive simulations},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3466752.3480134,
author = {Pal, Subhankar and Amarnath, Aporva and Feng, Siying and O'Boyle, Michael and Dreslinski, Ronald and Dubach, Christophe},
title = {SparseAdapt: Runtime Control for Sparse Linear Algebra on a Reconfigurable Accelerator},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480134},
doi = {10.1145/3466752.3480134},
abstract = {Dynamic adaptation is a post-silicon optimization technique that adapts the hardware to workload phases. However, current adaptive approaches are oblivious to implicit phases that arise from operating on irregular data, such as sparse linear algebra operations. Implicit phases are short-lived and do not exhibit consistent behavior throughout execution. This calls for a high-accuracy, low overhead runtime mechanism for adaptation at a fine granularity. Moreover, adopting such techniques for reconfigurable manycore hardware, such as coarse-grained reconfigurable architectures (CGRAs), adds complexity due to synchronization and resource contention. We propose a lightweight machine learning-based adaptive framework called SparseAdapt. It enables low-overhead control of configuration parameters to tailor the hardware to both implicit (data-driven) and explicit (code-driven) phase changes. SparseAdapt is implemented within the runtime of a recently-proposed CGRA called Transmuter, which has been shown to deliver high performance for irregular sparse operations. SparseAdapt can adapt configuration parameters such as resource sharing, cache capacities, prefetcher aggressiveness, and dynamic voltage-frequency scaling (DVFS). Moreover, it can operate under the constraints of either (i) high energy-efficiency (maximal GFLOPS/W), or (ii) high power-performance (maximal GFLOPS3/W). We evaluate SparseAdapt with sparse matrix-matrix and matrix-vector multiplication (SpMSpM and SpMSpV) routines across a suite of uniform random, power-law and real-world matrices, in addition to end-to-end evaluation on two graph algorithms. SparseAdapt achieves similar performance on SpMSpM as the largest static configuration, with 5.3\texttimes{} better energy-efficiency. Furthermore, on both performance and efficiency, SparseAdapt is at most within 13% of an Oracle that adapts the configuration of each phase with global knowledge of the entire program execution. Finally, SparseAdapt is able to outperform the state-of-the-art approach for runtime reconfiguration by up to 2.9\texttimes{} in terms of energy-efficiency.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1005–1021},
numpages = {17},
keywords = {sparse linear algebra, reconfigurable accelerators, predictive models, machine learning, energy-efficient computing},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@article{10.1145/2000791.2000794,
author = {Anvik, John and Murphy, Gail C.},
title = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000794},
doi = {10.1145/2000791.2000794},
abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {10},
numpages = {35},
keywords = {task assignment, recommendation, machine learning, configuration assistance, Bug report triage}
}

@article{10.1504/ijcse.2020.105729,
author = {Cao, Yunpeng and Wang, Haifeng},
title = {Communication optimisation for intermediate data of MapReduce computing model},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {21},
number = {2},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2020.105729},
doi = {10.1504/ijcse.2020.105729},
abstract = {MapReduce is a typical computing model for processing and analysis of big data. MapReduce computing job produces a large amount of intermediate data after map phase. Massive intermediate data results in a large amount of intermediate data communication across rack switches in the Shuffle process of MapReduce computing model, this degrades the performance of heterogeneous cluster computing. In order to optimise the intermediate data communication performance of map-intensive jobs, the characteristics of pre-running scheduling information of MapReduce computing jobs are extracted, and job classification is realised by machine learning. The jobs of active intermediate data communication are mapped into a rack to keep the communication locality of intermediate data. The jobs with inactive communication are deployed to the nodes sorted by computing performance. The experimental results show that the proposed communication optimisation scheme has a good effect on Shuffle-intensive jobs, and can reach 4%–5%. In the case of larger amount of input data, the communication optimisation scheme is robust and can adapt to heterogeneous cluster. In the case of multi-user application scene, the intermediate data communication can be reduced by 4.1%.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {226–233},
numpages = {7},
keywords = {machine learning, intermediate data, communication optimisation, big data processing, MapReduce computing model}
}

@inproceedings{10.1145/3387168.3389115,
author = {Momotov, Alex and Xie, Xianghua},
title = {Determining Lead-Lag Structure between Sentiment Index and Stock Price Returns},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3389115},
doi = {10.1145/3387168.3389115},
abstract = {This research contrasts and compares the state-of-the-art techniques of the two approaches within the domain of news sentiment analysis, as well as, investigates a novel document encoding representation of the 'TF-IDF momentum matrix'. The presented lexicon-based methodology is centred around Loughran &amp; McDonald financial sentiment word lists and reaches 86.4% explained stock momentum variance, whereas the classification approach follows a thematic analysis pipeline implementing Latent Dirichlet Allocation and achieves that of 94.8%. As an additional element of model evaluation, the research implements Thermal Optimal Path method which relies on a dynamic programming approach for performance optimisation.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {28},
numpages = {7},
keywords = {Stock Price, Sentiment Analysis, Lexicon, Lda},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@inproceedings{10.5555/3540261.3541537,
author = {Peng, Jizong and Wang, Ping and Desrosiers, Christian and Pedersoli, Marco},
title = {Self-paced contrastive learning for semi-supervised medical image segmentation with meta-labels},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classification. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1276},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1145/3430984.3431022,
author = {Virk, Jitender Singh and Bathula, Deepti R.},
title = {Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431022},
doi = {10.1145/3430984.3431022},
abstract = {Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {145–153},
numpages = {9},
keywords = {transfer learning, pseudo-labelling, mixed-kernels neural networks, integrated gradients, image perturbations, domain-specific, Semi-supervised learning, Neural networks, CT scans},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@article{10.1504/ijipt.2020.110306,
author = {Ruan, Jin-Jun},
title = {Multi-channel scheduling analysis of dynamic data in wireless networks oriented big data},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {4},
issn = {1743-8209},
url = {https://doi.org/10.1504/ijipt.2020.110306},
doi = {10.1504/ijipt.2020.110306},
abstract = {In order to reduce the running time of dynamic data multi-channel scheduling in wireless networks, a multi-channel scheduling analysis method based on partially observable Markov decision process (POMDP) for dynamic data of wireless networks is proposed. Firstly, the data type and dynamic data transmission process in wireless network are analysed. According to the node's request arrival rate and service rate, the network state transition probability and observation probability are calculated. Load balancing is used as the performance optimisation target of dynamic data multi-channel in wireless network, calculate its performance function. By calculating the observation probability and performance function, the dynamic data multi-channel scheduling analysis for big data wireless networks is finally realised. The experimental results show that the proposed method has high data transmission efficiency and low packet loss rate, and the scheduling operation time is relatively short. The effectiveness of the proposed method is verified.},
journal = {Int. J. Internet Protoc. Technol.},
month = jan,
pages = {193–201},
numpages = {8},
keywords = {scheduling, multi-channel, dynamic data, wireless network, big data}
}

@inproceedings{10.1145/3458817.3476168,
author = {Li, Baolin and Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Gettings, Karen and Tiwari, Devesh},
title = {RIBBON: cost-effective and qos-aware deep learning model inference using a diverse pool of cloud computing instances},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476168},
doi = {10.1145/3458817.3476168},
abstract = {Deep learning model inference is a key service in many businesses and scientific discovery processes. This paper introduces Ribbon, a novel deep learning inference serving system that meets two competing objectives: quality-of-service (QoS) target and cost-effectiveness. The key idea behind Ribbon is to intelligently employ a diverse set of cloud computing instances (heterogeneous instances) to meet the QoS target and maximize cost savings. Ribbon devises a Bayesian Optimization-driven strategy that helps users build the optimal set of heterogeneous instances for their model inference service needs on cloud computing platforms - and, Ribbon demonstrates its superiority over existing approaches of inference serving systems using homogeneous instance pools. Ribbon saves up to 16% of the inference service cost for different learning models including emerging deep learning recommender system models and drug-discovery enabling models.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {24},
numpages = {13},
keywords = {quality-of-service, machine learning, inference serving, bayesian optimization},
location = {St. Louis, Missouri},
series = {SC '21}
}

@inproceedings{10.5555/3437539.3437716,
author = {Spieck, Jan and Wildermann, Stefan and Teich, J\"{u}rgen},
title = {Scenario-based soft real-time hybrid application mapping for MPSoCs},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {For soft real-time applications, a fixed mapping to a heterogeneous MPSoC architecture can lead to high energy consumption and even deadline misses if tasks have input-dependent execution times. Here, specialized mappings are required that, e.g., map tasks with high execution times for the current input to resources with high computational power as they else may cause deadline misses. However, optimizing mappings for both energy and latency at run time is too compute-intensive. As a remedy, we propose a hybrid application mapping technique suited for black-box applications, i.e., no information about functional behaviors is available. It is based on clustering input data evoking similar workloads into so-called workload scenarios. At design time, we optimize the scenario distribution and their associated mappings regarding energy consumption and latency by an iterative design space exploration. At run time, a machine-learning-based runtime manager first identifies the scenario of the current input by monitoring its non-functional execution properties. Based on these identified scenarios, a mapping for subsequent data processing is selected so that missed deadlines and the energy are minimized. Evaluations performed based on two dynamic applications show that the proposed hybrid application mapping procedure consistently outperforms state-of-the-art mapping approaches with regard to both deadline misses and energy consumption.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {177},
numpages = {6},
keywords = {soft real-time, run-time manager, machine learning, hybrid application mapping, MPSoC},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.5555/1756006.1859896,
author = {Ghiasi-Shirazi, Kamaledin and Safabakhsh, Reza and Shamsi, Mostafa},
title = {Learning Translation Invariant Kernels for Classification},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {Appropriate selection of the kernel function, which implicitly defines the feature space of an algorithm, has a crucial role in the success of kernel methods. In this paper, we consider the problem of optimizing a kernel function over the class of translation invariant kernels for the task of binary classification. The learning capacity of this class is invariant with respect to rotation and scaling of the features and it encompasses the set of radial kernels. We show that how translation invariant kernel functions can be embedded in a nested set of sub-classes and consider the kernel learning problem over one of these sub-classes. This allows the choice of an appropriate sub-class based on the problem at hand. We use the criterion proposed by Lanckriet et al. (2004) to obtain a functional formulation for the problem. It will be proven that the optimal kernel is a finite mixture of cosine functions. The kernel learning problem is then formulated as a semi-infinite programming (SIP) problem which is solved by a sequence of quadratically constrained quadratic programming (QCQP) sub-problems. Using the fact that the cosine kernel is of rank two, we propose a formulation of a QCQP sub-problem which does not require the kernel matrices to be loaded into memory, making the method applicable to large-scale problems. We also address the issue of including other classes of kernels, such as individual kernels and isotropic Gaussian kernels, in the learning process. Another interesting feature of the proposed method is that the optimal classifier has an expansion in terms of the number of cosine kernels, instead of support vectors, leading to a remarkable speedup at run-time. As a by-product, we also generalize the kernel trick to complex-valued kernel functions. Our experiments on artificial and real-world benchmark data sets, including the USPS and the MNIST digit recognition data sets, show the usefulness of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {1353–1390},
numpages = {38}
}

@inproceedings{10.1007/978-3-030-74251-5_18,
author = {Peeters, Sven and Melnikov, Vitalik and H\"{u}llermeier, Eyke},
title = {Performance Prediction for Hardware-Software Configurations: A Case Study for Video Games},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_18},
doi = {10.1007/978-3-030-74251-5_18},
abstract = {Performance prediction for hardware-software configurations is a relevant and practically important problem. With an increasing availability of data in the form of performance measurements, this problem becomes amenable to machine learning, i.e., the data-driven construction of predictive models. In this paper, we propose a learning method that is specifically tailored to the task of performance prediction and takes two important characteristics of this problem into account: (i) prior knowledge in the form of monotonicity constraints, suggesting that certain properties of hard- or software can influence performance only positively or negatively, and (ii) strong differences in the precision and reliability of performance measurements available as training data. We evaluate our method on a real-world dataset from the domain of performance prediction in video games, which we specifically collected for this purpose.},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {222–234},
numpages = {13},
keywords = {Monotonicity, Imprecise data, Performance prediction},
location = {Porto, Portugal}
}

@inproceedings{10.1145/3195555.3195562,
author = {Mittal, Samir},
title = {Self-organizing infrastructure for machine (deep) learning at scale},
year = {2018},
isbn = {9781450357401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195555.3195562},
doi = {10.1145/3195555.3195562},
abstract = {Building machine (deep) learning1 systems is hard. Computation requirements grow non-linearly with the complexity of the task at hand creating acute challenges relating to data dimensionality, complex model development, slow experiments, and scalability of production deployments. The bulk of the ML/DL effort is consumed in infrastructure and data management. Automating such workflows has become the focus of recent research activity, so as to make ML/DL systems universally accessible. We extend these paradigms by infusing domain knowledge for infrastructure self-management. Key elements include understanding application design intent, fingerprinting the neural network for its computational, data and convergence properties, optimizing the implementation to achieving workload intent, and accelerating the neural network implementation in real-time hardware implementation. Keys to success require offline behavioural modelling coupled with online dynamic adaptation, made possible by the use of cognitive algorithms that accumulate knowledge in a dynamic and continuously evolving knowledgebase. In this way, we use machine learning to automate AI infrastructure management to minimize human engineering, and significantly accelerating application performance.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for Cognitive Services},
pages = {5–8},
numpages = {4},
keywords = {machine learning, deep learning, cognitive infrastructure},
location = {Gothenburg, Sweden},
series = {SE4COG '18}
}

@inproceedings{10.1109/WAIN52551.2021.00026,
author = {Muccini, Henry and Vaidhyanathan, Karthik},
title = {Software Architecture for ML-based Systems: What Exists and What Lies Ahead},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00026},
doi = {10.1109/WAIN52551.2021.00026},
abstract = {The increasing usage of machine learning (ML) coupled with the software architectural challenges of the modern era has resulted in two broad research areas: i) software architecture for ML-based systems, which focuses on developing architectural techniques for better developing ML-based software systems, and ii) ML for software architectures, which focuses on developing ML techniques to better architect traditional software systems. In this work, we focus on the former side of the spectrum with a goal to highlight the different architecting practices that exist in the current scenario for architecting ML-based software systems. We identify four key areas of software architecture that need the attention of both the ML and software practitioners to better define a standard set of practices for architecting ML-based software systems. We base these areas in light of our experience in architecting an ML-based software system for solving queuing challenges in one of the largest museums in Italy.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {121–128},
numpages = {8},
location = {Madrid, Spain}
}

@article{10.1007/s11263-018-1112-4,
author = {Zhang, Dingwen and Han, Junwei and Zhao, Long and Meng, Deyu},
title = {Leveraging Prior-Knowledge for Weakly Supervised Object Detection Under a Collaborative Self-Paced Curriculum Learning Framework},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {4},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1112-4},
doi = {10.1007/s11263-018-1112-4},
abstract = {Weakly supervised object detection is an interesting yet challenging research topic in computer vision community, which aims at learning object models to localize and detect the corresponding objects of interest only under the supervision of image-level annotation. For addressing this problem, this paper establishes a novel weakly supervised learning framework to leverage both the instance-level prior-knowledge and the image-level prior-knowledge based on a novel collaborative self-paced curriculum learning (C-SPCL) regime. Under the weak supervision, C-SPCL can leverage helpful prior-knowledge throughout the whole learning process and collaborate the instance-level confidence inference with the image-level confidence inference in a robust way. Comprehensive experiments on benchmark datasets demonstrate the superior capacity of the proposed C-SPCL regime and the proposed whole framework as compared with state-of-the-art methods along this research line.},
journal = {Int. J. Comput. Vision},
month = apr,
pages = {363–380},
numpages = {18},
keywords = {Weakly supervised learning, Self-paced larning, Object detection}
}

@inbook{10.5555/3454287.3455282,
author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
title = {Data parameters: a new family of parameters for learning a differentiable curriculum},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFAR10, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, without any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {995},
numpages = {11}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Combinatorial optimization, Multi-armed bandits, Machine learning, A/B testing, Continuous experimentation}
}

@article{10.1016/j.engappai.2018.06.002,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Yu, Yang and Yuan, Bo-Wen and Tang, Jia-Fu},
title = {Integration of an improved dynamic ensemble selection approach to enhance one-vs-one scheme},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.002},
doi = {10.1016/j.engappai.2018.06.002},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {43–53},
numpages = {11},
keywords = {Multi-class classification, Decomposition strategy, One-vs-one, Heterogeneous ensemble, Dynamic selection}
}

@inproceedings{10.1145/3350755.3400275,
author = {Dinh, Grace and Demmel, James},
title = {Communication-Optimal Tilings for Projective Nested Loops with Arbitrary Bounds},
year = {2020},
isbn = {9781450369350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350755.3400275},
doi = {10.1145/3350755.3400275},
abstract = {Abstract Reducing communication - either between levels of a memory hierarchy or between processors over a network - is a key component of performance optimization (in both time and energy) for many nested loop problems, including dense linear algebra, particle interactions, and machine learning. Previous tiling based approaches for these problems have been used to find both lower bounds on the communication required to execute them and optimal rearrangements, or blockings, to attain such lower bounds. However, such general approaches have typically assumed the problem sizes are large, an assumption that is often not met in practice. In this paper, we provide an efficient way to both find and obtain, via an appropriate, efficiently constructible blocking, communication lower bounds and matching tilings which attain these lower bounds for nested loop programs with arbitrary loop bounds that operate on multidimensional arrays in the projective case, where the array indices are subsets of the loop indices. Our approach works on all such problems, regardless of dimensionality, size, memory access patterns, or number of arrays.},
booktitle = {Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {523–525},
numpages = {3},
keywords = {optimal tilings, communication-avoiding algorithms, cache complexity},
location = {Virtual Event, USA},
series = {SPAA '20}
}

@inproceedings{10.5555/3489146.3489168,
author = {Liang, Chieh-Jan Mike and Xue, Hui and Yang, Mao and Zhou, Lidong and Zhu, Lifei and Li, Zhao Lucis and Wang, Zibo and Chen, Qi and Zhang, Quanlu and Liu, Chuanjie and Dai, Wenjun},
title = {AutoSys: the design and operation of learning-augmented systems},
year = {2020},
isbn = {978-1-939133-14-4},
publisher = {USENIX Association},
address = {USA},
abstract = {Although machine learning (ML) and deep learning (DL) provide new possibilities into optimizing system design and performance, taking advantage of this paradigm shift requires more than implementing existing ML/DL algorithms. This paper reports our years of experience in designing and operating several production learning-augmented systems at Microsoft. AutoSys is a framework that unifies the development process, and it addresses common design considerations including ad-hoc and nondeterministic jobs, learning-induced system failures, and programming extensibility. Furthermore, this paper demonstrates the benefits of adopting AutoSys with measurements from one production system, Web Search. Finally, we share long-term lessons stemmed from unforeseen implications that have surfaced over the years of operating learning-augmented systems.},
booktitle = {Proceedings of the 2020 USENIX Conference on Usenix Annual Technical Conference},
articleno = {22},
numpages = {14},
series = {USENIX ATC'20}
}

@inbook{10.5555/3454287.3455635,
author = {Leqi, Liu and Prasad, Adarsh and Ravikumar, Pradeep},
title = {On human-aligned risk minimization},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The statistical decision theoretic foundations of modern machine learning have largely focused on the minimization of the expectation of some loss function for a given task. However, seminal results in behavioral economics have shown that human decision-making is based on different risk measures than the expectation of any given loss function. In this paper, we pose the following simple question: in contrast to minimizing expected loss, could we minimize a better human-aligned risk measure? While this might not seem natural at first glance, we analyze the properties of such a revised risk measure, and surprisingly show that it might also better align with additional desiderata like fairness that have attracted considerable recent attention. We focus in particular on a class of human-aligned risk measures inspired by cumulative prospect theory. We empirically study these risk measures, and demonstrate their improved performance on desiderata such as fairness, in contrast to the traditional workhorse of expected loss minimization.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1348},
numpages = {10}
}

@inproceedings{10.1109/GLOBECOM42002.2020.9322515,
author = {Ngo, Hieu and Fang, Hua and Wang, Honggang},
title = {Deep Learning-based Adaptive Beamforming for mmWave Wireless Body Area Network},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM42002.2020.9322515},
doi = {10.1109/GLOBECOM42002.2020.9322515},
abstract = {Artificial intelligence (AI) is becoming a mainstream for telecommunication industry. With the utilization of millimeter-wave in 5G network, it becomes feasible to use beamforming techniques for on-body sensors in Wireless Body Area Network (WBAN) applications. Thus, there is a need for developing beamforming algorithms that can optimize WBAN network performance and a realistic dataset that can be used for training, testing, and benchmarking of the algorithms. Thus, we propose a dataset generation method for mmWave WBAN that utilizes computer vision and an adaptive deep learning-based algorithm for performance optimization of mmWave WBAN beamforming. Two major ideas are proposed: First, collecting human poses from estimation of 3D human poses in videos and generating more realistic poses using generative adversarial nets (GAN) are adopted; second, a GAN aims to predict the next beamforming directions using the previous set of directions as inputs. With available labeled human pose videos, the WBAN dataset we generate provides a sufficient amount of samples for training, testing, and benchmarking of beamforming algorithms. Additionally, the proposed adaptive beamforming algorithm does not require any intrusive data gathering methods. Our numerical studies show the advantages of our proposed approaches.},
booktitle = {GLOBECOM 2020 - 2020 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Taipei, Taiwan}
}

@inproceedings{10.1109/IPDPS.2014.59,
author = {Muralidharan, Saurav and Shantharam, Manu and Hall, Mary and Garland, Michael and Catanzaro, Bryan},
title = {Nitro: A Framework for Adaptive Code Variant Tuning},
year = {2014},
isbn = {9781479938001},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPS.2014.59},
doi = {10.1109/IPDPS.2014.59},
abstract = {Auto tuning systems intelligently navigate a search space of possible implementations of a computation to find the implementation(s) that best meets a specific optimization criteria, usually performance. This paper describes Nitro, a programmer-directed auto tuning framework that facilitates tuning of code variants, or alternative implementations of the same computation. Nitro provides a library interface that permits programmers to express code variants along with meta-information that aids the system in selecting among the set of variants at run time. Machine learning is employed to build a model through training on this meta-information, so that when a new input is presented, Nitro can consult the model to select the appropriate variant. In experiments with five real-world irregular GPU benchmarks from sparse numerical methods, graph computations and sorting, Nitro-tuned variants achieve over 93% of the performance of variants selected through exhaustive search. Further, we describe optimizations and heuristics in Nitro that substantially reduce training time and other overheads.},
booktitle = {Proceedings of the 2014 IEEE 28th International Parallel and Distributed Processing Symposium},
pages = {501–512},
numpages = {12},
keywords = {Autotuning, Performance Optimization, GPUs},
series = {IPDPS '14}
}

@article{10.1016/j.neucom.2014.06.096,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng and Wang, Yanjiang and Lu, Ke},
title = {Manifold regularized kernel logistic regression for web image annotation},
year = {2016},
issue_date = {Jan 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {172},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.06.096},
doi = {10.1016/j.neucom.2014.06.096},
journal = {Neurocomput.},
month = jan,
pages = {3–8},
numpages = {6},
keywords = {Manifold regularization, Kernel logistic regression, Laplacian eigenmaps, Semi-supervised learning, Image annotation}
}

@article{10.1016/j.ins.2018.06.014,
author = {Ma, Zilu and Liu, Shiqi and Meng, Deyu and Zhang, Yong and Lo, SioLong and Han, Zhi},
title = {On Convergence Properties of Implicit Self-paced Objective},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {462},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.06.014},
doi = {10.1016/j.ins.2018.06.014},
journal = {Inf. Sci.},
month = sep,
pages = {132–140},
numpages = {9},
keywords = {99-00, 00-01, Convergence, Non-convex optimization, Machine learning, Self-paced learning}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@article{10.3233/JCM-215225,
author = {Song, Caili and Liang, Bin and Li, Jiao},
title = {Resource clustering algorithm for cloud data centers},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {5},
issn = {1472-7978},
url = {https://doi.org/10.3233/JCM-215225},
doi = {10.3233/JCM-215225},
abstract = {Recently, the virtual machine deployment algorithm uses physical machine less or consumes higher energy in data centers, resulting in declined service quality of cloud data centers or rising operational costs, which leads to a decrease in cloud service provider’s earnings finally. According to this situation, a resource clustering algorithm for cloud data centers is proposed. This algorithm systematically analyzes the cloud data center model and physical machine’s use ratio, establishes the dynamic resource clustering rules through k-means clustering algorithm, and deploys the virtual machines based on clustering results, so as to promote the use ratio of physical machine and bring down energy consumption in cloud data centers. The experimental results indicate that, regarding the compute-intensive virtual machines in cloud data centers, compared to contrast algorithm, the physical machine’s use ratio of this algorithm is improved by 12% on average, and its energy consumption in cloud data center is lowered by 15% on average. Regarding the general-purpose virtual machines in cloud data center, compared to contrast algorithm, the physical machine’s use ratio is improved by 14% on average, and its energy consumption in cloud data centers is lowered by 12% on average. Above results demonstrate that this method shows a good effect in the resource management of cloud data centers, which may provide reference to some extent.},
journal = {J. Comp. Methods in Sci. and Eng.},
month = jan,
pages = {1575–1585},
numpages = {11},
keywords = {Virtual machine deployment, Machine learning, Clustering algorithm, Cloud data centers}
}

@article{10.1155/2017/3405463,
author = {Abuassba, Adnan O. M. and Zhang, Dezheng and Luo, Xiong and Shaheryar, Ahmad and Ali, Hazrat and Aric\`{o}, Pietro},
title = {Improving Classification Performance through an Advanced Ensemble Based Heterogeneous Extreme Learning Machines},
year = {2017},
issue_date = {2017},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2017},
issn = {1687-5265},
url = {https://doi.org/10.1155/2017/3405463},
doi = {10.1155/2017/3405463},
abstract = {Extreme Learning Machine (ELM) is a fast-learning algorithm for a single-hidden layer feedforward neural network (SLFN). It often has good generalization performance. However, there are chances that it might overfit the training data due to having more hidden nodes than needed. To address the generalization performance, we use a heterogeneous ensemble approach. We propose an Advanced ELM Ensemble (AELME) for classification, which includes Regularized-ELM, L2-norm-optimized ELM (ELML2), and Kernel-ELM. The ensemble is constructed by training a randomly chosen ELM classifier on a subset of training data selected through random resampling. The proposed AELM-Ensemble is evolved by employing an objective function of increasing diversity and accuracy among the final ensemble. Finally, the class label of unseen data is predicted using majority vote approach. Splitting the training data into subsets and incorporation of heterogeneous ELM classifiers result in higher prediction accuracy, better generalization, and a lower number of base classifiers, as compared to other models (Adaboost, Bagging, Dynamic ELM ensemble, data splitting ELM ensemble, and ELM ensemble). The validity of AELME is confirmed through classification on several real-world benchmark datasets.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {11}
}

@article{10.1007/s11227-017-2011-0,
author = {Mih\u{a}\'{z}Escu, Marian Cristian and Popescu, Paul \'{z}Tefan and Popescu, Elvira},
title = {Data analysis on social media traces for detection of "spam" and "don't care" learners},
year = {2017},
issue_date = {October   2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {73},
number = {10},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-017-2011-0},
doi = {10.1007/s11227-017-2011-0},
abstract = {Classification methods are becoming more and more useful as part of the standard data analyst's toolbox in many application domains. The specific data and domain characteristics of social media tools used in online educational contexts present the challenging problem of training high-quality classifiers that bring important insight into activity patterns of learners. Currently, standard and also very successful model for classification tasks is represented by decision trees. In this paper, we introduce a custom-designed data analysis pipeline for predicting "spam" and "don't care" learners from eMUSE online educational environment. The trained classifiers rely on social media traces as independent variables and on final grade of the learner as dependent variables. Current analysis evaluates performed activities of learners and the similarity of two derived data models. Experiments performed on social media traces from five years and 285 learners show satisfactory classification results that may be further used in productive environment. Accurate identification of "spam" and "don't care" users may have further a great impact on producing better classification models for the rest of the "regular" learners.},
journal = {J. Supercomput.},
month = oct,
pages = {4302–4323},
numpages = {22},
keywords = {Spam learners, Social media tools, Ranking, Online educational environment, Classification}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Variability modeling, Variability management, Software product family, Classification}
}

@article{10.5555/3288992.3288997,
author = {Vinci, Giuseppe and Ventura, Val\'{e}rie and Smith, Matthew A. and Kass, Robert E.},
title = {Adjusted regularization of cortical covariance},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {45},
number = {2},
issn = {0929-5313},
abstract = {It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an L1$L_{1}$ penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4.},
journal = {J. Comput. Neurosci.},
month = oct,
pages = {83–101},
numpages = {19},
keywords = {Penalized maximum likelihood estimation, Macaque visual cortex, High-dimensional estimation, Graphical lasso, Gaussian graphical model, Functional connectivity, False discovery rate, Bayesian inference}
}

@article{10.1016/j.asoc.2016.05.020,
author = {Sachdeva, Jainy and Kumar, Vinod and Gupta, Indra and Khandelwal, Niranjan and Ahuja, Chirag Kamal},
title = {A package-SFERCB-"Segmentation, feature extraction, reduction and classification analysis by both SVM and ANN for brain tumors"},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.020},
doi = {10.1016/j.asoc.2016.05.020},
abstract = {An interactive computer aided dignostic (CAD) system for assisting inexperience young radiologists is developed. The difficulty in brain tumors classification is due to similar size, shape, location, hetrogeniety, presence of oedema, cystic and isointense regions has been the key feature of this research. Genetic Algorithm is employed as it is an easy concept and is well understood by radiologists without going in much depth of engineering.Display Omitted Brain tumors as segmented regions of interests (SROIs) by content based active contour model (CBAC).Feature extraction-intensity and texture based features.Feature reduction by Genetic Algorithm.Classification by Hybrid Models-GA-SVM and GA-ANN. The objective of this experimentation is to develop an interactive CAD system for assisting radiologists in multiclass brain tumor classification. The study is performed on a diversified dataset of 428 post contrast T1-weighted MR images of 55 patients and publically available dataset of 260 post contrast T1-weighted MR images of 10 patients. The first dataset includes primary brain tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), childhood tumor-Medulloblastoma (MED) and Meningioma (MEN), along with secondary tumor-Metastatic (MET). The second dataset consists of Astrocytoma (AS), Low Grade Glioma (LGL) and Meningioma (MEN). The tumor regions are marked by content based active contour (CBAC) model. The regions are than saved as segmented regions of interest (SROIs). 71 intensity and texture feature set is extracted from these SROIs. The features are specifically selected based on the pathological details of brain tumors provided by the radiologist. Genetic Algorithm (GA) selects the set of optimal features from this input set. Two hybrid machine learning models are implemented using GA with support vector machine (SVM) and artificial neural network (ANN) (GA-SVM and GA-ANN) and are tested on two different datasets. GA-SVM is proposed for finding preliminary probability in identifying tumor class and GA-ANN is used for confirmation of accuracy. Test results of the first dataset show that the GA optimization technique has enhanced the overall accuracy of SVM from 79.3% to 91.7% and of ANN from 75.6% to 94.9%. Individual class accuracies delivered by GA-SVM are: AS-89.8%, GBM-83.3%, MED-95.6%, MEN-91.8%, and MET-97.1%. Individual class accuracies delivered by GA-ANN classifier are: AS-96.6%, GBM-86.6%, MED-93.3%, MEN-96%, MET-100%. Similar results are obtained for the second dataset. The overall accuracy of SVM has increased from 80.8% to 89% and that of ANN has increased from 77.5% to 94.1%. Individual class accuracies delivered by GA-SVM are: AS-85.3%, LGL-88.8%, MEN-93%. Individual class accuracies delivered by GA-ANN classifier are: AS-92.6%, LGL-94.4%, MED-95.3%. It is observed from the experiments that GA-ANN classifier has provided better results than GA-SVM. Further, it is observed that along with providing finer results, GA-SVM provides advantage in speed whereas GA-ANN provides advantage in accuracy. The combined results from both the classifiers will benefit the radiologists in forming a better decision for classifying brain tumors.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {151–167},
numpages = {17},
keywords = {Genetic Algorithm (GA), GA-SVM, GA-ANN, Brain tumors}
}

@article{10.1145/3291053,
author = {Mammadli, Rahim and Wolf, Felix and Jannesari, Ali},
title = {The Art of Getting Deep Neural Networks in Shape},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3291053},
doi = {10.1145/3291053},
abstract = {Training a deep neural network (DNN) involves selecting a set of hyperparameters that define the network topology and influence the accuracy of the resulting network. Often, the goal is to maximize prediction accuracy on a given dataset. However, non-functional requirements of the trained network -- such as inference speed, size, and energy consumption -- can be very important as well. In this article, we aim to automate the process of selecting an appropriate DNN topology that fulfills both functional and non-functional requirements of the application. Specifically, we focus on tuning two important hyperparameters, depth and width, which together define the shape of the resulting network and directly affect its accuracy, speed, size, and energy consumption. To reduce the time needed to search the design space, we train a fraction of DNNs and build a model to predict the performances of the remaining ones. We are able to produce tuned ResNets, which are up to 4.22 times faster than original depth-scaled ResNets on a batch of 128 images while matching their accuracy.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {62},
numpages = {21},
keywords = {parallel processing, computer vision, Deep neural networks}
}

@inproceedings{10.1007/978-3-030-99372-6_7,
author = {Qiu, Shenghao and You, Liang and Wang, Zheng},
title = {Optimizing Sparse Matrix Multiplications for Graph Neural Networks},
year = {2021},
isbn = {978-3-030-99371-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-99372-6_7},
doi = {10.1007/978-3-030-99372-6_7},
abstract = {Graph neural networks (GNNs) are emerging as a powerful technique for modeling graph structures. Due to the sparsity of real-world graph data, GNN performance is limited by extensive sparse matrix multiplication (SpMM) operations involved in computation. While the right sparse matrix storage format varies across input data, existing deep learning frameworks employ a single, static storage format, leaving much room for improvement. This paper investigates how the choice of sparse matrix storage formats affect the GNN performance. We observe that choosing a suitable sparse matrix storage format can significantly improve the GNN training performance, but the right format depends on the input workloads and can change as the GNN iterates over the input graph. We then develop a predictive model to dynamically choose a sparse matrix storage format to be used by a GNN layer based on the input matrices. Our model is first trained offline using training matrix samples, and the trained model can be applied to any input matrix and GNN kernels with SpMM computation. We implement our approach on top of PyTorch and apply it to 5 representative GNN models running on a multi-core CPU using real-life and synthetic datasets. Experimental results show that our approach gives an average speedup of 1.17x (up&nbsp;to 3x) for GNN running time.},
booktitle = {Languages and Compilers for Parallel Computing: 34th International Workshop, LCPC 2021, Newark, DE, USA, October 13–14, 2021, Revised Selected Papers},
pages = {101–117},
numpages = {17},
location = {Newark, DE, USA}
}

@article{10.1016/j.patcog.2018.11.030,
author = {Wang, Xiaohong and Jiang, Xudong and Ren, Jianfeng},
title = {Blood vessel segmentation from fundus image by a cascade classification framework},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.030},
doi = {10.1016/j.patcog.2018.11.030},
journal = {Pattern Recogn.},
month = apr,
pages = {331–341},
numpages = {11},
keywords = {Dimensionality reduction, Cascade classification, Retinal vessel segmentation, Fundus image}
}

@article{10.1007/s10586-017-1108-9,
author = {Ilavarasi, A. K. and Sathiyabhama, B.},
title = {An evolutionary feature set decomposition based anonymization for classification workloads: Privacy Preserving Data Mining},
year = {2017},
issue_date = {Dec 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1108-9},
doi = {10.1007/s10586-017-1108-9},
abstract = {Privacy has become an important concern while publishing micro data about a population. The emerging area called privacy preserving data mining (PPDM) focus on individual privacy without compromising data mining results. An adversarial exploitation of published data poses a risk of information disclosure about individuals. On the other hand, imposing privacy constraints on the data results in substantial information loss and compromises the legitimate data analysis. Motivated by the increasing growth of PPDM algorithms, we first investigate the privacy implications and the crosscutting issues between privacy versus utility of data. We present a privacy model that embeds the anonymization procedure in to a learning algorithm and this has mitigated the additional overheads imposed on data mining tasks. Our primary concern about PPDM is that the utility of data should not be compromised by the transformation applied. Different data mining classification workloads are analyzed with the proposed anonymization procedure for any side effects incurred. It is shown empirically that classification accuracy obtained for most of the datasets outperforms the results obtained with original dataset.},
journal = {Cluster Computing},
month = dec,
pages = {3515–3525},
numpages = {11},
keywords = {Privacy, Evolutionary partitioning, Decomposition, Data mining, Classification, Anonymization}
}

@inproceedings{10.5555/3504035.3504869,
author = {Fan, Xin and Liu, Risheng and Huyan, Kang and Feng, Yuyao and Luo, Zhongxuan},
title = {Self-reinforced cascaded regression for face alignment},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {834},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s11227-018-2300-2,
author = {Garea, Alberto S. and Heras, Dora B. and Arg\"{u}ello, Francisco},
title = {Caffe CNN-based classification of hyperspectral images on GPU},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {3},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2300-2},
doi = {10.1007/s11227-018-2300-2},
abstract = {Deep learning techniques based on Convolutional Neural Networks (CNNs) are extensively used for the classification of hyperspectral images. These techniques present high computational cost. In this paper, a GPU (Graphics Processing Unit) implementation of a spatial-spectral supervised classification scheme based on CNNs and applied to remote sensing datasets is presented. In particular, two deep learning libraries, Caffe and CuDNN, are used and compared. In order to achieve an efficient GPU projection, different techniques and optimizations have been applied. The implemented scheme comprises Principal Component Analysis (PCA) to extract the main features, a patch extraction around each pixel to take the spatial information into account, one convolutional layer for processing the spectral information, and fully connected layers to perform the classification. To improve the initial GPU implementation accuracy, a second convolutional layer has been added. High speedups are obtained together with competitive classification accuracies.},
journal = {J. Supercomput.},
month = mar,
pages = {1065–1077},
numpages = {13},
keywords = {Hyperspectral, GPU, Deep learning, CuDNN, Convolutional neural network, Classification, Caffe}
}

@article{10.1007/s10489-020-01730-3,
author = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
title = {Query set centered sparse projection learning for set based image classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01730-3},
doi = {10.1007/s10489-020-01730-3},
abstract = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
journal = {Applied Intelligence},
month = oct,
pages = {3400–3411},
numpages = {12},
keywords = {Discriminate subspace learning, Set based image classification, Sparse projection learning, Query set}
}

@inproceedings{10.5555/3045390.3045604,
author = {Diamos, Gregory and Sengupta, Shubho and Catanzaro, Bryan and Chrzanowski, Mike and Coates, Adam and Elsen, Erich and Engel, Jesse and Hannun, Awni and Satheesh, Sanjeev},
title = {Persistent RNNs: stashing recurrent weights on-chip},
year = {2016},
publisher = {JMLR.org},
abstract = {This paper introduces a new technique for mapping Deep Recurrent Neural Networks (RNN) efficiently onto GPUs. We show how it is possible to achieve substantially higher computational throughput at low mini-batch sizes than direct implementations of RNNs based on matrix multiplications. The key to our approach is the use of persistent computational kernels that exploit the GPU's inverted memory hierarchy to reuse network weights over multiple timesteps. Our initial implementation sustains 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU. This provides a 16\texttimes{} reduction in activation memory footprint, enables model training with 12\texttimes{} more parameters on the same hardware, allows us to strongly scale RNN training to 128 GPUs, and allows us to efficiently explore end-to-end speech recognition models with over 100 layers.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {2024–2033},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.1016/j.neucom.2019.08.002,
author = {Li, Zhenglai and Tang, Chang and Chen, Jiajia and Wan, Cheng and Yan, Weiqing and Liu, Xinwang},
title = {Diversity and consistency learning guided spectral embedding for multi-view clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.002},
doi = {10.1016/j.neucom.2019.08.002},
journal = {Neurocomput.},
month = dec,
pages = {128–139},
numpages = {12},
keywords = {99-00, 00-01, Diversity and consistency learning, Spectral embedding, Multi-view clustering}
}

@article{10.1007/s10618-016-0475-9,
author = {Garcia, Lu\'{\i}s P. and Lorena, Ana C. and Matwin, Stan and Carvalho, Andr\'{e} C.},
title = {Ensembles of label noise filters: a ranking approach},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-016-0475-9},
doi = {10.1007/s10618-016-0475-9},
abstract = {Label noise can be a major problem in classification tasks, since most machine learning algorithms rely on data labels in their inductive process. Thereupon, various techniques for label noise identification have been investigated in the literature. The bias of each technique defines how suitable it is for each dataset. Besides, while some techniques identify a large number of examples as noisy and have a high false positive rate, others are very restrictive and therefore not able to identify all noisy examples. This paper investigates how label noise detection can be improved by using an ensemble of noise filtering techniques. These filters, individual and ensembles, are experimentally compared. Another concern in this paper is the computational cost of ensembles, once, for a particular dataset, an individual technique can have the same predictive performance as an ensemble. In this case the individual technique should be preferred. To deal with this situation, this study also proposes the use of meta-learning to recommend, for a new dataset, the best filter. An extensive experimental evaluation of the use of individual filters, ensemble filters and meta-learning was performed using public datasets with imputed label noise. The results show that ensembles of noise filters can improve noise filtering performance and that a recommendation system based on meta-learning can successfully recommend the best filtering technique for new datasets. A case study using a real dataset from the ecological niche modeling domain is also presented and evaluated, with the results validated by an expert.},
journal = {Data Min. Knowl. Discov.},
month = sep,
pages = {1192–1216},
numpages = {25},
keywords = {Recommendation system, Noise ranking, Noise filters, Label noise, Ensemble filters}
}

@article{10.1016/j.jpdc.2019.08.008,
author = {Mart\'{\i}nez, Daniel and Brewer, Wesley and Strelzoff, Andrew and Wilson, Andrew and Wade, Daniel},
title = {Rotorcraft virtual sensors via deep regression},
year = {2020},
issue_date = {Jan 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.08.008},
doi = {10.1016/j.jpdc.2019.08.008},
journal = {J. Parallel Distrib. Comput.},
month = jan,
pages = {114–126},
numpages = {13},
keywords = {Evolutionary Optimization, Deep Neural Networks, High Performance Computing, Deep Learning, Virtual Sensors}
}

@article{10.1016/j.patrec.2018.05.011,
author = {Ayyalasomayajula, Kalyan Ram and Malmberg, Filip and Brun, Anders},
title = {PDNet: Semantic segmentation integrated with a primal-dual network for document binarization},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2018.05.011},
doi = {10.1016/j.patrec.2018.05.011},
journal = {Pattern Recogn. Lett.},
month = apr,
pages = {52–60},
numpages = {9},
keywords = {Primal-dual scheme, Energy minimization, Convolutional neural networks, Semantic segmentation, Binarization}
}

@inproceedings{10.1145/2807591.2807655,
author = {Sengupta, Dipanjan and Song, Shuaiwen Leon and Agarwal, Kapil and Schwan, Karsten},
title = {GraphReduce: processing large-scale graphs on accelerator-based systems},
year = {2015},
isbn = {9781450337236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2807591.2807655},
doi = {10.1145/2807591.2807655},
abstract = {Recent work on real-world graph analytics has sought to leverage the massive amount of parallelism offered by GPU devices, but challenges remain due to the inherent irregularity of graph algorithms and limitations in GPU-resident memory for storing large graphs. We present GraphReduce, a highly efficient and scalable GPU-based framework that operates on graphs that exceed the device's internal memory capacity. GraphReduce adopts a combination of edge- and vertex-centric implementations of the Gather-Apply-Scatter programming model and operates on multiple asynchronous GPU streams to fully exploit the high degrees of parallelism in GPUs with efficient graph data movement between the host and device. GraphReduce-based programming is performed via device functions that include gatherMap, gatherReduce, apply, and scatter, implemented by programmers for the graph algorithms they wish to realize. Extensive experimental evaluations for a wide variety of graph inputs and algorithms demonstrate that GraphReduce significantly outperforms other competing out-of-memory approaches.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {28},
numpages = {12},
keywords = {performance optimization, graph analytics, data movement optimization, big data, GPGPU},
location = {Austin, Texas},
series = {SC '15}
}

@inproceedings{10.1145/1835804.1835911,
author = {Prenger, Ryan J. and Lemmond, Tracy D. and Varshney, Kush R. and Chen, Barry Y. and Hanley, William G.},
title = {Class-specific error bounds for ensemble classifiers},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835911},
doi = {10.1145/1835804.1835911},
abstract = {The generalization error, or probability of misclassification, of ensemble classifiers has been shown to be bounded above by a function of the mean correlation between the constituent (i.e., base) classifiers and their average strength. This bound suggests that increasing the strength and/or decreasing the correlation of an ensemble's base classifiers may yield improved performance under the assumption of equal error costs. However, this and other existing bounds do not directly address application spaces in which error costs are inherently unequal. For applications involving binary classification, Receiver Operating Characteristic (ROC) curves, performance curves that explicitly trade off false alarms and missed detections, are often utilized to support decision making. To address performance optimization in this context, we have developed a lower bound for the entire ROC curve that can be expressed in terms of the class-specific strength and correlation of the base classifiers.We present empirical analyses demonstrating the efficacy of these bounds in predicting relative classifier performance. In addition, we specify performance regions of the ROC curve that are naturally delineated by the class-specific strengths of the base classifiers and show that each of these regions can be associated with a unique set of guidelines for performance optimization of binary classifiers within unequal error cost regimes.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {843–852},
numpages = {10},
keywords = {ensemble, cost-specific, classifiers},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1109/SCC.2012.24,
author = {Zhang, Meng and Liu, Xudong and Zhang, Richong and Sun, Hailong},
title = {A Web Service Recommendation Approach Based on QoS Prediction Using Fuzzy Clustering},
year = {2012},
isbn = {9780769547534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2012.24},
doi = {10.1109/SCC.2012.24},
abstract = {Web services, as loosely-coupled software systems, are increasingly being published to the web and there are a large number of services with similar functions. Therefore, service users compare the non-functional properties of services, e.g., Quality of Service (QoS), when they make service selection. This paper aims at generating a more comprehensive web service recommendation to users with a novel approach to fulfill more accurate prediction of unknown services' QoS values. We accomplish the QoS prediction by using fuzzy clustering method with calculating the users' similarity. Our approach improves the prediction accuracy and this is confirmed by comparing experiments with other methods. In addition, the quality of web services is considered as a multi-dimensional object, and each dimension is one aspect of the web service's non-functional properties. We also provide an application example to demonstrate how to utilize our approach to rank services by a score function and map multi-dimensional QoS properties into a single dimensional value.},
booktitle = {Proceedings of the 2012 IEEE Ninth International Conference on Services Computing},
pages = {138–145},
numpages = {8},
keywords = {Web service, Recommender system, QoS prediction, Fuzzy clustering},
series = {SCC '12}
}

@article{10.1007/s11390-019-1968-y,
author = {Ren, Rui and Cheng, Jiechao and He, Xi-Wen and Wang, Lei and Zhan, Jian-Feng and Gao, Wan-Ling and Luo, Chun-Jie},
title = {HybridTune: Spatio-Temporal Performance Data Correlation for Performance Diagnosis of Big Data Systems},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {6},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1968-y},
doi = {10.1007/s11390-019-1968-y},
abstract = {With tremendous growing interests in Big Data, the performance improvement of Big Data systems becomes more and more important. Among many steps, the first one is to analyze and diagnose performance bottlenecks of the Big Data systems. Currently, there are two major solutions. One is the pure data-driven diagnosis approach, which may be very time-consuming; the other is the rule-based analysis method, which usually requires prior knowledge. For Big Data applications like Spark workloads, we observe that the tasks in the same stages normally execute the same or similar codes on each data partition. On basis of the stage similarity and distributed characteristics of Big Data systems, we analyze the behaviors of the Big Data applications in terms of both system and micro-architectural metrics of each stage. Furthermore, for different performance problems, we propose a hybrid approach that combines prior rules and machine learning algorithms to detect performance anomalies, such as straggler tasks, task assignment imbalance, data skew, abnormal nodes and outlier metrics. Following this methodology, we design and implement a lightweight, extensible tool, named HybridTune, and measure the overhead and anomaly detection effectiveness of HybridTune using the BigDataBench benchmarks. Our experiments show that the overhead of HybridTune is only 5%, and the accuracy of outlier detection algorithm reaches up to 93%. Finally, we report several use cases diagnosing Spark and Hadoop workloads using BigDataBench, which demonstrates the potential use of HybridTune.},
journal = {J. Comput. Sci. Technol.},
month = nov,
pages = {1167–1184},
numpages = {18},
keywords = {machine learning, rule-based diagnosis, spatio-temporal correlation, Big Data system}
}

@inproceedings{10.1145/3451421.3451427,
author = {Liu, Xiaoli and Li, Jiali and Cao, Peng},
title = {SP-MTFL: A self paced multi-task feature learning method for cognitive performance predicting of Alzheimer's disease},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451427},
doi = {10.1145/3451421.3451427},
abstract = {Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Predicting cognitive performance of subjects from neuroimage measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimer's disease. Multi-task based feature learning (MTFL) have been widely studied to select a discriminative feature subset from MRI features, and improve the performance by incorporating inherent correlations among multiple clinical cognitive measures. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, we propose a self-paced multi-task feature learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task in this study. Experimental results on ADNI are provided, and the comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {23–27},
numpages = {5},
keywords = {regression, multi-task learning, Self-paced learning, Machine learning, Alzheimer's disease},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@inproceedings{10.1145/3126908.3126951,
author = {Li, Yan and Chang, Kenneth and Bel, Oceane and Miller, Ethan L. and Long, Darrell D. E.},
title = {CAPES: unsupervised storage performance tuning using neural network-based deep reinforcement learning},
year = {2017},
isbn = {9781450351140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126908.3126951},
doi = {10.1145/3126908.3126951},
abstract = {Parameter tuning is an important task of storage performance optimization. Current practice usually involves numerous tweak-benchmark cycles that are slow and costly. To address this issue, we developed CAPES, a model-less deep reinforcement learning-based unsupervised parameter tuning system driven by a deep neural network (DNN). It is designed to find the optimal values of tunable parameters in computer systems, from a simple client-server system to a large data center, where human tuning can be costly and often cannot achieve optimal performance. CAPES takes periodic measurements of a target computer system's state, and trains a DNN which uses Q-learning to suggest changes to the system's current parameter values. CAPES is minimally intrusive, and can be deployed into a production system to collect training data and suggest tuning actions during the system's daily operation. Evaluation of a prototype on a Lustre file system demonstrates an increase in I/O throughput up to 45% at saturation point.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {42},
numpages = {14},
keywords = {q-learning, performance tuning, deep learning},
location = {Denver, Colorado},
series = {SC '17}
}

@article{10.1145/3349265,
author = {Barua, Hrishav Bakul and Mondal, Kartick Chandra},
title = {A Comprehensive Survey on Cloud Data Mining (CDM) Frameworks and Algorithms},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3349265},
doi = {10.1145/3349265},
abstract = {Data mining is used for finding meaningful information out of a vast expanse of data. With the advent of Big Data concept, data mining has come to much more prominence. Discovering knowledge out of a gigantic volume of data efficiently is a major concern as the resources are limited. Cloud computing plays a major role in such a situation. Cloud data mining fuses the applicability of classical data mining with the promises of cloud computing. This allows it to perform knowledge discovery out of huge volumes of data with efficiency. This article presents the existing frameworks, services, platforms, and algorithms for cloud data mining. The frameworks and platforms are compared among each other based on similarity, data mining task support, parallelism, distribution, streaming data processing support, fault tolerance, security, memory types, storage systems, and others. Similarly, the algorithms are grouped on the basis of parallelism type, scalability, streaming data mining support, and types of data managed. We have also provided taxonomies on the basis of data mining techniques such as clustering, classification, and association rule mining. We also have attempted to discuss and identify the major applications of cloud data mining. The various taxonomies for cloud data mining frameworks, platforms, and algorithms have been identified. This article aims at gaining better insight into the present research realm and directing the future research toward efficient cloud data mining in future cloud systems.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {104},
numpages = {62},
keywords = {volume, velocity, variety, taxonomy, survey, parallelism, machine learning, graph mining, framework, distributed computing, data science, data mining, clustering, cloud data mining (CDM), cloud computing, classification and association rule mining, big data analytics, big data, Review}
}

@article{10.1145/3130945,
author = {Luo, Chu and Kuutila, Miikka and Klakegg, Simon and Ferreira, Denzil and Flores, Huber and Goncalves, Jorge and M\"{a}ntyl\"{a}, Mika and Kostakos, Vassilis},
title = {TestAWARE: A Laboratory-Oriented Testing Tool for Mobile Context-Aware Applications},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130945},
doi = {10.1145/3130945},
abstract = {Although mobile context instrumentation frameworks have simplified the development of mobile context-aware applications, it remains challenging to test such applications. In this paper, we present TestAWARE that enables developers to systematically test context-aware applications in laboratory settings. To achieve this, TestAWARE is able to download, replay and emulate contextual data on either physical devices or emulators. To support both white -box and black-box testing, TestAWARE has been implemented as a novel structure with a mobile client and code library. In blackbox testing scenarios, developers can manage data replay through the mobile client, without writing testing scripts or modifying the source code of the targeted application. In white-box testing scenarios, developers can manage data replay and test functional/non-functional properties of the targeted application by writing testing scripts using the code library. We evaluated TestAWARE by quantifying its maximal data replay speed, and by conducting a user study with 13 developers. We show that TestAWARE can overcome data synchronisation challenges, and found that PC-based emulators can replay data significantly faster than physical smartphones and tablets. The user study highlights the usefulness of TestAWARE in the systematic testing of mobile context-aware applications in laboratory settings.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {80},
numpages = {29},
keywords = {mobile sensing, mobile interaction, machine learning, context aware computing, Sensors}
}

@article{10.1016/j.jpdc.2012.12.012,
author = {Ahmad, Faraz and Lee, Seyong and Thottethodi, Mithuna and Vijaykumar, T. N.},
title = {MapReduce with communication overlap (MaRCO)},
year = {2013},
issue_date = {May, 2013},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {73},
number = {5},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2012.12.012},
doi = {10.1016/j.jpdc.2012.12.012},
abstract = {MapReduce is a programming model from Google for cluster-based computing in domains such as search engines, machine learning, and data mining. MapReduce provides automatic data management and fault tolerance to improve programmability of clusters. MapReduce's execution model includes an all-map-to-all-reduce communication, called the shuffle, across the network bisection. Some MapReductions move large amounts of data (e.g., as much as the input data), stressing the bisection bandwidth and introducing significant runtime overhead. Optimizing such shuffle-heavy MapReductions is important because (1) they include key applications (e.g., inverted indexing for search engines and data clustering for machine learning) and (2) they run longer than shuffle-light MapReductions (e.g., 5x longer). In MapReduce, the asynchronous nature of the shuffle results in some overlap between the shuffle and map. Unfortunately, this overlap is insufficient in shuffle-heavy MapReductions. We propose MapReduce with communication overlap (MaRCO) to achieve nearly full overlap via the novel idea of including reduce in the overlap. While MapReduce lazily performs reduce computation only after receiving all the map data, MaRCO employs eager reduce to process partial data from some map tasks while overlapping with other map tasks' communication. MaRCO's approach of hiding the latency of the inevitably high shuffle volume of shuffle-heavy MapReductions is fundamental for achieving performance. We implement MaRCO in Hadoop's MapReduce and show that on a 128-node Amazon EC2 cluster, MaRCO achieves 23% average speed-up over Hadoop for shuffle-heavy MapReductions.},
journal = {J. Parallel Distrib. Comput.},
month = may,
pages = {608–620},
numpages = {13},
keywords = {Performance optimization, Parallel computing, MapReduce, Large-scale data processing, Distributed processing, Cloud computing}
}

@inproceedings{10.5555/3540261.3542579,
author = {Ostapenko, Oleksiy and Rodr\'{\i}guez, Pau and Caccia, Massimo and Charlin, Laurent},
title = {Continual learning via local module composition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2318},
numpages = {15},
series = {NIPS '21}
}

@inproceedings{10.1145/1873951.1874276,
author = {Tomarchio, Antonio and Bellacci, Francesco and Privitera, Filippo},
title = {Data-driven behavioural algorithms for online advertising},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874276},
doi = {10.1145/1873951.1874276},
abstract = {In this paper, we describe an innovative data-driven behavioural approach that we developed for the optimization of performance online advertising on Simply, the new international adnetwork developed by Dada spa.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1533–1534},
numpages = {2},
keywords = {yeild optimization, user clustering, real-time bidding, online performance advertising, conversion optimization, clickstream analysis, advertising performance optimization},
location = {Firenze, Italy},
series = {MM '10}
}

@article{10.1016/j.patcog.2011.09.011,
author = {Rasmussen, Peter M. and Hansen, Lars K. and Madsen, Kristoffer H. and Churchill, Nathan W. and Strother, Stephen C.},
title = {Model sparsity and brain pattern interpretation of classification models in neuroimaging},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.09.011},
doi = {10.1016/j.patcog.2011.09.011},
abstract = {Interest is increasing in applying discriminative multivariate analysis techniques to the analysis of functional neuroimaging data. Model interpretation is of great importance in the neuroimaging context, and is conventionally based on a 'brain map' derived from the classification model. In this study we focus on the relative influence of model regularization parameter choices on both the model generalization, the reliability of the spatial patterns extracted from the classification model, and the ability of the resulting model to identify relevant brain networks defining the underlying neural encoding of the experiment. For a support vector machine, logistic regression and Fisher's discriminant analysis we demonstrate that selection of model regularization parameters has a strong but consistent impact on the generalizability and both the reproducibility and interpretable sparsity of the models for both @?"2 and @?"1 regularization. Importantly, we illustrate a trade-off between model spatial reproducibility and prediction accuracy. We show that known parts of brain networks can be overlooked in pursuing maximization of classification accuracy alone with either @?"2 and/or @?"1 regularization. This supports the view that the quality of spatial patterns extracted from models cannot be assessed purely by focusing on prediction accuracy. Our results instead suggest that model regularization parameters must be carefully selected, so that the model and its visualization enhance our ability to interpret the brain.},
journal = {Pattern Recogn.},
month = jun,
pages = {2085–2100},
numpages = {16},
keywords = {Sparsity, Regularization, Pattern analysis, Neuroimaging, NPAIRS resampling, Model interpretation, Machine learning, Kernel methods, Classification}
}

@inproceedings{10.1145/3358960.3379127,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Transferring Pareto Frontiers across Heterogeneous Hardware Environments},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379127},
doi = {10.1145/3358960.3379127},
abstract = {Software systems provide user-relevant configuration options called features. Features affect functional and non-functional system properties, whereas selections of features represent system configurations. A subset of configuration space forms a Pareto frontier of optimal configurations in terms of multiple properties, from which a user can choose the best configuration for a particular scenario. However, when a well-studied system is redeployed on a different hardware, information about property value and the Pareto frontier might not apply. We investigate whether it is possible to transfer this information across heterogeneous hardware environments. We propose a methodology for approximating and transferring Pareto frontiers of configurable systems across different hardware environments. We approximate a Pareto frontier by training an individual predictor model for each system property, and by aggregating predictions of each property into an approximated frontier. We transfer the approximated frontier across hardware by training a transfer model for each property, by applying it to a respective predictor, and by combining transferred properties into a frontier. We evaluate our approach by modeling Pareto frontiers as binary classifiers that separate all system configurations into optimal and non-optimal ones. Thus we can assess quality of approximated and transferred frontiers using common statistical measures like sensitivity and specificity. We test our approach using five real-world software systems from the compression domain, while paying special attention to their performance. Evaluation results demonstrate that accuracy of approximated frontiers depends linearly on predictors' training sample sizes, whereas transferring introduces only minor additional error to a frontier even for small training sizes.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {12–23},
numpages = {12},
keywords = {regression trees, performance prediction, linear regression, configurable software, Pareto frontier transferring, Pareto frontier},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@article{10.1016/j.ins.2021.08.085,
author = {Cassales, Guilherme and Gomes, Heitor and Bifet, Albert and Pfahringer, Bernhard and Senger, Hermes},
title = {Improving the performance of bagging ensembles for data streams through mini-batching},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {580},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.08.085},
doi = {10.1016/j.ins.2021.08.085},
journal = {Inf. Sci.},
month = nov,
pages = {260–282},
numpages = {23},
keywords = {Hoeffding tree, Bagging algorithms, Ensemble learners, Data-stream learning, Multicore task-parallelism}
}

@inproceedings{10.5555/3504035.3504406,
author = {Gong, Tieliang and Wang, Guangtao and Ye, Jieping and Xu, Zongben and Lin, Ming},
title = {Margin based PU learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The PU learning problem concerns about learning from positive and unlabeled data. A popular heuristic is to iteratively enlarge training set based on some margin-based criterion. However, little theoretical analysis has been conducted to support the success of these heuristic methods. In this work, we show that not all margin-based heuristic rules are able to improve the learned classifiers iteratively. We find that a so-called large positive margin oracle is necessary to guarantee the success of PU learning. Under this oracle, a provable positive-margin based PU learning algorithm is proposed for linear regression and classification under the truncated Gaussian distributions. The proposed algorithm is able to reduce the recovering error geometrically proportional to the positive margin. Extensive experiments on real-world datasets verify our theory and the state-of-the-art performance of the proposed PU learning algorithm.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {371},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/3362743.3362964,
author = {Dey, Swarnava and Mukherjee, Arijit and Pal, Arpan and P, Balamuralidhar},
title = {Embedded Deep Inference in Practice: Case for Model Partitioning},
year = {2019},
isbn = {9781450370110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362743.3362964},
doi = {10.1145/3362743.3362964},
abstract = {With increased focus on in situ analytics, artificial intelligence (AI) algorithms are getting deployed on embedded devices at the network edge. Growing popularity of Deep Learning (DL) and inference largely due to minimization of feature engineering, availability of pre-trained models and fine-tunable datasets especially in image and video analytics, have made these de-facto standard. However, the embedded systems employing these models are often resource constrained and fail to handle scenarios where arrival rate and input data volume increase over a given time period. This has a direct effect on the storage and network usage of such devices, rendering the traditional strategies of input buffering and network offloading ineffective. This paper investigates the use of dynamic layer-wise partitioning and partial execution of DL inference phase to enable inelastic embedded systems to support varying sensing rates and large data volume. The proposed partial execution scheme and partitioning algorithm perform better than standard frame-wise inference methods, when evaluated using workloads of few popular CNNs used in standard object detection models.},
booktitle = {Proceedings of the 1st Workshop on Machine Learning on Edge in Sensor Systems},
pages = {25–30},
numpages = {6},
keywords = {elastic, distributed, convolution, Fog, Edge, DAG, Cloud, CNN},
location = {New York, NY, USA},
series = {SenSys-ML 2019}
}

@article{10.1007/s10772-017-9429-x,
author = {Phu, Vo Ngoc and Tran, Vo Thi and Chau, Vo Thi and Dat, Nguyen Duy and Duy, Khanh Ly},
title = {A decision tree using ID3 algorithm for English semantic analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9429-x},
doi = {10.1007/s10772-017-9429-x},
abstract = {Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {593–613},
numpages = {21},
keywords = {id3, Sentiment classification, ID3 algorithm, English sentiment classification, English document opinion mining, Decision tree}
}

@article{10.1145/3345314,
author = {Wang, Qingyong and Zhou, Yun and Ding, Weiping and Zhang, Zhiguo and Muhammad, Khan and Cao, Zehong},
title = {Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3345314},
doi = {10.1145/3345314},
abstract = {Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {34},
numpages = {12},
keywords = {self-paced learning, random forest, classification, bootstrap, Lung cancer}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.asoc.2021.107520,
author = {Mu, Shengdong and Wang, Yuanyuan and Wang, Fengyu and Ogiela, Lidia},
title = {Transformative computing for products sales forecast based on SCIM},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {109},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107520},
doi = {10.1016/j.asoc.2021.107520},
journal = {Appl. Soft Comput.},
month = sep,
numpages = {13},
keywords = {Sales forecast, Deep learning, Fuzzy theory, SICM, Transformative computing}
}

@inproceedings{10.1007/978-3-030-32047-8_26,
author = {Khoshmanesh, Seyedehzahra and Lutz, Robyn R.},
title = {Leveraging Feature Similarity for Earlier Detection of Unwanted Feature Interactions in Evolving Software Product Lines},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_26},
doi = {10.1007/978-3-030-32047-8_26},
abstract = {Software product lines enable reuse of shared software across a family of products. As new products are built in the product line, new features are added. The features are units of functionality that provide services to users. Unwanted feature interactions, wherein one feature interferes with another feature’s operation, is a significant problem, especially as large software product lines evolve. Detecting feature interactions is a time-consuming and difficult task for developers. Moreover, feature interactions are often only discovered during testing, at which point costly re-work is needed. This paper proposes a similarity-based method to identify unwanted feature interactions much earlier in the development process. It uses knowledge of prior feature interactions stored with the software product line’s feature model to help find unwanted interactions between a new feature and existing features. The paper describes the framework and algorithms used to detect the feature interactions using three path similarity measures and evaluates the approach on a real-world, evolving software product line. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {293–307},
numpages = {15},
keywords = {Feature interaction, Similarity measures, Software product lines},
location = {Newark, NJ, USA}
}

@inproceedings{10.1145/3423211.3425692,
author = {Rocha, Isabelly and Morris, Nathaniel and Chen, Lydia Y. and Felber, Pascal and Birke, Robert and Schiavoni, Valerio},
title = {PipeTune: Pipeline Parallelism of Hyper and System Parameters Tuning for Deep Learning Clusters},
year = {2020},
isbn = {9781450381536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423211.3425692},
doi = {10.1145/3423211.3425692},
abstract = {DNN learning jobs are common in today's clusters due to the advances in AI driven services such as machine translation and image recognition. The most critical phase of these jobs for model performance and learning cost is the tuning of hyperparameters. Existing approaches make use of techniques such as early stopping criteria to reduce the tuning impact on learning cost. However, these strategies do not consider the impact that certain hyperparameters and systems parameters have on training time. This paper presents PipeTune, a framework for DNN learning jobs that addresses the trade-offs between these two types of parameters. PipeTune takes advantage of the high parallelism and recurring characteristics of such jobs to minimize the learning cost via a pipelined simultaneous tuning of both hyper and system parameters. Our experimental evaluation using three different types of workloads indicates that PipeTune achieves up to 22.6% reduction and 1.7\texttimes{} speed up on tuning and training time, respectively. PipeTune not only improves performance but also lowers energy consumption up to 29%.},
booktitle = {Proceedings of the 21st International Middleware Conference},
pages = {89–104},
numpages = {16},
keywords = {accuracy time trade-off, Parameter tuning, Deep Neural Networks training},
location = {Delft, Netherlands},
series = {Middleware '20}
}

@article{10.1016/j.ins.2019.02.051,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {         Munec: a mutual neighbor-based clustering algorithm},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {486},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.051},
doi = {10.1016/j.ins.2019.02.051},
journal = {Inf. Sci.},
month = jun,
pages = {148–170},
numpages = {23},
keywords = {Clustering, Distance, Density, Single link, Mutual neighbors}
}

@article{10.1155/2015/196098,
author = {Yang, Jinfeng and Xiao, Yong and Wang, Jiabing and Ma, Qianli and Shen, Yanhua},
title = {A fast clustering algorithm for data with a few labeled instances},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/196098},
doi = {10.1155/2015/196098},
abstract = {The diameter of a cluster is the maximum intracluster distance between pairs of instances within the same cluster, and the split of a cluster is the minimum distance between instances within the cluster and instances outside the cluster. Given a few labeled instances, this paper includes two aspects. First, we present a simple and fast clustering algorithm with the following property: if the ratio of the minimum split to the maximum diameter (RSD) of the optimal solution is greater than one, the algorithm returns optimal solutions for three clustering criteria. Second, we study the metric learning problem: learn a distance metric to make the RSD as large as possible. Compared with existing metric learning algorithms, one of our metric learning algorithms is computationally efficient: it is a linear programming model rather than a semidefinite programming model used by most of existing algorithms. We demonstrate empirically that the supervision and the learned metric can improve the clustering quality.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {21},
numpages = {1}
}

@article{10.5555/3322706.3361993,
author = {Glimsdal, Sondre and Granmo, Ole-Christoffer},
title = {Thompson sampling guided stochastic searching on the line for deceptive environments with applications to root-finding problems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the "left" or to the "right" of the arm pulled, with the feedback being erroneous with probability 1 - π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1910–1933},
numpages = {24},
keywords = {thompson sampling, stochastic point location, searching on the line, probabilistic bisection search, deceptive environment}
}

@inproceedings{10.1145/2831244.2831249,
author = {Eslami, Hassan and Kougkas, Anthony and Kotsifakou, Maria and Kasampalis, Theodoros and Feng, Kun and Lu, Yin and Gropp, William and Sun, Xian-He and Chen, Yong and Thakur, Rajeev},
title = {Efficient disk-to-disk sorting: a case study in the decoupled execution paradigm},
year = {2015},
isbn = {9781450339933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2831244.2831249},
doi = {10.1145/2831244.2831249},
abstract = {Many applications foreseen for exascale era should process huge amount of data. However, the IO infrastructure of current supercomputing architecture cannot be generalized to deal with this amount of data due to the need for excessive data movement from storage layers to compute nodes leading to limited scalability. There has been extensive studies addressing this challenge. Decoupled Execution Paradigm (DEP) is an attractive solution due to its unique features such as available fast storage devices close to computational units and available programmable units close to file system.In this paper we study the effectiveness of DEP for a well-known data-intensive kernel, disk-to-disk (aka out-of-core) sorting. We propose an optimized algorithm that uses almost all features of DEP pushing the performance of sorting in HPC even further compared to other existing solutions. Advantages in our algorithm are gained by exploiting programming units close to parallel file system to achieve higher IO throughput, compressing data before sending it over network or to disk, storing intermediate results of computation close to compute nodes, and fully overlapping IO with computation. We also provide an analytical model for our proposed algorithm. Our algorithm achieves 30% better performance compared to the theoretically optimal sorting algorithm running on the same testbed but not designed to exploit the DEP architecture.},
booktitle = {Proceedings of the 2015 International Workshop on Data-Intensive Scalable Computing Systems},
articleno = {2},
numpages = {8},
keywords = {performance optimization, parallel file system, parallel IO, disk-to-disk sorting, decoupled execution paradigm},
location = {Austin, Texas},
series = {DISCS '15}
}

@article{10.1007/s00521-014-1656-3,
author = {Razavi-Far, Roozbeh and Palade, Vasile and Zio, Enrico},
title = {Invasive weed classification},
year = {2015},
issue_date = {April     2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {3},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-014-1656-3},
doi = {10.1007/s00521-014-1656-3},
abstract = {Invasive weed optimization (IWO) is a recently published heuristic optimization technique that resembles other evolutionary optimization methods. This paper proposes a new classification technique based on the IWO algorithm, called the invasive weed classification (IWC), to face the problem of pattern classification for multi-class datasets. The aim of the IWC is to find the set of the positions of the class centers that minimize the multi-objective function, i.e., the optimal positions of the class centers. The classification performance is computed as the percentage of misclassified patterns in the testing dataset achieved by the best plants in terms of fitness performance. The performance of the IWC algorithm, both in terms of classification accuracy and training time, is compared with other commonly used classification algorithms.},
journal = {Neural Comput. Appl.},
month = apr,
pages = {525–539},
numpages = {15},
keywords = {Pattern recognition, Optimization, Invasive weed classification}
}

@article{10.1002/sys.21326,
author = {Alfaris, Anas and Khiyami, Abdulaziz and Alawad, Abdullah and Alsaati, Adnan and Hadhrawi, Mohammed},
title = {The Integrated Energy Decision Support System},
year = {2015},
issue_date = {October 2015},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {18},
number = {5},
issn = {1098-1241},
url = {https://doi.org/10.1002/sys.21326},
doi = {10.1002/sys.21326},
abstract = {In this paper we introduce a decision support system framework termed the Integrated Energy Decision Support System IEDSS. IEDSS was developed for energy planning at national and regional levels to inform energy planners at multiple levels of government. IEDSS employs system dynamics modeling to enable the rapid evaluation of the outcomes of different supply and demand policies at the national level. Agent-Based models are used to mimic the interactions between different entities when applying the framework at the regional level of government. Within the IEDSS framework policy makers specify a set of policy decisions and choose from a set of uncertain futures to investigate the performance of their policy decisions. Together these form a scenario for which IEDSS computes a set of output parameters that are used to evaluate the resulting outcome. As a model-driven DSS, IEDSS can be utilized in two ways. The first is as a single-user DSS that deploys a scenario-based planning approach which informs decision makers by mapping the solution space and the resultant effects caused by their policy choices. The second is as a group-based DSS that enhances communication and collaborative decision making between multiple entities. IEDSS is developed on a software platform that utilizes the front-end computation to handle templates, style sheets, and visualizations, while the backend is focused on data retrieval, models execution, and performance optimization. IEDSS was developed to address the power sector of the Kingdom of Saudi Arabia as a case study, but the framework and capabilities of its platform are applicable to any generalized case.},
journal = {Syst. Eng.},
month = oct,
pages = {511–529},
numpages = {19},
keywords = {system dynamics, scenario-based planning, energy modeling, decision support systems, collaborative planning, agent based modeling}
}

@article{10.4018/IJDAI.2020070103,
author = {Feltus, Christophe},
title = {Reinforcement Learning's Contribution to the Cyber Security of Distributed Systems: Systematization of Knowledge},
year = {2020},
issue_date = {Jul 2020},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {2},
issn = {2637-7888},
url = {https://doi.org/10.4018/IJDAI.2020070103},
doi = {10.4018/IJDAI.2020070103},
abstract = {Reinforcement learning (RL) is a machine learning paradigm, like supervised or unsupervised learning, which learns the best actions an agent needs to perform to maximize its rewards in a particular environment. Research into RL has been proven to have made a real contribution to the protection of cyberphysical distributed systems. In this paper, the authors propose an analytic framework constituted of five security fields and eight industrial areas. This framework allows structuring a systematic review of the research in artificial intelligence that contributes to cybersecurity. In this contribution, the framework is used to analyse the trends and future fields of interest for the RL-based research in information system security.},
journal = {International Journal of Distributed Artificial Intelligence},
month = jul,
pages = {35–55},
numpages = {21},
keywords = {Systematization-of-Knowledge, SoK, Reinforcement Learning, Distributed AI, Cybersecurity, Artificial Intelligence, Analytic Framework}
}

@inproceedings{10.1109/CIT.2014.122,
author = {Fan, Yuanquan and Wu, Weiguo and Xu, Yunlong and Cao, Yangjie and Li, Qian and Cui, Jinhua and Duan, Zhangfeng},
title = {Performance Prediction Model in Heterogeneous MapReduce Environments},
year = {2014},
isbn = {9781479962396},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CIT.2014.122},
doi = {10.1109/CIT.2014.122},
abstract = {Map Reduce has emerged as a popular computing model for parallel processing of cloud computing. Map Reduce performance analysis and modeling is needed to guide performance optimization and job scheduling. However, we observed that it is difficult to build a performance model due to various aspects of workload behavior and heterogeneity among cluster nodes in heterogeneous Map Reduce Environments. To address the above issues, in this paper, we propose a novel performance prediction model for Map Reduce in heterogeneous environments. This model consists of two components: (1) performance prediction model based on machine learning and (2) optimal parameters selection based on immune algorithm. Experiment results show that our model can accurately forecast the performance of Map Reduce jobs that run in heterogeneous Map Reduce systems.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Computer and Information Technology},
pages = {240–245},
numpages = {6},
keywords = {performance prediction, machine learning, cloud computing, MapReduce, Heterogeneity},
series = {CIT '14}
}

@article{10.1016/j.eswa.2019.03.031,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {A hierarchical clustering algorithm and an improvement of the single linkage criterion to deal with noise},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.031},
doi = {10.1016/j.eswa.2019.03.031},
journal = {Expert Syst. Appl.},
month = aug,
pages = {96–108},
numpages = {13},
keywords = {Density, Dissimilarity, Agglomerative}
}

@article{10.1145/3218823,
author = {Tan, Guangming and Liu, Junhong and Li, Jiajia},
title = {Design and Implementation of Adaptive SpMV Library for Multicore and Many-Core Architecture},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3218823},
doi = {10.1145/3218823},
abstract = {Sparse matrix vector multiplication (SpMV) is an important computational kernel in traditional high-performance computing and emerging data-intensive applications. Previous SpMV libraries are optimized by either application-specific or architecture-specific approaches but present difficulties for use in real applications. In this work, we develop an auto-tuning system (SMATER) to bridge the gap between specific optimizations and general-purpose use. SMATER provides programmers a unified interface based on the compressed sparse row (CSR) sparse matrix format by implicitly choosing the best format and fastest implementation for any input sparse matrix during runtime. SMATER leverages a machine-learning model and retargetable back-end library to quickly predict the optimal combination. Performance parameters are extracted from 2,386 matrices in the SuiteSparse matrix collection. The experiments show that SMATER achieves good performance (up to 10 times that of the Intel Math Kernel Library (MKL) on Intel E5-2680 v3) while being portable on state-of-the-art x86 multicore processors, NVIDIA GPUs, and Intel Xeon Phi accelerators. Compared with the Intel MKL library, SMATER runs faster by more than 2.5 times on average. We further demonstrate its adaptivity in an algebraic multigrid solver from the Hypre library and report greater than 20% performance improvement.},
journal = {ACM Trans. Math. Softw.},
month = aug,
articleno = {46},
numpages = {25},
keywords = {multicore, machine learning, auto-tuning, Sparse matrix vector multiplication}
}

@article{10.1007/s00607-019-00773-w,
author = {Li, Yuming and Ni, Pin and Chang, Victor},
title = {Application of deep reinforcement learning in stock trading strategies and stock forecasting},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {102},
number = {6},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-019-00773-w},
doi = {10.1007/s00607-019-00773-w},
abstract = {The role of the stock market across the overall financial market is indispensable. The way to acquire practical trading signals in the transaction process to maximize the benefits is a problem that has been studied for a long time. This paper put forward a theory of deep reinforcement learning in the stock trading decisions and stock price prediction, the reliability and availability of the model are proved by experimental data, and the model is compared with the traditional model to prove its advantages. From the point of view of stock market forecasting and intelligent decision-making mechanism, this paper proves the feasibility of deep reinforcement learning in financial markets and the credibility and advantages of strategic decision-making.},
journal = {Computing},
month = jun,
pages = {1305–1322},
numpages = {18},
keywords = {91G10, 68T01, Deep Q learning, Financial strategy, Reinforcement learning}
}

@inproceedings{10.1145/2694344.2694345,
author = {Nguyen, Khanh and Wang, Kai and Bu, Yingyi and Fang, Lu and Hu, Jianfei and Xu, Guoqing},
title = {FACADE: A Compiler and Runtime for (Almost) Object-Bounded Big Data Applications},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694345},
doi = {10.1145/2694344.2694345},
abstract = {The past decade has witnessed the increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice for implementing such applications, due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets Big Data, this cost is significantly magnified and becomes a scalability-prohibiting bottleneck. This paper presents a novel compiler framework, called Facade, that can generate highly-efficient data manipulation code by automatically transforming the data path of an existing Big Data application. The key treatment is that in the generated code, the number of runtime heap objects created for data types in each thread is (almost) statically bounded, leading to significantly reduced memory management cost and improved scalability. We have implemented Facade and used it to transform 7 common applications on 3 real-world, already well-optimized Big Data frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: the generated programs have (1) achieved a 3%--48% execution time reduction and an up to 88X GC reduction; (2) consumed up to 50% less memory, and (3) scaled to much larger datasets.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {675–690},
numpages = {16},
keywords = {performance optimization, memory management, managed languages, big data applications},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3377812.3381399,
author = {Abbas, Muhammad},
title = {Variability aware requirements reuse analysis},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381399},
doi = {10.1145/3377812.3381399},
abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {190–193},
numpages = {4},
keywords = {variability, software reuse, similarities, requirements, product line},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.5555/1390681.1390706,
author = {Ye, Jieping and Ji, Shuiwang and Chen, Jianhui},
title = {Multi-class Discriminant Kernel Learning via Convex Programming},
year = {2008},
issue_date = {6/1/2008},
publisher = {JMLR.org},
volume = {9},
issn = {1532-4435},
abstract = {Regularized kernel discriminant analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. Its performance depends on the selection of kernels. In this paper, we consider the problem of multiple kernel learning (MKL) for RKDA, in which the optimal kernel matrix is obtained as a linear combination of pre-specified kernel matrices. We show that the kernel learning problem in RKDA can be formulated as convex programs. First, we show that this problem can be formulated as a semidefinite program (SDP). Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a convex quadratically constrained quadratic programming (QCQP) formulation for kernel learning in RKDA. A semi-infinite linear programming (SILP) formulation is derived to further improve the efficiency. We extend these formulations to the multi-class case based on a key result established in this paper. That is, the multi-class RKDA kernel learning problem can be decomposed into a set of binary-class kernel learning problems which are constrained to share a common kernel. Based on this decomposition property, SDP formulations are proposed for the multi-class case. Furthermore, it leads naturally to QCQP and SILP formulations. As the performance of RKDA depends on the regularization parameter, we show that this parameter can also be optimized in a joint framework with the kernel. Extensive experiments have been conducted and analyzed, and connections to other algorithms are discussed.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {719–758},
numpages = {40}
}

@article{10.1504/IJBRA.2018.092685,
title = {Subspace module extraction from MI-based co-expression network},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {3},
issn = {1744-5485},
url = {https://doi.org/10.1504/IJBRA.2018.092685},
doi = {10.1504/IJBRA.2018.092685},
abstract = {Most of the existing methods in literature have used proximity measures in the construction of co-expression networks CEN consisting of functional gene modules. This work describes the construction of co-expression network using mutual information MI as a proximity measure with non-linear correlation. The network modules are extracted that are defined over a subset of samples. This method has been tested on several publicly available datasets and the subspace network modules obtained have been validated in terms of both internal and external measures.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {207–234},
numpages = {28}
}

@inproceedings{10.1007/978-3-030-86340-1_12,
author = {Zhao, Yuekai and Lu, Jianzhuang and Chen, Xiaowen},
title = {Accelerating Depthwise Separable Convolutions with Vector Processor},
year = {2021},
isbn = {978-3-030-86339-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86340-1_12},
doi = {10.1007/978-3-030-86340-1_12},
abstract = {Depthwise separable convolution has demonstrated its advantages in reducing the number of parameters and neural network calculations. Convolution-oriented hardware accelerators are outstanding in terms of saving resources and energy. However, lightweight networks designed for small processors do not perform efficiently on these accelerators. Moreover, there are too many models to design an application-specific circuit for each model. In this work, we propose a method of mapping depthwise separable convolution on a general-purpose vector processor. This method achieves high computational performance by increasing data reuse and parallel execution. First of all, we propose a multi-vector parallel convolution method to reduce the number of data reads and increase data utilization in depthwise convolution. Then, we divide the data of pointwise convolution into coarse-grained blocks and compute matrix multiplication in parallel on a multi-core processor, achieving high computational efficiency. Furthermore, we use a double buffer mechanism to optimize data transfer and shorten execution time. Overall, using MobileNet to evaluate depthwise separable convolution, multi-vector parallel convolution method on M-DSP reduces the number of reads and writes by up&nbsp;to 4 times. We achieve 1518 FPS and 1.783 TFLOPS at a batch size of 1, which is 1.87\texttimes{} faster than ZU9 MPSoc and 3.89\texttimes{} more calculation-efficient than 2080Ti GPU.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part II},
pages = {145–156},
numpages = {12},
keywords = {Matrix multiplication, Multi-vector parallel convolution, Depthwise separable convolution},
location = {Bratislava, Slovakia}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-paced co-training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.patrec.2013.07.016,
author = {Sanmiguel, Juan C. and Suja, Sergio},
title = {Skin detection by dual maximization of detectors agreement for video monitoring},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {34},
number = {16},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2013.07.016},
doi = {10.1016/j.patrec.2013.07.016},
abstract = {This paper presents an approach for skin detection which is able to adapt its parameters to image data captured from video monitoring tasks with a medium field of view. It is composed of two detectors designed to get high and low probable skin pixels (respectively, regions and isolated pixels). Each one is based on thresholding two color channels, which are dynamically selected. Adaptation is based on the agreement maximization framework, whose aim is to find the configuration with the highest similarity between the channel results. Moreover, we improve such framework by learning how detector parameters are related and proposing an agreement function to consider expected skin properties. Finally, both detectors are combined by morphological reconstruction filtering to keep the skin regions whilst removing wrongly detected regions. The proposed approach is evaluated on heterogeneous human activity recognition datasets outperforming the most relevant state-of-the-art approaches.},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {2102–2109},
numpages = {8},
keywords = {Skin detection, Performance optimization, Detector adaptation, Color space selection}
}

@inproceedings{10.1145/2509136.2509512,
author = {Xu, Guoqing},
title = {Resurrector: a tunable object lifetime profiling technique for optimizing real-world programs},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509512},
doi = {10.1145/2509136.2509512},
abstract = {Modern object-oriented applications commonly suffer from severe performance problems that need to be optimized away for increased efficiency and user satisfaction. Many existing optimization techniques (such as object pooling and pretenuring) require precise identification of object lifetimes. However, it is particularly challenging to obtain object lifetimes both precisely and efficiently: precise profiling techniques such as Merlin introduce several hundred times slowdown even for small programs while efficient approximation techniques often sacrifice precision and produce less useful lifetime information. This paper presents a tunable profiling technique, called Resurrector, that explores the middle ground between high precision and high efficiency to find the precision-efficiency sweetspot for various livenessbased optimization techniques. Our evaluation shows that Resurrector is both more precise and more efficient than the GC-based approximation, and it is orders-of-magnitude faster than Merlin. To demonstrate Resurrector's usefulness, we have developed client analyses to find allocation sites that create large data structures with disjoint lifetimes. By inspecting program source code and reusing data structures created from these allocation sites, we have achieved significant performance gains. We have also improved the precision of an existing optimization technique using the lifetime information collected by Resurrector.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {111–130},
numpages = {20},
keywords = {performance optimization, object lifetime information, memory management},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@inbook{10.5555/3454287.3454297,
author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
title = {GPipe: efficient training of giant neural networks using pipeline parallelism},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {10},
numpages = {10}
}

@article{10.1007/s11192-019-03307-5,
author = {Tattershall, E. and Nenadic, G. and Stevens, R. D.},
title = {Detecting bursty terms in computer science research},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {122},
number = {1},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-019-03307-5},
doi = {10.1007/s11192-019-03307-5},
abstract = {Research topics rise and fall in popularity over time, some more swiftly than others. The fastest rising topics are typically called bursts; for example “deep learning”, “internet of things” and “big data”. Being able to automatically detect and track bursty terms in the literature could give insight into how scientific thought evolves over time. In this paper, we take a trend detection algorithm from stock market analysis and apply it to over 30&nbsp;years of computer science research abstracts, treating the prevalence of each term in the dataset like the price of a stock. Unlike previous work in this domain, we use the free text of abstracts and titles, resulting in a finer-grained analysis. We report a list of bursty terms, and then use historical data to build a classifier to predict whether they will rise or fall in popularity in the future, obtaining accuracy in the region of 80%. The proposed methodology can be applied to any time-ordered collection of text to yield past and present bursty terms and predict their probable fate.},
journal = {Scientometrics},
month = jan,
pages = {681–699},
numpages = {19},
keywords = {MACD, DBLP, Machine learning, Term life cycles, Bibliometrics, Computer science}
}

@article{10.1007/s10489-015-0674-x,
author = {Yang, Chunsheng and Zou, Yanni and Lai, Pinhua and Jiang, Nan},
title = {Data mining-based methods for fault isolation with validated FMEA model ranking},
year = {2015},
issue_date = {December  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {43},
number = {4},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-015-0674-x},
doi = {10.1007/s10489-015-0674-x},
abstract = {FMEA (Failure Mode and Effects Analysis), which was developed to enhance the reliability of complex systems, is a standard method to characterize and document product and process problems and a systematic method for fault identification/isolation in maintenance. When a failure is predicted or detected, it is expected to identify which component is the root cause or to isolate the fault to a specific contributing component. To efficiently perform fault isolation, we proposed data mining-based methods for fault isolation by using the validated FMEA information to rank data-driven models. However, FMEA, as a standard document, is produced during the design of products or systems. Therefore, FMEA documentation is rarely validated or updated in practice after it was generated. In order to use reliable FMEA information to rank models for fault isolation, it is necessary to validate FMEA before using it. In this paper, we first present brief overview of FMEA validation. Then we introduce the proposed data mining based method for fault isolation. Finally we apply the proposed methods to Auxiliary Power Unit (APU) fault isolation for a given failure mode, "Inability to Start", by conducting large-scale experiments. The experimental results obtained from a case study demonstrate the usefulness and feasibility of the proposed methods for fault isolation.},
journal = {Applied Intelligence},
month = dec,
pages = {913–923},
numpages = {11},
keywords = {Model ranking, Fault isolation, Failure mode, FMEA validation, FMEA, Data mining, Data driven models, Binary classifier}
}

@article{10.1007/s11280-018-0622-x,
author = {Wen, Guoqiu and Zhu, Yonghua and Cai, Zhiguo and Zheng, Wei},
title = {Self-tuning clustering for high-dimensional data},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0622-x},
doi = {10.1007/s11280-018-0622-x},
abstract = {Spectral clustering is an important component of clustering method, via tightly relying on the affinity matrix. However, conventional spectral clustering methods 1). equally treat each data point, so that easily affected by the outliers; 2). are sensitive to the initialization; 3). need to specify the number of cluster. To conquer these problems, we have proposed a novel spectral clustering algorithm, via employing an affinity matrix learning to learn an intrinsic affinity matrix, using the local PCA to resolve the intersections; and further taking advantage of a robust clustering that is insensitive to initialization to automatically generate clusters without an input of number of cluster. Experimental results on both artificial and real high-dimensional datasets have exhibited our proposed method outperforms the clustering methods under comparison in term of four clustering metrics.},
journal = {World Wide Web},
month = nov,
pages = {1563–1573},
numpages = {11},
keywords = {Spectral clustering, Multi-manifold clustering, Local PCA, High-dimensional data}
}

@inproceedings{10.1145/2555243.2555271,
author = {Liu, Xu and Mellor-Crummey, John},
title = {A tool to analyze the performance of multithreaded programs on NUMA architectures},
year = {2014},
isbn = {9781450326568},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2555243.2555271},
doi = {10.1145/2555243.2555271},
abstract = {Almost all of today's microprocessors contain memory controllers and directly attach to memory. Modern multiprocessor systems support non-uniform memory access (NUMA): it is faster for a microprocessor to access memory that is directly attached than it is to access memory attached to another processor. Without careful distribution of computation and data, a multithreaded program running on such a system may have high average memory access latency. To use multiprocessor systems efficiently, programmers need performance tools to guide the design of NUMA-aware codes. To address this need, we enhanced the HPCToolkit performance tools to support measurement and analysis of performance problems on multiprocessor systems with multiple NUMA domains. With these extensions, HPCToolkit helps pinpoint, quantify, and analyze NUMA bottlenecks in executions of multithreaded programs. It computes derived metrics to assess the severity of bottlenecks, analyzes memory accesses, and provides a wealth of information to guide NUMA optimization, including information about how to distribute data to reduce access latency and minimize contention. This paper describes the design and implementation of our extensions to HPCToolkit. We demonstrate their utility by describing case studies in which we use these capabilities to diagnose NUMA bottlenecks in four multithreaded applications.},
booktitle = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {259–272},
numpages = {14},
keywords = {NUMA, memory access pattern, performance optimization, profiler, threads},
location = {Orlando, Florida, USA},
series = {PPoPP '14}
}

@inproceedings{10.1007/978-3-030-48077-6_3,
author = {Claris\'{o}, Robert and Cabot, Jordi},
title = {Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering},
year = {2020},
isbn = {978-3-030-48076-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48077-6_3},
doi = {10.1007/978-3-030-48077-6_3},
abstract = {Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.},
booktitle = {Rigorous State-Based Methods: 7th International Conference, ABZ 2020, Ulm, Germany, May 27–29, 2020, Proceedings},
pages = {27–43},
numpages = {17},
keywords = {Diversity, Clustering, Graph kernels, Testing, Verification and validation, Model-driven engineering},
location = {Ulm, Germany}
}

@article{10.1007/s10115-015-0846-3,
author = {Ando, Shin},
title = {Classifying imbalanced data in distance-based feature space},
year = {2016},
issue_date = {March     2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {46},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-015-0846-3},
doi = {10.1007/s10115-015-0846-3},
abstract = {Class imbalance is a significant issue in practical classification problems. Important countermeasures, such as re-sampling, instance-weighting, and cost-sensitive learning have been developed, but there are limitations as well as advantages to respective approaches. The synthetic re-sampling methods have wide applicability, but require a vector representation to generate additional instances. The instance-based methods can be applied to distance space data, but are not tractable with regard to a global objective. The cost-sensitive learning can minimize the expected cost given the costs of error, but generally does not extend to nonlinear measures, such as F-measure and area under the curve. In order to address the above shortcomings, this paper proposes a nearest neighbor classification model which employs a class-wise weighting scheme to counteract the class imbalance and a convex optimization technique to learn its weight parameters. As a result, the proposed model maintains the simple instance-based rule for prediction, yet retains a mathematical support for learning to maximize a nonlinear performance measure over the training set. An empirical study is conducted to evaluate the performance of the proposed algorithm on the imbalanced distance space data and make comparison with existing methods.},
journal = {Knowl. Inf. Syst.},
month = mar,
pages = {707–730},
numpages = {24},
keywords = {Weighted nearest neighbor classifier, Structural classifier, Class imbalance}
}

@inproceedings{10.1145/3205651.3208237,
author = {L\'{o}pez-L\'{o}pez, V\'{\i}ctor R. and Trujillo, Leonardo and Legrand, Pierrick},
title = {Novelty search for software improvement of a SLAM system},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208237},
doi = {10.1145/3205651.3208237},
abstract = {Genetic Improvement (GI) performs a search at the level of source code to find the best variant of a baseline system that improves non-functional properties while maintaining functionality with noticeable results in several domains. There a many aspects of this general approach that are currently being explored. In particular, this work deals to the way in which the search is guided to efficiently explore the search space of possible software versions in which GI operates. The proposal is to integrate Novelty Search (NS) within the GISMOE GI framework to improve KinectFusion, which is a vision-based Simultaneous Localization and Mapping (SLAM) system that is used for augmented reality, autonomous vehicle navigation, and many other real-world applications. This is one of a small set of works that have successfully combined NS with a GP system, and the first time that it has been used for software improvement. To achieve this, we propose a new behaviour descriptor for SLAM algorithms, based on state-of-the-art benchmarking and present results that show that NS can produce significant improvement gains in a GI setting, when considering execution time and trajectory estimation as the main performance criteria.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1598–1605},
numpages = {8},
keywords = {novelty search, genetic improvement, SLAM},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1016/j.asoc.2016.05.015,
author = {Dou, Dongyang and Zhou, Shishuai},
title = {Comparison of four direct classification methods for intelligent fault diagnosis of rotating machinery},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.015},
doi = {10.1016/j.asoc.2016.05.015},
abstract = {Display Omitted A rule-based method was proposed based on MLEM2 and enhanced by a new rule reasoning mechanism.Eight time-domain and five dimensionless frequency-domain parameters were adopted.The proposed method had the ability of feature reduction.The proposed method was an all-rounder compared with KNN, PNN and PSO-SVM as it was very friendly. Condition monitoring of rotating machinery is important to promptly detect early faults, identify potential problems, and prevent complete failure. Four direct classification methods were introduced to diagnose the regular condition, inner race defect, outer race defect, and rolling element defect of rolling bearings. These include the K-Nearest Neighbor algorithm (KNN), Probabilistic Neural Network (PNN), Particle Swarm Optimization optimized Support Vector Machine (PSO-SVM) and a Rule-Based Method (RBM) based on the MLEM2 algorithm and a new Rule Reasoning Mechanism (RRM). All of them can be run on the Fault Decision Table (FDT) containing numerical variables and output fault categories directly. The diagnosis results were discussed in terms of accuracy, time consumption, intelligibility, and maintainability. Especially, the interactions of the systems and human experts were compared in detail. It was concluded that all the four methods can work satisfactorily on accuracy, in an order of the PSO-SVM ranking the first, followed by the RBM that functioned the friendliest. Moreover, the RBM had the ability of feature reduction by itself, and would be most suitable for real-time applications.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {459–468},
numpages = {10},
keywords = {SVM, Rule, Rotating machinery, PNN, Fault diagnosis}
}

@inproceedings{10.5555/3370272.3370294,
author = {Mahmoudi, Nima and Lin, Changyuan and Khazaei, Hamzeh and Litoiu, Marin},
title = {Optimizing serverless computing: introducing an adaptive function placement algorithm},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {The main concept behind serverless computing is to build and run applications without the need for server management. It refers to a fine-grained deployment model where applications, comprising of one or more functions, are uploaded to a platform and then executed, scaled, and billed in response to the exact demand needed at the moment. While elite cloud vendors such as Amazon, Google, Microsoft, and IBM are now providing serverless computing, their approach for the placement of functions, i.e. associated container or sandbox, on servers is oblivious to the workload which may lead to poor performance and/or higher operational cost for software owners. In this paper, using statistical machine learning, we design and evaluate an adaptive function placement algorithm which can be used by serverless computing platforms to optimize the performance of running functions while minimizing the operational cost. Given a fixed amount of resources, our smart spread function placement algorithm results in higher performance compared to existing approaches; this will be achieved by maintaining the users' desired quality of service for a longer time which prevents premature scaling of the cloud resources. Extensive experimental studies revealed that the proposed adaptive function placement algorithm can be easily adopted by serverless computing providers and integrated to container orchestration platforms without introducing any limiting side effects.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {203–213},
numpages = {11},
keywords = {serverless computing, predictive performance modeling, optimization, machine learning, container placement algorithms},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@article{10.1145/3450964,
author = {Charles, Subodha and Mishra, Prabhat},
title = {A Survey of Network-on-Chip Security Attacks and Countermeasures},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3450964},
doi = {10.1145/3450964},
abstract = {With the advances of chip manufacturing technologies, computer architects have been able to integrate an increasing number of processors and other heterogeneous components on the same chip. Network-on-Chip (NoC) is widely employed by multicore System-on-Chip (SoC) architectures to cater to their communication requirements. NoC has received significant attention from both attackers and defenders. The increased usage of NoC and its distributed nature across the chip has made it a focal point of potential security attacks. Due to its prime location in the SoC coupled with connectivity with various components, NoC can be effectively utilized to implement security countermeasures to protect the SoC from potential attacks. There is a wide variety of existing literature on NoC security attacks and countermeasures. In this article, we provide a comprehensive survey of security vulnerabilities in NoC-based SoC architectures and discuss relevant countermeasures.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {101},
numpages = {36},
keywords = {machine learning, Hardware security}
}

@inproceedings{10.5555/3437539.3437636,
author = {Xydis, Sotirios and Christoforidis, Eleftherios and Soudris, Dimitrios},
title = {DDOT: data driven online tuning for energy efficient acceleration},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Modern accelerator platforms, are characterised by high micro-architectural complexity that affects both performance and energy consumption. Programmers usually are facing the problem of reasoning on differing trade-offs among the set of various code variants and their parameters configuration. While maximal configurations are usually adequate for performance optimization, this is not the case when optimizing for energy efficiency. Thus, efficient tuning methodologies accompanied with automated tools are of great importance for a quick and concrete evaluation of the explored design space. However, existing tuning frameworks are usually application-specific, i.e. performing well only on a priori known applications/workloads, and requiring heavy offline exploration and sampling procedures. In this paper, we present DDOT an online and scalable autotuning framework that enables the extraction of energy efficient tuning, with minimal online application characterisation. Instead of analyzing every application against every tuning configuration, it adopts a data driven approach, utilizing collaborative filtering, that quickly and with high accuracy configures the compiler-and runtime-tuning parameters by identifying similarities to previously optimized applications. We evaluate DDOT efficiency utilizing as driving vehicle the Intel Phi accelerator platform, and compare it with state-of-art iterative and machine-learning tuning strategies as well with the exact optimal configurations of the derived solution space, through which we show that with minimal online characterisation, e.g. only either two or four online evaluations, DDOT finds tuning configurations that achieve more than 94% in respect to the optimal.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {97},
numpages = {6},
keywords = {manycore architectures, data-driven energy optimization, auto-tuning, accelerated computing},
location = {Virtual Event, USA},
series = {DAC '20}
}

@inproceedings{10.1145/3207719.3207722,
author = {Brand, Peter and Falk, Joachim and Sue, Jonathan Ah and Brendel, Johannes and Hasholzner, Ralph and Teich, J\"{u}rgen},
title = {Reinforcement Learning for Power-Efficient Grant Prediction in LTE},
year = {2018},
isbn = {9781450357807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207719.3207722},
doi = {10.1145/3207719.3207722},
abstract = {Reducing the energy consumption of mobile phones is a major concern in the design of cellular modem solutions for LTE and 5G standards. Apart from optimizing hardware for power efficiency, dynamic power management, i.e., powering down idle system components, is a crucial means to achieve this goal. The techniques proposed so far, however, are reactive rather than proactive. This leads to the inability to exploit a significant amount of opportunities to power down components, as the opportunity is recognized too late. We propose a dynamic power management technique that is capable of exploiting said opportunities through the application of reinforcement learning prediction techniques for proactive power management. However, the additional computational effort for prediction algorithms must be carefully analyzed and taken into account. Therefore, we investigate which conditions have to be met in order to achieve net energy savings. The proposed technique has been implemented and evaluated for potential savings on simulated traces of LTE data. The resulting predictor is designed to be trained online, without any prior system knowledge. For a fair evaluation and comparison, the power consumption of the training phase is also considered in the analysis. It is shown that energy savings of up to 23.9 % may be obtained on a modem for scenarios such as HTTP streaming.},
booktitle = {Proceedings of the 21st International Workshop on Software and Compilers for Embedded Systems},
pages = {18–26},
numpages = {9},
keywords = {Reinforcement Learning, Predictability, Power Efficient Data Transmission, Machine Learning, LTE Radio Systems, Dynamic Network Communication},
location = {Sankt Goar, Germany},
series = {SCOPES '18}
}

@article{10.1007/s10766-016-0417-6,
author = {Allombert, V. and Gava, F. and Tesson, J.},
title = {Multi-ML: Programming Multi-BSP Algorithms in ML},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-016-0417-6},
doi = {10.1007/s10766-016-0417-6},
abstract = {bsp is a bridging model between abstract execution and concrete parallel systems. Structure and abstraction brought by bsp allow to have portable parallel programs with scalable performance predictions, without dealing with low-level details of architectures. In the past, we designed bsml for programming bsp algorithms in ml. However, the simplicity of the bsp model does not fit the complexity of today's hierarchical architectures such as clusters of machines with multiple multi-core processors. The multi-bsp model is an extension of the bsp model which brings a tree-based view of nested components of hierarchical architectures. To program multi-bsp algorithms in ml, we propose the multi-ml language as an extension of bsml where a specific kind of recursion is used to go through a hierarchy of computing nodes. We define a formal semantics of the language and present preliminary experiments which show performance improvements with respect to bsml.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {340–361},
numpages = {22},
keywords = {multi-bsp, ml, bsp, Parallel programming}
}

@article{10.1016/j.eswa.2016.01.035,
author = {Xu, Jingxin and Denman, Simon and Fookes, Clinton and Sridharan, Sridha},
title = {Detecting rare events using Kullback-Leibler divergence},
year = {2016},
issue_date = {July 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {54},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.01.035},
doi = {10.1016/j.eswa.2016.01.035},
abstract = {We present a weakly supervised approach for rare event detection.Coarse annotation, denoting only roughly when an event occurs is needed.The approach leverages the rare nature of the target events to its advantage.We demonstrate the proposed approach on the popular MIT traffic dataset.State-of-the-art performance is shown, alongside being real-time capable. Video surveillance infrastructure has been widely installed in public places for security purposes. However, live video feeds are typically monitored by human staff, making the detection of important events as they occur difficult. As such, an expert system that can automatically detect events of interest in surveillance footage is highly desirable. Although a number of approaches have been proposed, they have significant limitations: supervised approaches, which can detect a specific event, ideally require a large number of samples with the event spatially and temporally localised; while unsupervised approaches, which do not require this demanding annotation, can only detect whether an event is abnormal and not specific event types. To overcome these problems, we formulate a weakly-supervised approach using Kullback-Leibler (KL) divergence to detect rare events. The proposed approach leverages the sparse nature of the target events to its advantage, and we show that this data imbalance guarantees the existence of a decision boundary to separate samples that contain the target event from those that do not. This trait, combined with the coarse annotation used by weakly supervised learning (that only indicates approximately when an event occurs), greatly reduces the annotation burden while retaining the ability to detect specific events. Furthermore, the proposed classifier requires only a decision threshold, simplifying its use compared to other weakly supervised approaches. We show that the proposed approach outperforms state-of-the-art methods on a popular real-world traffic surveillance dataset, while preserving real time performance.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {13–28},
numpages = {16},
keywords = {Weakly supervised learning, Kullback-Leibler divergence, Event detection, Anomaly detection}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1609/aaai.v33i01.33015117,
author = {Tang, Ying-Peng and Huang, Sheng-Jun},
title = {Self-paced active learning: query the right thing at the right time},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015117},
doi = {10.1609/aaai.v33i01.33015117},
abstract = {Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {628},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.specom.2019.09.003,
author = {Shirzhiyan, Zahra and Shamsi, Elham and Jafarpisheh, Amir Salar and Jafari, Amir Homayoun},
title = {Objective classification of auditory brainstem responses to consonant-vowel syllables using local discriminant bases},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.09.003},
doi = {10.1016/j.specom.2019.09.003},
journal = {Speech Commun.},
month = nov,
pages = {36–48},
numpages = {13},
keywords = {Local discriminant bases, Speech encoding, Speech ABR}
}

@article{10.1016/j.eswa.2015.05.052,
author = {Yahyaoui, Hamdi and Own, Hala S. and Malik, Zaki},
title = {Modeling and classification of service behaviors},
year = {2015},
issue_date = {Nov 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {21},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.05.052},
doi = {10.1016/j.eswa.2015.05.052},
journal = {Expert Syst. Appl.},
month = nov,
pages = {7610–7619},
numpages = {10},
keywords = {Rough set, Classification, Pattern, Behavior, Services}
}

@article{10.1145/3345628,
author = {Kim, Yunho and Mun, Seokhyeon and Yoo, Shin and Kim, Moonzoo},
title = {Precise Learn-to-Rank Fault Localization Using Dynamic and Static Features of Target Programs},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3345628},
doi = {10.1145/3345628},
abstract = {Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information and structural complexity of program entities. All such information is used by GP to train a ranking model for fault localization. The empirical evaluation on 65 real-world faults from CoREBench, 84 artificial faults from SIR, and 310 real-world faults from Defects4J shows that PRINCE outperforms the state-of-the-art SBFL, MBFL, and learn-to-rank techniques significantly. PRINCE localizes a fault after reviewing 2.4% of the executed statements on average (4.2 and 3.0 times more precise than the best of the compared SBFL and MBFL techniques, respectively). Also, PRINCE ranks 52.9% of the target faults within the top ten suspicious statements.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {23},
numpages = {34},
keywords = {source file characteristics, mutation analysis, machine learning, Fault localization}
}

@article{10.1109/TCAD.2004.829817,
author = {Sze, C. N. and Wang, Ting-Chi and Wang, L. -C.},
title = {Multilevel circuit clustering for delay minimization},
year = {2006},
issue_date = {November 2006},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {0278-0070},
url = {https://doi.org/10.1109/TCAD.2004.829817},
doi = {10.1109/TCAD.2004.829817},
abstract = {In this paper, an effective algorithm is presented for multilevel circuit clustering for delay minimization, and is applicable to hierarchical field programmable gate arrays. With a novel graph contraction technique, which allows some crucial delay information of a lower-level clustering to be maintained in the contracted graph, our algorithm recursively divides the lower-level clustering into the next higher-level one in a way that each recursive clustering step is accomplished by applying a modified single-level circuit clustering algorithm based on . We test our algorithm on the two-level clustering problem and compare it with the latest algorithm in . Experimental results show that our algorithm achieves, on average, 12% more delay reduction when compared to the best results (from TLC with full node-duplication) in . In fact, our algorithm is the first one for the general multilevel circuit clustering problem with more than two levels.},
journal = {Trans. Comp.-Aided Des. Integ. Cir. Sys.},
month = nov,
pages = {1073–1085},
numpages = {13},
keywords = {very large scale integration, timing optimization, physical design, performance optimization, VLSI, Partitioning}
}

@article{10.1002/spe.2297,
author = {Shobaki, Ghassan and Sakka, Laith and Abu Rmaileh, Najm Eldeen and Al-Hamash, Hasan},
title = {Experimental evaluation of various register-pressure-reduction heuristics},
year = {2015},
issue_date = {November 2015},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {45},
number = {11},
issn = {0038-0644},
url = {https://doi.org/10.1002/spe.2297},
doi = {10.1002/spe.2297},
abstract = {Minimizing the amount of spill code is still an open problem in code generation and optimization. The amount of spill code depends on both the register allocation algorithm and the pre-allocation instruction scheduling algorithm that controls the register pressure. In this paper, we focus on the impact of pre-allocation instruction scheduling on the amount of spill code. Many heuristic techniques have been proposed to do instruction scheduling with the objective of minimizing register pressure and consequently the amount of spill code. However, the performance of these heuristic techniques has not been studied relative to optimality on real large-scale programs. In this paper, we present an experimental study that evaluates the performance of several pre-allocation scheduling heuristics. The evaluation involves computing an experimental lower bound on the size of gap between each heuristic's performance and optimal performance. We also propose a simple heuristic technique based on a specific permutation of two basic priority schemes and experimentally evaluate the performance of this technique compared with other heuristics, including the heuristics implemented in the LLVM open-source Compiler. The evaluation is carried out by running SPEC CPU2006 on real x86-64 hardware and measuring both the amount of spill code and the execution time. The results of our study show that the proposed heuristic technique gives better overall performance than LLVM's best heuristic on x86-64, although it produces slightly more spilling. The proposed heuristic has better overall performance, because it achieves a better balance between register pressure and instruction-level parallelism ILP. This result shows the importance of ILP in pre-allocation scheduling even on out-of-order machines. Furthermore, the results of the study show that there is a large gap between the performance of any of the studied heuristics and optimal performance; even the best heuristic in the study produces significantly more spill code than the optimal amount. This experimental result quantifies the intuitive belief that it is unlikely to find a heuristic that works well in all cases, thus showing the need for more rigorous solutions using combinatorial approaches. The paper discusses the challenges and complexities that are involved in developing such rigorous solutions. Copyright © 2014 John Wiley &amp; Sons, Ltd.},
journal = {Softw. Pract. Exper.},
month = nov,
pages = {1497–1517},
numpages = {21},
keywords = {spill code minimization, register pressure reduction, performance optimization, instruction scheduling, experimental algorithms, compiler optimizations, combinatorial optimization, NP-complete problems}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@article{10.1016/j.knosys.2017.03.026,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Tang, Jia-Fu and Herrera, Francisco},
title = {Exploring the effectiveness of dynamic ensemble selection in the one-versus-one scheme},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.026},
doi = {10.1016/j.knosys.2017.03.026},
abstract = {The One-versus-One (OVO) strategy is one of the most common and effective techniques to deal with multi-class classification problems. The basic idea of an OVO scheme is to divide a multi-class classification problem into several easier-to-solve binary classification problems with considering each possible pair of classes from the original problem, which is then built into a binary classifier by an independent base learner. In this study, we propose a novel methodology which attempts to select a group of base classifiers in each pairwise dataset for each unknown pattern. To implement this, the Dynamic Ensemble Selection (DES) method based on a competence measure is employed to select the most appropriate ensemble in each binary classification problem derived from the OVO decomposition. In order to verify the validity and effectiveness of our proposed method, we carry out a thorough experimental study. We first compare our proposal with several state-of-the-art approaches. Then, we perform the comparison of several well-known aggregation strategies to combine the binary ensemble obtained by Dynamic Ensemble Selection. Finally, we explore whether further improvement can be achieved by considering the competence-based method in OVO scheme. The extracted findings drawn from the empirical analysis are supported by the proper statistical analysis and indicate that there is a positive synergy between the DES method and the Distance-based Relative Competence Weighting (DRCW) approach for the OVO scheme.},
journal = {Know.-Based Syst.},
month = jun,
pages = {53–63},
numpages = {11},
keywords = {Pairwise learning, One-versus-One, Multi-classification, Dynamic ensemble selection, Decomposition strategies}
}

@inproceedings{10.1145/2503210.2503278,
author = {Behzad, Babak and Luu, Huong Vu Thanh and Huchette, Joseph and Byna, Surendra and Prabhat and Aydt, Ruth and Koziol, Quincey and Snir, Marc},
title = {Taming parallel I/O complexity with auto-tuning},
year = {2013},
isbn = {9781450323789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503210.2503278},
doi = {10.1145/2503210.2503278},
abstract = {We present an auto-tuning system for optimizing I/O performance of HDF5 applications and demonstrate its value across platforms, applications, and at scale. The system uses a genetic algorithm to search a large space of tunable parameters and to identify effective settings at all layers of the parallel I/O stack. The parameter settings are applied transparently by the auto-tuning system via dynamically intercepted HDF5 calls.To validate our auto-tuning system, we applied it to three I/O benchmarks (VPIC, VORPAL, and GCRM) that replicate the I/O activity of their respective applications. We tested the system with different weak-scaling configurations (128, 2048, and 4096 CPU cores) that generate 30 GB to 1 TB of data, and executed these configurations on diverse HPC platforms (Cray XE6, IBM BG/P, and Dell Cluster). In all cases, the auto-tuning framework identified tunable parameters that substantially improved write performance over default system settings. We consistently demonstrate I/O write speedups between 2x and 100x for test configurations.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {68},
numpages = {12},
keywords = {performance optimization, parallel file systems, parallel I/O, auto-tuning},
location = {Denver, Colorado},
series = {SC '13}
}

@inproceedings{10.1007/978-3-030-77385-4_42,
author = {Halilaj, Lavdim and Dindorkar, Ishan and L\"{u}ttin, J\"{u}rgen and Rothermel, Susanne},
title = {A Knowledge Graph-Based Approach for Situation Comprehension in Driving Scenarios},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_42},
doi = {10.1007/978-3-030-77385-4_42},
abstract = {Making an informed and right decision poses huge challenges for drivers in day-to-day traffic situations. This task vastly depends on many subjective and objective factors, including the current driver state, her destination, personal preferences and abilities as well as surrounding environment. In this paper, we present CoSI (Context and Situation Intelligence), a Knowledge Graph (KG)-based approach for fusing and organizing heterogeneous types and sources of information. The KG serves as a coherence layer representing information in the form of entities and their inter-relationships augmented with additional semantic axioms. Harnessing the power of axiomatic rules and reasoning capabilities enables inferring additional knowledge from what is already encoded. Thus, dedicated components exploit and consume the semantically enriched information to perform tasks such as situation classification, difficulty assessment, and trajectory prediction. Further, we generated a synthetic dataset to simulate real driving scenarios with a large range of driving styles and vehicle configurations. We use KG embedding techniques based on a Graph Neural Network (GNN) architecture for a classification task of driving situations and achieve over 95% accuracy whereas vector-based approaches achieve only 75% accuracy for the same task. The results suggest that the KG-based information representation combined with GNN are well suited for situation understanding tasks as required in driver assistance and automated driving systems.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {699–716},
numpages = {18},
keywords = {Graph neural network, Knowledge graph embedding, Knowledge graph, Situation comprehension}
}

@inproceedings{10.1145/3462757.3466101,
author = {McConnell, Devin J. and Zhu, James and Pandya, Sachin and Aguiar, Derek},
title = {Case-level prediction of motion outcomes in civil litigation},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466101},
doi = {10.1145/3462757.3466101},
abstract = {Lawyers regularly predict court outcomes to make strategic decisions, including when, if at all, to sue or settle, what to argue, and how to reduce their clients' liability risk. Yet, lawyer predictions tend to be poorly calibrated and biased, which exacerbate unjustifiable disparities in civil case outcomes. Current machine learning (ML) approaches for predicting court outcomes are typically constrained to final dispositions or are based on features unavailable in real-time during litigation, like judicial opinions. Here, we present the first ML-based methods to support lawyer and client decision making in real-time for motion filings in civil proceedings. Using the State of Connecticut Judicial Branch administrative data and court case documents, we trained six classifiers to predict motion to strike outcomes in tort and vehicular cases between July 1, 2004 and February 18, 2019. Integrating dense word embeddings from complaint documents, which contain information specific to the claims alleged, with the Judicial Branch data improved classification accuracy across all models. Subsequent models defined using a novel attorney case-entropy feature, dense word embeddings using corpus specific TF-IDF weightings, and algorithmic classification rules yielded the best predictor, Adaboost, with a classification accuracy of 64.4%. An analysis of feature importance weights confirmed the usefulness of incorporating attorney case-entropy and natural language features from complaint documents. Since all features used in model training are available during litigation, these methods will help lawyers make better predictions than they otherwise could given disparities in lawyer and client resources. All ML models, training code, and evaluation scripts are available at https://github.com/aguiarlab/motionpredict.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {99–108},
numpages = {10},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@article{10.1016/j.compeleceng.2017.06.020,
author = {Zhou, Bo and Zhang, Quan and Shi, Qi and Yang, Qiang and Yang, Po and Yu, Yinyan},
title = {Measuring web service security in the era of Internet of Things},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.06.020},
doi = {10.1016/j.compeleceng.2017.06.020},
abstract = {HihglightsComprehensive study on how to measure web service security.Solution uniforms fuzzy terms in SLA to calculate an overall security value.A practical solution allows direct comparison and ranking of web services.Tested by assessing the security features offered by major cloud services in market. Display Omitted Technologies such as Internet of Things allow small devices to offer web-based services in an open and dynamic networking environments on a massive scale. End users or service consumers face a hard decision over which service to choose among the available ones, as security holds a key in the decision making process. In this paper a base linguistic evaluation set is designed, based on which all the other fuzzy term sets that used for describing security attributes are uniformed and integrated for calculating an overall security value of the services. This work, to the best of our knowledge, is the first practical solution to offer direct comparisons and rankings of network services based on multiple security attributes such as confidentiality, availability, privacy and accountability. We analysed four major cloud service platforms to illustrate the proposed approach.},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {305–315},
numpages = {11},
keywords = {Web service, Service level agreement, Security measurement and evaluation, Quantitative service security, Multiple attribute decision making, Linguistic evaluation}
}

@article{10.1016/j.aei.2019.03.006,
author = {Si, Binghui and Wang, Jianguo and Yao, Xinyue and Shi, Xing and Jin, Xing and Zhou, Xin},
title = {Multi-objective optimization design of a complex building based on an artificial neural network and performance evaluation of algorithms},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2019.03.006},
doi = {10.1016/j.aei.2019.03.006},
journal = {Adv. Eng. Inform.},
month = apr,
pages = {93–109},
numpages = {17},
keywords = {Real-world building design, Performance evaluation of algorithms, Multi-objective optimization algorithms, Artificial neural network, Building design optimization}
}

@inproceedings{10.1145/3459637.3482458,
author = {Keramati, Mahsa and Zohrevand, Zahra and Gl\"{a}sser, Uwe},
title = {Norma: A Hybrid Feature Alignment for Class-Aware Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482458},
doi = {10.1145/3459637.3482458},
abstract = {Unsupervised domain adaptation is the problem of transferring extracted knowledge from a labeled source domain to an unlabeled target domain. To achieve discriminative domain adaptation recent studies take advantage of target sample pseudo-labels to impose class-aware distribution alignment across the source and target domains. Still, they have some shortcomings such as making decisions based on inaccurate pseudo-labeled samples that mislead the adaptation process. In this paper, we propose a progressive deep feature alignment, called Norma, to tackle class-aware unsupervised domain adaptation for image classification by enforcing inter-class compactness and intra-class discrepancy through a hybrid learning process. To this end, Norma's optimization process is defined based on a novel triplet loss which not only addresses soft prototype alignment but also pushes away multiple negative centroids. Also, to extract maximum discriminative domain knowledge per iteration, we propose a joint positive and negative learning procedure along with an uncertainty-guided progressive pseudo-labeling on the basis of prototype-based clustering and conditional probability. Our experimental results on several benchmarks demonstrate that Norma outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {833–843},
numpages = {11},
keywords = {transfer learning, pseudo-labeling, negative learning, image classification, deep-metric learning, class-aware alignment, adversarial unsupervised domain adaptation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1145/3425866,
author = {Liu, Xuanzhe and Wang, Shangguang and Ma, Yun and Zhang, Ying and Mei, Qiaozhu and Liu, Yunxin and Huang, Gang},
title = {Operating Systems for Resource-adaptive Intelligent Software: Challenges and Opportunities},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3425866},
doi = {10.1145/3425866},
abstract = {The past decades witnessed the fast and wide deployment of Internet. The Internet has bred the ubiquitous computing environment that is spanning the cloud, edge, mobile devices, and IoT. Software running over such a ubiquitous computing environment environment is eating the world. A recently emerging trend of Internet-based software systems is “resource adaptive,” i.e., software systems should be robust and intelligent enough to the changes of heterogeneous resources, both physical and logical, provided by their running environment. To keep pace of such a trend, we argue that some considerations should be taken into account for the future operating system design and implementation. From the structural perspective, rather than the “monolithic OS” that manages the aggregated resources on the single machine, the OS should be dynamically composed over the distributed resources and flexibly adapt to the resource and environment changes. Meanwhile, the OS should leverage advanced machine/deep learning techniques to derive configurations and policies and automatically learn to tune itself and schedule resources. This article envisions our recent thinking of the new OS abstraction, namely, ServiceOS, for future resource-adaptive intelligent software systems. The idea of ServiceOS is inspired by the delivery model of “Software-as-a-Service” that is supported by the Service-Oriented Architecture (SOA). The key principle of ServiceOS is based on resource disaggregation, resource provisioning as a service, and learning-based resource scheduling and allocation. The major goal of this article is not providing an immediately deployable OS. Instead, we aim to summarize the challenges and potentially promising opportunities and try to provide some practical implications for researchers and practitioners.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {27},
numpages = {19},
keywords = {machine learning, service-oriented, resource disaggregation, Operating systems}
}

@article{10.1016/j.neucom.2015.07.152,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng},
title = {Hessian regularization by patch alignment framework},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {204},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.07.152},
doi = {10.1016/j.neucom.2015.07.152},
abstract = {In recent years, semi-supervised learning has played a key part in large-scale image management, where usually only a few images are labeled. To address this problem, many representative works have been reported, including transductive SVM, universum SVM, co-training and graph-based methods. The prominent method is the patch alignment framework, which unifies the traditional spectral analysis methods. In this paper, we propose Hessian regression based on the patch alignment framework. In particular, we construct a Hessian using the patch alignment framework and apply it to regression problems. To the best of our knowledge, there is no report on Hessian construction from the patch alignment viewpoint. Compared with the traditional Laplacian regularization, Hessian can better match the data and then leverage the performance. To validate the effectiveness of the proposed method, we conduct human face recognition experiments on a celebrity face dataset. The experimental results demonstrate the superiority of the proposed solution in human face classification.},
journal = {Neurocomput.},
month = sep,
pages = {183–188},
numpages = {6},
keywords = {Semi-supervised learning, Patch alignment, Least squares, Hessian}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, \"{O}nder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Vector space model, R, Model-Driven Engineering, Model comparison, Hierarchical clustering}
}

@article{10.1155/2020/8863420,
author = {Lin, Shirong and Jiang, Shouxu and Femminella, Mauro},
title = {Learn-ing-Based On-AP TCP Performance Enhancement},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/8863420},
doi = {10.1155/2020/8863420},
abstract = {Data transmissions suffer from TCP’s poor performance since the introduction of the first commercial wireless services in the 1990s. Recent years have witnessed a surge of academia and industry activities in the field of TCP performance optimization. For a TCP flow whose last hop is a wireless link, congestions in the last hop dominate its performance. We implement an integral data sampling, network monitoring, and rate control software-defined wireless networking (SDWN) system. By analysing our sampled data, we find that there exist strong relationships between congestion packet loss behaviors and the instant cross-layer network metric measurements (states). We utilize these qualitative relationships to predict future congestions in wireless links and enhance TCP performance by launch necessary rate control locally on the access points (AP) before the congestions. We also implement modeling and rate control modules on this platform. Our platform senses the instant wireless dynamic and takes actions promptly to avoid future congestions. We conduct real-world experiments to evaluate its performance. The experiment results show that our methods outperform the bottleneck bandwidth and RTT (BBR) protocol and a recently proposed protocol Vivace on throughput, delay, and jitter performance at least 16.5%, 25%, and 12.6%, respectively.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {17}
}

@article{10.1016/j.knosys.2016.05.048,
author = {Zhang, Zhongliang and Krawczyk, Bartosz and Garc\`{\i}a, Salvador and Rosales-P\'{e}rez, Alejandro and Herrera, Francisco},
title = {Empowering one-vs-one decomposition with ensemble learning for multi-class imbalanced data},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.048},
doi = {10.1016/j.knosys.2016.05.048},
abstract = {Extending binary ensemble techniques to multi-class imbalanced data.OVO scheme enhancement for multi-class imbalanced data by ensemble learning.A complete experimental study of comparison of the ensemble learning techniques with OVO.Study of the impact of base classifiers used in the proposed scenario. Multi-class imbalance classification problems occur in many real-world applications, which suffer from the quite different distribution of classes. Decomposition strategies are well-known techniques to address the classification problems involving multiple classes. Among them binary approaches using one-vs-one and one-vs-all has gained a significant attention from the research community. They allow to divide multi-class problems into several easier-to-solve two-class sub-problems. In this study we develop an exhaustive empirical analysis to explore the possibility of empowering the one-vs-one scheme for multi-class imbalance classification problems with applying binary ensemble learning approaches. We examine several state-of-the-art ensemble learning methods proposed for addressing the imbalance problems to solve the pairwise tasks derived from the multi-class data set. Then the aggregation strategy is employed to combine the binary ensemble outputs to reconstruct the original multi-class task. We present a detailed experimental study of the proposed approach, supported by the statistical analysis. The results indicate the high effectiveness of ensemble learning with one-vs-one scheme in dealing with the multi-class imbalance classification problems.},
journal = {Know.-Based Syst.},
month = aug,
pages = {251–263},
numpages = {13},
keywords = {Multi-class classification, Imbalanced data, Ensemble learning, Classifier combination, Binary decomposition}
}

@article{10.1504/ijipt.2020.108001,
author = {Zhou, Yucai and Xu, Xiaoya and Liu, Caihong and Li, Yuelin},
title = {Optimisation method of MAC protocol based on SVM neural network in VANET},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {3},
issn = {1743-8209},
url = {https://doi.org/10.1504/ijipt.2020.108001},
doi = {10.1504/ijipt.2020.108001},
abstract = {This paper analyses the function expression of the optimal minimum competition window which integrates the network node number, average collision data frame length and sending rate of data frame. At the same time, the proposed optimised method of MAC protocol combines with the SVN neural network which can memory communication environment which include node density and mobile velocity. Each terminal node in the network runs proposed MAC protocol optimisation algorithm based on this function expression to adaptive adjust their minimum competition window and back off the optimal value to improve the network performance. The simulation results show that the effort of optimised algorithm in the Ad Hoc system is limited for unsaturated business VANET; but high accuracy and effect of the optimised algorithm in aspects of throughput and transmission delay has improved significantly for saturated business of VANET.},
journal = {Int. J. Internet Protoc. Technol.},
month = jan,
pages = {158–166},
numpages = {8},
keywords = {SVM neural network, MAC protocol, IEEE 80211, VANET}
}

@article{10.1007/s10470-014-0284-2,
author = {Mohanty, Saraju P. and Kougianos, Elias},
title = {Polynomial metamodel based fast optimization of nano-CMOS oscillator circuits},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {3},
issn = {0925-1030},
url = {https://doi.org/10.1007/s10470-014-0284-2},
doi = {10.1007/s10470-014-0284-2},
abstract = {Modern consumer electronics are designed as analog/mixed-signal systems-on-chip (AMS-SoCs). In an AMS-SoC, the analog and mixed-signal portions have not received systematic attention due to their complex nature and the fact that their optimization and simulation consume significant portions of the design cycle time. This paper presents a new approach to reduce the design cycle time by combining accurate polynomial metamodels and optimization algorithms. The approach relies on a mathematical representation (metamodel or surrogate model) of AMS-SoC subsystems/components. Polynomial metamodels are created from post-layout parasitic netlists and provide an accurate representation for each figure-of-merit over the entire design space of the AMS-SoC component. The metamodel approach saves a very significant amount of time during design iterations. Polynomial metamodels are reusable and language independent. Three algorithms are investigated to compare the speed for optimization on the polynomial metamodels. Two widely used circuits have been designed in two different technologies as comparative case studies: an 180 nm LC-VCO and a 45 nm ring oscillator (RO). Experimental results prove that the metamodel-based optimization achieved speed-up as high as 21,600  $$times$$   for the LC-VCO circuit and 11,750  $$times$$   for the RO in comparison to the actual circuit netlist-based (SPICE) optimization, with less than 1 % error. Thus, the paper demonstrates that the polynomial metamodeling approach to the design problem is an effective and accurate means for fast design space exploration and optimization.},
journal = {Analog Integr. Circuits Signal Process.},
month = jun,
pages = {437–453},
numpages = {17},
keywords = {Polynomial metamodeling, Performance optimization, Oscillator circuits, Optimization algorithms, Mixed-signal circuits}
}

@article{10.1016/j.cose.2021.102417,
author = {Sun, Hao and Cui, Lei and Li, Lun and Ding, Zhenquan and Hao, Zhiyu and Cui, Jiancong and Liu, Peng},
title = {VDSimilar: Vulnerability detection based on code similarity of vulnerabilities and patches},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {110},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102417},
doi = {10.1016/j.cose.2021.102417},
journal = {Comput. Secur.},
month = nov,
numpages = {14},
keywords = {Code similarity, Vulnerability detection, Attention, BiLSTM, Siamese network}
}

@inproceedings{10.1145/3087556.3087566,
author = {Kaler, Tim and He, Yuxiong and Elnikety, Sameh},
title = {Optimal Reissue Policies for Reducing Tail Latency},
year = {2017},
isbn = {9781450345934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3087556.3087566},
doi = {10.1145/3087556.3087566},
abstract = {Interactive services send redundant requests to multiple different replicas to meet stringent tail latency requirements. These additional (reissue) requests mitigate the impact of non-deterministic delays within the system and thus increase the probability of receiving an on-time response.There are two existing approaches of using reissue requests to reduce tail latency. (1) Reissue requests immediately to one or more replicas, which multiplies the load and runs the risk of overloading the system. (2) Reissue requests if not completed after a fixed delay. The delay helps to bound the number of extra reissue requests, but it also reduces the chance for those requests to respond before a tail latency target.We introduce a new family of reissue policies, Single-Time / Random (SingleR), that reissue requests after a delay d with probability q. SingleR employs randomness to bound the reissue rate, while allowing requests to be reissued early enough so they have sufficient time to respond, exploiting the benefits of both immediate and delayed reissue of prior work. We formally prove, within a simplified analytical model, that SingleR is optimal even when compared to more complex policies that reissue multiple times.To use SingleR for interactive services, we provide efficient algorithms for calculating optimal reissue delay and probability from response time logs through data-driven approach. We apply iterative adaptation for systems with load-dependent queuing delays. The key advantage of this data-driven approach is its wide applicability and effectiveness to systems with various design choices and workload properties.We evaluated SingleR policies thoroughly. We use simulation to illustrate its internals and demonstrate its robustness to a wide range of workloads. We conduct system experiments on the Redis key-value store and Lucene search server. The results show that for utilizations ranging from 40-60%, SingleR reduces the 99th-percentile latency of Redis by 30-$70% by reissuing only 2% of requests, and the 99th-percentile latency of Lucene by 15-25% by reissuing 1% only.},
booktitle = {Proceedings of the 29th ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {195–206},
numpages = {12},
keywords = {tail-latency, reissue policy, redundancy, performance optimization, interactive services, distributed computing, data-driven algorithms},
location = {Washington, DC, USA},
series = {SPAA '17}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {empirical study, expectation maximization, metric threshold}
}

@inproceedings{10.5555/3504035.3504690,
author = {Neill, James O' and Buitelaar, Paul},
title = {Few shot transfer learning between word relatedness and similarity tasks using a gated recurrent siamese network},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Word similarity and word relatedness are fundamental to natural language processing and more generally, understanding how humans relate concepts in semantic memory. A growing number of datasets are being proposed as evaluation benchmarks, however, the heterogeneity and focus of each respective dataset makes it difficult to draw plausible conclusions as to how a unified semantic model would perform. Additionally, we want to identify the transferability of knowledge obtained from one task to another, within the same domain and across domains. Hence, this paper first presents an evaluation and comparison of eight chosen datasets tested using the best performing regression models. As a baseline, we present regression models that incorporate both lexical features and word embeddings to produce consistent and competitive results compared to the state of the art. We present our main contribution, the best performing model across seven of the eight datasets - a Gated Recurrent Siamese Network that learns relationships between lexical word definitions. A parameter transfer learning strategy is employed for the Siamese Network. Subsequently, we present a secondary contribution which is the best performing non-sequential model: an Inductive and Transductive Transfer Learning strategy for transferring decision trees within a Random Forest to a target task that is learned from only few instances. The method involves measuring semantic distance between hidden factored matrix representations of decision tree traversal matrices.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {655},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s10796-014-9496-3,
author = {Own, Hala S. and Yahyaoui, Hamdi},
title = {Rough set based classification of real world Web services},
year = {2015},
issue_date = {December  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {6},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-014-9496-3},
doi = {10.1007/s10796-014-9496-3},
abstract = {We propose in this paper a rough set based classification technique for real world Web services. The set of Web service quality attributes is reduced to a core set, which represents the most relevant set of attributes. Such reduction is achieved through the analysis of the QWS dataset using rough set theory. The core set is then used during the training phase to derive a minimal set of decision rules. These rules are applied during the classification phase to classify real world Web services into one of four categories: Platinum, Gold, Silver, and Bronze. Our experimental results show that the use of rough set theory to classify QWS services allows to have better classification accuracy results compared to other classification techniques, which have been recently applied on QWS.},
journal = {Information Systems Frontiers},
month = dec,
pages = {1301–1311},
numpages = {11},
keywords = {Web services, Rough set theory, Reduction, Quality of service, Classification}
}

@article{10.1016/j.imavis.2016.06.005,
author = {Leng, Mengjun and Moutafis, Panagiotis and Kakadiaris, Ioannis A.},
title = {Joint prototype and metric learning for image set classification: Application to video face identification},
year = {2017},
issue_date = {Feb 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {58},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.06.005},
doi = {10.1016/j.imavis.2016.06.005},
journal = {Image Vision Comput.},
month = feb,
pages = {204–213},
numpages = {10},
keywords = {Video face recognition, Prototype learning, Metric learning, Image set classification}
}

@inproceedings{10.1145/3469096.3469872,
author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
title = {On minimizing cost in legal document review workflows},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469872},
doi = {10.1145/3469096.3469872},
abstract = {Technology-assisted review (TAR) refers to human-in-the-loop machine learning workflows for document review in legal discovery and other high recall review tasks. Attorneys and legal technologists have debated whether review should be a single iterative process (one-phase TAR workflows) or whether model training and review should be separate (two-phase TAR workflows), with implications for the choice of active learning algorithm. The relative cost of manual labeling for different purposes (training vs. review) and of different documents (positive vs. negative examples) is a key and neglected factor in this debate. Using a novel cost dynamics analysis, we show analytically and empirically that these relative costs strongly impact whether a one-phase or two-phase workflow minimizes cost. We also show how category prevalence, classification task difficulty, and collection size impact the optimal choice not only of workflow type, but of active learning method and stopping point.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {30},
numpages = {10},
keywords = {total recall, high-recall retrieval, cost modeling, active learning},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@inproceedings{10.1145/3356250.3361962,
author = {Yao, Shuochao and Wang, Tianshi and Li, Jinyang and Abdelzaher, Tarek},
title = {Stardust: A deep learning serving system in IoT: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361962},
doi = {10.1145/3356250.3361962},
abstract = {The deep neural network becomes an increasingly crucial component in recent intelligent applications. The excessive resource consumptions of state-of-the-art neural networks, however, remains a huge impediment towards their widespread deployment in the Internet of Things (IoT). In this paper, we propose an IoT-oriented deep learning serving system, Stardust, that accelerates the neural network inference to improve the quality of IoT services. Stardust integrates several joint contributions from both the system and AI perspectives, including system performance predictor, model compression, and compressive offloading. On one hand, the performance predictor profiles and predicts the runtime characteristics of neural network operations on a particular device with the targeted runtime environment, which enables a hardware and software oriented performance optimization during model compression and offloading. On the other hand, the model compression minimizes the computation time of neural networks on different devices, and the compressive offloading diminishes the network data transferring time during the mobile-edge offloading. Moreover, all these optimizations can be done with almost no compromise on inference accuracy. The integration of these modules, therefore, collaboratively reduce the end-to-end latency of serving deep learning services that reside across embedded/mobile devices and edge servers. We deploy illustrative applications on Stardust, performing human perception tasks with on-device camera microphone and motion sensors to demonstrate the capability of Stardust serving system.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {402–403},
numpages = {2},
keywords = {offloading, model compression, deep learning, IoT},
location = {New York, New York},
series = {SenSys '19}
}

@article{10.3233/JIFS-201990,
author = {Hamed, Nadir O. and Samak, Ahmed H. and Ahmad, Mostafa A.},
title = {Cloud e-mail security: An accurate e-mail spam classification based on enhanced binary differential evolution (BDE) algorithm},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-201990},
doi = {10.3233/JIFS-201990},
abstract = {The evolution of technology has brought new challenges and opportunities for the different dimensions of feature space. The higher dimension of the feature space is one of the most critical issues in e-mail classification problems due to accuracy considerations. The problem of finding the subset features that significantly influence the performance of e-mail spam classification has become one of the important challenges. This paper proposes to overcome such a problem, an intelligent approach to Binary Differential Evolution Support Vector Machine (BDE-SVM). The proposed approach enhances the Binary Differential Evolution (BDE) algorithm based on the correlation coefficient as a fitness function to select the significant subset feature evaluated by an SVM classifier. To our best of knowledge, the correlation coefficient as the fitness function has not been used in the differential evolution algorithm before. The selected subset feature is used to assess the most features that contribute to the reliability of the email spam classification. The finding of the enhanced BDE is to present a powerful accuracy. The tests were conducted using “Spambase” and “SpamAssassin.” Identified benchmark datasets are to assess the feasibility of the proposed solution. The result with full-feature accuracy was 93.55 percent compared to the proposed BDE-SVM approach, which is 93.99 percent. Empirical findings also show that our method is capable of effectively increasing the number of features required to enhance the reliability of the email spam classification.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {5943–5955},
numpages = {13},
keywords = {support vector machine (SVM), differential evolution (DE), e-mail classification, e-mail, Feature selection}
}

@article{10.1177/1094342012444795,
author = {Malas, Tareq and Ahmadia, Aron J. and Brown, Jed and Gunnels, John A. and Keyes, David E.},
title = {Optimizing the performance of streaming numerical kernels on the IBM Blue Gene/P PowerPC 450 processor},
year = {2013},
issue_date = {May       2013},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {27},
number = {2},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342012444795},
doi = {10.1177/1094342012444795},
abstract = {Several emerging petascale architectures use energy-efficient processors with vectorized computational units and in-order thread processing. On these architectures the sustained performance of streaming numerical kernels, ubiquitous in the solution of partial differential equations, represents a challenge despite the regularity of memory access. Sophisticated optimization techniques are required to fully utilize the CPU. We propose a new method for constructing streaming numerical kernels using a high-level assembly synthesis and optimization framework. We describe an implementation of this method in Python targeting the IBM ® Blue Gene ®/P supercomputer's PowerPC ® 450 core. This paper details the high-level design, construction, simulation, verification, and analysis of these kernels utilizing a subset of the CPU's instruction set. We demonstrate the effectiveness of our approach by implementing several three-dimensional stencil kernels over a variety of cached memory scenarios and analyzing the mechanically scheduled variants, including a 27-point stencil achieving a 1.7\texttimes{} speedup over the best previously published results.},
journal = {Int. J. High Perform. Comput. Appl.},
month = may,
pages = {193–209},
numpages = {17},
keywords = {performance optimization, high-performance computing, code generation, SIMD, Blue Gene/P}
}

@inproceedings{10.1145/3395035.3425182,
author = {Kaya, Heysem and Verkholyak, Oxana and Markitantov, Maxim and Karpov, Alexey},
title = {Combining Clustering and Functionals based Acoustic Feature Representations for Classification of Baby Sounds},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425182},
doi = {10.1145/3395035.3425182},
abstract = {This paper investigates different fusion strategies as well as provides insights on their effectiveness alongside standalone classifiers in the framework of paralinguistic analysis of infant vocalizations. The combinations of such systems as Support Vector Machines (SVM) and Extreme Learning Machines (ELM) based classifiers, as well as its weighted kernel version are explored, training systems on different acoustic feature representations and implementing weighted score-level fusion of the predictions. The proposed framework is tested on INTERSPEECH ComParE-2019 Baby Sounds corpus, which is a collection of Home Bank infant vocalization corpora annotated for five classes. Adhering to the challenge protocol, using a single test set submission we outperform the challenge baseline Unweighted Average Recall (UAR) score and achieve a comparable result to the state-of-the-art.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {509–513},
numpages = {5},
keywords = {support vector machines, information fusion, extreme learning machines, computational paralinguistics, baby sounds classification},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1145/3422575.3422773,
author = {Shen, Huanxing and Li, Cong},
title = {Runtime Estimation of Application Memory Latency for Performance Analysis and Optimization},
year = {2021},
isbn = {9781450388993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422575.3422773},
doi = {10.1145/3422575.3422773},
abstract = {Various runtime factors impact memory latency and consequently impact application performance. Unfortunately the causal relationship is buried especially at runtime. In this paper we propose a new method for runtime estimation of application memory latency which helps discover the causal relationship. The new method leverages the hardware performance counters to calculate the average time that memory requests wait before getting fulfilled. We evaluate the method empirically in multiple scenarios and the estimation closely approximates the ground truth. We further demonstrate two examples of using the runtime estimation of application memory latency in application performance optimization and analysis, one in mitigating memory access interference in workload co-location and the other in dissecting the performance problem in the memory subsystem.},
booktitle = {Proceedings of the International Symposium on Memory Systems},
pages = {1–9},
numpages = {9},
keywords = {workload co-location, performance analysis, memory latency, memory access interference},
location = {Washington, DC, USA},
series = {MEMSYS '20}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1007/978-3-030-26061-3_6,
author = {Avci, Umut and Akkurt, Gamze and Unay, Devrim},
title = {A Pattern Mining Approach in Feature Extraction for Emotion Recognition from Speech},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_6},
doi = {10.1007/978-3-030-26061-3_6},
abstract = {We address the problem of recognizing emotions from speech using features derived from emotional patterns. Because much work in the field focuses on using low-level acoustic features, we explicitly study whether high-level features are useful for classifying emotions. For this purpose, we convert a continuous speech signal to a discretized signal and extract discriminative patterns that are capable of distinguishing distinct emotions from each other. Extracted patterns are then used to create a feature set to be fed into a classifier. Experimental results show that patterns alone are good predictors of emotions. When used to build a classifier, pattern features achieve accuracy gains up&nbsp;to 25% compared to state-of-the-art acoustic features.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {54–63},
numpages = {10},
keywords = {Feature extraction, Pattern mining, Speech processing, Emotion recognition},
location = {Istanbul, Turkey}
}

@inproceedings{10.1109/CGO51591.2021.9370339,
author = {Zhou, Keren and Meng, Xiaozhu and Sai, Ryuichi and Mellor-Crummey, John},
title = {GPA: a GPU performance advisor based on instruction sampling},
year = {2021},
isbn = {9781728186139},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO51591.2021.9370339},
doi = {10.1109/CGO51591.2021.9370339},
abstract = {Developing efficient GPU kernels can be difficult because of the complexity of GPU architectures and programming models. Existing performance tools only provide coarse-grained tuning advice at the kernel level, if any. In this paper, we describe GPA, a performance advisor for NVIDIA GPUs that suggests potential code optimizations at a hierarchy of levels, including individual lines, loops, and functions. To relieve users of the burden of interpreting performance counters and analyzing bottlenecks, GPA uses data flow analysis to approximately attribute measured instruction stalls to their root causes and uses information about a program's structure and the GPU to match inefficiency patterns with optimization strategies. To quantify the potential benefits of each optimization strategy, we developed PC sampling-based performance models to estimate its speedup. Our experiments with benchmarks and applications show that GPA provides insightful reports to guide performance optimization. Using GPA, we obtained speedups on a Volta V100 GPU ranging from 1.01\texttimes{} to 3.58\texttimes{}, with a geometric mean of 1.22\texttimes{}.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {115–125},
numpages = {11},
keywords = {performance analysis, parallel programming, parallel architectures, high performance computing},
location = {Virtual Event, Republic of Korea},
series = {CGO '21}
}

@article{10.1016/j.ins.2013.08.011,
author = {Li, Jun-Bao and Wang, Yun-Heng and Chu, Shu-Chuan and Roddick, John F.},
title = {Kernel self-optimization learning for kernel-based feature extraction and recognition},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {257},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.08.011},
doi = {10.1016/j.ins.2013.08.011},
abstract = {Kernel learning is becoming an important research topic in the area of machine learning, and it has wide applications in pattern recognition, computer vision, image and signal processing. Kernel learning provides a promising solution to nonlinear problems, including nonlinear feature extraction, classification and clustering. However, in kernel-based systems, the problem of the kernel function and its parameters remains to be solved. Methods of choosing parameters from a discrete set of values have been presented in previous studies, but these methods do not change the data distribution structure in the kernel-based mapping space. Accordingly, performance is not improved because the current kernel optimization does not change the data distribution. Based on this problem, this paper presents a uniform framework for kernel self-optimization with the ability to adjust the data structure. The data-dependent kernel is extended and applied to kernel learning, and optimization equations with two criteria for measuring data discrimination are used to solve the optimal parameter values. Some experiments are performed to evaluate the performance in popular kernel learning methods, including kernel principal components analysis (KPCA), kernel discriminant analysis (KDA) and kernel locality-preserving projection (KLPP). These evaluations show that the framework of kernel self-optimization is feasible for enhancing kernel-based learning methods.},
journal = {Inf. Sci.},
month = feb,
pages = {70–80},
numpages = {11},
keywords = {Kernel self-optimization, Kernel principal component analysis, Kernel method, Kernel locality preserving projection, Kernel discriminant analysis, Data-dependent kernel}
}

@article{10.1016/j.eswa.2021.115546,
author = {Gregoriades, Andreas and Pampaka, Maria and Herodotou, Herodotos and Christodoulou, Evripides},
title = {Supporting digital content marketing and messaging through topic modelling and decision trees},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {184},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115546},
doi = {10.1016/j.eswa.2021.115546},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {18},
keywords = {Tourists’ reviews, Shapley additive explanation, Decision trees, Cultural and economic distance, Topic modelling}
}

@inproceedings{10.1007/978-3-030-00308-1_3,
author = {Hess, Timm and Mundt, Martin and Weis, Tobias and Ramesh, Visvanathan},
title = {Large-Scale Stochastic Scene Generation and Semantic Annotation for Deep Convolutional Neural Network Training in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_3},
doi = {10.1007/978-3-030-00308-1_3},
abstract = {Object detection and classification are essential tasks for any robotics scenario, where data-driven approaches, specifically deep learning techniques, have been widely adopted in recent years. However, in the context of the RoboCup standard platform league these methods have not yet gained comparable popularity in large part due to the lack of (publicly) available large enough data sets that involve a tedious gathering and error-prone manual annotation process. We propose a framework for stochastic scene generation, rendering and automatic creation of semantically annotated ground truth masks. Used as training data in conjunction with deep convolutional neural networks we demonstrate compelling classification accuracy on real-world data in a multi-class setting. An evaluation on multiple neural network architectures with varying depth and representational capacity, corresponding run-times on current NAO-H25 hardware, and required sampled training data is provided.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {33–44},
numpages = {12},
keywords = {Static Head Pose, Robotics, Standard Platform League (SPL), RoboCup SPL, Deep Convolutional Neural Networks},
location = {Nagoya, Japan}
}

@article{10.1016/j.patcog.2015.02.027,
author = {Shen, Jialie and Deng, Robert H. and Cheng, Zhiyong and Nie, Liqiang and Yan, Shuicheng},
title = {On robust image spam filtering via comprehensive visual modeling},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {10},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.02.027},
doi = {10.1016/j.patcog.2015.02.027},
abstract = {The Internet has brought about fundamental changes in the way peoples generate and exchange media information. Over the last decade, unsolicited message images (image spams) have become one of the most serious problems for Internet service providers (ISPs), business firms and general end users. In this paper, we report a novel system called RoBoTs (Robust BoosTrap based spam detector) to support accurate and robust image spam filtering. The system is developed based on multiple visual properties extracted from different levels of granularity, aiming to capture more discriminative contents for effective spam image identification. In addition, a resampling based learning framework is developed to effectively integrate random forest and linear discriminative analysis (LDA) to generate comprehensive signature of spam images. It can facilitate more accurate and robust spam classification process with very limited amount of initial training examples. Using three public available test collections, the proposed system is empirically compared with the state-of-the-art techniques. Our results demonstrate its significantly higher performance from different perspectives. Author-HighlightsWe develop a novel scheme to model contents of spam image and compute comprehensive signatures.A hybrid framework is developed to detect spam images effectively.Our approach achieves substantial performance improvement on spam detection in terms of effectiveness and robustness.},
journal = {Pattern Recogn.},
month = oct,
pages = {3227–3238},
numpages = {12},
keywords = {Spam, Security, Experimentation, Algorithm}
}

@phdthesis{10.5555/AAI28812877,
author = {Narayanan, Deepak and F., Bent, Stacey},
advisor = {Matei, Zaharia, and Kayvon, Fatahalian, and Christopher, R\'{e},},
title = {Resource-Efficient Execution of Deep Learning Computations},
year = {2021},
isbn = {9798494452825},
publisher = {Stanford University},
address = {Stanford, CA, USA},
abstract = {Deep Learning models have enabled state-of-the-art results across a broad range of applications. Training these models, however, is extremely time- and resource-intensive, taking weeks on clusters with thousands of expensive accelerators in the extreme case. As Moore's Law slows down, numerous parallel accelerators have been introduced to meet this new computational demand. This dissertation shows how model- and hardware-aware optimizations in software systems can help intelligently navigate this heterogeneity. In particular, it demonstrates how careful automated scheduling of computation across levels of the software stack can be used to perform distributed training and resource allocation more efficiently. In the first part of this dissertation, we study pipelining, a technique commonly used as a performance optimization in various systems, as a way to perform more efficient distributed model training for both models with small training footprints and those with training footprints larger than the memory capacity of a single GPU. For certain types of models, pipeline parallelism can facilitate model training with lower communication overhead than previous methods. We introduce new strategies for pipeline parallelism, with different tradeoffs between training throughput, memory footprint, and weight update semantics; these outperform existing methods in certain settings. Pipeline parallelism can also be used in conjunction with other forms of parallelism, helping create a richer search space of parallelization strategies. By partitioning the training graph across accelerators in a model-aware way, pipeline parallelism combined with data parallelism can be up to 5x faster than data parallelism in isolation. We also use a principled combination of pipeline parallelism, tensor model parallelism, and data parallelism to efficiently scale training to language models with a trillion parameters on 3072 A100 GPUs (aggregate throughput of 502 petaFLOP/s, which is 52% of peak device throughput). In the second part of this dissertation, we show how heterogeneous compute resources (e.g., different GPU generations like NVIDIA K80 and V100 GPUs) in a shared cluster (either in a private deployment or in the public cloud) should be partitioned among multiple users to optimize objectives specified over one or more training jobs. By formulating existing policies as optimization problems over the allocation, and then using a concept we call effective throughput, policies can be extended to be heterogeneity-aware. A policy-agnostic scheduling mechanism then helps realize the heterogeneity-aware allocations returned by these policies in practice. We can improve various scheduling objectives, such as average completion time, makespan, or cloud computing resource cost, by up to 3.5x, using these heterogeneity-aware policies. Towards the end of this dissertation, we also touch on how the dynamic pricing information of spot instances can be plugged into this heterogeneity-aware policy framework to optimize cost objectives in the public cloud. This can help reduce cost compared to using more expensive on-demand instances alone.},
note = {AAI28812877}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1007/978-3-030-59612-5_2,
author = {Javaid, Muhammad Usama and Kanoun, Ahmed Amir and Demesmaeker, Florian and Ghrab, Amine and Skhiri, Sabri},
title = {A Performance Prediction Model for Spark Applications},
year = {2020},
isbn = {978-3-030-59611-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59612-5_2},
doi = {10.1007/978-3-030-59612-5_2},
abstract = {Apache Spark is a popular open-source distributed processing framework that enables efficient processing of massive amounts of data. It has a large number of configuration parameters that are strongly related to performance. Spark performance, for a given application, can significantly vary because of input data type and size, design &amp; implementation of algorithm, computational resources and parameter configuration. So, involvement of all these variables makes performance prediction very difficult. In this paper, we take into account all the variables and try to learn machine learning based performance prediction model. We ran extensive experiments on a selected set of Spark applications that cover the most common workloads to generate a representative dataset of execution time. In addition, we extracted application and data features to build a machine learning based performance model to predict Spark applications execution time. The experiments show that boosting algorithms achieved better results compared to the other algorithms.},
booktitle = {Big Data – BigData 2020: 9th International Conference, Held as Part of the Services Conference Federation, SCF 2020, Honolulu, HI, USA, September 18-20, 2020, Proceedings},
pages = {13–22},
numpages = {10},
location = {Honolulu, HI, USA}
}

@article{10.1016/j.asoc.2015.03.045,
author = {Fahad, Labiba Gillani and Rajarajan, Muttukrishnan},
title = {Integration of discriminative and generative models for activity recognition in smart homes},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.045},
doi = {10.1016/j.asoc.2015.03.045},
abstract = {Graphical abstractDisplay Omitted HighlightsA hybrid activity recognition approach that combines DM with PE using SVM.DM is suitable for imbalanced number of activity instances.PE has better generalization ability in activity recognition.Evaluation on five smart home datasets validates an improved performance of the approach. Activity recognition in smart homes enables the remote monitoring of elderly and patients. In healthcare systems, reliability of a recognition model is of high importance. Limited amount of training data and imbalanced number of activity instances result in over-fitting thus making recognition models inconsistent. In this paper, we propose an activity recognition approach that integrates the distance minimization (DM) and probability estimation (PE) approaches to improve the reliability of recognitions. DM uses distances of instances from the mean representation of each activity class for label assignment. DM is useful in avoiding decision biasing towards the activity class with majority instances; however, DM can result in over-fitting. PE on the other hand has good generalization abilities. PE measures the probability of correct assignments from the obtained distances, while it requires a large amount of data for training. We apply data oversampling to improve the representation of classes with less number of instances. Support vector machine (SVM) is applied to combine the outputs of both DM and PE, since SVM performs better with imbalanced data and further improves the generalization ability of the approach. The proposed approach is evaluated using five publicly available smart home datasets. The results demonstrate better performance of the proposed approach compared to the state-of-the-art activity recognition approaches.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {992–1001},
numpages = {10},
keywords = {Support vector machine, Smart homes, Probability estimation, Pervasive healthcare, Distance minimization, Activity recognition}
}

@article{10.1007/s10916-015-0219-1,
author = {Ayd\i{}n, Serap and Tunga, M. Alper and Yetkin, Sinan},
title = {Mutual Information Analysis of Sleep EEG in Detecting Psycho-Physiological Insomnia},
year = {2015},
issue_date = {May       2015},
publisher = {Plenum Press},
address = {USA},
volume = {39},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-015-0219-1},
doi = {10.1007/s10916-015-0219-1},
abstract = {The primary goal of this study is to state the clear changes in functional brain connectivity during all night sleep in psycho-physiological insomnia (PPI). The secondary goal is to investigate the usefulness of Mutual Information (MI) analysis in estimating cortical sleep EEG arousals for detection of PPI. For these purposes, healthy controls and patients were compared to each other with respect to both linear (Pearson correlation coefficient and coherence) and nonlinear quantifiers (MI) in addition to phase locking quantification for six sleep stages (stage.1---4, rem, wake) by means of interhemispheric dependency between two central sleep EEG derivations. In test, each connectivity estimation calculated for each couple of epoches (C3-A2 and C4-A1) was identified by the vector norm of estimation. Then, patients and controls were classified by using 10 different types of data mining classifiers for five error criteria such as accuracy, root mean squared error, sensitivity, specificity and precision. High performance in a classification through a measure will validate high contribution of that measure to detecting PPI. The MI was found to be the best method in detecting PPI. In particular, the patients had lower MI, higher PCC for all sleep stages. In other words, the lower sleep EEG synchronization suffering from PPI was observed. These results probably stand for the loss of neurons that then contribute to less complex dynamical processing within the neural networks in sleep disorders an the functional central brain connectivity is nonlinear during night sleep. In conclusion, the level of cortical hemispheric connectivity is strongly associated with sleep disorder. Thus, cortical communication quantified in all existence sleep stages might be a potential marker for sleep disorder induced by PPI.},
journal = {J. Med. Syst.},
month = may,
pages = {1–10},
numpages = {10},
keywords = {Sleep EEG, Mutual information, Data mining, Classification, Brain connectivity}
}

@article{10.1016/j.procs.2017.11.036,
author = {Colley, Derek and Stanier, Clare},
title = {Identifying New Directions in Database Performance Tuning},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {121},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.11.036},
doi = {10.1016/j.procs.2017.11.036},
abstract = {Database performance tuning is a complex and varied active research topic. With enterprise relational database management systems still reliant on the set-based relational concepts that defined early data management products, the disparity between the object-oriented application development model and the object-relational database model, called the object-relational impedance mismatch problem, is addressed by techniques such as object-relational mapping (ORM). This, compounded with changes in the way data is produced, stored and managed can result in generally poor query performance for SQL produced by object-oriented applications and an irregular fit with cost-based optimisation algorithms, and leads to questions about the need for the relational model to better adapt to a more diverse set of queries. This paper discusses existing database performance optimisation techniques and approaches and makes the argument that current database performance tuning approaches need revisiting to support queries developed through ORM tools. This paper also introduces our current research, which includes exploring concepts such as dynamic schema redefinition; query analysis and optimisation modelling driven by machine learning; and augmentation of the cost-based optimiser model.},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {260–265},
numpages = {6},
keywords = {performance tuning, object-relational mapping, object-relational impedance mismatch, cost-based optimiser, SQL, Database}
}

@article{10.1016/j.ins.2019.01.015,
author = {Khanouche, Mohamed Essaid and Attal, Ferhat and Amirat, Yacine and Chibani, Abdelghani and Kerkar, Moussa},
title = {Clustering-based and QoS-aware services composition algorithm for ambient intelligence},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {482},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.01.015},
doi = {10.1016/j.ins.2019.01.015},
journal = {Inf. Sci.},
month = may,
pages = {419–439},
numpages = {21},
keywords = {k-means clustering method, QoS constraints, Services composition, Quality of service, Ambient intelligence}
}

@article{10.1007/s10515-019-00253-7,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Change impact analysis for maintenance and evolution of variable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00253-7},
doi = {10.1007/s10515-019-00253-7},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This article presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining the possibly impacted source code elements when changing the source code of a product family. The approach also supports engineers, who are adapting the code of specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanisms, it is more precise than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. We report evaluation results on the benefit and performance of the approach using industrial product lines.},
journal = {Automated Software Engg.},
month = jun,
pages = {417–461},
numpages = {45},
keywords = {Variability, Program analysis, Maintenance, Change impact analysis}
}

@article{10.1007/s11241-008-9060-7,
author = {Bini, Enrico and Buttazzo, Giorgio},
title = {The space of EDF deadlines: the exact region and a convex approximation},
year = {2009},
issue_date = {January   2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {41},
number = {1},
issn = {0922-6443},
url = {https://doi.org/10.1007/s11241-008-9060-7},
doi = {10.1007/s11241-008-9060-7},
abstract = {It is well known that the performance of computer controlled systems is heavily affected by delays and jitter occurring in the control loops, which are mainly caused by the interference introduced by other concurrent activities. A common approach adopted to reduce delay and jitter in periodic task systems is to decrease relative deadlines as much as possible, but without jeopardizing the schedulability of the task set.In this paper, we formally characterize the region of admissible deadlines so that the system designer can appropriately select the desired values to maximize a given performance index defined over the task set. Finally we also provide a sufficient region of feasible deadlines which is proved to be convex.},
journal = {Real-Time Syst.},
month = jan,
pages = {27–51},
numpages = {25},
keywords = {Performance optimization, Earliest deadline first, Deadline assignment}
}

@inproceedings{10.1145/3148055.3148072,
author = {Abdullah, Tariq and Ahmet, Ahmed},
title = {Genomics Analyser: A Big Data Framework for Analysing Genomics Data},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148072},
doi = {10.1145/3148055.3148072},
abstract = {Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {189–197},
numpages = {9},
keywords = {resource management, population scale clustering, machine learning, in-memory computing, data analysis, compute cluster, big data, algorithms},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@article{10.1007/s00521-015-2118-2,
author = {Zhang, Ning and Chandrasekar, Prathamesh},
title = {Sparse learning of maximum likelihood model for optimization of complex loss function},
year = {2017},
issue_date = {May       2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {5},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-015-2118-2},
doi = {10.1007/s00521-015-2118-2},
abstract = {Traditional machine learning methods usually minimize a simple loss function to learn a predictive model and then use a complex performance measure to measure the prediction performance. However, minimizing a simple loss function cannot guarantee an optimal performance. In this paper, we study the problem of optimizing the complex performance measure directly to obtain a predictive model. We proposed to construct a maximum likelihood model for this problem, and to learn the model parameter, we minimize a complex loss function corresponding to the desired complex performance measure. To optimize the loss function, we approximate the upper bound of the complex loss. We also propose to impose the sparsity to the model parameter to obtain a sparse model. An objective was constructed by combining the upper bound of the loss function and the sparsity of the model parameter, and we develop an iterative algorithm to minimize it by using the fast iterative shrinkage-thresholding algorithm framework. The experiments on optimization on three different complex performance measures, including F-score, receiver operating characteristic curve, and recall precision curve break-even point, over three real-world applications, aircraft event recognition of civil aviation safety, intrusion detection in wireless mesh networks, and image classification, show the advantages of the proposed method over state-of-the-art methods.},
journal = {Neural Comput. Appl.},
month = may,
pages = {1057–1067},
numpages = {11},
keywords = {Sparse learning, Maximum likelihood, Machine learning, Complex multivariate performance, Civil aviation safety}
}

@article{10.1007/s10845-015-1128-3,
author = {Hamdi, Faiza and Ghorbel, Ahmed and Masmoudi, Faouzi and Dupont, Lionel},
title = {Optimization of a supply portfolio in the context of supply chain risk management: literature review},
year = {2018},
issue_date = {April     2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {4},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-015-1128-3},
doi = {10.1007/s10845-015-1128-3},
abstract = {The aim of this paper is to review the literature in the field of supplier selection under supply chain risk management. Collected papers from 2003 to 2014 are analyzed and classified, first, according to the characteristics of the problem they deal with, secondly, according to the approach they propose, and thirdly, according to the techniques they use. The papers have been grouped into five categories: the first group relates to quantitative approaches to supplier selection, the second concerns qualitative approaches, the third consists of hybrid approaches that blend two or more different approaches together, the fourth relates to simulation approaches and the last group to artificial intelligence. The techniques used in each category are outlined. The different approaches and their associated techniques are analyzed and some recommendations are made on improving their efficiency and performance. This paper is thus a systematic scope review of journal articles and conference papers issued during this period. It brings together a collection of 124 papers on the topic of supplier selection under supply chain risk management.},
journal = {J. Intell. Manuf.},
month = apr,
pages = {763–788},
numpages = {26},
keywords = {Supply chain risk management, Simulation approach, Quantitaive approaches, Qualitative approaches, Optimization of a supply portfolio, Hybrid approach}
}

@inproceedings{10.5555/1639809.1639937,
author = {Rizvi, Syed and Thomas, Bevin and Elleithy, Khaled and Riasat, Aasia},
title = {A new technique of switch &amp; feedback job scheduling mechanism in a distributed system},
year = {2009},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In this paper we investigate on a special type of job scheduler for distributed heterogeneous systems called the switch and feedback job scheduler. The main objective of the centralized job scheduler is to achieve maximum performance with reduced waiting time and faster response time for independent tasks by reordering the processor queue list based on the feedback received from each individual local processing system.},
booktitle = {Proceedings of the 2009 Spring Simulation Multiconference},
articleno = {121},
numpages = {4},
keywords = {performance optimization, parallel and distributed systems, job scheduling},
location = {San Diego, California},
series = {SpringSim '09}
}

@article{10.1016/j.ins.2021.06.013,
author = {Yang, Guoli and Kang, Yuanji and Zhu, Xianqiang and Zhu, Cheng and Xiao, Gaoxi},
title = {Info2vec: An aggregative representation method in multi-layer and heterogeneous networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.013},
doi = {10.1016/j.ins.2021.06.013},
journal = {Inf. Sci.},
month = oct,
pages = {444–460},
numpages = {17},
keywords = {Cyberspace, Representation learning, Multi-layer networks}
}

@article{10.1016/j.peva.2017.07.003,
author = {Gebrehiwot, Misikir Eyob and Aalto, Samuli and Lassila, Pasi},
title = {Energy-aware SRPT server with batch arrivals},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0166-5316},
url = {https://doi.org/10.1016/j.peva.2017.07.003},
doi = {10.1016/j.peva.2017.07.003},
abstract = {We consider the optimal energy-aware control of a single server with batch arrivals and applying the SRPT scheduling rule. The server is modeled as an MXG1 queue with a particular control policy that puts the server to a sleep mode to save energy with an additional delay cost, the setup delay, after the server is turned on again. We first consider an ordinary MXG1-SRPT system that does not go to sleep, and derive the mean response time equation. We then consider the more elaborate energy-aware MXG1-SRPT system, and show that the mean response time can be decomposed into two parts: the mean response time of an ordinary MXG1-SRPT, and an additional penalty term for switching the server to a sleep state. Furthermore, we study the energy-performance optimization of the system and prove that, for the Energy Response time Weighted Sum (ERWS) and Energy Response time Product (ERP) cost metrics, the optimal control either puts the server into a sleep state immediately when it becomes idle or keeps it idling until the next job arrives.},
journal = {Perform. Eval.},
month = oct,
pages = {92–107},
numpages = {16},
keywords = {Setup delay, Performance-energy trade-off, MXG1-SRPT, Batch arrivals}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {software product lines, media arts, gentic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1504/IJAHUC.2008.017005,
author = {Peng, Mugen and Wang, Yingjie and Wang, Kun and Wang, Wenbo},
title = {Joint optimisation for power control, scheduling and routing algorithms in the infrastructure wireless mesh network},
year = {2008},
issue_date = {February 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {3},
number = {2},
issn = {1743-8225},
url = {https://doi.org/10.1504/IJAHUC.2008.017005},
doi = {10.1504/IJAHUC.2008.017005},
abstract = {The performance optimisation model for the IEEE 802.16 standard based wireless mesh network is presented and a joint optimisation method is proposed as the sub-optimal solution strategy, in which the power control, scheduling and routing algorithms are designed as integrated mechanisms. The distributed transmission power focuses on reducing the power consumption and decreasing interferences. Wireless scheduling is used to improve the spectrum efficiency, and the routing selection mechanism determines the optimum routing selection. Simulation results show that the proposed joint optimisation mechanism can effectively improve the network throughput performance, decrease the power consumption and achieve better performances.},
journal = {Int. J. Ad Hoc Ubiquitous Comput.},
month = feb,
pages = {122–131},
numpages = {10},
keywords = {wireless networks, wireless mesh networks, simulation, scheduling, routing, power control, performance optimisation, network throughput, WMN, IEEE 80216}
}

@article{10.1155/2021/8340925,
author = {Deshmukh, Shyam and Thirupathi Rao, Komati and Shabaz, Mohammad and Kaur, Manjit},
title = {Collaborative Learning Based Straggler Prevention in Large-Scale Distributed Computing Framework},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/8340925},
doi = {10.1155/2021/8340925},
abstract = {Modern big data applications tend to prefer a cluster computing approach as they are linked to the distributed computing framework that serves users jobs as per demand. It performs rapid processing of tasks by subdividing them into tasks that execute in parallel. Because of the complex environment, hardware and software issues, tasks might run slowly leading to delayed job completion, and such phenomena are also known as stragglers. The performance improvement of distributed computing framework is a bottleneck by straggling nodes due to various factors like shared resources, heavy system load, or hardware issues leading to the prolonged job execution time. Many state-of-the-art approaches use independent models per node and workload. With increased nodes and workloads, the number of models would increase, and even with large numbers of nodes. Not every node would be able to capture the stragglers as there might not be sufficient training data available of straggler patterns, yielding suboptimal straggler prediction. To alleviate such problems, we propose a novel collaborative learning-based approach for straggler prediction, the alternate direction method of multipliers (ADMM), which is resource-efficient and learns how to efficiently deal with mitigating stragglers without moving data to a centralized location. The proposed framework shares information among the various models, allowing us to use larger training data and bring training time down by avoiding data transfer. We rigorously evaluate the proposed method on various datasets with high accuracy results.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {9}
}

@inproceedings{10.1145/3447818.3460356,
author = {Ren, Jie and Luo, Jiaolin and Peng, Ivy and Wu, Kai and Li, Dong},
title = {Optimizing large-scale plasma simulations on persistent memory-based heterogeneous memory with effective data placement across memory hierarchy},
year = {2021},
isbn = {9781450383356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447818.3460356},
doi = {10.1145/3447818.3460356},
abstract = {Particle simulations of plasma are important for understanding plasma dynamics in space weather and fusion devices. However, production simulations that use billions and even trillions of computational particles require high memory capacity. In this work, we explore the latest persistent memory (PM) hardware to enable large-scale plasma simulations at unprecedented scales on a single machine. We use WarpX, an advanced plasma simulation code which is mission-critical and targets future exascale systems. We analyze the performance of WarpX on PM-based heterogeneous memory systems and propose to make the best use of memory hierarchy to avoid the impact of inferior performance of PM. We introduce a combination of static and dynamic data placement, and processor-cache prefetch mechanism for performance optimization. We develop a performance model to enable efficient data migration between PM and DRAM in the background, without reducing available bandwidth and parallelism to the application threads. We also build an analytical model to decide when to prefetch for the best use of caches. Our design achieves 66.4% performance improvement over the PM-only baseline and outperforms DRAM-cached, NUMA first-touch, and a state-of-the-art software solution by 38.8%, 45.1% and 83.3%, respectively.},
booktitle = {Proceedings of the 35th ACM International Conference on Supercomputing},
pages = {203–214},
numpages = {12},
keywords = {heterogeneous memory, memory management, plasma simulations},
location = {Virtual Event, USA},
series = {ICS '21}
}

@article{10.3233/JIFS-201400,
author = {Tripathi, Gaurav and Singh, Kuldeep and Vishwakarma, Dinesh Kumar},
title = {Violence recognition using convolutional neural network: A survey},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-201400},
doi = {10.3233/JIFS-201400},
abstract = {Violence detection is a challenging task in the computer vision domain. Violence detection framework depends upon the detection of crowd behaviour changes. Violence erupts due to disagreement of an idea, injustice or severe disagreement. The aim of any country is to maintain law and order and peace in the area. Violence detection thus becomes an important task for authorities to maintain peace. Traditional methods have existed for violence detection which are heavily dependent upon hand crafted features. The world is now transitioning in to Artificial Intelligence based techniques. Automatic feature extraction and its classification from images and videos is the new norm in surveillance domain. Deep learning platform has provided us the platter on which non-linear features can be extracted, self-learnt and classified as per the appropriate tool. One such tool is the Convolutional Neural Networks, also known as ConvNets, which has the ability to automatically extract features and classify them in to their respective domain. Till date there is no survey of deciphering violence behaviour techniques using ConvNets. We hope that this survey becomes an exclusive baseline for future violence detection and analysis in the deep learning domain.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7931–7952},
numpages = {22},
keywords = {survey, deep learning, convolutional neural networks, ConvNets, crowd behaviour, Violence detection}
}

@article{10.1016/j.specom.2012.01.002,
author = {Zelinka, Petr and Sigmund, Milan and Schimmel, Jiri},
title = {Impact of vocal effort variability on automatic speech recognition},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {6},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2012.01.002},
doi = {10.1016/j.specom.2012.01.002},
abstract = {The impact of changes in a speaker's vocal effort on the performance of automatic speech recognition has largely been overlooked by researchers and virtually no speech resources exist for the development and testing of speech recognizers at all vocal effort levels. This study deals with speech properties in the whole range of vocal modes - whispering, soft speech, normal speech, loud speech, and shouting. Fundamental acoustic and phonetic changes are documented. The impact of vocal effort variability on the performance of an isolated-word recognizer is shown and effective means of improving the system's robustness are tested. The proposed multiple model framework approach reaches a 50% relative reduction of word error rate compared to the baseline system. A new specialized speech database, BUT-VE1, is presented, which contains speech recordings of 13 speakers at 5 vocal effort levels with manual phonetic segmentation and sound pressure level calibration.},
journal = {Speech Commun.},
month = jul,
pages = {732–742},
numpages = {11},
keywords = {Vocal effort level, Robust speech recognition, Machine learning}
}

@article{10.1016/j.patrec.2015.11.027,
author = {Yang, Yi and Han, De-Qiang and Dezert, Jean},
title = {An angle-based neighborhood graph classifier with evidential reasoning},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {71},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2015.11.027},
doi = {10.1016/j.patrec.2015.11.027},
abstract = {A new neighborhood graph classifier with an angle parameter is proposed.The new designed classifier is geometrically intuitive and can be readily implemented.An evidential reasoning based approach is proposed for dealing with the parameter selection. Display Omitted A classification approach called angle-based neighborhood graph (ANG) is proposed in this paper, which can flexibly define the neighborhood of a given query sample based on the geometrical relation established using an angle parameter. The proposed ANG is geometrically intuitive and can be readily implemented. Compared with the traditional neighborhood graph classifiers, ANG can adjust the size of the neighborhood by tuning the angle parameter to obtain better classification accuracy. To deal with the parameter selection in ANG, an evidential reasoning based approach is proposed. Experimental results are provided for comparing ANG and the traditional neighborhood graph classifiers, including Gabriel Graph (GG), Relative Neighborhood Graph (RNG), β skeletons, and adaptive weighted k nearest neighbors classifiers. It can be concluded that ANG is a simple yet flexible and effective classifier, and the evidential reasoning based parameter selection approach for ANG is also effective.},
journal = {Pattern Recogn. Lett.},
month = feb,
pages = {78–85},
numpages = {8},
keywords = {Pattern classification, Neighborhood graph, Neighborhood classifier, Geometrical relation, Belief functions}
}

@article{10.1007/s00450-011-0193-x,
author = {Kunkel, Julian M. and Minartz, Timo and Kuhn, Michael and Ludwig, Thomas},
title = {Towards an energy-aware scientific I/O interface},
year = {2012},
issue_date = {November  2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-011-0193-x},
doi = {10.1007/s00450-011-0193-x},
abstract = {Intelligently switching energy saving modes of CPUs, NICs and disks is mandatory to reduce the energy consumption.Hardware and operating system have a limited perspective of future performance demands, thus automatic control is suboptimal. However, it is tedious for a developer to control the hardware by himself.In this paper we propose an extension of an existing I/O interface which on the one hand is easy to use and on the other hand could steer energy saving modes more efficiently. Furthermore, the proposed modifications are beneficial for performance analysis and provide even more information to the I/O library to improve performance.When a user annotates the program with the proposed interface, I/O, communication and computation phases are labeled by the developer. Run-time behavior is then characterized for each phase, this knowledge could be then exploited by the new library.},
journal = {Comput. Sci.},
month = nov,
pages = {337–345},
numpages = {9},
keywords = {Scientific I/O API, Performance optimization, Performance analysis, Energy efficiency, ADIOS}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {deep active learning, active learning, Deep learning}
}

@article{10.14778/3485450.3485462,
author = {Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J.},
title = {Accelerating recommendation system training by leveraging popular choices},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3485450.3485462},
doi = {10.14778/3485450.3485462},
abstract = {Recommender models are commonly used to suggest relevant items to a user for e-commerce and online advertisement-based applications. These models use massive embedding tables to store numerical representation of items' and users' categorical variables (memory intensive) and employ neural networks (compute intensive) to generate final recommendations. Training these large-scale recommendation models is evolving to require increasing data and compute resources. The highly parallel neural networks portion of these models can benefit from GPU acceleration however, large embedding tables often cannot fit in the limited-capacity GPU device memory. Hence, this paper deep dives into the semantics of training data and obtains insights about the feature access, transfer, and usage patterns of these models. We observe that, due to the popularity of certain inputs, the accesses to the embeddings are highly skewed with a few embedding entries being accessed up to 10000X more. This paper leverages this asymmetrical access pattern to offer a framework, called FAE, and proposes a hot-embedding aware data layout for training recommender models. This layout utilizes the scarce GPU memory for storing the highly accessed embeddings, thus reduces the data transfers from CPU to GPU. At the same time, FAE engages the GPU to accelerate the executions of these hot embedding entries. Experiments on production-scale recommendation models with real datasets show that FAE reduces the overall training time by 2.3X and 1.52X in comparison to XDL CPU-only and XDL CPU-GPU execution while maintaining baseline accuracy.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {127–140},
numpages = {14}
}

@inproceedings{10.1007/978-3-319-47157-0_11,
author = {Wang, Yan and Wu, Xi and Ma, Guangkai and Ma, Zongqing and Fu, Ying and Zhou, Jiliu},
title = {Patch-Based Hippocampus Segmentation Using a Local Subspace Learning Method},
year = {2016},
isbn = {978-3-319-47156-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47157-0_11},
doi = {10.1007/978-3-319-47157-0_11},
abstract = {Patch-based segmentation methods utilizing multiple atlases have been widely studied for alleviating some misalignments when registering atlases to the target image. However, weights assigned to the fused labels are typically computed based on predefined features (e.g. simple patch intensities), thus being not necessarily optimal. Due to lack of discriminating features for different regions of an anatomical structure, the original feature space defined by image intensities may limit the segmentation accuracy. To address these problems, we propose a novel local subspace learning based patch-wise label propagation method to estimate a voxel-wise segmentation of the target image. Specifically, multi-scale patch intensities and texture features are first extracted from the image patch in order to acquire the abundant appearance information. Then, margin fisher analysis (MFA) is applied to neighboring samples of each voxel to be segmented from the aligned atlases, in order to extract discriminant features. This process can enhance discrimination of features for different local regions in the anatomical structure. Finally, based on extracted discriminant features, the k-nearest neighbor (kNN) classifier is used to determine the final label for the target voxel. Moreover, for the patch-wise label propagation, we first translate label patches into several discrete class labels by using the k-means clustering method, and then apply MFA to ensure that samples with similar label patches achieve a higher similarity and those with dissimilar label patches achieve a lower similarity. To demonstrate segmentation performance, we comprehensively evaluated the proposed method on the ADNI dataset for hippocampus segmentation. Experimental results show that the proposed method outperforms several conventional multi-atlas based segmentation methods.},
booktitle = {Machine Learning in Medical Imaging: 7th International Workshop, MLMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 17, 2016, Proceedings},
pages = {86–94},
numpages = {9},
keywords = {Target Image, Image Patch, Label Propagation, Spatial Neighborhood, Deformable Image Registration},
location = {Athens, Greece}
}

@article{10.1016/j.pmcj.2019.02.001,
author = {Lee, Hyungu and Hwang, Jung Yeon and Lee, Shincheol and Kim, Dong In and Lee, Sung-Hoon and Lee, Jaehwan and Shin, Ji Sun},
title = {A parameterized model to select discriminating features on keystroke dynamics authentication on smartphones},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1574-1192},
url = {https://doi.org/10.1016/j.pmcj.2019.02.001},
doi = {10.1016/j.pmcj.2019.02.001},
journal = {Pervasive Mob. Comput.},
month = mar,
pages = {45–57},
numpages = {13},
keywords = {Machine learning, IoT, Smartphones, Edge devices, Keystroke dynamics authentication}
}

@article{10.1162/153244303321897726,
author = {Lanckriet, Gert R.G. and Ghaoui, Laurent El and Bhattacharyya, Chiranjib and Jordan, Michael I.},
title = {A robust minimax approach to classification},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
url = {https://doi.org/10.1162/153244303321897726},
doi = {10.1162/153244303321897726},
abstract = {When constructing a classifier, the probability of correct classification of future data points should be maximized. We consider a binary classification problem where the mean and covariance matrix of each class are assumed to be known. No further assumptions are made with respect to the class-conditional distributions. Misclassification probabilities are then controlled in a worst-case setting: that is, under all possible choices of class-conditional densities with given mean and covariance matrix, we minimize the worst-case (maximum) probability of misclassification of future data points. For a linear decision boundary, this desideratum is translated in a very direct way into a (convex) second order cone optimization problem, with complexity similar to a support vector machine problem. The minimax problem can be interpreted geometrically as minimizing the maximum of the Mahalanobis distances to the two classes. We address the issue of robustness with respect to estimation errors (in the means and covariances of the classes) via a simple modification of the input data. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries, yielding a classifier which proves to be competitive with current methods, including support vector machines. An important feature of this method is that a worst-case bound on the probability of misclassification of future data is always obtained explicitly.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {555–582},
numpages = {28},
keywords = {second order cone programming, kernel methods, convex optimization, classification}
}

@inproceedings{10.1109/ICWS.2011.11,
author = {Wanchun, Dou and Chao, Lv and Xuyun, Zhang and Chen, Jinjun},
title = {A QoS-Aware Service Evaluation Method for Co-selecting a Shared Service},
year = {2011},
isbn = {9780769544632},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICWS.2011.11},
doi = {10.1109/ICWS.2011.11},
abstract = {In service selection, an end user often has his or her personal preferences imposing on a candidate service's non-functional properties. For a service selection process promoted by a group of users, candidate services are often evaluated by a group of end users who may have different preferences or priorities. In this situation, it is often a challenging effort to make a tradeoff among various preferences or priorities of the users. In view of this challenge, a multi-criteria decision-making method, named AHP (Analytic Hierarchy Process), is introduced to transform both qualitative personal preferences and users' priorities into numeric weights. Furthermore, a QoS-aware service evaluation method is presented for a shared service's co-selection taking advantage of AHP theory. At last, a case study is presented to demonstrate the feasibility of the method.},
booktitle = {Proceedings of the 2011 IEEE International Conference on Web Services},
pages = {145–152},
numpages = {8},
keywords = {Web service, Quality of Service (QoS), AHP},
series = {ICWS '11}
}

@inproceedings{10.1109/MSR.2017.18,
author = {Ghotra, Baljinder and Mcintosh, Shane and Hassan, Ahmed E.},
title = {A large-scale study of the impact of feature selection techniques on defect classification models},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.18},
doi = {10.1109/MSR.2017.18},
abstract = {The performance of a defect classification model depends on the features that are used to train it. Feature redundancy, correlation, and irrelevance can hinder the performance of a classification model. To mitigate this risk, researchers often use feature selection techniques, which transform or select a subset of the features in order to improve the performance of a classification model. Recent studies compare the impact of different feature selection techniques on the performance of defect classification models. However, these studies compare a limited number of classification techniques and have arrived at contradictory conclusions about the impact of feature selection techniques. To address this limitation, we study 30 feature selection techniques (11 filter-based ranking techniques, six filter-based subset techniques, 12 wrapper-based subset techniques, and a no feature selection configuration) and 21 classification techniques when applied to 18 datasets from the NASA and PROMISE corpora. Our results show that a correlation-based filter-subset feature selection technique with a BestFirst search method outperforms other feature selection techniques across the studied datasets (it outperforms in 70%--87% of the PROMISE-NASA data sets) and across the studied classification techniques (it outperforms for 90% of the techniques). Hence, we recommend the application of such a selection technique when building defect classification models.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {146–157},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.1007/s11276-021-02545-x,
author = {Al-Mousawi, Ali Jameel},
title = {Wireless communication networks and swarm intelligence},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {3},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-021-02545-x},
doi = {10.1007/s11276-021-02545-x},
abstract = {This paper is a comprehensive survey on the role of swarm intelligence in wireless communication networks. The main aim of preparing this paper is to lead the way for the researchers in the field of wireless networks to recognize the role of swarm intelligence in optimizing the network features. The research paths are divided into four main tracks which are: network routing, network quality of service, network congestion, and network security. Swarm intelligence involves a wide range of applications but in this paper, we are focusing on its adaptability with the communication networks to accomplish performance optimization. In each of the four tracks, three standards-based networks are examined to show the effect of swarm intelligence on these networks which are IEEE 802.11, IEEE 802.16, and IEEE 802.20. At the end of each section, a graphical qualitative comparison is represented to show the performance differences in terms of network optimization.},
journal = {Wirel. Netw.},
month = apr,
pages = {1755–1782},
numpages = {28},
keywords = {Optimization, Communication networks, Intelligence, Swarm}
}

@article{10.1109/TCBB.2007.1014,
author = {Bontempi, Gianluca},
title = {A Blocking Strategy to Improve Gene Selection for Classification of Gene Expression Data},
year = {2007},
issue_date = {April 2007},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {4},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2007.1014},
doi = {10.1109/TCBB.2007.1014},
abstract = {Because of high dimensionality, machine learning algorithms typically rely on feature selection techniques in order to perform effective classification in microarray gene expression data sets. However, the large number of features compared to the number of samples makes the task of feature selection computationally hard and prone to errors. This paper interprets feature selection as a task of stochastic optimization, where the goal is to select among an exponential number of alternative gene subsets the one expected to return the highest generalization in classification. Blocking is an experimental design strategy which produces similar experimental conditions to compare alternative stochastic configurations in order to be confident that observed differences in accuracy are due to actual differences rather than to fluctuations and noise effects. We propose an original blocking strategy for improving feature selection which aggregates in a paired way the validation outcomes of several learning algorithms to assess a gene subset and compare it to others. This is a novelty with respect to conventional wrappers, which commonly adopt a sole learning algorithm to evaluate the relevance of a given set of variables. The rationale of the approach is that, by increasing the amount of experimental conditions under which we validate a feature subset, we can lessen the problems related to the scarcity of samples and consequently come up with a better selection. The paper shows that the blocking strategy significantly improves the performance of a conventional forward selection for a set of 16 publicly available cancer expression data sets. The experiments involve six different classifiers and show that improvements take place independent of the classification algorithm used after the selection step. Two further validations based on available biological annotation support the claim that blocking strategies in feature selection may improve the accuracy and the quality of the solution. The first validation is based on retrieving PubMEd abstracts associated to the selected genes and matching them to regular expressions describing the biological phenomenon underlying the expression data sets. The biological validation that follows is based on the use of the Bioconductor package GoStats in order to perform Gene Ontology statistical analysis.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = apr,
pages = {293–300},
numpages = {8},
keywords = {machine learning, feature evaluation and selection., data mining, Bioinformatics (genome or protein) databases}
}

@article{10.1016/j.aei.2019.100926,
author = {Luo, X.J. and Oyedele, Lukumon O. and Ajayi, Anuoluwapo O. and Monyei, Chukwuka G. and Akinade, Olugbenga O. and Akanbi, Lukman A.},
title = {Development of an IoT-based big data platform for day-ahead prediction of building heating and cooling demands},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {41},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2019.100926},
doi = {10.1016/j.aei.2019.100926},
journal = {Adv. Eng. Inform.},
month = aug,
numpages = {27},
keywords = {Big data, Internet of Things, Building heating and cooling demand, Artificial neural network, Clustering, Day-ahead prediction}
}

@inproceedings{10.5555/3304222.3304394,
author = {Xia, Rui and Pan, Zhenchun and Xu, Feng},
title = {Instance weighting for domain adaptation via trading off sample selection bias and variance},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Domain adaptation is an important problem in natural language processing (NLP) due to the distributional difference between the labeled source domain and the target domain. In this paper, we study the domain adaptation problem from the instance weighting perspective. By using density ratio as the instance weight, the traditional instance weighting approaches can potentially correct the sample selection bias in domain adaptation. However, researchers often failed to achieve good performance when applying instance weighting to domain adaptation in NLP and many negative results were reported in the literature. In this work, we conduct an in-depth study on the causes of the failure, and find that previous work only focused on reducing the sample selection bias, but ignored another important factor, sample selection variance, in domain adaptation. On this basis, we propose a new instance weighting framework by trading off two factors in instance weight learning. We evaluate our approach on two cross-domain text classification tasks and compare it with eight instance weighting methods. The results prove our approach's advantages in domain adaptation performance, optimization efficiency and parameter stability.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4489–4495},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {semi-supervised learning, multi-modal learning, label propagation, Curriculum learning}
}

@article{10.1007/s00500-021-06095-4,
author = {Emamgholizadeh, Samad and Mohammadi, Babak},
title = {New hybrid nature-based algorithm to integration support vector machine for prediction of soil cation exchange capacity},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06095-4},
doi = {10.1007/s00500-021-06095-4},
abstract = {Soil cation exchange capacity (CEC) strongly influences the chemical, physical, and biological properties of soil. As the direct measurement of the CEC is difficult, costly, and time-consuming, the indirect estimation of CEC from chemical and physical parameters has been considered as an alternative method by researchers. Accordingly, in this study, a new hybrid model using a support vector machine (SVM), coupling with particle swarm optimization (PSO), and integrated invasive weed optimization (IWO) algorithm is developed for estimating the soil CEC. The physical and chemical data (i.e., clay, organic matter (OM), and pH) from two field sites of Taybad and Semnan in Iran were used for validating the new proposed approach. The ability of the proposed model (SVM-PSOIWO) was compared with the individual model (SVM) and the hybrid model (SVM-PSO). The results of the SVM-PSOIWO model were also compared with those of existing studies. Different performance evaluation criteria such as RMSE, R2, MAE, RRMSE, and MAPE, Box plots, and scatter diagrams were used to test the ability of the proposed models for estimation of the CEC values. The results showed that the SVM-PSOIWO model with the RMSE (R2) of 0.229 Cmol + kg−1 (0.924) was better than those of the SVM and SVM-PSO models with the RMSE (R2) of 0.335 Cmol + kg−1 (0.843) and 0.279 Cmol + kg−1 (0.888), respectively. Furthermore, the ability of the SVM-PSOIWO model compared with existing studies, which used the genetic expression programming, artificial neural network, and multivariate adaptive regression splines models. The results indicated that the SVM-PSOIWO model estimates the CEC more accurately than existing studies.},
journal = {Soft Comput.},
month = nov,
pages = {13451–13464},
numpages = {14},
keywords = {Invasive weed optimization algorithm, Particle swarm optimization, Support vector machine, Soil physics, Soil cation exchange capacity}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {static analysis, nonvolatile memory, machine learning, code optimization, Silent stores}
}

@article{10.5555/3122009.3176843,
author = {Park, Young Woong and Klabjan, Diego},
title = {Bayesian network learning via topological order},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a signi_cantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3451–3482},
numpages = {32},
keywords = {topological orders, directed acyclic graphs, Gaussian Bayesian network, Bayesian networks}
}

@article{10.5555/3291125.3309615,
author = {Lamprier, Sylvain and Gisselbrecht, Thibault and Gallinari, Patrick},
title = {Profile-based bandit with unknown profiles},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such proffles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2060–2099},
numpages = {40},
keywords = {upper confidence bounds, stochastic linear bandits, profile-based exploration}
}

@article{10.3233/JIFS-179862,
author = {Wang, Yu and Garc\'{\i}a Guirao, Juan Luis},
title = {Data stable aggregation algorithm based on fuzzy algorithm in cloud computing},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179862},
doi = {10.3233/JIFS-179862},
abstract = {In order to improve the ability of cloud computing data scheduling, this paper proposes a new method for multi-task, multi-level cloud computing data aggregation based on fuzzy association feature extraction. Heterogeneous directed graph analysis method is used to design cloud computing data. The semantic correlation fusion method is used to implement cloud computing data feature extraction and adaptive scheduling. The fuzzy clustering is used to process the characteristic amount of cloud computing data, and the optimal aggregation of cloud computing data is realized. Simulation results show that the method has a higher recall rate for multi-task and multi-level cloud data aggregation, and the highest recall rate can reach 1, which improves the accuracy of resource aggregation.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7935–7944},
numpages = {10},
keywords = {clustering, stable convergence, data, fuzzy algorithm, Cloud computing}
}

@inproceedings{10.5555/3042817.3043084,
author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
title = {Maxout networks},
year = {2013},
publisher = {JMLR.org},
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–1319–III–1327},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{10.5555/2032567.2032600,
author = {Lensing, Tobias and Dickmann, Lutz},
title = {Lg: a computational framework for research in sketch-based interfaces},
year = {2011},
isbn = {9783642225703},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present Lg, a computational framework for the development and scientific evaluation of assistive technologies for sketch-based interfaces on the Mac OS X platform. Lg provides its own Python API that allows access to raw and refined sketch data, machine learning algorithms, and scientific analysis tools. While it has been designed with special attention to time series analysis tasks and machine learning applications, Lg is easily adaptable to perform different tasks in sketch processing.},
booktitle = {Proceedings of the 11th International Conference on Smart Graphics},
pages = {190–193},
numpages = {4},
keywords = {software framework, sketch-based interfaces, machine learning},
location = {Bremen, Germany},
series = {SG'11}
}

@inproceedings{10.5555/1308171.1308193,
author = {Gruler, Alexander and Harhurin, Alexander and Hartmann, Judith},
title = {Development and Configuration of Service-based Product Lines},
year = {2007},
isbn = {0769528880},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced during the development of multi-functional system families. Addressing this trend we present an approach combining model-based development with product line techniques aiming at a consistent description of a software product family as well as supporting the configuration of its variants. We integrate the concept of variability in our framework [7] which only supported the representation of single software systems on subsequent abstraction levels so far. For the configuration of a concrete product we extend this framework by a feature-based model which allows to configure and derive single systems from a system family model. Furthermore, we explain how the complexity due to the possibly huge amount of configuration decisions can be handled by means of a staged configuration process.},
booktitle = {Proceedings of the 11th International Software Product Line Conference},
pages = {107–116},
numpages = {10},
series = {SPLC '07}
}

@inproceedings{10.1145/3490035.3490262,
author = {Bansal, Rahul and Biswas, Soma},
title = {CT-DANN: co-teaching meets DANN for wild unsupervised domain adaptation},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490262},
doi = {10.1145/3490035.3490262},
abstract = {Unsupervised domain adaptation aims at leveraging supervision from an annotated source domain for performing tasks like classification/segmentation on an unsupervised target domain. However, a large enough related dataset with clean annotations may not be always available in real scenarios, since annotations are usually obtained from crowd sourcing, and thus are noisy. Here, we consider a more realistic and challenging setting, wild unsupervised domain adaptation (WUDA), where the source domain annotations can be noisy. Standard domain adaptation approaches which directly use these noisy source labels and the unlabeled targets for the domain adaptation task perform poorly, due to severe negative transfer from the noisy source domain. In this work, we propose a novel end-to-end framework, termed CT-DANN (Co-teaching meets DANN), which seamlessly integrates a state-of-the-art approach for handling noisy labels (Co-teaching) with a standard domain adaptation framework (DANN). CT-DANN effectively utilizes all the source samples after accounting for both their noisy labels as well as transferability with respect to the target domain. Extensive experiments on three benchmark datasets with different types and levels of noise and comparison with state-of-the-art WUDA approach justify the effectiveness of the proposed framework.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {5},
numpages = {8},
keywords = {wild unsupervised domain adaptation, source data weighting, noisy source data, co-teaching},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inproceedings{10.5555/3299905.3299978,
author = {Safavi, Saeid and Wang, Wenwu and Plumbley, Mark and Choobbasti, Ali Janalizadeh and Fazekas, George},
title = {Predicting the Perceived Level of Reverberation using Features from Nonlinear Auditory Model},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Perceptual measurements have typically been recognized as the most reliable measurements in assessing perceived levels of reverberation. In this paper, a combination of blind RT60 estimation method and a binaural, nonlinear auditory model is employed to derive signal-based measures (features) that are then utilized in predicting the perceived level of rever- beration. Such measures lack the excess of effort necessary for calculating perceptual measures; not to mention the variations in either stimuli or assessors that may cause such measures to be statistically insigni?cant. As a result, the automatic extraction of objective measurements that can be applied to predict the perceived level of reverberation become of vital signi?cance. Consequently, this work is aimed at discovering measurements such as clarity, reverberance, and RT60 which can automatically be derived directly from audio data. These measurements along with labels from human listening tests are then forwarded to a machine learning system seeking to build a model to estimate the perceived level of reverberation, which is labeled by an expert, autonomously. The data has been labeled by an expert human listener for a unilateral set of ?les from arbitrary audio source types. By examining the results, it can be observed that the automatically extracted features can aid in estimating the perceptual rates.},
booktitle = {Proceedings of the 23rd Conference of Open Innovations Association FRUCT},
articleno = {73},
numpages = {5},
keywords = {machine learning, Reverberation, Human experiments, Feature extraction, Audio signal processing},
location = {Bologna, Italy},
series = {FRUCT'23}
}

@inproceedings{10.1007/978-3-030-82472-3_20,
author = {Ghazouani, Souad and Tissaoui, Anis and Chbeir, Richard},
title = {Towards a Cloud-WSDL Metamodel: A New Extension of WSDL for Cloud Service Description},
year = {2021},
isbn = {978-3-030-82471-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82472-3_20},
doi = {10.1007/978-3-030-82472-3_20},
abstract = {Several approaches have been proposed to describe services in a rich and generic manner (such as WSDL, OWL-S, WSMO, and SAWSDL). However, current approaches remain inappropriate for cloud computing since: 1) they lack in a way or another semantic or business aspect, 2) they cannot fully cope with non-functional properties and cloud characteristics, 3) they are unable to cover all kinds of services (such as SaaS, PaaS, IaaS). Despite the existence of several attempts which have tried to extent existing studies, the problem remains open. In this paper, we propose Cloud-WSDL, a new description model aligned with WSDL language, the most popular language, to make it more suitable for describing cloud services. The idea is to enhance WSDL description with our ontological Generic Cloud Service Description called GCSD to cope with many aspects (technical, operational, business, semantic and contextual) to ensure a high interoperability between services belonging to multiple heterogeneous clouds, and to support all the kinds of cloud services (SaaS, PaaS, and IaaS).},
booktitle = {Advances in Databases and Information Systems: 25th European Conference, ADBIS 2021, Tartu, Estonia, August 24–26, 2021, Proceedings},
pages = {275–288},
numpages = {14},
keywords = {Cloud computing, OWL-S ontology, WSDL, Generic Cloud Service Description, Cloud service},
location = {Tartu, Estonia}
}

@article{10.1016/j.patcog.2013.12.002,
author = {Zhou, Xiang-Dong and Zhang, Yan-Ming and Tian, Feng and Wang, Hong-An and Liu, Cheng-Lin},
title = {Minimum-risk training for semi-Markov conditional random fields with application to handwritten Chinese/Japanese text recognition},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2013.12.002},
doi = {10.1016/j.patcog.2013.12.002},
abstract = {Semi-Markov conditional random fields (semi-CRFs) are usually trained with maximum a posteriori (MAP) criterion which adopts the 0/1 cost for measuring the loss of misclassification. In this paper, based on our previous work on handwritten Chinese/Japanese text recognition (HCTR) using semi-CRFs, we propose an alternative parameter learning method by minimizing the risk on the training set, which has unequal misclassification costs depending on the hypothesis and the ground-truth. Based on this framework, three non-uniform cost functions are compared with the conventional 0/1 cost, and training data selection is incorporated to reduce the computational complexity. In experiments of online handwriting recognition on databases CASIA-OLHWDB and TUAT Kondate, we compared the performances of the proposed method with several widely used learning criteria, including conditional log-likelihood (CLL), softmax-margin (SMM), minimum classification error (MCE), large-margin MCE (LM-MCE) and max-margin (MM). On the test set (online handwritten texts) of ICDAR 2011 Chinese handwriting recognition competition, the proposed method outperforms the best system in competition.},
journal = {Pattern Recogn.},
month = may,
pages = {1904–1916},
numpages = {13},
keywords = {Semi-Markov conditional random fields, Minimum-risk training, Character string recognition}
}

@article{10.1109/TCBB.2014.2388227,
author = {Kleftogiannis, Dimitrios and Theofilatos, Konstantinos and Likothanassis, Spiros and Mavroudi, Seferina},
title = {YamiPred: A Novel Evolutionary Method for Predicting Pre-miRNAs and Selecting Relevant Features},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {12},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2014.2388227},
doi = {10.1109/TCBB.2014.2388227},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = sep,
pages = {1183–1192},
numpages = {10}
}

@inproceedings{10.1145/3019612.3019650,
author = {Wang, Tianchen and Hao, Kangli and Liu, Chun-chen and Shi, Yiyu},
title = {Resource constrained real-time lane-vehicle detection for advanced driver assistance on mobile devices},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019650},
doi = {10.1145/3019612.3019650},
abstract = {Recently, advanced driver assistance system (ADAS) has attracted a lot of attention due to the fast growing industry of smart cars, which is believed to be the next human-computer interaction after smart phones. As ADAS is a critical enabling component in a human-in-the-loop cyber-physical system (CPS) involving complicated physical environment, it has stringent requirements on reliability, accuracy as well as latency. Lane and vehicle detections are the basic functions in ADAS, which provide lane departure warning (LDW) and forward collision warning (FCW) to predict the dangers and warn the drivers. While extensive literature exists on this topic, none of them considers the important fact that many vehicles today do not have powerful embedded electronics or cameras. It will be costly to upgrade the vehicle just for ADAS enhancement. To address this issue, we demonstrate a new framework that utilizes microprocessors in mobile devices with embedded cameras for advanced driver assistance. The main challenge that comes with this low cost solution is the dilemma between limited computing power and tight latency requirement, and uncalibrated camera and high accuracy requirement. Accordingly, we propose an efficient, accurate, flexible yet light-weight real-time lane and vehicle detection method and implement it on Android devices. Real road test results suggest that an average latency of 15 fps can be achieved with a high accuracy of 12.58 average pixel offset for each lane in all scenarios and 97+ precision for vehicle detection. To the best of the authors' knowledge, this is the very first implementation of both lane and vehicle detections on mobile devices with un-calibrated embedded camera.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1430–1435},
numpages = {6},
keywords = {vehicle detection, mobile devices, lane detection, CPS, ADAS},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3363347.3363361,
author = {Naucke, Jakob and Hunt, Hamish and Crawford, Jack and Steffinlongo, Enrico and Masters, Oliver and Bergamaschi, Flavio},
title = {Homomorphically Securing AI at the Edge},
year = {2019},
isbn = {9781450370134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3363347.3363361},
doi = {10.1145/3363347.3363361},
abstract = {Edge devices are becoming increasingly pervasive in everyday life. These devices have become more computationally capable allowing for more complex AI reasoning at the edge. Subsequently, there is a need for protecting data to comply with privacy laws and confidentiality regulations. In this paper, we demonstrate the applicability of homomorphic encryption for protecting the data of AI-enabled cameras at the edge by implementing our solution on a commercial edge device. Our solution comprises a local homomorphic key-value database on an edge device populated by an AI camera; permitting the service of homomorphic search to be performed directly on the edge device. We characterize our implementation demonstrating linear behavior with respect to the database size that the edge device can support. Good enough performance is known to be difficult to achieve when employing homomorphic encryption. Our results are encouraging as we achieved solutions considered to be homomorphically fast, for example, linear performance of 1.28 seconds per database entry at over 256 bits of security. This amounts to a query being processed on a database of 200 entries in ~ 5 minutes.},
booktitle = {Proceedings of the First International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things},
pages = {32–38},
numpages = {7},
keywords = {security and privacy, information retrieval, homomorphic encryption, homomorphic computation, edge networks, IoT},
location = {New York, NY, USA},
series = {AIChallengeIoT'19}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.cie.2016.12.025,
author = {Dey, Balaram and Bairagi, Bipradas and Sarkar, Bijan and Sanyal, Subir Kumar},
title = {Group heterogeneity in multi member decision making model with an application to warehouse location selection in a supply chain},
year = {2017},
issue_date = {March 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {105},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.12.025},
doi = {10.1016/j.cie.2016.12.025},
abstract = {Display Omitted A novel MCGDM methodology based on group heterogeneity concept.Group heterogeneity established from pair wise preference comparison matrix.Biasness of information restricted by consistency check mechanism of AHP.The algorithm fits itself in the class of established MCDM techniques.Investigation finds the model as a robust and effective decision making tool. Group decision making (GDM) is more effective in extracting the real case scenarios of the decision problems to add competitive advantages in a supply chain. Group members from wider spectrum of the environment naturally command variation in knowledge level to their respective domain. The degree of heterogeneity of the decision makers in a group plays a crucial role in realistic assessment of both alternatives and selection criteria. This paper proposes a new Multi criteria GDM approach in adroit exploitation of the group heterogeneity during evaluation process and restrict the biasness of information while decision making. The importance of the heterogeneous degree of expertise is established through pair wise preference comparison matrix. To overcome the biasness, the consistency check mechanism of analytical hierarchy process (AHP) is employed. A real case example on warehouse location selection in a supply chain is illustrated to demonstrate the validity and effectiveness of the proposed approach. In order to ensure the applicability, compatibility and validity of the proposed approach, comparative study is carried out with the proven and established MCDM methodologies SAW, MOORA, TOPSIS, VIKOR, ELECTRE II, COPRAS and PROMETHEE. ANOVA, Sensitivity analysis (SA) and other investigations find the proposed approach as a rational, robust, effective and precise decision making aid to the supply chain managers.},
journal = {Comput. Ind. Eng.},
month = mar,
pages = {101–122},
numpages = {22},
keywords = {Warehouse location selection, Sensitivity analysis, Pair wise comparison, Multi criteria group decision making (MCGDM), Group heterogeneity, Analysis of variances}
}

@inproceedings{10.1007/978-3-030-76384-8_11,
author = {Jang, Jae-Won and Verbeek, Freek and Ravindran, Binoy},
title = {Verification of Functional Correctness of Code Diversification Techniques},
year = {2021},
isbn = {978-3-030-76383-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-76384-8_11},
doi = {10.1007/978-3-030-76384-8_11},
abstract = {Code diversification techniques are popular code-reuse attacks defense. The majority of code diversification research focuses on analyzing non-functional properties, such as whether the technique improves security. This paper provides a methodology to verify functional equivalence between the original and a diversified binary. We present a formal notion of binary equivalence resilient to diversification. Moreover, an algorithm is presented that checks whether two binaries – one original and one diversified – satisfy that notion of equivalence. The purpose of our work is to allow untrusted diversification techniques in a safety-critical context. We apply the methodology to three state-of-the-art diversification techniques used on the GNU Coreutils package. Overall, results show that our method can prove functional equivalence for 85,315 functions in the analyzed binaries.},
booktitle = {NASA Formal Methods: 13th International Symposium, NFM 2021, Virtual Event, May 24–28, 2021, Proceedings},
pages = {160–179},
numpages = {20},
keywords = {Verification, Functional equivalence, Code diversification}
}

@inproceedings{10.1109/SOCA.2014.48,
author = {Jungmann, Alexander and Mohr, Felix and Kleinjohann, Bernd},
title = {Applying Reinforcement Learning for Resolving Ambiguity in Service Composition},
year = {2014},
isbn = {9781479968336},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOCA.2014.48},
doi = {10.1109/SOCA.2014.48},
abstract = {Automatically composing service-based software solutions is still a challenging task. Functional as well as non-functional properties have to be considered in order to satisfy individual user requests. Regarding non-functional properties, the composition process can be modeled as optimization problem and solved accordingly. Functional properties, in turn, can be described by means of a formal specification language. State-space based planning approaches can then be applied to solve the underlying composition problem. However, depending on the expressiveness of the applied formalism and the completeness of the functional descriptions, formally equivalent services may still differ with respect to their implemented functionality. As a consequence, the most appropriate solution for a desired functionality can hardly be determined without considering additional information. In this paper, we demonstrate how to overcome this lack of information by means of Reinforcement Learning. In order to resolve ambiguity, we expand state-space based service composition by a recommendation mechanism that supports decision-making beyond formal specifications. The recommendation mechanism adjusts its recommendation strategy based on feedback from previous composition runs. Image processing serves as case study. Experimental results show the benefit of our proposed solution.},
booktitle = {Proceedings of the 2014 IEEE 7th International Conference on Service-Oriented Computing and Applications},
pages = {105–112},
numpages = {8},
keywords = {Service Recommendation, Service Functionality, Service Composition, Reinforcement Learning, Machine Learning},
series = {SOCA '14}
}

@article{10.1007/s00500-012-0896-3,
author = {Banos, Oresti and Damas, Miguel and Pomares, Hector and Rojas, Fernando and Delgado-Marquez, Blanca and Valenzuela, Olga},
title = {Human activity recognition based on a sensor weighting hierarchical classifier},
year = {2013},
issue_date = {February  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-012-0896-3},
doi = {10.1007/s00500-012-0896-3},
abstract = {The analysis of daily living human behavior has proven to be of key importance to prevent unhealthy habits. The diversity of activities and the individuals' particular execution style determine that several sources of information are normally required. One of the main issues is to optimally combine them to guarantee performance, scalability and robustness. In this paper we present a fusion classification methodology which takes into account the potential of the individual decisions yielded at both activity and sensor classification levels. Particularly tested on a wearable sensors based system, the method reinforces the idea that some parts of the body (i.e., sensors) may be specially informative for the recognition of each particular activity, thus supporting the ranking of the decisions provided by each associated sensor decision entity. Our method systematically outperforms the results obtained by traditional multiclass models which otherwise may require a high-dimensional feature space to acquire a similar performance. The comparison with other activity-recognition fusion approaches also demonstrates our model scales significantly better for small sensor networks.},
journal = {Soft Comput.},
month = feb,
pages = {333–343},
numpages = {11},
keywords = {Weighted decision, Wearable sensors, Multisource fusion, Hierarchical classification, Binary classifiers, Activity recognition}
}

@inproceedings{10.1145/3167132.3167162,
author = {Hielscher, Tommy and V\"{o}lzke, Henry and Papapetrou, Panagiotis and Spiliopoulou, Myra},
title = {Discovering, selecting and exploiting feature sequence records of study participants for the classification of epidemiological data on hepatic steatosis},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167162},
doi = {10.1145/3167132.3167162},
abstract = {In longitudinal epidemiological studies, participants undergo repeated medical examinations and are thus represented by a potentially large number of short examination outcome sequences. Some of those sequences may contain important information in various forms, such as patterns, with respect to the disease under study, while others may be on features of little relevance to the outcome. In this work, we propose a framework for Discovery, Selection and Exploitation (DiSelEx) of longitudinal epidemiological data, aiming to identify informative patterns among these sequences. DiSelEx combines sequence clustering with supervised learning to identify sequence groups that contribute to class separation. Newly derived and old features are evaluated and selected according to their redundancy and informativeness regarding the target variable. The selected feature set is then used to learn a classification model on the study data. We evaluate DiSelEx on cohort participants for the disorder "hepatic steatosis" and report on the impact on predictive performance when using sequential data in comparison to utilizing only the basic classifier.1},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {6–13},
numpages = {8},
keywords = {time-series clustering, patient similarity, medical data mining, hepatic steatosis, feature selection, epidemiological studies, classification},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/3485539,
author = {Pitchanathan, Arjun and Ulmann, Christian and Weber, Michel and Hoefler, Torsten and Grosser, Tobias},
title = {FPL: fast Presburger arithmetic through transprecision},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485539},
doi = {10.1145/3485539},
abstract = {Presburger arithmetic provides the mathematical core for the polyhedral compilation techniques that drive analytical cache models, loop optimization for ML and HPC, formal verification, and even hardware design. Polyhedral compilation is widely regarded as being slow due to the potentially high computational cost of the underlying Presburger libraries. Researchers typically use these libraries as powerful black-box tools, but the perceived internal complexity of these libraries, caused by the use of C as the implementation language and a focus on end-user-facing documentation, holds back broader performance-optimization efforts. With FPL, we introduce a new library for Presburger arithmetic built from the ground up in modern C++. We carefully document its internal algorithmic foundations, use lightweight C++ data structures to minimize memory management costs, and deploy transprecision computing across the entire library to effectively exploit machine integers and vector instructions. On a newly-developed comprehensive benchmark suite for Presburger arithmetic, we show a 5.4x speedup in total runtime over the state-of-the-art library isl in its default configuration and 3.6x over a variant of isl optimized with element-wise transprecision computing. We expect that the availability of a well-documented and fast Presburger library will accelerate the adoption of polyhedral compilation techniques in production compilers.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {162},
numpages = {26},
keywords = {Transprecision, Presburger Arithmetic, Polyhedral Compilation, Integer Sets}
}

@inproceedings{10.1145/3292500.3330990,
author = {Chen, Yu-Chia and Bijral, Avleen S. and Ferres, Juan Lavista},
title = {On Dynamic Network Models and Application to Causal Impact},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330990},
doi = {10.1145/3292500.3330990},
abstract = {Dynamic extensions of Stochastic block model (SBM) are of importance in several fields that generate temporal interaction data. These models, besides producing compact and interpretable network representations, can be useful in applications such as link prediction or network forecasting. In this paper we present a conditional pseudo-likelihood based extension to dynamic SBM that can be efficiently estimated by optimizing a regularized objective. Our formulation leads to a highly scalable approach that can handle very large networks, even with millions of nodes. We also extend our formalism to causal impact for networks that allows us to quantify the impact of external events on a time dependent sequence of networks. We support our work with extensive results on both synthetic and real networks.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1194–1204},
numpages = {11},
keywords = {stochastic block model, dynamic networks, clustering, causal impact},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@article{10.1093/ietfec/e89-a.9.2356,
author = {Lin, Jing-Ran and Peng, Qi-Cong and Huang, Qi-Shan},
title = {Adaptive Beamforming with Robustness against Both Finite-Sample Effects and Steering Vector Mismatches},
year = {2006},
issue_date = {September 2006},
publisher = {Oxford University Press, Inc.},
address = {USA},
volume = {E89-A},
number = {9},
issn = {0916-8508},
url = {https://doi.org/10.1093/ietfec/e89-a.9.2356},
doi = {10.1093/ietfec/e89-a.9.2356},
abstract = {A novel approach of robust adaptive beamforming (RABF) is presented in this paper, aiming at robustness against both finite-sample effects and steering vector mismatches. It belongs to the class of diagonal loading approaches with the loading level determined based on worst-case performance optimization. The proposed approach, however, is distinguished by two points. (1) It takes finite-sample effects into account and applies worst-case performance optimization to not only the constraints, but also the objective of the constrained quadratic equation, for which it is referred to as joint worst-case RABF (JW-RABF). (2) It suggests a simple closed-form solution to the optimal loading after some approximations, revealing how different factors affect the loading. Compared with many existing methods in this field, the proposed one achieves better robustness in the case of small sample data size as well as steering vector mismatches. Moreover, it is less computationally demanding for presenting a simple closed-form solution to the optimal loading. Numerical examples confirm the effectiveness of the proposed approach.},
journal = {IEICE Trans. Fundam. Electron. Commun. Comput. Sci.},
month = sep,
pages = {2356–2362},
numpages = {7},
keywords = {steering vector mismatches, robust adaptive beamforming (RABF), joint worst-case performance optimization, finite-sample effects, diagonal loading}
}

@article{10.1016/j.jbi.2021.103716,
author = {Consuegra-Ayala, Juan Pablo and Guti\'{e}rrez, Yoan and Piad-Morffis, Alejandro and Almeida-Cruz, Yudivian and Palomar, Manuel},
title = {Automatic extension of corpora from the intelligent ensembling of eHealth knowledge discovery systems outputs},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {116},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103716},
doi = {10.1016/j.jbi.2021.103716},
journal = {J. of Biomedical Informatics},
month = apr,
numpages = {16},
keywords = {Natural language processing, Relation extraction, Entity recognition, Information extraction, Annotated corpora, Ensemble methods}
}

@inproceedings{10.1007/978-3-319-94289-6_17,
author = {Ali, Kashif and Hamilton, Margaret and Thevathayan, Charles and Zhang, Xiuzhen},
title = {Social Information Services: A Service Oriented Analysis of Social Media},
year = {2018},
isbn = {978-3-319-94288-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-94289-6_17},
doi = {10.1007/978-3-319-94289-6_17},
abstract = {Social media has emerged as a free source of public data. Despite the diversity in social media platforms, current social media analysis tools consider them as a similar entity. Hence, these tools lack the flexibility to interpret the diversity of social media platforms and its analysis requirements. In order to interpret the diversity, we propose a novel service oriented approach to visualize and analyze social media platforms. Firstly, we formalize social media as a service and classify its functional and non-functional properties. Secondly, we conduct experiments on the real-world datasets for a variety of topics. We empirically quantify the functional and non-functional properties of social media platforms. We also present future directions and research challenges associated with the service orientation of social media.},
booktitle = {Web Services – ICWS 2018: 25th International Conference, Held as Part of the Services Conference Federation, SCF 2018, Seattle, WA, USA, June 25-30, 2018, Proceedings},
pages = {263–279},
numpages = {17},
keywords = {Social networks and services, Social information services, Service orientation, Quality of Service (QoS)},
location = {Seattle, WA, USA}
}

@article{10.1007/s11277-021-08827-z,
author = {Ebhota, Virginia C. and Srivastava, Viranjay M.},
title = {Improved Electromagnetic Signal Power Loss Prediction Using Fused Adaptive Linear Based Smoothing Technique},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {121},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08827-z},
doi = {10.1007/s11277-021-08827-z},
abstract = {This research work designs and implements an improved signal power loss prediction model using fused adaptive Linear Mean Square (LMS) filter smoothing technique combined with a hybrid Artificial Neural Network (ANN) of Adaptive Linear Element (ADALINE) and Multi-Layer Perceptron (MLP). The performance of the designed model termed as filtered adaptive hybrid ANN has been trained and tested via simulation using normalized measurement data, which have been collected from two metropolitan areas termed as location-1 and location-2. This has been compared with the performance of conventional MLP-ANN using the same dataset. Four different first-order statistical performance indices have been applied for error analysis for the prediction performances of the models. These statistical performance indices basically measure the closeness of the prediction values to the measurement data with the difference being the error. The filtered adaptive hybrid model predicted the measurement data with lesser errors in comparison to the MLP-ANN. The training states, correlation coefficient, regression analysis and prediction graphs of the filtered adaptive hybrid ANN and MLP-ANN have been plotted and analyzed using measurement datasets from location-1 and location-2. The results demonstrate improved prediction ability of the filtered adaptive hybrid ANN over the conventional MLP-ANN. Furthermore, it shows that an adaptive LMS filter for data denoising fused with hybrid ANN of ADALINE and MLP-ANN improves signal power loss prediction performance over convention MLP-ANN.},
journal = {Wirel. Pers. Commun.},
month = dec,
pages = {2371–2392},
numpages = {22},
keywords = {Multi-layer perceptron, Adaptive linear element, Artificial neural network, Electromagnetic signal power loss, Signal denoising, Adaptive filters}
}

@inproceedings{10.1145/3307681.3325398,
author = {Doudali, Thaleia Dimitra and Blagodurov, Sergey and Vishnu, Abhinav and Gurumurthi, Sudhanva and Gavrilovska, Ada},
title = {Kleio: A Hybrid Memory Page Scheduler with Machine Intelligence},
year = {2019},
isbn = {9781450366700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307681.3325398},
doi = {10.1145/3307681.3325398},
abstract = {The increasing demand of big data analytics for more main memory capacity in datacenters and exascale computing environments is driving the integration of heterogeneous memory technologies. The new technologies exhibit vastly greater differences in access latencies, bandwidth and capacity compared to the traditional NUMA systems. Leveraging this heterogeneity while also delivering application performance enhancements requires intelligent data placement. We present Kleio, a page scheduler with machine intelligence for applications that execute across hybrid memory components. Kleio is a hybrid page scheduler that combines existing, lightweight, history-based data tiering methods for hybrid memory, with novel intelligent placement decisions based on deep neural networks. We contribute new understanding toward the scope of benefits that can be achieved by using intelligent page scheduling in comparison to existing history-based approaches, and towards the choice of the deep learning algorithms and their parameters that are effective for this problem space. Kleio incorporates a new method for prioritizing pages that leads to highest performance boost, while limiting the resulting system resource overheads. Our performance evaluation indicates that Kleio reduces on average 80% of the performance gap between the existing solutions and an oracle with knowledge of future access pattern. Kleio provides hybrid memory systems with fast and effective neural network training and prediction accuracy levels, which bring significant application performance improvements with limited resource overheads, so as to lay the grounds for its practical integration in future systems.},
booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {37–48},
numpages = {12},
keywords = {recurrent neural networks, page scheduler, non volatile memory, machine learning, machine intelligence, long short term memory networks, hybrid memory systems, heterogeneous memory systems, emerging memory technologies, data tiering},
location = {Phoenix, AZ, USA},
series = {HPDC '19}
}

@article{10.1007/s10489-018-01399-9,
author = {Abolpour Mofrad, Asieh and Yazidi, Anis and Lewi Hammer, Hugo},
title = {On solving the SPL problem using the concept of probability flux},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-01399-9},
doi = {10.1007/s10489-018-01399-9},
abstract = {The Stochastic Point Location (SPL) problem Oommen is a fundamental learning problem that has recently found a lot of research attention. SPL can be summarized as searching for an unknown point in an interval under faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic Environment which in turn informs it about the direction of the search. Since the Environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem, which was pioneered two decades ago by Oommen, relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution, i.e., infinite memory. However, this strategy yields rather poor accuracy for low discretization resolutions. In this paper, we present two major contributions to the SPL problem. First, we demonstrate that the estimation of the point location can significantly be improved by resorting to the concept of mutual probability flux between neighboring states along the line. Second, we are able to accurately track the position of the optimal point and simultaneously show a method by which we can estimate the error probability characterizing the Environment. Interestingly, learning this error probability of the Environment takes place in tandem with the unknown location estimation. We present and analyze several experiments discussing the weaknesses and strengths of the different methods.},
journal = {Applied Intelligence},
month = jul,
pages = {2699–2722},
numpages = {24},
keywords = {Stochastic Point Location (SPL), Stochastic Learning Weak Estimation (SLWE), Mutual probability flux, Last Transition-based Estimation Solution (LTES), Flux-based Estimation Solution (FES), Estimating environment effectiveness}
}

@inproceedings{10.1145/3219819.3220018,
author = {Shan, Ying and jiao, Jian and Zhu, Jie and Mao, JC},
title = {Recurrent Binary Embedding for GPU-Enabled Exhaustive Retrieval from Billion-Scale Semantic Vectors},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220018},
doi = {10.1145/3219819.3220018},
abstract = {Rapid advances in GPU hardware and multiple areas of Deep Learning open up a new opportunity for billion-scale information retrieval with exhaustive search. Building on top of the powerful concept of semantic learning, this paper proposes a Recurrent Binary Embedding (RBE) model that learns compact representations for real-time retrieval. The model has the unique ability to refine a base binary vector by progressively adding binary residual vectors to meet the desired accuracy. The refined vector enables efficient implementation of exhaustive similarity computation with bit-wise operations, followed by a near-lossless k-NN selection algorithm, also proposed in this paper. The proposed algorithms are integrated into an end-to-end multi-GPU system that retrieves thousands of top items from over a billion candidates in real-time. The RBE model and the retrieval system were evaluated with data from a major paid search engine. When measured against the state-of-the-art model for binary representation and the full precision model for semantic embedding, RBE significantly outperformed the former, and filled in over 80% of the AUC gap in-between. Experiments comparing with our production retrieval system also demonstrated superior performance. While the primary focus of this paper is to build RBE based on a particular class of semantic models, generalizing to other types is straightforward, as exemplified by two different models at the end of the paper.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2170–2179},
numpages = {10},
keywords = {sponsored search, semantic embedding, k-nn, information retrieval, gpu, deep neural network (dnn), deep learning, cntk, cdssm, binary network},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3442391.3442402,
author = {Lesoil, Luc and Acher, Mathieu and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {Deep Software Variability: Towards Handling Cross-Layer Configuration},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442402},
doi = {10.1145/3442391.3442402},
abstract = {Configuring software is a powerful means to reach functional and performance goals of a system. However, many layers (hardware, operating system, input data, etc.), themselves subject to variability, can alter performances of software configurations. For instance, configurations’ options of the x264 video encoder may have very different effects on x264’s encoding time when used with different input videos, depending on the hardware on which it is executed. In this vision paper, we coin the term deep software variability to refer to the interaction of all external layers modifying the behavior or non-functional properties of a software. Deep software variability challenges practitioners and researchers: the combinatorial explosion of possible executing environments complicates the understanding, the configuration, the maintenance, the debug, and the test of configurable systems. There are also opportunities: harnessing all variability layers (and not only the software layer) can lead to more efficient systems and configuration knowledge that truly generalizes to any usage and context.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {8},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s00500-014-1364-z,
author = {Baladhandapani, Arunadevi and Nachimuthu, Deepa Subramaniam},
title = {Evolutionary learning of spiking neural networks towards quantification of 3D MRI brain tumor tissues},
year = {2015},
issue_date = {July      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {7},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-014-1364-z},
doi = {10.1007/s00500-014-1364-z},
abstract = {This paper presents a new classification technique for 3D MR images, based on a third-generation network of spiking neurons. Implementation of multi-dimensional co-occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed. The results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue-specific structural patterns and achieve segmentation of tumor part. The spiking network classifier has been validated and tested for various real-time and Harvard benchmark datasets, where appreciable performance in terms of mean square error, accuracy and computational time is obtained. The spiking network employed Izhikevich neurons as nodes in a multi-layered structure. The classifier has been compared with computational power of multi-layer neural networks with sigmoidal neurons. The results on misclassified tumors are analyzed and suggestions for future work are discussed.},
journal = {Soft Comput.},
month = jul,
pages = {1803–1816},
numpages = {14},
keywords = {Spiking neural networks, Multi-dimensional co-occurrence matrices, Izhikevich neurons, Genetic algorithm, 3D Magnetic resonance imaging}
}

@article{10.5555/2598944.2599210,
author = {Maldonado, Sebasti\'{a}n and L\'{o}pez, Julio},
title = {Alternative second-order cone programming formulations for support vector classification},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {268},
issn = {0020-0255},
abstract = {This paper presents two novel second-order cone programming (SOCP) formulations that determine a linear predictor using Support Vector Machines (SVMs). Inspired by the soft-margin SVM formulation, our first approach (@x-SOCP-SVM) proposes a relaxation of the conic constraints via a slack variable, penalizing it in the objective function. The second formulation (r-SOCP-SVM) is based on the LP-SVM formulation principle: the bound of the VC dimension is loosened properly using the l"~-norm, and the margin is directly maximized. The proposed methods have several advantages: The first approach constructs a flexible classifier, extending the benefits of the soft-margin SVM formulation to second-order cones. The second method obtains comparable results to the SOCP-SVM formulation with less computational effort, since one conic restriction is eliminated. Experiments on well-known benchmark datasets from the UCI Repository demonstrate that our approach accomplishes the best classification performance compared to the traditional SOCP-SVM formulation, LP-SVM, and to standard linear SVM.},
journal = {Inf. Sci.},
month = jun,
pages = {328–341},
numpages = {14},
keywords = {Support Vector Machine, Second-order cone programming, Linear programming SVM}
}

@article{10.1016/j.asoc.2016.06.043,
author = {Basto-Fernandes, Vitor and Yevseyeva, Iryna and M\'{e}ndez, Jos\'{e} R. and Zhao, Jiaqi and Fdez-Riverola, Florentino and T.M. Emmerich, Michael},
title = {A spam filtering multi-objective optimization study covering parsimony maximization and three-way classification},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.06.043},
doi = {10.1016/j.asoc.2016.06.043},
abstract = {Display Omitted Advances on applications of multi-objective optimization to anti-SPAM filtering.Parsimony maximization of rule-based SPAM classifiers.Three-way classification balancing user effort and confidence level.Indicator-based/machine learning/decomposition-based evolutionary optimization. Classifier performance optimization in machine learning can be stated as a multi-objective optimization problem. In this context, recent works have shown the utility of simple evolutionary multi-objective algorithms (NSGA-II, SPEA2) to conveniently optimize the global performance of different anti-spam filters. The present work extends existing contributions in the spam filtering domain by using three novel indicator-based (SMS-EMOA, CH-EMOA) and decomposition-based (MOEA/D) evolutionary multi-objective algorithms. The proposed approaches are used to optimize the performance of a heterogeneous ensemble of classifiers into two different but complementary scenarios: parsimony maximization and e-mail classification under low confidence level. Experimental results using a publicly available standard corpus allowed us to identify interesting conclusions regarding both the utility of rule-based classification filters and the appropriateness of a three-way classification system in the spam filtering domain.},
journal = {Appl. Soft Comput.},
month = nov,
pages = {111–123},
numpages = {13},
keywords = {Three-way classification, SpamAssassin, Spam filtering, Rule-based classifiers, Parsimony, Multi-objective optimization}
}

@inproceedings{10.1145/3357766.3359531,
author = {Broman, David},
title = {A vision of miking: interactive programmatic modeling, sound language composition, and self-learning compilation},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359531},
doi = {10.1145/3357766.3359531},
abstract = {This paper introduces a vision of Miking, a language framework for constructing efficient and sound language environments and compilers for domain-specific modeling languages. In particular, this language framework has three key objectives: (i) to automatically generate interactive programmatic modeling environments, (ii) to guarantee sound compositions of language fragments that enable both rapid and safe domain-specific language development, (iii) to include first-class support for self-learning compilation, targeting heterogeneous execution platforms. The initiative is motivated in the domain of mathematical modeling languages. Specifically, two different example domains are discussed: (i) modeling, simulation, and verification of cyber-physical systems, and (ii) domain-specific differentiable probabilistic programming. The paper describes the main objectives of the vision, as well as concrete research challenges and research directions.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {55–60},
numpages = {6},
keywords = {semantics, modeling languages, machine learning, domain-specific languages, composition, compilers},
location = {Athens, Greece},
series = {SLE 2019}
}

@article{10.1016/j.patcog.2008.09.015,
author = {Liu, Manhua and Jiang, Xudong and Kot, Alex C.},
title = {A multi-prototype clustering algorithm},
year = {2009},
issue_date = {May, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.09.015},
doi = {10.1016/j.patcog.2008.09.015},
abstract = {Clustering is an important unsupervised learning technique widely used to discover the inherent structure of a given data set. Some existing clustering algorithms uses single prototype to represent each cluster, which may not adequately model the clusters of arbitrary shape and size and hence limit the clustering performance on complex data structure. This paper proposes a clustering algorithm to represent one cluster by multiple prototypes. The squared-error clustering is used to produce a number of prototypes to locate the regions of high density because of its low computational cost and yet good performance. A separation measure is proposed to evaluate how well two prototypes are separated. Multiple prototypes with small separations are grouped into a given number of clusters in the agglomerative method. New prototypes are iteratively added to improve the poor cluster separations. As a result, the proposed algorithm can discover the clusters of complex structure with robustness to initial settings. Experimental results on both synthetic and real data sets demonstrate the effectiveness of the proposed clustering algorithm.},
journal = {Pattern Recogn.},
month = may,
pages = {689–698},
numpages = {10},
keywords = {Squared-error clustering, Separation measure, Data clustering, Cluster prototype}
}

@inproceedings{10.5555/3386691.3386706,
author = {Lu, Sidi and Luo, Bing and Patel, Tirthak and Yao, Yongtao and Tiwari, Devesh and Shi, Weisong},
title = {Making disk failure predictions SMARTer!},
year = {2020},
isbn = {9781939133120},
publisher = {USENIX Association},
address = {USA},
abstract = {Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.},
booktitle = {Proceedings of the 18th USENIX Conference on File and Storage Technologies},
pages = {151–168},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {FAST'20}
}

@inproceedings{10.1145/3447555.3465327,
author = {Herzog, Benedict and H\"{u}gel, Fabian and Reif, Stefan and H\"{o}nig, Timo and Schr\"{o}der-Preikschat, Wolfgang},
title = {Automated Selection of Energy-efficient Operating System Configurations},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3465327},
doi = {10.1145/3447555.3465327},
abstract = {Edge computing systems need to use their available resources efficiently. Operating systems and run-time systems offer numerous configuration parameters to fine-tune their behaviour, which are adjustable to balance the execution time and energy demand of applications. However, the number of parameters produces a vast space of possible configurations and the exact consequences on non-functional properties are often poorly documented. Thus, identifying efficient configurations proves challenging.This paper presents Polar, an approach for the automated determination of energy-efficient configurations, as well as an implementation for Linux. Polar combines application profiles and system-level information to select efficient configurations dynamically and does not require application changes. Configurations are predicted by an oracle either based on linear models or neural networks. Our evaluation shows that Polar improves the mean energy efficiency by 11.5 % for typical applications.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {309–315},
numpages = {7},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@article{10.1007/s10664-017-9580-7,
author = {Przyby\l{}ek, Adam},
title = {An empirical study on the impact of AspectJ on software evolvability},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9580-7},
doi = {10.1007/s10664-017-9580-7},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2018–2050},
numpages = {33},
keywords = {Understandability, Separation of concerns, Maintainability, Controlled experiment, Aspect-oriented programming, AOP}
}

@article{10.3233/IDT-180326,
author = {Schmidt, Michael and Weber, Gerhard},
title = {Dynamic time warping for the recognition of single-stroke input},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {12},
number = {2},
issn = {1872-4981},
url = {https://doi.org/10.3233/IDT-180326},
doi = {10.3233/IDT-180326},
abstract = {The work at hand presents thorough investigations of dynamic time warping (DTW) for on-line recognition of single-stroke input. We survey and give a systematization of nearest neighbor methods and show how concepts of DTW outperform current approaches without adding too much complexity. Observing mostly conservative and ‘expensive’ utilization and underestimation of DTW in this field, the contribution of this work is to give best practices for enhancing template matching by application of DTW which can be implemented in few lines of code. Our survey is substantiated by tests regarding questions of proper feature selection, pre-processing, and suitable parametrization. To provide flexible, device-independent recognition and as human input is also affected by practice or exhaustion, premise is a classification without interference by an input’s natural variances in speed, translation, scaling, and rotation. A set of geometric features prevalent in literature is given and extended by own contributions. Various specifications for DTW are evaluated with three different test sets. Our results show that features based on distances and common step patterns are outperformed by specific types of chain codes and local constraints. Classification also benefits from global warping windows usually applied in the context of speech recognition.},
journal = {Int. Dec. Tech.},
month = jan,
pages = {173–186},
numpages = {14},
keywords = {single-stroke, template-based, classification, benchmark, survey, Dynamic time warping (DTW)}
}

@inproceedings{10.1007/978-3-030-59612-5_4,
author = {Kantere, Verena},
title = {Processing Big Data Across Infrastructures},
year = {2020},
isbn = {978-3-030-59611-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59612-5_4},
doi = {10.1007/978-3-030-59612-5_4},
abstract = {For a range of major scientific computing challenges that span fundamental and applied science, the deployment of Big Data Applications on a large-scale system, such as an internal or external cloud, a cluster or even distributed public resources (“crowd computing”), needs to be offered with guarantees of predictable performance and utilization cost. Currently, however, this is not possible, because scientific communities lack the technology, both at the level of modelling and analytics, which identifies the key characteristics of BDAs and their impact on performance. There is also little data or simulations available that address the role of the system operation and infrastructure in defining overall performance. Our vision is to fill this gap by producing a deeper understanding of how to optimize the deployment of Big Data Applications on hybrid large-scale infrastructures. Our objective is the optimal deployment of BDAs that run on systems operating on large infrastructures, in order to achieve optimal performance, while taking into account running costs. We describe a methodology to achieve this vision. The methodology starts with the modeling and profiling of applications, as well as with the exploration of alternative systems for their execution, which are hybridization’s of cloud, cluster and crowd. It continues with the employment of predictions to create schemes for performance optimization with respect to cost limitations for system utilization. The schemes can accommodate execution by adapting, i.e. extend or change, the system.},
booktitle = {Big Data – BigData 2020: 9th International Conference, Held as Part of the Services Conference Federation, SCF 2020, Honolulu, HI, USA, September 18-20, 2020, Proceedings},
pages = {38–51},
numpages = {14},
keywords = {Component, Formatting, Style, Styling, Insert (key words)},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/3097983.3097989,
author = {Mautz, Dominik and Ye, Wei and Plant, Claudia and B\"{o}hm, Christian},
title = {Towards an Optimal Subspace for K-Means},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3097989},
doi = {10.1145/3097983.3097989},
abstract = {Is there an optimal dimensionality reduction for k-means, revealing the prominent cluster structure hidden in the data? We propose SUBKMEANS, which extends the classic k-means algorithm. The goal of this algorithm is twofold: find a sufficient k-means-style clustering partition and transform the clusters onto a common subspace, which is optimal for the cluster structure. Our solution is able to pursue these two goals simultaneously. The dimensionality of this subspace is found automatically and therefore the algorithm comes without the burden of additional parameters. At the same time this subspace helps to mitigate the curse of dimensionality. The SUBKMEANS optimization algorithm is intriguingly simple and efficient. It is easy to implement and can readily be adopted to the current situation. Furthermore, it is compatible to many existing extensions and improvements of k-means.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {365–373},
numpages = {9},
keywords = {subspace, k-means, dimensionality reduction, clustering},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3123266.3123428,
author = {Chen, Jing-jing and Ngo, Chong-Wah and Chua, Tat-Seng},
title = {Cross-modal Recipe Retrieval with Rich Food Attributes},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123428},
doi = {10.1145/3123266.3123428},
abstract = {Food is rich of visible (e.g., colour, shape) and procedural (e.g., cutting, cooking) attributes. Proper leveraging of these attributes, particularly the interplay among ingredients, cutting and cooking methods, for health-related applications has not been previously explored. This paper investigates cross-modal retrieval of recipes, specifically to retrieve a text-based recipe given a food picture as query. As similar ingredient composition can end up with wildly different dishes depending on the cooking and cutting procedures, the difficulty of retrieval originates from fine-grained recognition of rich attributes from pictures. With a multi-task deep learning model, this paper provides insights on the feasibility of predicting ingredient, cutting and cooking attributes for food recognition and recipe retrieval. In addition, localization of ingredient regions is also possible even when region-level training examples are not provided. Experiment results validate the merit of rich attributes when comparing to the recently proposed ingredient-only retrieval techniques.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1771–1779},
numpages = {9},
keywords = {recipe retrieval, ingredient recognition, cross-modal retrieval, cooking and cutting recognition},
location = {Mountain View, California, USA},
series = {MM '17}
}

@article{10.1145/2512432,
author = {Shobaki, Ghassan and Shawabkeh, Maxim and Rmaileh, Najm Eldeen Abu},
title = {Preallocation instruction scheduling with register pressure minimization using a combinatorial optimization approach},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/2512432},
doi = {10.1145/2512432},
abstract = {Balancing Instruction-Level Parallelism (ILP) and register pressure during preallocation instruction scheduling is a fundamentally important problem in code generation and optimization. The problem is known to be NP-complete. Many heuristic techniques have been proposed to solve this problem. However, due to the inherently conflicting requirements of maximizing ILP and minimizing register pressure, heuristic techniques may produce poor schedules in many cases. If such cases occur in hot code, significant performance degradation may result. A few combinatorial optimization approaches have also been proposed, but none of them has been shown to solve large real-world instances within reasonable time. This article presents the first combinatorial algorithm that is efficient enough to optimally solve large instances of this problem (basic blocks with hundreds of instructions) within a few seconds per instance. The proposed algorithm uses branch-and-bound enumeration with a number of powerful pruning techniques to efficiently search the solution space. The search is based on a cost function that incorporates schedule length and register pressure. An implementation of the proposed scheduling algorithm has been integrated into the LLVM Compiler and evaluated using SPEC CPU 2006. On x86-64, with a time limit of 10ms per instruction, it optimally schedules 79% of the hot basic blocks in FP2006. Another 19% of the blocks are not optimally scheduled but are improved in cost relative to LLVM's heuristic. This improves the execution time of some benchmarks by up to 21%, with a geometric-mean improvement of 2.4% across the entire benchmark suite. With the use of precise latency information, the geometric-mean improvement is increased to 2.8%.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {14},
numpages = {31},
keywords = {register pressure reduction, performance optimization, optimal instruction scheduling, instruction-level parallelism (ILP), branch-and-bound enumeration, NP-complete problems, Compiler optimizations}
}

@article{10.1007/s10586-020-03047-9,
author = {Maamar, Zakaria and Asim, Muhammad and Boukadi, Khouloud and Baker, Thar and Saeed, Saad and Guidara, Ikbel and Yahya, Fadwa and Ugljanin, Emir and Benslimane, Djamal},
title = {Towards a Quality-of-Thing based approach for assigning things to federations},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-020-03047-9},
doi = {10.1007/s10586-020-03047-9},
abstract = {In the context of an Internet-of-Things&nbsp;(IoT) ecosystem, this paper discusses two&nbsp;necessary stages for managing federations of things. The first stage defines things in terms of duties and non-functional properties that define the quality of these duties. And, the second stage uses these properties to assign appropriate things to future federations. Specialized into ad hoc and planned, federations are expected to satisfy needs and requirements of real-life situations like traffic control that arise at run-time. A set of experiments using a mix of real and simulated datasets, demonstrate the technical doability of thing assignment to federations and are presented in the paper, as well.},
journal = {Cluster Computing},
month = sep,
pages = {1589–1602},
numpages = {14},
keywords = {Assignment, Quality-of-Things, IoT, Federation}
}

@inproceedings{10.5555/1659450.1659529,
author = {Pavlik, Philip I. and Cen, Hao and Koedinger, Kenneth R.},
title = {Performance Factors Analysis --A New Alternative to Knowledge Tracing},
year = {2009},
isbn = {9781607500285},
publisher = {IOS Press},
address = {NLD},
abstract = {Knowledge tracing (KT)[1] has been used in various forms for adaptive computerized instruction for more than 40 years. However, despite its long history of application, it is difficult to use in domain model search procedures, has not been used to capture learning where multiple skills are needed to perform a single action, and has not been used to compute latencies of actions. On the other hand, existing models used for educational data mining (e.g. Learning Factors Analysis (LFA)[2]) and model search do not tend to allow the creation of a “model overlay” that traces predictions for individual students with individual skills so as to allow the adaptive instruction to automatically remediate performance. Because these limitations make the transition from model search to model application in adaptive instruction more difficult, this paper describes our work to modify an existing data mining model so that it can also be used to select practice adaptively. We compare this new adaptive data mining model (PFA, Performance Factors Analysis) with two versions of LFA and then compare PFA with standard KT.},
booktitle = {Proceedings of the 2009 Conference on Artificial Intelligence in Education: Building Learning Systems That Care: From Knowledge Representation to Affective Modelling},
pages = {531–538},
numpages = {8},
keywords = {Knowledge Tracing, Educational Data Mining, Adaptive Modeling}
}

@inproceedings{10.1007/978-3-030-30709-7_9,
author = {Song, Yang and Li, Yunchun and Wu, Shuhan and Yang, Hailong and Li, Wei},
title = {ASTracer: An Efficient Tracing Tool for HDFS with Adaptive Sampling},
year = {2019},
isbn = {978-3-030-30708-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30709-7_9},
doi = {10.1007/978-3-030-30709-7_9},
abstract = {Existing distributed tracing tools such as HTrace use static probabilistic samplers to collect the function call trees for performance analysis, which may fail to capture important but less executed function call trees and thus miss the opportunities for performance optimization. To address the above problem, we propose ASTracer, a new distributed tracing tool with two adaptive samplers. The advantage of adaptive samplers is that they can adjust the sampling rate dynamically, which is able to capture comprehensive function call trees and in the meanwhile maintain the size of trace file acceptable. In addition, we propose an auto-tuning mechanism to search for the optimal parameter settings of the adaptive samplers in ASTracer. The experiment results demonstrate the adaptive samplers are more effective in tracing the function call trees compared to probabilistic sampler. Moreover, we provide several case studies to demonstrate the usage of ASTracer in identifying potential performance bottlenecks.},
booktitle = {Network and Parallel Computing: 16th IFIP WG 10.3 International Conference, NPC 2019, Hohhot, China, August 23–24, 2019, Proceedings},
pages = {107–119},
numpages = {13},
keywords = {Adaptive sampling, Distributed tracing tool, HDFS},
location = {Hohhot, China}
}

@inproceedings{10.5555/3495724.3496750,
author = {Zhang, Dingwen and Tian, Haibin and Han, Jungong},
title = {Few-cost salient object detection with adversarial-paced learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1026},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1155/2021/5513349,
author = {Shi, Xiaoxiao and Khan, Rahim},
title = {A Method of Optimizing Network Topology Structure Combining Viterbi Algorithm and Bayesian Algorithm},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/5513349},
doi = {10.1155/2021/5513349},
abstract = {With Internet entering all walks of life, development of internet and usage expansion demand better performance, especially the application of 5G network that adopts NAS networking mode. Some of the network bandwidth cannot fully support the current network demand, which causes network fluctuations and other concerns. In this paper, a method for optimizing the topological structure of the bottom layer of the communication network is proposed that has outage performance close to optimal routing scheme. In specific, path in areas with poor network conditions is first optimized using Viterbi algorithm. Then, network element nodes on the path are optimized using Bayes recommendation algorithm for reasonable flow distribution. Dual planning of improved Viterbi algorithm is used to realize the main and standby path planning, and then, Bayesian recommendation algorithm based on the average value is used to optimize the network elements. Therefore, it is very efficient to realize overall performance optimization.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {12}
}

@article{10.1016/j.future.2015.11.025,
author = {Sun, Le and Ma, Jiangang and Zhang, Yanchun and Dong, Hai and Hussain, Farookh Khadeer},
title = {Cloud-FuSeR},
year = {2016},
issue_date = {April 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.11.025},
doi = {10.1016/j.future.2015.11.025},
abstract = {With the rapidly growing number of available Cloud services, to fulfill the need for ordinary users to select accurate services has become a significant challenge. However, as a Cloud service environment encompasses many uncertainties that may hinder users to make sound decisions, it is highly desirable to handle fuzzy information when choosing a suitable service in an uncertain environment. In this paper, we present a novel fuzzy decision-making framework that improves the existing Cloud service selection techniques. In particular, we build a fuzzy ontology to model uncertain relationships between objects in databases for service matching, and present a novel analytic hierarchy process approach to calculate the semantic similarity between concepts. We also present a multi-criteria decision-making technique to rank Cloud services. Furthermore, we conduct extensive experiments to evaluate the performance of the fuzzy ontology-based similarity matching. The experimental results show the efficiency of the proposed method. Discuss why Cloud service selection problem is important.Design a fuzzy ontology for Cloud service selection problems.Identify proper time points and approaches for operating fuzzy variables.Consider the functional similarity and QoS performance simultaneously by distinguishing compositions of service functions.Combine user preferences and expert perceptions on service functions and their evaluation properties.},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {42–55},
numpages = {14},
keywords = {Multi-criteria decision-making, Fuzzy ontology, Fuzzy TOPSIS, Fuzzy AHP, Cloud service selection}
}

@article{10.1007/s00521-020-04969-6,
author = {Schorn, Christoph and Elsken, Thomas and Vogel, Sebastian and Runge, Armin and Guntoro, Andre and Ascheid, Gerd},
title = {Automated design of error-resilient and hardware-efficient deep neural networks},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {24},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04969-6},
doi = {10.1007/s00521-020-04969-6},
abstract = {Applying deep neural networks (DNNs) in mobile and safety-critical systems, such as autonomous vehicles, demands a reliable and efficient execution on hardware. The design of the neural architecture has a large influence on the achievable efficiency and bit error resilience of the network on hardware. Since there are numerous design choices for the architecture of DNNs, with partially opposing effects on the preferred characteristics (such as small error rates at low latency), multi-objective optimization strategies are necessary. In this paper, we develop an evolutionary optimization technique for the automated design of hardware-optimized DNN architectures. For this purpose, we derive a set of inexpensively computable objective functions, which enable the fast evaluation of DNN architectures with respect to their hardware efficiency and error resilience. We observe a strong correlation between predicted error resilience and actual measurements obtained from fault injection simulations. Furthermore, we analyze two different quantization schemes for efficient DNN computation and find one providing a significantly higher error resilience compared to the other. Finally, a comparison of the architectures provided by our algorithm with the popular MobileNetV2 and NASNet-A models reveals an up to seven times improved bit error resilience of our models. We are the first to combine error resilience, efficiency, and performance optimization in a neural architecture search framework.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {18327–18345},
numpages = {19},
keywords = {AutoML, Multi-objective optimization, Neural architecture search, Hardware faults, Error resilience, Neural network hardware}
}

@article{10.1016/j.compag.2019.105023,
author = {Moon, Taewon and Hong, Seojung and Choi, Ha Young and Jung, Dae Ho and Chang, Se Hong and Son, Jung Eek},
title = {Interpolation of greenhouse environment data using multilayer perceptron},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105023},
doi = {10.1016/j.compag.2019.105023},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {8},
keywords = {Spline, Random forest, Multivariate regression, Linear, Data loss}
}

@article{10.3233/JIFS-179824,
author = {Zhou, Yuebin and Xu, Jianlong and Garc\'{\i}a Guirao, Juan Luis},
title = {IoT perception layer scheduling deadlock relieving optimization method},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179824},
doi = {10.3233/JIFS-179824},
abstract = {When resources are limited and the perception tasks are time-constrained, if multiple perception tasks request the response at the same time, it is easy to cause scheduling deadlocks in the perception layer and affect its access speed and perception efficiency. This paper studies the mathematical feature expression method of task and resource allocation when deadlock occurs in the perception layer. According to the conditions of deadlock mutual exclusion, non preemption, request and hold, cycle waiting, a deadlock detection model of hierarchical scheduling system based on time constraints is adopted to establish the time constraint relationship of the deadlock detector and the genetic algorithm is applied to solve the problem of multi-tasks deadlock relieving strategy optimization in the perceptive layer. The research results and simulation experiments show that the deadlock relieving optimization method based on genetic algorithm can quickly relieve the deadlock, ensure the minimum cost of relieving, and significantly improve the efficiency of deadlock relieving.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7521–7529},
numpages = {9},
keywords = {deadlock relieving, information scheduling, perception layer, Internet of things}
}

@inproceedings{10.1007/978-3-030-96140-4_1,
author = {Ghazouani, Souad and Tissaoui, Anis and Chbeir, Richard},
title = {Cloud-WSDL: Making WSDL Suitable for Cloud Computing},
year = {2021},
isbn = {978-3-030-96139-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-96140-4_1},
doi = {10.1007/978-3-030-96140-4_1},
abstract = {Several approaches have been proposed to describe services in a rich and generic manner (such as WSDL, OWL-S, WSMO, and SAWSDL). However, current approaches remain inappropriate for cloud computing since: 1) they lack in a way or another semantic or business aspect, 2) they cannot fully cope with non-functional properties, 3) they are unable to cover all kinds of services (such as SaaS, PaaS, IaaS). Despite the existence of several attempts which have tried to extent existing studies, the problem remains open. In this paper, we propose Cloud-WSDL, a new description model aligned with WSDL language, the most popular language, to make it more suitable for describing cloud services. The idea is to enhance WSDL description with new functional, non-functional and cloud features so to cope with many aspects (technical, operational, business, semantic and contextual). The proposed extension ensures a high interoperability between services belonging to multiple heterogeneous clouds, and supports all the kinds of cloud services (SaaS, PaaS, and IaaS). To do that, we rely on MDA principle to extent WSDL metamodel through several metamodel transformation. The extension process begins by transforming WSDL metamodel into Cloud-WSDL metamodel, thus we obtain a syntactic Cloud-WSDL extension. After that, as a second step, the produced model is transformed into an OWL-S ontology to offer a semantic Cloud-WSDL extension.},
booktitle = {Web Services – ICWS 2021: 28th International Conference, Held as Part of the Services Conference Federation, SCF 2021, Virtual Event, December 10–14, 2021, Proceedings},
pages = {1–14},
numpages = {14},
keywords = {Cloud computing, OWL-S ontology, WSDL, Generic cloud service description, Cloud service}
}

@article{10.1007/s10844-009-0080-0,
author = {Aouiche, Kamel and Darmont, J\'{e}r\^{o}me},
title = {Data mining-based materialized view and index selection in data warehouses},
year = {2009},
issue_date = {August    2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-009-0080-0},
doi = {10.1007/s10844-009-0080-0},
abstract = {Materialized views and indexes are physical structures for accelerating data access that are casually used in data warehouses. However, these data structures generate some maintenance overhead. They also share the same storage space. Most existing studies about materialized view and index selection consider these structures separately. In this paper, we adopt the opposite stance and couple materialized view and index selection to take view---index interactions into account and achieve efficient storage space sharing. Candidate materialized views and indexes are selected through a data mining process. We also exploit cost models that evaluate the respective benefit of indexing and view materialization, and help select a relevant configuration of indexes and materialized views among the candidates. Experimental results show that our strategy performs better than an independent selection of materialized views and indexes.},
journal = {J. Intell. Inf. Syst.},
month = aug,
pages = {65–93},
numpages = {29},
keywords = {Performance optimization, Materialized views, Indexes, Data warehouses, Data mining, Cost models}
}

@inproceedings{10.1007/978-3-030-04239-4_20,
author = {Dheenadayalan, Kumar and Srinivasaraghavan, Gopalakrishnan and Muralidhara, V. N.},
title = {Dynamic Control of Storage Bandwidth Using Double Deep Recurrent Q-Network},
year = {2018},
isbn = {978-3-030-04238-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-04239-4_20},
doi = {10.1007/978-3-030-04239-4_20},
abstract = {We propose a novel approach to optimize the performance of a large scale physical system by mapping the performance optimization problem into a reinforcement learning framework. A reasonably efficient manual bandwidth control for large storage servers seems to be a difficult task for system administrators, but a dynamic bandwidth control can be effectively learned by a reinforcement learning agent. We adopt a combination of Double Deep Q-Network and a Recurrent Neural Network as our function approximator to identify the extent of bandwidth control (actions) given the state representation of a storage server. Allowing the agent to control the amount of allowable bandwidth to each logical unit within a filer has shown to enhance throughput as-well-as reduce the overload duration of storage servers.},
booktitle = {Neural Information Processing: 25th International Conference, ICONIP 2018, Siem Reap, Cambodia, December 13–16, 2018, Proceedings, Part VII},
pages = {222–234},
numpages = {13},
location = {Siem Reap, Cambodia}
}

@article{10.5555/3122009.3122038,
author = {Bertsimas, Dimitris and Copenhaver, Martin S. and Mazumder, Rahul},
title = {Certifiably optimal low rank factor analysis},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix (Σ) by the sum of a Positive Semidefinite (PSD) low-rank component (θ) and a diagonal matrix (Φ) (with nonnegative entries) subject to Σ - Φ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {907–959},
numpages = {53},
keywords = {semidefinite optimization, rank minimization, nonlinear optimization, global optimization, first order methods, factor analysis, discrete optimization}
}

@inproceedings{10.5555/2449288.2449311,
author = {Khan, Umer and Shin, Hyunjung and Choi, Jong Pill and Kim, Minkoo},
title = {wFDT: weighted fuzzy decision trees for prognosis of breast cancer survivability},
year = {2008},
isbn = {9781920682682},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Accurate and less invasive personalized predictive medicine can spare many breast cancer patients from receiving complex surgical biopsies, unnecessary adjuvant treatments and its expensive medical cost. Cancer prognosis estimates recurrence of disease and predict survival of patient; hence resulting in improved patient management. To develop such knowledge based prognostic system, this paper examines potential hybridization of accuracy and interpretability in the form of Fuzzy Logic and Decision Trees, respectively. Effect of rule weights on fuzzy decision trees is investigated to be an alternative to membership function modifications for performance optimization.Experiments were performed using different combinations of: number of decision tree rules, types of fuzzy membership functions and inference techniques for breast cancer survival analysis. SEER breast cancer data set (1973-2003), the most comprehensible source of information on cancer incidence in United States, is considered. Performance comparisons suggest that predictions of weighted fuzzy decision trees (wFDT) are more accurate and balanced, than independently applied crisp decision tree classifiers; moreover it has a potential to adapt for significant performance enhancement.},
booktitle = {Proceedings of the 7th Australasian Data Mining Conference - Volume 87},
pages = {141–152},
numpages = {12},
keywords = {prognosis, membership functions, knowledge based, interpretability, inference, hybridization, crisp and fuzzy, accuracy},
location = {Glenelg, Australia},
series = {AusDM '08}
}

@inproceedings{10.1145/3337821.3337908,
author = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
title = {Massively Parallel Automated Software Tuning},
year = {2019},
isbn = {9781450362955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337821.3337908},
doi = {10.1145/3337821.3337908},
abstract = {This article presents an implementation of a distributed autotuning engine developed as part of the Bench-testing OpenN Software Autotuning Infrastructure project. The system is geared towards performance optimization of computational kernels for graphics processing units, and allows for the deployment of vast autotuning sweeps to massively parallel machines. The software implements dynamic work scheduling to distributed-memory resources and takes advantage of multithreading for parallel compilation and dispatches kernel launches to multiple accelerators. This paper lays out the main design principles of the system and discusses the basic mechanics of the initial implementation. Preliminary performance results are presented, encountered challenges are discussed, and the future directions are outlined.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
articleno = {92},
numpages = {10},
keywords = {graphics processing unit, automated software tuning},
location = {Kyoto, Japan},
series = {ICPP '19}
}

@article{10.1016/j.specom.2017.04.002,
title = {Fourier model based features for analysis and classification of out-of-breath speech},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.04.002},
doi = {10.1016/j.specom.2017.04.002},
abstract = {A new stressed speech database, named out-of-breath speech (OBS) database, is created, which contains three classes, out-of-breath speech, low out-of-breath speech and normal speech.Four features are proposed using mutual information (MI) on the Fourier parameters for analysis and classification of different classes of OBS database.For multi-class classification, support vector machine (SVM) classifier is used with binary cascade approach.Recognition results show that the proposed features have higher potential to classify the out-of-breath speech, compared to the breathiness, MFCC and TEO-CB-Auto-Env features. This paper presents a new method of feature extraction using Fourier model for analysis of out-of-breath speech. The proposed feature is evaluated using mutual information (MI) on the difference and ratio values of the Fourier parameters, amplitude and frequency. The difference and ratio are calculated between two contiguous values of the Fourier parameters. To analyze the out-of-breath speech, a new stressed speech database, named out-of-breath speech (OBS) database, is created. The database contains three classes of speech, out-of-breath speech, low out-of-breath speech and normal speech. The effectiveness of the proposed features is evaluated with the statistical analysis. The proposed features not only differentiate the normal speech and the out-of-breath speech, but also can discriminate different breath emission levels of speech. Hidden Markov model (HMM) and support vector machine (SVM) are used to evaluate the performance of the proposed features using the OBS database. For multi-class classification problem, SVM classifier is used with binary cascade approach. The performance of the proposed features is compared with the breathiness feature, the mel frequency cepstral coefficient (MFCC) feature and the Teager energy operator (TEO) based critical band TEO autocorrelation envelope (TEO-CB-Auto-Env) feature. The proposed feature outperforms the breathiness feature, the MFCC feature and the TEO-CB-Auto-Env feature.},
journal = {Speech Commun.},
month = jun,
pages = {1–14},
numpages = {14}
}

@inproceedings{10.1145/3319619.3326820,
author = {Conrady, Simon and Manuel, Manu and Kreddig, Arne and Stechele, Walter},
title = {LCS-based automatic configuration of approximate computing parameters for FPGA system designs},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326820},
doi = {10.1145/3319619.3326820},
abstract = {In application domains like data analysis or image processing, ever-increasing performance demands push the capabilities of computational systems to their limits. With technology scaling plateauing out, engineers are forced to rethink their approach to system design. The research field of approximate computing provides a new design paradigm which trades off accuracy against computational resources. In a complex system, multiple approximation methods can be combined to maximize the resulting benefits, but because of error propagation in the system, doing this in a controlled manner is challenging. To solve this problem, we propose to use concepts developed in the field of evolutionary machine learning to optimize approximation parameters, focusing on systems implemented on FPGA hardware. Our approach uses the rules of a learning classifier system to adjust approximation parameters. The resulting effects on both the application quality and resource usage are estimated on-the-fly and fed back to the rules with every fitness update, allowing the system to be carefully tuned to specific design goals. We illustrate the application of the proposed system to a real-world image processing problem and highlight some practical implications. As this is work in progress, we outline remaining open questions and future directions for our research.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1271–1279},
numpages = {9},
keywords = {learning classifier system, approximate computing},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1007/978-3-031-08999-2_37,
author = {Singh, Har Shwinder},
title = {Brain Tumor Segmentation Using Attention Activated U-Net with Positive Mining},
year = {2021},
isbn = {978-3-031-08998-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08999-2_37},
doi = {10.1007/978-3-031-08999-2_37},
abstract = {This paper proposes a Deeply Supervised Attention U-Net Deep Learning network with a novel image mining augmentation method to segment brain tumors in MR images. The network was trained on the 3D segmentation task of the BraTS2021 Challenge Task 1. The Attention U-Net model improves upon the original U-Net by increasing focus on relevant feature maps, increasing training efficiency and increasing model performance. Notably, a novel data augmentation technique termed Positive Mining was applied. This technique crops out randomly scaled, positively labelled training samples and adds them to the training pipeline. This can effectively increase the discriminative ability of the Network to identify a tumor and use tumor feature-specific attention maps. The metrics used to train and validate the network were the Dice coefficient and the Hausdorff metric. The best performance on the online final dataset with the aforementioned network and augmentation technique was: Dice Scores of 0.858, 0.869 and 0.913 and Hausdorff Distance of 12.7, 16.9 and 5.43 for the Enhancing Tumor (ET), Tumor Core (TC) and Whole Tumor (WT).},
booktitle = {Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021, Held in Conjunction with MICCAI 2021, Virtual Event, September 27, 2021, Revised Selected Papers, Part I},
pages = {431–440},
numpages = {10},
keywords = {Positive Mining, Brain tumor segmentation, Attention U-Net}
}

@inproceedings{10.1145/3313831.3376560,
author = {Gorkovenko, Katerina and Burnett, Daniel J. and Thorp, James K. and Richards, Daniel and Murray-Rust, Dave},
title = {Exploring The Future of Data-Driven Product Design},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376560},
doi = {10.1145/3313831.3376560},
abstract = {Connected devices present new opportunities to advance design through data collection in the wild, similar to the way digital services evolve through analytics. However, it is still unclear how live data transmitted by connected devices informs the design of these products, going beyond performance optimisation to support creative practices. Design can be enriched by data captured by connected devices, from usage logs to environmental sensors, and data about the devices and people around them. Through a series of workshops, this paper contributes industry and academia perspectives on the future of data-driven product design. We highlight HCI challenges, issues and implications, including sensemaking and the generation of design insight. We further challenge current notions of data-driven design and envision ways in which future HCI research can develop ways to work with data in the design process in a connected, rich, human manner.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {data-driven design, design research, human-centred design, in the wild, iot, smart devices},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1016/j.neucom.2015.04.087,
author = {Liu, Weifeng and Li, Yang and Tao, Dapeng and Wang, Yanjiang},
title = {A general framework for co-training and its applications},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.04.087},
doi = {10.1016/j.neucom.2015.04.087},
abstract = {Co-training is one of the major semi-supervised learning paradigms in which two classifiers are alternately trained on two distinct views and they teach each other by adding the predictions of unlabeled data to the training set of the other view. Co-training can achieve promising performance, especially when there is only a small number of labeled data. Hence, co-training has received considerable attention, and many variant co-training algorithms have been developed. It is essential and informative to provide a systematic framework for a better understanding of the common properties and differences in these algorithms. In this paper, we propose a general framework for co-training according to the diverse learners constructed in co-training. Specifically, we provide three types of co-training implementations, including co-training on multiple views, co-training on multiple classifiers, and co-training on multiple manifolds. Finally, comprehensive experiments of different methods are conducted on the UCF-iPhone dataset for human action recognition and the USAA dataset for social activity recognition. The experimental results demonstrate the effectiveness of the proposed solutions.},
journal = {Neurocomput.},
month = nov,
pages = {112–121},
numpages = {10},
keywords = {Social activity recognition, Semi-supervised learning, Multi-view, Human action recognition, Co-training}
}

@article{10.1109/TCBB.2021.3109557,
author = {Chen, Eric and Chu, Justin and Zhang, Jessica and Warren, Ren\'{e} L. and Birol, Inanc},
title = {GapPredict – A Language Model for Resolving Gaps in Draft Genome Assemblies},
year = {2021},
issue_date = {Nov.-Dec. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3109557},
doi = {10.1109/TCBB.2021.3109557},
abstract = {Short-read DNA sequencing instruments can yield over 10&lt;sup&gt;12&lt;/sup&gt; bases per run, typically composed of reads 150 bases long. Despite this high throughput, &lt;italic&gt;de novo&lt;/italic&gt; assembly algorithms have difficulty reconstructing contiguous genome sequences using short reads due to both repetitive and difficult-to-sequence regions in these genomes. Some of the short read assembly challenges are mitigated by scaffolding assembled sequences using paired-end reads. However, unresolved sequences in these scaffolds appear as “gaps”. Here, we introduce GapPredict – An implementation of a proof of concept that uses a character-level language model to predict unresolved nucleotides in scaffold gaps. We benchmarked GapPredict against the state-of-the-art gap-filling tool Sealer, and observed that the former can fill 65.6% of the sampled gaps that were left unfilled by the latter with high similarity to the reference genome, demonstrating the practical utility of deep learning approaches to the gap-filling problem in genome assembly.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = sep,
pages = {2802–2808},
numpages = {7}
}

@article{10.1016/j.artmed.2018.10.006,
author = {Delgado, Fernando M. and G\'{o}mez-Vela, Francisco},
title = {Computational methods for Gene Regulatory Networks reconstruction and analysis: A review},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {95},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2018.10.006},
doi = {10.1016/j.artmed.2018.10.006},
journal = {Artif. Intell. Med.},
month = apr,
pages = {133–145},
numpages = {13},
keywords = {Gene Network inference, Gene Regulatory Network, Networks validation, Systems biology, Gene Network}
}

@article{10.5555/3122009.3176858,
author = {Fenn, Shannon and Moscato, Pablo},
title = {Target curricula via selection of minimum feature sets: a case study in Boolean networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4070–4095},
numpages = {26},
keywords = {target curriculum, multi-label classification, k-feature Set, Boolean betworks}
}

@article{10.5555/3122009.3242055,
author = {Patrascu, Andrei and Necoara, Ion},
title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k1/2), where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7204–7245},
numpages = {42},
keywords = {stochastic proximal point, stochastic convex optimization, rates of convergence, nonasymptotic convergence analysis, intersection of convex constraints}
}

@article{10.1109/TNET.2021.3085031,
author = {Yu, Tianqi and Wang, Xianbin and Hu, Jianling},
title = {A Fast Hierarchical Physical Topology Update Scheme for Edge-Cloud Collaborative IoT Systems},
year = {2021},
issue_date = {Oct. 2021},
publisher = {IEEE Press},
volume = {29},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3085031},
doi = {10.1109/TNET.2021.3085031},
abstract = {The awareness of physical network topology in a large-scale Internet of Things (IoT) system is critical to enable location-based service provisioning and performance optimization. However, due to the dynamics and complexity of IoT networks, it is usually very difficult to discover and update the physical topology of the large-scale IoT systems in real-time. Considering the stringent latency requirements in IoT systems, while the initial processing time for topology discovery can be tolerated, latency due to real-time topology update constitutes an even higher level of challenge. In this paper, a novel fast hierarchical topology update scheme is proposed for the large-scale IoT systems enabled by using the edge-cloud collaborative architecture. Specifically, an event-driven neighbor update algorithm, termed as TriggerOn, is firstly developed to update the local neighbor table of the end devices when device association or disassociation occurs. Based on the updated neighbor tables, the physical topology update of the subnet is conducted at the coordinated edge device, where a hybrid multidimensional scaling (MDS) based 3D localization algorithm is developed to locate the newly associated devices. Simulation results have indicated that as compared to the benchmark methods, the neighbor discovery latency has been reduced dramatically, and the 3D localization accuracy has been improved. Furthermore, the overall latency incurred by the proposed hierarchical physical topology update scheme is significantly lower than the distributed consensus-based update scheme, especially for the large-scale IoT subnets.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {2254–2266},
numpages = {13}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {deep learning, constraint solving, configurable systems, AutoML},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.jbi.2008.12.012,
author = {Saha, Sujan Kumar and Sarkar, Sudeshna and Mitra, Pabitra},
title = {Feature selection techniques for maximum entropy based biomedical named entity recognition},
year = {2009},
issue_date = {October, 2009},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {42},
number = {5},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2008.12.012},
doi = {10.1016/j.jbi.2008.12.012},
abstract = {Named entity recognition is an extremely important and fundamental task of biomedical text mining. Biomedical named entities include mentions of proteins, genes, DNA, RNA, etc which often have complex structures, but it is challenging to identify and classify such entities. Machine learning methods like CRF, MEMM and SVM have been widely used for learning to recognize such entities from an annotated corpus. The identification of appropriate feature templates and the selection of the important feature values play a very important role in the success of these methods. In this paper, we provide a study on word clustering and selection based feature reduction approaches for named entity recognition using a maximum entropy classifier. The identification and selection of features are largely done automatically without using domain knowledge. The performance of the system is found to be superior to existing systems which do not use domain knowledge.},
journal = {J. of Biomedical Informatics},
month = oct,
pages = {905–911},
numpages = {7},
keywords = {Maximum entropy classifier, Machine learning, Feature selection, Feature reduction, Biomedical named entity recognition}
}

@article{10.1007/s11042-018-6007-4,
author = {Liu, Hui and Wang, Beilei and Yan, Fusheng and Song, Jie},
title = {An optimized phase-shifting algorithm for depth image acquisition},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {5},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6007-4},
doi = {10.1007/s11042-018-6007-4},
abstract = {As developing science and technology, traditional two-dimensional computer vision cannot meet the people's needs for the three-dimensional recognition, and the depth information of objects are required by more and more applications. Recently, structured light has become one of the core techniques of depth acquisition. The main idea of the approach is first projecting pre-designed pattern onto objects, then capturing an image and processing further. In a structured light system, the phase-shifting algorithm, which is a depth acquiring algorithm for sinusoidal pattern, is discussed in this paper, and is argued that its performance weakness for applying in the real time environments. Then we propose the performance optimization on its phase wrapping step and phase unwrapping step, respectively. Finally, we compare acquiring results and advantages as well as disadvantages of them by experiments results. Experiments show that the optimized phase-shifting algorithm is 4.6 faster than the original one with the ignorable errors.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {5367–5380},
numpages = {14},
keywords = {Structured light, Phase shifting algorithm, Optimization, Depth acquiring}
}

@inproceedings{10.5555/3326943.3327112,
author = {Song, Guocong and Chai, Wei},
title = {Collaborative learning for deep neural networks},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation rescaling aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1837–1846},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.5555/1158337.1158678,
author = {Czarnecki, Krzysztof and Peter Kim, Chang Hwan and Kalleberg, Karl Trygve},
title = {Feature Models are Views on Ontologies},
year = {2006},
isbn = {0769525997},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Feature modeling has been proposed as an approach for describing variable requirements for software product lines. In this paper, we explore the relationship between feature models and ontologies. First, we examine how previous extensions to basic feature modeling move it closer to richer formalisms for specifying ontologies such as MOF and OWL. Then, we explore the idea of feature models as views on ontologies. Based on that idea, we propose two approaches for the combined use of feature models and ontologies: view derivation and view integration. Finally, we give some ideas about tool support for these approaches.},
booktitle = {Proceedings of the 10th International on Software Product Line Conference},
pages = {41–51},
numpages = {11},
series = {SPLC '06}
}

@inproceedings{10.5555/3016100.3016199,
author = {Wang, Jim Jing-Yan and Tsang, Ivor Wai-Hung and Gao, Xin},
title = {Optimizing multivariate performance measures from multi-view data},
year = {2016},
publisher = {AAAI Press},
abstract = {To date, many machine learning applications have multiple views of features, and different applications require specific multivariate performance measures, such as the F-score for retrieval. However, existing multivariate performance measure optimization methods are limited to single-view data, while traditional multi-view learning methods cannot optimize multivariate performance measures directly. To fill this gap, in this paper, we propose the problem of optimizing multivariate performance measures from multi-view data, and an effective method to solve it. We propose to learn linear discriminant functions for different views, and combine them to construct an overall multivariate mapping function for multi-view data. To learn the parameters of the linear discriminant functions of different views to optimize a given multivariate performance measure, we formulate an optimization problem. In this problem, we propose to minimize the complexity of the linear discriminant function of each view, promote the consistency of the responses of different views over the same data points, and minimize the upper boundary of the corresponding loss of a given multivariate performance measure. To optimize this problem, we develop an iterative cutting-plane algorithm. Experiments on four benchmark data sets show that it not only outperforms traditional single-view based multivariate performance optimization methods, but also achieves better results than ordinary multi-view learning methods.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {2152–2158},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1016/j.patcog.2013.05.027,
author = {Impedovo, Sebastiano},
title = {More than twenty years of advancements on Frontiers in handwriting recognition},
year = {2014},
issue_date = {March, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {3},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2013.05.027},
doi = {10.1016/j.patcog.2013.05.027},
abstract = {In this paper the results of the research in handwriting/handwritten character recognition in about the last quarter of a century are reported, illustrating the results presented during the International Workshop on Frontiers in Handwriting Recognition (IWFHR) and the International Conference on Frontiers in Handwriting Recognition (ICFHR). The most relevant scientific results achieved in the IWFHR and the ICFHR are presented.Emerging research trends in handwriting recognition are identified.Contributions which are not easily retrievable are examined.},
journal = {Pattern Recogn.},
month = mar,
pages = {916–928},
numpages = {13},
keywords = {Systems for PR, Pattern recognition, Handwriting recognition}
}

@article{10.1007/s00180-012-0381-6,
author = {Schmidt, Miriam and Palm, G\"{u}nther and Schwenker, Friedhelm},
title = {Spectral graph features for the classification of graphs and graph sequences},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1–2},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-012-0381-6},
doi = {10.1007/s00180-012-0381-6},
abstract = {In this paper, the classification power of the eigenvalues of six graph-associated matrices is investigated. Each matrix contains a certain type of geometric/ spatial information, which may be important for the classification process. The performances of the different feature types is evaluated on two data sets: first a benchmark data set for optical character recognition, where the extracted eigenvalues were utilized as feature vectors for multi-class classification using support vector machines. Classification results are presented for all six feature types, as well as for classifier combinations at decision level. For the decision level combination, probabilistic output support vector machines have been applied, with a performance up to 92.4 %. To investigate the power of the spectra for time dependent tasks, too, a second data set was investigated, consisting of human activities in video streams. To model the time dependency, hidden Markov models were utilized and the classification rate reached 98.3 %.},
journal = {Comput. Stat.},
month = feb,
pages = {65–80},
numpages = {16},
keywords = {Spectrum, Optical character recognition, Human activity recognition, Graph-associated matrices, Graph classification}
}

@inproceedings{10.1145/3338286.3344387,
author = {Lee, Hao-Ping and Dingler, Tilman and Lin, Chih-Heng and Chen, Kuan-Yin and Chung, Yu-Lin and Chen, Chia-Yu and Chang, Yung-Ju},
title = {Predicting Smartphone Users' General Responsiveness to IM Contacts Based on IM Behavior},
year = {2019},
isbn = {9781450368254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338286.3344387},
doi = {10.1145/3338286.3344387},
abstract = {History of conversations through instant messaging (IM) contains abundant information about the communication patterns of the dyad, including conversation partners' mutual responsiveness to messages. We have, however, not seen many examinations of using such information in modeling mobile users' responsiveness in IM communication. In this paper, we present an in-the-wild study, in which we leverage participants' IM messaging logs to build models predicting their general responsiveness. Our models based on data from 33 IM user achieved an accuracy of up to 71% (AUROC). In particular, we show that 90-day IM-communication patterns, in general, outperformed their 14-day equivalent in our prediction models, indicating better coherence between long-term IM patterns with their general communication experience.},
booktitle = {Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {40},
numpages = {6},
keywords = {mobile receptivity, machine learning, Mobile notifications, ESM},
location = {Taipei, Taiwan},
series = {MobileHCI '19}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Systematic literature review, Software product lines, Requirements engineering}
}

@article{10.1016/j.neucom.2018.11.060,
author = {Li, Shaoyong and Tang, Chang and Liu, Xinwang and Liu, Yaping and Chen, Jiajia},
title = {Dual graph regularized compact feature representation for unsupervised feature selection},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.060},
doi = {10.1016/j.neucom.2018.11.060},
journal = {Neurocomput.},
month = feb,
pages = {77–96},
numpages = {20},
keywords = {Feature representation, Local geometrical structure preservation, Dictionary learning, Unsupervised feature selection}
}

@article{10.1145/3442696,
author = {Makrani, Hosein Mohamamdi and Sayadi, Hossein and Nazari, Najmeh and Dinakarrao, Sai Mnoj Pudukotai and Sasan, Avesta and Mohsenin, Tinoosh and Rafatirad, Setareh and Homayoun, Houman},
title = {Adaptive Performance Modeling of Data-intensive Workloads for Resource Provisioning in Virtualized Environment},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3442696},
doi = {10.1145/3442696},
abstract = {The processing of data-intensive workloads is a challenging and time-consuming task that often requires massive infrastructure to ensure fast data analysis. The cloud platform is the most popular and powerful scale-out infrastructure to perform big data analytics and eliminate the need to maintain expensive and high-end computing resources at the user side. The performance and the cost of such infrastructure depend on the overall server configuration, such as processor, memory, network, and storage configurations. In addition to the cost of owning or maintaining the hardware, the heterogeneity in the server configuration further expands the selection space, leading to non-convergence. The challenge is further exacerbated by the dependency of the application’s performance on the underlying hardware. Despite an increasing interest in resource provisioning, few works have been done to develop accurate and practical models to proactively predict the performance of data-intensive applications corresponding to the server configuration and provision a cost-optimal configuration online.In this work, through a comprehensive real-system empirical analysis of performance, we address these challenges by introducing ProMLB: a proactive machine-learning-based methodology for resource provisioning. We first characterize diverse types of data-intensive workloads across different types of server architectures. The characterization aids in accurately capture applications’ behavior and train a model for prediction of their performance.Then, ProMLB builds a set of cross-platform performance models for each application. Based on the developed predictive model, ProMLB uses an optimization technique to distinguish close-to-optimal configuration to minimize the product of execution time and cost. Compared to the oracle scheduler, ProMLB achieves 91% accuracy in terms of application-resource matching. On average, ProMLB improves the performance and resource utilization by 42.6% and 41.1%, respectively, compared to baseline scheduler. Moreover, ProMLB improves the performance per cost by 2.5\texttimes{} on average.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = mar,
articleno = {18},
numpages = {24},
keywords = {optimization, cloud, big data, Performance}
}

@inproceedings{10.1007/978-3-030-37188-3_16,
author = {Basak, Sanchita and Sun, Fangzhou and Sengupta, Saptarshi and Dubey, Abhishek},
title = {Data-Driven Optimization of Public Transit Schedule},
year = {2019},
isbn = {978-3-030-37187-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-37188-3_16},
doi = {10.1007/978-3-030-37188-3_16},
abstract = {Bus transit systems are the backbone of public transportation in the United States. An important indicator of the quality of service in such infrastructures is on-time performance at stops, with published transit schedules playing an integral role governing the level of success of the service. However there are relatively few optimization architectures leveraging stochastic search that focus on optimizing bus timetables with the objective of maximizing probability of bus arrivals at timepoints with delays within desired on-time ranges. In addition to this, there is a lack of substantial research considering monthly and seasonal variations of delay patterns integrated with such optimization strategies. To address these, this paper makes the following contributions to the corpus of studies on transit on-time performance optimization: (a) an unsupervised clustering mechanism is presented which groups months with similar seasonal delay patterns, (b) the problem is formulated as a single-objective optimization task and a greedy algorithm, a genetic algorithm (GA) as well as a particle swarm optimization (PSO) algorithm are employed to solve it, (c) a detailed discussion on empirical results comparing the algorithms are provided and sensitivity analysis on hyper-parameters of the heuristics are presented along with execution times, which will help practitioners looking at similar problems. The analyses conducted are insightful in the local context of improving public transit scheduling in the Nashville metro region as well as informative from a global perspective as an elaborate case study which builds upon the growing corpus of empirical studies using nature-inspired approaches to transit schedule optimization.},
booktitle = {Big Data Analytics: 7th International Conference, BDA 2019, Ahmedabad, India, December 17–20, 2019, Proceedings},
pages = {265–284},
numpages = {20},
keywords = {Timetable optimization, Genetic algorithm, Particle swarm optimization, Sensitivity analysis, Scheduling},
location = {Ahmedabad, India}
}

@inproceedings{10.1145/2970276.2975938,
author = {Babur, \"{O}nder},
title = {Statistical analysis of large sets of models},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2975938},
doi = {10.1145/2970276.2975938},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {vector space model, model comparison, clustering, Model-driven engineering},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3358331.3358359,
author = {Luo, Yaqing and Zhang, Jin and Luo, Gaoyong},
title = {Unmanned Plant Control and Optimisation by Real-time Deep Neural Networks for Power Saving},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358359},
doi = {10.1145/3358331.3358359},
abstract = {Compressed air is essential to a wide range of industries and highly specialised applications where it is a particularly critical resource, such as medical gas systems. However, current control system of unmanned medical compressed air plant mostly using fixed speed compressors is operated inefficiently and without optimisation in terms of power saving. This paper investigates the complexity of unmanned plant control and proposes performance optimisation by an intelligent compressed air system with the integration of advanced communication technology and artificial intelligence (AI), where a new energy-efficient and reliable operation of unmanned plant is developed and implemented by applying intelligent control to provide optimum performance. A deep neural network (DNN) using multilayer perceptron (MLP) model is thus derived and used to train and identify network coefficients for minimizing energy consumption. Experimental results demonstrate that the intelligent control and optimisation by real-time deep neural network can achieve maximum power efficiency leading to a satisfactory solution to unmanned plant.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {28},
numpages = {6},
keywords = {unmanned plant control, optimisation by real-time AI, maximum power efficiency, deep neural networks},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/1553374.1553468,
author = {de Mesmay, Fr\'{e}d\'{e}ric and Rimmel, Arpad and Voronenko, Yevgen and P\"{u}schel, Markus},
title = {Bandit-based optimization on graphs with application to library performance tuning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553468},
doi = {10.1145/1553374.1553468},
abstract = {The problem of choosing fast implementations for a class of recursive algorithms such as the fast Fourier transforms can be formulated as an optimization problem over the language generated by a suitably defined grammar. We propose a novel algorithm that solves this problem by reducing it to maximizing an objective function over the sinks of a directed acyclic graph. This algorithm valuates nodes using Monte-Carlo and grows a subgraph in the most promising directions by considering local maximum k-armed bandits. When used inside an adaptive linear transform library, it cuts down the search time by an order of magnitude compared to the existing algorithm. In some cases, the performance of the implementations found is also increased by up to 10% which is of considerable practical importance since it consequently improves the performance of all applications using the library.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {729–736},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@article{10.1007/s11227-020-03182-5,
author = {Yang, Chao-Tung and Chen, Yuan-An and Chan, Yu-Wei and Lee, Chia-Lin and Tsan, Yu-Tse and Chan, Wei-Cheng and Liu, Po-Yu},
title = {Influenza-like illness prediction using a long short-term memory deep learning model with multiple open data sources},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {12},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03182-5},
doi = {10.1007/s11227-020-03182-5},
abstract = {The influenza problem has always been an important global issue. It not only affects people’s health problems but is also an essential topic of governments and health care facilities. Early prediction and response is the most effective control method for flu epidemics. It can effectively predict the influenza-like illness morbidity, and provide reliable information to the relevant facilities. For social facilities, it is possible to strengthen epidemic prevention and care for highly sick groups. It can also be used as a reminder for the public. This study collects information on the influenza-like illness emergency department visits to the Taiwan Centers for Disease Control, and the PM2.5 open-source data from the Taiwan Environmental Protection Administration's air quality monitoring network. By using deep learning techniques, the relevance of short-term estimates and the outbreak calculation method can be determined. The techniques are published by the WHO to determine whether the influenza-like illness situation is still in a stage of reasonable control. Finally, historical data and future forecasted data are integrated on the web page for visual presentation, to show the actual regional air quality situation and influenza-like illness data and to predict whether there is an outbreak of influenza in the region.},
journal = {J. Supercomput.},
month = dec,
pages = {9303–9329},
numpages = {27},
keywords = {Deep learning, PM2.5, LSTM, Influenza-like illness}
}

@inproceedings{10.1145/2908961.2931693,
author = {L\'{o}pez-L\'{o}pez, V\'{\i}ctor R. and Trujillo, Leonardo and Legrand, Pierrick and Olague, Gustavo},
title = {Genetic Programming: From Design to Improved Implementation},
year = {2016},
isbn = {9781450343237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908961.2931693},
doi = {10.1145/2908961.2931693},
abstract = {Genetic programming (GP) is an evolutionary-based search paradigm that is well suited to automatically solve difficult design problems. The general principles of GP have been used to evolve mathematical functions, models, image operators, programs, and even antennas and lenses. Since GP evolves the syntax and structure of a solution, the evolutionary process can be carried out in one environment and the solution can then be ported to another. However, given the nature of GP it is common that the evolved designs are unorthodox compared to traditional approaches used in the problem domain. Therefore, efficiently porting, improving or optimizing an evolved design might not be a trivial task. In this work we argue that the same GP principles used to evolve the solution can then be used to optimize a particular new implementation of the design, following the Genetic Improvement approach. In particular, this paper presents a case study where evolved image operators are ported from Matlab to OpenCV, and then the source code is optimized an improved using Genetic Improvement of Software for Multiple Objectives (GISMOE). In the example we show that functional behavior is maintained (output image) while improving non-functional properties (computation time). Despite the fact that this first example is a simple case, it clearly illustrates the possibilities of using GP principles in two distinct stages of the software development process, from design to improved implementation.},
booktitle = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion},
pages = {1147–1154},
numpages = {8},
keywords = {genetic programming, genetic improvement, computer vision},
location = {Denver, Colorado, USA},
series = {GECCO '16 Companion}
}

@inproceedings{10.1109/GreenCom.2011.43,
author = {Rao, Subrahmanya VRK and Ramesh, S. and Muthuraj, V. Arun and Sundararaman, Karthik and Parthasarathi, Jinka},
title = {CGLive - A Real Time Power Monitoring Solution for Enterprises},
year = {2011},
isbn = {9780769544663},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/GreenCom.2011.43},
doi = {10.1109/GreenCom.2011.43},
abstract = {CGLive is a real time energy monitoring software tool that could be used to monitor and reduce energy consumption of PC's in a enterprise network environment. Working of CGLive has been tested in a lab environment and was found to be successful. Dashboard of CGLive can show current status of a work station, energy consumed by it and also the Process which consume higher percentage of CPU. CGLive can issue commands towards ensuring shutdown, logon, logoff and hibernate a particular machine.},
booktitle = {Proceedings of the 2011 IEEE/ACM International Conference on Green Computing and Communications},
pages = {212–215},
numpages = {4},
keywords = {Sensor Networks, Remote monitoring, Performance Optimization, M2M technology, Intelligent Green IT Management, Green IT, Energy audit},
series = {GREENCOM '11}
}

@article{10.1504/ijcat.2020.103917,
author = {Kumar, Nand and Gaidhane, Vilas H. and Mittal, Ravi Kant},
title = {Cloud-based electricity consumption analysis using neural network},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {62},
number = {1},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.103917},
doi = {10.1504/ijcat.2020.103917},
abstract = {In recent years, optimisation of the resource usages is necessary to analyse and understand the energy consumption pattern. In the literature, analysis has been carried out using the algorithms, which needs many assumptions, and meeting all the assumptions in practice is a very difficult task. However, there are other methods available to analyse and understand the energy consumption. In this paper, an efficient approach for energy consumption pattern analysis is proposed. It is based on the Levenberg-Marquardt algorithm-based Neural Network (LMNN) and clustering technique. The energy consumption data is collected from the educational institute building using smart system. The various experimentations are carried out on the collected real time database. The experimental results illustrate that the proposed approach is effective and computationally efficient for consumption pattern classification. The performance of the presented approach is found superior to existing clustering approaches.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {45–56},
numpages = {11},
keywords = {ROC curve, confusion matrix, classification, neural network, Levenberg-Marquardt algorithm, educational institute building}
}

@inproceedings{10.1145/3447545.3451175,
author = {Domaschka, J\"{o}rg and Leznik, Mark and Seybold, Daniel and Eismann, Simon and Grohmann, Johannes and Kounev, Samuel},
title = {Buzzy: Towards Realistic DBMS Benchmarking via Tailored, Representative, Synthetic Workloads: Vision Paper},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451175},
doi = {10.1145/3447545.3451175},
abstract = {Distributed Database Management Systems~(DBMS) are a crucial component of modern IT applications. Understanding their performance and non-functional properties is of paramount importance. Yet, benchmarking distributed DBMS has proven to be difficult in practice. Either, a realistic workload is often mapped to a synthetic workload without knowing if this mapping is correct or available workload traces are replayed. While the latter approach provides more realistic results, real-world traces are hard to obtain and their scope is limited in time scale and variance.We propose collecting real-world traces and then applying data generation techniques to synthesize similar realistic traces based on it. Based in this approach, we can obtain workloads for benchmarking, exhibit variability with respect to different aspects of interest while still being similar to the original traces. Varying generation parameters, we are able to support benchmarking what-if scenarios with hypothetical workloads and introduced anomalies.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {175–178},
numpages = {4},
keywords = {measurement-based performance evaluation, database management systems, data synthesis},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1016/j.dss.2012.10.005,
author = {Van Valkenhoef, Gert and Tervonen, Tommi and Zwinkels, Tijs and De Brock, Bert and Hillege, Hans},
title = {ADDIS: A decision support system for evidence-based medicine},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {2},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.10.005},
doi = {10.1016/j.dss.2012.10.005},
abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results.},
journal = {Decis. Support Syst.},
month = may,
pages = {459–475},
numpages = {17},
keywords = {ePRO, eLab, eCRF, caBIG, WHO, UMLS, TDM, SmPC, SPL, SNOMEDCT, SMAA, SEND, SDTM, RIM, QRD, PhRMA, PRM, PMDA, PIM, OWL, ODM, OCRe, OBX, NIHUS, NDA, NCI, MedDRA, MeSH, MCDA, LAB, JAMA, ICTRP, ICMJE, ICD, HSDB, HL7, GUI, GCP, FDAAA, FDA, Evidence-based medicine, Evidence synthesis, EPAR, EMA, EHR, EDC, EBM, EAV, Decision analysis, Data model, DSS, DOI, DIS, DED, DB, Clinical trial, CTMS, CTIS, CRO, CRF, CPOE, CHMP, CDMS, CDISC, CDASH, BRIDG, ATC, ANSI, AMIA, ADaM, ADR, ADE}
}

@article{10.1016/j.websem.2011.01.001,
author = {Lehmann, Jens and Auer, S\"{o}ren and B\"{u}hmann, Lorenz and Tramp, Sebastian},
title = {Class expression learning for ontology engineering},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {9},
number = {1},
issn = {1570-8268},
url = {https://doi.org/10.1016/j.websem.2011.01.001},
doi = {10.1016/j.websem.2011.01.001},
abstract = {Abstract: While the number of knowledge bases in the Semantic Web increases, the maintenance and creation of ontology schemata still remain a challenge. In particular creating class expressions constitutes one of the more demanding aspects of ontology engineering. In this article we describe how to adapt a semi-automatic method for learning OWL class expressions to the ontology engineering use case. Specifically, we describe how to extend an existing learning algorithm for the class learning problem. We perform rigorous performance optimization of the underlying algorithms for providing instant suggestions to the user. We also present two plugins, which use the algorithm, for the popular Protege and OntoWiki ontology editors and provide a preliminary evaluation on real ontologies.},
journal = {Web Semant.},
month = mar,
pages = {71–81},
numpages = {11},
keywords = {Supervised machine learning, Ontology engineering, Ontology editor plugins, OWL, Heuristics, Concept learning}
}

@inproceedings{10.5555/3495724.3496445,
author = {Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff A.},
title = {Curriculum learning by dynamic instance hardness},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A good teacher can adjust a curriculum based on students' learning history. By analogy, in this paper, we study the dynamics of a deep neural network's (DNN) performance on individual samples during its learning process. The observed properties allow us to develop an adaptive curriculum that leads to faster learning of more accurate models. We introduce dynamic instance hardness (DIH), the exponential moving average of a sample's instantaneous hardness (e.g., a loss, or a change in output) over the training history. A low DIH indicates that a model retains knowledge about a sample over time. For DNNs, we find that a sample's DIH early in training predicts its DIH in later stages. Hence, we can train a model using samples mostly with higher DIH and safely deprioritize those with lower DIH. This motivates a DIH guided curriculum learning (DIHCL) procedure. Compared to existing CL methods: (1) DIH is more stable over time than using only instantaneous hardness, which is noisy due to stochastic training and DNN's non-smoothness; (2) DIHCL is computationally inexpensive since it uses only a byproduct of back-propagation and thus does not require extra inference. On 11 datasets, DIHCL significantly outperforms random mini-batch SGD and recent CL methods in terms of efficiency and final performance.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/2889160.2889207,
author = {Staples, Mark and Zhu, Liming and Grundy, John},
title = {Continuous validation for data analytics systems},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889207},
doi = {10.1145/2889160.2889207},
abstract = {From a future history of 2025: Continuous development is common for build/test (continuous integration) and operations (devOps). This trend continues through the lifecycle, into what we call 'devUsage': continuous usage validation. In addition to ensuring systems meet user needs, organisations continuously validate their legal and ethical use. The rise of end-user programming and multi-sided platforms exacerbate validation challenges. A separate trend is the specialisation of software engineering for technical domains, including data analytics. This domain has specific validation challenges. We must validate the accuracy of statistical models, but also whether they have illegal or unethical biases. Usage needs addressed by machine learning are sometimes not specifiable in the traditional sense, and statistical models are often 'black boxes'. We describe future research to investigate solutions to these devUsage challenges for data analytics systems. We will adapt risk management and governance frameworks previously used for software product qualities, use social network communities for input from aligned stakeholder groups, and perform cross-validation using autonomic experimentation, cyber-physical data streams, and online discursive feedback.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {769–772},
numpages = {4},
keywords = {software validation, machine learning, governance, ethics, devOps, data analytics, continuous development},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.knosys.2015.06.009,
author = {Nilashi, Mehrbakhsh and Zakaria, Rozana and Ibrahim, Othman and Majid, Muhd Zaimi Abd. and Mohamad Zin, Rosli and Chugtai, Muhammad Waseem and Zainal Abidin, Nur Izieadiana and Sahamir, Shaza Rina and Aminu Yakubu, Dodo},
title = {A knowledge-based expert system for assessing the performance level of green buildings},
year = {2015},
issue_date = {September 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.06.009},
doi = {10.1016/j.knosys.2015.06.009},
abstract = {Sustainability has become an important initiative discussed and undertaken, not only by private buildings, but also by public buildings which both dealing with residential, office, commercial as well as hospital. Sustainable building is the practice of designing, constructing, operating, maintaining, and removing buildings in ways that conserve natural resources and reduce pollution. Rating systems provide effective framework for assessing building environmental performance and they measure a building's sustainability by applying a set of criteria organized in different categories. A good Green Building Rating System (GBRS) should cover key indicators reflecting a building's characteristics and keep their performance in balance. This paper proposed a knowledge-based expert system as a tool to assess the performance level of a green building based on assessment factors of green building rating systems. Analytic Hierarchy Process (AHP) and fuzzy logic is adopted in order to develop the knowledge-based expert system. The data for this research collected from the experts in the field via pair-wise and Likert-based questionnaires. Using AHP, the most important parameters of rating systems according to their weights selected to be incorporated in the Fuzzy Inferences System (FIS) of fuzzy logic model. The fuzzy rules (knowledge) discovered from the collected data for FIS to assess the performance level of the green buildings from the Environmental, Social and Economical perspectives denoted as SE2. The outcome of this research is accordingly a performance assessment tool that analyzes the effect of factors in developing the sustainable building.},
journal = {Know.-Based Syst.},
month = sep,
pages = {194–209},
numpages = {16},
keywords = {Rating system, Green building, Fuzzy rule, Fuzzy inferences system, AHP}
}

@article{10.1145/3424239,
author = {Arka, Aqeeb Iqbal and Joardar, Biresh Kumar and Kim, Ryan Gary and Kim, Dae Hyun and Doppa, Janardhan Rao and Pande, Partha Pratim},
title = {HeM3D: Heterogeneous Manycore Architecture Based on Monolithic 3D Vertical Integration},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3424239},
doi = {10.1145/3424239},
abstract = {Heterogeneous manycore architectures are the key to efficiently execute compute- and data-intensive applications. Through-silicon-via (TSV)-based 3D manycore system is a promising solution in this direction as it enables the integration of disparate computing cores on a single system. Recent industry trends show the viability of 3D integration in real products (e.g., Intel Lakefield SoC Architecture, the AMD Radeon R9 Fury X graphics card, and Xilinx Virtex-7 2000T/H580T, etc.). However, the achievable performance of conventional TSV-based 3D systems is ultimately bottlenecked by the horizontal wires (wires in each planar die). Moreover, current TSV 3D architectures suffer from thermal limitations. Hence, TSV-based architectures do not realize the full potential of 3D integration. Monolithic 3D (M3D) integration, a breakthrough technology to achieve “More Moore and More Than Moore,” opens up the possibility of designing cores and associated network routers using multiple layers by utilizing monolithic inter-tier vias (MIVs) and hence, reducing the effective wire length. Compared to TSV-based 3D integrated circuits (ICs), M3D offers the “true” benefits of vertical dimension for system integration: the size of an MIV used in M3D is over 100 \texttimes{} smaller than a TSV. This dramatic reduction in via size and the resulting increase in density opens up numerous opportunities for design optimizations in 3D manycore systems: designers can use up to millions of MIVs for ultra-fine-grained 3D optimization, where individual cores and routers can be spread across multiple tiers for extreme power and performance optimization. In this work, we demonstrate how M3D-enabled vertical core and uncore elements offer significant performance and thermal improvements in manycore heterogeneous architectures compared to its TSV-based counterpart. To overcome the difficult optimization challenges due to the large design space and complex interactions among the heterogeneous components (CPU, GPU, Last Level Cache, etc.) in a M3D-based manycore chip, we leverage novel design-space exploration algorithms to trade off different objectives. The proposed M3D-enabled heterogeneous architecture, called HeM3D, outperforms its state-of-the-art TSV-equivalent counterpart by up to 18.3% in execution time while being up to 19°C cooler.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = feb,
articleno = {16},
numpages = {21},
keywords = {temperature, performance, multi-tier, execution time, NoC, M3D, Heterogeneous manycore}
}

@article{10.1007/s00500-015-2004-y,
author = {Qin, Jindong and Liu, Xinwang and Pedrycz, Witold},
title = {A multiple attribute interval type-2 fuzzy group decision making and its application to supplier selection with extended LINMAP method},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {12},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-2004-y},
doi = {10.1007/s00500-015-2004-y},
abstract = {Supplier selection is a key issue in supply chain management, which directly impacts the manufacturer's performance. The problem can be viewed as a multiple attribute group decision making (MAGDM) that concerns many conflicting evaluation attributes, both being of qualitative and quantitative nature. Due to the increasing complexity and uncertainty of socio-economic environment, some evaluations of attributes are not adequately represented by numerical assessments and type-1 fuzzy sets. In this paper, we develop some linear programming models with the aid of multidimensional analysis of preference (LINMAP) method to solve interval type-2 fuzzy MAGDM problems, in which the information about attribute weights is incompletely known, and all pairwise comparison judgments over alternatives are represented by IT2FSs. First, we introduce a new distance measure based on the centroid interval between the IT2FSs. Then, we construct the linear programming model to determine the interval type-2 fuzzy positive ideal solution (IT2PIS) and corresponding attributes weight vector. Based on it, an extended LINMAP method to solve MAGDM problem under IT2FSs environment is developed. Finally, a supplier selection example is provided to demonstrate the usefulness of the proposed method.},
journal = {Soft Comput.},
month = jun,
pages = {3207–3226},
numpages = {20},
keywords = {Supplier selection, Multiple attribute group decision making (MAGDM), Linear programming techniques for multidimensional analysis of preference (LINMAP) method, Interval type-2 fuzzy sets (IT2FSs)}
}

@article{10.1007/s10922-013-9289-x,
author = {Bashar, Abul and Parr, Gerard and Mcclean, Sally and Scotney, Bryan and Nauck, Detlef},
title = {Application of Bayesian Networks for Autonomic Network Management},
year = {2014},
issue_date = {April     2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9289-x},
doi = {10.1007/s10922-013-9289-x},
abstract = {The ever evolving telecommunication networks in terms of their technology, infrastructure, and supported services have always posed challenges to the network managers to come up with an efficient Network Management System (NMS) for effective network management. The need for automated and efficient management of the current networks, more specifically the Next Generation Network (NGN), is the subject addressed in this research. A detailed description of the management challenges in the context of current networks is presented and then this work enlists the desired features and characteristics of an efficient NMS. It then proposes that there is a need to apply Artificial Intelligence (AI) and Machine Learning (ML) approaches for enhancing and automating the functions of NMS. The first contribution of this work is a comprehensive survey of the AI and ML approaches applied to the domain of NM. The second contribution of this work is that it presents the reasoning and evidence to support the choice of Bayesian Networks (BN) as a viable solution for ML-based NMS. The final contribution of this work is that it proposes and implements three novel NM solutions based on the BN approach, namely BN-based Admission Control (BNAC), BN-based Distributed Admission Control (BNDAC) and BN-based Intelligent Traffic Engineering (BNITE), along with the description of algorithms underpinning the proposed framework.},
journal = {J. Netw. Syst. Manage.},
month = apr,
pages = {174–207},
numpages = {34},
keywords = {Next Generation Networks, Network Management, Machine Learning, Intelligent Traffic Engineering, Data Mining, Call Admission Control, Bayesian Networks, Artificial Intelligence}
}

@article{10.1007/s10009-019-00544-0,
author = {Giantamidis, Georgios and Tripakis, Stavros and Basagiannis, Stylianos},
title = {Learning Moore machines from input–output traces},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-019-00544-0},
doi = {10.1007/s10009-019-00544-0},
abstract = {The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper, we study this problem for finite-state machines with inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input–output traces into an incomplete Moore machine and then completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the fundamental identification in the limit property. We compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. We also carry out a performance comparison against two existing tools (LearnLib and flexfringe). Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {1–29},
numpages = {29},
keywords = {Characteristic sample, Passive learning, Automata learning, Mealy machine, Moore machine, Finite state machine}
}

@article{10.5555/1756006.1859931,
author = {Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
title = {Spectral Regularization Algorithms for Learning Large Incomplete Matrices},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efficient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efficiently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semidefinite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 106 X 106 incomplete matrix with 107 observed entries, and fits a rank-95 approximation to the full Netflix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {2287–2322},
numpages = {36}
}

@inproceedings{10.1007/11908029_95,
author = {Ichihashi, Hidetomo and Honda, Katsuhiro and Notsu, Akira},
title = {Postsupervised hard c-means classifier},
year = {2006},
isbn = {3540476938},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11908029_95},
doi = {10.1007/11908029_95},
abstract = {Miyamoto et al. derived a hard clustering algorithms by defuzzifying a generalized entropy-based fuzzy c-means in which covariance matrices are introduced as decision variables. We apply the hard c-means (HCM) clustering algorithms to a postsupervised classifier to improve resubstitution error rate by choosing best clustering results from local minima of an objective function. Due to the nature of the prototype based classifier, the error rates can easily be improved by increasing the number of clusters with the cost of computer memory and CPU speed. But, with the HCM classifier, the resubstitution error rate along with the data set compression ratio is improved on several benchmark data sets by using a small number of clusters for each class.},
booktitle = {Proceedings of the 5th International Conference on Rough Sets and Current Trends in Computing},
pages = {918–927},
numpages = {10},
location = {Kobe, Japan},
series = {RSCTC'06}
}

@article{10.1016/j.patcog.2011.12.019,
author = {Graves, Daniel and Noppen, Joost and Pedrycz, Witold},
title = {Clustering with proximity knowledge and relational knowledge},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {7},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.12.019},
doi = {10.1016/j.patcog.2011.12.019},
abstract = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes.},
journal = {Pattern Recogn.},
month = jul,
pages = {2633–2644},
numpages = {12},
keywords = {Software requirements, Relational clustering, Proximity, Knowledge representation, Fuzzy clustering}
}

@inproceedings{10.1007/978-3-642-34166-3_71,
author = {Hidaka, Akinori and Kurita, Takio},
title = {Sparse discriminant analysis based on the bayesian posterior probability obtained by L1 regression},
year = {2012},
isbn = {9783642341656},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34166-3_71},
doi = {10.1007/978-3-642-34166-3_71},
abstract = {Recently the kernel discriminant analysis (KDA) has been successfully applied in many applications. However, kernel functions are usually defined a priori and it is not known what the optimum kernel function for nonlinear discriminant analysis is. Otsu derived the optimum nonlinear discriminant analysis (ONDA) by assuming the underlying probabilities similar with the Bayesian decision theory. Kurita derived discriminant kernels function (DKF) as the optimum kernel functions in terms of the discriminant criterion by investigating the optimum discriminant mapping constructed by the ONDA. The derived kernel function is defined by using the Bayesian posterior probabilities. We can define a family of DKFs by changing the estimation method of the Bayesian posterior probabilities. In this paper, we propose a novel discriminant kernel function based on L1-regularized regression, called L1 DKF. L1 DKF is given by using the Bayesian posterior probabilities estimated by L1 regression. Since L1 regression yields a sparse representation for given samples, we can naturally introduce the sparseness into the discriminant kernel function. To introduce the sparseness into LDA, we use L1 DKF as the kernel function of LDA. In experiments, we show sparseness and classification performance of L1 DKF.},
booktitle = {Proceedings of the 2012 Joint IAPR International Conference on Structural, Syntactic, and Statistical Pattern Recognition},
pages = {648–656},
numpages = {9},
location = {Hiroshima, Japan},
series = {SSPR'12/SPR'12}
}

@article{10.1109/TCBB.2012.111,
author = {Ozyurt, I. Burak},
title = {Automatic Identification and Classification of Noun Argument Structures in Biomedical Literature},
year = {2012},
issue_date = {November 2012},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {9},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2012.111},
doi = {10.1109/TCBB.2012.111},
abstract = {The accelerating increase in the biomedical literature makes keeping up with recent advances challenging for researchers thus making automatic extraction and discovery of knowledge from this vast literature a necessity. Building such systems requires automatic detection of lexico-semantic event structures governed by the syntactic and semantic constraints of human languages in sentences of biomedical texts. The lexico-semantic event structures in sentences are centered around the predicates and most semantic role labeling (SRL) approaches focus only on the arguments of verb predicates and neglect argument taking nouns which also convey information in a sentence. In this article, a noun argument structure (NAS) annotated corpus named BioNom and a SRL system to identify and classify these structures is introduced. Also, a genetic algorithm-based feature selection (GAFS) method is introduced and global inference is applied to significantly improve the performance of the NAS Bio SRL system.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = nov,
pages = {1639–1648},
numpages = {10},
keywords = {semantic role labeling, nominalizations, genetic algorithms, biomedical text mining, Training, Syntactics, Support vector machines, Semantics, Optimization, Natural language processing, Genetic algorithms, Biological cells}
}

@article{10.1007/s11227-020-03487-5,
author = {Javanmardi, Abdol Karim and Yaghoubyan, S. Hadi and BagheriFard, Karamollah and Nejatian, Samad and Parvin, Hamid},
title = {An architecture for scheduling with the capability of minimum share to heterogeneous Hadoop systems},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {6},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03487-5},
doi = {10.1007/s11227-020-03487-5},
abstract = {Job scheduling in Hadoop has been thus far investigated in several studies. However, some challenges including minimum share (min-share), heterogeneous cluster, execution time estimation, and scheduling program size facing Hadoop clusters have received less attention. Accordingly, one of the most important algorithms with regard to min-share is that presented by Facebook Inc., i.e., FAIR scheduler, based on its own needs, in which an equal min-share has been considered for users. In this article, an attempt has been made to make the proposed method superior to existing methods through automation and configuration, performance optimization, fairness and data locality. A high-level architectural model is designed. Then a scheduler is defined on this architectural model. The provided scheduler contains four components. Three components schedule jobs and one component distributes the data for each job among the nodes. The given scheduler will be capable of being executed on heterogeneous Hadoop clusters and running jobs in parallel, in which disparate min-shares can be assigned to each job or user. Moreover, an approach is presented for each problem associated with min-share, cluster heterogeneity, execution time estimation, and scheduler program size. These approaches can be also utilized on its own to improve the performance of other scheduling algorithms. The scheduler presented in this paper showed acceptable performance compared with First-In, First-Out (FIFO), and FAIR schedulers.},
journal = {J. Supercomput.},
month = jun,
pages = {5289–5318},
numpages = {30},
keywords = {Heterogeneous clusters, Minimum share, High-level architecture, Hadoop, Scheduling}
}

@article{10.1016/j.comcom.2019.03.003,
author = {Gupta, Lav and Jain, Raj and Erbad, Aiman and Bhamare, Deval},
title = {The P-ART framework for placement of virtual network services in a multi-cloud environment},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {139},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2019.03.003},
doi = {10.1016/j.comcom.2019.03.003},
journal = {Comput. Commun.},
month = may,
pages = {103–122},
numpages = {20},
keywords = {Dynamic placement, Machine learning, Multi-cloud systems, Virtual network function, Service function chain, Network function virtualization, Virtual network services}
}

@inproceedings{10.1007/978-3-030-89363-7_28,
author = {Dai, Huan and Zhang, Yupei and Yun, Yue and Shang, Xuequn},
title = {An Improved Deep Model for Knowledge Tracing and Question-Difficulty Discovery},
year = {2021},
isbn = {978-3-030-89362-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89363-7_28},
doi = {10.1007/978-3-030-89363-7_28},
abstract = {Knowledge Tracing (KT) aims to analyze a student’s acquisition of skills over time by examining the student’s performance on questions of those skills. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, DKT and its variants often lead to oscillation results on a skill’s state may due to it ignoring the skill’s difficulty or the question’s difficulty. As a result, even when a student performs well on a skill, the prediction of that skill’s mastery level decreases instead, and vice versa. This is undesirable and unreasonable because student’s performance is expected to transit gradually over time. In this paper, we propose to learn the knowledge tracing model in a “simple-to-difficult” process, leading to a method of Self-paced Deep Knowledge Tracing (SPDKT). SPDKT learns the difficulty of per question from the student’s responses to optimize the question’s order and smooth the learning process. With mitigating the cause of oscillations, SPDKT has the capability of robustness to the puzzling questions. The experiments on real-world datasets show SPDKT achieves state-of-the-art performance on question response prediction and reaches interesting interpretations in education.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part II},
pages = {362–375},
numpages = {14},
keywords = {Personalized education, Deep learning, Self-paced learning, Knowledge tracing},
location = {Hanoi, Vietnam}
}

@article{10.1007/s10586-015-0484-2,
author = {Peng, Zhiping and Cui, Delong and Zuo, Jinglong and Li, Qirui and Xu, Bo and Lin, Weiwei},
title = {Random task scheduling scheme based on reinforcement learning in cloud computing},
year = {2015},
issue_date = {Dec 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-015-0484-2},
doi = {10.1007/s10586-015-0484-2},
abstract = {Task scheduling is a necessary prerequisite for performance optimization and resource management in the cloud computing system. Focusing on accurate scaled cloud computing environment and efficient task scheduling under resource constraints problems, we introduce fine-grained cloud computing system model and optimization task scheduling scheme in this paper. The system model is comprised of clearly defined separate submodels including task schedule submodel, task execute submodel and task transmission submodel, so that they can be accurately analyzed in the order of processing of user requests. Moreover the submodels are scalable enough to capture the flexibility of the cloud computing paradigm. By analyzing the submodels, where results are repeated to obtain sufficient accuracy, we design a novel task scheduling scheme based on reinforcement learning and queuing theory to optimize task scheduling under the resource constraints, and the state aggregation technologies is employed to accelerate the learning progress. Our results, on the one hand, demonstrate the efficiency of the task scheduling scheme and, on the other hand, reveal the relationship between the arrival rate, server rate, number of VMs and the number of buffer size.},
journal = {Cluster Computing},
month = dec,
pages = {1595–1607},
numpages = {13},
keywords = {Task scheduling, State aggregation, Reinforcement learning, Queuing theory, Cloud computing}
}

@article{10.1016/j.comnet.2006.08.003,
author = {Weiss, M. and Esfandiari, B. and Luo, Y.},
title = {Towards a classification of web service feature interactions},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {51},
number = {2},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2006.08.003},
doi = {10.1016/j.comnet.2006.08.003},
abstract = {The rapid introduction of new web services into a dynamic business environment can lead to undesirable interactions that negatively affect service quality and user satisfaction. In previous work, we have demonstrated how such interactions between web services can be modeled as feature interactions. In this paper, we outline a classification of web service feature interactions. The goals of this classification are to understand the scope of the feature interaction problem in the web services domain, and to propose a benchmark against which to assess the coverage of solutions to this problem. As there is no standard set of web services that one could use as examples, we illustrate the interactions using a fictitious e-commerce scenario.},
journal = {Comput. Netw.},
month = feb,
pages = {359–381},
numpages = {23},
keywords = {Web services, Feature interaction, Classification}
}

@article{10.1016/j.knosys.2013.01.018,
author = {Fern\'{a}Ndez, Alberto and L\'{o}Pez, Victoria and Galar, Mikel and Del Jesus, Mar\'{\i}A Jos\'{e} and Herrera, Francisco},
title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.01.018},
doi = {10.1016/j.knosys.2013.01.018},
abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository.},
journal = {Know.-Based Syst.},
month = apr,
pages = {97–110},
numpages = {14},
keywords = {Preprocessing, Pairwise learning, Multi-classification, Imbalanced data-sets, Cost-sensitive learning}
}

@article{10.1016/j.eswa.2021.115554,
author = {Xu, Xiujuan and Sun, Yuzhi and Bai, Yulin and Zhang, Kai and Liu, Yu and Zhao, Xiaowei},
title = {Seq2Img-DRNET: A travel time index prediction algorithm for complex road network at regional level},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115554},
doi = {10.1016/j.eswa.2021.115554},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {12},
keywords = {Intelligent transportation, Convolution neural network, Feature learning network of regional complex road network, Travel time index prediction}
}

@inbook{10.5555/3454287.3455521,
author = {Kuralenok, Igor and Ershov, Vasily and Labutin, Igor},
title = {MonoForest framework for tree ensemble analysis},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this work, we introduce a new decision tree ensemble representation framework: instead of using a graph model we transform each tree into a well-known polynomial form. We apply the new representation to three tasks: theoretical analysis, model reduction, and interpretation. The polynomial form of a tree ensemble allows a straightforward interpretation of the original model. In our experiments, it shows comparable results with state-of-the-art interpretation techniques. Another application of the framework is the ensemble-wise pruning: we can drop monomials from the polynomial, based on train data statistics. This way we reduce the model size up to 3 times without loss of its quality. It is possible to show the equivalence of tree shape classes that share the same polynomial. This fact gives us the ability to train a model in one tree's shape and exploit it in another, which is easier for computation or interpretation. We formulate a problem statement for optimal tree ensemble translation from one form to another and build a greedy solution to this problem.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1234},
numpages = {10}
}

@article{10.4018/IJSI.2021070105,
author = {Jo, Jun-Hyuk and Lee, Jihyun and Jaffari, Aman and Kim, Eunmi},
title = {Fault Localization With Data Flow Information and an Artificial Neural Network},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {3},
issn = {2166-7160},
url = {https://doi.org/10.4018/IJSI.2021070105},
doi = {10.4018/IJSI.2021070105},
abstract = {Fault localization is a technique for identifying the exact source code line with faults. It typically requires a lot of time and cost because, to locate the fault, a developer must track the execution of the failed program line by line. To reduce the fault localization efforts, many methods have been proposed. However, their localized suspicious code range is wide, and their fault localization effect is not high. To cope with this limitation, this paper computes the degree of fault suspiciousness of statements by using an artificial neural network and information of the executed test case, such as statement coverage, execution result, and definition-use pair. Compared to the approach that uses only statement coverage as input data for training an artificial neural network, the experiment results show higher accuracy in 15 types of faults out of 29 real fault types in the approach that the definition-use pair included.},
journal = {Int. J. Softw. Innov.},
month = jul,
pages = {66–78},
numpages = {13},
keywords = {Software Verification, Software Testing, Fault Suspiciousness, Fault Localization, Du-Pair, Definition-Use, Data Flow Coverage, Artificial Neural Network}
}

@inproceedings{10.1145/3241403.3241426,
author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
title = {Model-based support for decision-making in architecture evolution of complex software systems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241426},
doi = {10.1145/3241403.3241426},
abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {21},
numpages = {7},
keywords = {systems-of-systems composition, software variability management, software migration, software architecture evolution, model-based decision support},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1145/3400302.3415715,
author = {Dai, Xiaotian and Zhao, Shuai and Jiang, Yu and Jiao, Xun and Hu, Xiaobo Sharon and Chang, Wanli},
title = {Fixed-priority scheduling and controller co-design for time-sensitive networks},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415715},
doi = {10.1145/3400302.3415715},
abstract = {Time-sensitive networking (TSN) is a set of standardised communication protocols developed under the IEEE 802.1 working group. TSN aims to support deterministic communication based on network schedules that are distributively configured. It is widely considered as the future in-vehicle network solution for highly automated driving, where the requirement on timing guarantee is alongside the demand of high communication bandwidth. In this work, we study a setting of periodic control and non-control packets, with implicit and arbitrary deadlines, respectively. As the FIFO (first-in, first-out) queues in the 802.1Qbv switch incur long delay in the worst case, which prevents the control tasks from achieving short sampling periods and thus impedes control performance optimisation, we propose the first fixed-priority scheduling (FPS) approach for TSN by leveraging its gate control features. In this context, we develop a finer-grained frame-level response time analysis, which provides a tighter bound than the conventional packet-level analysis. Building upon FPS and the above analysis, we formulate a co-design optimisation problem to decide the sampling periods and poles of real-time controllers with settling time as the objective to minimise, whilst satisfying the schedulability constraint.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {99},
numpages = {9},
keywords = {time-sensitive networks, scheduling-controller co-design, real-time communication, fixed-priority scheduling},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1007/s10618-010-0175-9,
author = {Silla, Carlos N. and Freitas, Alex A.},
title = {A survey of hierarchical classification across different application domains},
year = {2011},
issue_date = {January   2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1–2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-010-0175-9},
doi = {10.1007/s10618-010-0175-9},
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
journal = {Data Min. Knowl. Discov.},
month = jan,
pages = {31–72},
numpages = {42},
keywords = {Tree-structured class hierarchies, Hierarchical classification, DAG-structured class hierarchies}
}

@inproceedings{10.5555/3408352.3408590,
author = {NOUBIR, Safouane and REAL, Maria MENDEZ and PILLEMENT, S\'{e}bastien},
title = {Towards malicious exploitation of energy management mechanisms},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Architectures are becoming more and more complex to keep up with the increase of algorithmic complexity. To fully exploit those architectures, dynamic resources managers are required. The goal of dynamic managers is either to optimize the resource usage (e.g. cores, memory) or to reduce energy consumption under performance constraints. However, performance optimization being their main goal, they have not been designed to be secure and present vulnerabilities. Recently, it has been proven that energy managers can be exploited to cause faults within a processor allowing to steal information from a user device. However, this exploitation is not often possible in current commercial devices. In this work, we show current security vulnerabilities through another type of malicious usage of energy management, experimentation shows that it is possible to remotely lock out a device, denying access to all services and data, requiring for example the user to pay a ransom to unlock it. The main target of this exploit are embedded systems and we demonstrate this work by its implementation on two different commercial ARM-based devices.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1043–1048},
numpages = {6},
location = {Grenoble, France},
series = {DATE '20}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jul,
articleno = {15},
numpages = {26},
keywords = {policy distillation, deep reinforcement learning, autonomous driving, Transfer reinforcement learning}
}

@article{10.1016/j.neucom.2019.11.046,
author = {He, Juan-Juan and Lin, Ya-Qi and Ge, Ming-Feng and Liang, Chang-Duo and Ding, Teng-Fei and Wang, Leimin},
title = {Adaptive finite-time cluster synchronization of neutral-type coupled neural networks with mixed delays},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {384},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.046},
doi = {10.1016/j.neucom.2019.11.046},
journal = {Neurocomput.},
month = apr,
pages = {11–20},
numpages = {10},
keywords = {Mixed delays, Adaptive control, Finite-time cluster synchronization, Neutral-type coupled neural networks (NCNNs)}
}

@article{10.5555/1340786.1343151,
author = {Sawahata, Yasuhito and Khosla, Rajiv and Komine, Kazuteru and Hiruma, Nobuyuki and Itou, Takayuki and Watanabe, Seiji and Suzuki, Yuji and Hara, Yumiko and Issiki, Nobuo},
title = {Determining comprehension and quality of TV programs using eye-gaze tracking},
year = {2008},
issue_date = {May, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {5},
issn = {0031-3203},
abstract = {Currently, TV programs are evaluated by using questionnaires given after previews or by using TV ratings. There are few objective criteria useful for describing technical know-how about program production. One of the TV program producers' concerns is how to choose expression methods that convey their ideas to viewers correctly and efficiently. Research has shown that eye-gaze direction is related to the human focus and attention. Gaze-based evaluations have been proposed for image-quality evaluations and certain usability tests. Such approaches are mainly based on how often a specific region attracted the subjects' gaze or how long their gaze was fixed on it. To apply these approaches to TV programs, all the object regions that seem to attract a viewer's gaze need to be specified in advance. This causes several problems including the accuracy of specifying the region by using an image processing technique is not equal to the human subject's recognition ability and it is not feasible to manually specify such regions in an enormous number of frames (images) comprising the program. Further, how characteristics of well-produced TV programs appear on the viewer's gaze has not been objectively analyzed yet. There is a need to investigate the relationship between gaze and program contents which can be used as means for improving comprehension and quality of the TV programs. In this paper, we propose a new measurement and evaluating method for this purpose. This paper focuses on the relationship between a viewer's comprehension of a program and their gaze direction in a real experimental TV educational program involving 26 elementary school children and broadcast by NHK Broadcasting Corporation of Japan. Correlation between TV program comprehension and entropy is investigated. That is, variances in the gaze direction in relation to program comprehension are based on a entropy value that represents the degree of dispersion in each frame and is calculated from a probability density function estimated from the gaze directions. The results indicate that the variances of the gaze direction for scenes that gave better comprehension tended to be lower. This tendency was further noticeable after a keyword utterance were related to the answers of corresponding questions.},
journal = {Pattern Recogn.},
month = may,
pages = {1610–1626},
numpages = {17},
keywords = {TV program comprehension, TV contents evaluation, Performance optimization framework, Gaussian mixture models, Eye-gaze direction, Entropy}
}

@inproceedings{10.1109/CCGrid.2014.118,
author = {Rughetti, Diego and Di Sanzo, Pierangelo and Ciciani, Bruno and Quaglia, Francesco},
title = {Analytical/ML mixed approach for concurrency regulation in software transactional memory},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.118},
doi = {10.1109/CCGrid.2014.118},
abstract = {In this article we exploit a combination of analytical and Machine Learning (ML) techniques in order to build a performance model allowing to dynamically tune the level of concurrency of applications based on Software Transactional Memory (STM). Our mixed approach has the advantage of reducing the training time of pure machine learning methods, and avoiding approximation errors typically affecting pure analytical approaches. Hence it allows very fast construction of highly reliable performance models, which can be promptly and effectively exploited for optimizing actual application runs. We also present a real implementation of a concurrency regulation architecture, based on the mixed modeling approach, which has been integrated with the open source TinySTM package, together with experimental data related to runs of applications taken from the STAMP benchmark suite demonstrating the effectiveness of our proposal.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {81–91},
numpages = {11},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@article{10.1145/3409114,
author = {Zhang, Jialiang and Zha, Yue and Beckwith, Nicholas and Liu, Bangya and Li, Jing},
title = {MEG: A RISCV-based System Emulation Infrastructure for Near-data Processing Using FPGAs and High-bandwidth Memory},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1936-7406},
url = {https://doi.org/10.1145/3409114},
doi = {10.1145/3409114},
abstract = {Emerging three-dimensional (3D) memory technologies, such as the Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM), provide high-bandwidth and massive memory-level parallelism. With the growing heterogeneity and complexity of computer systems (CPU cores and accelerators, etc.), efficiently integrating emerging memories into existing systems poses new challenges and requires detailed evaluation in a realistic computing environment. In this article, we propose MEG, an open source, configurable, cycle-exact, and RISC-V-based full-system emulation infrastructure using FPGA and HBM. MEG provides a highly modular hardware design and includes a bootable Linux image for a realistic software flow, so that users can perform cross-layer software-hardware co-optimization in a full-system environment. To improve the observability and debuggability of the system, MEG also provides a flexible performance monitoring scheme to guide the performance optimization. The proposed MEG infrastructure can potentially benefit broad communities across computer architecture, system software, and application software. Leveraging MEG, we present two cross-layer system optimizations as illustrative cases to demonstrate the usability of MEG. In the first case study, we present a reconfigurable memory controller to improve the address mapping of standard memory controller. This reconfigurable memory controller along with its OS support allows us to optimize the address mapping scheme to fully exploit the massive parallelism provided by the emerging three-dimensional (3D) memories. In the second case study, we present a lightweight IOMMU design to tackle the unique challenges brought by 3D memory in providing virtual memory support for near-memory accelerators. We provide a prototype implementation of MEG on a Xilinx VU37P FPGA and demonstrate its capability, fidelity, and flexibility on real-world benchmark applications. We hope MEG fills a gap in the space of publicly available FPGA-based full-system emulation infrastructures, specifically targeting memory systems, and inspires further collaborative software/hardware innovations.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = sep,
articleno = {19},
numpages = {24},
keywords = {near-memory acceleration, high bandwidth memory, address mapping scheme, RISC-V core, Full-system emulation, FPGAs, 3d-stacking memory}
}

@article{10.1016/j.engappai.2018.06.010,
author = {Chin, Cheng Siong and Ji, Xi},
title = {Adaptive online sequential extreme learning machine for frequency-dependent noise data on offshore oil rig},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.010},
doi = {10.1016/j.engappai.2018.06.010},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {226–241},
numpages = {16},
keywords = {Root mean square error, Training time, Noise prediction, Oil-rig, Extreme learning machine, Multiple frequency dependent data}
}

@article{10.1007/s11554-019-00880-z,
author = {Fathy, Ghada M. and Hassan, Hanan A. and Rahwan, Shaheera and Sheta, Walaa M.},
title = {Parallel implementation of multiple kernel self-organizing maps for spectral unmixing},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-019-00880-z},
doi = {10.1007/s11554-019-00880-z},
abstract = {Spectral unmixing algorithms are commonly used in processing of hyperspectral images to identify the elemental components, called end-members, and their corresponding
information in each pixel of the image. However, these algorithms are computationally intensive and can become a bottleneck for remote sensing hyperspectral image processing, especially in large aerial imagery processing centers. This paper, explores the use of massive parallel processing graphical processing unit to speed up the multi kernel self-organizing map (MKSOM) unmixing algorithm. MKSOM is based on artificial neural networks, which makes it suitable to be efficiently parallelized. Two real benchmark hyperspectral images; AVIRIS Cuprite and Brullus are used to evaluate the performance of the parallel algorithm. The experimental results show that the proposed implementation is appropriated for real-time hyperspectral remote sensing applications due to a very small worst case parallel execution time (0.83&nbsp;s when the number of classes is less than 9) which makes it feasible to be integrated as on-board processing on any Hyperspectral remote sensors. Our parallel technique achieved a significant speedup compared with a multi-threaded CPU implementation applied on the same hyperspectral image. The results showed a speedup of 93.46 \texttimes{} for SOM size of 256 and trained for 100 epochs on medium-sized HSI such as AVIRIS Cuprite.},
journal = {J. Real-Time Image Process.},
month = oct,
pages = {1267–1284},
numpages = {18},
keywords = {Remote sensing applications, Self-organization map, GPU, Hyperspectral image, Spectral unmixing}
}

@article{10.1007/s00500-020-05028-x,
author = {Wang, Min and Zhang, Ying-Yi and Min, Fan and Deng, Li-Ping and Gao, Lei},
title = {A two-stage density clustering algorithm},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {23},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05028-x},
doi = {10.1007/s00500-020-05028-x},
abstract = {Clustering by fast search and find of density peaks (CFDP) is a popular density-based algorithm. However, it is criticized because it is inefficient and applicable only to some types of data, and requires the manual setting of the key parameter. In this paper, we propose the two-stage density clustering algorithm, which takes advantage of granular computing to address the aforementioned issues. The new algorithm is highly efficient, adaptive to various types of data, and requires minimal parameter setting. The first stage uses the two-round-means algorithm to obtain n small blocks, where n is the number of instances. This stage decreases the data size directly from n to n. The second stage constructs the master tree and obtains the final blocks. This stage borrows the structure of CFDP, while the cutoff distance parameter is not required. The time complexity of the algorithm is O(mn32), which is lower than O(mn2) for CFDP. We report the results of some experiments performed on 21 datasets from various domains to compare a new clustering algorithm with some state-of-the-art clustering algorithms. The results demonstrated that the new algorithm is adaptive to different types of datasets. It is two or more orders of magnitude faster than CFDP.},
journal = {Soft Comput.},
month = dec,
pages = {17797–17819},
numpages = {23},
keywords = {Efficiency, Density peak, Clustering}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Decision-making, Cloud service selection, Cloud computing}
}

@article{10.5555/2946645.3007037,
author = {Wei, Ermo and Luke, Sean},
title = {Lenient learning in independent-learner stochastic cooperative games},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional ("Distributed") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2914–2955},
numpages = {42},
keywords = {reinforcement learning, multiagent learning, lenient learning, independent learner, game theory}
}

@article{10.3233/JIFS-179495,
author = {Mao, Kedun and Elhoseny, Mohamed and Yuan, X.},
title = {Research on key technology analysis and system design of enterprise patent management system},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179495},
doi = {10.3233/JIFS-179495},
abstract = {Patent documents, the paper constructs a patent model tree and establishes a similar patent determination method based on vector space model and patent document features. The paper then proposes to use the patent application number as the association identifier, and associate the ontology and the ontology instance with the existing patent information database, which improves the speed of establishing the ontology and reduces the maintenance and update of the ontology. At the same time, based on the integration of existing workflow management and knowledge flow management, this paper proposes an integration framework with patent knowledge as the core knowledge and integration link, and describes the nature of knowledge retrieval and the method of importance differentiation. Finally, the paper designs a patent retrieval system, establishes the structural model and functional model of the system and the technical framework of the implementation system. On this basis, the design of the update module of the PMAS system is discussed. Finally, the performance optimization of the PMAS system is designed in detail.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1319–1328},
numpages = {10},
keywords = {key technology analysis, integrated knowledge environment, workflow, Patent management analysis system}
}

@article{10.1016/j.jisa.2014.03.002,
author = {Rahbarinia, Babak and Perdisci, Roberto and Lanzi, Andrea and Li, Kang},
title = {PeerRush},
year = {2014},
issue_date = {July 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {19},
number = {3},
issn = {2214-2126},
url = {https://doi.org/10.1016/j.jisa.2014.03.002},
doi = {10.1016/j.jisa.2014.03.002},
abstract = {In this paper we present PeerRush, a novel system for the identification of unwanted P2P traffic. Unlike most previous work, PeerRush goes beyond P2P traffic detection, and can accurately categorize the detected P2P traffic and attribute it to specific P2P applications, including malicious applications such as P2P botnets. PeerRush achieves these results without the need of deep packet inspection, and can accurately identify applications that use encrypted P2P traffic.We implemented a prototype version of PeerRush and performed an extensive evaluation of the system over a variety of P2P traffic datasets. Our results show that we can detect all the considered types of P2P traffic with up to 99.5% true positives and 0.1% false positives. Furthermore, PeerRush can attribute the P2P traffic to a specific P2P application with a misclassification rate of 0.68% or less.},
journal = {J. Inf. Secur. Appl.},
month = jul,
pages = {194–208},
numpages = {15},
keywords = {Traffic classification, P2P, Botnets}
}

@inproceedings{10.5555/2969033.2969059,
author = {Jiang, Lu and Meng, Deyu and Yu, Shoou-I and Lan, Zhenzhong and Shan, Shiguang and Hauptmann, Alexander G.},
title = {Self-paced learning with diversity},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2078–2086},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{10.1016/j.cie.2016.01.019,
author = {Dey, Balaram and Bairagi, Bipradas and Sarkar, Bijan and Sanyal, Subir Kumar},
title = {Multi objective performance analysis},
year = {2016},
issue_date = {April 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.01.019},
doi = {10.1016/j.cie.2016.01.019},
abstract = {Display Omitted A novel modified weight concept included in algorithm MOPA is unique in nature.New concept reduces inherent inaccuracy of weights significantly.MOPA can handle subjective and objective attributes; benefit and cost criteria.The algorithm MOPA fits itself in the class of applied MCDM techniques.ANOVA and SA reveal MOPA as precisely accurate and effective decision making tool. This investigation introduces multi objective performance analysis (MOPA), a novel multi-criteria decision making (MCDM) approach to solve decision problems in a supply chain. In this paper, an innovative modified weight concept is employed to modify the weights of the criteria in order to reduce the affect of the inherent inaccuracy involved with direct use of weights. Modified weight and normalized performance rating are integrated to compute modified weighted performance (MWP). Aggregate modified weighted performances (AMWP) of the alternatives are determined to evaluate benefit cost ratio (BCR) which is considered as the final selection index of the alternative. The proposed algorithm MOPA is illustrated with six real life decision problems in various stages of a supply chain to adjudge its enviable significance from the point of simplicity, feasibility and applicability. In order to ensure the compatibility, the result obtained by the proposed algorithm MOPA is compared with the proven and established MCDM methodologies TOPSIS, SAW, MOORA, ELECTRE II, and VIKOR. The comparative analysis shows that the achieved result perfectly matches with most of the cited decision problems of previous research works published in various journals. Analysis of variance (ANOVA) reveal that the modified weight concept reduces the relative dispersion of weights significantly, leads to precise decision. Sensitivity analysis (SA) and other investigations also find MOPA as a simple, robust, effective and precise decision making tool.},
journal = {Comput. Ind. Eng.},
month = apr,
pages = {105–124},
numpages = {20},
keywords = {Supply chain, Sensitivity analysis, Multi objective performance analysis (MOPA), Modified weight concept, MCDM, Analysis of variances}
}

@inproceedings{10.1145/3302333.3302335,
author = {Siegmund, Norbert},
title = {Challenges and Insights from Optimizing Configurable Software Systems},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302335},
doi = {10.1145/3302333.3302335},
abstract = {Configuring a software system to optimize non-functional properties is a hard task. There are dozens to thousands of configuration options that can affect performance, energy consumption, and other attributes of the resulting program. Even worse, options may interact, such that their combined presence (or absence) has an influence on a non-functional property.In this talk, I report on our experiences with learning different performance models based on a multitude of sampling techniques. The goal is to raise awareness of the distinct challenges in this domain: constraints among options, the exponential search space, and suitable sampling and learning techniques. I show a variety of approaches including their strengths and weaknesses and close the talk with new challenges relevant for our community: changing environments and reproducibility.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {Sampling, Performance, Learning Prediction, Configurable Software Systems},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1109/TASLP.2014.2359628,
author = {Chen, Austin and Hasegawa-Johnson, Mark A.},
title = {Mixed stereo audio classification using a stereo-input mixed-to-panned level feature},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359628},
doi = {10.1109/TASLP.2014.2359628},
abstract = {Many past studies have been conducted on speech/music discrimination due to the potential applications for broadcast and other media; however, it remains possible to expand the experimental scope to include samples of speech with varying amounts of background music. This paper focuses on the development and evaluation of two measures of the ratio between speech energy and music energy: a reference measure called speech-to-music ratio (SMR), which is known objectively only prior to mixing, and a feature called the stereo-input mix-to-peripheral level feature (SIMPL), which is computed from the stereo mixed signal as an imprecise estimate of SMR. SIMPL is an objective signal measure calculated by taking advantage of broadcast mixing techniques in which vocals are typically placed at stereo center, unlike most instruments. Conversely, SMR is a hidden variable defined by the relationship between the powers of portions of audio attributed to speech and music. It is shown that SIMPL is predictive of SMR and can be combined with state-of-the-art features in order to improve performance. For evaluation, this new metric is applied in speech/music (binary) classification, speech/music/mixed (trinary) classification, and a new speech-to-music ratio estimation problem. Promising results are achieved, including 93.06% accuracy for trinary classification and 3.86 dB RMSE for estimation of the SMR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2025–2033},
numpages = {9},
keywords = {speech/music discrimination, speech processing, music processing, music information retrieval, mel-frequency cepstral coefficients, classification algorithms, audio segmentation, audio processing, audio classification, Gaussian mixture model}
}

@article{10.1109/TC.2007.1082,
author = {Pal, Sourav and Kundu, Sumantra and Chatterjee, Mainak and Das, Sajal},
title = {Combinatorial Reverse Auction based Scheduling in Multi-Rate Wireless Systems},
year = {2007},
issue_date = {October 2007},
publisher = {IEEE Computer Society},
address = {USA},
volume = {56},
number = {10},
issn = {0018-9340},
url = {https://doi.org/10.1109/TC.2007.1082},
doi = {10.1109/TC.2007.1082},
abstract = {Opportunistic scheduling algorithms are effective in exploiting channel variations and maximizing system throughput in multi-rate wireless networks. However, most scheduling algorithms ignore the per-user quality of service (QoS) requirements and try to allocate resources (e.g., the time slots) among multiple users. This leads to a phenomenon commonly referred to as the exposure problem wherein the algorithms fail to satisfy the minimum slot requirements of the users due to substitutability and complementarity requirements of user slots. To eliminate this exposure problem, we propose a novel scheduling algorithm based on two-phase combinatorial reverse auction with the primary objective to maximize the number of satisfied users in the system. We also consider maximizing the system throughput as a secondary objective. In the proposed scheme, multiple users bid for the required number of time slots, and the allocations are done to satisfy the two objectives in a sequential manner. We provide an approximate solution to the proposed scheduling problem which is NP-complete. The proposed algorithm has an approximation ratio of (1 + logm) with respect to the optimal solution, where m is the number of slots in a schedule cycle. Simulation results are provided to compare the proposed scheduling algorithm with other competitive schemes.},
journal = {IEEE Trans. Comput.},
month = oct,
pages = {1329–1341},
numpages = {13},
keywords = {reverse auction, performance optimization, multi-rate wireless system, Scheduling}
}

@inproceedings{10.1145/3458817.3476197,
author = {Shu, Tong and Guo, Yanfei and Wozniak, Justin and Ding, Xiaoning and Foster, Ian and Kurc, Tahsin},
title = {Bootstrapping in-situ workflow auto-tuning via combining performance models of component applications},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476197},
doi = {10.1145/3458817.3476197},
abstract = {In an in-situ workflow, multiple components such as simulation and analysis applications are coupled with streaming data transfers. The multiplicity of possible configurations necessitates an auto-tuner for workflow optimization. Existing auto-tuning approaches are computationally expensive because many configurations must be sampled by running the whole workflow repeatedly in order to train the auto-tuner surrogate model or otherwise explore the configuration space. To reduce these costs, we instead combine the performance models of component applications by exploiting the analytical workflow structure, selectively generating test configurations to measure and guide the training of a machine learning workflow surrogate model. Because the training can focus on well-performing configurations, the resulting surrogate model can achieve high prediction accuracy for good configurations despite training with fewer total configurations. Experiments with real applications demonstrate that our approach can identify significantly better configurations than other approaches for a fixed computer time budget.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {28},
numpages = {15},
keywords = {in-situ workflow, component model combination, bootstrapping, auto-tuning},
location = {St. Louis, Missouri},
series = {SC '21}
}

@article{10.1016/j.cviu.2021.103255,
author = {Landi, Federico and Baraldi, Lorenzo and Cornia, Marcella and Corsini, Massimiliano and Cucchiara, Rita},
title = {Multimodal attention networks for low-level vision-and-language navigation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {210},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103255},
doi = {10.1016/j.cviu.2021.103255},
journal = {Comput. Vis. Image Underst.},
month = sep,
numpages = {9},
keywords = {Multi-modal attention, Embodied AI, Vision-and-language navigation, 68T45, 68T40, 68T01}
}

@inproceedings{10.1007/978-3-030-66415-2_37,
author = {Perry, Jonathan and Fernandez, Amanda S.},
title = {EyeSeg: Fast and Efficient Few-Shot Semantic Segmentation},
year = {2020},
isbn = {978-3-030-66414-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66415-2_37},
doi = {10.1007/978-3-030-66415-2_37},
abstract = {Semantic segmentation is a key component in eye- and gaze- tracking for virtual reality (VR) and augmented reality (AR) applications. While it is a well-studied computer vision problem, most state-of-the-art models require large amounts of labeled data, which is limited in this specific domain. An additional consideration in eye tracking is the capacity for real-time predictions, necessary for responsive AR/VR interfaces. In this work, we propose EyeSeg, an encoder-decoder architecture designed for accurate pixel-wise few-shot semantic segmentation with limited annotated data. We report results from the OpenEDS2020 Challenge, yielding a 94.5% mean Intersection Over Union (mIOU) score, which is a 10.5% score increase over the baseline approach. The experimental results demonstrate state-of-the-art performance while preserving a low latency framework. Source code is available: .},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part I},
pages = {570–582},
numpages = {13},
keywords = {OpenEDS2020, Computer vision, Eye tracking, Semantic segmentation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/1776814.1776823,
author = {Lappas, Georgios},
title = {Estimating the size of neural networks from the number of available training data},
year = {2007},
isbn = {3540746897},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Estimating a priori the size of neural networks for achieving high classification accuracy is a hard problem. Existing studies provide theoretical upper bounds on the size of neural networks that are unrealistic to implement. This work provides a computational study for estimating the size of neural networks using as an estimation parameter the size of available training data. We will also show that the size of a neural network is problem dependent and that one only needs the number of available training data to determine the size of the required network for achieving high classification rate. We use for our experiments a threshold neural network that combines the perceptron algorithm with simulated annealing and we tested our results on datasets from the UCI Machine Learning Repository. Based on our experimental results, we propose a formula to estimate the number of perceptrons that have to be trained in order to achieve a high classification accuracy.},
booktitle = {Proceedings of the 17th International Conference on Artificial Neural Networks},
pages = {68–77},
numpages = {10},
location = {Porto, Portugal},
series = {ICANN'07}
}

@inproceedings{10.1145/3289602.3293915,
author = {Chen, Yao and He, Jiong and Zhang, Xiaofan and Hao, Cong and Chen, Deming},
title = {Cloud-DNN: An Open Framework for Mapping DNN Models to Cloud FPGAs},
year = {2019},
isbn = {9781450361378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289602.3293915},
doi = {10.1145/3289602.3293915},
abstract = {The efficacy and effectiveness of Convolutional Neural Networks (CNNs) have been proven in a wide range of machine learning applications. However, the high computational complexity of CNNs presents a critical challenge towards their broader adoption in real-time and power-efficient scenarios. FPGAs are poised to take a significant role for high-performance and energy-efficient computation of CNNs for both mobile (e.g., UAVs, self-driving cars, and IoT devices) and cloud computing domains. However, implementing an effective CNN system onto FPGAs efficiently remains problematic. The current cloud-based FPGAs with unique design constraints and architectural characteristics further increase the challenges. To address these challenges, we propose a novel open-source automated tool chain called Cloud-DNN. Our tool chain takes trained CNN models specified in Caffe as input, performs a set of transformations, and maps the model to a cloud-based FPGA. Cloud-DNN can significantly improve the overall design productivity of CNNs on FPGAs while satisfying the emergent computational requirements. Our design provides an alternative solution compared to other cloud-based options (e.g., GPUs or TPUs) while offering flexible, and high performance DNN inferences. The unique features of Cloud-DNN include the optimizations with cloud-platform characteristics and the support of easier and streamlined implementation. Experimental results demonstrate up to 104.55x performance improvement when compared to CPU implementation and comparable usability, flexibility, and strong quality compared to other state-of-the-art DNN inference implementations on standalone FPGAs.},
booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {73–82},
numpages = {10},
keywords = {reconfiguration, neural network, high-level synthesis, fpga, dnn accelerator, cloud computing},
location = {Seaside, CA, USA},
series = {FPGA '19}
}

@article{10.1007/s11334-020-00382-3,
author = {Swathi, A. and Aarti and Kumar, Sandeep},
title = {A smart application to detect pupil for small dataset with low illumination},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {1},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-020-00382-3},
doi = {10.1007/s11334-020-00382-3},
abstract = {Biometric applications are very sensitive to the process because of its complexity in presenting unstructured input to the processing. The existing applications of image processing are based on the implementation of different programing segments such as image acquisition, segmentation, extraction, and final output. The proposed model is designed with 2 convolution layers and 3 dense layers. We examined the module with 5 datasets including 3 benchmark datasets, namely CASIA, UBIRIS, MMU, random dataset, and the live video. We calculated the FPR, FNR, Precision, Recall, and accuracy of each dataset. The calculated accuracy of CASIA using the proposed system is 82.8%, for UBIRIS is 86%, MMU is 84%, and the random dataset is 84%. On live video with low resolution, calculated accuracy is 72.4%. The proposed system achieved better accuracy compared to existing state-of-the-art systems.},
journal = {Innov. Syst. Softw. Eng.},
month = mar,
pages = {29–43},
numpages = {15},
keywords = {CNN, Segmentation, Pupil}
}

@inproceedings{10.5555/2840819.2840911,
author = {Blanton, Ronald D. and Li, Xin and Mai, Ken and Marculescu, Diana and Marculescu, Radu and Paramesh, Jeyanandh and Schneider, Jeff and Thomas, Donald E.},
title = {Statistical Learning in Chip (SLIC)},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Despite best efforts, integrated systems are "born" (manufactured) with a unique `personality' that stems from our inability to precisely fabricate their underlying circuits, and create software a priori for controlling the resulting uncertainty. It is possible to use sophisticated test methods to identify the best-performing systems but this would result in unacceptable yields and correspondingly high costs. The system personality is further shaped by its environment (e.g., temperature, noise and supply voltage) and usage (i.e., the frequency and type of applications executed), and since both can fluctuate over time, so can the system's personality. Systems also "grow old" and degrade due to various wear-out mechanisms (e.g., negative-bias temperature instability), and unexpectedly due to various early-life failure sources. These "nature and nurture" influences make it extremely difficult to design a system that will operate optimally for all possible personalities. To address this challenge, we propose to develop statistical learning in-chip (SLIC). SLIC is a holistic approach to integrated system design based on continuously learning key personality traits on-line, for self-evolving a system to a state that optimizes performance hierarchically across the circuit, platform, and application levels. SLIC will not only optimize integrated-system performance but also reduce costs through yield enhancement since systems that would have before been deemed to have weak personalities (unreliable, faulty, etc.) can now be recovered through the use of SLIC.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {664–669},
numpages = {6},
keywords = {statistical and machine learning, low-power design, Integrated system design},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1007/978-3-642-40270-8_4,
author = {Bari, A. T. and Reaz, Mst. Rokeya and Choi, Ho-Jin and Jeong, Byeong-Soo},
title = {DNA Encoding for Splice Site Prediction in Large DNA Sequence},
year = {2013},
isbn = {9783642402692},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40270-8_4},
doi = {10.1007/978-3-642-40270-8_4},
abstract = {Splice site prediction in the pre-mRNA is a very important task for understanding gene structure and its function. To predict splice sites, SVM support vector machine based classification technique is frequently used because of its classification accuracy. High classification accuracy of SVM largely depends on DNA encoding method for feature extraction of DNA sequences. However, existing encoding approaches do not reveal the characteristics of DNA sequence very well enough to provide as much information as DNA sequences have. In this paper, we propose new effective DNA encoding method which can give more information of DNA sequence. Our encoding method can provide density information of each nucleotide along with positional information and chemical property. Extensive performance study shows that our method can provide better performance than existing encoding methods based on several performance criteria such as classification accuracy, sensitivity, specificity and area under receiver operating characteristics curve ROC.},
booktitle = {Proceedings of the 18th International Conference on Database Systems for Advanced Applications - Volume 7827},
pages = {46–58},
numpages = {13},
keywords = {support vector machine, splice site, orthogonal encoding, nucleotide density, gene prediction, ROC, DNA sequence}
}

@inproceedings{10.5555/3437539.3437755,
author = {Zhu, Maohua and Xie, Yuan},
title = {Taming unstructured sparsity on GPUs via latency-aware optimization},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Neural Networks (NNs) exhibit high redundancy in their parameters so that pruning methods can achieve high compression ratio without accuracy loss. However, the very high sparsity produced by unstructured pruning methods is difficult to be efficiently mapped onto Graphics Processing Units (GPUs) because of its decoding overhead and workload imbalance. With the introduction of Tensor Core, the latest GPUs achieve even higher throughput for the dense neural networks. This makes unstructured neural networks fail to outperform their dense counterparts because they are not currently supported by Tensor Core. To tackle this problem, prior work suggests structured pruning to improve the performance of sparse NNs on GPUs. However, such structured pruning methods have to sacrifice a significant part of sparsity to retain the model accuracy, which limits the speedup on the hardware. In this paper, we observe that the Tensor Core is also able to compute unstructured sparse NNs efficiently. To achieve this goal, we first propose ExTensor, a set of sparse Tensor Core instructions with a variable input matrix tile size. The variable tile size allows a matrix multiplication to be implemented by mixing different types of ExTensor instructions. We build a performance model to estimate the latency of an ExTensor instruction given an operand sparse weight matrix. Based on this model, we propose a heuristic algorithm to find the optimal sequence of the instructions for an ExTensor based kernel to achieve the best performance on the GPU. Experimental results demonstrate that our approach achieves 36% better performance than the state-of-the-art sparse Tensor Core design.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {216},
numpages = {6},
location = {Virtual Event, USA},
series = {DAC '20}
}

@inproceedings{10.1007/978-3-642-25085-9_62,
author = {Ribeiro, Bernardete and Gon\c{c}alves, Ivo and Santos, S\'{e}rgio and Kovacec, Alexander},
title = {Deep learning networks for off-line handwritten signature recognition},
year = {2011},
isbn = {9783642250842},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25085-9_62},
doi = {10.1007/978-3-642-25085-9_62},
abstract = {Reliable identification and verification of off-line handwritten signatures from images is a difficult problem with many practical applications. This task is a difficult vision problem within the field of biometrics because a signature may change depending on psychological factors of the individual. Motivated by advances in brain science which describe how objects are represented in the visual cortex, advanced research on deep neural networks has been shown to work reliably on large image data sets. In this paper, we present a deep learning model for off-line handwritten signature recognition which is able to extract high-level representations. We also propose a two-step hybrid model for signature identification and verification improving the misclassification rate in the well-known GPDS database.},
booktitle = {Proceedings of the 16th Iberoamerican Congress Conference on Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
pages = {523–532},
numpages = {10},
keywords = {signature recognition, generative models, deep learning},
location = {Puc\'{o}n, Chile},
series = {CIARP'11}
}

@inproceedings{10.1007/978-3-030-93046-2_7,
author = {Guo, Jingwen and Lu, Zhisheng and Wang, Ti and Huang, Weibo and Liu, Hong},
title = {Object Goal Visual Navigation Using Semantic Spatial Relationships},
year = {2021},
isbn = {978-3-030-93045-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93046-2_7},
doi = {10.1007/978-3-030-93046-2_7},
abstract = {The target-driven visual navigation is a popular learning-based method and has been successfully applied to a wide range of applications. However, it has some disadvantages, including being ineffective at adapting to unseen environments. In this paper, a navigation method based on Semantic Spatial Relationships (SSR) is proposed and is shown to have more reliable performance when dealing with novel conditions. The construction of joint semantic hierarchical feature vector allows for learning implicit relationship between current observation and target objects, which benefits from construction of prior knowledge graph and semantic space. This differs from the traditional target driven methods, which integrate the visual input vector directly into the reinforcement learning path planning module. Moreover, the proposed method takes both local and global features of observed image into consideration and is thus less conservative and more robust in regards to random scenes. An additional analysis indicates that the proposed SSR performs well on classical metrics. The effectiveness of the proposed SSR model is demonstrated comparing with state-of-the-art methods in unknown scenes.},
booktitle = {Artificial Intelligence: First CAAI International Conference, CICAI 2021, Hangzhou, China, June 5–6, 2021, Proceedings, Part I},
pages = {77–88},
numpages = {12},
keywords = {Hierarchical relationship, Semantic graph, Visual navigation},
location = {Hangzhou, China}
}

@inproceedings{10.1145/3459637.3481899,
author = {Yang, Zimeng and Yan, Song and Lad, Abhimanyu and Liu, Xiaowei and Guo, Weiwei},
title = {Cascaded Deep Neural Ranking Models in LinkedIn People Search},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481899},
doi = {10.1145/3459637.3481899},
abstract = {LinkedIn connects the world's professionals to make them more productive and successful. People Search plays an important role in fulfilling this goal by helping members find the most relevant and personalized results through a broad range of queries like names, job titles, skills, companies, locations, etc. It is one of the biggest search verticals at LinkedIn both in terms of engineering footprint and search traffic. In this paper, we present an overview of the People Search system, and discuss how we build and serve deep neural network (DNN) models, leveraging state-of-the-art deep natural language processing (NLP) techniques (e.g., convolutional neural networks (CNN) and Bidirectional Encoder Representations from Transformers (BERT)). We describe our journey of applying deep neural ranking models to a real-life product, including the modeling and system bottleneck challenges, crucial design choices, and lessons learned along the way. We hope a story of our endeavors and successes will provide meaningful insights to other similar systems.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4312–4320},
numpages = {9},
keywords = {personalization, people search, natural language processing, learning to rank, deep learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1007/s11227-019-03134-8,
author = {Li, Chunlin and Bai, Jingpan and Luo, Youlong},
title = {Efficient resource scaling based on load fluctuation in edge-cloud computing environment},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {9},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-03134-8},
doi = {10.1007/s11227-019-03134-8},
abstract = {With the rapid development of information technology, edge computing has grown rapidly by pushing large amounts of computing to the edge of the network. However, due to the rapid growth of edge access devices and limited edge storage space, the edge cloud faces many challenges in addressing the workloads. In this paper, a cost-optimized resource scaling strategy is proposed based on load fluctuation. Firstly, the load prediction model is built based on DBN with supervised learning to predict the workloads of edge cloud. Then, a cost-optimized resource scaling strategy is presented, which comprehensively considers reservation planning and on-demand planning. In the reservation phase, the long-term resource reservation problem is planned as a two-stage stochastic programming problem, which is transformed into a deterministic integer programming problem. In the on-demand phase, the on-demand resource scaling problem planning is solved as an integer programming problem. Finally, extensive experiments are conducted to evaluate the performance of the proposed cost-optimized resource scaling strategy based on load fluctuation.},
journal = {J. Supercomput.},
month = sep,
pages = {6994–7025},
numpages = {32},
keywords = {Edge-cloud computing environment, Load fluctuation, Resource scaling}
}

@article{10.1016/j.comnet.2019.107042,
author = {Elmasry, Wisam and Akbulut, Akhan and Zaim, Abdul Halim},
title = {Evolving deep learning architectures for network intrusion detection using a double PSO metaheuristic},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.107042},
doi = {10.1016/j.comnet.2019.107042},
journal = {Comput. Netw.},
month = feb,
numpages = {21},
keywords = {Particle swarm optimization, Network intrusion detection, Hyperparameter selection, Feature selection, Deep learning, Cyber security}
}

@inproceedings{10.1145/2567948.2577034,
author = {Imran, Muhammad and Castillo, Carlos and Lucas, Ji and Meier, Patrick and Vieweg, Sarah},
title = {AIDR: artificial intelligence for disaster response},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2577034},
doi = {10.1145/2567948.2577034},
abstract = {We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (e.g., "needs", "damage", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr.qcri.org/.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {159–162},
numpages = {4},
keywords = {stream processing, online machine learning, crowdsourcing, classification},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@article{10.4018/IJACI.2021010105,
author = {Ahmed, Hassan I. and Nasr, Abdurrahman A. and Abdel-Mageid, Salah M. and Aslan, Heba K.},
title = {DADEM: Distributed Attack Detection Model Based on Big Data Analytics for the Enhancement of the Security of Internet of Things (IoT)},
year = {2021},
issue_date = {Jan 2021},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {1},
issn = {1941-6237},
url = {https://doi.org/10.4018/IJACI.2021010105},
doi = {10.4018/IJACI.2021010105},
abstract = {Nowadays, Internet of Things (IoT) is considered as part our lives and it includes different aspects - from wearable devices to smart devices used in military applications. IoT connects a variety of devices and as such, the generated data is considered as ‘Big Data'. There has however been an increase in attacks in this era of IoT since IoT carries crucial information regarding banking, environmental, geographical, medical, and other aspects of the daily lives of humans. In this paper, a Distributed Attack Detection Model (DADEM) that combines two techniques - Deep Learning and Big Data analytics - is proposed. Sequential Deep Learning model is chosen as a classification engine for the distributed processing model after testing its classification accuracy against other classification algorithms like logistic regression, KNN, ID3 decision tree, CART, and SVM. Results showed that Sequential Deep Learning model outperforms the aforementioned ones. The classification accuracy of DADEM approaches 99.64% and 99.98% for the UNSW-NB15 and BoT-IoT datasets, respectively. Moreover, a plan is proposed for optimizing the proposed model to reduce the overhead of the overall system operation in a constrained environment like IoT.},
journal = {Int. J. Ambient Comput. Intell.},
month = jan,
pages = {114–139},
numpages = {26},
keywords = {Sequential Deep Learning (SDL), Internet of Things (IoT), Distributed Attack Detection Model (DADEM), Big Data Analytics (BDA)}
}

@inproceedings{10.5555/1887176.1887222,
author = {Kostoulas, Theodoros and Ganchev, Todor and Lazaridis, Alexandros and Fakotakis, Nikos},
title = {Enhancing emotion recognition from speech through feature selection},
year = {2010},
isbn = {3642157599},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In the present work we aim at performance optimization of a speaker-independent emotion recognition system through speech feature selection process. Specifically, relying on the speech feature set defined in the Interspeech 2009 Emotion Challenge, we studied the relative importance of the individual speech parameters, and based on their ranking, a subset of speech parameters that offered advantageous performance was selected. The affect-emotion recognizer utilized here relies on a GMM-UBM-based classifier. In all experiments, we followed the experimental setup defined by the Interspeech 2009 Emotion Challenge, utilizing the FAU Aibo Emotion Corpus of spontaneous, emotionally coloured speech. The experimental results indicate that the correct choice of the speech parameters can lead to better performance than the baseline one.},
booktitle = {Proceedings of the 13th International Conference on Text, Speech and Dialogue},
pages = {338–344},
numpages = {7},
keywords = {real-world data, feature selection, emotion recognition, affect recognition},
location = {Brno, Czech Republic},
series = {TSD'10}
}

@article{10.1007/s11263-007-0095-3,
author = {Leibe, Bastian and Leonardis, Ale\v{s} and Schiele, Bernt},
title = {Robust Object Detection with Interleaved Categorization and Segmentation},
year = {2008},
issue_date = {May       2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {1–3},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-007-0095-3},
doi = {10.1007/s11263-007-0095-3},
abstract = {This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance.

The core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to focus its efforts on object pixels and to discard misleading influences from the background. Moreover, the information from where in the image a hypothesis draws its support is employed in an MDL based hypothesis verification stage to resolve ambiguities between overlapping hypotheses and factor out the effects of partial occlusion.

An extensive evaluation on several large data sets shows that the proposed system is applicable to a range of different object categories, including both rigid and articulated objects. In addition, its flexible representation allows it to achieve competitive object detection performance already from training sets that are between one and two orders of magnitude smaller than those used in comparable systems.},
journal = {Int. J. Comput. Vision},
month = may,
pages = {259–289},
numpages = {31},
keywords = {Segmentation, Object detection, Object categorization, MDL, Hypothesis selection, Hough transform, Clustering}
}

@article{10.1007/s00500-018-3403-7,
author = {Liu, Bo and Li, Pengfei and Lin, Weiwei and Shu, Na and Li, Yin and Chang, Victor},
title = {A new container scheduling algorithm based on multi-objective optimization},
year = {2018},
issue_date = {December  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {23},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3403-7},
doi = {10.1007/s00500-018-3403-7},
abstract = {Docker container has been used in cloud computing at a rapid rate in the past 2 years, and Docker container resource scheduling problem has gradually become a research hot issue. It is NP-complete as the optimization criteria is to minimize the overall processing time of all the tasks. Nevertheless, minimization of makespan does not equate to customers' satisfaction. Aiming at the performance optimization of Docker container resource scheduling, the authors propose a multi-objective container scheduling algorithm, namely Multiopt. The algorithm considers five key factors: CPU usage of every node, memory usage of every node, the time consumption transmitting images on the network, the association between containers and nodes, the clustering of containers, which affect the performance of applications in containers. To select the most suitable node to deploy containers needed to be allocated in the scheduling process, the authors define a metric method for every key factor and establish a scoring function for each one and then combine them into a composite function. The experimental results show that compared with the other three well-known algorithms: Spread, Binpack, and Random, Multiopt increases the maximum TPS by 7% and reduces the average response time per request by 7.5% while consuming roughly same allocation time.},
journal = {Soft Comput.},
month = dec,
pages = {7741–7752},
numpages = {12},
keywords = {Swarm, Multi-objective optimization, Docker, Container scheduling}
}

@inproceedings{10.1145/1865909.1865912,
author = {Marvel, Jeremy A. and Newman, Wyatt S.},
title = {Internal model generation for evolutionary acceleration of automated robotic assembly optimization},
year = {2009},
isbn = {9781605587479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1865909.1865912},
doi = {10.1145/1865909.1865912},
abstract = {While machine learning algorithms have been successfully applied to a myriad of task configurations for parameter optimization, without the benefit of a virtual representation to permit offline training, the learning process can be costly in terms of time being spent and components being worn or broken. Parameter spaces for which the model is not known or are too complex to simulate stand to benefit from the generation of model approximations to reduce the evaluation overhead. In this paper, we describe a computational learning approach for dynamically generating internal models for Genetic Algorithms (GA) performance optimization. Through the process of exploring the parameter gene pool, a stochastic search method can effectively build a virtual model of the task space and improve the performance of the learning process. Experiments demonstrate that, in the presence of noise, neural network abstractions of the mappings of sequence parameters to their resulting performances can effectively enhance the performance of stochastic parameter optimization techniques. And results are presented that illustrate the benefits of internal model building as it pertains to simulated experiments of complex problems and to physical trials in robot assembly utilizing an industrial robotic arm to put together an aluminum puzzle.},
booktitle = {Proceedings of the 9th Workshop on Performance Metrics for Intelligent Systems},
pages = {9–15},
numpages = {7},
keywords = {robotic assembly, parameter optimization, model building, genetic algorithms},
location = {Gaithersburg, Maryland},
series = {PerMIS '09}
}

@inproceedings{10.1007/978-3-642-34321-6_8,
author = {Ye, Zhen and Bouguettaya, Athman and Zhou, Xiaofang},
title = {QoS-Aware cloud service composition based on economic models},
year = {2012},
isbn = {9783642343209},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34321-6_8},
doi = {10.1007/978-3-642-34321-6_8},
abstract = {Cloud service composition is usually long term based and economically driven. We consider cloud service composition from a user-based perspective. Specifically, the contributions are shown in three aspects. We propose to use discrete Bayesian Network to represent the economic model of end users. The cloud service composition problem is modeled as an Influence Diagram problem. A novel influence-diagram-based cloud service composition approach is proposed. Analytical and simulational results are presented to show the performance of the proposed composition approach.},
booktitle = {Proceedings of the 10th International Conference on Service-Oriented Computing},
pages = {111–126},
numpages = {16},
location = {Shanghai, China},
series = {ICSOC'12}
}

@article{10.1145/3198457,
author = {Fallahzadeh, Ramin and Ghasemzadeh, Hassan},
title = {Trading Off Power Consumption and Prediction Performance in Wearable Motion Sensors: An Optimal and Real-Time Approach},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3198457},
doi = {10.1145/3198457},
abstract = {Power consumption is identified as one of the main complications in designing practical wearable systems, mainly due to their stringent resource limitations. When designing wearable technologies, several system-level design choices, which directly contribute to the energy consumption of these systems, must be considered. In this article, we propose a computationally lightweight system optimization framework that trades off power consumption and performance in connected wearable motion sensors. While existing approaches exclusively focus on one or a few hand-picked design variables, our framework holistically finds the optimal power-performance solution with respect to the specified application need. Our design tackles a multi-variant non-convex optimization problem that is theoretically hard to solve. To decrease the complexity, we propose a smoothing function that reduces this optimization to a convex problem. The reduced optimization is then solved in linear time using a devised derivative-free optimization approach, namely cyclic coordinate search. We evaluate our framework against several holistic optimization baselines using a real-world wearable activity recognition dataset. We minimize the energy consumption for various activity-recognition performance thresholds ranging from 40% to 80% and demonstrate up to 64% energy savings.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = oct,
articleno = {67},
numpages = {23},
keywords = {wearable monitoring systems, energy optimization, body sensor networks, activity recognition, Embedded systems}
}

@inproceedings{10.1145/3368826.3377922,
author = {Chen, Yu and Peng, Ivy B. and Peng, Zhen and Liu, Xu and Ren, Bin},
title = {ATMem: adaptive data placement in graph applications on heterogeneous memories},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377922},
doi = {10.1145/3368826.3377922},
abstract = {Active development in new memory devices, such as non-volatile memories and high-bandwidth memories, brings heterogeneous memory systems (HMS) as a promising solution for implementing large-scale memory systems with cost, area, and power limitations. Typical HMS consists of a small-capacity high-performance memory and a large-capacity low-performance memory. Data placement on such systems plays a critical role in performance optimization. Existing efforts have explored coarse-grained data placement in applications with dense data structures; however, a thorough study of applications that are based on graph data structures is still missing. This work proposes ATMem—a runtime framework for adaptive granularity data placement optimization in graph applications. ATMem consists of a lightweight profiler, an analyzer using a novel m-ary tree-based strategy to identify sampled and estimated critical data chunks, and a high-bandwidth migration mechanism using a multi-stage multi-threaded approach. ATMem is evaluated in five applications on two HMS hardware, including the Intel Optane byte-addressable NVM and MCDRAM. Experimental results show that ATMem selects 5%-18% data to be placed on high-performance memory and achieves an average of 1.7\texttimes{}-3.4\texttimes{} speedup on NVM-DRAM and 1.2\texttimes{}-2.0\texttimes{} speedup on MCDRAM-DRAM, over the baseline that places all data on the large-capacity memory. On NVM-DRAM, ATMem achieves performance comparable to a full-DRAM system with as low as 9%-54% slowdown.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {293–304},
numpages = {12},
keywords = {Data Place-ment, Graph Application, Heterogeneous Memory Systems},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@article{10.1016/j.jss.2015.09.001,
title = {Qualitative optimization in software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.001},
doi = {10.1016/j.jss.2015.09.001},
abstract = {Some problems in software optimization involve qualitative (subjective) human input.Quantitative optimization uses heuristics/surrogates to quantify qualitative input.Qualitative optimization allows incomparability among solutions to find optimal one.Qualitative preference reasoning languages developed in AI are potentially useful.SBSE &amp; recommender systems can readily leverage such qualitative formalisms. Many software engineering problems involve finding optimal solutions from a set of feasible solutions. Such methods often require stakeholders such as developers and testers to specify preferences over multiple attributes/objectives that are to be optimized. However, in many cases it is more natural for stakeholders to express such preferences in simple, qualitative terms. We survey relevant literature within software engineering for problems in which qualitative optimization techniques can be useful. We also present a model of optimization that relies on the stakeholders qualitative preferences leveraging recent advances in decision theoretic artificial intelligence, which could prove useful and spawn connections between qualitative decision theory and software engineering.},
journal = {J. Syst. Softw.},
month = jan,
pages = {149–156},
numpages = {8}
}

@inproceedings{10.1145/3480433.3480447,
author = {Selitskiy, Stanislav and Christou, Nikolaos and Selitskaya, Natalya},
title = {Isolating Uncertainty of the Face Expression Recognition with the Meta-Learning Supervisor Neural Network},
year = {2021},
isbn = {9781450384148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480433.3480447},
doi = {10.1145/3480433.3480447},
abstract = {We investigate whether the well-known poor performance of the head-on usage of the convolutional neural networks for the facial expression recognition task may be improved in terms of reducing the false positive and false negative errors. An uncertainty isolating technique is used that introduces an additional “unknown” class. A self-attention supervisor artificial neural network is used to “learn about learning” of the underlying convolutional neural networks, in particular, to learn patterns of the underlying neural network parameters that accompany wrong or correct verdicts. A novel data set containing artistic makeup and occlusions images is used to aggravate the problem of the training data not representing the test data distribution.},
booktitle = {2021 5th International Conference on Artificial Intelligence and Virtual Reality (AIVR)},
pages = {104–112},
numpages = {9},
keywords = {Uncertainty isolation, Self-attention, Meta-learning, Face expression recognition},
location = {Kumamoto, Japan},
series = {AIVR 2021}
}

@article{10.1016/j.compag.2021.106406,
author = {Niloofar, Parisa and Francis, Deena P. and Lazarova-Molnar, Sanja and Vulpe, Alexandru and Vochin, Marius-Constantin and Suciu, George and Balanescu, Mihaela and Anestis, Vasileios and Bartzanas, Thomas},
title = {Data-driven decision support in livestock farming for improved animal health, welfare and greenhouse gas emissions: Overview and challenges},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.106406},
doi = {10.1016/j.compag.2021.106406},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {16},
keywords = {Precision livestock farming, Modelling and simulation, GHG emission, Data analytics, Data-driven decision support}
}

@article{10.14778/3476311.3476362,
author = {Sen, Rathijit and Roy, Abhishek and Jindal, Alekh and Fang, Rui and Zheng, Jeff and Liu, Xiaolei and Li, Ruiping},
title = {AutoExecutor: predictive parallelism for spark SQL queries},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476362},
doi = {10.14778/3476311.3476362},
abstract = {Right-sizing resources for query execution is important for cost-efficient performance, but estimating how performance is affected by resource allocations, upfront, before query execution is difficult. We demonstrate AutoExecutor, a predictive system that uses machine learning models to predict query run times as a function of the number of allocated executors, that limits the maximum allowed parallelism, for Spark SQL queries running on Azure Synapse.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2855–2858},
numpages = {4}
}

@article{10.1007/s10825-020-01577-4,
author = {Damodaran, V. and Choudhury, Kaustav and Ghosh, Kaustab},
title = {Modelling and simulation of carrier transport in quantum dot memory device for longer data retention and minimized power consumption},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1569-8025},
url = {https://doi.org/10.1007/s10825-020-01577-4},
doi = {10.1007/s10825-020-01577-4},
abstract = {The performance of a group&nbsp;III–V material quantum dot (QD) nanostructure memory is investigated using a self-consistent Schr\"{o}dinger solver, eight-band k·p model, and carrier dynamics modelling. This model is used to explore the information loss due to the carrier emission rate in the QDs as a function of temperature, size and confinement potential. The results reveal the dominant emission mechanisms that should occur at different operating temperatures. To minimize the loss and improve the performance at room temperature, our findings reveal an increase in the carrier storage time and a reduction in the power dissipation with increasing dot size. It is further illustrated that electrons are advantageous as information carriers over holes and that the inclusion of high-bandgap barrier layers favours longer-duration data retention. The model is extended to include trap states in realistic QDs, whose effect is found to become more prominent with performance optimization. The computed results are in close agreement with other experimental data for different QDs along with barrier layer. This validates the efficacy of the model, which can be utilized as a design tool for fabricating nanoscale memories with better data retention capability.},
journal = {J. Comput. Electron.},
month = feb,
pages = {178–194},
numpages = {17},
keywords = {Information storage, Power dissipation, Tunnelling emission, Carrier dynamics, Memory device, Quantum dots}
}

@article{10.1007/s11227-018-2448-9,
author = {Gazzarri, Leonardo and Danelutto, Marco},
title = {Supporting structured parallel program design, development and tuning in FastFlow
},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2448-9},
doi = {10.1007/s11227-018-2448-9},
abstract = {The design and tuning of parallel programs is known to be a hard and error-prone process. Structured parallel programming helps overcoming part of the related problems by properly and carefully ensuring separation of concerns in between application and system programmers. In this work, we describe the design and the implementation of ff-RPL, a shell supporting structured parallel programming development in FastFlow. The shell provides ways to explore the space of functionally equivalent, alternative parallel implementations of the same application with different non functional properties. It also provides ways to tune and optimize existing parallel applications standalone or while targeting particular hardware architectures. The tool is entirely written in C++. It has been designed in such a way it can be easily extended to take into account new non functional features, refactoring and optimization rules, as well as different parallel patterns. Experimental results are shown validating the general ff-RPL design as well as its FastFlow code generation features.},
journal = {J. Supercomput.},
month = aug,
pages = {4026–4041},
numpages = {16},
keywords = {Code refactoring, Algorithmic skeletons, Parallel patterns}
}

@inproceedings{10.1145/3459104.3459194,
author = {Peng, Daowan and Chen, Zizhong and Fu, Jingcheng and Xia, Shuyin and Wen, Qing},
title = {Fast k-means Clustering Based on the Neighbor Information},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459194},
doi = {10.1145/3459104.3459194},
abstract = {The k-means algorithm has been widely used in the last several decades, but the efficiency of Lloyd's k-means algorithm drops sharply in dealing with large-scale data scenarios. To solve this problem, this paper proposes a fast k-means algorithm based on neighbor information. Firstly, we propose a localization strategy in the reassignment step of k-means. Through this strategy, the scale of distance calculation is greatly reduced. Secondly, we propose the neighbor update strategy. In such a way, more accurate neighbors for each cluster could be found in each iteration, thereby ensuring the clustering quality when the k-means algorithm converges. The proposed k-means algorithm was evaluated on multiple real-world datasets and increased the speed up to hundreds of times while only losing about 1.10% of the clustering result quality.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {551–555},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3341105.3374224,
author = {Ferreira, Mafalda Falc\~{a}o},
title = {Extracting architectural patterns of deep neural networks for disease detection: student research abstract},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374224},
doi = {10.1145/3341105.3374224},
abstract = {The importance of early detection of diseases with high-mortality is crucial to save lives. Deep Learning algorithms are recurrently used by many researchers that aim to model the progression and treatment of these conditions.There is growing evidence that the complexity of a Deep Learning model is correlated to its performance: the deeper the network, the more accurate it is. However, as the topology deepens, training gets more demanding: (1) increased need of data, (2) increased computational costs, and (3) increased time for evaluation, fine-tuning, and subsequent feedback-based activities inherent to Data Science, with direct impact on the exploration towards finding the best model, due to an inherent trial-and-error approach.We hypothesize that there exist (domain-specific) architectural patterns that, if applied during the model exploration phase, allow an overall improvement of the training performance. Should it be true, it would significantly reduce the exploration phase length, contributing to both Medicine and Computer Science fields.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1150–1153},
numpages = {4},
keywords = {patterns, neural networks, disease detection, deep learning, classification},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/2911451.2914709,
author = {Roegiest, Adam and Cormack, Gordon V.},
title = {Impact of Review-Set Selection on Human Assessment for Text Classification},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914709},
doi = {10.1145/2911451.2914709},
abstract = {In a laboratory study, human assessors were significantly more likely to judge the same documents as relevant when they were presented for assessment within the context of documents selected using random or uncertainty sampling, as compared to relevance sampling. The effect is substantial and significant [0.54 vs. 0.42, p&lt;0.0002] across a population of documents including both relevant and non-relevant documents, for several definitions of ground truth. This result is in accord with Smucker and Jethani's SIGIR 2010 finding that documents were more likely to be judged relevant when assessed within low-precision versus high-precision ranked lists. Our study supports the notion that relevance is malleable, and that one should take care in assuming any labeling to be ground truth, whether for training, tuning, or evaluating text classifiers.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {861–864},
numpages = {4},
keywords = {assessor error, ediscovery, electronic discovery, evaluation, recall, supervised learning, user study},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@article{10.1016/j.peva.2008.10.006,
author = {Mukherjee, Tridib and Gupta, Sandeep K. S. and Varsamopoulos, Georgios},
title = {Energy optimization for proactive unicast route maintenance in MANETs under end-to-end reliability requirements},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {3–5},
issn = {0166-5316},
url = {https://doi.org/10.1016/j.peva.2008.10.006},
doi = {10.1016/j.peva.2008.10.006},
abstract = {Many time-critical applications for Mobile Ad hoc NETworks (MANETs), such as the military applications and disaster response, call for proactive link and route maintenance to ensure low latency for reliable data delivery. The goal of this paper is to minimize the energy overhead due to the high control traffic caused by the periodic route and link maintenance operations in the proactive routing protocols for MANETs. This paper - (i) categorizes the proactive protocols based on the maintenance operations performed; (ii) derives analytical estimates of the optimum route and link update periods for the different protocol classes by considering (a) the data traffic intensity, (b) link dynamics, (c) target reliability, measured in terms of Packet Delivery Ratio (PDR), and (d) the network size; and (iii) proposes a network layer dynamic Optimization of Periodic Timers (OPT) method based on the analytical estimates to locally vary the update periods in the distributed nodes. Simulation results show that DSDV-Opt, a variation of DSDV protocol using OPT, - (i) achieves the target PDR with 98.7% accuracy while minimizing the overhead energy; (ii) improves the protocol scalability; and (iii) reduces the control traffic for low data traffic intensity.},
journal = {Perform. Eval.},
month = mar,
pages = {141–157},
numpages = {17},
keywords = {Reliability, Proactive routing, Performance optimization, MANETs, Energy efficiency, Analytical modeling}
}

@article{10.1016/j.comnet.2016.06.016,
author = {Goussevskaia, Olga and Vieira, Luiz F.M. and Vieira, Marcos A.M.},
title = {Wireless scheduling with multiple data rates: From physical interference to disk graphs},
year = {2016},
issue_date = {Sep 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2016.06.016},
doi = {10.1016/j.comnet.2016.06.016},
journal = {Comput. Netw.},
month = sep,
pages = {64–76},
numpages = {13},
keywords = {Disk graph, Approximation algorithms, SINR, Scheduling, Ad-Hoc networks, Wireless communication}
}

@inproceedings{10.1145/3448734.3450809,
author = {He, Yuan and Zhao, Xiaowei and Guo, Runze and Gan, Xusheng},
title = {Multi-resolution Wavelet Neural Network Learning Algorithm Based on Artificial Fish Swarm Algorithm},
year = {2021},
isbn = {9781450389570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448734.3450809},
doi = {10.1145/3448734.3450809},
abstract = {For the shortcoming on the algorithm of Wavelet Neural Network (WNN), a WNN learning algorithm based on Artificial Fish Swarm Algorithm (AFSA) is proposed. In the method, AFSA is used to train the WNN model. The simulation result show that, the proposed method has a good ability with features of modeling accuracy, training time, controlling precocity and local convergence, and it is also effective and feasible for the modeling of nonlinear complex system.},
booktitle = {The 2nd International Conference on Computing and Data Science},
articleno = {79},
numpages = {5},
keywords = {Wavelet Neural Network, Piecewise Function, Learning Algorithm, Artificial Fish Swarm Algorithm},
location = {Stanford, CA, USA},
series = {CONF-CDS 2021}
}

@inproceedings{10.5555/1789574.1789602,
author = {Dhir, Chandra Shekhar and Lee, Soo Young},
title = {Discriminant independent component analysis},
year = {2009},
isbn = {3642043933},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, a new approach for extraction of discriminative and independent features is proposed. The proposed discriminant ICA (dICA) method jointly maximizes the inter-class variance and Negentropy of a given feature. Experimental results shows much improved classification performance when dICA features are used for recognition tasks over conventional ICA features. Moreover, dICA features show higher Fisher criterion score value suggesting a better capability to do class discrimination.},
booktitle = {Proceedings of the 10th International Conference on Intelligent Data Engineering and Automated Learning},
pages = {219–225},
numpages = {7},
location = {Burgos, Spain},
series = {IDEAL'09}
}

@article{10.1016/j.neunet.2015.03.009,
author = {Capecci, Elisa and Kasabov, Nikola and Wang, Grace Y.},
title = {Analysis of connectivity in NeuCube spiking neural network models trained on EEG data for the understanding of functional changes in the brain},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {68},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2015.03.009},
doi = {10.1016/j.neunet.2015.03.009},
abstract = {The paper presents a methodology for the analysis of functional changes in brain activity across different conditions and different groups of subjects. This analysis is based on the recently proposed NeuCube spiking neural network (SNN) framework and more specifically on the analysis of the connectivity of a NeuCube model trained with electroencephalography (EEG) data. The case study data used to illustrate this method is EEG data collected from three groups-subjects with opiate addiction, patients undertaking methadone maintenance treatment, and non-drug users/healthy control group. The proposed method classifies more accurately the EEG data than traditional statistical and artificial intelligence (AI) methods and can be used to predict response to treatment and dose-related drug effect. But more importantly, the method can be used to compare functional brain activities of different subjects and the changes of these activities as a result of treatment, which is a step towards a better understanding of both the EEG data and the brain processes that generated it. The method can also be used for a wide range of applications, such as a better understanding of disease progression or aging.},
journal = {Neural Netw.},
month = aug,
pages = {62–77},
numpages = {16},
keywords = {Spiking neural networks, Response to treatment, Opiates, NeuCube, Methadone maintenance, EEG}
}

@article{10.3233/JIFS-179958,
author = {Xu, Nianli and Liu, Fengying and Patnaik, Srikanta},
title = {Application of image content feature retrieval based on deep learning in sports public industry},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179958},
doi = {10.3233/JIFS-179958},
abstract = {The image content retrieval can effectively promote the development of the entire industry. At present, sports competition is becoming more and more fierce, and the requirements for image content retrieval are getting higher and higher. In this paper, research has been carried out on image descriptor generation, image feature quantization and coding, accurate nearest neighbor cluster center fast search, multi-dimensional inverted index construction and fast retrieval. Moreover, based on deep learning, this paper constructed an effective detection algorithm for the characteristics of sports images, and compared the image shape and color as examples. It can be seen from the comparative study that the research method of this paper can effectively reduce the size of the candidate set of query results without affecting the accuracy of the query, which is of great significance for improving the speed of image query and has certain significance for promoting the development of sports public industry.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1867–1877},
numpages = {11},
keywords = {image feature analysis, sports industry, feature extraction, image content retrieval, Deep learning}
}

@article{10.1145/3418055,
author = {Liou, Jhe-Yu and Wang, Xiaodong and Forrest, Stephanie and Wu, Carole-Jean},
title = {GEVO: GPU Code Optimization Using Evolutionary Computation},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3418055},
doi = {10.1145/3418055},
abstract = {GPUs are a key enabler of the revolution in machine learning and high-performance computing, functioning as de facto co-processors to accelerate large-scale computation. As the programming stack and tool support have matured, GPUs have also become accessible to programmers, who may lack detailed knowledge of the underlying architecture and fail to fully leverage the GPU’s computation power. GEVO (Gpu optimization using EVOlutionary computation) is a tool for automatically discovering optimization opportunities and tuning the performance of GPU kernels in the LLVM representation. GEVO uses population-based search to find edits to GPU code compiled to LLVM-IR and improves performance on desired criteria while retaining required functionality. We demonstrate that GEVO improves the execution time of general-purpose GPU programs and machine learning (ML) models on NVIDIA Tesla P100. For the Rodinia benchmarks, GEVO improves GPU kernel runtime performance by an average of 49.48% and by as much as 412% over the fully compiler-optimized baseline. If kernel output accuracy is relaxed to tolerate up to 1% error, GEVO can find kernel variants that outperform the baseline by an average of 51.08%. For the ML workloads, GEVO achieves kernel performance improvement for SVM on the MNIST handwriting recognition (3.24\texttimes{}) and the a9a income prediction (2.93\texttimes{}) datasets with no loss of model accuracy. GEVO achieves 1.79\texttimes{} kernel performance improvement on image classification using ResNet18/CIFAR-10, with less than 1% model accuracy reduction.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {33},
numpages = {28},
keywords = {multi-objective evolutionary computation, approximate computing, LLVM intermediate representation, Genetic improvement, GPU code optimization}
}

@article{10.1016/j.neucom.2021.04.070,
author = {Yang, Zhao and Liu, Jiehao and Liu, Tie and Zhu, Yuanxin and Wang, Li and Tao, Dapeng},
title = {Equidistant distribution loss for person re-identification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {455},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.070},
doi = {10.1016/j.neucom.2021.04.070},
journal = {Neurocomput.},
month = sep,
pages = {255–264},
numpages = {10},
keywords = {Imbalance learning, Equidistant distribution loss, Person re-identification}
}

@article{10.1016/j.rcim.2012.11.008,
author = {Tansel undefined\c{c}, Yusuf and Yurdakul, Mustafa and Dengiz, Berna},
title = {Development of a decision support system for robot selection},
year = {2013},
issue_date = {August, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {29},
number = {4},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2012.11.008},
doi = {10.1016/j.rcim.2012.11.008},
abstract = {With the availability of more different robot types and models along with their separate specifications, selecting the most appropriate robot is becoming more difficult and complicated for companies. Furthermore, a common set of robot selection criteria is not available for the decision makers. In this study, a two-phase robot selection decision support system, namely ROBSEL, is developed to help the decision makers in their robot selection decisions. In development of ROBSEL, an independent set of criteria is obtained first and arranged in the Fuzzy Analytical Hierarchy Process (FAHP) decision hierarchy. In the first elimination phase of the decision support system, the user obtains the feasible set of robots by providing limited values for the 15 requirements. ROBSEL, then, uses FAHP decision hierarchy to rank the feasible robots in the second phase. ROBSEL is illustrated and tested and several critical issues in its practical usage are explored in the paper. The applications of ROBSEL show that ROBSEL is a useful, practical and easy to use robot selection tool and improves robot selection decisions in the companies.},
journal = {Robot. Comput.-Integr. Manuf.},
month = aug,
pages = {142–157},
numpages = {16},
keywords = {Robot selection, Industrial robot, Fuzzy AHP, Correlation test}
}

@article{10.4018/IJEHMC.20211101.oa8,
author = {Hassan, Musavir and Butt, Muheet Ahmed and Zaman, Majid},
title = {An Ensemble Random Forest Algorithm for Privacy Preserving Distributed Medical Data Mining},
year = {2021},
issue_date = {Sep 2021},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {6},
issn = {1947-315X},
url = {https://doi.org/10.4018/IJEHMC.20211101.oa8},
doi = {10.4018/IJEHMC.20211101.oa8},
abstract = {As the voluminous amount of data is generated because of inexorably widespread proliferation of electronic data maintained using the Electronic Health Records (EHRs). Medical health facilities have great potential to discern the patterns from this data and utilize them in diagnosing a specific disease or predicting outbreak of an epidemic etc. This discern of patterns might reveal sensitive information about individuals and this information is vulnerable to misuse. This is, however, a challenging task to share such sensitive data as it compromises the privacy of patients. In this paper, a random forest-based distributed data mining approach is proposed. Performance of the proposed model is evaluated using accuracy, f-measure and appa statistics analysis. Experimental results reveal that the proposed model is efficient and scalable enough in both performance and accuracy within the imbalanced data and also in maintaining the privacy by sharing only useful healthcare knowledge in the form of local models without revealing and sharing of sensitive data.},
journal = {Int. J. E-Health Med. Commun.},
month = nov,
pages = {1–23},
numpages = {23},
keywords = {Random Forest, Privacy Preserving, Horizontally Partitioned Data, Healthcare, F-Measure, Decentralized Data Mining}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Feature selection, Feature ranking, Data mining, Classification}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@inproceedings{10.1145/3400302.3415657,
author = {Ustun, Ecenur and Deng, Chenhui and Pal, Debjit and Li, Zhijing and Zhang, Zhiru},
title = {Accurate operation delay prediction for FPGA HLS using graph neural networks},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415657},
doi = {10.1145/3400302.3415657},
abstract = {Modern heterogeneous FPGA architectures incorporate a variety of hardened blocks for boosting the performance of arithmetic-intensive designs, such as DSP blocks and carry blocks. Since hardened blocks can be configured in different ways, a variety of datapath patterns can be mapped into these blocks. We observe that existing high-level synthesis (HLS) tools often fail to capture some of the operation mapping patterns, leading to limited estimation accuracy in terms of resource usage and delay. To address this deficiency, we propose to exploit graph neural networks (GNN) to automatically learn operation mapping patterns. We apply GNN models that are trained on microbenchmarks directly to realistic designs through inductive learning. Experimental results show that our approach can effectively infer various valid mapping patterns on both microbenchmarks and realistic designs. Furthermore, the proposed framework is exploited to improve the accuracy of delay estimation in HLS.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {87},
numpages = {9},
keywords = {high-level synthesis, graph neural networks, FPGAs},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1109/TC.2011.156,
author = {Hanumaiah, Vinay and Vrudhula, Sarma},
title = {Temperature-Aware DVFS for Hard Real-Time Applications on Multicore Processors},
year = {2012},
issue_date = {October 2012},
publisher = {IEEE Computer Society},
address = {USA},
volume = {61},
number = {10},
issn = {0018-9340},
url = {https://doi.org/10.1109/TC.2011.156},
doi = {10.1109/TC.2011.156},
abstract = {This paper addresses the problem of determining the feasible speeds and voltages of multicore processors with hard real-time and temperature constraints. This is an important problem, which has applications in time-critical execution of programs like audio and video encoding on application-specific embedded processors. Two problems are solved. The first is the computation of the optimal time-varying voltages and speeds of each core in a heterogeneous multicore processor, that minimize the makespan—the latest completion time of all tasks, while satisfying timing and temperature constraints. The solution to the makespan minimization problem is then extended to the problem of determining the feasible speeds and voltages that satisfy task deadlines. The methods presented in this paper also provide a theoretical basis and analytical relations between speed, voltage, power and temperature, which provide greater insight into the early-phase design of processors and are also useful for online dynamic thermal management.},
journal = {IEEE Trans. Comput.},
month = oct,
pages = {1484–1494},
numpages = {11},
keywords = {thermal management, task deadlines, real-time, performance optimization, optimal control., makespan minimization, leakage dependence on temperature, dynamic voltage and frequency scaling, Voltage control, Temperature dependence, Program processors, Optimization, Multicore processing, Multicore, Minimization, Equations}
}

@article{10.5555/1005332.1005334,
author = {Lanckriet, Gert R. G. and Cristianini, Nello and Bartlett, Peter and Ghaoui, Laurent El and Jordan, Michael I.},
title = {Learning the Kernel Matrix with Semidefinite Programming},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {27–72},
numpages = {46}
}

@article{10.1016/j.comnet.2021.108511,
author = {Mao, Qian and Zhang, Lin and Hu, Fei and Bentley, Elizabeth Serena and Kumar, Sunil},
title = {Deep Learning (DL)-based adaptive transport layer control in UAV Swarm Networks},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108511},
doi = {10.1016/j.comnet.2021.108511},
journal = {Comput. Netw.},
month = dec,
numpages = {11},
keywords = {UAV Networks, Transport Layer, Network coding, Deep Learning (DL), Congestion control}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@article{10.1016/j.jnca.2019.102408,
author = {Yang, Bo and Li, Bo and Yan, Zhongjiang and Deng, Der-Jiunn and Yang, Mao},
title = {Performance analysis of multi-channel MAC with single transceiver for the next generation WLAN},
year = {2019},
issue_date = {Nov 2019},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {146},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2019.102408},
doi = {10.1016/j.jnca.2019.102408},
journal = {J. Netw. Comput. Appl.},
month = nov,
numpages = {17},
keywords = {The next generation WLAN, Channel reservation, Medium access control, Multichannel}
}

@inproceedings{10.5555/3172077.3172181,
author = {Li, Hao and Gong, Maoguo},
title = {Self-paced convolutional neural networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks. In order to distinguish the reliable data from the noisy and confusing data, we improve CNNs with self-paced learning (SPL) for enhancing the learning robustness of CNNs. In the proposed self-paced convolutional network (SPCN), each sample is assigned to a weight to reflect the easiness of the sample. Then a dynamic self-paced function is incorporated into the leaning objective of CNN to jointly learn the parameters of CNN and the latent weight variable. SPCN learns the samples from easy to complex and the sample weights can dynamically control the learning rates for converging to better values. To gain more insights of SPCN, theoretical studies are conducted to show that SPCN converges to a stationary solution and is robust to the noisy and confusing data. Experimental results on MNIST and  rectangles  datasets demonstrate that the proposed method outperforms baseline methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2110–2116},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1007/978-3-030-49435-3_29,
author = {Reinhartz-Berger, Iris and Abbas, Sameh},
title = {A Variability-Driven Analysis Method for Automatic Extraction of Domain Behaviors},
year = {2020},
isbn = {978-3-030-49434-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49435-3_29},
doi = {10.1007/978-3-030-49435-3_29},
abstract = {Domain engineering focuses on modeling knowledge in an application domain for supporting systematic reuse in the context of complex and constantly evolving systems. Automatically supporting this task is challenging; most existing methods assume high similarity of variants which limits reuse of the generated domain artifacts, or provide very low-level features rather than actual domain features. As a result, these methods are limited in handling common scenarios such as similarly behaving systems developed by different teams, or merging existing products. To address this gap, we propose a method for extracting domain knowledge in the form of domain behaviors, building on a previously developed framework for behavior-based variability analysis among class operations. Machine learning techniques are applied for identifying clusters of operations that can potentially form domain behaviors. The approach is evaluated on a set of open-source video games, named apo-games.},
booktitle = {Advanced Information Systems Engineering: 32nd International Conference, CAiSE 2020, Grenoble, France, June 8–12, 2020, Proceedings},
pages = {467–481},
numpages = {15},
keywords = {Domain engineering, Systematic reuse, Variability analysis},
location = {Grenoble, France}
}

@inproceedings{10.5555/3382225.3382412,
author = {Xylogiannopoulos, Konstantinos F.},
title = {From data points to data curves: a new approach on big data curves clustering},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {In the new era of IoT, enormous real-values datasets are produced daily. Time series created by smart devices, financial data, weather analysis, medical applications, traffic control etc. become more and more important in human day life. Analyzing and clustering these time series or in general any kind of curve could be critical. In the current paper, a new methodology (BD2C) is presented, which applies text mining and pattern detection techniques in order to cluster curves according to their shape. Several experiments have been conducted on artificial and real datasets in order to present the accuracy, efficiency and rapid discovery of the best possible clustering that the proposed methodology can achieve.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {881–884},
numpages = {4},
keywords = {pattern detection, multivariate data analytics, curve clustering, LERP-RSA, BD2C, ARPaD},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1007/s11276-016-1293-0,
author = {Plets, David and Chemmangat, Krishnan and Deschrijver, Dirk and Mehari, Michael and Ulaganathan, Selvakumar and Pakparvar, Mostafa and Dhaene, Tom and Hoebeke, Jeroen and Moerman, Ingrid and Tanghe, Emmeric},
title = {Surrogate modeling based cognitive decision engine for optimization of WLAN performance},
year = {2017},
issue_date = {November  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {8},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-016-1293-0},
doi = {10.1007/s11276-016-1293-0},
abstract = {Due to the rapid growth of wireless networks and the dearth of the electromagnetic spectrum, more interference is imposed to the wireless terminals which constrains their performance. In order to mitigate such performance degradation, this paper proposes a novel experimentally verified surrogate model based cognitive decision engine which aims at performance optimization of IEEE 802.11 links. The surrogate model takes the current state and configuration of the network as input and makes a prediction of the QoS parameter that would assist the decision engine to steer the network towards the optimal configuration. The decision engine was applied in two realistic interference scenarios where in both cases, utilization of the cognitive decision engine significantly outperformed the case where the decision engine was not deployed.},
journal = {Wirel. Netw.},
month = nov,
pages = {2347–2359},
numpages = {13},
keywords = {WiFi, WLAN, Surrogate modeling, Interference management, Dynamic spectrum access, Cognitive decision engine}
}

@article{10.3233/JIFS-169565,
author = {Han, Ying and Yu, Xiao Qiang and Li, Xu and Yao, Chun Long and Zhao, Xin and Patnaik, Srikanta},
title = {Multiple query optimization approach based on hive+1},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {35},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169565},
doi = {10.3233/JIFS-169565},
abstract = {To improve the processing efficiency on batch query for MapReduce, a multiple query optimization approach based on Hive+ is proposed to reduce the number of MapReduce tasks on multiple query, decrease the start time of MapReduce task and the overhead of fault tolerance, improve the query efficiency. TPC-H benchmark test set is selected as the use cases to experiment on Hive-0.12. The experiment shows that the processing efficiency of batch query is effectively improved.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {47–53},
numpages = {7},
keywords = {MapReduce, inter-query flow analysis, multiple query optimization, Hive+}
}

@inproceedings{10.1109/MODELS.2017.22,
author = {Taentzer, Gabriele and Salay, Rick and Str\"{u}ber, Daniel and Chechik, Marsha},
title = {Transformations of software product lines: a generalizing framework based on category theory},
year = {2017},
isbn = {9781538634929},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS.2017.22},
doi = {10.1109/MODELS.2017.22},
abstract = {Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.},
booktitle = {Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
pages = {101–111},
numpages = {11},
location = {Austin, Texas},
series = {MODELS '17}
}

@article{10.1016/j.cor.2004.06.008,
author = {El-Zoghdy, Said Fathy and Kameda, Hisao and Li, Jie},
title = {Numerical studies on a paradox for non-cooperative static load balancing in distributed computer systems},
year = {2006},
issue_date = {February 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {33},
number = {2},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2004.06.008},
doi = {10.1016/j.cor.2004.06.008},
abstract = {Numerical examples of a Braess-like paradox in which adding capacity to a distributed computer system may degrade the performance of all users in the system under non-cooperative optimization have been reported. Unlike the original Braess paradox, in the models examined, this behavior occurs only in the case of finitely many users and not in the case of infinite number of users and the degree of performance degradation can increase without bound. This study examines numerically some examples around the Braess-like paradox in a distributed computer system. In the numerical examples, it is observed that the worst-case degree of the paradox (WCDP) is largest in complete symmetry. The dependence of the WCDP on some system parameters is also examined.},
journal = {Comput. Oper. Res.},
month = feb,
pages = {345–355},
numpages = {11},
keywords = {Wardrop equilibrium, Performance optimization, Numerical examples, Nash equilibrium, Load balancing, Distributed computer system, Braess paradox}
}

@article{10.1007/s10115-018-1166-1,
author = {Wang, Chengyu and Fan, Yan and He, Xiaofeng and Zhou, Aoying},
title = {Predicting hypernym---hyponym relations for Chinese taxonomy learning},
year = {2019},
issue_date = {March     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {58},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-018-1166-1},
doi = {10.1007/s10115-018-1166-1},
abstract = {Hypernym---hyponym ("is-a") relations are key components in taxonomies, object hierarchies and knowledge graphs. Robustly harvesting of such relations requires the analysis of the linguistic characteristics of is-a word pairs in the target language. While there is abundant research on is-a relation extraction in English, it still remains a challenge to accurately identify such relations from Chinese knowledge sources due to the flexibility of language expression and the significant differences between the two language families. In this paper, we introduce a weakly supervised framework to extract Chinese is-a relations from user-generated categories. It employs piecewise linear projection models trained on an existing Chinese taxonomy built from Wikipedia and an iterative learning algorithm to update model parameters incrementally. A pattern-based relation selection method is proposed to prevent "semantic drift" in the learning process using bi-criteria optimization. Experimental results on the publicly available test set illustrate that the proposed approach outperforms state-of-the-art methods.},
journal = {Knowl. Inf. Syst.},
month = mar,
pages = {585–610},
numpages = {26},
keywords = {Word embedding, Weakly supervised learning, User-generated categories, Taxonomy expansion, Hypernym---hyponym relation extraction}
}

@inproceedings{10.5555/3495724.3496497,
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {Self-paced deep reinforcement learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {773},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.5555/2009542.2009578,
author = {Mart\'{\i}nez-Zarzuela, M. and D\'{\i}az-Pernas, F. J. and Ant\'{o}n-Rodr\'{\i}guez, M. and Perozo-Rond\'{o}n, F. and Gonz\'{a}lez-Ortega, D.},
title = {AdaBoost face detection on the gpu using Haar-like features},
year = {2011},
isbn = {9783642213250},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Face detection is a time consuming task in computer vision applications. In this article, an approach for AdaBoost face detection using Haar-like features on the GPU is proposed. The GPU adapted version of the algorithm manages to speed-up the detection process when compared with the detection performance of the CPU using a well-known computer vision library. An overall speed-up of \texttimes{} 3.3 is obtained on the GPU for video resolutions of 640\texttimes{}480 px when compared with the CPU implementation. Moreover, since the CPU is idle during face detection, it can be used simultaneously for other computer vision tasks.},
booktitle = {Proceedings of the 4th International Conference on Interplay between Natural and Artificial Computation: New Challenges on Bioinspired Applications - Volume Part II},
pages = {333–342},
numpages = {10},
keywords = {haar-like features, face detection, adaboost, OpenGL, GPU, CUDA},
location = {Canary Islands, Spain},
series = {IWINAC'11}
}

@article{10.1016/j.knosys.2020.106660,
author = {Liu, Zhen and Feng, Xiaodong and Wang, Yecheng and Zuo, Wenbo},
title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106660},
doi = {10.1016/j.knosys.2020.106660},
journal = {Know.-Based Syst.},
month = feb,
numpages = {12},
keywords = {Self-paced learning, Instance weighting, Noisy and outlier corruption, Deep learning, Recommendation}
}

@inproceedings{10.1145/3240508.3240648,
author = {Zheng, Xiaoju and Zha, Zheng-Jun and Zhuang, Liansheng},
title = {A Feature-Adaptive Semi-Supervised Framework for Co-saliency Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240648},
doi = {10.1145/3240508.3240648},
abstract = {Co-saliency detection, which refers to the discovery of common salient foreground regions in a group of relevant images, has attracted increasing attention due to its widespread applications in many vision tasks. Existing methods assemble features from multiple views toward a comprehensive representation, however overlook the efficacy disparity among various features in detecting co-saliency. This paper proposes a novel feature-adaptive semi-supervised (FASS) framework for co-saliency detection, which seamlessly integrates multi-view feature learning, graph structure optimization and co-saliency prediction in a unified solution. In particular, the FASS exploits the efficacy disparity of multi-view features at both view and element levels by a joint formulation of view-wise feature weighting and element-wise feature selection, leading to an effective representation robust to feature noise and redundancy as well as adaptive to the task at hand. It predicts co-saliency map by optimizing co-saliency label prorogation over a graph of both labeled and unlabeled image regions. The graph structure is optimized jointly with feature learning and co-saliency prediction to precisely characterize underlying correlation among regions. The FASS is thus able to produce satisfactory co-saliency map based on the effective exploration of multi-view features as well as inter-region correlation. Extensive experiments on three benchmark datasets, i.e., iCoseg, Cosal2015 and MSRC, have demonstrated that the proposed FASS outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {959–966},
numpages = {8},
keywords = {semi-supervised learning, multi-view feature, graph optimization, co-saliency detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1155/2021/1058825,
author = {Pham, Tuan Anh and Hern\'{a}ndez-P\'{e}rez, Jos\'{e} Alfredo},
title = {Application of Feedforward Neural Network and SPT Results in the Estimation of Seismic Soil Liquefaction Triggering},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/1058825},
doi = {10.1155/2021/1058825},
abstract = {Soil liquefaction is a dangerous phenomenon for structures that lose their shear strength and soil resistance, occurring during seismic shocks such as earthquakes or sudden stress conditions. Determining the liquefaction and nonliquefaction capacity of soil is a difficult but necessary job when constructing structures in earthquake zones. Usually, the possibility of soil liquefaction is determined by laboratory tests on soil samples subjected to dynamic loads, and this is time-consuming and costly. Therefore, this study focuses on the development of a machine learning model called a Forward Neural Network (FNN) to estimate the activation of soil liquefaction under seismic condition. The database is collected from the published literature, including 270 liquefaction cases and 216 nonliquefaction case histories under different geological conditions and earthquakes used for construction and confirming the model. The model is built and optimized for hyperparameters based on a technique known as random search (RS). Then, the L2 regularization technique is used to solve the overfitting problem of the model. The analysis results are compared with a series of empirical formulas as well as some popular machine learning (ML) models. The results show that the RS-L2-FNN model successfully predicts soil liquefaction with an accuracy of 90.33% on the entire dataset and an average accuracy of 88.4% after 300 simulations which takes into account the random split of the datasets. Compared with the empirical formulas as well as other machine learning models, the RS-L2-FNN model shows superior performance and solves the overfitting problem of the model. In addition, the global sensitivity analysis technique is used to detect the most important input characteristics affecting the activation prediction of liquefied soils. The results show that the corrected SPT resistance (N1)60 is the most important input variable, affecting the determination of the liquefaction capacity of the soil. This study provides a powerful tool that allows rapid and accurate prediction of liquefaction based on several basic soil properties.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {14}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@article{10.1504/ijcse.2021.118108,
author = {Xu, Jiachang and Huang, Yourui and Zhao, Ruijuan and Liu, Yu},
title = {Attitude control of an unmanned patrol helicopter based on an optimised spiking neural membrane system for use in coal mines},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {24},
number = {5},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2021.118108},
doi = {10.1504/ijcse.2021.118108},
abstract = {For the attitude control of unmanned helicopters used in the intelligent patrolling of coal mines, a spiking neural membrane system is introduced for attitude optimisation control. First, a geometry-based attitude dynamics model suitable for coal mine scenarios is constructed. Second, in accordance with the attitude dynamics model, an extended spiking neural membrane system (ESNMS) is constructed, and an optimised spiking neural membrane system (OSNMS) and accompanying algorithm are designed to optimise the ESNMS. Then, the attitude control performance of the developed system is theoretically analysed. Finally, through the simulation of a semiphysical experimental platform, trajectory tracking is effectively realised. Under normal and wind disturbance conditions, comparisons with the traditional synovium controller (TSC) and linear feedback controller (LFC) show that the performance of the designed OSNMS is greatly improved, and the experimental results show that the OSNMS has good stability and effectiveness.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {538–549},
numpages = {11},
keywords = {spiking neural membrane system, membrane computing, attitude control, unmanned helicopter, intelligent coal mining}
}

@article{10.1016/j.jbi.2013.05.005,
author = {Zhu, Qian and Freimuth, Robert R. and Pathak, Jyotishman and Durski, Matthew J. and Chute, Christopher G.},
title = {Disambiguation of PharmGKB drug-disease relations with NDF-RT and SPL},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {46},
number = {4},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2013.05.005},
doi = {10.1016/j.jbi.2013.05.005},
abstract = {We disambiguate PharmGKB drug and disease associations by NDF-RT and SPL.Detailed clinical associations are clearly represented in PharmGKB.The work helps to understand drug and disease relations in details from PharmGKB.Reveals standardized drug information will accelerate clinical drug integration. PharmGKB is a leading resource of high quality pharmacogenomics data that provides information about how genetic variations modulate an individual's response to drugs. PharmGKB contains information about genetic variations, pharmacokinetic and pharmacodynamic pathways, and the effect of variations on drug-related phenotypes. These relationships are represented using very general terms, however, and the precise semantic relationships among drugs, and diseases are not often captured. In this paper we develop a protocol to detect and disambiguate general clinical associations between drugs and diseases using more precise annotation terms from other data sources. PharmGKB provides very detailed clinical associations between genetic variants and drug response, including genotype-specific drug dosing guidelines, and this procedure will armGKB. The availability of more detailed data will help investigators to conduct more precise queries, such as finding particular diseases caused or treated by a specific drug.We first mapped drugs extracted from PharmGKB drug-disease relationships to those in the National Drug File Reference Terminology (NDF-RT) and to Structured Product Labels (SPLs). Specifically, we retrieved drug and disease role relationships describing and defining concepts according to their relationships with other concepts from NDF-RT. We also used the NCBO (National Center for Biomedical Ontology) annotator to annotate disease terms from the free text extracted from five SPL sections (indication, contraindication, ADE, precaution, and warning). Finally, we used the detailed drug and disease relationship information from NDF-RT and the SPLs to annotate and disambiguate the more general PharmGKB drug and disease associations.},
journal = {J. of Biomedical Informatics},
month = aug,
pages = {690–696},
numpages = {7},
keywords = {SPL, Pharmacogenomics, PharmGKB, NDF-RT, Clinical associations}
}

@inproceedings{10.1145/3377930.3390217,
author = {Colas, C\'{e}dric and Madhavan, Vashisht and Huizinga, Joost and Clune, Jeff},
title = {Scaling MAP-Elites to deep neuroevolution},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390217},
doi = {10.1145/3377930.3390217},
abstract = {Quality-Diversity (QD) algorithms, and MAP-Elites (ME) in particular, have proven very useful for a broad range of applications including enabling real robots to recover quickly from joint damage, solving strongly deceptive maze tasks or evolving robot morphologies to discover new gaits. However, present implementations of ME and other QD algorithms seem to be limited to low-dimensional controllers with far fewer parameters than modern deep neural network models. In this paper, we propose to leverage the efficiency of Evolution Strategies (ES) to scale MAP-Elites to high-dimensional controllers parameterized by large neural networks. We design and evaluate a new hybrid algorithm called MAP-Elites with Evolution Strategies (ME-ES) for post-damage recovery in a difficult high-dimensional control task where traditional ME fails. Additionally, we show that ME-ES performs efficient exploration, on par with state-of-the-art exploration algorithms in high-dimensional control tasks with strongly deceptive rewards.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {67–75},
numpages = {9},
keywords = {quality-diversity, map-elites, exploration, evolution strategies},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3474085.3475471,
author = {Huang, Zongmo and Ren, Yazhou and Pu, Xiaorong and He, Lifang},
title = {Non-Linear Fusion for Self-Paced Multi-View Clustering},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475471},
doi = {10.1145/3474085.3475471},
abstract = {With the advance of the multi-media and multi-modal data, multi-view clustering (MVC) has drawn increasing attentions recently. In this field, one of the most crucial challenges is that the characteristics and qualities of different views usually vary extensively. Therefore, it is essential for MVC methods to find an effective approach that handles the diversity of multiple views appropriately. To this end, a series of MVC methods focusing on how to integrate the loss from each view have been proposed in the past few years. Among these methods, the mainstream idea is assigning weights to each view and then combining them linearly. In this paper, inspired by the effectiveness of non-linear combination in instance learning and the auto-weighted approaches, we propose Non-Linear Fusion for Self-Paced Multi-View Clustering (NSMVC), which is totally different from the the conventional linear-weighting algorithms. In NSMVC, we directly assign different exponents to different views according to their qualities. By this way, the negative impact from the corrupt views can be significantly reduced. Meanwhile, to address the non-convex issue of the MVC model, we further define a novel regularizer-free modality of Self-Paced Learning (SPL), which fits the proposed non-linear model perfectly. Experimental results on various real-world data sets demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3211–3219},
numpages = {9},
keywords = {self-paced learning, non-linear fusion, multi-view clustering},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1145/1168910.1168912,
author = {Won, Youjip and Chang, Hyungkyu and Ryu, Jaemin and Kim, Yongdai and Shim, Junseok},
title = {Intelligent storage: Cross-layer optimization for soft real-time workload},
year = {2006},
issue_date = {August 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/1168910.1168912},
doi = {10.1145/1168910.1168912},
abstract = {In this work, we develop an intelligent storage system framework for soft real-time applications. Modern software systems consist of a collection of layers and information exchange across the layers is performed via well-defined interfaces. Due to the strictness and inflexibility of interface definition, it is not possible to pass the information specific to one layer to other layers. In practice, the exploitation of this information across the layers can greatly enhance the performance, reliability, and manageability of the system. We address the limitation of legacy interface definition via enabling intelligence in the storage system. The objective is to enable the lower-layer entity, for example, a physical or block device, to conjecture the semantic and contextual information of that application behavior which cannot be passed via the legacy interface. Based upon the knowledge obtained by the intelligence module, the system can perform a number of actions to improve the performance, reliability, security, and manageability of the system. Our intelligence storage system focuses on optimizing the I/O subsystem performance for a soft real-time application. Our intelligence framework consists of three components: the workload monitor, workload analyzer, and system optimizer. The workload monitor maintains a window of recent I/O requests and extracts feature vectors in regular intervals. The workload analyzer is trained to determine the class of the incoming workload by using the feature vector. The system optimizer performs various actions to tune the storage system for a given workload. We use confidence rate boosting to train the workload analyzer. This sophisticated learner achieves a higher than 97% accuracy of workload class prediction. We develop a prototype intelligence storage system on the legacy operating system platform. The system optimizer performs; (1) dynamic adjustment of the file-system-level read-ahead size; (2) dynamic adjustment of I/O request size; and (3) filtering of I/O requests. We examine the effect of this autonomic optimization via experimentation. We find that the storage level pro-active optimization greatly enhances the efficiency of the underlying storage system. The sophisticated intelligence module developed in this work does not restrict its usage for performance optimization. It can be effectively used as classification engine for generic autonomic computing environment, i.e. management, diagnosis, security and etc.},
journal = {ACM Trans. Storage},
month = aug,
pages = {255–282},
numpages = {28},
keywords = {storage, multimedia, machine learning, file system, cross layer optimization, boosting, autonomic computing, Intelligence}
}

@article{10.1504/ijguc.2021.119573,
author = {Ji, Hongbo and Wang, Mingyue and Sun, Mingwei and Liu, Qiang},
title = {Neural network classifier based on genetic algorithm image segmentation of subject robot optimisation system},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {4},
issn = {1741-847X},
url = {https://doi.org/10.1504/ijguc.2021.119573},
doi = {10.1504/ijguc.2021.119573},
abstract = {Robot optimisation system is a kind of complex, nonlinear, strong coupling system with serious uncertainty. The effect of image segmentation has become an important index to judge the merits of many algorithms. The purpose of this study is to explore the effect of neural network based on genetic algorithm on image segmentation in the optimisation system of classifier subject robot. The method used in this study is to calculate the pre trained VGGl6 NET model as the pre training model through the framework of genetic algorithm. The resolution of the training picture used is 640 * 480, the learning rate is 10−5, the value of batch size is l, the number of iterations is set to 12,000 and then the trained model is used to detect the image. The results show that the average error of group B of SNN trained by BP algorithm is 11.62%, the SNN trained by SGA has reduced the result to 9.75% and the error reduced to 7.75% by the genetic algorithm in this study. Moreover, genetic algorithm is better in feature point extraction, and the detection rate reaches 94.62%, which is higher than 77.53% and 88.74% of other methods. The missing rate of this study is only 3.04%, far lower than 12.49% and 7.36%. The conclusion is that our genetic algorithm has obvious advantages, small error, high efficiency and applicability. The neural network based on genetic algorithm in this study has a certain value in image segmentation technology.},
journal = {Int. J. Grid Util. Comput.},
month = jan,
pages = {369–379},
numpages = {10},
keywords = {feature point extraction, image segmentation, robot optimisation system, neural network classifier, genetic algorithm}
}

@article{10.1016/j.sysarc.2021.102041,
author = {Mittal, Sparsh and Vibhu},
title = {A survey of accelerator architectures for 3D convolution neural networks},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {115},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2021.102041},
doi = {10.1016/j.sysarc.2021.102041},
journal = {J. Syst. Archit.},
month = may,
numpages = {20},
keywords = {Image processing, Artificial neural networks, Artificial intelligence, 3D convolution neural networks, 3D CNNs}
}

@inproceedings{10.1007/978-3-030-92273-3_25,
author = {Zheng, Jinfang and Xie, Jinyang and Lyu, Chen and Lyu, Lei},
title = {SS-CCN: Scale Self-guided Crowd Counting Network},
year = {2021},
isbn = {978-3-030-92272-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92273-3_25},
doi = {10.1007/978-3-030-92273-3_25},
abstract = {With the emergence of deep learning, many CNN-based methods have achieved competitive performance in crowd counting, in which how to effectively solve the scale variation problem plays a key role. To tackle with the problem, we present an innovative scale self-guided crowd counting network (SS-CCN) by taking full advantage of scale information in a multi-level network. The proposed SS-CCN highlights crowd information by applying scale enhancement and scale-aware attention modules in multi-level features. Moreover, semantic attention module is applied on deep layers to extract semantic information. Besides, the fine-grained residual module is proposed to further refine the crowd information. Furthermore, we pioneer a scale pyramid loss with different loss functions applied to different scales. Integrating the proposed module, our method can effectively solve the scale variation problem. Extensive experimental results on several public datasets show that our proposed SS-CCN achieves satisfactory and superior performance compared to the state-of-the-art methods.},
booktitle = {Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part IV},
pages = {299–310},
numpages = {12},
keywords = {Scale pyramid loss, Scale-aware, Attention mechanism, Deep learning, Crowd counting},
location = {Sanur, Bali, Indonesia}
}

@inproceedings{10.1145/3468264.3473935,
author = {Ye, Jiaming and Chen, Ke and Xie, Xiaofei and Ma, Lei and Huang, Ruochen and Chen, Yingfeng and Xue, Yinxing and Zhao, Jianjun},
title = {An empirical study of GUI widget detection for industrial mobile games},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473935},
doi = {10.1145/3468264.3473935},
abstract = {With the widespread adoption of smartphones in our daily life, mobile games experienced increasing demand over the past years. Meanwhile, the quality of mobile games has been continuously drawing more and more attention, which can greatly affect the player experience. For better quality assurance, general-purpose testing has been extensively studied for mobile apps. However, due to the unique characteristic of mobile games, existing mobile testing techniques may not be directly suitable and applicable. To better understand the challenges in mobile game testing, in this paper, we first initiate an early step to conduct an empirical study towards understanding the challenges and pain points of mobile game testing process at our industrial partner NetEase Games. Specifically, we first conduct a survey from the mobile test development team at NetEase Games via both scrum interviews and questionnaires. We found that accurate and effective GUI widget detection for mobile games could be the pillar to boost the automation of mobile game testing and other downstream analysis tasks in practice.  We then continue to perform comparative studies to investigate the effectiveness of state-of-the-art general-purpose mobile app GUI widget detection methods in the context of mobile games. To this end, we also develop a technique to automatically collect GUI widgets region information of industrial mobile games, which is equipped with a heuristic-based data cleaning method for quality refinement of the labeling results. Our evaluation shows that: (1) Existing GUI widget detection methods for general-purpose mobile apps cannot perform well on industrial mobile games. (2) Mobile game exhibits obvious difference from other general-purpose mobile apps in the perspective GUI widgets. Our further in-depth analysis reveals high diversity and density characteristics of mobile game GUI widgets could be the major reasons that post the challenges for existing methods, which calls for new research methods and better industry practices. To enable further research along this line, we construct the very first GUI widget detection benchmark, specially designed for mobile games, incorporating both our collected dataset and the state-of-the-art widget detection methods for mobile apps, which could also be the basis for further study of many downstream quality assurance tasks (e.g., testing and analysis) for mobile games.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1427–1437},
numpages = {11},
keywords = {Game Testing, GUI Detection, Deep Learning},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1007/s11227-021-03746-z,
author = {Fu, Hao and Tang, Shanjiang and He, Bingsheng and Yu, Ce and Sun, Jizhou},
title = {HGP4CNN: an efficient parallelization framework for training convolutional neural networks on modern GPUs},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {11},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-021-03746-z},
doi = {10.1007/s11227-021-03746-z},
abstract = {Graphics Processing Units (GPUs) have evolved into powerful accelerators for the development of Convolutional Neural Network (CNN) models. Most existing GPU-based frameworks adopt a kernel-based execution approach and only focus on optimizing individual kernels for better performance and resource utilization. With this approach, kernels involved will be launched sequentially, which may result in the underutilization of GPU resources due to the limited optimization space of a single kernel. In this paper, we propose an efficient software parallelization framework, called HGP4CNN, to accelerate the training of CNN models by considering the characteristics of workloads from both the same layer and adjacent layers as well as new GPU features such as concurrent kernel execution. In the intra-layer level, to achieve a better training performance of a single network layer, we design a novel model-based lightweight parallelization module to make better use of the concurrent kernel execution feature on modern GPUs. An asynchronous resource tracker is used to collect kernels’ information at runtime, and a kernel analyzer is devised to calculate the number of kernels that can be dispatched concurrently. Moreover, to avoid consuming too many CPU threads or process resources, we integrate a runtime scheduler module for kernel launch and a pool-based stream manager for GPU work queue management. While in the inter-layer level, we present a pipeline execution strategy to overlap the processing of workloads from adjacent layers. To determine the number of samples to be processed by a single pipeline stage, the analysis result from the intra-layer module is considered. In the end, we implement a prototype of the proposed framework with Caffe, a well-known deep learning framework and conduct experiments with four off-the-shelf CNN models on three NVIDIA GPUs. Results show that HGP4CNN can be exploited to achieve better performance over the original implementation and keep the convergence property of networks. We can achieve a speedup of up to 6.X for a single convolutional layer and 2.X for multiple layers within pipelines of a network model.},
journal = {J. Supercomput.},
month = nov,
pages = {12741–12770},
numpages = {30},
keywords = {HGP4CNN, GPU, Light-weight parallelization framework, Convolutional neural network}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00029,
author = {Ham, MyungJoo and Moon, Jijoong and Lim, Geunsik and Jung, Jaeyun and Ahn, Hyoungjoo and Song, Wook and Woo, Sangjung and Kapoor, Parichay and Chae, Dongju and Jang, Gichan and Ahn, Yongjoo and Lee, Jihoon},
title = {NNStreamer: efficient and Agile† development of on-device AI systems},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00029},
doi = {10.1109/ICSE-SEIP52600.2021.00029},
abstract = {We propose NNStreamer, a software system that handles neural networks as filters of stream pipelines, applying the stream processing paradigm to deep neural network applications. A new trend with the wide-spread of deep neural network applications is on-device AI. It is to process neural networks on mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy issues, data transmission costs, and operational costs signify the need for on-device AI, especially if we deploy a massive number of devices. NNStreamer efficiently handles neural networks with complex data stream pipelines on devices, significantly improving the overall performance with minimal effort. Besides, NNStreamer simplifies implementations and allows reusing off-the-shelf media filters directly, which reduces developmental costs significantly. We are already deploying NNStreamer for a wide range of products and platforms, including the Galaxy series and various consumer electronic devices. The experimental results suggest a reduction in developmental costs and enhanced performance of pipeline architectures and NNStreamer. It is an open-source project incubated by Linux Foundation AI &amp; Data, available to the public and applicable to various hardware and software platforms.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {198–207},
numpages = {10},
keywords = {stream processing, pipe and filter architecture, open source software, on-device AI, neural network},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1109/ICCIT.2007.79,
author = {Zhang, Changyou and Zhu, Dongfeng and Zhang, Yu and Yang, Minghua},
title = {A Web Service Discovery Mechanism Based on Immune Communication},
year = {2007},
isbn = {0769530389},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCIT.2007.79},
doi = {10.1109/ICCIT.2007.79},
abstract = {The greatest advantage of the P2P network is the robustness which provides an opportunity to create a decentralized environment for web service publication and discovery. A new study shows that the roaming and fleeting communication between immune cells form an information-sharing network. By analogy, we design a web service discovery mechanism on unstructured P2P network. Firstly, all the web services are clustered into communities through functional properties. Then, several query packets wander among these communities to make rapid recognition and exchange their experience at their encounter. And then, the query packet will be proliferated and spread through the most matched community. Each service in this community will be evaluated through non-functional properties. Eventually, the best service for our integration in the future step is elected out. The service clustering and experience exchanging enhanced the efficiency in discovery.},
booktitle = {Proceedings of the 2007 International Conference on Convergence Information Technology},
pages = {456–461},
numpages = {6},
series = {ICCIT '07}
}

@inproceedings{10.1145/1254882.1254899,
author = {Osogami, Takayuki and Kato, Sei},
title = {Optimizing system configurations quickly by guessing at the performance},
year = {2007},
isbn = {9781595936394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1254882.1254899},
doi = {10.1145/1254882.1254899},
abstract = {The performance of a Web system can be greatly improved by tuning its configuration parameters. However, finding the optimal configuration has been a time-consuming task due to the long measurement time needed to evaluate the performance of a given configuration. We propose an algorithm, which we refer to as Quick Optimization via Guessing (QOG), that quickly selects one of nearly best configurations with high probability. The key ideas in QOG are (i) the measurement of a configuration is terminated as soon as the configuration is found to be suboptimal, and (ii) the performance of a configuration is guessed at based on the measured similar configurations, so that the better configurations are more likely to be measured before the others. If the performance of a good configuration has been measured, a poor configuration will be quickly found to be suboptimal with short measurement time. We apply QOG to optimizing the configuration of a real Web system, and find that QOG can drastically reduce the total measurement time needed to select the best configuration. Our experiments also illuminate several interesting properties of QOG specifically when it is applied to optimizing Web systems.},
booktitle = {Proceedings of the 2007 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {145–156},
numpages = {12},
keywords = {web system, regression, ranking and selection, performance optimization, configuration parameters},
location = {San Diego, California, USA},
series = {SIGMETRICS '07}
}

@article{10.1016/j.neucom.2008.02.004,
author = {Abdel-Aal, Radwan E. and El-Alfy, El-Sayed M.},
title = {Constructing optimal educational tests using GMDH-based item ranking and selection},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {72},
number = {4–6},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2008.02.004},
doi = {10.1016/j.neucom.2008.02.004},
abstract = {Item ranking and selection plays a key role in constructing concise and informative educational tests. Traditional techniques based on the item response theory (IRT) have been used to automate this task, but they require model parameters to be determined a priori for each item and their application becomes more tedious with larger item banks. Machine-learning techniques can be used to build data-based models that relate the test result as output to the examinees' responses to various test items as inputs. With this approach, test item selection can benefit from the vast amount of literature on feature selection in many areas of machine learning and artificial intelligence that are characterized by high data dimensionality. This paper describes a novel technique for item ranking and selection using abductive network pass/fail classifiers based on the group method of data handling (GMDH). Experiments were carried out on a dataset consisting of the response of 2000 examinees to 45 test items together with the examinee's true ability level. The approach utilizes the ability of GMDH-based learning algorithms to automatically select optimum input features from a set of available inputs. Rankings obtained by iteratively applying this procedure are similar to those based on the average item information function (IIF) at the pass-fail ability threshold, IIF (@q=0), and the average information gain (IG). An optimum item subset derived from the GMDH-based ranking contains only one third of the test items and performs pass/fail classification with 91.2% accuracy on a 500-case evaluation subset, compared to 86.8% for a randomly selected item subset of the same size and 92% for a subset of the 15 items having the largest values for IIF (@q=0). Item rankings obtained with the proposed approach compare favorably with those obtained using neural network modeling and popular filter type feature selection methods, and the proposed approach is much faster than wrapper methods employing genetic search.},
journal = {Neurocomput.},
month = jan,
pages = {1184–1197},
numpages = {14},
keywords = {Wrapper methods, Optimal test design, Neural networks, Mutual information, Machine learning, Item response theory, Genetic algorithms, GMDH algorithm, Filter methods, Feature selection, Feature ranking, Educational measurements, Abductive networks}
}

@inproceedings{10.1145/3474085.3475179,
author = {Yan, Xu and Fei, Zhengcong and Li, Zekang and Wang, Shuhui and Huang, Qingming and Tian, Qi},
title = {Semi-Autoregressive Image Captioning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475179},
doi = {10.1145/3474085.3475179},
abstract = {Current state-of-the-art approaches for image captioning typically adopt an autoregressive manner, i.e., generating descriptions word by word, which suffers from slow decoding issue and becomes a bottleneck in real-time applications. Non-autoregressive image captioning with continuous iterative refinement, which eliminates the sequential dependence in a sentence generation, can achieve comparable performance to the autoregressive counterparts with a considerable acceleration. Nevertheless, based on a well-designed experiment, we empirically proved that iteration times can be effectively reduced when providing sufficient prior knowledge for the language decoder. Towards that end, we propose a novel two-stage framework, referred to as Semi-Autoregressive Image Captioning (SAIC), to make a better trade-off between performance and speed. The proposed SAIC model maintains autoregressive property in global but relieves it in local. Specifically, SAIC model first jumpily generates an intermittent sequence in an autoregressive manner, that is, it predicts the first word in every word group in order. Then, with the help of the partially deterministic prior information and image features, SAIC model non-autoregressively fills all the skipped words with one iteration. Experimental results on the MS COCO benchmark demonstrate that our SAIC model outperforms the preceding non-autoregressive image captioning models while obtaining a competitive inference speedup.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2708–2716},
numpages = {9},
keywords = {training strategies, semi-autoregressive decoding, outliner and filler, image captioning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1016/j.cviu.2019.05.001,
author = {Osman, Ahmed and Samek, Wojciech},
title = {DRAU: Dual Recurrent Attention Units for Visual Question Answering},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2019.05.001},
doi = {10.1016/j.cviu.2019.05.001},
journal = {Comput. Vis. Image Underst.},
month = aug,
pages = {24–30},
numpages = {7},
keywords = {Natural Language Processing, Machine Vision, Multi-modal Learning, Attention Mechanisms, Visual Question Answering, 68T10, 68T30, 68T50, 68T45}
}

@article{10.1023/A:1015394302429,
author = {Cervin, Anton and Eker, Johan and Bernhardsson, Bo and \r{A}rz\'{e}n, Karl-Erik},
title = {Feedback–Feedforward Scheduling of Control Tasks},
year = {2002},
issue_date = {July-September 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1/2},
issn = {0922-6443},
url = {https://doi.org/10.1023/A:1015394302429},
doi = {10.1023/A:1015394302429},
abstract = {A scheduling architecture for real-time control tasks is proposed. The scheduler uses feedback from execution-time measurements and feedforward from workload changes to adjust the sampling periods of the control tasks so that the combined performance of the controllers is optimized. The performance of each controller is described by a cost function. Based on the solution to the optimal resource allocation problem, explicit solutions are derived for linear and quadratic approximations of the cost functions. It is shown that a linear rescaling of the nominal sampling frequencies is optimal for both of these approximations. An extensive inverted pendulum example is presented, where the performance obtained with open-loop, feedback, combined feedback and feedforward scheduling, and earliest-deadline first scheduling are compared. The performance under earliest-deadline first scheduling is explained by studying the behavior of periodic tasks under overload conditions. It is shown that the average values of the sampling periods equal the nominal periods, rescaled by the processor utilization.},
journal = {Real-Time Syst.},
month = jul,
pages = {25–53},
numpages = {29},
keywords = {resource distribution, real-time control, performance optimization, feedback scheduling}
}

@article{10.1016/j.neucom.2018.04.001,
author = {Lu, Quanmao and Li, Xuelong and Dong, Yongsheng},
title = {Structure preserving unsupervised feature selection},
year = {2018},
issue_date = {August 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {301},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.001},
doi = {10.1016/j.neucom.2018.04.001},
abstract = {Spectral analysis was usually used to guide unsupervised feature selection. However, the performances of these methods are not always satisfactory due to that they may generate continuous pseudo labels to approximate the discrete real labels. In this paper, a novel unsupervised feature selection method is proposed based on self-expression model. Unlike existing spectral analysis based methods, we utilize self-expression model to capture the relationships between the features without learning the cluster labels. Specifically, each feature can be reconstructed by using a linear combination of all the features in the original feature space, and a representative feature should give a large weight to reconstruct other features. Besides, a structure preserved constraint is incorporated into our model for keeping the local manifold structure of the data. Then an efficient alternative iterative algorithm is utilized to solve our proposed model with the theoretical analysis on its convergence. The experimental results on different datasets show the effectiveness of our method.},
journal = {Neurocomput.},
month = aug,
pages = {36–45},
numpages = {10},
keywords = {Unsupervised feature selection, Structure preserving, Self-expression model}
}

@article{10.1016/j.specom.2015.01.003,
author = {Sadeghian, Amir and Dajani, Hilmi R. and Chan, Adrian D.C.},
title = {Classification of speech-evoked brainstem responses to English vowels},
year = {2015},
issue_date = {April 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2015.01.003},
doi = {10.1016/j.specom.2015.01.003},
abstract = {We investigated the automatic classification of speech-evoked brainstem responses.Responses to five vowels were classified based on onset and sustained features.Combined sustained features gave a classification accuracy of 83.33%.Classification accuracy with onset response features was better than chance.Vowel-specific information in responses may be useful for fitting hearing aids. This study investigated whether speech-evoked auditory brainstem responses (speech ABRs) can be automatically separated into distinct classes. With five English synthetic vowels, the speech ABRs were classified using linear discriminant analysis based on features contained in the transient onset response, the sustained envelope following response (EFR), and the sustained frequency following response (FFR). EFR contains components mainly at frequencies well below the first formant, while the FFR has more energy around the first formant. Accuracies of 83.33% were obtained for combined EFR and FFR features and 38.33% were obtained for transient response features. The EFR features performed relatively well with a classification accuracy of 70.83% despite the belief that vowel discrimination is primarily dependent on the formants. The FFR features obtained a lower accuracy of 59.58% possibly because the second formant is not well represented in all the responses. Moreover, the classification accuracy based on the transient features exceeded chance level which indicates that the initial response transients contain vowel specific information. The results of this study will be useful in a proposed application of speech ABR to objective hearing aid fitting, if the separation of the brain's responses to different vowels is found to be correlated with perceptual discrimination.},
journal = {Speech Commun.},
month = apr,
pages = {69–84},
numpages = {16},
keywords = {Speech-evoked auditory brainstem response, Frequency following response, Fitting hearing aids, Envelope following response, Classification of evoked responses, Auditory processing of speech}
}

@inproceedings{10.5555/3466184.3466193,
author = {R\"{o}\ss{}ler, Matthias and Wastian, Matthias and Jellen, Anna and Frisch, Sarah and Weinberger, Dominic and Hungerl\"{a}nder, Philipp and Bicher, Martin and Popper, Niki},
title = {Simulation and optimization of traction unit circulations},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {The planning of traction unit circulations in a railway network is a very time-consuming task. In order to support the planning personnel, the paper proposes a combination of optimization, simulation and machine learning. This ensemble creates mathematically nearly optimal circulations that are also feasible in real operating procedures. An agent-based simulation model is developed that tests the circulation for its robustness against delays. The delays introduced into the system are based on predictions from a machine learning model built upon historical operational data. The paper first presents the used data and the delay prediction. Afterwards, the modeling and simulation part and the optimization are presented. At last, the interaction of simulation and optimization are described and promising results of a test case are shown.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {90–101},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1287/inte.2013.0693,
author = {Mahdavi Pajouh, Foad and Xing, Dahai and Zhou, Yingjue and Hariharan, Sharethram and Balasundaram, Balabhaskar and Liu, Tieming and Sharda, Ramesh},
title = {A Specialty Steel Bar Company Uses Analytics to Determine Available-to-Promise Dates},
year = {2013},
issue_date = {December 2013},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {43},
number = {6},
issn = {0092-2102},
url = {https://doi.org/10.1287/inte.2013.0693},
doi = {10.1287/inte.2013.0693},
abstract = {In this paper, we describe an application of prescriptive analytics to enhance data-driven decision making at a specialty steel bar products supplier and manufacturer in North America. As part of the company's daily business, it must make available-to-promise ATP decisions, which determine in real time the dates by which it can promise delivery of products that customers requested during the quotation stage. Previously, a salesperson had to make such decisions by analyzing reports on available inventory. To support these ATP decisions, we developed a real-time decision support system DSS to find an optimal assignment of the available inventory and to support additional what-if analysis. The DSS uses a suite of mixed-integer programming models and commercial software to solve the models. The company has incorporated the DSS into its enterprise resource planning system to seamlessly facilitate its use of business analytics.},
journal = {Interfaces},
month = dec,
pages = {503–517},
numpages = {15},
keywords = {prescriptive analytics, optimization-based decision support, available-to-promise}
}

@inproceedings{10.5555/3305381.3305445,
author = {Budden, David and Matveev, Alexander and Santurkar, Shibani and Chaudhuri, Shraman Ray and Shavit, Nir},
title = {Deep Tensor convolution on multicores},
year = {2017},
publisher = {JMLR.org},
abstract = {Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of spatiotemporal features. These networks have improved performance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Existing CPU implementations overcome this constraint but are impractically slow. Here we extend and optimize the faster Winograd-class of convolutional algorithms to the N-dimensional case and specifically for CPU hardware. First, we remove the need to manually hand-craft algorithms by exploiting the relaxed constraints and cheap sparse access of CPU memory. Second, we maximize CPU utilization and multi-core scalability by transforming data matrices to be cache-aware, integer multiples of AVX vector widths. Treating 2D ConvNets as a special case, we demonstrate a 5 to 25-fold improvement in throughput compared to previous state-of-the-art.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {615–624},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1145/3340531.3412773,
author = {Gallagher, Luke and Mallia, Antonio and Culpepper, J. Shane and Suel, Torsten and Cambazoglu, B. Barla},
title = {Feature Extraction for Large-Scale Text Collections},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412773},
doi = {10.1145/3340531.3412773},
abstract = {Feature engineering is a fundamental but poorly documented component in Learning-to-Rank (LTR) search engines. Such features are commonly used to construct learning models for web and product search engines, recommender systems, and question-answering tasks. In each of these domains, there is a growing interest in the creation of open-access test collections that promote reproducible research. However, there are still few open-source software packages capable of extracting high-quality machine learning features from large text collections. Instead, most feature-based LTR research relies on "canned" test collections, which often do not expose critical details about the underlying collection or implementation details of the extracted features. Both of these are crucial to collection creation and deployment of a search engine into production. So in this regard, the experiments are rarely reproducible with new features or collections, or helpful for companies wishing to deploy LTR systems.  In this paper, we introduce Fxt, an open-source framework to perform efficient and scalable feature extraction. Fxt can easily be integrated into complex, high-performance software applications to help solve a wide variety of text-based machine learning problems. To demonstrate the software's utility, we build and document a reproducible feature extraction pipeline and show how to recreate several common LTR experiments using the ClueWeb09B collection. Researchers and practitioners can benefit from Fxt to extend their machine learning pipelines for various text-based retrieval tasks, and learn how some static document features and query-specific features are implemented.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3015–3022},
numpages = {8},
keywords = {ltr, learning to rank, lambdamart, feature repository, feature index, feature importance, feature extraction, clueweb},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1109/ISCA52012.2021.00010,
author = {Jouppi, Norman P. and Yoon, Doe Hyun and Ashcraft, Matthew and Gottscho, Mark and Jablin, Thomas B. and Kurian, George and Laudon, James and Li, Sheng and Ma, Peter and Ma, Xiaoyu and Norrie, Thomas and Patil, Nishant and Prasad, Sushma and Young, Cliff and Zhou, Zongwei and Patterson, David},
title = {Ten lessons from three generations shaped Google's TPUv4i},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00010},
doi = {10.1109/ISCA52012.2021.00010},
abstract = {Google deployed several TPU generations since 2015, teaching us lessons that changed our views: semi-conductor technology advances unequally; compiler compatibility trumps binary compatibility, especially for VLIW domain-specific architectures (DSA); target total cost of ownership vs initial cost; support multi-tenancy; deep neural networks (DNN) grow 1.5X annually; DNN advances evolve workloads; some inference tasks require floating point; inference DSAs need air-cooling; apps limit latency, not batch size; and backwards ML compatibility helps deploy DNNs quickly. These lessons molded TPUv4i, an inference DSA deployed since 2020.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {1–14},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@article{10.1155/2020/7607545,
author = {Pento\'{s}, Katarzyna and Pieczarka, Krzysztof and Lejman, Krzysztof and Andrea, Murari},
title = {Application of Soft Computing Techniques for the Analysis of Tractive Properties of a Low-Power Agricultural Tractor under Various Soil Conditions},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/7607545},
doi = {10.1155/2020/7607545},
abstract = {Considering the fuel consumption and soil compaction, optimization of the performance of tractors is crucial for modern agricultural practices. The tractive performance is influenced by many factors, making it difficult to be modeled. In this work, the traction force and tractive efficiency of a low-power tractor, as affected by soil coefficient, vertical load, horizontal deformation, soil compaction, and soil moisture, were studied. The optimal work of a tractor is a compromise between the maximum traction force and the maximum tractive efficiency. Optimizing these factors is complex and requires accurate models. To this end, the performances of soft computing approaches, including neural networks, genetic algorithms, and adaptive network fuzzy inference system, were evaluated. The optimal performance was realized by neural networks trained by backpropagation as well as backpropagation combined with a genetic algorithm, with a coefficient of determination of 0.955 for the traction force and 0.954 for the tractive efficiency. Based on models with the best accuracy, a sensitivity analysis was performed. The results showed that the traction performance is mainly influenced by the soil type; nevertheless, the vertical load and soil moisture also exhibited a relatively strong influence.},
journal = {Complex.},
month = jan,
numpages = {11}
}

@article{10.1007/s11042-017-5172-1,
author = {Ma, Xueqi and Tao, Dapeng and Liu, Weifeng},
title = {Effective human action recognition by combining manifold regularization and pairwise constraints},
year = {2019},
issue_date = {May       2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {10},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5172-1},
doi = {10.1007/s11042-017-5172-1},
abstract = {The ever-growing popularity of mobile networks and electronics has prompted intensive research on multimedia data (e.g. text, image, video, audio, etc.) management. This leads to the researches of semi-supervised learning that can incorporate a small number of labeled and a large number of unlabeled data by exploiting the local structure of data distribution. Manifold regularization and pairwise constraints are representative semi-supervised learning methods. In this paper, we introduce a novel local structure preserving approach by considering both manifold regularization and pairwise constraints. Specifically, we construct a new graph Laplacian that takes advantage of pairwise constraints compared with the traditional Laplacian. The proposed graph Laplacian can better preserve the local geometry of data distribution and achieve the effective recognition. Upon this, we build the graph regularized classifiers including support vector machines and kernel least squares as special cases for action recognition. Experimental results on a multimodal human action database (CAS-YNU-MHAD) show that our proposed algorithms outperform the general algorithms.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {13313–13329},
numpages = {17},
keywords = {Pairwise constraints, Manifold regularization, Local structure preserving, Action recognition}
}

@article{10.5555/2541581.2541582,
author = {Sandberg, Kristian and Bahrami, Bahador and Kanai, Ryota and Barnes, Gareth Robert and Overgaard, Morten and Rees, Geraint},
title = {Early visual responses predict conscious face perception within and between subjects during binocular rivalry},
year = {2013},
issue_date = {June 2013},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {25},
number = {6},
issn = {0898-929X},
abstract = {Previous studies indicate that conscious face perception may be related to neural activity in a large time window around 170-800 msec after stimulus presentation, yet in the majority of these studies changes in conscious experience are confounded with changes in physical stimulation. Using multivariate classification on MEG data recorded when participants reported changes in conscious perception evoked by binocular rivalry between a face and a grating, we showed that only MEG signals in the 120-320 msec time range, peaking at the M170 around 180 msec and the P2m at around 260 msec, reliably predicted conscious experience. Conscious perception could not only be decoded significantly better than chance from the sensors that showed the largest average difference, as previous studies suggest, but also from patterns of activity across groups of occipital sensors that individually were unable to predict perception better than chance. In addition, source space analyses showed that sources in the early and late visual system predicted conscious perception more accurately than frontal and parietal sites, although conscious perception could also be decoded there. Finally, the patterns of neural activity associated with conscious face perception generalized from one participant to another around the times of maximum prediction accuracy. Our work thus demonstrates that the neural correlates of particular conscious contents here, faces are highly consistent in time and space within individuals and that these correlates are shared to some extent between individuals.},
journal = {J. Cognitive Neuroscience},
month = jun,
pages = {969–985},
numpages = {17}
}

@article{10.4018/jicthd.2013040102,
author = {Bernardino, Jorge},
title = {Open Business Intelligence for Better Decision-Making},
year = {2013},
issue_date = {April 2013},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {2},
issn = {1935-5661},
url = {https://doi.org/10.4018/jicthd.2013040102},
doi = {10.4018/jicthd.2013040102},
abstract = {Decision-making is a crucial, yet challenging task in enterprise management. In many organizations, decisions are still made based on experience and intuition rather than on facts and rigorous approaches. This is often due to lack of data, unknown relationships between data and goals, conflicting goals and poorly understood risks. The success of organizations depends on fast and well-founded decisions taken by relevant people in their specific area of responsibility. Business Intelligence BI is a collection of decision support technologies for enterprises aimed at enabling knowledge workers such as executives, managers, and analysts to make better and faster decisions. In this paper, the authors review the concept of BI as an open innovation strategy and address the importance of BI in revolutionizing knowledge towards economics and business sustainability. The main objective is to discuss why the concept of BI has become increasingly important and presents some of the top key applications and technologies to implement open BI in organizations, which would like to enter into the new market and operate on a global scale.},
journal = {Int. J. Inf. Comm. Technol. Hum. Dev.},
month = apr,
pages = {20–36},
numpages = {17},
keywords = {Open Source, Open Business Intelligence Systems, Online Analytical Processing, Decision-Making, Business Intelligence}
}

@article{10.1016/j.procs.2015.02.001,
author = {Patri, Ashutosh and Patnaik, Yugesh},
title = {Random Forest and Stochastic Gradient Tree Boosting Based Approach for the Prediction of Airfoil Self-noise},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.001},
doi = {10.1016/j.procs.2015.02.001},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {109–121},
numpages = {13},
keywords = {NACA0012., regression, CART, Stochastic Gradient Boosting, Random Forest, prediction, airfoil self-noise}
}

@article{10.1016/j.matcom.2021.06.023,
author = {Lou, Xuyang and Ji, Zheng},
title = {Self-triggering adaptive optimal control for nonlinear systems based on encoding mechanism},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0378-4754},
url = {https://doi.org/10.1016/j.matcom.2021.06.023},
doi = {10.1016/j.matcom.2021.06.023},
journal = {Math. Comput. Simul.},
month = dec,
pages = {1027–1047},
numpages = {21},
keywords = {Adaptive control, Self-triggering, Encoding mechanism, Lyapunov stability}
}

@article{10.1016/j.parco.2007.06.005,
author = {Pje\v{s}ivac-Grbovi\'{c}, Jelena and Bosilca, George and Fagg, Graham E. and Angskun, Thara and Dongarra, Jack J.},
title = {MPI collective algorithm selection and quadtree encoding},
year = {2007},
issue_date = {September, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {33},
number = {9},
issn = {0167-8191},
url = {https://doi.org/10.1016/j.parco.2007.06.005},
doi = {10.1016/j.parco.2007.06.005},
abstract = {We explore the applicability of the quadtree encoding method to the run-time MPI collective algorithm selection problem. Measured algorithm performance data was used to construct quadtrees with different properties. The quality and performance of generated decision functions and in-memory decision systems were evaluated. Experimental data shows that in some cases, a decision function based on a quadtree structure with a mean depth of three, incurs on average as little as a 5% performance penalty. In all cases, experimental data can be fully represented with a quadtree containing a maximum of six levels. Our results indicate that quadtrees may be a feasible choice for both processing of the performance data and automatic decision function generation.},
journal = {Parallel Comput.},
month = sep,
pages = {613–623},
numpages = {11},
keywords = {Quadtree encoding, Performance optimization, Performance evaluation, MPI collective operations, Algorithm selection problem}
}

@article{10.1155/2021/9934585,
author = {Ye, Shitong and Xu, Lijuan and Li, Xiaomin and Wang, Wei},
title = {Vehicle-Mounted Self-Organizing Network Routing Algorithm Based on Deep Reinforcement Learning},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9934585},
doi = {10.1155/2021/9934585},
abstract = {Through the research on the vehicle-mounted self-organizing network, in view of the current routing technical problems of the vehicle-mounted self-organizing network under the condition of no roadside auxiliary communication unit cooperation, this paper proposes a vehicle network routing algorithm based on deep reinforcement learning. For the problems of massive vehicle nodes and multiple performance evaluation indexes in vehicular ad hoc network, this paper proposes a time prediction model of vehicle communication to reduce the probability of communication interruption and proposes the routing technology of vehicle network by studying the deep reinforcement learning method. This technology can quickly select routing nodes and plan the optimal route according to the required performance evaluation indicators.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {9}
}

@article{10.4018/IJOSSP.2016010102,
author = {Chahal, Kuljit Kaur and Saini, Munish},
title = {Open Source Software Evolution: A Systematic Literature Review Part 2},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016010102},
doi = {10.4018/IJOSSP.2016010102},
abstract = {This paper presents the results of a systematic literature review conducted to understand the Open Source Software OSS development process on the basis of evidence found in the empirical research studies. The study targets the OSS project evolution research papers to understand the methods and techniques employed for analysing the OSS evolution process. Our results suggest that there is lack of a uniform approach to analyse and interpret the results. The use of prediction techniques that just extrapolate the historic trends into the future should be a conscious task as it is observed that there are no long-term correlations in data of such systems. OSS evolution as a research area is still in nascent stage. Even after a number of empirical studies, the field has failed to establish a theory. There is need to formalize the field as a systematic and formal approach can produce better software.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {28–48},
numpages = {21},
keywords = {Software Reuse, Software Evolution Theory, Programming Languages, OSS Prediction, Co-Evolution, Automation Support, ARIMA Modelling}
}

@article{10.1016/j.camwa.2013.06.027,
author = {Borges, Helyane Bronoski and Silla, Carlos N. and Nievola, J\'{u}lio Cesar},
title = {An evaluation of global-model hierarchical classification algorithms for hierarchical classification problems with single path of labels},
year = {2013},
issue_date = {December, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {10},
issn = {0898-1221},
url = {https://doi.org/10.1016/j.camwa.2013.06.027},
doi = {10.1016/j.camwa.2013.06.027},
abstract = {Several classification tasks in different application domains can be seen as hierarchical classification problems. In order to deal with hierarchical classification problems, the use of existing flat classification approaches is not appropriate. For these reason, there has been a growing number of studies focusing on the development of novel algorithms able to induce classification models for hierarchical classification problems. In this paper we study the performance of a novel algorithm called Hierarchical Classification using a Competitive Neural Network (HC-CNN) and compare its performance against the Global-Model Naive Bayes (GMNB) on eight protein function prediction datasets. Interestingly enough, the comparison of two global-model hierarchical classification algorithms for single path of labels hierarchical classification problems has never been done before.},
journal = {Comput. Math. Appl.},
month = dec,
pages = {1991–2002},
numpages = {12},
keywords = {Hierarchical classification, Global approach}
}

@article{10.1007/s10772-016-9367-z,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with Parkinson's diseases using PCA and NPCA},
year = {2016},
issue_date = {December  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-016-9367-z},
doi = {10.1007/s10772-016-9367-z},
abstract = {In this study, we wanted to discriminate between two groups of people. The database used in this study contains 20 patients with Parkinson's disease and 20 healthy people. Three types of sustained vowels (/a/, /o/ and /u/) were recorded from each participant and then the analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used linear and nonlinear feature extraction techniques, principal component analysis (PCA), and nonlinear PCA. These techniques reduce the number of parameters and choose the most effective acoustic features used for classification. Support vector machine with its different kernel was used for classification. We obtained an accuracy up to 87.50 % for discrimination between PD patients and healthy people.},
journal = {Int. J. Speech Technol.},
month = dec,
pages = {743–754},
numpages = {12},
keywords = {SVM, Parkinson's disease, PCA, NPCA, Feature selection}
}

@inproceedings{10.1007/978-3-030-98682-7_13,
author = {Antonioni, Emanuele and Suriani, Vincenzo and Solimando, Filippo and Nardi, Daniele and Bloisi, Domenico D.},
title = {Learning from the Crowd: Improving the Decision Making Process in Robot Soccer Using the Audience Noise},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_13},
doi = {10.1007/978-3-030-98682-7_13},
abstract = {Fan input and support is an important component in many individual and team sports, ranging from athletics to basketball. Audience interaction provides a consistent impact on the athletes’ performance. The analysis of the crowd noise can provide a global indication on the ongoing game situation, less conditioned by subjective factors that can influence a single fan. In this work, we exploit the collective intelligence of the audience of a robot soccer match to improve the performance of the robot players. In particular, audio features extracted from the crowd noiseare used in a Reinforcement Learning process to possibly modify the game strategy. The effectiveness of the proposed approach is demonstrated by experiments on registered crowd noise samples from several past RoboCup SPL matches.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {153–164},
numpages = {12},
keywords = {Sound recognition, RoboCup SPL, Crowd noise interpretation},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3292500.3330701,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {machine learning system, hyperparameter optimization, black-box optimization, Bayesian optimization},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1016/j.dam.2019.02.012,
author = {Atencia, Manuel and David, J\'{e}r\^{o}me and Euzenat, J\'{e}r\^{o}me and Napoli, Amedeo and Vizzini, J\'{e}r\'{e}my},
title = {Link key candidate extraction with relational concept analysis},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {273},
number = {C},
issn = {0166-218X},
url = {https://doi.org/10.1016/j.dam.2019.02.012},
doi = {10.1016/j.dam.2019.02.012},
journal = {Discrete Appl. Math.},
month = feb,
pages = {2–20},
numpages = {19},
keywords = {Resource description framework, Data interlinking, Link key, Linked data, Relational concept analysis, Formal concept analysis}
}

@article{10.1016/j.jbi.2011.11.002,
author = {Forestier, Germain and Lalys, Florent and Riffaud, Laurent and Trelhu, Brivael and Jannin, Pierre},
title = {Classification of surgical processes using dynamic time warping},
year = {2012},
issue_date = {April, 2012},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {45},
number = {2},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2011.11.002},
doi = {10.1016/j.jbi.2011.11.002},
abstract = {In the creation of new computer-assisted intervention systems, Surgical Process Models (SPMs) are an emerging concept used for analyzing and assessing surgical interventions. SPMs represent Surgical Processes (SPs) which are formalized as symbolic structured descriptions of surgical interventions using a pre-defined level of granularity and a dedicated terminology. In this context, one major challenge is the creation of new metrics for the comparison and the evaluation of SPs. Thus, correlations between these metrics and pre-operative data are used to classify surgeries and highlight specific information on the surgery itself and on the surgeon, such as his/her level of expertise. In this paper, we explore the automatic classification of a set of SPs based on the Dynamic Time Warping (DTW) algorithm. DTW is used to compute a similarity measure between two SPs that focuses on the different types of activities performed during surgery and their sequencing, by minimizing time differences. Indeed, it turns out to be a complementary approach to the classical methods that only focus on differences in the time and the number of activities. Experiments were carried out on 24 lumbar disk herniation surgeries to discriminate the surgeons level of expertise according to a prior classification of SPs. Supervised and unsupervised classification experiments have shown that this approach was able to automatically identify groups of surgeons according to their level of expertise (senior and junior), and opens many perspectives for the creation of new metrics for comparing and evaluating surgeries.},
journal = {J. of Biomedical Informatics},
month = apr,
pages = {255–264},
numpages = {10},
keywords = {Surgical process models, Surgery evaluation, Dynamic time warping, Clustering, Classification}
}

@inproceedings{10.1007/978-3-319-29339-4_24,
author = {Leottau, David L. and Ruiz-del-Solar, Javier and MacAlpine, Patrick and Stone, Peter},
title = {A Study of Layered Learning Strategies Applied to Individual Behaviors in Robot Soccer},
year = {2015},
isbn = {978-3-319-29338-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29339-4_24},
doi = {10.1007/978-3-319-29339-4_24},
abstract = {Hierarchical task decomposition strategies allow robots and agents in general to address complex decision-making tasks. Layered learning is a hierarchical machine learning paradigm where a complex behavior is learned from a series of incrementally trained sub-tasks. This paper describes how layered learning can be applied to design individual behaviors in the context of soccer robotics. Three different layered learning strategies are implemented and analyzed using a ball-dribbling behavior as a case study. Performance indices for evaluating dribbling speed and ball-control are defined and measured. Experimental results validate the usefulness of the implemented layered learning strategies showing a trade-off between performance and learning speed.},
booktitle = {RoboCup 2015: Robot World Cup XIX},
pages = {290–302},
numpages = {13},
keywords = {Reinforcement learning, Layered learning, Machine learning, Soccer robotics, Biped robot, NAO, Behavior, Dribbling, Fuzzy logic},
location = {Hefei, China}
}

@article{10.1016/j.future.2011.08.020,
author = {Farsandaj, Kian and Ding, Chen},
title = {Scatter/Gather browsing of web service QoS data},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {28},
number = {7},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2011.08.020},
doi = {10.1016/j.future.2011.08.020},
abstract = {Many of the current Quality of Service (QoS) based web service selection systems assume that users will formulate QoS queries accurately. However, this may not always be the case. It would be helpful if users could browse through the QoS data of the service repository before they start the searching process. In this way, they could know what the actual QoS value distributions are, so that they could put down reasonable numbers in their QoS queries. The browsing process performed on QoS queries, on the other hand, could help service providers understand the actual QoS demands from users. In this paper, we propose to use the Scatter/Gather model-a well-known browsing model for large document collections, to implement QoS browsing. We modified the original model to make it fit QoS data and implemented a few variations of the model. We tested and compared performance on both synthetic and real QoS datasets, mainly focusing on the ability to handle real time interaction with users while balancing efficiency and accuracy.},
journal = {Future Gener. Comput. Syst.},
month = jul,
pages = {1145–1154},
numpages = {10},
keywords = {Web service selection, Symbolic clustering, Scatter/Gather model, QoS browsing}
}

@article{10.3233/JCM-214991,
author = {Jiang, Yue and Xu, Gaochao and Fang, Zhiyi and Song, Shinan and Li, Bingbing},
title = {Heterogeneous fairness algorithm based on federated learning in intelligent transportation system},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {5},
issn = {1472-7978},
url = {https://doi.org/10.3233/JCM-214991},
doi = {10.3233/JCM-214991},
abstract = {With the development of the Intelligent Transportation System, various distributed sensors (including GPS, radar, infrared sensors) process massive data and make decisions for emergencies. Federated learning is a new distributed machine learning paradigm, in which system heterogeneity is the difficulty of fairness design. This paper designs a system heterogeneous fair federated learning algorithm (SHFF). SHFF introduces the equipment influence factor I into the optimization target and dynamically adjusts the equipment proportion with other performance. By changing the global fairness parameter θ, the algorithm can control fairness according to the actual needs. Experimental results show that, compared with the popular q-FedAvg algorithm, the SHFF algorithm proposed in this paper improves the average accuracy of the Worst 10% by 26% and reduces the variance by 61%.},
journal = {J. Comp. Methods in Sci. and Eng.},
month = jan,
pages = {1365–1373},
numpages = {9},
keywords = {distributed learning, intelligent transportation system, heterogeneous system, Federated learning}
}

@inproceedings{10.5555/2499604.2499611,
author = {Yun, Daqing and Wu, Qishi and Gu, Yi and Liu, Xiyang},
title = {On an integrated mapping and scheduling solution to large-scale scientific workflows in resource sharing environments},
year = {2013},
isbn = {9781627480307},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Next-generation e-science applications feature large-scale data-intensive workflows comprised of many interrelated computing modules. The end-to-end performance of such scientific workflows depends on both the mapping scheme, which determines module assignment, and the scheduling policy, which determines resource allocation if multiple modules are mapped to the same node. These two aspects of workflow optimization are traditionally treated as two separated topics, and the interactions between them have not been fully explored by any existing efforts. As the scale of scientific workflows and the complexity of network environments rapidly increase, each individual aspect of performance optimization alone can only meet with limited success. We conduct an in-depth investigation into workflow execution dynamics of both mapping and scheduling, and propose an integrated solution, referred to as Mapping and Scheduling Interaction (MSI), to achieve a higher level of resource utilization and workflow performance. The efficacy of MSI is illustrated by extensive simulation-based workflow experiments.},
booktitle = {Proceedings of the 46th Annual Simulation Symposium},
articleno = {7},
numpages = {8},
keywords = {workflow mapping, on-node job scheduling, end-to-end delay},
location = {San Diego, California},
series = {ANSS 13}
}

@inproceedings{10.1145/3011141.3011222,
author = {Perlasca, Paolo and Valentini, Giorgio and Frasca, Marco and Mesiti, Marco},
title = {Multi-species protein function prediction: towards web-based visual analytics},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011222},
doi = {10.1145/3011141.3011222},
abstract = {The visualization and analysis of big bio-molecular networks is a key feature for the investigation and prediction of protein functions in a multi-species context. In this paper we present the design of a system that integrates data management, machine learning and visualization facilities to make effective the visual analysis of big networks by means of web-based interfaces.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {489–493},
numpages = {5},
keywords = {visual analytics, protein function prediction, heterogeneous networks, graph visualization},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@article{10.1007/s00500-020-05250-7,
author = {Yang, Jing and Hu, Yingpeng and Zhang, Kaixi and Wu, Yanghui},
title = {An improved evolution algorithm using population competition genetic algorithm and self-correction BP neural network based on fitness landscape},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {3},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05250-7},
doi = {10.1007/s00500-020-05250-7},
abstract = {The genetic algorithm-backpropagation neural network algorithm (GA-BP) makes full use of the advantages of genetic algorithm (GA) and BP neural network (BPNN). It has been widely used in practical problems, but it also has shortcomings such as slow convergence. Inspired by the generative adversarial network, a population competition mechanism (PCM) is proposed to improve the search ability of the GA, two populations are generated to compete, and the winning population can obtain the optimal individual from the failed population to ensure that the winning population has a sustainable advantage. The failed population will randomly generate new individuals and add new genes so as to get better individuals, through such a mechanism to ensure the rapid optimization of the entire population, avoid the risk of premature convergence, speed up the iterative speed and improve the stability of the GA. According to the characteristics of the fitness landscape, the learning rate of the BPNN is optimized, and it can change adaptively, which can effectively improve the network convergence speed and greatly reduce the time cost. We define the GA-BP that improves the learning rate based on fitness landscape as FL-GA-BP algorithm. On the basis of the FL-GA-BP algorithm, adding GA improved with PCM, we define this new algorithm as I-GA-BP algorithm, namely the I-GA-BP algorithm that combines PCM-improved GA and fitness landscape-improved BPNN. In this paper, we use two types of test functions with different characteristics and complexity to conduct experiments to verify the effectiveness of the I-GA-BP algorithm. By comparing the experimental data of the three algorithms GA-BP, FL-GA-BP and I-GA-BP, it is obtained that the I-GA-BP algorithm can better escape from the local optimal solution, which is more conducive to finding the global optimal solution. It also greatly improves the convergence speed of the neural network. Finally, we briefly discussed the effect of adjusting the number of neurons on the stability of the I-GA-BP algorithm.},
journal = {Soft Comput.},
month = feb,
pages = {1751–1776},
numpages = {26},
keywords = {Learning rate, BP neural network, Fitness landscape, Population competition mechanism, Genetic algorithm}
}

@inproceedings{10.5555/3042094.3042398,
author = {Liotta, Giacomo and Chaudhuri, Atanu},
title = {Minimizing recall risk by collaborative digitized information sharing between OEM and suppliers: a simulation based investigation},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Many Original Equipment Manufacturers (OEMs) and their suppliers face recall and warranty risks due to complex supply chains and products. OEMs and suppliers can hardly take appropriate actions for mitigating these quality risks due to lack of product history data and understanding of their probability. In this work, the product consists of two components delivered by two Tier II suppliers. Probabilities of OEM's acceptance, rework and rejection of the assembled product by a Tier I supplier and probabilities of acceptance, warranty and recall are calculated combining Bayesian Belief Network and simulation of a digitized supply chain. Results show that sharing of incoming quality information between an OEM and Tier I supplier and decision models to estimate warranty and recall probabilities can help in assessing quality improvement benefits to minimize recall risks. Suitable quality improvement contracts between an OEM and Tier I supplier can be designed using embedded product quality data.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2454–2465},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

