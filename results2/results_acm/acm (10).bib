@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Clafer, Energy Efficiency, Metrics, Optimisation, Repository, Software Product Line, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Service-oriented architecture, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/1982185.1982336,
author = {Asadi, Mohsen and Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Mohabbati, Bardia},
title = {Goal-driven software product line engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982336},
doi = {10.1145/1982185.1982336},
abstract = {Feature Models encapsulate functionalities and quality properties of a product family. The employment of feature models for managing variability and commonality of large-scale product families raises an important question: on what basis should the features of a product family be selected for a target software application, which is going to be derived from the product family. Thus, the selection of the most suitable features for a specific application requires the understanding of its stakeholders' intentions and also the relationship between their intentions and the available software features. To address this important issue, we adopt a standard goal-oriented requirements engineering framework, i.e., the i* framework, for identifying stakeholders' intentions and propose an approach for explicitly mapping and bridging between the features of a product family and the goals and objectives of the stakeholders. We propose a novel approach to automatically preconfigure a given feature model based on the objectives of the target product stakeholders. Also, our approach is able to elucidate the rationale behind the selection of the most important features of a family for a target application.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {691–698},
numpages = {8},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/3385032.3385043,
author = {Bilic, Damir and Sundmark, Daniel and Afzal, Wasif and Wallin, Peter and Causevic, Adnan and Amlinger, Christoffer and Barkah, Dani},
title = {Towards a Model-Driven Product Line Engineering Process: An Industrial Case Study},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385043},
doi = {10.1145/3385032.3385043},
abstract = {Many organizations developing software-intensive systems face challenges with high product complexity and large numbers of variants. In order to effectively maintain and develop these product variants, Product-Line Engineering methods are often considered, while Model-based Systems Engineering practices are commonly utilized to tackle product complexity. In this paper, we report on an industrial case study concerning the ongoing adoption of Product Line Engineering in the Model-based Systems Engineering environment at Volvo Construction Equipment (Volvo CE) in Sweden. In the study, we identify and define a Product Line Engineering process that is aligned with Model-based Systems Engineering activities at the engines control department of Volvo CE. Furthermore, we discuss the implications of the migration from the current development process to a Model-based Product Line Engineering-oriented process. This process, and its implications, are derived by conducting and analyzing interviews with Volvo CE employees, inspecting artifacts and documents, and by means of participant observation. Based on the results of a first system model iteration, we were able to document how Model-based Systems Engineering and variability modeling will affect development activities, work products and stakeholders of the work products.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {9},
numpages = {11},
keywords = {Engine System Development, Model-Based Systems Engineering, Product Line Engineering},
location = {Jabalpur, India},
series = {ISEC '20}
}

@article{10.1016/j.scico.2010.07.005,
author = {Apel, Sven and Kolesnikov, Sergiy and Liebig, J\"{o}rg and K\"{a}stner, Christian and Kuhlemann, Martin and Leich, Thomas},
title = {Access control in feature-oriented programming},
year = {2012},
issue_date = {March, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {77},
number = {3},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2010.07.005},
doi = {10.1016/j.scico.2010.07.005},
abstract = {In feature-oriented programming (FOP) a programmer decomposes a program in terms of features. Ideally, features are implemented modularly so that they can be developed in isolation. Access control mechanisms in the form of access or visibility modifiers are an important ingredient to attain feature modularity as they allow programmers to hide and expose internal details of a module's implementation. But developers of contemporary feature-oriented languages have not considered access control mechanisms so far. The absence of a well-defined access control model for FOP breaks encapsulation of feature code and leads to unexpected program behaviors and inadvertent type errors. We raise awareness of this problem, propose three feature-oriented access modifiers, and present a corresponding access modifier model. We offer an implementation of the model on the basis of a fully-fledged feature-oriented compiler. Finally, by analyzing ten feature-oriented programs, we explore the potential of feature-oriented modifiers in FOP.},
journal = {Sci. Comput. Program.},
month = mar,
pages = {174–187},
numpages = {14},
keywords = {Access control, Access modifier model, Feature modularity, Feature-oriented programming, Fuji}
}

@inproceedings{10.1007/978-3-642-39038-8_2,
author = {Oliveira, Bruno C. d. S. and van der Storm, Tijs and Loh, Alex and Cook, William R.},
title = {Feature-Oriented programming with object algebras},
year = {2013},
isbn = {9783642390371},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39038-8_2},
doi = {10.1007/978-3-642-39038-8_2},
abstract = {Object algebras are a new programming technique that enables a simple solution to basic extensibility and modularity issues in programming languages. While object algebras excel at defining modular features, the composition mechanisms for object algebras (and features) are still cumbersome and limited in expressiveness. In this paper we leverage two well-studied type system features, intersection types and type-constructor polymorphism, to provide object algebras with expressive and practical composition mechanisms. Intersection types are used for defining expressive run-time composition operators (combinators) that produce objects with multiple (feature) interfaces. Type-constructor polymorphism enables generic interfaces for the various object algebra combinators. Such generic interfaces can be used as a type-safe front end for a generic implementation of the combinators based on reflection. Additionally, we also provide a modular mechanism to allow different forms of self-references in the presence of delegation-based combinators. The result is an expressive, type-safe, dynamic, delegation-based composition technique for object algebras, implemented in Scala, which effectively enables a form of Feature-Oriented Programming using object algebras.},
booktitle = {Proceedings of the 27th European Conference on Object-Oriented Programming},
pages = {27–51},
numpages = {25},
location = {Montpellier, France},
series = {ECOOP'13}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten years of product line engineering at Danfoss: lessons learned and way ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {industrial experiences, product line adoption, product line evaluation},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2362536.2362545,
author = {Lee, Jihyun and Kang, Sungwon and Lee, Danhyung},
title = {A survey on software product line testing},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362545},
doi = {10.1145/2362536.2362545},
abstract = {Software product line (SPL) testing consists of two separate but closely related test engineering activities: domain testing and application testing. Various software product line testing approaches have been developed over the last decade, and surveys have been conducted on them. However, thus far none of them deeply addressed the questions of what researches have been conducted in order to overcome the challenges posed by the two separate testing activities and their relationships. Thus, this paper surveys the current software product line testing approaches by defining a reference SPL testing processes and identifying, based on them, key research perspectives that are important in SPL testing. Through this survey, we identify the researches that addressed the challenges and also derive open research opportunities from each perspective.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {31–40},
numpages = {10},
keywords = {software product line engineering, software product line testing, software testing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2647908.2655970,
author = {ter Beek, Maurice H. and de Vink, Erik P.},
title = {Software product line analysis with mCRL2},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655970},
doi = {10.1145/2647908.2655970},
abstract = {The mCRL2 language and supporting software provide a state-of-the-art tool suite for the verification of distributed systems. In this paper, we present the general principles, extrapolated from [7,8], which make us believe that mCRL2 can also be used for behavioral variability analysis of product families. The mCRL2 data language allows to smoothly deal with feature sets and attributes, its process language is sufficiently rich to model feature selection, as well as product behavior based on an FTS-like semantics. Because of the feature-orientation, our modeling strategy allows a natural refactoring of the semantic model of a product family into a parallel composition of components that reflects coherent sets of features. This opens the way for dedicated abstraction and reduction techniques that strengthen the prospect of a scalable verification approach to software product lines. In this paper, we sketch how to model product families in mCRL2 and how to apply a modular verification method, preparing the ground to further assess the scalability of our approach, in particular regarding model checking.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {78–85},
numpages = {8},
keywords = {behavioral analysis, model checking, modular verification, product families, variability},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1007/s11219-011-9165-4,
author = {Lochau, Malte and Oster, Sebastian and Goltz, Ursula and Sch\"{u}rr, Andy},
title = {Model-based pairwise testing for feature interaction coverage in software product line engineering},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9165-4},
doi = {10.1007/s11219-011-9165-4},
abstract = {Testing software product lines (SPLs) is very challenging due to a high degree of variability leading to an enormous number of possible products. The vast majority of today's testing approaches for SPLs validate products individually using different kinds of reuse techniques for testing. Because of their reusability and adaptability capabilities, model-based approaches are suitable to describe variability and are therefore frequently used for implementation and testing purposes of SPLs. Due to the enormous number of possible products, individual product testing becomes more and more infeasible. Pairwise testing offers one possibility to test a subset of all possible products. However, according to the best of our knowledge, there is no contribution discussing and rating this approach in the SPL context. In this contribution, we provide a mapping between feature models describing the common and variable parts of an SPL and a reusable test model in the form of statecharts. Thereby, we interrelate feature model-based coverage criteria and test model-based coverage criteria such as control and data flow coverage and are therefore able to discuss the potentials and limitations of pairwise testing. We pay particular attention to test requirements for feature interactions constituting a major challenge in SPL engineering. We give a concise definition of feature dependencies and feature interactions from a testing point of view, and we discuss adequacy criteria for SPL coverage under pairwise feature interaction testing and give a generalization to the T-wise case. The concept and implementation of our approach are evaluated by means of a case study from the automotive domain.},
journal = {Software Quality Journal},
month = sep,
pages = {567–604},
numpages = {38},
keywords = {Combinatorial testing, Feature interaction, Model-based engineering and testing, Software product lines, Test generation and coverage}
}

@article{10.1016/j.cl.2016.07.007,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {Evolutionary robust optimization for software product line scoping},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {P2},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.07.007},
doi = {10.1016/j.cl.2016.07.007},
abstract = {Background: Software product line (SPL) scoping is an important phase when planning for product line adoption. An SPL scope specifies: (1) the extent of the domain supported by the product line, (2) portfolio of products in the product line and (3) list of assets to be developed for reuse across the family of products.Issue: SPL scope planning is usually based on estimates about the state of the market and the engineering capabilities of the development team. One challenge with these estimates is that there are inaccuracies due to uncertainty in the environment or accuracy of measurement. This may result in issues ranging from suboptimal plans to infeasible plans.Objective: To address the above, we propose to include uncertainty as part of the SPL scoping model. Plans developed in consideration of uncertainty would be more robust against possible fluctuations in estimates.Approach: In this paper, a method to incorporate uncertainty in scoping optimization and its application to generate robust solutions is proposed. We capture uncertainty as part of the formulation and model scoping optimization as a multi-objective problem with profit and stability as fitness functions. Profit stability and feasibility stability are considered to represent stability concerns.Results: Results show that, compared to other scope optimization approaches, both performance stability and feasibility stability are improved while maintaining near optimal performance for profit objective. Also, generated results consist of solutions with trade-offs between profit and stability, providing the decision maker with enhanced decision support.Conclusion: Multi-objective optimization with stability consideration for SPL scoping provides project managers with a robust and flexible way to address uncertainty in the process of SPL scoping. HighlightsA robust multi-objective optimization approach for SPL scoping is proposed.Two types of stability are considered: performance stability and feasibility stability.Approach was able to find plans with higher stability.},
journal = {Comput. Lang. Syst. Struct.},
month = jan,
pages = {189–210},
numpages = {22},
keywords = {Evolutionary optimization, Robust optimization, Search-based software engineering, Software product line scoping}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1007/s10845-020-01572-3,
author = {Gauss, Leandro and Lacerda, Daniel P. and Cauchick Miguel, Paulo A.},
title = {Module-based product family design: systematic literature review and meta-synthesis},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01572-3},
doi = {10.1007/s10845-020-01572-3},
abstract = {Increased demand for a greater variety of products has forced many companies to rethink their strategies to offer more product variants without sacrificing production efficiency. In this context, research has found that such a trade-off can be properly managed by exploiting the module-based product family (MBPF) design. Over the years, active work in developing methods to design MBPFs has been carried out. Nevertheless, many of them have been created, and consequently exist, in isolation from one other. As a result, the adoption of these methods in industry and academy alike is inhibited by the seemingly broad array of material without a coherent organizing structure. To bridge this gap, this paper performs a systematic literature review and a meta-synthesis, wherein 72 methods to design MBPFs and their respective instances are connected in the form of a functional model and structured classes of design problems. These entities together serve as a meta-method for organizing the research on MBPF design, from which it was possible to identify the common underlying structure among the methods developed over the past 20&nbsp;years. The main contributions of this work include: (1) constructing a functional model that connects the design methods for MBPFs; (2) suggesting structured classes of design problems that complement the functional model by cataloging the techniques meant to execute each sub-function of the model; (3) proposing a construction heuristic to build and assess functional models and classes of design problems.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {265–312},
numpages = {48},
keywords = {Modularity, Product family design, Systematic literature review, Meta-synthesis, Functional model, Design science}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3382025.3414953,
author = {Abbas, Muhammad and Jongeling, Robbert and Lindskog, Claes and Enoiu, Eduard Paul and Saadatmand, Mehrdad and Sundmark, Daniel},
title = {Product line adoption in industry: an experience report from the railway domain},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414953},
doi = {10.1145/3382025.3414953},
abstract = {The software system controlling a train is typically deployed on various hardware architectures and must process various signals across those deployments. The increase of such customization scenarios and the needed adherence of the software to various safety standards in different application domains has led to the adoption of product line engineering within the railway domain. This paper explores the current state-of-practice of software product line development within a team developing industrial embedded software for a train propulsion control system. Evidence is collected using a focus group session with several engineers and through inspection of archival data. We report several benefits and challenges experienced during product line adoption and deployment. Furthermore, we identify and discuss improvement opportunities, focusing mainly on product line evolution and test automation.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {3},
numpages = {11},
keywords = {challenges and opportunities, overloaded assets, software product-line engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.knosys.2019.104883,
author = {Ayala, Inmaculada and Amor, Mercedes and Horcas, Jose-Miguel and Fuentes, Lidia},
title = {A goal-driven software product line approach for evolving multi-agent systems in the Internet of Things},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {184},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104883},
doi = {10.1016/j.knosys.2019.104883},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {Software product line, Evolution, Internet of Things, MAS-PL, Goal models, GORE}
}

@inproceedings{10.1145/2993236.2993251,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {Towards a software product line of trie-based collections},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993251},
doi = {10.1145/2993236.2993251},
abstract = {Collection data structures in standard libraries of programming languages are designed to excel for the average case by carefully balancing memory footprint and runtime performance. These implicit design decisions and hard-coded trade-offs do constrain users from using an optimal variant for a given problem. Although a wide range of specialized collections is available for the Java Virtual Machine (JVM), they introduce yet another dependency and complicate user adoption by requiring specific Application Program Interfaces (APIs) incompatible with the standard library.  A product line for collection data structures would relieve library designers from optimizing for the general case. Furthermore, a product line allows evolving the potentially large code base of a collection family efficiently. The challenge is to find a small core framework for collection data structures which covers all variations without exhaustively listing them, while supporting good performance at the same time.  We claim that the concept of Array Mapped Tries (AMTs) embodies a high degree of commonality in the sub-domain of immutable collection data structures. AMTs are flexible enough to cover most of the variability, while minimizing code bloat in the generator and the generated code. We implemented a Data Structure Code Generator (DSCG) that emits immutable collections based on an AMT skeleton foundation. The generated data structures outperform competitive hand-optimized implementations, and the generator still allows for customization towards specific workloads.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {168–172},
numpages = {5},
keywords = {Code generation, Hash trie, Immutability, Performance, Persistent data structure, Software product line},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1016/j.infsof.2012.07.020,
author = {Siegmund, Norbert and Rosenm\"{u}Ller, Marko and K\"{a}Stner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.020},
doi = {10.1016/j.infsof.2012.07.020},
abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {491–507},
numpages = {17},
keywords = {Measurement, Non-functional properties, Prediction, SPL Conqueror, Software product lines}
}

@article{10.1016/j.jss.2007.07.006,
author = {Del Rosso, Christian},
title = {Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.07.006},
doi = {10.1016/j.jss.2007.07.006},
abstract = {Software performance is an important non-functional quality attribute and software performance evaluation is an essential activity in the software development process. Especially in embedded real-time systems, software design and evaluation are driven by the needs to optimize the limited resources, to respect time deadlines and, at the same time, to produce the best experience for end-users. Software product family architectures add additional requirements to the evaluation process. In this case, the evaluation includes the analysis of the optimizations and tradeoffs for the whole products in the family. Performance evaluation of software product family architectures requires knowledge and a clear understanding of different domains: software architecture assessments, software performance and software product family architecture. We have used a scenario-driven approach to evaluate performance and dynamic memory management efficiency in one Nokia software product family architecture. In this paper we present two case studies. Furthermore, we discuss the implications and tradeoffs of software performance against evolvability and maintenability in software product family architectures.},
journal = {J. Syst. Softw.},
month = jan,
pages = {1–19},
numpages = {19},
keywords = {Dynamic memory management, Embedded real-time systems, Software architecture assessments, Software performance, Software product family}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Feature-oriented software development, Measurement and optimization, Non-functional properties, SPL Conqueror, Software product lines}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {Quality attributes, SPL, reconfiguration, software architecture, variability},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@article{10.1016/j.cie.2019.06.039,
author = {Okpoti, Evans Sowah and Jeong, In-Jae and Moon, Seung Ki},
title = {Decentralized determination of design variables among cooperative designers for product platform design in a product family},
year = {2019},
issue_date = {Sep 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.06.039},
doi = {10.1016/j.cie.2019.06.039},
journal = {Comput. Ind. Eng.},
month = sep,
pages = {601–614},
numpages = {14},
keywords = {Product family, Product platform design, Decentralized coordination, Collaborative design, Multi-agent}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Domain specific modeling, Model driven - software product lines, Quality evaluation, Sensitivity points, Software architecture, Variability management}
}

@article{10.1016/j.infsof.2012.02.005,
author = {Thurimella, Anil Kumar and Bruegge, Bernd},
title = {Issue-based variability management},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.005},
doi = {10.1016/j.infsof.2012.02.005},
abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {933–950},
numpages = {18},
keywords = {Empirical software engineering, Product line engineering, Rationale management, Requirements engineering}
}

@inproceedings{10.5555/1753235.1753274,
author = {Pech, Daniel and Knodel, Jens and Carbon, Ralf and Schitter, Clemens and Hein, Dirk},
title = {Variability management in small development organizations: experiences and lessons learned from a case study},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Product line practices promise to reduce development and maintenance efforts, to improve the productivity and to reduce the time to market by systematic reuse of commonalities and variabilities. However, in order to reap the fruits of exploiting those, an upfront investment is required. This paper presents a case study, which analyzes the cost-benefit ratio for one product line discipline -- variability management. Wikon GmbH -- a small German development organization evolving a product line of remote monitoring and controlling devices -- switched from manual, file-based conditional compilation to tool-supported decision models. We discuss experiences made and show that the break-even was reached with the 4th product derivation.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {285–294},
numpages = {10},
keywords = {decision model, evolution, product line engineering, software architecture, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.scico.2012.05.003,
author = {Laguna, Miguel A. and Crespo, Yania},
title = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.05.003},
doi = {10.1016/j.scico.2012.05.003},
abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {1010–1034},
numpages = {25},
keywords = {Evolution, Legacy system, Reengineering, Refactoring, Software product line}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Dynamic Software Product Lines, Dynamic variability, Feature models, Software architecture}
}

@article{10.1007/s10845-016-1267-1,
author = {Wang, Wenyuan and Mo, Daniel Y. and Wang, Yue and Tseng, Mitchell M.},
title = {Assessing the cost structure of component reuse in a product family for remanufacturing},
year = {2019},
issue_date = {Feb 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {2},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-016-1267-1},
doi = {10.1007/s10845-016-1267-1},
abstract = {Component reuse is a crucial remanufacturing strategy that assists manufacturers to achieve sustainable supply chain management. However, few manufacturers obtain economic benefits from component reuse strategies due to the demand for increasing product variety and its related complex cost structure. In this paper, we propose an integrated quantitative decision model to assess the economic aspects of component reuse for remanufacturing management. Given numerous cost factors, such as component manufacturing, reverse logistics, reprocessing, disposal and penalty costs, we derive the optimal acquisition cost to retrieve end-of-life products for component reuse. Then, we identify the component commonality effects to quantify the component reuse rate from a variety of end-of-life products. Finally, our models and results are demonstrated through an industrial case study. Accordingly, the cost savings from reusing components could be achieved by 25&nbsp;% of the manufacturing cost offered to acquire the used products from customers at a low reverse logistics cost. Based on the 80&nbsp;% yield rate observed in the case study, the commonality of components in a product family would affect 35&nbsp;% of the total cost savings of component reuse for remanufacturing.},
journal = {J. Intell. Manuf.},
month = feb,
pages = {575–587},
numpages = {13},
keywords = {Remanufacturing, Product design, Component reuse, Decision cost model}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {configurable system, decision propagation, software product lines, t-wise sampling, uniform random sampling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Empirical studies, Software product line, Systematic literature reviews, Variability management}
}

@article{10.5555/3288338.3288341,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Finding correlations of features affecting energy consumption and performance of web servers using the HADAS eco-assistant},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {100},
number = {11},
issn = {0010-485X},
abstract = {The impact of energy consumption on the environment and the economy is raising awareness of "green" software engineering. HADAS is an eco-assistant that makes developers aware of the influence of their designs and implementations on the energy consumption and performance of the final product. In this paper, we extend HADAS to better support the requirements of users: researchers, automatically dumping the energy-consumption of different software solutions; and developers, who want to perform a sustainability analysis of different software solutions. This analysis has been extended by adding Pearson's chi-squared differentials and Bootstrapping statistics, to automatically check the significance of correlations of the energy consumption, or the execution time, with any other variable (e.g., the number of users) that can influence the selection of a particular eco-efficient configuration. We have evaluated our approach by performing a sustainability analysis of the most common web servers (i.e. PHP servers) using the time and energy data measured with the Watts Up? Pro tool previously dumped in HADAS. We show how HADAS helps web server providers to make a trade-off between energy consumption and execution time, allowing them to sell different server configurations with different costs without modifying the hardware.},
journal = {Computing},
month = nov,
pages = {1155–1173},
numpages = {19},
keywords = {68M20, 68N30, 68U35, 97K80, Energy efficiency, Linux, Performance, Web servers}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Quality attributes, Service-based systems, Systematic literature review, Variability}
}

@inproceedings{10.1145/1629716.1629729,
author = {Liebig, J\"{o}rg and Apel, Sven and Lengauer, Christian and Leich, Thomas},
title = {RobbyDBMS: a case study on hardware/software product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629729},
doi = {10.1145/1629716.1629729},
abstract = {The development of a highly configurable data management system is a challenging task, especially if it is to be implemented on an embedded system that provides limited resources. We present a case study of such a data management system, called RobbyDBMS, and give it a feature-oriented design. In our case study, we evaluate the system's efficiency and variability. We pay particular attention to the interaction between the features of the data management system and the components of the underlying embedded platform. We also propose an integrated development process covering both hardware and software.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {63–68},
numpages = {6},
keywords = {FeatureC++, domain engineering, feature oriented software development, hardware product lines, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {architecture description language, dynamic reconfiguration, feature-oriented analysis, self-adaptive systems, software architecture},
location = {Trento, Italy},
series = {SAC '12}
}

@article{10.1007/s10664-015-9414-4,
author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and O\'{z} Cinne\'{z}Ide, Mel and Deb, Kalyanmoy},
title = {On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9414-4},
doi = {10.1007/s10664-015-9414-4},
abstract = {Search-based software engineering (SBSE) solutions are still not scalable enough to handle high-dimensional objectives space. The majority of existing work treats software engineering problems from a single or bi-objective point of view, where the main goal is to maximize or minimize one or two objectives. However, most software engineering problems are naturally complex in which many conflicting objectives need to be optimized. Software refactoring is one of these problems involving finding a compromise between several quality attributes to improve the quality of the system while preserving the behavior. To this end, we propose a novel representation of the refactoring problem as a many-objective one where every quality attribute to improve is considered as an independent objective to be optimized. In our approach based on the recent NSGA-III algorithm, the refactoring solutions are evaluated using a set of 8 distinct objectives. We evaluated this approach on one industrial project and seven open source systems. We compared our findings to: several other many-objective techniques (IBEA, MOEA/D, GrEA, and DBEA-Eps), an existing multi-objective approach a mono-objective technique and an existing refactoring technique not based on heuristic search. Statistical analysis of our experiments over 31 runs shows the efficiency of our approach.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2503–2545},
numpages = {43},
keywords = {Many-objective optimization, Refactoring, Search-based software engineering, Software quality}
}

@article{10.1007/s10845-012-0630-0,
author = {Shahzad, Kashif M. and Hadj-Hamou, Khaled},
title = {Integrated supply chain and product family architecture under highly customized demand},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-012-0630-0},
doi = {10.1007/s10845-012-0630-0},
abstract = {Mass customization efforts are challenged by an unpredictable growth or shrink in the market segments and shortened product life cycles which result in an opportunity loss and reduced profitability; hence we propose a concept of sustainable mass customization to address these challenges where an economically infeasible product for a market segment is replaced by an alternative superior product variant nearly at the cost of mass production. This concept provides sufficient time to restructure the product family architecture for the inclusion of a new innovative product variant while fulfilling the market segments with the customer delight and an extended profitability. To implement the concept of sustainable mass customization we have proposed the notions of generic-bill-Of-products (GBOP: list of product variants agreed for the market segments), its interface with generic-supply-chain-structure and strategic decisions about opening or closing of a market segment as an optimization MILP (mixed integer linear program) model including logistics and GBOP constraints. Model is tested with the varying market segments demands, sales prices and production costs against 1 to 40 market segments. Simulation results provide us an optimum GBOP, its respective segments and decisions on the opening or closing of the market segments to sustain mass customization efforts.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1005–1018},
numpages = {14},
keywords = {Generic-bill-of-products, Mass customization, Product family architecture, Supply chain configuration}
}

@inproceedings{10.1145/3382026.3431247,
author = {Meixner, Kristof},
title = {Integrating Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431247},
doi = {10.1145/3382026.3431247},
abstract = {The Industry 4.0 initiative envisions the flexible and optimized production of customized products on Cyber-Physical Production Systems (CPPSs) that consist of subsystems coordinated to conduct complex production processes. Hence, accurate CPPS modeling requires integrating the modeling of variability for Product-Process-Resource (PPR) aspects. Yet, current variability modeling approaches treat structural and behavioral variability separately, leading to inaccurate CPPS production models that impede CPPS engineering and optimization. This paper proposes a PhD project for integrated variability modeling of PPR aspects to improve the accuracy of production models with variability for CPPS engineers and production optimizers. The research project follows the Design Science approach aiming for the iterative design and evaluation of (a) a framework to categorize currently incomplete and scattered models and methods for PPR variability modeling as a foundation for an integrated model; and (b) a modeling approach for more accurate integrated PPR variability modeling. The planned research will provide the Software Product Line (SPL) and CPPS engineering research communities with (a) novel models, methods, and insights on integrated PPR variability modeling, (b) open data from CPPS engineering use cases for common modeling, and (c) empirical data from field studies for shared analysis and evaluation.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {96–103},
numpages = {8},
keywords = {Cyber-Physical Production System, Product-Process-Resource, Variability Modelling},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/1939399.1939424,
author = {Kim, Chang Hwan Peter and Bodden, Eric and Batory, Don and Khurshid, Sarfraz},
title = {Reducing configurations to monitor in a software product line},
year = {2010},
isbn = {3642166113},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A software product line is a family of programs where each program is defined by a unique combination of features. Product lines, like conventional programs, can be checked for safety properties through execution monitoring. However, because a product line induces a number of programs that is potentially exponential in the number of features, it would be very expensive to use existing monitoring techniques: one would have to apply those techniques to every single program. Doing so would also be wasteful because many programs can provably never violate the stated property. We introduce a monitoring technique dedicated to product lines that, given a safety property, statically determines the feature combinations that cannot possibly violate the property, thus reducing the number of programs to monitor. Experiments show that our technique is effective, particularly for safety properties that crosscut many optional features.},
booktitle = {Proceedings of the First International Conference on Runtime Verification},
pages = {285–299},
numpages = {15},
location = {St. Julians, Malta},
series = {RV'10}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Software Product Line (SPL) adoption, Taxonomy-Based Software Construction (TABASCO) toolkit},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1007/978-3-642-12107-4_7,
author = {Zschaler, Steffen and S\'{a}nchez, Pablo and Santos, Jo\~{a}o and Alf\'{e}rez, Mauricio and Rashid, Awais and Fuentes, Lidia and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Kulesza, Uir\'{a}},
title = {VML* – a family of languages for variability management in software product lines},
year = {2009},
isbn = {3642121063},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12107-4_7},
doi = {10.1007/978-3-642-12107-4_7},
abstract = {Managing variability is a challenging issue in software-product-line engineering. A key part of variability management is the ability to express explicitly the relationship between variability models (expressing the variability in the problem space, for example using feature models) and other artefacts of the product line, for example, requirements models and architecture models. Once these relations have been made explicit, they can be used for a number of purposes, most importantly for product derivation, but also for the generation of trace links or for checking the consistency of a product-line architecture. This paper bootstraps techniques from product-line engineering to produce a family of languages for variability management for easing the creation of new members of the family of languages. We show that developing such language families is feasible and demonstrate the flexibility of our language family by applying it to the development of two variability-management languages.},
booktitle = {Proceedings of the Second International Conference on Software Language Engineering},
pages = {82–102},
numpages = {21},
keywords = {domain-specific languages, family of languages, software product lines, variability management},
location = {Denver, CO},
series = {SLE'09}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Feature Selection, Linux, Software Product Lines, Software Tailoring},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {benchmark, product lines, software evolution, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.jss.2011.06.026,
author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
title = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.026},
doi = {10.1016/j.jss.2011.06.026},
abstract = {Abstract: Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2208–2221},
numpages = {14},
keywords = {Configuration, Feature models, Genetic algorithm, Optimization, Product derivation, Software product lines}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Cyber-physical systems, Product line engineering, Automated configuration, Multi-stage and multi-step configuration process, Constraint classification, Variability modeling, Real-world case studies}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {dynamic software product lines, reconfiguration decisions, timed automata},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.asoc.2016.07.040,
author = {Xue, Yinxing and Zhong, Jinghui and Tan, Tian Huat and Liu, Yang and Cai, Wentong and Chen, Manman and Sun, Jun},
title = {IBED},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.040},
doi = {10.1016/j.asoc.2016.07.040},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose to combine IBEA and DE for the optimal feature selection in SPLE.We propose a feedback-directed method into EAs to improve the correctness of results.Our IBED with the seeding method has significantly shortened the search time.In most cases, IBED finds more unique and non-dominated solutions than IBEA. Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1215–1231},
numpages = {17},
keywords = {Differential evolutionary algorithm (DE), Indicator-based evolutionary algorithm (IBEA), Optimal feature selection, Software product line engineering}
}

@article{10.1007/s10618-010-0175-9,
author = {Silla, Carlos N. and Freitas, Alex A.},
title = {A survey of hierarchical classification across different application domains},
year = {2011},
issue_date = {January   2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1–2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-010-0175-9},
doi = {10.1007/s10618-010-0175-9},
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
journal = {Data Min. Knowl. Discov.},
month = jan,
pages = {31–72},
numpages = {42},
keywords = {DAG-structured class hierarchies, Hierarchical classification, Tree-structured class hierarchies}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/2629395,
author = {Jain, Radhika and Cao, Lan and Mohan, Kannan and Ramesh, Balasubramaniam},
title = {Situated Boundary Spanning: An Empirical Investigation of Requirements Engineering Practices in Product Family Development},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629395},
doi = {10.1145/2629395},
abstract = {Requirements Engineering (RE) faces considerable challenges that are often related to boundaries between various stakeholders involved in the software development process. These challenges may be addressed by boundary spanning practices. We examine how boundary spanning can be adapted to address RE challenges in Product Family Development (PFD), a context that involves complex RE. We study two different development approaches, namely, conventional and agile PFD, because these present considerably different challenges. Our findings from a multisite case study present boundary spanning as a solution to improve the quality of RE processes and highlight interesting differences in how boundary spanner roles and boundary objects are adapted in conventional and agile PFD.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {29},
keywords = {Boundary spanning, agile development, product family development, requirements engineering}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Classification, Data mining, Feature ranking, Feature selection}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {domain models, feature interaction, sampling algorithms, software product lines, testing},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {architecture, decision-making, design exploration, model-driven engineering, multi-criteria decision analysis, systems engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106210,
author = {Markiegi, Urtzi and Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based product line fault detection allocating test cases iteratively},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106210},
doi = {10.1145/3106195.3106210},
abstract = {The large number of possible configurations makes it unfeasible to test every single system variant in a product line. Consequently, a small subset of the product line products must be selected, typically following combinatorial interaction testing approaches. Recently, many product line engineering approaches have considered the selection and prioritization of relevant products within the product line. In a further step, these products are thoroughly tested individually. However, the test cases that must be executed in each of the products are not always insignificant, and in systems such as Cyber-Physical System Product Lines (CPSPLs), their test execution time can vary from tens to thousands of seconds. This issue leads to spending a lot of time testing each individual product. To solve this problem we propose a search-based approach to perform the testing of product lines by allocating small number of test cases in each of the products. This approach increases the probability of detecting faults faster. Specifically, our search-based approach obtains a set of products, which are derived from using any state-of-the-art approach as inputs, and a set of attributed test cases. As an output a list of allocated test cases for each product is obtained. We also define a novel fitness function to guide the search and we propose corresponding crossover and mutation operators. The search and test process is iteratively repeated until the time budget is consumed. We performed an evaluation with a CPSPL as a case study. Results suggest that our approach can reduce the fault detection time by 61% and 65% on average when compared with the traditional test process and the Random Search algorithm respectively.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {123–132},
numpages = {10},
keywords = {Fault Detection, Product Line Testing, Search-based Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1007/s10664-014-9336-6,
author = {Sobernig, Stefan and Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert},
title = {Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9336-6},
doi = {10.1007/s10664-014-9336-6},
abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition--it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features  in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1670–1705},
numpages = {36},
keywords = {Feature-oriented programming, Fuji, Software measurement, Software product lines, Structural cohesion, Structural coupling}
}

@inproceedings{10.1145/2648511.2648521,
author = {Olaechea, Rafael and Rayside, Derek and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Comparison of exact and approximate multi-objective optimization for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648521},
doi = {10.1145/2648511.2648521},
abstract = {Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (e.g., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good parameter ranges. Finally, we discuss the tradeoff between accuracy and time consumption when using exact and approximate techniques for SPL multi-objective optimization and guide stakeholders to choose one or the other in practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {92–101},
numpages = {10},
keywords = {multi-objective optimization, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Feature Identification, Natural Language Documents, Reverse Engineering, Software Product Lines, Systematic Literature Review, Variability Extraction},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {cyber-physical system product lines, search-based software engineering, test case selection},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2866614.2866627,
author = {Devroey, Xavier and Perrouin, Gilles and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Search-based Similarity-driven Behavioural SPL Testing},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866627},
doi = {10.1145/2866614.2866627},
abstract = {Dissimilar test cases have been proven to be effective to reveal faults in software systems. In the Software Product Line (SPL) context, this criterion has been applied successfully to mimic combinatorial interaction testing in an efficient and scalable manner by selecting and prioritising most dissimilar configurations of feature models using evolutionary algorithms. In this paper, we extend dissimilarity to behavioural SPL models (FTS) in a search-based approach, and evaluate its effectiveness in terms of product and fault coverage. We investigate different distances as well as as single-objective algorithms, (dissimilarity on actions, random, all-actions). Our results on four case studies show the relevance of dissimilarity-based test generation for behavioural SPL models, especially on the largest case-study where no other approach can match it.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {89–96},
numpages = {8},
keywords = {Dissimilarity Testing, Featured Transition System, Software Product Line Testing},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {CVL, clafer, energy efficiency, metrics, optimisation, repository, software product line, variability},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/3233027.3233046,
author = {Beek, Maurice H. ter and Fantechi, Alessandro and Gnesi, Stefania},
title = {Product line models of large cyber-physical systems: the case of ERTMS/ETCS},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233046},
doi = {10.1145/3233027.3233046},
abstract = {A product line perspective may help to understand the possible variants in interactions between the subsystems of a large, cyber-physical system. This observation is exemplified in this paper by proposing a feature model of the family of ERTMS/ETCS train control systems and their foreseen extensions. This model not only shows the different components that have to be installed when deploying the system at the different levels established by the ERTMS/ETCS standards, but it also helps to identify and discuss specific issues, such as the borders between onboard and wayside equipment, different manufacturers of the subsystems, interoperability among systems developed at different levels, backward compatibility of trains equipped with higher level equipment running on lines equipped with lower level equipment, and evolution towards future trends of railway signalling. The feature model forms the basis for formal modelling of the behaviour of the critical components of the system and for evaluating the overall cost, effectiveness and sustainability, for example by adding cost and performance attributes to the feature model.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {208–214},
numpages = {7},
keywords = {ERTMS/ETCS train control systems, cyber-physical systems, feature models, product lines, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Commonality decision, Product platform scope, Software product line engineering}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Extended product line, Product configuration, Systematic literature review}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@inproceedings{10.1145/2517208.2517213,
author = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
title = {A comparison of product-based, feature-based, and family-based type checking},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517213},
doi = {10.1145/2517208.2517213},
abstract = {Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {115–124},
numpages = {10},
keywords = {feature-oriented programming, fuji, product-line analysis, type checking},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing selection of competing features via feedback-directed evolutionary algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = {Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {SAT solvers, Software product line, evolutionary algorithms},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@article{10.1145/3389397,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Pattern-based Interactive Configuration Derivation for Cyber-physical System Product Lines},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3389397},
doi = {10.1145/3389397},
abstract = {Deriving a Cyber-Physical System (CPS) product from a product line requires configuring hundreds to thousands of configurable parameters of components and devices from multiple domains, e.g., computing, control, and communication. A fully automated configuration process for a CPS product line is seldom possible in practice, and a dynamic and interactive process is expected. Therefore, some configurable parameters are to be configured manually, and the rest can be configured either automatically or manually, depending on pre-defined constraints, the order of configuration steps, and previous configuration data in such a dynamic and interactive configuration process. In this article, we propose a pattern-based, interactive configuration derivation methodology (named as Pi-CD) to maximize opportunities of automatically deriving correct configurations of CPSs by benefiting from pre-defined constraints and configuration data of previous configuration steps. Pi-CD requires architectures of CPS product lines modeled with Unified Modeling Language extended with four types of variabilities, along with constraints specified in Object Constraint Language (OCL). Pi-CD is equipped with 324 configuration derivation patterns that we defined by systematically analyzing the OCL constructs and semantics. We evaluated Pi-CD by configuring 20 CPS products of varying complexity from two real-world CPS product lines. Results show that Pi-CD can achieve up to 72% automation degree with a negligible time cost. Moreover, its time performance remains stable with the increase in the number of configuration parameters as well as constraints.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jun,
articleno = {44},
numpages = {24},
keywords = {Product line engineering, configuration derivation, object constraint language, product configuration}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Evolutionary algorithm, Metaheuristics, Search based software engineering, Software product line, Systematic mapping study}
}

@article{10.1007/s10664-014-9359-z,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: proposing theories from a case study},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9359-z},
doi = {10.1007/s10664-014-9359-z},
abstract = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1623–1669},
numpages = {47},
keywords = {Case study, Software architecture, Software product line, Variability}
}

@inproceedings{10.1007/978-3-642-41533-3_23,
author = {Nie, Kunming and Yue, Tao and Ali, Shaukat and Zhang, Li and Fan, Zhiqiang},
title = {Constraints: The Core of Supporting Automated Product Configuration of Cyber-Physical Systems},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_23},
doi = {10.1007/978-3-642-41533-3_23},
abstract = {In the context of product line engineering of cyber-physical systems, there exists a large number of constraints to support, for example, consistency checking of design decisions made in hardware and software components during configuration. Manual configuration is not feasible in this context considering that managing and manipulating all these constraints in a real industrial context is very complicated and thus warrants an automated solution. Typical automation activities in this context include automated configuration value inference, optimizing configuration steps and consistency checking. However, to this end, relevant constraints have to be well-specified and characterized in the way such that automated configuration can be enabled. In this paper, we classify and characterize constraints that are required to be specified to support most of the key functionalities of any automated product configuration solution, based on our experience of studying three industrial product lines.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {370–387},
numpages = {18},
keywords = {Classification, Configuration, Constraints, Cyber-Physical Systems, Industrial Case Studies, Product Line Engineering}
}

@inproceedings{10.1145/2791060.2791073,
author = {Lachmann, Remo and Lity, Sascha and Lischke, Sabrina and Beddig, Simon and Schulze, Sandro and Schaefer, Ina},
title = {Delta-oriented test case prioritization for integration testing of software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791073},
doi = {10.1145/2791060.2791073},
abstract = {Software product lines have potential to allow for mass customization of products. Unfortunately, the resulting, vast amount of possible product variants with commonalities and differences leads to new challenges in software testing. Ideally, every product variant should be tested, especially in safety-critical systems. However, due to the exponentially increasing number of product variants, testing every product variant is not feasible. Thus, new concepts and techniques are required to provide efficient SPL testing strategies exploiting the commonalities of software artifacts between product variants to reduce redundancy in testing. In this paper, we present an efficient integration testing approach for SPLs based on delta modeling. We focus on test case prioritization. As a result, only the most important test cases for every product variant are tested, reducing the number of executed test cases significantly, as testing can stop at any given point because of resource constraints while ensuring that the most important test cases have been covered. We present the general concept and our evaluation results. The results show a measurable reduction of executed test cases compared to single-software testing approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {81–90},
numpages = {10},
keywords = {architecture-based testing, delta-oriented software product lines, regression testing, test case prioritization},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {active learning, adaptive instance selection, benchmarking, fractional factorial design, non-functional properties},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@inproceedings{10.5555/1884110.1884147,
author = {Jansen, Slinger and Houben, Geert-Jan and Brinkkemper, Sjaak},
title = {Customization realization in multi-tenant web applications: case studies from the library sector},
year = {2010},
isbn = {3642139108},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {There are insufficient examples available of how customization is realized in multi-tenant web applications, whereas developers are looking for examples and patterns as inspiration for their own web applications. This paper presents an overview of how variability realization techniques from the software product line world can be applied to realize customization when building multitenant web applications. The paper addresses this issue by providing a catalogue of customization realization techniques, which are illustrated using occurrences of customization in two practical innovative cases from the library sector. The catalogue and its examples assist developers in evaluating and selecting customization realization techniques for their multi-tenant web application.},
booktitle = {Proceedings of the 10th International Conference on Web Engineering},
pages = {445–459},
numpages = {15},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1145/302405.302409,
author = {DeBaud, Jean-Marc and Schmid, Klaus},
title = {A systematic approach to derive the scope of software product lines},
year = {1999},
isbn = {1581130740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302405.302409},
doi = {10.1145/302405.302409},
booktitle = {Proceedings of the 21st International Conference on Software Engineering},
pages = {34–43},
numpages = {10},
keywords = {domain engineering, product line scoping, reuse economic models, software product line},
location = {Los Angeles, California, USA},
series = {ICSE '99}
}

@inproceedings{10.1007/978-3-642-25535-9_29,
author = {Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Asadi, Mohsen and Bagheri, Ebrahim and Bo\v{s}kovi\'{c}, Marko},
title = {A quality aggregation model for service-oriented software product lines based on variability and composition patterns},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_29},
doi = {10.1007/978-3-642-25535-9_29},
abstract = {Quality evaluation is a challenging task in monolithic software systems. It is even more complex when it comes to Service-Oriented Software Product Lines (SOSPL), as it needs to analyze the attributes of a family of SOA systems. In SOSPL, variability can be planned and managed at the architectural level to develop a software product with the same set of functionalities but different degrees of non-functional quality attribute satisfaction. Therefore, architectural quality evaluation becomes crucial due to the fact that it allows for the examination of whether or not the final product satisfies and guarantees all the ranges of quality requirements within the envisioned scope. This paper addresses the open research problem of aggregating QoS attribute ranges with respect to architectural variability. Previous solutions for quality aggregation do not consider architectural variability for composite services. Our approach introduces variability patterns that can possibly occur at the architectural level of an SOSPL. We propose an aggregation model for QoS computation which takes both variability and composition patterns into account.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {436–451},
numpages = {16},
keywords = {QoS aggregation, feature modeling, non-functional properties, process family, service variability, service-oriented architecture (SOA), software product line (SPL), variability management},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@inproceedings{10.1145/2362536.2362544,
author = {Dietrich, Christian and Tartler, Reinhard and Schr\"{o}der-Preikschat, Wolfgang and Lohmann, Daniel},
title = {A robust approach for variability extraction from the Linux build system},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362544},
doi = {10.1145/2362536.2362544},
abstract = {With more than 11,000 optional and alternative features, the Linux kernel is a highly configurable piece of software. Linux is generally perceived as a textbook example for preprocessor-based product derivation, but more than 65 percent of all features are actually handled by the build system. Hence, variability-aware static analysis tools have to take the build system into account.However, extracting variability information from the build system is difficult due to the declarative and turing-complete make language. Existing approaches based on text processing do not cover this challenges and tend to be tailored to a specific Linux version and architecture. This renders them practically unusable as a basis for variability-aware tool support -- Linux is a moving target!We describe a robust approach for extracting implementation variability from the Linux build system. Instead of extracting the variability information by a text-based analysis of all build scripts, our approach exploits the build system itself to produce this information. As our results show, our approach is robust and works for all versions and architectures from the (git-)history of Linux.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {21–30},
numpages = {10},
keywords = {Linux, VAMOS, build systems, configurability, kbuild, maintenance, static analysis},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Service composition, Feature model, Software product lines, Self adaptation}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Variability, Configuration workflow, Process mining, Process discovery, Clustering}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {configuration, literature review, performance, product line, product requirements, scalability, survey},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1016/j.future.2018.09.006,
author = {Munoz, Daniel-Jesus and Montenegro, Jos\'{e} A. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Energy-aware environments for the development of green applications for cyber–physical systems},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.006},
doi = {10.1016/j.future.2018.09.006},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {536–554},
numpages = {19},
keywords = {Energy consumption, Cyber–physical systems, Green plugin, HADAS eco-assistant}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Constraint solving, Feature models, Multi-objective evolutionary algorithms, Search-based software engineering, Software product lines}
}

@article{10.1016/j.cie.2021.107742,
author = {Longo, Francesco and Padovano, Antonio and Cimmino, Barbara and Pinto, Paolo},
title = {Towards a mass customization in the fashion industry: An evolutionary decision aid model for apparel product platform design and optimization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107742},
doi = {10.1016/j.cie.2021.107742},
journal = {Comput. Ind. Eng.},
month = dec,
numpages = {20},
keywords = {Industry 4.0, Product platform design, Mass customization, Fashion industry, Genetic algorithm, Decision support system}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@article{10.1016/j.infsof.2015.12.004,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Model-based incremental conformance checking to enable interactive product configuration},
year = {2016},
issue_date = {April 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {72},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.12.004},
doi = {10.1016/j.infsof.2015.12.004},
abstract = {ContextModel-based product line engineering (PLE) is a paradigm that can enable automated product configuration of large-scale software systems, in which models are used as an abstract specification of commonalities and variabilities of products of a product line. ObjectiveIn the context of PLE, providing immediate feedback on the correctness of a manual configuration step to users has a practical impact on whether a configuration process with tool support can be successfully adopted in practice. MethodIn an existing work, a UML-based variability modeling methodology named as SimPL and an interactive configuration process was proposed. Based on the existing work, we propose an automated, incremental and efficient conformance checking approach to ensure that the manual configuration of a variation point conforms to a set of pre-defined conformance rules specified in the Object Constraint Language (OCL). The proposed approach, named as Zen-CC, has been implemented as an integrated part of our product configuration and derivation tool: Zen-Configurator. ResultsThe performance and scalability of Zen-CC have been evaluated with a real-world case study. Results show that Zen-CC significantly outperformed two baseline engines in terms of performance. Besides, the performance of Zen-CC remains stable during the configuration of all the 10 products of the product line and its efficiency also remains un-impacted even with the growing product complexity, which is not the case for both of the baseline engines. ConclusionThe results suggest that Zen-CC performs practically well and is much more scalable than the two baseline engines and is scalable for configuring products with a larger number of variation points.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {68–89},
numpages = {22},
keywords = {Incremental conformance checking, Interactive product configuration, Model based engineering, Product line engineering, Variation point}
}

@article{10.1007/s10664-017-9499-z,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Reengineering legacy applications into software product lines: a systematic mapping},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9499-z},
doi = {10.1007/s10664-017-9499-z},
abstract = {Software Product Lines (SPLs) are families of systems that share common assets allowing a disciplined reuse. Rarely SPLs start from scratch, instead they usually start from a set of existing systems that undergo a reengineering process. Many approaches to conduct the reengineering process have been proposed and documented in research literature. This scenario is a clear testament to the interest in this research area. We conducted a systematic mapping study to provide an overview of the current research on reengineering of existing systems to SPLs, identify the community activity in regarding of venues and frequency of publications in this field, and point out trends and open issues that could serve as references for future research. This study identified 119 relevant publications. These primary sources were classified in six different dimensions related to reengineering phases, strategies applied, types of systems used in the evaluation, input artefacts, output artefacts, and tool support. The analysis of the results points out the existence of a consolidate community on this topic and a wide range of strategies to deal with different phases and tasks of the reengineering process, besides the availability of some tools. We identify some open issues and areas for future research such as the implementation of automation and tool support, the use of different sources of information, need for improvements in the feature management, the definition of ways to combine different strategies and methods, lack of sophisticated refactoring, need for new metrics and measures and more robust empirical evaluation. Reengineering of existing systems into SPLs is an active research topic with real benefits in practice. This mapping study motivates new research in this field as well as the adoption of systematic reuse in software companies.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2972–3016},
numpages = {45},
keywords = {Evolution, Legacy systems, Product family, Reengineering, Systematic reuse}
}

@article{10.1016/j.jss.2019.01.057,
author = {Kr\"{u}ger, Jacob and Mukelabai, Mukelabai and Gu, Wanzi and Shen, Hui and Hebig, Regina and Berger, Thorsten},
title = {Where is my feature and what is it about? A case study on recovering feature facets},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.057},
doi = {10.1016/j.jss.2019.01.057},
journal = {J. Syst. Softw.},
month = jun,
pages = {239–253},
numpages = {15},
keywords = {Feature location, Marlin, Bitcoin-wallet, Case study, Feature facets, Software product line}
}

@article{10.1007/s10515-010-0066-8,
author = {Apel, Sven and K\"{a}stner, Christian and Gr\"{o}βlinger, Armin and Lengauer, Christian},
title = {Type safety for feature-oriented product lines},
year = {2010},
issue_date = {September 2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0066-8},
doi = {10.1007/s10515-010-0066-8},
abstract = {A feature-oriented product line is a family of programs that share a common set of features. A feature implements a stakeholder's requirement and represents a design decision or configuration option. When added to a program, a feature involves the introduction of new structures, such as classes and methods, and the refinement of existing ones, such as extending methods. A feature-oriented decomposition enables a generator to create an executable program by composing feature code solely on the basis of the feature selection of a user--no other information needed. A key challenge of product line engineering is to guarantee that only well-typed programs are generated. As the number of valid feature combinations grows combinatorially with the number of features, it is not feasible to type check all programs individually. The only feasible approach is to have a type system check the entire code base of the feature-oriented product line. We have developed such a type system on the basis of a formal model of a feature-oriented Java-like language. The type system guaranties type safety for feature-oriented product lines. That is, it ensures that every valid program of a well-typed product line is well-typed. Our formal model including type system is sound and complete.},
journal = {Automated Software Engg.},
month = sep,
pages = {251–300},
numpages = {50},
keywords = {Feature featherweight Java, Feature-oriented programming, Safe composition, Software product lines, Type systems}
}

@article{10.1007/s10515-011-0080-5,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Apel, Sven and Saake, Gunter},
title = {Flexible feature binding in software product lines},
year = {2011},
issue_date = {June      2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0080-5},
doi = {10.1007/s10515-011-0080-5},
abstract = {A software product line (SPL) is a family of programs that share assets from a common code base. The programs of an SPL can be distinguished in terms of features, which represent units of program functionality that satisfy stakeholders' requirements. The features of an SPL can be bound either statically at program compile time or dynamically at run time. Both binding times are used in SPL development and have different advantages. For example, dynamic binding provides high flexibility whereas static binding supports fine-grained customizability without any impact on performance (e.g., for use on embedded systems). However, contemporary techniques for implementing SPLs force a programmer to choose the binding time already when designing an SPL and to mix different implementation techniques when multiple binding times are needed. We present an approach that integrates static and dynamic feature binding seamlessly. It allows a programmer to implement an SPL once and to decide per feature at deployment time whether it should be bound statically or dynamically. Dynamic binding usually introduces an overhead regarding resource consumption and performance. We reduce this overhead by statically merging features that are used together into dynamic binding units. A program can be configured at run time by composing binding units on demand. We use feature models to ensure that only valid feature combinations can be selected at compile and at run time. We provide a compiler and evaluate our approach on the basis of two non-trivial SPLs.},
journal = {Automated Software Engg.},
month = jun,
pages = {163–197},
numpages = {35},
keywords = {Dynamic binding, Feature binding time, Feature composition, Feature-oriented programming, Software product lines, Static binding}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Method Engineering, Method Oriented Architecture MOA, Semantic Web, Software Development, Software Product Line}
}

@inproceedings{10.1145/2371401.2371404,
author = {Th\"{u}m, Thomas and Schaefer, Ina and Apel, Sven and Hentschel, Martin},
title = {Family-based deductive verification of software product lines},
year = {2012},
isbn = {9781450311298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371401.2371404},
doi = {10.1145/2371401.2371404},
abstract = {A software product line is a set of similar software products that share a common code base. While software product lines can be implemented efficiently using feature-oriented programming, verifying each product individually does not scale, especially if human effort is required (e.g., as in interactive theorem proving). We present a family-based approach of deductive verification to prove the correctness of a software product line efficiently. We illustrate and evaluate our approach for software product lines written in a feature-oriented dialect of Java and specified using the Java Modeling Language. We show that the theorem prover KeY can be used off-the-shelf for this task, without any modifications. Compared to the individual verification of each product, our approach reduces the verification time needed for our case study by more than 85%.},
booktitle = {Proceedings of the 11th International Conference on Generative Programming and Component Engineering},
pages = {11–20},
numpages = {10},
keywords = {deductive verification, product-line analysis, program families, software product lines, theorem proving},
location = {Dresden, Germany},
series = {GPCE '12}
}

@inproceedings{10.1145/2430502.2430518,
author = {Liebig, J\"{o}rg and Daniel, Rolf and Apel, Sven},
title = {Feature-oriented language families: a case study},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430518},
doi = {10.1145/2430502.2430518},
abstract = {Software-language engineering is gaining momentum in research and practice, but it faces many challenges regarding language evolution, reuse, and variation. We propose language families, a feature-oriented approach to language engineering inspired by product lines and program families. The goal is to systematically manage the development and evolution of variants and versions of a software language in terms of the language features it provides. We offer a tool chain for the development and management of composable language features and language families. By means of a case study on the Web-programming language Mobl, we evaluate the practicality of our approach and discuss our experiences, open issues, and perspectives.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {11},
numpages = {8},
keywords = {feature-oriented programming, language evolution, language families},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {BPM, Business process management, PL, Software product line}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {decision modelling, design-space exploration, multi-criteria decision analysis, multi-objective optimisation},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1016/j.jss.2007.06.002,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Industrial validation of COVAMOF},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.06.002},
doi = {10.1016/j.jss.2007.06.002},
abstract = {COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%.},
journal = {J. Syst. Softw.},
month = apr,
pages = {584–600},
numpages = {17},
keywords = {Industrial validation, Product family engineering, Software Variability Management}
}

@article{10.1007/s10270-017-0641-6,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Enabling automated requirements reuse and configuration},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0641-6},
doi = {10.1007/s10270-017-0641-6},
abstract = {A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2177–2211},
numpages = {35},
keywords = {Configuration, Feature model, Product line, Requirements, Reuse}
}

@article{10.1016/j.infsof.2019.08.007,
author = {Nogueira Teixeira, Eld\^{a}nae and Aleixo, Fellipe Ara\'{u}jo and Am\^{a}ncio, Francisco Dione de Sousa and OliveiraJr, Edson and Kulesza, Uir\'{a} and Werner, Cl\'{a}udia},
title = {Software process line as an approach to support software process reuse: A systematic literature review},
year = {2019},
issue_date = {Dec 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {116},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.08.007},
doi = {10.1016/j.infsof.2019.08.007},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Systematic review, Software process, Process reuse, Software process line, Process variability management}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.1016/j.jss.2017.03.044,
author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
title = {Source code metrics},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.044},
doi = {10.1016/j.jss.2017.03.044},
abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
journal = {J. Syst. Softw.},
month = jun,
pages = {164–197},
numpages = {34},
keywords = {Aspect-oriented metrics, Feature-oriented metrics, Object-oriented metrics, Software metrics, Source code metrics, Systematic mapping study}
}

@inproceedings{10.1145/3278122.3278123,
author = {Nieke, Michael and Mauro, Jacopo and Seidl, Christoph and Th\"{u}m, Thomas and Yu, Ingrid Chieh and Franzke, Felix},
title = {Anomaly analyses for feature-model evolution},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278123},
doi = {10.1145/3278122.3278123},
abstract = {Software Product Lines (SPLs) are a common technique to capture families of software products in terms of commonalities and variabilities. On a conceptual level, functionality of an SPL is modeled in terms of features in Feature Models (FMs). As other software systems, SPLs and their FMs are subject to evolution that may lead to the introduction of anomalies (e.g., non-selectable features). To fix such anomalies, developers need to understand the cause for them. However, for large evolution histories and large SPLs, explanations may become very long and, as a consequence, hard to understand. In this paper, we present a method for anomaly detection and explanation that, by encoding the entire evolution history, identifies the evolution step of anomaly introduction and explains which of the performed evolution operations lead to it. In our evaluation, we show that our method significantly reduces the complexity of generated explanations.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {188–201},
numpages = {14},
keywords = {Anomalies, Evolution, Evolution Operation, Explanation, Feature Model, Software Product Line},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@article{10.1007/s10270-019-00752-x,
author = {Anjorin, Anthony and Buchmann, Thomas and Westfechtel, Bernhard and Diskin, Zinovy and Ko, Hsiang-Shang and Eramo, Romina and Hinkel, Georg and Samimi-Dehkordi, Leila and Z\"{u}ndorf, Albert},
title = {Benchmarking bidirectional transformations: theory, implementation, application, and assessment},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00752-x},
doi = {10.1007/s10270-019-00752-x},
abstract = {Bidirectional transformations (bx) are relevant for a wide range of application domains. While bx problems may be solved with unidirectional languages and tools, maintaining separate implementations of forward and backward synchronizers with mutually consistent behavior can be difficult, laborious, and error-prone. To address the challenges involved in handling bx problems, dedicated languages and tools for bx have been developed. Due to their heterogeneity, however, the numerous and diverse approaches to bx are difficult to compare, with the consequence that fundamental differences and similarities are not yet well understood. This motivates the need for suitable benchmarks that facilitate the comparison of bx approaches. This paper provides a comprehensive treatment of benchmarking bx, covering theory, implementation, application, and assessment. At the level of theory, we introduce a conceptual framework that defines and classifies architectures of bx tools. At the level of implementation, we describe Benchmarx, an infrastructure for benchmarking bx tools which is based on the conceptual framework. At the level of application, we report on a wide variety of solutions to the well-known Families-to-Persons benchmark, which were developed and compared with the help of Benchmarx. At the level of assessment, we reflect on the usefulness of the Benchmarx approach to benchmarking bx, based on the experiences gained from the Families-to-Persons benchmark.},
journal = {Softw. Syst. Model.},
month = may,
pages = {647–691},
numpages = {45},
keywords = {Bidirectional transformation, Benchmark, Model synchronization, Framework}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {AutoML, configurable systems, constraint solving, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Continuous experimentation, A/B testing, Machine learning, Multi-armed bandits, Combinatorial optimization}
}

@article{10.1007/s11219-011-9153-8,
author = {Mussbacher, Gunter and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Amyot, Daniel},
title = {AoURN-based modeling and analysis of software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9153-8},
doi = {10.1007/s11219-011-9153-8},
abstract = {Software Product Line Engineering concerns itself with domain engineering and application engineering. During domain engineering, the whole product family is modeled with a preferred flavor of feature models and additional models as required (e.g., domain models or scenario-based models). During application engineering, the focus shifts toward a single family member and the configuration of the member's features. Recently, aspectual concepts have been employed to better encapsulate individual features of a Software Product Line (SPL), but the existing body of SPL work does not include a unified reasoning framework that integrates aspect-oriented feature description artifacts with the capability to reason about stakeholders' goals while taking feature interactions into consideration. Goal-oriented SPL approaches have been proposed, but do not provide analysis capabilities that help modelers meet the needs of the numerous stakeholders involved in an SPL while at the same time considering feature interactions. We present an aspect-oriented SPL approach for the requirements phase that allows modelers (a) to capture features, goals, and scenarios in a unified framework and (b) to reason about stakeholders' needs and perform trade-off analyses while considering undesirable interactions that are not obvious from the feature model. The approach is based on the Aspect-oriented User Requirements Notation (AoURN) and helps identify, prioritize, and choose products based on analysis results provided by AoURN editor and analysis tools. We apply the AoURN-based SPL framework to the Via Verde SPL to demonstrate the feasibility of this approach through the selection of a Via Verde product configuration that satisfies stakeholders' needs and results in a high-level, scenario-based specification that is free from undesirable feature interactions.},
journal = {Software Quality Journal},
month = sep,
pages = {645–687},
numpages = {43},
keywords = {Aspect-oriented modeling, Feature interactions, Goal-based requirements engineering, Scenario-based requirements engineering, Software product lines, User Requirements Notation}
}

@inproceedings{10.1007/978-3-030-33246-4_45,
author = {Gonz\'{a}lez-Rojas, Oscar and Tafurth, Juan},
title = {Multi-cloud Services Configuration Based on Risk Optimization},
year = {2019},
isbn = {978-3-030-33245-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33246-4_45},
doi = {10.1007/978-3-030-33246-4_45},
abstract = {Nowadays risk analysis becomes critical in the Cloud Computing domain due to the increasing number of threats affecting applications running on cloud infrastructures. Multi-cloud environments allow connecting and migrating services from multiple cloud providers to manage risks. This paper addresses the question of how to model and configure multi-cloud services that can adapt to changes in user preferences and threats on individual and composite services. We propose an approach that combines Product Line (PL) and Machine Learning (ML) techniques to model and timely find optimal configurations of large adaptive systems such as multi-cloud services. A three-layer variability modeling on domain, user preferences, and adaptation constraints is proposed to configure multi-cloud solutions. ML regression algorithms are used to quantify the risk of resulting configurations by analyzing how a service was affected by incremental threats over time. An experimental evaluation on a real life electronic identification and trust multi-cloud service shows the applicability of the proposed approach to predict the risk for alternative re-configurations on autonomous and decentralized services that continuously change their availability and provision attributes.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&amp;TC 2019, Rhodes, Greece, October 21–25, 2019, Proceedings},
pages = {733–749},
numpages = {17},
keywords = {Multi-cloud services, Variability modeling, Product line configuration, Risk optimization, Machine learning},
location = {Rhodes, Greece}
}

@article{10.1016/j.jss.2014.10.037,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Galindo, Jos\'{e} A. and Parejo, Jos\'{e} A. and Benavides, David and Segura, Sergio and Egyed, Alexander},
title = {An assessment of search-based techniques for reverse engineering feature models},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.10.037},
doi = {10.1016/j.jss.2014.10.037},
abstract = {HighlightsSearch based techniques perform well for reverse engineering feature models.Different algorithms and objectives favour precision and recall differently.The F1 objective function provides a trade-off between precision and recall. Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on Fβ, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.},
journal = {J. Syst. Softw.},
month = may,
pages = {353–369},
numpages = {17},
keywords = {Feature model, Reverse engineering, Search Based Software Engineering}
}

@article{10.1145/3176644,
author = {Xiang, Yi and Zhou, Yuren and Zheng, Zibin and Li, Miqing},
title = {Configuring Software Product Lines by Combining Many-Objective Optimization and SAT Solvers},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3176644},
doi = {10.1145/3176644},
abstract = {A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search--based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {14},
numpages = {46},
keywords = {Optimal feature selection, many-objective optimization, satisfiability (SAT) solvers, vector angle--based evolutionary algorithm (VaEA)}
}

@article{10.1016/j.csi.2019.04.011,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Matalonga, Santiago},
title = {Trends in software reuse research: A tertiary study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.011},
doi = {10.1016/j.csi.2019.04.011},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {18},
keywords = {Software reuse, Trends in software reuse, Systematic literature review, Tertiary study}
}

@inproceedings{10.1145/2695664.2695909,
author = {Burity, Tha\'{\i}s and Elias, Gledson},
title = {A quantitative, evidence-based approach for recommending software modules},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695909},
doi = {10.1145/2695664.2695909},
abstract = {In distributed software product line projects, dependencies between components influence on communication and coordination needs among their respective development teams. As an alternative to reduce such needs, it seems interesting to cluster tightly-coupled components into loosely-coupled modules as long as each module is developed by a single team. In such a context, considering that numerous clustering possibilities exist, this paper presents a quantitative, evidence-based approach for recommending software modules by clustering software components. The proposed approach has a threefold foundation: quantitative measures that characterize dependencies between components; a search-based clustering algorithm to recommend modules; and a quantitative measure that characterize dependencies between modules, which can guide the allocation of development teams to modules.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1449–1456},
numpages = {8},
keywords = {architectural metrics, design structure matrix, global software development, search based software engineering, simulated annealing, software product line},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1016/j.infsof.2014.04.002,
author = {Machado, Ivan Do Carmo and Mcgregor, John D. and Cavalcanti, Yguarat\~{a} Cerqueira and De Almeida, Eduardo Santana},
title = {On strategies for testing software product lines: A systematic literature review},
year = {2014},
issue_date = {October, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2014.04.002},
doi = {10.1016/j.infsof.2014.04.002},
abstract = {Context: Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective: The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method: We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results: The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion: This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1183–1199},
numpages = {17},
keywords = {Software product lines, Software quality, Software testing, Systematic literature review}
}

@inproceedings{10.1145/3194133.3194143,
author = {Olaechea, Rafael and Atlee, Joanne and Legay, Axel and Fahrenberg, Uli},
title = {Trace checking for dynamic software product lines},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194143},
doi = {10.1145/3194133.3194143},
abstract = {A key objective of self-adaptive systems is to continue to provide optimal quality of service when the environment changes. A dynamic software product line (DSPL) can benefit from knowing how its various product variants would have performed (in terms of quality of service) with respect to the recent history of inputs. We propose a family-based analysis that simulates all the product variants of a DSPL simultaneously, at runtime, on recent environmental inputs to obtain an estimate of the quality of service that each one of the product variants would have had, provided it had been executing. We assessed the efficiency of our DSPL analysis compared to the efficiency of analyzing each product individually on three case studies. We obtained mixed results due to the explosion of quality-of-service values for the product variants of a DSPL. After introducing a simple data abstraction on the values of quality-of- service variables, our DSPL analysis is between 1.4 and 7.7 times faster than analyzing the products one at a time.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {69–75},
numpages = {7},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@article{10.1007/s10515-010-0076-6,
author = {Dhungana, Deepak and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {The DOPLER meta-tool for decision-oriented variability modeling: a multiple case study},
year = {2011},
issue_date = {March     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0076-6},
doi = {10.1007/s10515-010-0076-6},
abstract = {The variability of a product line is typically defined in models. However, many existing variability modeling approaches are rigid and don't allow sufficient domain-specific adaptations. We have thus been developing a flexible and extensible approach for defining product line variability models. Its main purposes are to guide stakeholders through product derivation and to automatically generate product configurations. Our approach is supported by the DOPLER (  D ecision-  O riented  P roduct  L ine  E ngineering for effective  R euse) meta-tool that allows modelers to specify the types of reusable assets, their attributes, and dependencies for their specific system and context. The aim of this paper is to investigate the suitability of our approach for different domains. More specifically, we explored two research questions regarding the implementation of variability and the utility of DOPLER for variability modeling in different domains. We conducted a multiple case study consisting of four cases in the domains of industrial automation systems and business software. In each of these case studies we analyzed variability implementation techniques. Experts from our industry partners then developed domain-specific meta-models, tool extensions, and variability models for their product lines using DOPLER. The four cases demonstrate the flexibility of the DOPLER approach and the extensibility and adaptability of the supporting meta tool.},
journal = {Automated Software Engg.},
month = mar,
pages = {77–114},
numpages = {38},
keywords = {Decision models, Meta-tools, Product line engineering}
}

@article{10.1145/2581376,
author = {Behjati, Razieh and Nejati, Shiva and Briand, Lionel C.},
title = {Architecture-Level Configuration of Large-Scale Embedded Software Systems},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2581376},
doi = {10.1145/2581376},
abstract = {Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {25},
numpages = {43},
keywords = {Model-based product-line engineering, UML/OCL, consistent configuration, constraint satisfaction techniques, formal specification, product configuration}
}

@inproceedings{10.1109/ASE.2013.6693103,
author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
title = {Measuring the structural complexity of feature models},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693103},
doi = {10.1109/ASE.2013.6693103},
abstract = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {automated analysis, feature model, performance measurement, software product line},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.4018/IJISMD.2016070103,
author = {Geist, Verena and Illibauer, Christa and Natschl\"{a}ger, Christine and Hutter, Robert},
title = {Supporting Customizable Business Process Models Using Graph Transformation Rules},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {3},
issn = {1947-8186},
url = {https://doi.org/10.4018/IJISMD.2016070103},
doi = {10.4018/IJISMD.2016070103},
abstract = {Business Process customization is an active research area in the process management field, dealing with variations/commonalities among processes of a given process family and runtime adaptations of single process instances. Many theoretical approaches have been suggested in the last years; however, practical implementations are rare and limited in their functionality. In this article, a new approach is proposed for capturing customizable process models based on well-known graph transformation techniques and with focus on practical aspects like definition of variation points, linking and propagation of changes, visual highlighting of differences in process variants, and dynamically selecting a specific variant at runtime. The suggested concepts are discussed within case studies, comprising different graph transformation systems for generating process variants supporting a variability by restriction, b variability by restriction and by extension, and c runtime adaptations due to the executing actor. The overall approach is being implemented in the FireStart BPM suite.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = jul,
pages = {51–71},
numpages = {21},
keywords = {Actor-Based Adaptations, Business Process Management, Flexibility by Underspecification, Graph Transformation, Process Customization, Runtime Adaptations, Variability Modelling, Variant Management}
}

@article{10.1016/j.infsof.2008.04.002,
author = {Deelstra, Sybren and Sinnema, Marco and Bosch, Jan},
title = {Variability assessment in software product families},
year = {2009},
issue_date = {January, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.04.002},
doi = {10.1016/j.infsof.2008.04.002},
abstract = {Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {195–218},
numpages = {24},
keywords = {Assessment, Evolution, Software product families, Variability}
}

@inproceedings{10.1145/2737182.2737190,
author = {Feitosa, Daniel and Ampatzoglou, Apostolos and Avgeriou, Paris and Nakagawa, Elisa Yumi},
title = {Investigating Quality Trade-offs in Open Source Critical Embedded Systems},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737190},
doi = {10.1145/2737182.2737190},
abstract = {During the development of Critical Embedded Systems (CES), quality attributes that are critical for them (e.g., correctness, security, etc.) must be guaranteed. However, this often leads to complex quality trade-offs, since non-critical qualities (e.g., reusability, understandability, etc.) may be compromised. In this study, we aim at empirically investigating the existence of quality trade-offs, on the implemented architecture, among versions of open source CESs, and compare them with those of systems from other application domains. The results of the study suggest that in CES, non-critical quality attributes are usually compromised in favor of critical quality attributes. On the contrary, we have not observed compromises of critical qualities in favor of non-critical ones in either CES or other application domains. Furthermore, quality trade-offs are more frequent among critical quality attributes, compared to trade-offs among non-critical quality attributes. Our study has implications for both practitioners when making trade-offs in practice, as well as researchers that investigate quality trade-offs.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {113–122},
numpages = {10},
keywords = {embedded systems, quality trade-offs, software metrics},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@article{10.1007/s10270-015-0459-z,
author = {S\'{a}nchez, Ana B. and Segura, Sergio and Parejo, Jos\'{e} A. and Ruiz-Cort\'{e}s, Antonio},
title = {Variability testing in the wild: the Drupal case study},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0459-z},
doi = {10.1007/s10270-015-0459-z},
abstract = {Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in single features and 160 faults triggered by the interaction of up to four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {173–194},
numpages = {22},
keywords = {Automated testing, Non-functional properties, Test case prioritization, Test case selection, Variability testing, Variability-intensive systems}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Performance prediction, Configurable systems, Regression, Model selection, Parameter tuning}
}

@article{10.1016/j.jss.2016.06.102,
author = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
title = {Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.102},
doi = {10.1016/j.jss.2016.06.102},
abstract = {We proposed a novel software architecture-level adaptation approach.We adopted known architectural patterns in distributed and concurrent systems.We developed a framework to support the self-adaptive mechanism.We developed and evaluated five adaptive policies.Our approach improved performance and increased reliability in our experiments. More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.},
journal = {J. Syst. Softw.},
month = nov,
pages = {311–328},
numpages = {18},
keywords = {Autonomic computing, Distributed and concurrent architecture, Elastic computing, Patterns, Performance, Reliability, Software architecture}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {feature selection, performance analysis, software architectures, uncertainty},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1016/j.cose.2021.102381,
author = {ter Beek, Maurice H. and Legay, Axel and Lluch Lafuente, Alberto and Vandin, Andrea},
title = {Quantitative Security Risk Modeling and Analysis with RisQFLan},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {109},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102381},
doi = {10.1016/j.cose.2021.102381},
journal = {Comput. Secur.},
month = oct,
numpages = {23},
keywords = {Graph-based security risk models, Attack-defense trees, Probabilistic model checking, Statistical model checking, Formal analysis tools}
}

@article{10.1016/j.jss.2017.05.052,
author = {Bastos, Jonatas Ferreira and da Mota Silveira Neto, Paulo Anselmo and OLeary, Pdraig and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {Software product lines adoption in small organizations},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.05.052},
doi = {10.1016/j.jss.2017.05.052},
abstract = {Provides a better understanding of SPL adoption in the context of SMEs.A set of empirical studies performed in academic and industry environments.Inputs to establish guidelines for SPL adoption.A discussion of the evidences, with insights to guide future investigations. ContextAn increasing number of studies has demonstrated improvements in product quality, and time-to-market reductions when Software Product Line (SPL) engineering is introduced. However, despite the amount of successful stories about the use of SPL engineering, there is a lack of guidelines to support its adoption, especially to small-sized software organizations. ObjectiveThe aim of this study is to investigate SPL adoption in small organizations and to improve the generalization of evidence through the use of a multi-method approach. MethodThis paper reports on a multi-method study, where results from a mapping study, industrial case study and also expert opinion survey were considered to identify a set of findings. ResultsThe study provides a better understanding of SPL adoption in the context of small to medium-sized organizations, by documenting evidence observed during the transition from single-system development to an SPL approach. This evidence is strengthened by the use of different research methods, which results in 22 findings regarding to the SPL adoption. ConclusionThis research has synthesized the available evidence in SPL adoption and identifies gaps between required strategies, organizational structures, maturity level and existing adoption barriers. These findings are an important step to establish guidelines for SPL adoption.},
journal = {J. Syst. Softw.},
month = sep,
pages = {112–128},
numpages = {17},
keywords = {Adoption barriers, Case study, Mapping study, Multi-method approach, SPL adoption, Software product lines, Survey}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {Quality evaluation, evolutionary algorithms, exact method, heuristic, indicator, measure, metaheuristic, metric, multi-criteria optimisation, multobjective optimisation, performance assessment}
}

@article{10.1145/1363102.1363104,
author = {Mohagheghi, Parastoo and Conradi, Reidar},
title = {An empirical investigation of software reuse benefits in a large telecom product},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/1363102.1363104},
doi = {10.1145/1363102.1363104},
abstract = {Background. This article describes a case study on the benefits of software reuse in a large telecom product. The reused components were developed in-house and shared in a product-family approach. Methods. Quantitative data mined from company repositories are combined with other quantitative data and qualitative observations. Results. We observed significantly lower fault density and less modified code between successive releases of the reused components. Reuse and standardization of software architecture and processes allowed easier transfer of development when organizational changes happened. Conclusions. The study adds to the evidence of quality benefits of large-scale reuse programs and explores organizational motivations and outcomes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {13},
numpages = {31},
keywords = {Software reuse, fault density, product family, risks, standardization}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@article{10.1016/j.rcim.2015.02.009,
author = {Mourtzis, D. and Doukas, M.},
title = {On the configuration of supply chains for assemble-to-order products},
year = {2015},
issue_date = {December 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2015.02.009},
doi = {10.1016/j.rcim.2015.02.009},
abstract = {The product customisation trend has an unprecedented impact on manufacturing companies, as the ever-increasing number of product variants and the enlarged pool of cooperating partners vastly increase the feasible alternative supply chain configurations. In terms of decision theory, this is translated to enormous search spaces. For tackling these NP-hard problems, metaheuristic optimisation methods are utilised, which provide a trade-off between the quality of solutions and the computation time. This research work describes the modelling and solving of two supply chain configuration problems using the Simulated Annealing and Tabu Search methods. The performance of the identified solutions in terms of optimisation of multiple conflicting criteria, is compared against the results obtained from a custom Intelligent Search Algorithm and an Exhaustive enumerative method. The algorithms are developed into a web-based software platform. The approach is validated through real life applications to case studies from the automotive and CNC laser welding machine building industries.},
journal = {Robot. Comput.-Integr. Manuf.},
month = dec,
pages = {13–24},
numpages = {12},
keywords = {Decision-making, Manufacturing network design, Mass customisation, Metaheuristics, Simulated annealing, Tabu search}
}

@article{10.1016/j.jss.2012.11.006,
author = {Teixeira, Leopoldo and Borba, Paulo and Gheyi, Rohit},
title = {Safe composition of configuration knowledge-based software product lines},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.11.006},
doi = {10.1016/j.jss.2012.11.006},
abstract = {Mistakes made when implementing or specifying the models of a Software Product Line (SPL) can result in ill-formed products - the safe composition problem. Such problem can hinder productivity and it might be hard to detect, since SPLs can have thousands of products. In this article, we propose a language independent approach for verifying safe composition of SPLs with dedicated Configuration Knowledge models. We translate feature model and Configuration Knowledge into propositional logic and use the Alloy Analyzer to perform the verification. To provide evidence for the generality of our approach, we instantiate this approach in different compositional settings. We deal with different kinds of assets such as use case scenarios and Eclipse RCP components. We analyze both the code and the requirements for a larger scale SPL, finding problems that affect thousands of products in minutes. Moreover, our evaluation suggests that the analysis time grows linearly with respect to the number of products in the analyzed SPLs.},
journal = {J. Syst. Softw.},
month = apr,
pages = {1038–1053},
numpages = {16},
keywords = {Configuration Knowledge, Safe composition, Software Product Lines}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Search-based software engineering, Systematic review, Research trends, Spanish community}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Product lines, Feature model, Product-line configuration, Recommender systems, Personalized recommendations}
}

@article{10.1007/s10270-020-00782-w,
author = {Munk, Peter and Nordmann, Arne},
title = {Model-based safety assessment with SysML and component fault trees: application and lessons learned},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00782-w},
doi = {10.1007/s10270-020-00782-w},
abstract = {Mastering the complexity of safety assurance for modern, software-intensive systems is challenging in several domains, such as automotive, robotics, and avionics. Model-based safety analysis techniques show promising results to handle this challenge by automating the generation of required artifacts for an assurance case. In this work, we adapt prominent approaches and propose to augment of SysML models with component fault trees (CFTs) to support the fault tree analysis and the failure mode and effects analysis. While most existing approaches based on CFTs are only targeting the system topology, e.&nbsp;g., UML class diagrams, we propose an integration of CFTs with SysML internal block diagrams as well as SysML activity diagrams. We realized our approach in a prototypical tool. We conclude with best practices and lessons learned that emerged from our case studies with an electronic power steering system and a boost recuperation system.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {889–910},
numpages = {22},
keywords = {Model-based systems engineering, MBSE, Model-based safety analysis, MBSA, Fault trees, Fault tree analysis, FTA, Component fault tree, CFT, Failure mode and effects analysis, FMEA, Safety, Reliability, Dependability}
}

@article{10.1007/s10009-017-0466-1,
author = {Chadli, Mounir and Kim, Jin H. and Larsen, Kim G. and Legay, Axel and Naujokat, Stefan and Steffen, Bernhard and Traonouez, Louis-Marie},
title = {High-level frameworks for the specification and verification of scheduling problems},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-017-0466-1},
doi = {10.1007/s10009-017-0466-1},
abstract = {Over the years, schedulability of Cyber-Physical Systems (CPS) has mainly been performed by analytical methods. These techniques are known to be effective but limited to a few classes of scheduling policies. In a series of recent work, we have shown that schedulability analysis of CPS could be performed with a model-based approach and extensions of verification tools such as UPPAAL. One of our main contributions has been to show that such models are flexible enough to embed various types of scheduling policies, which goes beyond those in the scope of analytical tools. However, the specification of scheduling problems with model-based approaches requires a substantial modeling effort, and a deep understanding of the techniques employed in order to understand their results. In this paper we propose simplicity-driven high-level specification and verification frameworks for various scheduling problems. These frameworks consist of graphical and user-friendly languages for describing scheduling problems. The high-level specifications are then automatically translated to formal models, and results are transformed back into the comprehensible model view. To construct these frameworks we exploit a meta-modeling approach based on the tool generator Cinco . Additionally we propose in this paper two new techniques for scheduling analysis. The first performs runtime monitoring using the CUSUM algorithm to detect alarming change in the system. The second performs optimization using efficient statistical techniques. We illustrate our frameworks and techniques on two case studies.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = aug,
pages = {397–422},
numpages = {26},
keywords = {Energy, Formal methods, Hierarchical scheduling, High-level language, Meta-modeling, Scheduling, Statistical model-checking}
}

@article{10.5555/2594638.2594643,
author = {Hotz, Lothar and Wolter, Katharina},
title = {Beyond physical product configuration --Configuration in unusual domains},
year = {2013},
issue_date = {January 2013},
publisher = {IOS Press},
address = {NLD},
volume = {26},
number = {1},
issn = {0921-7126},
abstract = {Configuration technologies are typically applied in domains with physical products. In this article, we determine characteristics of configuration technologies that are used to compose non-pure physical products. Starting from two case studies software-intensive systems and scene interpretation where we successfully applied configuration, we determine some characteristics of knowledge representation languages and configuration systems that enable to solve configuration tasks in domains beyond pure physical products. As such, the article provides thinking outside the box of physical product configuration.},
journal = {AI Commun.},
month = jan,
pages = {39–66},
numpages = {28},
keywords = {Knowledge Representation, Knowledge-Based Configuration, Scene Interpretation, Software-Intensive Systems}
}

@article{10.1007/s10845-021-01813-z,
author = {Guo, Yuming},
title = {Towards the efficient generation of variant design in product development networks: network nodes importance based product configuration evaluation approach},
year = {2021},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {2},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01813-z},
doi = {10.1007/s10845-021-01813-z},
abstract = {The variant design node configuration is an effective method to solve a trade-off that exist in mass customization production between the requirements of product diversification and the product’s cost and delivery time. However, configuration changes in a product development network may generate an avalanche effect of design change propagation, which can consume substantial design resources. In order to control design change propagations, an evaluation method based on network nodes importance for variant design node schemes is proposed for a product development network. Firstly, the evaluation indexes, including the betweenness, variant deign node set importance, and network clustering coefficient, are integrated to describe the network characteristics of the set of variant design nodes. Then, combining the time and resource constraints for the variant design, a discrete particle swarm algorithm is employed to optimize the configuration of the nodes. The configuration solution for the variant design nodes satisfies the need to control the variant design propagation. Compared with the established greedy algorithm, the discrete particle swarm optimization can achieve better optimization performance in terms of the algorithm’s convergence and computation time. It is meaningful to understand the mechanism of product configuration change propagation in depth in order to choose the variant design nodes rationally and efficiently in complex product development networks. Lastly, an example of variant design for a type of cleaning robot product verifies the effectiveness of the proposed method.},
journal = {J. Intell. Manuf.},
month = jul,
pages = {615–631},
numpages = {17},
keywords = {Product development network, Variant design, Nodes configuration, Nodes importance, Network clustering coefficient, Discrete particle swarm algorithm}
}

@inproceedings{10.1145/1370062.1370078,
author = {Espinoza, Huascar and Servat, David and G\'{e}rard, S\'{e}bastien},
title = {Leveraging analysis-aided design decision knowledge in UML-based development of embedded systems},
year = {2008},
isbn = {9781605580388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370062.1370078},
doi = {10.1145/1370062.1370078},
abstract = {Many important works have been carried out to provide modeling languages (e.g., UML, SDL) with expressiveness to support embedded system design, validation and verification. A fundamental shortcoming in current model-driven approaches is the inability to explicitly capture design decisions and trade-offs between different non-functional parameters, among which timeliness, memory usage, and power consumption are of primary interest. This paper highlights technical limitations in UML to specify complex non-functional evaluation scenarios of candidate architectures, and outlines our current work to provide straightforward solutions.},
booktitle = {Proceedings of the 3rd International Workshop on Sharing and Reusing Architectural Knowledge},
pages = {55–62},
numpages = {8},
keywords = {UML, design space exploration, embedded systems, model-driven engineering, trade-off analysis},
location = {Leipzig, Germany},
series = {SHARK '08}
}

@article{10.1007/s10270-016-0569-2,
author = {Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Lochau, Malte and Meinicke, Jens and Saake, Gunter},
title = {Effective product-line testing using similarity-based product prioritization},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0569-2},
doi = {10.1007/s10270-016-0569-2},
abstract = {A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufficient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efficiently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-based prioritization on three product lines with real faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {499–521},
numpages = {23},
keywords = {Combinatorial interaction testing, Model-based testing, Product-line testing, Software product lines, Test-case prioritization}
}

@article{10.5555/3168027.3168284,
author = {Xu, Yang and Landon, Yann and Segonds, Stphane and Zhang, Yicha},
title = {A decision support model in mass customization},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {114},
number = {C},
issn = {0360-8352},
abstract = {Decision support model in mass customization.Analysis of customer preference, product features and cost.Optimization using genetic algorithm. Mass customization (MC) is one of the leading strategies used in production industries in todays market filled with competition. MC is an oxymoron of controlling production costs and satisfying customers individual requirements. It is well known that economy of scale and economy of scope is a pair of conflicts, and how to get the balance between them is the key issue to promote enterprises competition. By analyzing and processing information of customer preference, product features and cost, this paper proposes a decision support model in mass customization to obtain the optimized production solution. Genetic algorithm is used for optimization, and the results of an illustrative example show that the model is efficient in production industries.},
journal = {Comput. Ind. Eng.},
month = dec,
pages = {11–21},
numpages = {11},
keywords = {Decision support systems, Genetic algorithms, Mass customization, Optimization}
}

@article{10.1016/j.infsof.2021.106620,
author = {Tran, Huynh Khanh Vi and Unterkalmsteiner, Michael and B\"{o}rstler, J\"{u}rgen and Ali, Nauman bin},
title = {Assessing test artifact quality—A tertiary study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106620},
doi = {10.1016/j.infsof.2021.106620},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {22},
keywords = {Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance}
}

@inproceedings{10.1145/3460319.3464823,
author = {Mordahl, Austin and Wei, Shiyi},
title = {The impact of tool configuration spaces on the evaluation of configurable taint analysis for Android},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464823},
doi = {10.1145/3460319.3464823},
abstract = {The most popular static taint analysis tools for Android allow users to change the underlying analysis algorithms through configuration options. However, the large configuration spaces make it difficult for developers and users alike to understand the full capabilities of these tools, and studies to-date have only focused on individual configurations. In this work, we present the first study that evaluates the configurations in Android taint analysis tools, focusing on the two most popular tools, FlowDroid and DroidSafe. First, we perform a manual code investigation to better understand how configurations are implemented in both tools. We formalize the expected effects of configuration option settings in terms of precision and soundness partial orders which we use to systematically test the configuration space. Second, we create a new dataset of 756 manually classified flows across 18 open-source real-world apps and conduct large-scale experiments on this dataset and micro-benchmarks. We observe that configurations make significant tradeoffs on the performance, precision, and soundness of both tools. The studies to-date would reach different conclusions on the tools' capabilities were they to consider configurations or use real-world datasets. In addition, we study the individual options through a statistical analysis and make actionable recommendations for users to tune the tools to their own ends. Finally, we use the partial orders to test the tool configuration spaces and detect 21 instances where options behaved in unexpected and incorrect ways, demonstrating the need for rigorous testing of configuration spaces.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {466–477},
numpages = {12},
keywords = {Android taint analysis, configurable static analysis, empirical study},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Run-time architecture evaluation, Runtime architecture evaluation, Software architectures for dynamic environments, Internet of things, IoT, Design diversity}
}

@article{10.1016/j.jss.2018.06.027,
author = {Merino, L. and Ghafari, M. and Anslow, C. and Nierstrasz, O.},
title = {A systematic literature review of software visualization evaluation},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.06.027},
doi = {10.1016/j.jss.2018.06.027},
journal = {J. Syst. Softw.},
month = oct,
pages = {165–180},
numpages = {16},
keywords = {Software visualisation, Evaluation, Literature review}
}

@article{10.1007/s10664-016-9462-4,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9462-4},
doi = {10.1007/s10664-016-9462-4},
abstract = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1763–1794},
numpages = {32},
keywords = {Empirical evaluation, Feature models, Multi-objective evolutionary algorithms, Reverse engineering}
}

@book{10.5555/2669162,
author = {Felfernig, Alexander and Hotz, Lothar and Bagley, Claire and Tiihonen, Juha},
title = {Knowledge-based Configuration: From Research to Business Cases},
year = {2014},
isbn = {012415817X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1},
abstract = {Knowledge-based Configuration incorporates knowledge representation formalisms to capture complex product models and reasoning methods to provide intelligent interactive behavior with the user. This book represents the first time that corporate and academic worlds collaborate integrating research and commercial benefits of knowledge-based configuration. Foundational interdisciplinary material is provided for composing models from increasingly complex products and services. Case studies, the latest research, and graphical knowledge representations that increase understanding of knowledge-based configuration provide a toolkit to continue to push the boundaries of what configurators can do and how they enable companies and customers to thrive.Includes detailed discussion of state-of-the art configuration knowledge engineering approaches such as automated testing and debugging, redundancy detection, and conflict management Provides an overview of the application of knowledge-based configuration technologies in the form of real-world case studies from SAP, Siemens, Kapsch, and more Explores the commercial benefits of knowledge-based configuration technologies to business sectors from services to industrial equipment Uses concepts that are based on an example personal computer configuration knowledge base that is represented in an UML-based graphical language}
}

@article{10.5555/3044222.3051232,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {Requirement-driven evolution in software product lines},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
abstract = {We conducted a systematic mapping study on SPL evolution.We identified 107 relevant contributions on the topic up to mid 2015.We elaborated on the traditional change mini-cycle to classify the contributions.We identified well-established topics, trends and open research issues. CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring.OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps.RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products).CONCLUSION. Analyses of the results indicate that "Solution proposals" are the most common type of contribution (31%). Regarding the evolution activity, "Implement change" (43%) and "Analyze and plan change" (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.},
journal = {J. Syst. Softw.},
month = dec,
pages = {110–143},
numpages = {34},
keywords = {Evolution, Software product lines, Systematic mapping study}
}

@article{10.1007/s11219-018-9424-8,
author = {Alkharabsheh, Khalid and Crespo, Yania and Manso, Esperanza and Taboada, Jos\'{e} A.},
title = {Software Design Smell Detection: a systematic mapping study},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9424-8},
doi = {10.1007/s11219-018-9424-8},
abstract = {Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18&nbsp;years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.},
journal = {Software Quality Journal},
month = sep,
pages = {1069–1148},
numpages = {80},
keywords = {DesignSmell, Antipatterns, Detection tools, Quality models, Systematic mapping study}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1016/j.cl.2018.05.004,
author = {Combemale, Benoit and Kienzle, J\"{o}rg and Mussbacher, Gunter and Barais, Olivier and Bousse, Erwan and Cazzola, Walter and Collet, Philippe and Degueule, Thomas and Heinrich, Robert and J\'{e}z\'{e}quel, Jean-Marc and Leduc, Manuel and Mayerhofer, Tanja and Mosser, S\'{e}bastien and Sch\"{o}ttle, Matthias and Strittmatter, Misha and Wortmann, Andreas},
title = {Concern-oriented language development (COLD): Fostering reuse in language engineering},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.05.004},
doi = {10.1016/j.cl.2018.05.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {139–155},
numpages = {17},
keywords = {Domain-specific languages, Language concern, Language reuse}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {Software performance prediction, adversarial learning, configurable systems, regularization},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Search-based Software Testing, Hyper-heuristics, Systematic Mapping, Evolutionary Algorithms, Genetic Algorithms, Meta-heuristics}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Personalized Recommendations, Product-Line Configuration, Recommenders, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th international symposium on the maintenance and evolution of service-oriented and cloud-based systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {34–37},
numpages = {4},
keywords = {SOA, cloud computing, cloudbased systems, service-oriented systems, serviceoriented architecture, services, software evolution, software maintenance}
}

@inproceedings{10.5555/2388996.2389126,
author = {Williams, Samuel and Kalamkar, Dhiraj D. and Singh, Amik and Deshpande, Anand M. and Van Straalen, Brian and Smelyanskiy, Mikhail and Almgren, Ann and Dubey, Pradeep and Shalf, John and Oliker, Leonid},
title = {Optimization of geometric multigrid for emerging multi- and manycore processors},
year = {2012},
isbn = {9781467308045},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {Multigrid methods are widely used to accelerate the convergence of iterative solvers for linear systems used in a number of different application areas. In this paper, we explore optimization techniques for geometric multigrid on existing and emerging multicore systems including the Opteron-based Cray XE6, Intel® Xeon® E5-2670 and X5550 processor-based Infiniband clusters, as well as the new Intel® Xeon Phi™ coprocessor (Knights Corner). Our work examines a variety of novel techniques including communication-aggregation, threaded wavefront-based DRAM communication-avoiding, dynamic threading decisions, SIMDization, and fusion of operators. We quantify performance through each phase of the V-cycle for both single-node and distributed-memory experiments and provide detailed analysis for each class of optimization. Results show our optimizations yield significant speedups across a variety of subdomain sizes while simultaneously demonstrating the potential of multi- and manycore processors to dramatically accelerate single-node performance. However, our analysis also indicates that improvements in networks and communication will be essential to reap the potential of manycore processors in large-scale multigrid calculations.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {96},
numpages = {11},
keywords = {Knights corner, OpenMP, Xeon Phi, auto-tuning, communication-avoiding, geometric multigrid, multicore},
location = {Salt Lake City, Utah},
series = {SC '12}
}

@article{10.1016/j.vlsi.2018.02.013,
author = {Stamelakos, Ioannis and Xydis, Sotirios and Palermo, Gianluca and Silvano, Cristina},
title = {Workload- and process-variation aware voltage/frequency tuning for energy efficient performance sustainability of NTC manycores},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {C},
issn = {0167-9260},
url = {https://doi.org/10.1016/j.vlsi.2018.02.013},
doi = {10.1016/j.vlsi.2018.02.013},
journal = {Integr. VLSI J.},
month = mar,
pages = {252–262},
numpages = {11},
keywords = {Near-threshold computing, Manycore architectures, Low power, Energy efficiency, Variability}
}

@article{10.1016/j.jss.2017.09.022,
author = {Cetina, Carlos and Font, Jaime and Arcega, Lorena and Prez, Francisca},
title = {Improving feature location in long-living model-based product families designed with sustainability goals},
year = {2017},
issue_date = {December 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.022},
doi = {10.1016/j.jss.2017.09.022},
abstract = {A feature location approach for long-living software systems is proposed.Feature location is guided by feature description, commonality and modifications.Feature commonality and modifications improve precision results. The benefits of Software Product Lines (SPL) are very appealing: software development becomes better, faster, and cheaper. Unfortunately, these benefits come at the expense of a migration from a family of products to a SPL. Feature Location could be useful in achieving the transition to SPLs. This work presents our FeLLaCaM approach for Feature Location. Our approach calculates similarity to a description of the feature to locate, occurrences where the candidate features remain unchanged, and changes performed to the candidate features throughout the retrospective of the product family. We evaluated our approach in two long-living industrial domains: a model-based family of firmwares for induction hobs that was developed over more than 15 years, and a model-based family of PLC software to control trains that was developed over more than 25 years. In our evaluation, we compare our FeLLaCaM approach with two other approaches for Feature Location: (1) FLL (Feature Location through Latent Semantic Analysis) and (2) FLC (Feature Location through Comparisons). We measure the performance of FeLLaCaM, FLL, and FLC in terms of recall, precision, Matthews Correlation Coefficient, and Area Under the Receiver Operating Characteristics curve. The results show that FeLLaCaM outperforms FLL and FLC.},
journal = {J. Syst. Softw.},
month = dec,
pages = {261–278},
numpages = {18},
keywords = {Architecture sustainability, Feature location, Long-Living software systems, ModelDriven engineering, Software product lines}
}

@article{10.1016/j.infsof.2019.106198,
author = {Assun\c{c}\~{a}o, Wesley K.G. and Vergilio, Silvia R. and Lopez-Herrejon, Roberto E.},
title = {Automatic extraction of product line architecture and feature models from UML class diagram variants},
year = {2020},
issue_date = {Jan 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {117},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106198},
doi = {10.1016/j.infsof.2019.106198},
journal = {Inf. Softw. Technol.},
month = jan,
numpages = {19},
keywords = {Model merging, Feature model, SPL architecture, Search-based techniques}
}

@article{10.1007/s00450-014-0273-9,
author = {Goltz, Ursula and Reussner, Ralf H. and Goedicke, Michael and Hasselbring, Wilhelm and M\"{a}rtin, Lukas and Vogel-Heuser, Birgit},
title = {Design for future: managed software evolution},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3–4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-014-0273-9},
doi = {10.1007/s00450-014-0273-9},
abstract = {Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future--Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.},
journal = {Comput. Sci.},
month = aug,
pages = {321–331},
numpages = {11},
keywords = {Co-evolution, Design, Knowledge carrying software, Legacy systems, Software life cycle, maintenance and operation}
}

@inproceedings{10.1145/3377811.3380372,
author = {Lienhardt, Michael and Damiani, Ferruccio and Johnsen, Einar Broch and Mauro, Jacopo},
title = {Lazy product discovery in huge configuration spaces},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380372},
doi = {10.1145/3377811.3380372},
abstract = {Highly-configurable software systems can have thousands of interdependent configuration options across different subsystems. In the resulting configuration space, discovering a valid product configuration for some selected options can be complex and error prone. The configuration space can be organized using a feature model, fragmented into smaller interdependent feature models reflecting the configuration options of each subsystem.We propose a method for lazy product discovery in large fragmented feature models with interdependent features. We formalize the method and prove its soundness and completeness. The evaluation explores an industrial-size configuration space. The results show that lazy product discovery has significant performance benefits compared to standard product discovery, which in contrast to our method requires all fragments to be composed to analyze the feature model. Furthermore, the method succeeds when more efficient, heuristics-based engines fail to find a valid configuration.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1509–1521},
numpages = {13},
keywords = {Linux distribution, composition, configurable software, feature models, software product lines, variability modeling},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.future.2018.12.025,
author = {Cao, Yang and Lung, Chung-Horng and Ajila, Samuel A. and Li, Xiaolin},
title = {Support mechanisms for cloud configuration using XML filtering techniques: A case study in SaaS},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {95},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.12.025},
doi = {10.1016/j.future.2018.12.025},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {52–67},
numpages = {16},
keywords = {Cloud Computing, Software-as-a-Service, Multi-Tenancy, Feature Modeling, XML Filtering, Yfilter}
}

@article{10.1016/j.jss.2014.01.021,
author = {Walraven, Stefan and Van Landuyt, Dimitri and Truyen, Eddy and Handekyn, Koen and Joosen, Wouter},
title = {Efficient customization of multi-tenant Software-as-a-Service applications with service lines},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.01.021},
doi = {10.1016/j.jss.2014.01.021},
abstract = {Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort.},
journal = {J. Syst. Softw.},
month = may,
pages = {48–62},
numpages = {15},
keywords = {Multi-tenancy, SaaS, Variability}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Component-based software development, Component-based software engineering, Software component, Systematic mapping study}
}

@inproceedings{10.1007/978-3-642-34032-1_23,
author = {Marrone, Stefano and Nardone, Roberto and Orazzo, Antonio and Petrone, Ida and Velardi, Luigi},
title = {Improving verification process in driverless metro systems: the MBAT project},
year = {2012},
isbn = {9783642340314},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34032-1_23},
doi = {10.1007/978-3-642-34032-1_23},
abstract = {Complex systems are experiencing increasing needs to obtain a higher level of safety and to reduce time to market. This is in particular true for automotive, aerospace and railway domains, pushing the research community to define novel development and verification methods and techniques. The ARTEMIS EU-project MBAT (Combined Model-Based Analysis and Testing of Embedded Systems) represents one of the most important attempts in this direction since it aims to achieve such improvements in several application domains. Starting from the Ansaldo STS implementation of Communication-Based Train Control system (CBTC), which is the base of automatic driverless metro systems, we describe in this paper the improvement that MBAT would bring to the Verification process. To this aim an accurate description of existing Verification process in automatic metro systems and discussion about critical points are provided. Then we describe the expected results of MBAT project on such kind of processes. The proposed approach and developed tools will be part of a common reference platform and, also if related to the railway domain, for its generality, they can be used for different domains with similar needs. The basic agreement in the project will guarantee the cross use of the platform and will give quantitative measures of the obtained improvement.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Applications and Case Studies - Volume Part II},
pages = {231–245},
numpages = {15},
keywords = {automatic metro systems, model-based analysis, model-based testing, safety critical embedded systems, validation process},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@article{10.1016/j.cie.2021.107741,
author = {Naderi, Bahman and Azab, Ahmed},
title = {Production scheduling for reconfigurable assembly systems: Mathematical modeling and algorithms},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107741},
doi = {10.1016/j.cie.2021.107741},
journal = {Comput. Ind. Eng.},
month = dec,
numpages = {9},
keywords = {Reconfigurable manufacturing systems, Scheduling, Mathematical modeling, Artificial immune algorithms}
}

@article{10.1016/j.infsof.2009.11.008,
author = {Ovaska, Eila and Evesti, Antti and Henttonen, Katja and Palviainen, Marko and Aho, Pekka},
title = {Knowledge based quality-driven architecture design and evaluation},
year = {2010},
issue_date = {June, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.008},
doi = {10.1016/j.infsof.2009.11.008},
abstract = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {577–601},
numpages = {25},
keywords = {Evaluation, Model-driven development, Ontology, Quality attribute, Software architecture, Tool}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@article{10.1145/3464939,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat},
title = {Recommending Faulty Configurations for Interacting Systems Under Test Using Multi-objective Search},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464939},
doi = {10.1145/3464939},
abstract = {Modern systems, such as cyber-physical systems, often consist of multiple products within/across product lines communicating with each other through information networks. Consequently, their runtime behaviors are influenced by product configurations and networks. Such systems play a vital role in our daily life; thus, ensuring their correctness by thorough testing becomes essential. However, testing these systems is particularly challenging due to a large number of possible configurations and limited available resources. Therefore, it is important and practically useful to test these systems with specific configurations under which products will most likely fail to communicate with each other. Motivated by this, we present a search-based configuration recommendation (SBCR) approach to recommend faulty configurations for the system under test (SUT) based on cross-product line (CPL) rules. CPL rules are soft constraints, constraining product configurations while indicating the most probable system states with a certain degree of confidence. In SBCR, we defined four search objectives based on CPL rules and combined them with six commonly applied search algorithms. To evaluate SBCR (i.e., SBCRNSGA-II, SBCRIBEA, SBCRMoCell, SBCRSPEA2, SBCRPAES, and SBCRSMPSO), we performed two case studies (Cisco and Jitsi) and conducted difference analyses. Results show that for both of the case studies, SBCR significantly outperformed random search-based configuration recommendation (RBCR) for 86% of the total comparisons based on six quality indicators, and 100% of the total comparisons based on the percentage of faulty configurations (PFC). Among the six variants of SBCR, SBCRSPEA2 outperformed the others in 85% of the total comparisons based on six quality indicators and 100% of the total comparisons based on PFC.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {53},
numpages = {36},
keywords = {Product line, configuration recommendation, interacting products, mined rules, multi-objective search, testing}
}

@inproceedings{10.1145/2463372.2463545,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Minimizing test suites in software product lines using weight-based genetic algorithms},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463545},
doi = {10.1145/2463372.2463545},
abstract = {Test minimization techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. In the context of software product line, we can save effort and cost in the selection and minimization of test cases for testing a specific product by modeling the product line. However, minimizing the test suite for a product requires addressing two potential issues: 1) the minimized test suite may not cover all test requirements compared with the original suite; 2) the minimized test suite may have less fault revealing capability than the original suite. In this paper, we apply weight-based Genetic Algorithms (GAs) to minimize the test suite for testing a product, while preserving fault detection capability and testing coverage of the original test suite. The challenge behind is to define an appropriate fitness function, which is able to preserve the coverage of complex testing criteria (e.g., Combinatorial Interaction Testing criterion). Based on the defined fitness function, we have empirically evaluated three different weight-based GAs on an industrial case study provided by Cisco Systems, Inc. Norway. We also presented our results of applying the three weight-based GAs on five existing case studies from the literature. Based on these case studies, we conclude that among the three weight-based GAs, Random-Weighted GA (RWGA) achieved significantly better performance than the other ones.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1493–1500},
numpages = {8},
keywords = {fault detection capability, feature pairwise coverage, test minimization, weight-based gas},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@article{10.1016/j.jpdc.2019.10.012,
author = {Kahveci, Basri and Gedik, Bu\u{g}ra},
title = {Joker: Elastic stream processing with organic adaptation},
year = {2020},
issue_date = {Mar 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.10.012},
doi = {10.1016/j.jpdc.2019.10.012},
journal = {J. Parallel Distrib. Comput.},
month = mar,
pages = {205–223},
numpages = {19},
keywords = {Stream processing, Elasticity, Parallelization}
}

@article{10.1145/2897760,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
title = {SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2897760},
doi = {10.1145/2897760},
abstract = {A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classical many-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {17},
numpages = {39},
keywords = {Product selection}
}

@article{10.1016/j.cie.2021.107782,
author = {Chien, Chen-Fu and Lan, Yu-Bin},
title = {Agent-based approach integrating deep reinforcement learning and hybrid genetic algorithm for dynamic scheduling for Industry 3.5 smart production},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107782},
doi = {10.1016/j.cie.2021.107782},
journal = {Comput. Ind. Eng.},
month = dec,
numpages = {11},
keywords = {Deep reinforcement learning, Dynamic scheduling, Hybrid genetic algorithm, Semiconductor manufacturing, Industry 3.5}
}

@article{10.1007/s11219-011-9170-7,
author = {Acher, Mathieu and Collet, Philippe and Gaignard, Alban and Lahire, Philippe and Montagnat, Johan and France, Robert B.},
title = {Composing multiple variability artifacts to assemble coherent workflows},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9170-7},
doi = {10.1007/s11219-011-9170-7},
abstract = {The development of scientific workflows is evolving toward the systematic use of service-oriented architectures, enabling the composition of dedicated and highly parameterized software services into processing pipelines. Building consistent workflows then becomes a cumbersome and error-prone activity as users cannot manage such large-scale variability. This paper presents a rigorous and tooled approach in which techniques from Software Product Line (SPL) engineering are reused and extended to manage variability in service and workflow descriptions. Composition can be facilitated while ensuring consistency. Services are organized in a rich catalog which is organized as a SPL and structured according to the common and variable concerns captured for all services. By relying on sound merging techniques on the feature models that make up the catalog, reasoning about the compatibility between connected services is made possible. Moreover, an entire workflow is then seen as a multiple SPL (i.e., a composition of several SPLs). When services are configured within, the propagation of variability choices is then automated with appropriate techniques and the user is assisted in obtaining a consistent workflow. The approach proposed is completely supported by a combination of dedicated tools and languages. Illustrations and experimental validations are provided using medical imaging pipelines, which are representative of current scientific workflows in many domains.},
journal = {Software Quality Journal},
month = sep,
pages = {689–734},
numpages = {46},
keywords = {Composition, Feature models, Scientific workflows, Software product lines}
}

@article{10.1007/s00521-018-3560-8,
author = {Anwar, Zeeshan and Afzal, Hammad and Bibi, Nazia and Abbas, Haider and Mohsin, Athar and Arif, Omar},
title = {A hybrid-adaptive neuro-fuzzy inference system for multi-objective regression test suites optimization},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3560-8},
doi = {10.1007/s00521-018-3560-8},
abstract = {Regression testing is a mandatory activity of software development life cycle, which is performed to ensure that modifications have not caused any adverse effects on the system’s functionality. With every change in software in the maintenance phase, the size of regression test suite grows as new test cases are written to validate changes. The bigger size of regression test suite makes the testing expensive and time-consuming. Optimization of regression test suite is a possible solution to cope with this problem. Various techniques of optimization have been proposed; however, there is no perfect solution for the problem and therefore, requires better solutions to improve the optimization process. This paper presents a novel technique named as hybrid-adaptive neuro-fuzzy inference system tuned with genetic algorithm and particle swarm optimization algorithm that is used to optimize the regression test suites. Evaluation of the proposed approach is performed on benchmark test suites including “previous date problem” and “Siemens print token.” Experimental results are compared with existing state-of-the-art techniques, and results show that the proposed approach is more effective for the reduction in a regression test suites with higher requirement coverage. The size of regression test suites can be reduced up to 48% using the proposed approach without reducing the fault detection rate.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {7287–7301},
numpages = {15},
keywords = {Regression test suite optimization, Genetic algorithm, Particle swarm algorithm, Adaptive neuro-fuzzy inference system}
}

@article{10.1016/j.cie.2021.107616,
author = {Zhang, Wei and Hou, Liang and Jiao, Roger J.},
title = {Dynamic takt time decisions for paced assembly lines balancing and sequencing considering highly mixed-model production: An improved artificial bee colony optimization approach},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107616},
doi = {10.1016/j.cie.2021.107616},
journal = {Comput. Ind. Eng.},
month = nov,
numpages = {13},
keywords = {Balancing, Sequencing, Paced assembly lines, Dynamic takt time, Artificial bee colony}
}

@inproceedings{10.1145/2000259.2000274,
author = {Brosch, Franz and Buhnova, Barbora and Koziolek, Heiko and Reussner, Ralf},
title = {Reliability prediction for fault-tolerant software architectures},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000274},
doi = {10.1145/2000259.2000274},
abstract = {Software fault tolerance mechanisms aim at improving the reliability of software systems. Their effectiveness (i.e., reliability impact) is highly application-specific and depends on the overall system architecture and usage profile. When examining multiple architecture configurations, such as in software product lines, it is a complex and error-prone task to include fault tolerance mechanisms effectively. Existing approaches for reliability analysis of software architectures either do not support modelling fault tolerance mechanisms or are not designed for an efficient evaluation of multiple architecture variants. We present a novel approach to analyse the effect of software fault tolerance mechanisms in varying architecture configurations. We have validated the approach in multiple case studies, including a large-scale industrial system, demonstrating its ability to support architecture design, and its robustness against imprecise input data.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {75–84},
numpages = {10},
keywords = {component-based software architectures, fault tolerance, reliability prediction, software product lines},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@article{10.1016/j.jss.2016.09.045,
author = {Parejo, Jos\'{e} A. and S\'{a}nchez, Ana B. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio and Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Multi-objective test case prioritization in highly configurable systems},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.045},
doi = {10.1016/j.jss.2016.09.045},
abstract = {A multi-objective test case prioritization real-world case study is presented.Seven objective functions based on functional and non-functional data are proposed.Comparison of the effectiveness of 63 combinations of up to three objectives.NSGA-II evolutionary algorithm to solve the multi-objective prioritization problem.Multi-objective prioritization is more effective than mono-objective approaches. Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non-functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
journal = {J. Syst. Softw.},
month = dec,
pages = {287–310},
numpages = {24},
keywords = {Automated software testing, Highly-configurable systems, Test case prioritization, Variability}
}

@inproceedings{10.1007/978-3-030-83978-9_1,
author = {Munksgaard, Philip and Breddam, Svend Lund and Henriksen, Troels and Gieseke, Fabian Cristian and Oancea, Cosmin},
title = {Dataset Sensitive Autotuning of&nbsp;Multi-versioned Code Based on&nbsp;Monotonic Properties: Autotuning in Futhark},
year = {2021},
isbn = {978-3-030-83977-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-83978-9_1},
doi = {10.1007/978-3-030-83978-9_1},
abstract = {Functional languages allow rewrite-rule systems that aggressively generate a multitude of semantically-equivalent but differently-optimized code versions. In the context of GPGPU execution, this paper addresses the important question of how to compose these code versions into a single program that (near-)optimally discriminates them across different datasets. Rather than aiming at a general autotuning framework reliant on stochastic search, we argue that in some cases, a more effective solution can be obtained by customizing the tuning strategy for the compiler transformation producing the code versions.We present a simple and highly-composable strategy which requires that the (dynamic) program property used to discriminate between code versions conforms with a certain monotonicity assumption. Assuming the monotonicity assumption holds, our strategy guarantees that if an optimal solution exists it will be found. If an optimal solution doesn’t exist, our strategy produces human tractable and deterministic results that provide insights into what went wrong and how it can be fixed.We apply our tuning strategy to the incremental-flattening transformation supported by the publicly-available Futhark compiler and compare with a previous black-box tuning solution that uses the popular OpenTuner library. We demonstrate the feasibility of our solution on a set of standard datasets of real-world applications and public benchmark suites, such as Rodinia and FinPar. We show that our approach shortens the tuning time by a factor of 6\texttimes{} on average, and more importantly, in five out of eleven cases, it produces programs that are (as high as 10\texttimes{}) faster than the ones produced by the OpenTuner-based technique.},
booktitle = {Trends in Functional Programming: 22nd International Symposium, TFP 2021, Virtual Event, February 17–19, 2021, Revised Selected Papers},
pages = {3–23},
numpages = {21},
keywords = {Autotuning, GPGPU, Compilers, Nested parallelism, Flattening, Performance}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Automated production systems, Automation, Evolution, Software engineering}
}

@article{10.1145/3385398,
author = {Shaikhha, Amir and Elseidy, Mohammed and Mihaila, Stephan and Espino, Daniel and Koch, Christoph},
title = {Synthesis of Incremental Linear Algebra Programs},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3385398},
doi = {10.1145/3385398},
abstract = {This article targets the Incremental View Maintenance (IVM) of sophisticated analytics (such as statistical models, machine learning programs, and graph algorithms) expressed as linear algebra programs. We present LAGO, a unified framework for linear algebra that automatically synthesizes efficient incremental trigger programs, thereby freeing the user from error-prone manual derivations, performance tuning, and low-level implementation details. The key technique underlying our framework is abstract interpretation, which is used to infer various properties of analytical programs. These properties give the reasoning power required for the automatic synthesis of efficient incremental triggers. We evaluate the effectiveness of our framework on a wide range of applications from regression models to graph computations.},
journal = {ACM Trans. Database Syst.},
month = aug,
articleno = {12},
numpages = {44},
keywords = {Incremental view maintenance (IVM), abstract interpretation, compilation, domain-specific languages, incremental linear algebra, materialized views}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Feature interactions, Highly configurable software systems, Machine learning, Performance prediction, Performance-influence models, Software product lines, Variability}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {dynamic features, energy analysis, probabilistic model checking, software product lines},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@article{10.1016/j.infsof.2021.106693,
author = {Shamsujjoha, Md. and Grundy, John and Li, Li and Khalajzadeh, Hourieh and Lu, Qinghua},
title = {Developing Mobile Applications Via Model Driven Development: A&nbsp;Systematic Literature Review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {140},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106693},
doi = {10.1016/j.infsof.2021.106693},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {24},
keywords = {Systematic Literature Review, Model Driven Development, Mobile App, Tools and Techniques}
}

@article{10.1007/s10009-012-0225-2,
author = {Tartler, Reinhard and Sincero, Julio and Dietrich, Christian and Schr\"{o}der-Preikschat, Wolfgang and Lohmann, Daniel},
title = {Revealing and repairing configuration inconsistencies in large-scale system software},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0225-2},
doi = {10.1007/s10009-012-0225-2},
abstract = {System software typically offers a large amount of compile-time options and variability. A good example is the Linux kernel, which provides more than 10,000 configurable features, growing rapidly. This allows users to tailor it with respect to a broad range of supported hardware architectures and application domains. From the maintenance point of view, compile-time configurability poses big challenges. The configuration model (the selectable features and their constraints as presented to the user) and the configurability that is actually implemented in the code have to be kept in sync, which, if performed manually, is a tedious and error-prone task. In the case of Linux, this has led to numerous defects in the source code, many of which are actual bugs. In order to ensure consistency between the variability expressed in the code and the configuration models, we propose an approach that extracts variability from both into propositional logic. This reveals inconsistencies between variability as expressed by the C Preprocessor (CPP) and an explicit variability model, which manifest themselves in seemingly conditional code that is in fact unconditional. We evaluate our approach with the Linux, for which our tool detects 1,766 configurability defects, which turned out as dead/superfluous source code and bugs. Our findings have led to numerous source-code improvements and bug fixes in Linux: 123 patches (49 merged) fix 364 defects, 147 of which have been confirmed by the corresponding Linux developers and 20 as fixing a previously unknown bug.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {531–551},
numpages = {21},
keywords = {Configurability, Experimentation, Linux, Maintenance, Management, Static analysis}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/2491411.2491459,
author = {Kim, Chang Hwan Peter and Marinov, Darko and Khurshid, Sarfraz and Batory, Don and Souto, Sabrina and Barros, Paulo and D'Amorim, Marcelo},
title = {SPLat: lightweight dynamic analysis for reducing combinatorics in testing configurable systems},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491459},
doi = {10.1145/2491411.2491459},
abstract = {Many programs can be configured through dynamic and/or static selection of configuration variables. A software product line (SPL), for example, specifies a family of programs where each program is defined by a unique combination of features. Systematically testing SPL programs is expensive as it can require running each test against a combinatorial number of configurations. Fortunately, a test is often independent of many configuration variables and need not be run against every combination. Configurations that are not required for a test can be pruned from execution. This paper presents SPLat, a new way to dynamically prune irrelevant configurations: the configurations to run for a test can be determined during test execution by monitoring accesses to configuration variables. SPLat achieves an optimal reduction in the number of configurations and is lightweight compared to prior work that used static analysis and heavyweight dynamic execution. Experimental results on 10 SPLs written in Java show that SPLat substantially reduces the total test execution time in many cases. Moreover, we demonstrate the scalability of SPLat by applying it to a large industrial code base written in Ruby on Rails.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {Automated testing, Configurable Systems, Efficiency, Software Product Lines},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1016/j.micpro.2021.104350,
author = {Sau, Carlo and Rinaldi, Claudia and Pomante, Luigi and Palumbo, Francesca and Valente, Giacomo and Fanni, Tiziana and Martinez, Marcos and van der Linden, Frank and Basten, Twan and Geilen, Marc and Peeren, Geran and Kadlec, Ji\v{r}\'{\i} and J\"{a}\"{a}skel\"{a}inen, Pekka and Bulej, Lubom\'{\i}r and Barranco, Francisco and Saarinen, Jukka and S\"{a}ntti, Tero and Zedda, Maria Katiuscia and Sanchez, Victor and Nikkhah, Shayan Tabatabaei and Goswami, Dip and Amat, Guillermo and Mar\v{s}\'{\i}k, Luk\'{a}\v{s} and van Helvoort, Mark and Medina, Luis and Al-Ars, Zaid and de Beer, Ad},
title = {Design and management of image processing pipelines within CPS: Acquired experience towards the end of the FitOptiVis ECSEL Project},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {87},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.104350},
doi = {10.1016/j.micpro.2021.104350},
journal = {Microprocess. Microsyst.},
month = nov,
numpages = {23},
keywords = {Image processing, Video processing, Distributed system, Heterogeneous system, Multi-objective optimization, Cyber-physical systems}
}

@article{10.1016/j.rcim.2016.01.008,
author = {Luo, Hao and Wang, Kai and Kong, Xiang T.R. and Lu, Shaoping and Qu, Ting},
title = {Synchronized production and logistics via ubiquitous computing technology},
year = {2017},
issue_date = {June 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {45},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.01.008},
doi = {10.1016/j.rcim.2016.01.008},
abstract = {The integration of manufacturing and logistics has drawn widespread research attentions in recent years. This paper focuses on the Synchronized Production and Logistics (SPL), which is operational level integration. SPL\'{z}is defined as synchronizing the processing, moving and storing of raw material, WIP and finished product within one manufacturing unit by high level information sharing and joint scheduling to achieve synergic decision, execution and overall performance improvement. Through analysing the requirements and challenges in real life industry, the ubiquitous computing is adopted as an enabling technology and an Ubi-SPL (Synchronized Production and Logistics via Ubiquitous Technology) framework is proposed. This framework is consists of four layers, which creates a close decision-execution loop by linking the frontline real time data, user feedback and optimized decision together. A real life case study of applying Ubi-SPL solution in a chemical industry has been conducted. The implementation results show that the proposed Ubi-SPL solution can significantly improve the overall performance in both production and logistics service. This paper focuses on the Synchronized Production and Logistics.The ubiquitous computing is adopted as an enabling technology.Synchronized Production &amp; Logistics via Ubiquitous Technics framework is proposed.A real life case study in a chemical industry has been conducted.},
journal = {Robot. Comput.-Integr. Manuf.},
month = jun,
pages = {99–115},
numpages = {17},
keywords = {RFID, Synchronized production and logistics, Ubiquitous manufacturing}
}

@article{10.1007/s10270-017-0594-9,
author = {Famelis, Michalis and Chechik, Marsha},
title = {Managing design-time uncertainty},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0594-9},
doi = {10.1007/s10270-017-0594-9},
abstract = {Managing design-time uncertainty, i.e., uncertainty that developers have about making design decisions, requires creation of "uncertainty-aware" software engineering methodologies. In this paper, we propose a methodological approach for managing uncertainty using partial models. To this end, we identify the stages in the lifecycle of uncertainty-related design decisions and characterize the tasks needed to manage it. We encode this information in the Design-Time Uncertainty Management (DeTUM) model. We then use the DeTUM model to create a coherent, tool-supported methodology centred around partial model management. We demonstrate the effectiveness and feasibility of our methodology through case studies.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1249–1284},
numpages = {36},
keywords = {Design space management, Software design, Software methodology, Software modelling, Uncertainty}
}

@book{10.5555/2838856,
author = {Jeffers, Jim and Reinders, James},
title = {High Performance Parallelism Pearls Volume Two: Multicore and Many-core Programming Approaches},
year = {2015},
isbn = {0128038195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {High Performance Parallelism Pearls Volume 2 offers another set of examples that demonstrate how to leverage parallelism. Similar to Volume 1, the techniques included here explain how to use processors and coprocessors with the same programming illustrating the most effective ways to combine Xeon Phi coprocessors with Xeon and other multicore processors. The book includes examples of successful programming efforts, drawn from across industries and domains such as biomed, genetics, finance, manufacturing, imaging, and more. Each chapter in this edited work includes detailed explanations of the programming techniques used, while showing high performance results on both Intel Xeon Phi coprocessors and multicore processors. Learn from dozens of new examples and case studies illustrating "success stories" demonstrating not just the features of Xeon-powered systems, but also how to leverage parallelism across these heterogeneous systems. Promotes write-once, run-anywhere coding, showing how to code for high performance on multicore processors and Xeon Phi Examples from multiple vertical domains illustrating real-world use of Xeon Phi coprocessors Source code available for download to facilitate further exploration}
}

@article{10.1016/j.future.2015.03.006,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated configuration support for infrastructure migration to the cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.03.006},
doi = {10.1016/j.future.2015.03.006},
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration-in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. We support the decision making in migration planning to the cloud.We use Feature Models to describe the configuration space of an IaaS.We automate the search of the most suitable IaaS configuration.Our approach improves the results of commercial applications on Amazon EC2.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {200–212},
numpages = {13},
keywords = {Automated analysis, Cloud migration, EC2, Feature model, IaaS}
}

@article{10.1016/j.jss.2016.06.068,
author = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
title = {Cloud migration process-A survey, evaluation framework, and open challenges},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.068},
doi = {10.1016/j.jss.2016.06.068},
abstract = {The relevant approaches for migrating legacy applications to the cloud are surveyed.An extensive analysis of existing approaches on the basis of a set of important criteria/features.Important cloud migration activities, techniques, and concerns that need to be properly addressed in a typical cloud migration process are delineated.Existing open issues and future research opportunities on the cloud migration research area are discussed. Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.},
journal = {J. Syst. Softw.},
month = oct,
pages = {31–69},
numpages = {39},
keywords = {Cloud computing, Cloud migration, Evaluation framework, Legacy application, Migration methodology, Process model}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {curriculum learning, reinforcement learning, self-paced learning, tempered inference, rl-as-inference}
}

@inbook{10.5555/1793854.1793863,
author = {Liu, Chunjian Robin and Gibbs, Celina and Coady, Yvonne},
title = {Safe and sound evolution with SONAR: sustainable optimization and navigation with aspects for system-wide reconciliation},
year = {2007},
isbn = {3540770410},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Traditional diagnostic and optimization techniques typically rely on static instrumentation of a small portion of an overall system. Unfortunately, solely static and localized approaches are simply no longer sustainable in the evolution of today's complex and dynamic systems. Sustainable Optimization and Navigation with Aspects for system-wide Reconciliation is a fluid and unified framework that enables stakeholders to explore and adapt meaningful entities that are otherwise spread across predefined abstraction boundaries. Through a combination of Aspect-Oriented Programming, Extensible Markup Language, and management tools such as Java Management Extensions, SONAR can comprehensively coalesce scattered artifacts--enabling evolution to be more inclusive of system-wide considerations by supporting both iterative and interactive practices. We believe this system-wide approach promotes the application of safe and sound principles in system evolution. This paper presents SONAR's model, examples of its concrete manifestation, and an overview of its associated costs and benefits. Case studies demonstrate how SONAR can be used to accurately identify performance bottlenecks and effectively evolve systems by optimizing behaviour, even at runtime.},
booktitle = {Transactions on Aspect-Oriented Software Development IV},
pages = {163–190},
numpages = {28}
}

@article{10.1016/j.jss.2019.03.011,
author = {Zhang, Man and Ali, Shaukat and Yue, Tao},
title = {Uncertainty-wise test case generation and minimization for Cyber-Physical Systems},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {153},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.011},
doi = {10.1016/j.jss.2019.03.011},
journal = {J. Syst. Softw.},
month = jul,
pages = {1–21},
numpages = {21},
keywords = {Uncertainty, Cyber-Physical System, Test case generation and minimization, Multi-objective search}
}

@inproceedings{10.5555/2682923.2682935,
author = {Bittner, B. and Bozzano, M. and Cimatti, A. and Gario, M. and Griggio, A.},
title = {Towards Pareto-Optimal Parameter Synthesis for Monotonic Cost Functions},
year = {2014},
isbn = {9780983567844},
publisher = {FMCAD Inc},
address = {Austin, Texas},
abstract = {Designers are often required to explore alternative solutions, trading off along different dimensions (e.g., power consumption, weight, cost, reliability, response time). Such exploration can be encoded as a problem of parameter synthesis, i.e., finding a parameter valuation (representing a design solution) such that the corresponding system satisfies a desired property. In this paper, we tackle the problem of parameter synthesis with multi-dimensional cost functions by finding solutions that are in the Pareto front: in the space of best trade-offs possible. We propose several algorithms, based on IC3, that interleave in various ways the search for parameter valuations that satisfy the property, and the optimization with respect to costs. The most effective one relies on the reuse of inductive invariants and on the extraction of unsatisfiable cores to accelerate convergence. Our experimental evaluation shows the feasibility of the approach on practical benchmarks from diagnosability synthesis and product-line engineering, and demonstrates the importance of a tight integration between model checking and cost optimization.},
booktitle = {Proceedings of the 14th Conference on Formal Methods in Computer-Aided Design},
pages = {23–30},
numpages = {8},
location = {Lausanne, Switzerland},
series = {FMCAD '14}
}

@inproceedings{10.5555/3291291.3291317,
author = {Rivera, Luis F. and Villegas, Norha M. and Tamura, Gabriel and Jim\'{e}nez, Miguel and M\"{u}ller, Hausi A.},
title = {UML-driven automated software deployment},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software companies face the challenge of ensuring customer satisfaction through the continuous delivery of functionalities and rapid response to quality issues. However, achieving frequent software delivery is not a trivial task. It requires agile---and continuous---design, development and deployment of existing and new software features. Over time, managing these systems becomes increasingly complex. This complexity stems, in part, from the deployment pipelines and the myriad possible configurations of the software components. Furthermore, software deployment is a time-consuming and error-prone process, which, even when automated, can lead to configuration errors and cost overruns. In this paper, we address deployment challenges that developers face during continuous delivery and DevOps. Our proposal consists of Urano, a mechanism for automating the deployment process, which uses UML, an interoperable and de facto modeling standard, as a means of specifying a software architecture and its associated deployment. Our approach is based on the model-driven architecture principles to generate executable deployment specifications from user-defined UML deployment diagrams. We extend this kind of diagrams by defining and applying a UML profile that captures the semantics and requirements of the installation, configuration, and update of software components. Thus, enabling more expressive deployment specifications and their automatic realization. To evaluate Urano, we conducted three case studies that demonstrate its potential to effectively automate software deployment processes in industry.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {257–268},
numpages = {12},
keywords = {DevOps, UML, continuous delivery, deployment, model-driven architecture, model-driven engineering},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1016/j.jvlc.2013.08.001,
author = {Anjorin, Anthony and Saller, Karsten and Reimund, Ingo and Oster, Sebastian and Zorcic, Ivan and Sch\"{u}rr, Andy},
title = {Model-driven rapid prototyping with programmed graph transformations},
year = {2013},
issue_date = {December, 2013},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {24},
number = {6},
issn = {1045-926X},
url = {https://doi.org/10.1016/j.jvlc.2013.08.001},
doi = {10.1016/j.jvlc.2013.08.001},
abstract = {Modern software systems are constantly increasing in complexity and supporting the rapid prototyping of such systems has become crucial to check the feasibility of extensions and optimizations, thereby reducing risks and, consequently, the cost of development. As modern software systems are also expected to be reused, extended, and adapted over a much longer lifetime than ever before, ensuring the maintainability of such systems is equally gaining relevance. In this paper, we present the development, optimization and maintenance of MoSo-PoLiTe, a framework for Software Product Line (SPL) testing, as a novel case study for rapid prototyping via metamodelling and programmed graph transformations. The first part of the case study evaluates the use of programmed graph transformations for optimizing an existing, hand-written system (MoSo-PoLiTe) via rapid prototyping of various strategies. In the second part, we present a complete re-engineering of the hand-written system with programmed graph transformations and provide a critical comparison of both implementations. Our results and conclusions indicate that metamodelling and programmed graph transformation are not only suitable techniques for rapid prototyping, but also lead to more maintainable systems.},
journal = {J. Vis. Lang. Comput.},
month = dec,
pages = {441–462},
numpages = {22},
keywords = {Metamodelling, Model-driven testing, Programmed graph transformations, Rapid prototyping, Software product lines}
}

@inproceedings{10.5555/3466184.3466364,
author = {Heger, Jens and Voss, Thomas},
title = {Dynamically changing sequencing rules with reinforcement learning in a job shop system with stochastic influences},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Sequencing operations can be difficult, especially under uncertain conditions. Applying decentral sequencing rules has been a viable option; however, no rule exists that can outperform all other rules under varying system performance. For this reason, reinforcement learning (RL) is used as a hyper heuristic to select a sequencing rule based on the system status. Based on multiple training scenarios considering stochastic influences, such as varying inter arrival time or customers changing the product mix, the advantages of RL are presented. For evaluation, the trained agents are exploited in a generic manufacturing system. The best agent trained is able to dynamically adjust sequencing rules based on system performance, thereby matching and outperforming the presumed best static sequencing rules by ≈ 3%. Using the trained policy in an unknown scenario, the RL heuristic is still able to change the sequencing rule according to the system status, thereby providing robust performance.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1608–1618},
numpages = {11},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3239372.3239392,
author = {Arcega, Lorena and Font, Jaime and Cetina, Carlos},
title = {Evolutionary Algorithm for Bug Localization in the Reconfigurations of Models at Runtime},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239392},
doi = {10.1145/3239372.3239392},
abstract = {Systems with models at runtime are becoming increasingly complex, and this is also accompanied by more software bugs. In this paper, we focus on bugs appearing as the result of dynamic reconfigurations of the system due to context changes. We materialize our approach for bug localization in reconfigurations as an evolutionary algorithm. We guide the evolutionary algorithm with a fitness function that measures the similarity to the description of the bug report. The result is a ranked list of reconfiguration sequences, which is intended to identify the reconfiguration rules that are relevant to the bug. We evaluated our approach in BSH and CAF, two real-world industrial case studies, measuring the results in terms of recall, precision, F-measure and Matthews Correlation Coefficient (MCC). In our evaluation, we compare our approach with two other approaches: a baseline that is the one used by our industrial partners for bug localization and a random search as sanity check. Our study shows that our approach, which takes advantage of the reconfigurations of models at runtime, outperforms the other two approaches. We also performed a statistical analysis to provide evidence of the significance of the results.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {90–100},
numpages = {11},
keywords = {Bug Localization, Models at Runtime},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Control-flow feature interaction, Feature, Feature interaction, Feature-interaction prediction, Highly configurable software system, Performance feature interaction, Variability}
}

@inproceedings{10.5555/3042094.3042444,
author = {Aurich, Paul and Nahhas, Abdulrahman and Reggelin, Tobias and Tolujew, Juri},
title = {Simulation-based optimization for solving a hybrid flow shop scheduling problem},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {This paper describes the solution of a hybrid flow shop (HFS) scheduling problem of a printed circuit board assembly. The production comprises four surface-mount device placement machines on the first stage and five automated optical inspection machines on the second stage. The objective is to minimize the makespan and the total tardiness. The paper compares three approaches to solve the HFS scheduling problem: an integrated simulation-based optimization algorithm (ISBO) developed by the authors and two metaheuristics, simulated annealing and tabu search. All approaches lead to an improvement in terms of producing more jobs on time while minimizing the makespan compared to the decision rules used so far in the analyzed company. The ISBO delivers results much faster than the two metaheuristics. The two metaheuristics lead to slightly better results than the ISBO in terms of total tardiness.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2809–2819},
numpages = {11},
location = {Arlington, Virginia},
series = {WSC '16}
}

@article{10.3233/SW-150186,
author = {Polleres, Axel and Saleem, Muhammad and Khan, Yasar and Hasnain, Ali and Ermilov, Ivan and Ngonga Ngomo, Axel-Cyrille},
title = {A fine-grained evaluation of SPARQL endpoint federation systems},
year = {2016},
issue_date = {2016},
publisher = {IOS Press},
address = {NLD},
volume = {7},
number = {5},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-150186},
doi = {10.3233/SW-150186},
abstract = {The Web of Data has grown enormously over the last years. Currently, it comprises a large compendium of interlinked and distributed datasets from multiple domains. Running complex queries on this compendium often requires accessing data from different endpoints within one query. The abundance of datasets and the need for running complex query has thus motivated a considerable body of work on SPARQL query federation systems, the dedicated means to access data distributed over the Web of Data. However, the granularity of previous evaluations of such systems has not allowed deriving of insights concerning their behavior in different steps involved during federated query processing. In this work, we perform extensive experiments to compare state-of-the-art SPARQL endpoint federation systems using the comprehensive performance evaluation framework FedBench. In addition to considering the tradition query runtime as an evaluation criterion, we extend the scope of our performance evaluation by considering criteria, which have not been paid much attention to in previous studies. In particular, we consider the number of sources selected, the total number of SPARQL ASK requests used, the completeness of answers as well as the source selection time. Yet, we show that they have a significant impact on the overall query runtime of existing systems. Moreover, we extend FedBench to mirror a highly distributed data environment and assess the behavior of existing systems by using the same performance criteria. As the result we provide a detailed analysis of the experimental outcomes that reveal novel insights for improving current and future SPARQL federation systems.},
journal = {Semant. Web},
month = jan,
pages = {493–518},
numpages = {26},
keywords = {SPARQL federation, Web of Data, RDF}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {deep sparse feedforward neural network, highly configurable systems, software performance prediction, sparsity regularization},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@article{10.1007/s10845-015-1078-9,
author = {Chamnanlor, Chettha and Sethanan, Kanchana and Gen, Mitsuo and Chien, Chen-Fu},
title = {Embedding ant system in genetic algorithm for re-entrant hybrid flow shop scheduling problems with time window constraints},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {8},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-015-1078-9},
doi = {10.1007/s10845-015-1078-9},
abstract = {This paper focuses on minimizing the makespan for a reentrant hybrid flow shop scheduling problem with time window constraints (RHFSTW), which is often found in manufacturing systems producing the slider part of hard-disk drive products, in which production needs to be monitored to ensure high quality. For this reason, production time control is required from the starting-time-window stage to the ending-time-window stage. Because of the complexity of the RHFSTW problem, in this paper, genetic algorithm hybridized ant colony optimization (GACO) is proposed to be used as a support tool for scheduling. The results show that the GACO can solve problems optimally with reasonable computational effort.},
journal = {J. Intell. Manuf.},
month = dec,
pages = {1915–1931},
numpages = {17},
keywords = {Ant colony optimization, Hybrid genetic algorithm, Local search, Reentrant flexible flow shop, Time window}
}

@article{10.1016/j.jss.2017.05.051,
author = {Vogel-Heuser, Birgit and Fischer, Juliane and Feldmann, Stefan and Ulewicz, Sebastian and Rsch, Susanne},
title = {Modularity and architecture of PLC-based software for automated production Systems},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.05.051},
doi = {10.1016/j.jss.2017.05.051},
abstract = {Overview of the state of the art in industrial software engineering of aPS using PLCs.Benchmark process to evaluate the maturity of aPS application software.SWMAT4aPS consists of a self-assessment questionnaire and an expert analysis.3 Maturity measures: modularity, test/quality assurance and start-up/operation/maintenance.Weaknesses of aPS software related to mechanics and automation hardware in comparison to pure software identified. Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry.},
journal = {J. Syst. Softw.},
month = sep,
pages = {35–62},
numpages = {28},
keywords = {Automated production systems, Control software, Factory automation, Maturity, Modularity, Programmable logic controller}
}

@inproceedings{10.5555/3466184.3466362,
author = {Rolf, Benjamin and Reggelin, Tobias and Nahhas, Abdulrahman and M\"{u}ller, Marcel and Lang, Sebastian},
title = {Scheduling jobs in a two-stage hybrid flow shop with a simulation-based genetic algorithm and standard dispatching rules},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {The paper proposes a simulation-based hyperheuristics approach to generate schedules for a two-stage hybrid flow shop scheduling problem with sequence-dependent setup times. The scheduling problem is derived from a company that is assembling printed circuit boards. A genetic algorithm determines sequences of standard dispatching rules that are evaluated by a discrete-event simulation model minimizing a multi-criteria objective composed of makespan and total tardiness. To reduce the computation time of the algorithm a dispatching rule-based chromosome representation is used containing a sequence of dispatching rules and time intervals in which the rules are applied. Different experiment configurations and their impact on solution quality and computation time are analyzed. The optimization model generates efficient schedules for multiple real-world data sets.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1584–1595},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1016/j.datak.2014.07.003,
author = {Bre\ss{}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
title = {Load-aware inter-co-processor parallelism in database query processing},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2014.07.003},
doi = {10.1016/j.datak.2014.07.003},
abstract = {For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. Contribute heuristics to enhance performance by exploiting inter-device parallelismHeuristics consider load and speed on (co-)processors.Extensive evaluation on four use cases: aggregation, selection, sort, and joinAssess the performance of best heuristic for systems with multiple co-processorsDiscuss how operator-stream-based scheduling can be used in a query processor},
journal = {Data Knowl. Eng.},
month = sep,
pages = {60–79},
numpages = {20},
keywords = {Co-processing, Query optimization, Query processing}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Dynamic Software Product Lines, Reconfiguration Decisions, Timed Automata},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1007/11921998_14,
author = {Florentz, Bastian and Huhn, Michaela},
title = {Embedded systems architecture: evaluation and analysis},
year = {2006},
isbn = {3540488197},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11921998_14},
doi = {10.1007/11921998_14},
abstract = {Short innovation cycles in software and hardware make architecture design a key issue in future development processes for embedded systems. The basis for architectural design decisions is a transparent architecture evaluation.Our model-based approach supports a uniform representation of hierarchies of quality attributes and an integration of different architecture evaluation techniques and methods. We present a metamodel for architecture evaluation as a basis for the precise description of the quality attribute structure and the evaluation methodology. By modelling architecture evaluation, the relationships between architectural elements and quality attributes and interdependencies between quality attributes can be represented and investigated. Thereby, the architecture exploration process with its evaluations, decisions, and optimizations is made explicit, traceable, and analyzable.},
booktitle = {Proceedings of the Second International Conference on Quality of Software Architectures},
pages = {145–162},
numpages = {18},
location = {V\"{a}ster\r{a}s, Sweden},
series = {QoSA'06}
}

@article{10.1016/j.cie.2016.08.016,
author = {Mei, Ying and Ye, Jiawei and Zeng, Zhigang},
title = {Entropy-weighted ANP fuzzy comprehensive evaluation of interim product production schemes in one-of-a-kind production},
year = {2016},
issue_date = {October 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {100},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.08.016},
doi = {10.1016/j.cie.2016.08.016},
abstract = {Analyze the multi-criteria decision making problem in OKP.Provide an influence factor system for optimizing interim product production scheme.Propose the entropy-weighted ANP fuzzy comprehensive evaluation method.Case study in shipbuilding. Most one-of-a-kind production (OKP) products are complex-shaped structural components, where there are only few identical OKP interim products in the same batch. OKP flexible interim product production has the nature of multi-criteria production; the multi-criteria production characteristics correspond to different manufacturing technologies and methods. How to reasonably and effectively determine group/batch production schemes of OKP interim products has become a key strategic consideration in OKP enterprises multi-criteria production decisions. The primary problem of the current multi-criteria evaluation method is how to judge the matrix scale and determine the weight set, which primarily depends on the rich experience of relevant specialists. These subjective decisions make the evaluation results prone to disagreements. To comprehensively and scientifically evaluate the production processes influenced by the complicated factors in OKP, we establish an influence factor system and present a method that combines subjective and objective weights based on entropy and an analytic network process (ANP). The method avoids a multifarious optimizing process. In addition, this is a novel attempt to use fuzzy comprehensive evaluation based on ANP in production scheme optimization, which to an extent, enriches process optimization methods in manufacturing. Through case studies, we demonstrate our methods feasibility and rationality. The improved evaluation result provides support for multi-criteria production decisions of interim products in OKP.},
journal = {Comput. Ind. Eng.},
month = oct,
pages = {144–152},
numpages = {9},
keywords = {Analytic network process (ANP), Entropy, Fuzzy comprehensive evaluation, Multi-criteria decision making problem, One-of-a-kind production (OKP)}
}

@article{10.1016/j.infsof.2019.03.015,
author = {Borg, Markus and Chatzipetrou, Panagiota and Wnuk, Krzysztof and Al\'{e}groth, Emil and Gorschek, Tony and Papatheocharous, Efi and Shah, Syed Muhammad Ali and Axelsson, Jakob},
title = {Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.015},
doi = {10.1016/j.infsof.2019.03.015},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {18–34},
numpages = {17},
keywords = {Component-based software engineering, Sourcing, Software architecture, Decision making, Survey}
}

@article{10.5555/ios.INF1041,
author = {Lupeikiene, Audrone and Dzemyda, Gintautas and Kiss, Ferenc and Caplinskas, Albertas},
title = {Advanced Planning and Scheduling Systems: Modeling and Implementation Challenges},
year = {2015},
issue_date = {Apr 2015},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {4},
issn = {0868-4952},
abstract = {The paper summarizes the results of research on the modeling and implementation of advanced planning and scheduling (APS) systems done in recent twenty years. It discusses the concept of APS system – how it is thought of today – and highlights the modeling and implementation challenges with which the developers of such systems should cope. Some from these challenges were identified as a result of the study of scientific literature, others – through an in-depth analysis of the experience gained during the development of real-world APS system – a Production Efficiency Navigator (PEN system). The paper contributes to APS systems theory by proposing the concept of an ensemble of collaborating algorithms.},
journal = {Informatica},
month = apr,
pages = {581–616},
numpages = {36},
keywords = {advanced planning and scheduling, optimization, simulation, reference modeling, architectural modeling}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {auto-tuning, multi-tenancy, self-optimization, software-as-a-service, variability modeling},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1016/j.jss.2021.111070,
author = {Rosiak, Kamil and Schlie, Alexander and Linsbauer, Lukas and Vogel-Heuser, Birgit and Schaefer, Ina},
title = {Custom-tailored clone detection for IEC 61131-3 programming languages},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111070},
doi = {10.1016/j.jss.2021.111070},
journal = {J. Syst. Softw.},
month = dec,
numpages = {18},
keywords = {Clone detection, Variability mining, IEC&nbsp;61131-3, Reverse engineering}
}

@article{10.4018/ijsse.2015010103,
author = {Raschke, Wolfgang and Zilli, Massimiliano and Baumgartner, Philip and Loinig, Johannes and Steger, Christian and Kreiner, Christian},
title = {Balancing Product and Process Assurance for Evolving Security Systems},
year = {2015},
issue_date = {January 2015},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {1},
issn = {1947-3036},
url = {https://doi.org/10.4018/ijsse.2015010103},
doi = {10.4018/ijsse.2015010103},
abstract = {At present, security-related engineering usually requires a big up-front design BUFD regarding security requirements and security design. In addition to the BUFD, at the end of the development, a security evaluation process can take up to several months. In today's volatile markets customers want to be able to influence the software design during the development process. Agile processes have proven to support these demands. Nevertheless, there is a clash between traditional security design and evaluation processes. In this paper, the authors propose an agile security evaluation method for the Common Criteria standard. This method is complemented by an implementation of a change detection analysis for model-based security requirements. This system facilitates the agile security evaluation process to a high degree. However, the application of the proposed evaluation method is limited by several constraints. The authors discuss these constraints and show how traditional certification schemes could be extended to better support modern industrial software development processes.},
journal = {Int. J. Secur. Softw. Eng.},
month = jan,
pages = {47–75},
numpages = {29},
keywords = {Agile Development, Common Criteria, Model Evolution, Model-Based Software Development, Security, Traceability}
}

@article{10.1007/s10664-020-09922-8,
author = {Grebhahn, Alexander and Kaltenecker, Christian and Engwer, Christian and Siegmund, Norbert and Apel, Sven},
title = {Lightweight, semi-automatic variability extraction: a case study on scientific computing},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09922-8},
doi = {10.1007/s10664-020-09922-8},
abstract = {In scientific computing, researchers often use feature-rich software frameworks to simulate physical, chemical, and biological processes. Commonly, researchers follow a clone-and-own approach: Copying the code of an existing, similar simulation and adapting it to the new simulation scenario. In this process, a user has to select suitable artifacts (e.g., classes) from the given framework and replaces the existing artifacts from the cloned simulation. This manual process incurs substantial effort and cost as scientific frameworks are complex and provide large numbers of artifacts. To support researchers in this area, we propose a lightweight API-based analysis approach, called VORM, that recommends appropriate artifacts as possible alternatives for replacing given artifacts. Such alternative artifacts can speed up performance of the simulation or make it amenable to other use cases, without modifying the overall structure of the simulation. We evaluate the practicality of VORM—especially, as it is very lightweight but possibly imprecise—by means of a case study on the DUNE numerics framework and two simulations from the realm of physical simulations. Specifically, we compare the recommendations by VORM with recommendations by a domain expert (a developer of DUNE). VORM recommended 34 out of the 37 artifacts proposed by the expert. In addition, it recommended 2 artifacts that are applicable but have been missed by the expert and 32 artifacts not recommended by the expert, which however are still applicable in the simulation scenario with slight modifications. Diving deeper into the results, we identified an undiscovered bug and an inconsistency in DUNE, which corroborates the usefulness of VORM.},
journal = {Empirical Softw. Engg.},
month = mar,
numpages = {22},
keywords = {Software variability, Configuration, Variability extraction, Variability analysis}
}

@article{10.1007/s10619-013-7130-x,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Automatic optimization of stream programs via source program operator graph transformations},
year = {2013},
issue_date = {December  2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {4},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-013-7130-x},
doi = {10.1007/s10619-013-7130-x},
abstract = {Distributed data stream processing is a data analysis paradigm where massive amounts of data produced by various sources are analyzed online within real-time constraints. Execution performance of a stream program/query executed on such middleware is largely dependent on the ability of the programmer to fine tune the program to match the topology of the stream processing system. However, manual fine tuning of a stream program is a very difficult, error prone process that demands huge amounts of programmer time and expertise which are expensive to obtain. We describe an automated process for stream program performance optimization that uses semantic preserving automatic code transformation to improve stream processing job performance. We first identify the structure of the input program and represent the program structure in a Directed Acyclic Graph. We transform the graph using the concepts of Tri-OP Transformation and Bi-Op Transformation. The resulting sample program space is pruned using both empirical as well as profiling information to obtain a ranked list of sample programs which have higher performance compared to their parent program. We successfully implemented this methodology on a prototype stream program performance optimization mechanism called Hirundo. The mechanism has been developed for optimizing SPADE programs which run on System S stream processing run-time. Using five real world applications (called VWAP, CDR, Twitter, Apnoea, and Bargain) we show the effectiveness of our approach. Hirundo was able to identify a 31.1 times higher performance version of the CDR application within seven minutes time on a cluster of 4 nodes.},
journal = {Distrib. Parallel Databases},
month = dec,
pages = {543–599},
numpages = {57},
keywords = {Automatic tuning, Code transformation, Data-intensive computing, Performance optimization, Stream processing}
}

@article{10.1145/2379776.2379778,
author = {Bernardi, Simona and Merseguer, Jos\'{e} and Petriu, Dorina C.},
title = {Dependability modeling and analysis of software systems specified with UML},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2379776.2379778},
doi = {10.1145/2379776.2379778},
abstract = {The goal is to survey dependability modeling and analysis of software and systems specified with UML, with focus on reliability, availability, maintainability, and safety (RAMS). From the literature published in the last decade, 33 approaches presented in 43 papers were identified. They are evaluated according to three sets of criteria regarding UML modeling issues, addressed dependability characteristics, and quality assessment of the surveyed approaches. The survey shows that more works are devoted to reliability and safety, fewer to availability and maintainability, and none to integrity. Many methods support early life-cycle phases (from requirements to design). More research is needed for tool development to automate the derivation of analysis models and to give feedback to designers.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {2},
numpages = {48},
keywords = {Availability, dependability analysis, maintainability, model transformation, safety}
}

@article{10.1016/j.infsof.2011.06.002,
author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
title = {A systematic review of software architecture evolution research},
year = {2012},
issue_date = {January, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.002},
doi = {10.1016/j.infsof.2011.06.002},
abstract = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–40},
numpages = {25},
keywords = {Architecture analysis, Architecture evolution, Evolvability analysis, Software architecture, Software evolvability, Systematic review}
}

@inproceedings{10.1145/2851141.2851148,
author = {De Matteis, Tiziano and Mencagli, Gabriele},
title = {Keep calm and react with foresight: strategies for low-latency and energy-efficient elastic data stream processing},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851148},
doi = {10.1145/2851141.2851148},
abstract = {This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The results demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {13},
numpages = {12},
keywords = {DVFS, data stream processing, elasticity, model predictive control, multicore programming},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@inproceedings{10.1145/2460999.2461003,
author = {Garousi, Golara and Garousi, Vahid and Moussavi, Mahmoud and Ruhe, Guenther and Smith, Brian},
title = {Evaluating usage and quality of technical software documentation: an empirical study},
year = {2013},
isbn = {9781450318488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460999.2461003},
doi = {10.1145/2460999.2461003},
abstract = {Context: Software documentation is an integral part of any software development process. However, software practitioners are often concerned about the lack of usage and quality of documentation in practice. Unfortunately, in many projects, practitioners find that software documentation artifacts are outdated, incomplete and sometimes not beneficial. Objective: Motivated by the needs of NovAtel Inc. (NovAtel), a world-leading company of GPS software systems, we propose in this paper an approach to analyze the usage and quality of software documentation in development and maintenance phases. Method: The approach incorporates inputs from automated analysis (e.g., mining of project's data) and also experts' opinion extracted from survey-based questionnaire. The approach has been designed based on the "action-research" approach and in close collaboration between industry and academia. Results: To evaluate the feasibility and usefulness of the proposed approach, we have applied it in an industrial setting and results are presented in this paper. One of the results is that, in the context of our case-study, usage of documentation for an implementation purpose is higher than the usage for maintenance purposes. Conclusion: It is concluded that the usage of documentation differs for various purposes and it depends on the type of the information needs as well as the task to be completed (e.g. development and maintenance). In addition, we identify the most important and relevant quality attributes which are critical to improving documentation quality.},
booktitle = {Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering},
pages = {24–35},
numpages = {12},
keywords = {action research, case study, empirical software engineering, maintenance, quality, software development, software documentation, technical software documentation, usage},
location = {Porto de Galinhas, Brazil},
series = {EASE '13}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Multi-objective optimization, Search-based software engineering, Software library, Software reuse}
}

@article{10.1016/j.jss.2013.11.1121,
author = {Sutcliffe, Alistair and Papamargaritis, George},
title = {End-user development by application-domain configuration},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.11.1121},
doi = {10.1016/j.jss.2013.11.1121},
abstract = {An application generator/tailoring tool aimed at end users is described. It employs conceptual models of problem domains to drive configuration of an application generator suitable for a related set of applications, such as reservation and resource allocation. The tool supports a two-phase approach of configuring the general architecture for a domain, such as reservation-booking problems, then customisation and generation of specific applications. The tool also provides customisable natural language-style queries for spatial and temporal terms. Development and use of the tool to generate two applications, service engineer call allocation, and airline seat reservation, are reported with a specification exercise to configure the generic architecture to a new problem domain for monitoring-sensing applications. The application generator/tailoring tool is evaluated with novice end users and experts to demonstrate its effectiveness.},
journal = {J. Syst. Softw.},
month = may,
pages = {85–99},
numpages = {15},
keywords = {Application generation, Domain-oriented design, End-user development}
}

@article{10.1007/s10617-009-9041-7,
author = {Islam, Shariful and Suri, Neeraj and Balogh, Andr\'{a}s and Csert\'{a}n, Gy\"{o}rgy and Pataricza, Andr\'{a}s},
title = {An optimization based design for integrated dependable real-time embedded systems},
year = {2009},
issue_date = {December  2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {0929-5585},
url = {https://doi.org/10.1007/s10617-009-9041-7},
doi = {10.1007/s10617-009-9041-7},
abstract = {Moving from the traditional federated design paradigm, integration of mixed-criticality software components onto common computing platforms is increasingly being adopted by automotive, avionics and the control industry. This method faces new challenges such as the integration of varied functionalities (dependability, responsiveness, power consumption, etc.) under platform resource constraints and the prevention of error propagation. Based on model driven architecture and platform based design's principles, we present a systematic mapping process for such integration adhering a transformation based design methodology. Our aim is to convert/transform initial platform independent application specifications into post integration platform specific models. In this paper, a heuristic based resource allocation approach is depicted for the consolidated mapping of safety critical and non-safety critical applications onto a common computing platform meeting particularly dependability/fault-tolerance and real-time requirements. We develop a supporting tool suite for the proposed framework, where VIATRA (VIsual Automated model TRAnsformations) is used as a transformation tool at different design steps. We validate the process and provide experimental results to show the effectiveness, performance and robustness of the approach.},
journal = {Des. Autom. Embedded Syst.},
month = dec,
pages = {245–285},
numpages = {41},
keywords = {Constraints, Fault-tolerance, Mapping, Real-time, Transformation}
}

@inproceedings{10.1007/978-3-642-37057-1_7,
author = {Rubin, Julia and Chechik, Marsha},
title = {Quality of merge-refactorings for product lines},
year = {2013},
isbn = {9783642370564},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-37057-1_7},
doi = {10.1007/978-3-642-37057-1_7},
abstract = {In this paper, we consider the problem of refactoring related software products specified in UML into annotative product line representations. Our approach relies on identifying commonalities and variabilities in existing products and further merging those into product line representations which reduce duplications and facilitate reuse. Varying merge strategies can lead to producing several semantically correct, yet syntactically different refactoring results. Depending on the goal of the refactoring, one result can be preferred to another. We thus propose to capture the goal using a syntactic quality function and use that function to guide the merge strategy. We define and implement a quality-based merge-refactoring framework for UML models containing class and statechart diagrams and report on our experience applying it on three case-studies.},
booktitle = {Proceedings of the 16th International Conference on Fundamental Approaches to Software Engineering},
pages = {83–98},
numpages = {16},
location = {Rome, Italy},
series = {FASE'13}
}

@article{10.1145/3284971.3284975,
author = {Lazreg, Sami and Collet, Philippe and Mosser, S\'{e}bastien},
title = {Functional feasibility analysis of variability-intensive data flow-oriented applications over highly-configurable platforms},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3284971.3284975},
doi = {10.1145/3284971.3284975},
abstract = {Data-flow oriented embedded systems, such as automotive systems used to render HMI (e.g., instrument clusters, infotainments), are increasingly built from highly variable specifications while targeting different constrained hardware platforms configurable in a fine-grained way. These variabilities at two different levels lead to a huge number of possible embedded system solutions, which functional feasibility is extremely complex and tedious to predetermine. In this paper, we propose a tooled approach that capture high level specifications as variable dataflows, and targeted platforms as variable component models. Dataflows can then be mapped onto platforms to express a specification of such variability-intensive systems. The proposed solution transforms this specification into structural and behavioral variability models and reuses automated reasoning techniques to explore and assess the functional feasibility of all variants in a single run. We also report on the validation of the proposed approach. A qualitative evaluation has been conducted on an industrial case study of automotive instrument cluster, while a quantitative one is reported on large generated datasets.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {behavioral product lines model checking, embedded system design engineering, feature model, variability modeling}
}

@inproceedings{10.1007/978-3-642-31491-9_18,
author = {Behjati, Razieh and Nejati, Shiva and Yue, Tao and Gotlieb, Arnaud and Briand, Lionel},
title = {Model-based automated and guided configuration of embedded software systems},
year = {2012},
isbn = {9783642314902},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31491-9_18},
doi = {10.1007/978-3-642-31491-9_18},
abstract = {Configuring Integrated Control Systems (ICSs) is largely manual, time-consuming and error-prone. In this paper, we propose a model-based configuration approach that interactively guides engineers to configure software embedded in ICSs. Our approach verifies engineers' decisions at each configuration iteration, and further, automates some of the decisions. We use a constraint solver, SICStus Prolog, to automatically infer configuration decisions and to ensure the consistency of configuration data. We evaluated our approach by applying it to a real subsea oil production system. Specifically, we rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach successfully enforces consistency of configurations, can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
booktitle = {Proceedings of the 8th European Conference on Modelling Foundations and Applications},
pages = {226–243},
numpages = {18},
keywords = {UML/OCL, constraint satisfaction, model-based software engineering, product configuration},
location = {Kgs. Lyngby, Denmark},
series = {ECMFA'12}
}

@article{10.1016/j.is.2019.03.011,
author = {Vassiliadis, Panos and Marcel, Patrick and Rizzi, Stefano},
title = {Beyond roll-up’s and drill-down’s: An intentional analytics model to reinvent OLAP},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {85},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2019.03.011},
doi = {10.1016/j.is.2019.03.011},
journal = {Inf. Syst.},
month = nov,
pages = {68–91},
numpages = {24}
}

@article{10.4018/jismd.2012040102,
author = {Mazo, Ra\'{u}l and Salinesi, Camille and Diaz, Daniel and Djebbi, Olfa and Lora-Michiels, Alberto},
title = {Constraints: The Heart of Domain and Application Engineering in the Product Lines Engineering Strategy},
year = {2012},
issue_date = {April 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012040102},
doi = {10.4018/jismd.2012040102},
abstract = {Drawing from an analogy between features based Product Line PL models and Constraint Programming CP, this paper explores the use of CP in the Domain Engineering and Application Engineering activities that are put in motion in a Product Line Engineering strategy. Specifying a PL as a constraint program instead of a feature model carries out two important qualities of CP: expressiveness and direct automation. On the one hand, variables in CP can take values over boolean, integer, real or even complex domains and not only boolean values as in most PL languages such as the Feature-Oriented Domain Analysis FODA. Specifying boolean, arithmetic, symbolic and reified constraint, provides a power of expression that spans beyond that provided by the boolean dependencies in FODA models. On the other hand, PL models expressed as constraint programs can directly be executed and analyzed by off-the-shelf solvers. This paper explores the issues of a how to specify a PL model using CP, including in the presence of multi-model representation, b how to verify PL specifications, c how to specify configuration requirements, and d how to support the product configuration activity. Tests performed on a benchmark of 50 PL models show that the approach is efficient and scales up easily to very large and complex PL specifications.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = apr,
pages = {33–68},
numpages = {36},
keywords = {Computer Science, Constraint-Based Product Lines, Information Systems, Product Line Analysis, Product Line Configuration, Product Line Integration, Product Line Reasoning, Product Line Specification, Product Line Verification}
}

@article{10.1007/s00165-021-00563-2,
author = {Cordy, Maxime and Lazreg, Sami and Papadakis, Mike and Legay, Axel},
title = {Statistical model checking for variability-intensive systems: applications to bug detection and minimization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00563-2},
doi = {10.1007/s00165-021-00563-2},
abstract = {We propose a new Statistical Model Checking (SMC) method to
identify bugs in variability-intensive systems (VIS). The
state-space of such systems is exponential in the number of
variants, which makes the verification problem harder than for
classical systems. To reduce verification time, we propose to
combine SMC with featured transition systems (FTS)—a
model that represents jointly the state spaces of all variants. Our
new methods allow the sampling of executions from one or more
(potentially all) variants. We investigate their utility in two
complementary use cases. The first case considers the problem of
finding all variants that violate a given property expressed in
Linear-Time Logic (LTL) within a given simulation budget. To achieve
this, we perform random walks in the featured transition system
seeking accepting lassos. We show that our method allows us to find
bugs much faster (up to 16 times according to our experiments) than
exhaustive methods. As any simulation-based approach, however, the
risk of Type-1 error exists. We provide a lower bound and an upper
bound for the number of simulations to perform to achieve the
desired level of confidence. Our empirical study involving 59
properties over three case studies reveals that our method manages
to discover all variants violating 41 of the properties.
This indicates that SMC can act as a coarse-grained
analysis method to quickly identify the set of buggy variants.
The second case complements the first one. In case the
coarse-grained analysis reveals that no variant can guarantee to
satisfy an intended property in all their executions, one should
identify the variant that minimizes the probability of violating
this property. Thus, we propose a fine-grained SMC method that
quickly identifies promising variants and accurately estimates their
violation probability. We evaluate different selection strategies
and reveal that a genetic algorithm combined with elitist selection
yields the best results.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {1147–1172},
numpages = {26},
keywords = {Statistical model checking, Variability, Verification, Simulation, Sampling}
}

@inproceedings{10.1145/581339.581415,
author = {Schmid, Klaus},
title = {A comprehensive product line scoping approach and its validation},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581415},
doi = {10.1145/581339.581415},
abstract = {Product Line Engineering is a recent approach to software development that specifically aims at exploiting commonalities and systematic variabilities among functionally overlapping systems in terms of large scale reuse. Taking full advantage of this potential requires adequate planning and management of the reuse approach as otherwise huge economic benefits will be missed due to an inappropriate alignment of the reuse infrastructure.Key in product line planning is the scoping activity, which aims at focussing the reuse investment where it pays. Scoping actually happens on several levels in the process: during the domain analysis step (analysis of product line requirements) a focusing needs to happen just like during the decision of what to implement for reuse. The latter decision has also important ramifications for the development of an appropriate reference architecture as it provides the reusability requirements for this step.In this paper, we describe an integrated approach that has been developed, improved, and validated over the last few years. The approach fully covers the scoping activities of domain scoping and reuse infrastructure scoping and was validated in several industrial case studies.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {593–603},
numpages = {11},
location = {Orlando, Florida},
series = {ICSE '02}
}

@inproceedings{10.1145/3335550.3335580,
author = {Ghrab, Yahya and Sali, Mustapha},
title = {Sales and Operations Planning (S&amp;OP) Performance Under Highly Diversified Mass Production Systems},
year = {2019},
isbn = {9781450362641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335550.3335580},
doi = {10.1145/3335550.3335580},
abstract = {Sales and operations planning (S&amp;OP) is a midterm planning practice that is widely used in industries with relatively volatile demand and limited capacities to align demand and supply. Several research studies, as well as practical guides, have been developed to examine the design and implementation of S&amp;OP processes. Most of these studies are general and have no reference to the industry behind them. However, regarding planning, each industry has its properties. Taking a "one size fits all" approach is not profitable for extreme cases, such as for an automotive industry known for the very large product portfolios. Our contribution intends to examine this issue in detail by performing a literature review on the effectiveness of S&amp;OP and its impacts on supply chain performance. We try to reveal why actual S&amp;OP practices are not fit for highly diversified mass production systems, and we deduce that there is a need to redefine the S&amp;OP process for such industries.},
booktitle = {Proceedings of the 2019 International Conference on Management Science and Industrial Engineering},
pages = {42–47},
numpages = {6},
keywords = {Automotive industry, Diversity management, Planning, Sales and operations planning, Supply chain performance},
location = {Phuket, Thailand},
series = {MSIE '19}
}

@article{10.1016/j.artmed.2016.05.004,
title = {An ensemble method for extracting adverse drug events from social media},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {70},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2016.05.004},
doi = {10.1016/j.artmed.2016.05.004},
abstract = {We propose a relation extraction system to distinguish between adverse drug events (ADEs) and non-ADEs on social media.We develop a feature-based method, investigate the effectiveness of feature selection, and analyze the contributions of different features.We investigate whether kernel-based methods can effectively extract ADEs from social media.We propose several classifier ensembles to further enhance ADE extraction capabilities. ObjectiveBecause adverse drug events (ADEs) are a serious health problem and a leading cause of death, it is of vital importance to identify them correctly and in a timely manner. With the development of Web 2.0, social media has become a large data source for information on ADEs. The objective of this study is to develop a relation extraction system that uses natural language processing techniques to effectively distinguish between ADEs and non-ADEs in informal text on social media. Methods and materialsWe develop a feature-based approach that utilizes various lexical, syntactic, and semantic features. Information-gain-based feature selection is performed to address high-dimensional features. Then, we evaluate the effectiveness of four well-known kernel-based approaches (i.e., subset tree kernel, tree kernel, shortest dependency path kernel, and all-paths graph kernel) and several ensembles that are generated by adopting different combination methods (i.e., majority voting, weighted averaging, and stacked generalization). All of the approaches are tested using three data sets: two health-related discussion forums and one general social networking site (i.e., Twitter). ResultsWhen investigating the contribution of each feature subset, the feature-based approach attains the best area under the receiver operating characteristics curve (AUC) values, which are 78.6%, 72.2%, and 79.2% on the three data sets. When individual methods are used, we attain the best AUC values of 82.1%, 73.2%, and 77.0% using the subset tree kernel, shortest dependency path kernel, and feature-based approach on the three data sets, respectively. When using classifier ensembles, we achieve the best AUC values of 84.5%, 77.3%, and 84.5% on the three data sets, outperforming the baselines. ConclusionsOur experimental results indicate that ADE extraction from social media can benefit from feature selection. With respect to the effectiveness of different feature subsets, lexical features and semantic features can enhance the ADE extraction capability. Kernel-based approaches, which can stay away from the feature sparsity issue, are qualified to address the ADE extraction problem. Combining different individual classifiers using suitable combination methods can further enhance the ADE extraction effectiveness.},
journal = {Artif. Intell. Med.},
month = jun,
pages = {62–76},
numpages = {15}
}

@article{10.1016/j.compind.2006.09.004,
author = {Boucher, Xavier and Bonjour, Eric and Grabot, Bernard},
title = {Formalisation and use of competencies for industrial performance optimisation: A survey},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {2},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2006.09.004},
doi = {10.1016/j.compind.2006.09.004},
abstract = {For many years, industrial performance has been implicitly considered as deriving from the optimisation of technological and material resources (machines, inventories, etc.), made possible by centralized organisations. The topical requirements for reactive and flexible industrial systems have progressively reintroduced the human workforce as the main source of industrial performance. Making this paradigm operational requires the identification and careful formalisation of the link between human resource and industrial performance, through concepts like skills, competencies or know-how. This paper provides a general survey of the formalisation and integration of competence-oriented concepts within enterprise information systems and decision systems, aiming at providing new methods and tools for performance management.},
journal = {Comput. Ind.},
month = feb,
pages = {98–117},
numpages = {20},
keywords = {Competence model, Enterprise modelling, Information and decision systems, Performance}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing feature model by identifying variability-aware modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {configuration, feature model recovery, feature modules, product line, variability-aware modularity},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@article{10.1007/s10664-017-9500-x,
author = {Zhang, Huihui and Wang, Shuai and Yue, Tao and Ali, Shaukat and Liu, Chao},
title = {Search and similarity based selection of use case scenarios: An empirical study},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9500-x},
doi = {10.1007/s10664-017-9500-x},
abstract = {Use case modeling is a well-known requirements specification method and has been widely applied in practice. Use case scenarios of use case models are input elements for requirements inspection and analysis, requirements-based testing, and other downstream activities. It is, however, a practical challenge to inspect all use case scenarios that can be obtained from any non-trivial use case model, as such an inspection activity is often performed manually by domain experts. Therefore, it is needed to propose an automated solution for selecting a subset of use case scenarios with the ultimate aim of enabling cost-effective requirements (use case) inspection, analysis, and other relevant activities. Our solution is built on a natural language based, restricted use case modeling methodology (named as RUCM), in the sense that requirements specifications are specified as RUCM use case models. Use case scenarios can be automatically derived from RUCM use case models with the already established Zen-RUCM framework. In this paper, we propose a search-based and similarity-based approach called S3RCUM, through an empirical study, to select most diverse use case scenarios to enable cost-effective use case inspections. The empirical study was designed to evaluate the performance of three search algorithms together with eight similarity functions, through one real-world case study and six case studies from literature. Results show that (1+1) Evolutionary Algorithm together with Needleman-Wunsch similarity function significantly outperformed the other 31 combinations of the search algorithms and similarity functions. The combination managed to select 50% of all the generated RUCM use case scenarios for all the case studies to detect all the seeded defects.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {87–164},
numpages = {78},
keywords = {Empirical study, Search algorithms, Similarity functions, Use case modeling, Use case scenario selection}
}

@inproceedings{10.1145/1944892.1944908,
author = {K\"{a}stner, Christian and Giarrusso, Paolo G. and Ostermann, Klaus},
title = {Partial preprocessing C code for variability analysis},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944908},
doi = {10.1145/1944892.1944908},
abstract = {The C preprocessor is commonly used to implement variability. Given a feature selection, code fragments can be excluded from compilation with #ifdef and similar directives. However, the token-based nature of the C preprocessor makes variability implementation difficult and error-prone. Additionally, variability mechanisms are intertwined with macro definitions, macro expansion, and file inclusion. To determine whether a code fragment is compiled, the entire file must be preprocessed. We present a partial preprocessor that preprocesses file inclusion and macro expansion, but retains variability information for further analysis. We describe the mechanisms of the partial preprocessor, provide a full implementation, and present some initial experimental results. The partial preprocessor is part of a larger endeavor in the TypeChef project to check variability implementations (syntactic correctness, type correctness) in C projects such as the Linux kernel.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {127–136},
numpages = {10},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1016/j.jss.2016.05.024,
author = {S\'{a}nchez Guinea, Alejandro and Nain, Gr\'{e}gory and Le Traon, Yves},
title = {A systematic review on the engineering of software for ubiquitous systems},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.05.024},
doi = {10.1016/j.jss.2016.05.024},
abstract = {A systematic literature review on engineering software for ubiquitous systems.We identified 132 approaches addressing issues on different phases of the software engineering cycle for ubiquitous systems.Implementation, evolution/maintenance, and feedback phases have been the most studied.The testing phase needs to receive more attention, especially in what respect to simulations. Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it.Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems.Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations.Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers.Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback.},
journal = {J. Syst. Softw.},
month = aug,
pages = {251–276},
numpages = {26},
keywords = {Development methods, Empirical-software engineering, Evidence-based software engineering, Pervasive systems, Research synthesis, Software development cycle, Systematic review, Ubiquitous systems}
}

@inproceedings{10.1007/978-3-642-29645-1_22,
author = {Mussbacher, Gunter and Al Abed, Wisam and Alam, Omar and Ali, Shaukat and Beugnard, Antoine and Bonnet, Valentin and Br\ae{}k, Rolv and Capozucca, Alfredo and Cheng, Betty H. C. and Fatima, Urooj and France, Robert and Georg, Geri and Guelfi, Nicolas and Istoan, Paul and J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Klein, Jacques and L\'{e}zoray, Jean-Baptiste and Malakuti, Somayeh and Moreira, Ana and Phung-Khac, An and Troup, Lucy},
title = {Comparing six modeling approaches},
year = {2011},
isbn = {9783642296444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29645-1_22},
doi = {10.1007/978-3-642-29645-1_22},
abstract = {While there are many aspect-oriented modeling (AOM) approaches, from requirements to low-level design, it is still difficult to compare them and know under which conditions different approaches are most applicable. This comparison, however, is crucially important to unify existing AOM and more traditional object-oriented modeling (OOM) approaches and to generalize individual approaches into a comprehensive end-to-end method. Such a method does not yet exist. This paper reports on work done at the inaugural Comparing Modeling Approaches (CMA) workshop towards the goal of identifying potential comprehensive methodologies: (i) a common, focused case study for six modeling approaches, (ii) a set of criteria applied to each of the six approaches, and (iii) the assessment results.},
booktitle = {Proceedings of the 2011th International Conference on Models in Software Engineering},
pages = {217–243},
numpages = {27},
keywords = {aspect-oriented modeling, case study, comparison criteria, object-oriented modeling},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.1145/2393216.2393284,
author = {Mannava, Vishnuvardhan and Ramesh, T.},
title = {A Service Configuration and Composition Design Pattern for autonomic computing systems using Service Oriented Architecture},
year = {2012},
isbn = {9781450313100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393216.2393284},
doi = {10.1145/2393216.2393284},
abstract = {Software methodologies to develop autonomic computing systems are not mature. As far as we know, there are no studies on composition of design patterns and pattern languages for autonomic computing domain. Static approach to configure services yields inflexible, often inefficient, applications and software architectures. Complex service request involving multiple tasks (operations) may fail due to unavailability of suitable Web services advertised in the registry. In this paper, we propose software architecture using our Service Configuration and Composition Design Patterns for dynamically configuring and composition of communication services by satisfying the Self-configuration and Self-composition characteristics of the autonomic computing systems where software designers and/or programmers can exploit to drive their work. In the proposed system the server will invoke, compose, configure, amend and manage the services based on user request using composition of Design Patterns and Service Oriented Architecture (SOA) in the distributed environment at runtime. The proposed pattern is described using a java-like notation for the classes and interfaces. A simple UML class and Sequence diagrams are depicted.},
booktitle = {Proceedings of the Second International Conference on Computational Science, Engineering and Information Technology},
pages = {401–407},
numpages = {7},
keywords = {Design Patterns, Service Oriented Architecture (SOA), autonomic computing system, database access patterns, dynamic services, web service composition},
location = {Coimbatore UNK, India},
series = {CCSEIT '12}
}

@article{10.1016/j.rcim.2016.03.002,
author = {Valente, A.},
title = {Reconfigurable industrial robots},
year = {2016},
issue_date = {October 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.03.002},
doi = {10.1016/j.rcim.2016.03.002},
abstract = {Industrial robots undergo design and re-configuration processes to target extremely challenging precision and reliability performance with agile and efficient architectures. The need for such features currently prevent the exploitation of reconfigurable robotics in manufacturing. The current work presents an approach to design and configure reconfigurable robots for the high precision manufacturing industry. The work proposes a configuration algorithm that enables the identification of the robot architectures and the related reconfigurability features by selecting the type and number of robot modules to be implemented over time in order to better accomplish a number of production requirements. Particularly, assuming the robot will work by utilising a finite set of robotic modules, the algorithm determines the set of modules to form the arm and the ones to be allocated in the robot storage for possible usage over time. Results show a number of benefits such as a robotic chain with customised reaching and degrees of freedom with a reduced cost by performing an accurate module selection and configuration; this should lead the robot users to prefer reconfigurable robots to commercial rigid catalogue solutions proposed by robot manufacturers. An approach to build and configure robotic chains for Reconfigurable Industrial Robots.The Problem Formulation is based on Stochastic Programming Technique to match future evolutions of production requirements.A real industrial case studies proofs the actual benefits coming from reconfigurable robotics in manufacturing.},
journal = {Robot. Comput.-Integr. Manuf.},
month = oct,
pages = {115–126},
numpages = {12},
keywords = {Reconfigurable Industrial Robotics, Stochastic programming}
}

@article{10.1145/2846096,
author = {Wang, Chao and Dong, Chuansheng and Zeng, Haibo and Gu, Zonghua},
title = {Minimizing Stack Memory for Hard Real-Time Applications on Multicore Platforms with Partitioned Fixed-Priority or EDF Scheduling},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/2846096},
doi = {10.1145/2846096},
abstract = {Multicore processors are increasingly adopted in resource-constrained real-time embedded applications. In the development of such applications, efficient use of RAM memory is as important as the effective scheduling of software tasks. Preemption Threshold Scheduling (PTS) is a well-known technique for controlling the degree of preemption, possibly improving system schedulability, and to reduce system stack usage. In this paper, we consider partitioned multi-processor scheduling on a multicore processor with either Fixed-Priority or Earliest Deadline First scheduling algorithms with PTS and address the design optimization problem of mapping tasks to processor cores and assignment of task priorities and preemption thresholds with the optimization objective of minimizing system stack usage. We present both optimal solution techniques based on Mixed Integer Linear Programming and efficient heuristic algorithms that can achieve high-quality results. We perform extensive performance evaluations using both synthetic tasksets and industrial case studies.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
articleno = {46},
numpages = {25},
keywords = {Multicore, partitioned scheduling, real-time scheduling}
}

@inbook{10.5555/2167810.2167829,
author = {Purhonen, Anu},
title = {Performance evaluation approaches for software architects},
year = {2005},
isbn = {3540306447},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Performance analysis techniques have already been developed for decades. As software architecture research has matured, performance analysis techniques have also been adapted to the evaluation of software architectures. However, the performance evaluation of software architectures is not yet systematically used in the industry. One of the reasons may be that it is difficult to select what method to use. The contribution of this work is to define a comparison framework for performance evaluation approaches. In addition, the framework is applied in comparing existing performance evaluation approaches. The framework can be used to select methods for evaluating architectures, to increase understanding of the methods, and to point out needs for future work.},
booktitle = {Component-Based Software Development for Embedded Systems: An Overview of Current Research Trends},
pages = {275–295},
numpages = {21}
}

@inproceedings{10.1145/2528265.2528267,
author = {Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Garvin, Brady},
title = {Exploring feature interactions in the wild: the new feature-interaction challenge},
year = {2013},
isbn = {9781450321686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2528265.2528267},
doi = {10.1145/2528265.2528267},
abstract = {The feature-interaction problem has been keeping researchers and practitioners in suspense for years. Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems. In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility. Understanding this nature will have significant implications on research in this area, for example, on the efficiency of interaction-detection or performance-prediction techniques. A set of preliminary results as well as a discussion of possible experimental setups and corresponding challenges give us confidence that this endeavor is within reach but requires a collaborative effort of the community.},
booktitle = {Proceedings of the 5th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {feature interactions, feature modularity, feature-interaction problem, feature-oriented software development},
location = {Indianapolis, Indiana, USA},
series = {FOSD '13}
}

@article{10.1016/j.infsof.2015.08.007,
author = {Sep\'{u}lveda, Samuel and Cravero, Ania and Cachero, Cristina},
title = {Requirements modeling languages for software product lines},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.08.007},
doi = {10.1016/j.infsof.2015.08.007},
abstract = {There is a concern for generating proposals with higher levels of expressiveness.There is not a strong relationship between the proposals and SPL development process.There is a need for better ways to validate the modeling proposals.The proposals have a low level of empirical validation and adoption in industry.The level of maturity, expressive power and tool support of the proposals is low. Display Omitted Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs.Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption.Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013.Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46% of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6% of the languages have been empirically validated, 41% report some kind of industry adoption and 71% of the languages are independent from any development process. Last but not least, 57% of the languages have been proposed by the academia, while 43% have been the result of a joint effort between academia and industry.Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–36},
numpages = {21},
keywords = {Modeling languages, Requirements engineering, Software product lines, Systematic literature review}
}

@inproceedings{10.1007/11531142_8,
author = {Lopez-Herrejon, Roberto E. and Batory, Don and Cook, William},
title = {Evaluating support for features in advanced modularization technologies},
year = {2005},
isbn = {354027992X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11531142_8},
doi = {10.1007/11531142_8},
abstract = {A software product-line is a family of related programs. Each program is defined by a unique combination of features, where a feature is an increment in program functionality. Modularizing features is difficult, as feature-specific code often cuts across class boundaries. New modularization technologies have been proposed in recent years, but their support for feature modules has not been thoroughly examined. In this paper, we propose a variant of the expression problem as a canonical problem in product-line design. The problem reveals a set of technology-independent properties that feature modules should exhibit. We use these properties to evaluate five technologies: AspectJ, Hyper/J, Jiazzi, Scala, and AHEAD. The results suggest an abstract model of feature composition that is technology-independent and that relates compositional reasoning with algebraic reasoning.},
booktitle = {Proceedings of the 19th European Conference on Object-Oriented Programming},
pages = {169–194},
numpages = {26},
location = {Glasgow, UK},
series = {ECOOP'05}
}

@article{10.1016/j.future.2016.01.003,
author = {Sharma, Sugam},
title = {Expanded cloud plumes hiding Big Data ecosystem},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2016.01.003},
doi = {10.1016/j.future.2016.01.003},
abstract = {Today, a paradigm shift is being observed in science, where the focus is gradually shifting away from operation to data, which is greatly influencing the decision making also. The data is being inundated proactively from several sources in various forms; especially social media and in modern data science vocabulary is being recognized as Big Data. Today, Big Data is permeating through the bigger aspect of human life for scientific and commercial dependencies, especially for massive scale data analytics of beyond the exabyte magnitude. As the footprint of Big Data applications is continuously expanding, the reliability on cloud environments is also increasing to obtain appropriate, robust and affordable services to deal with Big Data challenges. Cloud computing avoids any need to locally maintain the overly scaled computing infrastructure that include not only dedicated space, but the expensive hardware and software also. Several data models to process Big Data are already developed and a number of such models are still emerging, potentially relying on heterogeneous underlying storage technologies, including cloud computing. In this paper, we investigate the growing role of cloud computing in Big Data ecosystem. Also, we propose a novel XCLOUDX {XCloudX, X X}classification to zoom in to gauge the intuitiveness of the scientific name of the cloud-assisted NoSQL Big Data models and analyze whether XCloudX always uses cloud computing underneath or vice versa. XCloudX symbolizes those NoSQL Big Data models that embody the term "cloud" in their name, where X is any alphanumeric variable. The discussion is strengthen by a set of important case studies. Furthermore, we study the emergence of as-a-Service era, motivated by cloud computing drive and explore the new members beyond traditional cloud computing stack, developed in the past couple of years. In sum, growing role of cloud in Big Data ecosystem is extensively investigated.A novel XCLOUDX classification is proposed for cloud-assisted Big Data models.A set of important case studies on cloud Big Data models is discussed.The as-a-Service modality of cloud computing is extensively discussed.IoT, Smart Data, and Smart Data Lakes are briefly discussed as future directions.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {63–92},
numpages = {30},
keywords = {Big Data, Cloud, IoT, Smart Data and Lakes, XCLOUDX, as-a-Service}
}

@article{10.1007/s10270-016-0575-4,
author = {Voelter, Markus and Kolb, Bernd and Szab\'{o}, Tam\'{a}s and Ratiu, Daniel and Deursen, Arie},
title = {Lessons learned from developing mbeddr: a case study in language engineering with MPS},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0575-4},
doi = {10.1007/s10270-016-0575-4},
abstract = {Language workbenches are touted as a promising technology to engineer languages for use in a wide range of domains, from programming to science to business. However, not many real-world case studies exist that evaluate the suitability of language workbench technology for this task. This paper contains such a case study. In particular, we evaluate the development of mbeddr, a collection of integrated languages and language extensions built with the Jetbrains MPS language workbench. mbeddr consists of 81 languages, with their IDE support, 34 of them C extensions. The mbeddr languages use a wide variety of notations--textual, tabular, symbolic and graphical--and the C extensions are modular; new extensions can be added without changing the existing implementation of C. mbeddr's development has spanned 10 person-years so far, and the tool is used in practice and continues to be developed. This makes mbeddr a meaningful case study of non-trivial size and complexity. The evaluation is centered around five research questions: language modularity, notational freedom and projectional editing, mechanisms for managing complexity, performance and scalability issues and the consequences for the development process. We draw generally positive conclusions; language engineering with MPS is ready for real-world use. However, we also identify a number of areas for improvement in the state of the art in language engineering in general, and in MPS in particular.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {585–630},
numpages = {46},
keywords = {Case study, Domain-specific language, Experimentation, Language engineering, Language extension, Language workbenches, Languages}
}

@article{10.1145/3280988,
author = {Razzaq, Abdul and Wasala, Asanka and Exton, Chris and Buckley, Jim},
title = {The State of Empirical Evaluation in Static Feature Location},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3280988},
doi = {10.1145/3280988},
abstract = {Feature location (FL) is the task of finding the source code that implements a specific, user-observable functionality in a software system. It plays a key role in many software maintenance tasks and a wide variety of Feature Location Techniques (FLTs), which rely on source code structure or textual analysis, have been proposed by researchers. As FLTs evolve and more novel FLTs are introduced, it is important to perform comparison studies to investigate “Which are the best FLTs?” However, an initial reading of the literature suggests that performing such comparisons would be an arduous process, based on the large number of techniques to be compared, the heterogeneous nature of the empirical designs, and the lack of transparency in the literature. This article presents a systematic review of 170 FLT articles, published between the years 2000 and 2015. Results of the systematic review indicate that 95% of the articles studied are directed towards novelty, in that they propose a novel FLT. Sixty-nine percent of these novel FLTs are evaluated through standard empirical methods but, of those, only 9% use baseline technique(s) in their evaluations to allow cross comparison with other techniques. The heterogeneity of empirical evaluation is also clearly apparent: altogether, over 60 different FLT evaluation metrics are used across the 170 articles, 272 subject systems have been used, and 235 different benchmarks employed. The review also identifies numerous user input formats as contributing to the heterogeneity. Analysis of the existing research also suggests that only 27% of the FLTs presented might be reproduced from the published material. These findings suggest that comparison across the existing body of FLT evaluations is very difficult. We conclude by providing guidelines for empirical evaluation of FLTs that may ultimately help to standardise empirical research in the field, cognisant of FLTs with different goals, leveraging common practices in existing empirical evaluations and allied with rationalisations. This is seen as a step towards standardising evaluation in the field, thus facilitating comparison across FLTs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {2},
numpages = {58},
keywords = {Feature location, bug location, concept location, requirement traceability}
}

@inproceedings{10.1007/978-3-642-28872-2_19,
author = {Shi, Jiangfan and Cohen, Myra B. and Dwyer, Matthew B.},
title = {Integration testing of software product lines using compositional symbolic execution},
year = {2012},
isbn = {9783642288715},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-28872-2_19},
doi = {10.1007/978-3-642-28872-2_19},
abstract = {Software product lines are families of products defined by feature commonality and variability, with a well-managed asset base. Recent work in testing of software product lines has exploited similarities across development phases to reuse shared assets and reduce test effort. The use of feature dependence graphs has also been employed to reduce testing effort, but little work has focused on code level analysis of dataflow between features. In this paper we present a compositional symbolic execution technique that works in concert with a feature dependence graph to extract the set of possible interaction trees in a product family. It composes these to incrementally and symbolically analyze feature interactions. We experiment with two product lines and determine that our technique can reduce the overall number of interactions that must be considered during testing, and requires less time to run than a traditional symbolic execution technique.},
booktitle = {Proceedings of the 15th International Conference on Fundamental Approaches to Software Engineering},
pages = {270–284},
numpages = {15},
location = {Tallinn, Estonia},
series = {FASE'12}
}

@article{10.5555/2010403.2010409,
author = {Campagna, Dario and De Rosa, Christian and Dovier, Agostino and Montanari, Angelo and Piazza, Carla},
title = {Morphos Configuration Engine: the Core of a Commercial Configuration System in CLP(FD)},
year = {2010},
issue_date = {January 2010},
publisher = {IOS Press},
address = {NLD},
volume = {105},
number = {1–2},
issn = {0169-2968},
abstract = {Product configuration systems are an emerging software technology that supports companies in deploying mass customization strategies. In this paper, we describe a CLP-based reasoning engine that we developed for a commercial configuration system. We first illustrate the advantages of the CLP approach to product configuration over other ones. Then, we describe the actual encoding of the considered product configuration problem as a constraint satisfaction problem. We devote a special attention to the key issues of constraint propagation and optimization as well as to the relevant process of assignment revision. A comparison with existing systems for product configuration concludes the paper.},
journal = {Fundam. Inf.},
month = jan,
pages = {105–133},
numpages = {29}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@article{10.1145/3278120,
author = {Smirnov, Fedor and Reimann, Felix and Teich, J\"{u}rgen and Gla\ss{}, Michael},
title = {Automatic Optimization of the VLAN Partitioning in Automotive Communication Networks},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3278120},
doi = {10.1145/3278120},
abstract = {Dividing the communication network into so-called Virtual Local Area Networks (VLANs), i.e., subnetworks that are isolated at the data link layer (OSI layer 2), is a promising approach to address the increasing security challenges in automotive networks. The automation of the VLAN partitioning is a well-researched problem in the domain of local or metropolitan area networks. However, the approaches used there are hardly applicable for the design of automotive networks as they mainly focus on reducing the amount of broadcast traffic and cannot capture the many design objectives of automotive networks like the message timing or the link load, which are affected by the VLAN partitioning. As a remedy, this article proposes an approach based on a set of Pseudo-Boolean constraints to generate a message routing which is feasible with respect to the VLAN-related routing restrictions in automotive networks. This approach can be used for a design space exploration to optimize not only the VLAN partitioning but also other routing-related objectives. We demonstrate both the efficiency of our message routing approach and the now accessible optimization potential for the complete Electric/Electronic architecture with a mixed-criticality system from the automotive domain. There we thoroughly investigate the impact of the VLAN partitioning on the message timing and the link loads by optimizing these design objectives concurrently. During the exploration of the huge design space, where each resource can be assigned to one of four VLANs, our approach requires less than 40ms for the creation of a valid solution and ensures that all messages satisfy their deadlines and link load bounds.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {9},
numpages = {23},
keywords = {Packet-switching networks, automotive ethernet, design automation, virtual local area networks}
}

@article{10.1007/s10515-017-0215-4,
author = {Boussa\"{\i}d, Ilhem and Siarry, Patrick and Ahmed-Nacer, Mohamed},
title = {A survey on search-based model-driven engineering},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0215-4},
doi = {10.1007/s10515-017-0215-4},
abstract = {Model-driven engineering (MDE) and search-based software engineering (SBSE) are both relevant approaches to software engineering. MDE aims to raise the level of abstraction in order to cope with the complexity of software systems, while SBSE involves the application of metaheuristic search techniques to complex software engineering problems, reformulating engineering tasks as optimization problems. The purpose of this paper is to survey the relatively recent research activity lying at the interface between these two fields, an area that has come to be known as search-based model-driven engineering. We begin with an introduction to MDE, the concepts of models, of metamodels and of model transformations. We also give a brief introduction to SBSE and metaheuristics. Then, we survey the current research work centered around the combination of search-based techniques and MDE. The literature survey is accompanied by the presentation of references for further details.},
journal = {Automated Software Engg.},
month = jun,
pages = {233–294},
numpages = {62},
keywords = {Metaheuristic, Metaheuristics, Model-driven engineering (MDE), Search-based software engineering (SBSE)}
}

@article{10.1016/j.jss.2019.03.064,
author = {Pradhan, Dipesh and Wang, Shuai and Ali, Shaukat and Yue, Tao and Liaaen, Marius},
title = {Employing rule mining and multi-objective search for dynamic test case prioritization},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {153},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.064},
doi = {10.1016/j.jss.2019.03.064},
journal = {J. Syst. Softw.},
month = jul,
pages = {86–104},
numpages = {19},
keywords = {Multi-objective optimization, Rule mining, Dynamic test case prioritization, Search, Black-box regression testing}
}

@inproceedings{10.1145/1966445.1966451,
author = {Tartler, Reinhard and Lohmann, Daniel and Sincero, Julio and Schr\"{o}der-Preikschat, Wolfgang},
title = {Feature consistency in compile-time-configurable system software: facing the linux 10,000 feature problem},
year = {2011},
isbn = {9781450306348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1966445.1966451},
doi = {10.1145/1966445.1966451},
abstract = {Much system software can be configured at compile time to tailor it with respect to a broad range of supported hardware architectures and application domains. A good example is the Linux kernel, which provides more than 10,000 configurable features, growing rapidly.From the maintenance point of view, compile-time configurability imposes big challenges. The configuration model (the selectable features and their constraints as presented to the user) and the configurability that is actually implemented in the code have to be kept in sync, which, if performed manually, is a tedious and error-prone task. In the case of Linux, this has led to numerous defects in the source code, many of which are actual bugs.We suggest an approach to automatically check for configurability-related implementation defects in large-scale configurable system software. The configurability is extracted from its various implementation sources and examined for inconsistencies, which manifest in seemingly conditional code that is in fact unconditional. We evaluate our approach with the latest version of Linux, for which our tool detects 1,776 configurability defects, which manifest as dead/superfluous source code and bugs. Our findings have led to numerous source-code improvements and bug fixes in Linux: 123 patches (49 merged) fix 364 defects, 147 of which have been confirmed by the corresponding Linux developers and 20 as fixing a new bug.},
booktitle = {Proceedings of the Sixth Conference on Computer Systems},
pages = {47–60},
numpages = {14},
keywords = {configurability, linux, maintenance, static analysis, vamos},
location = {Salzburg, Austria},
series = {EuroSys '11}
}

@article{10.1155/2009/179680,
author = {D\"{u}rr, Peter and Karlen, Walter and Guignard, J\'{e}r\'{e}mie and Mattiussi, Claudio and Floreano, Dario},
title = {Evolutionary selection of features for neural sleep/wake discrimination},
year = {2009},
issue_date = {January 2009},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2009},
number = {S1},
issn = {1687-6229},
url = {https://doi.org/10.1155/2009/179680},
doi = {10.1155/2009/179680},
abstract = {In biomedical signal analysis, artificial neural networks are often used for pattern classification because of their capability for nonlinear class separation and the possibility to efficiently implement them on a microcontroller. Typically, the network topology is designed by hand, and a gradient-based search algorithm is used to find a set of suitable parameters for the given classification task. In many cases, however, the choice of the network architecture is a critical and difficult task. For example, hand-designed networks often require more computational resources than necessary because they rely on input features that provide no information or are redundant. In the case of mobile applications, where computational resources and energy are limited, this is especially detrimental. Neuroevolutionary methods which allow for the automatic synthesis of network topology and parameters offer a solution to these problems. In this paper, we use analog genetic encoding (AGE) for the evolutionary synthesis of a neural classifier for a mobile sleep/wake discrimination system. The comparison with a hand-designed classifier trained with back propagation shows that the evolved neural classifiers display similar performance to the hand-designed networks, but using a greatly reduced set of inputs, thus reducing computation time and improving the energy efficiency of the mobile system.},
journal = {J. Artif. Evol. App.},
month = jan,
articleno = {1},
numpages = {9}
}

@article{10.1016/j.engappai.2009.03.001,
author = {Borangiu, Theodor and Gilbert, Pascal and Ivanescu, Nick-Andrei and Rosu, Andrei},
title = {An implementing framework for holonic manufacturing control with multiple robot-vision stations},
year = {2009},
issue_date = {June, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {4–5},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.03.001},
doi = {10.1016/j.engappai.2009.03.001},
abstract = {The paper describes a holonic control architecture and implementing issues for agile job shop assembly with networked intelligent robots, based on the dynamic simulation of material processing and transportation. The holarchy was defined considering the PROSA reference architecture relative to which in-line vision-based quality control was added by help of feature-based descriptions of the material flow. Two solutions for production planning are proposed: a knowledge-based algorithm using production rules, and an OO resolved scheduling rate planner (RSRP) based on variable-timing simulation. Failure- and recovery-management are developed as generic scenarios embedding the CNP mechanism into production self-rescheduling. Aggregate Order Holon execution is realized by OPC-based PLC software integration and event-driven product transportation. The holonic control of multiple networked robot-vision stations also features tolerance to station computer- (IBM PC-type), station controller- (robot controller), quality control- (machine vision) and communication- (LAN) failure. Fault tolerance and high availability at shop-floor level are provided due to the multiple physical communication capabilities of the robot controllers, to their multiple-axis multitasking operating capability, and to hardware redundancy of single points of failure (SPOF). Implementing solutions and experiments are reported for a 6-station robot-vision assembly cell with twin-track closed-loop pallet transportation system and product-racking RD/WR devices. Future developments will consider manufacturing integration at enterprise level.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {505–521},
numpages = {17},
keywords = {Applied AI, Holonic manufacturing, Real-time vision, Robotics, Semi-heterarchical control}
}

@inproceedings{10.1109/SEAMS.2019.00018,
author = {Bennaceur, Amel and Ghezzi, Carlo and Tei, Kenji and Kehrer, Timo and Weyns, Danny and Calinescu, Radu and Dustdar, Schahram and Hu, Zhenjiang and Honiden, Shinichi and Ishikawa, Fuyuki and Jin, Zhi and Kramer, Jeffrey and Litoiu, Marin and Loreti, Michele and Moreno, Gabriel A. and M\"{u}ller, Hausi A. and Nenzi, Laura and Nuseibeh, Bashar and Pasquale, Liliana and Reisig, Wolfgang and Schmidt, Heinz and Tsigkanos, Christos and Zhao, Haiyan},
title = {Modelling and analysing resilient cyber-physical systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00018},
doi = {10.1109/SEAMS.2019.00018},
abstract = {From smart buildings to medical devices to smart nations, software systems increasingly integrate computation, networking, and interaction with the physical environment. These systems are known as Cyber-Physical Systems (CPS). While these systems open new opportunities to deliver improved quality of life for people and reinvigorate computing, their engineering is a difficult problem given the level of heterogeneity and dynamism they exhibit. While progress has been made, we argue that complexity is now at a level such that existing approaches need a major re-think to define principles and associated techniques for CPS. In this paper, we identify research challenges when modelling, analysing and engineering CPS. We focus on three key topics: theoretical foundations of CPS, self-adaptation methods for CPS, and exemplars of CPS serving as a research vehicle shared by a larger community. For each topic, we present an overview and suggest future research directions, thereby focusing on selected challenges. This paper is one of the results of the Shonan Seminar 118 on Modelling and Analysing Resilient Cyber-Physical Systems, which took place in December 2018.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {70–76},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@article{10.1016/j.asoc.2016.08.030,
author = {Saeed, Aneesa and Ab Hamid, Siti Hafizah and Mustafa, Mumtaz Begum},
title = {The experimental applications of search-based techniques for model-based testing},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.030},
doi = {10.1016/j.asoc.2016.08.030},
abstract = {Graphical abstractDisplay Omitted HighlightsA systematic review of applications of search-based techniques for model-based testing is provided.Four taxonomies are proposed to classify the applications based on the purpose, problems, solutions and evaluations.The applications are analyzed based on the proposed taxonomies.The development of search-based techniques for model-based testing is discussed.Limitations and potential research directions are summarized. ContextModel-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. ObjectiveThe objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research. MethodWe conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. ResultsThe results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT. ConclusionThis extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1094–1117},
numpages = {24},
keywords = {Model-based testing, Search-based techniques, Software testing, Systematic literature review, Taxonomy, Test case generation}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {machine learning, uncertainty},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@article{10.1016/j.datak.2021.101929,
author = {Ali, Mughees and Khan, Saif Ur Rehman and Hussain, Shahid},
title = {Self-adaptation in smartphone applications: Current state-of-the-art techniques, challenges, and future directions},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {136},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101929},
doi = {10.1016/j.datak.2021.101929},
journal = {Data Knowl. Eng.},
month = nov,
numpages = {19},
keywords = {Self-adaptation, Mobile applications, Smartphone applications, Self-adaptive mobile apps}
}

@article{10.1145/3084225,
author = {Storer, Tim},
title = {Bridging the Chasm: A Survey of Software Engineering Practice in Scientific Programming},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3084225},
doi = {10.1145/3084225},
abstract = {The use of software is pervasive in all fields of science. Associated software development efforts may be very large, long lived, and complex, requiring the commitment of significant resources. However, several authors have argued that the “gap” or “chasm” between software engineering and scientific programming is a serious risk to the production of reliable scientific results, as demonstrated in a number of case studies. This article reviews the research that addresses the gap, exploring how both software engineering and research practice may need to evolve to accommodate the use of software in science.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {47},
numpages = {32},
keywords = {Software engineering, scientific programming}
}

@article{10.1016/j.ins.2012.09.023,
author = {Yan, Hong-Sen and Wang, Hao-Xiang and Zhang, Xiao-Dong},
title = {Simultaneous batch splitting and scheduling on identical parallel production lines},
year = {2013},
issue_date = {February, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {221},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2012.09.023},
doi = {10.1016/j.ins.2012.09.023},
abstract = {This paper explores the simultaneous batch splitting and scheduling problem for identical parallel production lines, which is different from the parallel machine scheduling in that the processing time of a batch on parallel production lines is not fully proportional to that of a product. To solve the problem, a formula is derived to determine the processing time of a batch on a production line which is not taken as a single machine for each item in the batch. A lower bound is provided for the starting time of a batch production, which overlaps the previous one. A sufficient condition for guaranteeing the constant availability of the end buffer is given and proven mathematically with respect to a continuously delivered batch. Based on the above, an algorithm whose complexity is unrelated to the batch size is proposed to obtain the starting time of a batch production. Finally, a heuristic method based on genetic algorithm is constructed to solve the splitting and scheduling problems simultaneously. Numerical experiments show that this method functions well to balance the loads of the production lines. In fact, the proposed approach is significantly more effective than the existing methods for production planning and scheduling on production lines (each of which is considered as a single machine for both batch and item).},
journal = {Inf. Sci.},
month = feb,
pages = {501–519},
numpages = {19},
keywords = {Batch splitting, Delivered, Genetic algorithm, Parallel production lines, Scheduling}
}

@article{10.1016/j.cie.2014.09.024,
author = {Ba\c{s}ak, \"{O}zkan and Albayrak, Y. Esra},
title = {Petri net based decision system modeling in real-time scheduling and control of flexible automotive manufacturing systems},
year = {2015},
issue_date = {August 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2014.09.024},
doi = {10.1016/j.cie.2014.09.024},
abstract = {High level Petri nets and object-oriented design approaches have been presented.PN based method is adapted to the flexible automotive manufacturing system (FAMS).PN based method is illustrated by modeling a real-time scheduling and control for FAMS.The method formulates the dynamic behavior of the system.Artifex PN software tool is employed to model the system. This paper presents the design and the implementation of a Petri net (PN) model for the control of a flexible manufacturing system (FMS). A flexible automotive manufacturing system used in this environment enables quick cell configuration, and the efficient operation of cells. In this paper, we attempt to propose a flexible automotive manufacturing approach for modeling and analysis of shop floor scheduling problem of FMSs using high-level PNs. Since PNs have emerged as the principal performance modeling tools for FMS, this paper provides an object-oriented Petri nets (OOPNs) approach to performance modeling and to implement efficient production control. In this study, we modeled the system as a timed marked graph (TMG), a well-known subclass of PNs, and we showed that the problem of performance evaluation can be reduced to a simple linear programming (LP) problem with m-n+1 variables and n constraints, where m and n represent the number of places and transitions in the marked graph, respectively. The presented PN based method is illustrated by modeling a real-time scheduling and control for flexible automotive manufacturing system (FAMS) in Valeo Turkey.},
journal = {Comput. Ind. Eng.},
month = aug,
pages = {116–126},
numpages = {11},
keywords = {Decision system modeling, Flexible automotive manufacturing systems, Linear programming (LP), Petri net, Time scheduling, Timed marked graph (TMG)}
}

@book{10.5555/2785643,
author = {Kaeslin, Hubert},
title = {Top-Down Digital VLSI Design: From Architectures to Gate-Level Circuits and FPGAs},
year = {2014},
isbn = {0128007303},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Top-Down VLSI Design: From Architectures to Gate-Level Circuits and FPGAs represents a unique approach to learning digital design. Developed from more than 20 years teaching circuit design, Doctor Kaeslins approach follows the natural VLSI design flow and makes circuit design accessible for professionals with a background in systems engineering or digital signal processing. It begins with hardware architecture and promotes a system-level view, first considering the type of intended application and letting that guide your design choices. Doctor Kaeslin presents modern considerations for handling circuit complexity, throughput, and energy efficiency while preserving functionality. The book focuses on application-specific integrated circuits (ASICs), which along with FPGAs are increasingly used to develop products with applications in telecommunications, IT security, biomedical, automotive, and computer vision industries. Topics include field-programmable logic, algorithms, verification, modeling hardware, synchronous clocking, and more. Demonstrates a top-down approach to digital VLSI design. Provides a systematic overview of architecture optimization techniques. Features a chapter on field-programmable logic devices, their technologies and architectures. Includes checklists, hints, and warnings for various design situations. Emphasizes design flows that do not overlook important action items and which include alternative options when planning the development of microelectronic circuits.}
}

@article{10.1145/2993231.2993234,
author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
title = {A retrospective analysis of SAC requirements: engineering track},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2993231.2993234},
doi = {10.1145/2993231.2993234},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = aug,
pages = {26–41},
numpages = {16},
keywords = {SAC, relevance, requirements engineering, retrospective, scoping study, symposium on applied computing, systematic mapping study, trends}
}

@inproceedings{10.1145/3377930.3389810,
author = {Arrieta, Aitor and Agirre, Joseba Andoni and Sagardui, Goiuria},
title = {Seeding strategies for multi-objective test case selection: an application on simulation-based testing},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389810},
doi = {10.1145/3377930.3389810},
abstract = {The time it takes software systems to be tested is usually long. This is often caused by the time it takes the entire test suite to be executed. To optimize this, regression test selection approaches have allowed for improvements to the cost-effectiveness of verification and validation activities in the software industry. In this area, multi-objective algorithms have played a key role in selecting the appropriate subset of test cases from the entire test suite. In this paper, we propose a set of seeding strategies for the test case selection problem that generate the initial population of multi-objective algorithms. We integrated these seeding strategies with an NSGA-II algorithm for solving the test case selection problem in the context of simulation-based testing. We evaluated the strategies with six case studies and a total of 21 fitness combinations for each case study (i.e., a total of 126 problems). Our evaluation suggests that these strategies are indeed helpful for solving the multi-objective test case selection problem. In fact, two of the proposed seeding strategies outperformed the NSGA-II algorithm without seeding population with statistical significance for 92.8 and 96% of the problems.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1222–1231},
numpages = {10},
keywords = {regression testing, search-based software testing, test case selection},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/2908812.2908871,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Test Case Prioritization of Configurable Cyber-Physical Systems with Weight-Based Search Algorithms},
year = {2016},
isbn = {9781450342063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908812.2908871},
doi = {10.1145/2908812.2908871},
abstract = {Cyber-Physical Systems (CPSs) can be found in many sectors (e.g., automotive and aerospace). These systems are usually configurable to give solutions based on different needs. The variability of these systems is large, which implies they can be set into millions of configurations. As a result, different testing processes are needed to efficiently test these systems: the appropriate configurations must be selected and relevant test cases for each configuration must be chosen as well as prioritized. Prioritizing the order in which the test cases are executed reduces the time for detecting faults in these kinds of systems. However, the test suite size is often large and exploring all the possible test case orders is infeasible. Search algorithms can help find optimal solutions from a large solution space. This paper presents an approach based on weight-based search algorithms for prioritizing the test cases for configurable CPSs. We empirically evaluate the performance of the following algorithms with two case studies: Weight-Based Genetic Algorithms, Random Weighted Genetic Algorithms, Greedy, Alternating Variable Method and Random Search (RS). Our results suggest that all the search algorithms outperform RS, which is taken as a baseline. Local search algorithms have shown better performance than global search algorithms.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
pages = {1053–1060},
numpages = {8},
keywords = {configurable cyber-physical systems, search algorithms, test case prioritization, testing},
location = {Denver, Colorado, USA},
series = {GECCO '16}
}

@article{10.1016/j.scico.2014.11.016,
author = {Salvaneschi, Guido and Ghezzi, Carlo and Pradella, Matteo},
title = {ContextErlang},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2014.11.016},
doi = {10.1016/j.scico.2014.11.016},
abstract = {Self-adaptive software modifies its behavior at run time to satisfy changing requirements in a dynamic environment. Context-oriented programming (COP) has been recently proposed as a specialized programming paradigm for context-aware and adaptive systems. COP mostly focuses on run time adaptation of the application's behavior by supporting modular descriptions of behavioral variations. However, self-adaptive applications must satisfy additional requirements, such as distribution and concurrency, support for unforeseen changes and enforcement of correct behavior in the presence of dynamic change. Addressing these issues at the language level requires a holistic design that covers all aspects and takes into account the possibly cumbersome interaction of those features, for example concurrency and dynamic change.We present ContextErlang, a COP programming language in which adaptive abstractions are seamlessly integrated with distribution and concurrency. We define ContextErlang's formal semantics, validated through an executable prototype, and we show how it supports formal proofs that the language design ensures satisfaction of certain safety requirements. We provide empirical evidence that ContextErlang is an effective solution through case studies and a performance assessment. We also show how the same design principles that lead to the development of ContextErlang can be followed to systematically design contextual extensions of other languages. A concrete example is presented concerning ContextScala.},
journal = {Sci. Comput. Program.},
month = may,
pages = {20–43},
numpages = {24},
keywords = {Concurrency, Context, Context-oriented programming, Distribution, Self-adaptive software}
}

@inproceedings{10.1145/3139258.3139265,
author = {Sailer, Andreas and Deubzer, Michael and L\"{u}ttgen, Gerald and Mottok, J\"{u}rgen},
title = {Comparing trace recordings of automotive real-time software},
year = {2017},
isbn = {9781450352864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139258.3139265},
doi = {10.1145/3139258.3139265},
abstract = {The process of engineering models of existing real-time system components is often difficult and time consuming, especially when legacy code has to be re-used or information about the exact timing behaviour is needed. In order to tackle this reverse engineering problem, we have developed the tool CoreTAna. CoreTAna derives an AUTOSAR compliant model of a real-time system by conducting dynamic analysis using trace recordings.Motivated by the challenge of assessing the quality of reverse engineered models of real-time software, we present a novel mathematical measure for comparing trace recordings from embedded real-time systems regarding their temporal behaviour. We also introduce a benchmark framework based on this measure, for evaluating reverse engineering tools such as CoreTAna. This considers common system architectures and also includes randomly generated systems and three systems of industrial automotive projects. Finally, an industrial case study demonstrates other use cases of our measure, such as impact analysis.},
booktitle = {Proceedings of the 25th International Conference on Real-Time Networks and Systems},
pages = {118–127},
numpages = {10},
keywords = {AUTOSAR, CoreTAna, profiling, real-time systems, reverse engineering, timing model, tracing},
location = {Grenoble, France},
series = {RTNS '17}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1007/s10515-018-0247-4,
author = {Gonzalez-Fernandez, Yasser and Hamidi, Saeideh and Chen, Stephen and Liaskos, Sotirios},
title = {Efficient elicitation of software configurations using crowd preferences and domain knowledge},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-018-0247-4},
doi = {10.1007/s10515-018-0247-4},
abstract = {As software systems grow in size and complexity, the process of configuring them to meet individual needs becomes more and more challenging. Users, especially those that are new to a system, are faced with an ever increasing number of configuration possibilities, making the task of choosing the right one more and more daunting. However, users are rarely alone in using a software system. Crowds of other users or the designers themselves can provide with examples and rules as to what constitutes a meaningful configuration. We introduce a technique for designing optimal interactive configuration elicitation dialogs, aimed at utilizing crowd and expert information to reduce the amount of manual configuration effort. A repository of existing user configurations supplies us with information about popular ways to complete an existing partial configuration. Designers augment this information with their own constraints. A Markov decision process (MDP) model is then created to encode configuration elicitation dialogs that maximize the automatic configuration decisions based on the crowd and the designers' information. A genetic algorithm is employed to solve the MDP when problem sizes prevent use of common exact techniques. In our evaluation with various configuration models we show that the technique is feasible, saves configuration effort and scales for real problem sizes of a few hundreds of features.},
journal = {Automated Software Engg.},
month = mar,
pages = {87–123},
numpages = {37},
keywords = {Genetic algorithms, Markov decision processes, Software configuration, Software customization}
}

@article{10.1016/j.jss.2017.03.005,
author = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
title = {Automotive software engineering},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.005},
doi = {10.1016/j.jss.2017.03.005},
abstract = {A comprehensive survey of literature on Automotive Software Engineering (ASE).679 primary studies were identified, classified and analyzed with respect to five dimensions.Three most investigated areas include software architecture &amp; design, testing and reuse.ASE seems to have high industrial relevance but is relatively lower in its scientific rigor.Validation &amp; comparative studies are less represented and literature lacks practitioner-oriented guidelines. The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.},
journal = {J. Syst. Softw.},
month = jun,
pages = {25–55},
numpages = {31},
keywords = {Automotive software engineering, Automotive systems, Embedded systems, Literature survey, Software-intensive systems, Systematic mapping study}
}

@article{10.5555/1239022.1239025,
author = {Siddique, Zahed and Ninan, Jiju A.},
title = {Modeling of modularity and scaling for integration of customer in design of engineer-to-order products},
year = {2006},
issue_date = {April 2006},
publisher = {IOS Press},
address = {NLD},
volume = {13},
number = {2},
issn = {1069-2509},
abstract = {To survive in today's volatile market, companies are striving to deliver greater quality, more customization and innovative designs by offering user customizable products. Mass customization features products that can be altered or changed by the customer to fit his/her needs. In order to incorporate customer specifications, it is necessary to integrate the customer into the design process. Design tools and methodologies need to be altered to accommodate the customer into the process of designing customized products. This paper presents a web based framework to provide customizable products in real time by integrating the customer into the design process. The approach is based on templates to generate automatically Computer Aided Design (CAD) models from customer specifications. Structural feasibility of the user specified products are then evaluated using automated Finite Element Analysis (FEA), optimized and communicated back to the user in real time. The CAD and FEA product family template generalizes the rules and guidelines for entire product family. The template, optimization formulations, and the framework is implemented using commercial software. The applicability of the system is illustrated for web based customization of bicycle frames.},
journal = {Integr. Comput.-Aided Eng.},
month = apr,
pages = {133–148},
numpages = {16}
}

@article{10.1007/s10270-018-00712-x,
author = {Bencomo, Nelly and G\"{o}tz, Sebastian and Song, Hui},
title = {Models@run.time: a guided tour of the state of the art and research challenges},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00712-x},
doi = {10.1007/s10270-018-00712-x},
abstract = {More than a decade ago, the research topic models@run.time was coined. Since then, the research area has received increasing attention. Given the prolific results during these years, the current outcomes need to be sorted and classified. Furthermore, many gaps need to be categorized in order to further develop the research topic by experts of the research area but also newcomers. Accordingly, the paper discusses the principles and requirements of models@run.time and the state of the art of the research line. To make the discussion more concrete, a taxonomy is defined and used to compare the main approaches and research outcomes in the area during the last decade and including ancestor research initiatives. We identified and classified 275 papers on models@run.time, which allowed us to identify the underlying research gaps and to elaborate on the corresponding research challenges. Finally, we also facilitate sustainability of the survey over time by offering tool support to add, correct and visualize data.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {3049–3082},
numpages = {34},
keywords = {Causal connection, Models@run.time, Self-reflection, Systematic literature review}
}

@article{10.1007/s10732-015-9295-0,
author = {Toledo, Claudio Fabiano and Silva Arantes, M\'{a}rcio and Hossomi, Marcelo Yukio and Fran\c{c}a, Paulo Morelato and Akartunal\i{}, Kerem},
title = {A relax-and-fix with fix-and-optimize heuristic applied to multi-level lot-sizing problems},
year = {2015},
issue_date = {October   2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1381-1231},
url = {https://doi.org/10.1007/s10732-015-9295-0},
doi = {10.1007/s10732-015-9295-0},
abstract = {In this paper, we propose a simple but efficient heuristic that combines construction and improvement heuristic ideas to solve multi-level lot-sizing problems. A relax-and-fix heuristic is firstly used to build an initial solution, and this is further improved by applying a fix-and-optimize heuristic. We also introduce a novel way to define the mixed-integer subproblems solved by both heuristics. The efficiency of the approach is evaluated solving two different classes of multi-level lot-sizing problems: the multi-level capacitated lot-sizing problem with backlogging and the two-stage glass container production scheduling problem (TGCPSP). We present extensive computational results including four test sets of the Multi-item Lot-Sizing with Backlogging library, and real-world test problems defined for the TGCPSP, where we benchmark against state-of-the-art methods from the recent literature. The computational results show that our combined heuristic approach is very efficient and competitive, outperforming benchmark methods for most of the test problems.},
journal = {Journal of Heuristics},
month = oct,
pages = {687–717},
numpages = {31},
keywords = {90C11, Backlogging, Fix-and-optimize, Heuristics, Lot-sizing, Relax-and-fix}
}

@inproceedings{10.1145/2491411.2491437,
author = {Liebig, J\"{o}rg and von Rhein, Alexander and K\"{a}stner, Christian and Apel, Sven and D\"{o}rre, Jens and Lengauer, Christian},
title = {Scalable analysis of variable software},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491437},
doi = {10.1145/2491411.2491437},
abstract = {The advent of variability management and generator technology enables users to derive individual variants from a variable code base based on a selection of desired configuration options. This approach gives rise to the generation of possibly billions of variants that, however, cannot be efficiently analyzed for errors with classic analysis techniques. To address this issue, researchers and practitioners usually apply sampling heuristics. While sampling reduces the analysis effort significantly, the information obtained is necessarily incomplete and it is unknown whether sampling heuristics scale to billions of variants. Recently, researchers have begun to develop variability-aware analyses that analyze the variable code base directly exploiting the similarities among individual variants to reduce analysis effort. However, while being promising, so far, variability-aware analyses have been applied mostly only to small academic systems. To learn about the mutual strengths and weaknesses of variability-aware and sampling-based analyses of software systems, we compared the two strategies by means of two concrete analysis implementations (type checking and liveness analysis), applied them to three subject systems: Busybox, the x86 Linux kernel, and OpenSSL. Our key finding is that variability-aware analysis outperforms most sampling heuristics with respect to analysis time while preserving completeness.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {81–91},
numpages = {11},
keywords = {C Preprocessor, Liveness Analysis, Software Product Lines, Type Checking, Variability-aware Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.5555/2684511.2684560,
author = {Lozano, Alvaro J. and Medaglia, Andr\'{e}s L.},
title = {Scheduling of parallel machines with sequence-dependent batches and product incompatibilities in an automotive glass facility},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {6},
issn = {1094-6136},
abstract = {This application is motivated by a complex real-world scheduling problem found in the bottleneck workstation of the production line of an automotive safety glass manufacturing facility. The scheduling problem consists of scheduling jobs (glass parts) on a number of parallel batch processing machines (furnaces), assigning each job to a batch, and sequencing the batches on each machine. The two main objectives are to maximize the utilization of the parallel machines and to minimize the delay in the completion date of each job in relation to a required due date (specific for each job). Aside from the main objectives, the output batches should also produce a balanced workload on the parallel machines, balanced job due dates within each batch, and minimal capacity loss in the batches. The scheduling problem also considers a batch capacity constraint, sequence-dependent processing times, incompatible product families, additional resources, and machine capability. We propose a two-phase heuristic approach that combines exact methods with search heuristics. The first phase comprises a four-stage mixed-integer linear program for building the batches; the second phase is based on a Greedy Randomized Adaptive Search Procedure for sequencing the batches assigned to each machine. We conducted experiments on instances with up to 100 jobs built with real data from the manufacturing facility. The results are encouraging both in terms of computing time--5 min in average--and quality of the solutions--less than 10 % relative gap from the optimal solution in the first phase and less than 5 % in the second phase. Additional experiments were conducted on randomly generated instances of small, medium, and large size.},
journal = {J. of Scheduling},
month = dec,
pages = {521–540},
numpages = {20},
keywords = {GRASP, Incompatible product families, MILP, Parallel batch-processing machines, Sequence-dependent processing times}
}

@article{10.1007/s11219-011-9160-9,
author = {Perrouin, Gilles and Oster, Sebastian and Sen, Sagar and Klein, Jacques and Baudry, Benoit and Traon, Yves},
title = {Pairwise testing for software product lines: comparison of two approaches},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9160-9},
doi = {10.1007/s11219-011-9160-9},
abstract = {Software Product Lines (SPL) are difficult to validate due to combinatorics induced by variability, which in turn leads to combinatorial explosion of the number of derivable products. Exhaustive testing in such a large products space is hardly feasible. Hence, one possible option is to test SPLs by generating test configurations that cover all possible t feature interactions (t-wise). It dramatically reduces the number of test products while ensuring reasonable SPL coverage. In this paper, we report our experience on applying t-wise techniques for SPL with two independent toolsets developed by the authors. One focuses on generality and splits the generation problem according to strategies. The other emphasizes providing efficient generation. To evaluate the respective merits of the approaches, measures such as the number of generated test configurations and the similarity between them are provided. By applying these measures, we were able to derive useful insights for pairwise and t-wise testing of product lines.},
journal = {Software Quality Journal},
month = sep,
pages = {605–643},
numpages = {39},
keywords = {Alloy, Model-based engineering and testing, Software product lines, Test generation, t-wise and pairwise}
}

@article{10.14778/2536360.2536364,
author = {Psaroudakis, Iraklis and Athanassoulis, Manos and Ailamaki, Anastasia},
title = {Sharing data and work across concurrent analytical queries},
year = {2013},
issue_date = {July 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536360.2536364},
doi = {10.14778/2536360.2536364},
abstract = {Today's data deluge enables organizations to collect massive data, and analyze it with an ever-increasing number of concurrent queries. Traditional data warehouses (DW) face a challenging problem in executing this task, due to their query-centric model: each query is optimized and executed independently. This model results in high contention for resources. Thus, modern DW depart from the query-centric model to execution models involving sharing of common data and work. Our goal is to show when and how a DW should employ sharing. We evaluate experimentally two sharing methodologies, based on their original prototype systems, that exploit work sharing opportunities among concurrent queries at run-time: Simultaneous Pipelining (SP), which shares intermediate results of common sub-plans, and Global Query Plans (GQP), which build and evaluate a single query plan with shared operators.First, after a short review of sharing methodologies, we show that SP and GQP are orthogonal techniques. SP can be applied to shared operators of a GQP, reducing response times by 20%-48% in workloads with numerous common sub-plans. Second, we corroborate previous results on the negative impact of SP on performance for cases of low concurrency. We attribute this behavior to a bottleneck caused by the push-based communication model of SP. We show that pull-based communication for SP eliminates the overhead of sharing altogether for low concurrency, and scales better on multi-core machines than push-based SP, further reducing response times by 82%-86% for high concurrency. Third, we perform an experimental analysis of SP, GQP and their combination, and show when each one is beneficial. We identify a trade-off between low and high concurrency. In the former case, traditional query-centric operators with SP perform better, while in the latter case, GQP with shared operators enhanced by SP give the best results.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {637–648},
numpages = {12}
}

@book{10.5555/1207340,
author = {Contos, Brian T. and Hunt, Steve and Derodeff, Colby},
title = {Physical and Logical Security Convergence: Powered By Enterprise Security Management: Powered By Enterprise Security Management},
year = {2007},
isbn = {1597491225},
publisher = {Syngress Publishing},
abstract = {Government and companies have already invested hundreds of millions of dollars in the convergence of physical and logical security solutions, but there are no books on the topic. This book begins with an overall explanation of information security, physical security, and why approaching these two different types of security in one way (called convergence) is so critical in today's changing security landscape. It then details enterprise security management as it relates to incident detection and incident management. This is followed by detailed examples of implementation, taking the reader through cases addressing various physical security technologies such as: video surveillance, HVAC, RFID, access controls, biometrics, and more. *This topic is picking up momentum every day with every new computer exploit, announcement of a malicious insider, or issues related to terrorists, organized crime, and nation-state threats *The author has over a decade of real-world security and management expertise developed in some of the most sensitive and mission-critical environments in the world *Enterprise Security Management (ESM) is deployed in tens of thousands of organizations worldwide}
}

@article{10.1016/j.compind.2011.10.005,
author = {Rol\'{o}n, Milagros and Mart\'{\i}nez, Ernesto},
title = {Agent-based modeling and simulation of an autonomic manufacturing execution system},
year = {2012},
issue_date = {January, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {63},
number = {1},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2011.10.005},
doi = {10.1016/j.compind.2011.10.005},
abstract = {Production management systems must constantly deal with unplanned disruptive events and disturbances such as arrivals of rush orders, raw material shortage/delays or equipment breakdowns along with a multitude of interactions in the supply chain which constantly demand on-line task rescheduling and order execution control. For responsiveness and agility at the shop-floor, a distributed design for manufacturing execution systems is proposed based on autonomic units that fill the gap between production planning and shop-floor control. An interaction mechanism designed around the concept of order and resource agents implementing the monitor-analyze-plan-execution loop is described. Generative simulation modeling of an autonomic manufacturing execution system (@MES) is proposed in order to evaluate emerging behaviors and macroscopic dynamics in a multiproduct batch plant. Results obtained for an industrial case study using a simulation model of the proposed @MES are presented. The usefulness of agent-based modeling and simulation as a tool for distributed MESs design and to verify performance, stability and disturbance rejection capability of an interaction mechanism is highlighted.},
journal = {Comput. Ind.},
month = jan,
pages = {53–78},
numpages = {26},
keywords = {Distributed scheduling, Intelligent automation, Manufacturing execution systems, Multi-agent simulation, Production control}
}

@article{10.1155/2013/683615,
author = {Chattopadhyay, Anupam},
title = {Ingredients of adaptability: a survey of reconfigurable processors},
year = {2013},
issue_date = {January 2013},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2013},
issn = {1065-514X},
url = {https://doi.org/10.1155/2013/683615},
doi = {10.1155/2013/683615},
abstract = {For a design to survive unforeseen physical effects like aging, temperature variation, and/or emergence of new application standards, adaptability needs to be supported. Adaptability, in its complete strength, is present in reconfigurable processors, which makes it an important IP in modern System-on-Chips (SoCs). Reconfigurable processors have risen to prominence as a dominant computing platform across embedded, general-purpose, and high-performance application domains during the last decade. Significant advances have been made in many areas such as, identifying the advantages of reconfigurable platforms, their modeling, implementation flow and finally towards early commercial acceptance. This paper reviews these progresses from various perspectives with particular emphasis on fundamental challenges and their solutions. Empowered with the analysis of past, the future research roadmap is proposed.},
journal = {VLSI Des.},
month = jan,
articleno = {10},
numpages = {1}
}

@inproceedings{10.1145/2660190.2662115,
author = {Baller, Hauke and Lochau, Malte},
title = {Towards incremental test suite optimization for software product lines},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2662115},
doi = {10.1145/2660190.2662115},
abstract = {The design of an appropriate test suite for software testing is a challenging task. It requires a suitable tradeoff between effectiveness, e.g., a sufficient amount of test cases to satisfy the test goals of a given coverage criterion, and efficiency, e.g., a redundancy-reduced selection of test cases. Recent test suite optimization approaches, therefore, usually require an explicit enumeration of existing test cases to select from. The test suite design for covering entire software product lines is even more problematic as the dependency between test cases, test goals and product configurations has to be taken into account. Due to the exponential number of configurations w.r.t. the number of features, an explicit enumeration of all products for optimizing a product-line test suite is impractible. To tackle this problem, we propose an incremental test suite optimization approach for product-line testing that does not require an explicit representation of the set of configurations under test, but rather uses a symbolic representation in terms of feature constraints. The approach is illustrated by means of a running example.},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {30–36},
numpages = {7},
keywords = {model-based software engineering, software product lines, test case generation},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@article{10.1145/3375636,
author = {Ali, Shaukat and Arcaini, Paolo and Pradhan, Dipesh and Safdar, Safdar Aqeel and Yue, Tao},
title = {Quality Indicators in Search-based Software Engineering: An Empirical Evaluation},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3375636},
doi = {10.1145/3375636},
abstract = {Search-Based Software Engineering (SBSE) researchers who apply multi-objective search algorithms (MOSAs) often assess the quality of solutions produced by MOSAs with one or more quality indicators (QIs). However, SBSE lacks evidence providing insights on commonly used QIs, especially about agreements among them and their relations with SBSE problems and applied MOSAs. Such evidence about QIs agreements is essential to understand relationships among QIs, identify redundant QIs, and consequently devise guidelines for SBSE researchers to select appropriate QIs for their specific contexts. To this end, we conducted an extensive empirical evaluation to provide insights on commonly used QIs in the context of SBSE, by studying agreements among QIs with and without considering differences of SBSE problems and MOSAs. In addition, by defining a systematic process based on three common ways of comparing MOSAs in SBSE, we present additional observations that were automatically produced based on the results of our empirical evaluation. These observations can be used by SBSE researchers to gain a better understanding of the commonly used QIs in SBSE, in particular, regarding their agreements. Finally, based on the results, we also provide a set of guidelines for SBSE researchers to select appropriate QIs for their particular context.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {10},
numpages = {29},
keywords = {Search-based software engineering, agreement, multi-objective search algorithm, quality indicator}
}

@article{10.1016/j.infsof.2019.06.009,
author = {Arrieta, Aitor and Wang, Shuai and Markiegi, Urtzi and Arruabarrena, Ainhoa and Etxeberria, Leire and Sagardui, Goiuria},
title = {Pareto efficient multi-objective black-box test case selection for simulation-based testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.009},
doi = {10.1016/j.infsof.2019.06.009},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {137–154},
numpages = {18},
keywords = {Test case selection, Search-based software engineering, Simulation-based testing, Cyber-physical systems}
}

@inproceedings{10.1145/2593489.2593493,
author = {Beek, Maurice H. ter and de Vink, Erik P.},
title = {Using mCRL2 for the analysis of software product lines},
year = {2014},
isbn = {9781450328531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593489.2593493},
doi = {10.1145/2593489.2593493},
abstract = {We show how the formal specification language mCRL2 and its state-of-the-art toolset can be used successfully to model and analyze variability in software product lines. The mCRL2 toolset supports parametrized modeling, model reduction and quality assurance techniques like model checking. We present a proof-of-concept, which moreover illustrates the use of data in mCRL2 and also how to exploit its data language to manage feature attributes of software product lines and quantitative constraints between attributes and features.},
booktitle = {Proceedings of the 2nd FME Workshop on Formal Methods in Software Engineering},
pages = {31–37},
numpages = {7},
keywords = {Model checking, mCRL2, product lines, variability analysis},
location = {Hyderabad, India},
series = {FormaliSE 2014}
}

@article{10.1016/j.compind.2008.07.009,
author = {Zha, Xuan F. and Sriram, Ram D. and Fernandez, Marco G. and Mistree, Farrokh},
title = {Knowledge-intensive collaborative decision support for design processes: A hybrid decision support model and agent},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {9},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2008.07.009},
doi = {10.1016/j.compind.2008.07.009},
abstract = {This paper proposes a hybrid decision support model within a multi-agent framework to facilitate integration and collaboration for design decisions. The hybrid decision model integrates the compromise decision support problem technique (cDSP) and the fuzzy synthetic decision model (FSD) to quantitatively incorporate qualitative design knowledge and preferences of designers for multiple, conflicting attributes stored in a knowledge repository so that a better understanding of the consequences of design decisions can be achieved from a more comprehensive perspective. The focus of this work is on the provision of a hybrid decision model and framework for improved cooperative design decision support. The novelty of the work is in combination of different models or methods, algorithms for making collaborative design decisions in both objective and subjective natures, in particular the fuzzy negotiation mechanism using the hybrid decision model. The developed hybrid decision model and framework are generic and flexible enough to be used in a variety of decision problems. Finally, the proposed hybrid decision model and framework are illustrated with applications in collaborative concept evaluation and selection in product family design for mass customization.},
journal = {Comput. Ind.},
month = dec,
pages = {905–922},
numpages = {18},
keywords = {Autonomous decision agent, Decision model, Decision-based design, Fuzzy negotiation, Hybrid, Multi-agent framework}
}

@inproceedings{10.1145/3458864.3467880,
author = {Garg, Nakul and Bai, Yang and Roy, Nirupam},
title = {Owlet: enabling spatial information in ubiquitous acoustic devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467880},
doi = {10.1145/3458864.3467880},
abstract = {This paper presents a low-power and miniaturized design for acoustic direction-of-arrival (DoA) estimation and source localization, called Owlet. The required aperture, power consumption, and hardware complexity of the traditional array-based spatial sensing techniques make them unsuitable for small and power-constrained IoT devices. Aiming to overcome these fundamental limitations, Owlet explores acoustic microstructures for extracting spatial information. It uses a carefully designed 3D-printed metamaterial structure that covers the microphone. The structure embeds a direction-specific signature in the recorded sounds. Owlet system learns the directional signatures through a one-time in-lab calibration. The system uses an additional microphone as a reference channel and develops techniques that eliminate environmental variation, making the design robust to noises and multipaths in arbitrary locations of operations. Owlet prototype shows 3.6° median error in DoA estimation and 10cm median error in source localization while using a 1.5cm \texttimes{} 1.3cm acoustic structure for sensing. The prototype consumes less than 100th of the energy required by a traditional microphone array to achieve similar DoA estimation accuracy. Owlet opens up possibilities of low-power sensing through 3D-printed passive structures.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {255–268},
numpages = {14},
keywords = {IoT, acoustic metamaterial, low-power sensing, spatial sensing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}

@article{10.1007/s10664-013-9255-y,
author = {Borg, Markus and Runeson, Per and Ard\"{o}, Anders},
title = {Recovering from a decade: a systematic mapping  of information retrieval approaches  to software traceability},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9255-y},
doi = {10.1007/s10664-013-9255-y},
abstract = {Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude that the overall quality of reporting should be improved regarding both context and tool details, measures reported, and use of IR terminology. Finally, based on our empirical findings, we present suggestions on how to advance research on IR-based  trace recovery.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1565–1616},
numpages = {52},
keywords = {Information retrieval, Software artifacts, Systematic mapping study, Traceability}
}

@article{10.1016/j.infsof.2017.03.011,
author = {Martnez-Fernndez, Silverio and Ayala, Claudia P. and Franch, Xavier and Marques, Helena Martins},
title = {Benefits and drawbacks of software reference architectures},
year = {2017},
issue_date = {August 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {88},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.011},
doi = {10.1016/j.infsof.2017.03.011},
abstract = {ContextSoftware Reference Architectures (SRAs) play a fundamental role for organizations whose business greatly depends on the efficient development and maintenance of complex software applications. However, little is known about the real value and risks associated with SRAs in industrial practice. ObjectiveTo investigate the current industrial practice of SRAs in a single company from the perspective of different stakeholders. MethodAn exploratory case study that investigates the benefits and drawbacks perceived by relevant stakeholders in nine SRAs designed by a multinational software consulting company. ResultsThe study shows the perceptions of different stakeholders regarding the benefits and drawbacks of SRAs (e.g., both SRA designers and users agree that they benefit from reduced development costs; on the contrary, only application builders strongly highlighted the extra learning curve as a drawback associated with mastering SRAs). Furthermore, some of the SRA benefits and drawbacks commonly highlighted in the literature were remarkably not mentioned as a benefit of SRAs (e.g., the use of best practices). Likewise, other aspects arose that are not usually discussed in the literature, such as higher time-to-market for applications when their dependencies on the SRA are managed inappropriately. ConclusionsThis study aims to help practitioners and researchers to better understand real SRAs projects and the contexts where these benefits and drawbacks appeared, as well as some SRA improvement strategies. This would contribute to strengthening the evidence regarding SRAs and support practitioners in making better informed decisions about the expected SRA benefits and drawbacks. Furthermore, we make available the instruments used in this study and the anonymized data gathered to motivate others to provide similar evidence to help mature SRA research and practice.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {37–52},
numpages = {16},
keywords = {Benefits, Case study, Drawbacks, Empirical software engineering, Reference architecture, Software architecture}
}

@article{10.1007/s10664-021-09964-6,
author = {Duchien, Laurence and Gr\"{u}nbacher, Paul and Th\"{u}m, Thomas},
title = {Foreword to the Special Issue on Configurable Systems},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09964-6},
doi = {10.1007/s10664-021-09964-6},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {3}
}

@article{10.1007/s10009-012-0254-x,
author = {J\"{o}rges, Sven and Lamprecht, Anna-Lena and Margaria, Tiziana and Schaefer, Ina and Steffen, Bernhard},
title = {A constraint-based variability modeling framework},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0254-x},
doi = {10.1007/s10009-012-0254-x},
abstract = {Constraint-based variability modeling is a flexible, declarative approach to managing solution-space variability. Product variants are defined in a top-down manner by successively restricting the admissible combinations of product artifacts until a specific product variant is determined. In this paper, we illustrate the range of constraint-based variability modeling by discussing two of its extreme flavors: constraint-guarded variability modeling and constraint-driven variability modeling. The former applies model checking to establish the global consistency of product variants which are built by manual specification of variations points, whereas the latter uses synthesis technology to fully automatically generate product variants that satisfy all given constraints. Each flavor is illustrated by means of a concrete case study.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {511–530},
numpages = {20},
keywords = {Constraint-based variability modeling, Software product lines, Variability}
}

@article{10.1007/s10845-018-1430-y,
author = {Zheng, Pai and Xu, Xun and Chen, Chun-Hsien},
title = {A data-driven cyber-physical approach for personalised smart, connected product co-development in a cloud-based environment},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-018-1430-y},
doi = {10.1007/s10845-018-1430-y},
abstract = {The rapid development of information and communication technology enables a promising market of information densely product, i.e. smart, connected product (SCP), and also changes the way of user–designer interaction in the product development process. For SCP, massive data generated by users drives its design innovation and somehow determines its final success. Nevertheless, most existing works only look at the new functionalities or values that are derived in the one-way communication by introducing novel data analytics methods. Few work discusses about an effective and systematic approach to enable individual user innovation in such context, i.e. co-development process, which sets the fundamental basis of the prevailing concept of data-driven design. Aiming to fill this gap, this paper proposes a generic data-driven cyber-physical approach for personalised SCP co-development in a cloud-based environment. A novel concept of smart, connected, open architecture product is hence introduced with a generic cyber-physical model established in a cloud-based environment, of which the interaction processes are enabled by co-development toolkits with smartness and connectedness. Both the personalized SCP modelling method and the establishment of its cyber-physical product model are described in details. To further demonstrate the proposed approach, a case study of a smart wearable device (i.e. i-BRE respiratory mask) development process is given with general discussions.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {3–18},
numpages = {16},
keywords = {Data-driven design, Open architecture product, Smart product, Cyber physical system, Cloud, Mass personalisation}
}

@article{10.1007/s11265-018-1343-1,
author = {Palossi, Daniele and Gomez, Andres and Draskovic, Stefan and Marongiu, Andrea and Thiele, Lothar and Benini, Luca},
title = {Extending the Lifetime of Nano-Blimps via Dynamic Motor Control},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {91},
number = {3–4},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-018-1343-1},
doi = {10.1007/s11265-018-1343-1},
abstract = {Nano-sized unmanned aerial vehicles (UAVs), e.g. quadcopters, have received significant attention in recent years. Although their capabilities have grown, they continue to have very limited flight times, tens of minutes at most. The main constraints are the battery's energy density and the engine power required for flight. In this work, we present a nano-sized blimp platform, consisting of a helium balloon and a rotorcraft. Thanks to the lift provided by helium, the blimp requires relatively little energy to remain at a stable altitude. This lift, however, decreases with time as the balloon inevitably deflates requiring additional control mechanisms to keep the desired altitude. We study how duty-cycling high power actuators can further reduce the average energy requirements for hovering. With the addition of a solar panel, it is even feasible to sustain tens or hundreds of flight hours in modest lighting conditions. Furthermore, we study how a balloon's deflation rate affects the blimp's energy budget and lifetime. A functioning 68-gram prototype was thoroughly characterized and its lifetime was measured under different harvesting conditions and different power management strategies. Both our system model and the experimental results indicate our proposed platform requires less than 200 mW to hover indefinitely with an ideal balloon. With a non-ideal balloon the maximum lifetime of ~400 h is bounded by the rotor's maximum thrust. This represents, to the best of our knowledge, the first nano-size UAV for long term hovering with low power requirements.},
journal = {J. Signal Process. Syst.},
month = mar,
pages = {339–361},
numpages = {23},
keywords = {Blimp, Energy neutrality, Self sustainability, UAV}
}

@book{10.5555/2505467,
author = {Vacca, John R. and Vacca, John R.},
title = {Computer and Information Security Handbook, Second Edition},
year = {2013},
isbn = {0123943973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions}
}

@article{10.1007/s11219-016-9341-7,
author = {Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire and Zander, Justyna},
title = {Automatic generation of test system instances for configurable cyber-physical systems},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9341-7},
doi = {10.1007/s11219-016-9341-7},
abstract = {Cyber-physical systems (CPSs) are ubiquitous systems that integrate digital technologies with physical processes. These systems are becoming configurable to respond to the different needs that users demand. As a consequence, their variability is increasing, and they can be configured in many system variants. To ensure a systematic test execution of CPSs, a test system must be elaborated encapsulating several sources such as test cases or test oracles. Manually building a test system for each configuration is a non-systematic, time-consuming, and error-prone process. To overcome these problems, we designed a test system for testing CPSs and we analyzed the variability that it needed to test different configurations. Based on this analysis, we propose a methodology supported by a tool named ASTERYSCO that automatically generates simulation-based test system instances to test individual configurations of CPSs. To evaluate the proposed methodology, we selected different configurations of a configurable Unmanned Aerial Vehicle, and measured the time required to generate their test systems. On average, around 119 s were needed by our tool to generate the test system for 38 configurations. In addition, we compared the process of generating test system instances between the method we propose and a manual approach. Based on this comparison, we believe that the proposed tool allows a systematic method of generating test system instances. We believe that our approach permits an important step toward the full automation of testing in the field of configurable CPSs.},
journal = {Software Quality Journal},
month = sep,
pages = {1041–1083},
numpages = {43},
keywords = {Configurable cyber-physical systems, Test automation, Test system generation}
}

@inproceedings{10.1145/2488222.2488268,
author = {Schneider, Scott and Hirzel, Martin and Gedik, Bu\u{g}ra},
title = {Tutorial: stream processing optimizations},
year = {2013},
isbn = {9781450317580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488222.2488268},
doi = {10.1145/2488222.2488268},
abstract = {This tutorial starts with a survey of optimizations for streaming applications. The survey is organized as a catalog that introduces uniform terminology and a common categorization of optimizations across disciplines, such as data management, programming languages, and operating systems. After this survey, the tutorial continues with a deep-dive into the fission optimization, which automatically transforms streaming applications for data-parallelism. Fission helps an application improve its throughput by taking advantage of multiple cores in a machine, or, in the case of a distributed streaming engine, multiple machines in a cluster. While the survey of optimizations covers a wide range of work from the literature, the in-depth discussion of fission relies more heavily on the presenters' own research and experience in the area. The tutorial concludes with a discussion of open research challenges in the field of stream processing optimizations.},
booktitle = {Proceedings of the 7th ACM International Conference on Distributed Event-Based Systems},
pages = {249–258},
numpages = {10},
keywords = {data parallelism, fission, stream processing},
location = {Arlington, Texas, USA},
series = {DEBS '13}
}

@book{10.5555/1543376,
author = {Jacob, Bruce and Ng, Spencer and Wang, David},
title = {Memory Systems: Cache, DRAM, Disk},
year = {2007},
isbn = {0123797519},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Is your memory hierarchy stopping your microprocessor from performing at the high level it should be? Memory Systems: Cache, DRAM, Disk shows you how to resolve this problem. The book tells you everything you need to know about the logical design and operation, physical design and operation, performance characteristics and resulting design trade-offs, and the energy consumption of modern memory hierarchies. You learn how to to tackle the challenging optimization problems that result from the side-effects that can appear at any point in the entire hierarchy.As a result you will be able to design and emulate the entire memory hierarchy. . Understand all levels of the system hierarchy -Xcache, DRAM, and disk. . Evaluate the system-level effects of all design choices. . Model performance and energy consumption for each component in the memory hierarchy.}
}

@article{10.1016/j.jss.2016.02.010,
author = {Alegre, Unai and Augusto, Juan Carlos and Clark, Tony},
title = {Engineering context-aware systems and applications},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.010},
doi = {10.1016/j.jss.2016.02.010},
abstract = {Survey the efforts of the community in order to encourage a Context-Aware Systems Engineering process.Analysis of the state-of-the-art engineering techniques applied in the most common development stages.A study of existing methodologies within these development stages.The main challenges remaining open in the Context-Aware Computing field. Context-awareness is an essential component of systems developed in areas like Intelligent Environments, Pervasive &amp; Ubiquitous Computing and Ambient Intelligence. In these emerging fields, there is a need for computerized systems to have a higher understanding of the situations in which to provide services or functionalities, to adapt accordingly. The literature shows that researchers modify existing engineering methods in order to better fit the needs of context-aware computing. These efforts are typically disconnected from each other and generally focus on solving specific development issues. We encourage the creation of a more holistic and unified engineering process that is tailored for the demands of these systems. For this purpose, we study the state-of-the-art in the development of context-aware systems, focusing on: (A) Methodologies for developing context-aware systems, analyzing the reasons behind their lack of adoption and features that the community wish they can use; (B) Context-aware system engineering challenges and techniques applied during the most common development stages; (C) Context-aware systems conceptualization.},
journal = {J. Syst. Softw.},
month = jul,
pages = {55–83},
numpages = {29},
keywords = {Ambient Intelligence, Context-Aware Systems Engineering, Context-aware computing, Context-awareness, Context-sensitive, Intelligent Environments, Pervasive &amp; Ubiquitous Computing, Sentient computing, Software engineering}
}

@article{10.1007/s10009-016-0425-2,
author = {Dimovski, Aleksandar S. and Al-Sibahi, Ahmad Salim and Brabrand, Claus and W\k{a}sowski, Andrzej},
title = {Efficient family-based model checking via variability abstractions},
year = {2017},
issue_date = {Oct 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-016-0425-2},
doi = {10.1007/s10009-016-0425-2},
abstract = {Many software systems are variational: they can be configured to meet diverse sets of requirements. They can produce a (potentially huge) number of related systems, known as products or variants, by systematically reusing common parts. For variational models (variational systems or families of related systems), specialized family-based model checking algorithms allow efficient verification of multiple variants, simultaneously, in a single run. These algorithms, implemented in a tool $$overline{text {SNIP}}$$SNIP , scale much better than "the brute force" approach, where all individual systems are verified using a single-system model checker, one-by-one. Nevertheless, their computational cost still greatly depends on the number of features and variants. For variational models with a large number of features and variants, the family-based model checking may be too costly or even infeasible. In this work, we address two key problems of family-based model checking. First, we improve scalability by introducing abstractions that simplify variability. Second, we reduce the burden of maintaining specialized family-based model checkers, by showing how the presented variability abstractions can be used to model check variational models using the standard version of (single-system) SPIN. The variability abstractions are first defined as Galois connections on semantic domains. We then show how to use them for defining abstract family-based model checking, where a variability model is replaced with an abstract version of it, which preserves the satisfaction of LTL properties. Moreover, given an abstraction, we define a syntactic source-to-source transformation on high-level modeling languages that describe variational models, such that the model checking of the transformed high-level variational model coincides with the abstract model checking of the concrete high-level variational model. This allows the use of SPIN with all its accumulated optimizations for efficient verification of variational models without any knowledge about variability. We have implemented the transformations in a prototype tool, and we illustrate the practicality of this method in several case studies.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {585–603},
numpages = {19},
keywords = {Abstract model checking, Features, Software product lines, Variability abstractions}
}

@article{10.1016/j.jpdc.2016.05.003,
author = {Gedik, B. and \"{O}zsema, H.G. and \"{O}zt\"{u}rk, \"{O}.},
title = {Pipelined fission for stream programs with dynamic selectivity and partitioned state},
year = {2016},
issue_date = {October 2016},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {96},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2016.05.003},
doi = {10.1016/j.jpdc.2016.05.003},
abstract = {There is an ever increasing rate of digital information available in the form of online data streams. In many application domains, high throughput processing of such data is a critical requirement for keeping up with the soaring input rates. Data stream processing is a computational paradigm that aims at addressing this challenge by processing data streams in an on-the-fly manner, in contrast to the more traditional and less efficient store-and-then process approach. In this paper, we study the problem of automatically parallelizing data stream processing applications in order to improve throughput. The parallelization is automatic in the sense that stream programs are written sequentially by the application developers and are parallelized by the system. We adopt the asynchronous data flow model for our work, which is typical in Data Stream Processing Systems (DSPS), where operators often have dynamic selectivity and are stateful. We solve the problem of pipelined fission, in which the original sequential program is parallelized by taking advantage of both pipeline parallelism and data parallelism at the same time. Our pipelined fission solution supports partitioned stateful data parallelism with dynamic selectivity and is designed for shared-memory multi-core machines. We first develop a cost-based formulation that enables us to express pipelined fission as an optimization problem. The bruteforce solution of this problem takes a long time for moderately sized stream programs. Accordingly, we develop a heuristic algorithm that can quickly, but approximately, solve the pipelined fission problem. We provide an extensive evaluation studying the performance of our pipelined fission solution, including simulations as well as experiments with an industrial-strength DSPS. Our results show good scalability for applications that contain sufficient parallelism, as well as close to optimal performance for the heuristic pipelined fission algorithm. Formalizes the pipelined fission problem for streaming applications.Models the throughput of pipelined fission configurations.Develops a three-stage heuristic algorithm to quickly locate a close to optimal pipelined fission configuration.Experimentally evaluates the solution and demonstrate its efficacy.},
journal = {J. Parallel Distrib. Comput.},
month = oct,
pages = {106–120},
numpages = {15},
keywords = {Auto-parallelization, Data stream processing, Fission, Pipelining}
}

@article{10.1007/s10489-018-1282-3,
author = {Fern\'{a}ndez, C\'{e}sar and Salinas, Luis and Torres, Claudio E.},
title = {A meta extreme learning machine method for forecasting financial time series},
year = {2019},
issue_date = {February  2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {2},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1282-3},
doi = {10.1007/s10489-018-1282-3},
abstract = {In the last decade, the problem of forecasting time series in very different fields has received increasing attention due to its many real-world applications. In particular, in the very challenging case of financial time series, the underlying phenomenon of stock time series exhibits complex behaviors, including non-stationary, non-linearity and non-trivial scaling properties. In the literature, a wide-used strategy to improve the forecasting capability is the combination of several models. However, the majority of the published researches in the field of financial time series use different machine learning models where only one type of predictor, either linear or nonlinear, is considered. In this paper we first measure relevant features present in the underlying process to propose a forecast method. We select the Sample Entropy and Hurst Exponent to characterize the behavior of stock time series. The characterization reveals the presence of moderate randomness, long-term memory and scaling properties. Thus, based on the measured properties, this paper proposes a novel one-step-ahead off-line meta-learning model, called μ-XNW, for the prediction of the next value xt+1 of a financial time series xt$x_{t}$, t = 1, 2, 3, ? , that integrates a naive or linear predictor (LP), for which the predicted value of xt+1$x_{t + 1}$ is just repeating the last value xt$x_{t}$, an extreme learning machine (ELM) and a discrete wavelet transform (DWT), both based on the nprevious values of xt+1$x_{t + 1}$. LP, ELM and DWT are the constituent of the proposed model μ-XNW. We evaluate the proposed model using four well-known performance measures and validated the usefulness of the model using six high-frequency stock time series belong to the technology sector. The experimental results validate that including internal estimators that are able to the capture the relevant features measured (randomness, long-term memory and scaling properties) successfully improve the accuracy of the forecasting over methods that do not include them.},
journal = {Applied Intelligence},
month = feb,
pages = {532–554},
numpages = {23},
keywords = {Discrete wavelet transform, Extreme learning machine, Financial time series, Forecasting}
}

@inproceedings{10.1007/11946441_74,
author = {Bonelli, Andreas and Franchetti, Franz and Lorenz, Juergen and P\"{u}schel, Markus and Ueberhuber, Christoph W.},
title = {Automatic performance optimization of the discrete fourier transform on distributed memory computers},
year = {2006},
isbn = {3540680675},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11946441_74},
doi = {10.1007/11946441_74},
abstract = {This paper introduces a formal framework for automatically generating performance optimized implementations of the discrete Fourier transform (DFT) for distributed memory computers. The framework is implemented as part of the program generation and optimization system Spiral. DFT algorithms are represented as mathematical formulas in Spiral's internal language SPL. Using a tagging mechanism and formula rewriting, we extend Spiral to automatically generate parallelized formulas. Using the same mechanism, we enable the generation of rescaling DFT algorithms, which redistribute the data in intermediate steps to fewer processors to reduce communication overhead. It is a novel feature of these methods that the redistribution steps are merged with the communication steps of the algorithm to avoid additional communication overhead. Among the possible alternative algorithms, Spiral's search mechanism now determines the fastest for a given platform, effectively generating adapted code without human intervention. Experiments with DFT MPI programs generated by Spiral show performance gains of up to 30% due to rescaling. Further, our generated programs compare favorably with Fftw-MPI 2.1.5.},
booktitle = {Proceedings of the 4th International Conference on Parallel and Distributed Processing and Applications},
pages = {818–832},
numpages = {15},
location = {Sorrento, Italy},
series = {ISPA'06}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@inproceedings{10.1007/978-3-030-26250-1_32,
author = {Robin, Jacques and Mazo, Raul and Madeira, Henrique and Barbosa, Raul and Diaz, Daniel and Abreu, Salvador},
title = {A Self-certifiable Architecture for Critical Systems Powered by Probabilistic Logic Artificial Intelligence},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_32},
doi = {10.1007/978-3-030-26250-1_32},
abstract = {We present a versatile architecture for AI-powered self-adaptive self-certifiable critical systems. It aims at supporting semi-automated low-cost re-certification for self-adaptive systems after each adaptation of their behavior to a persistent change in their operational environment throughout their lifecycle.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {391–397},
numpages = {7},
keywords = {AI certification, Autonomic architecture, Argumentation, Rule-based constraint solving, Probabilistic logic machine learning},
location = {Turku, Finland}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {combinatorial interaction testing, feature models, pairwise testing, software product lines},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@article{10.1016/j.jss.2010.02.005,
author = {Petersen, Kai and Wohlin, Claes},
title = {Software process improvement through the Lean Measurement (SPI-LEAM) method},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.02.005},
doi = {10.1016/j.jss.2010.02.005},
abstract = {Software process improvement methods help to continuously refine and adjust the software process to improve its performance (e.g., in terms of lead-time, quality of the software product, reduction of change requests, and so forth). Lean software development propagates two important principles that help process improvement, namely identification of waste in the process and considering interactions between the individual parts of the software process from an end-to-end perspective. A large shift of thinking about the own way of working is often required to adopt lean. One of the potential main sources of failure is to try to make a too large shift about the ways of working at once. Therefore, the change to lean has to be done in a continuous and incremental way. In response to this we propose a novel approach to bring together the quality improvement paradigm and lean software development practices, the approach being called Software Process Improvement through the Lean Measurement (SPI-LEAM) Method. The method allows to assess the performance of the development process and take continuous actions to arrive at a more lean software process over time. The method is under implementation in industry and an initial evaluation of the method has been performed.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1275–1287},
numpages = {13},
keywords = {Lean software development, Quality improvement paradigm, Software process improvement}
}

@article{10.1016/j.engappai.2011.05.011,
author = {Guimar\~{a}es, Luis and Klabjan, Diego and Almada-Lobo, Bernardo},
title = {Annual production budget in the beverage industry},
year = {2012},
issue_date = {March, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {25},
number = {2},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2011.05.011},
doi = {10.1016/j.engappai.2011.05.011},
abstract = {Driven by a real-world application in the beverage industry, this paper provides a design of a new VNS variant to tackle the annual production budget problem. The problem consists of assigning and scheduling production lots in a multi-plant environment, where each plant has a set of filling lines that bottle and pack drinks. Plans also consider final product transfers between the plants. Our algorithm fixes setup variables for family of products and determines production, inventory and transfer decisions by solving a linear programming (LP) model. As we are dealing with very large problem instances, it is inefficient and unpractical to search the entire neighborhood of the incumbent solution at each iteration of the algorithm. We explore the sensitivity analysis of the LP to guide the partial neighborhood search. Dual-reoptimization is also used to speed-up the solution procedure. Tests with instances from our case study have shown that the algorithm can substantially improve the current business practice, and it is more competitive than state-of-the-art commercial solvers and other VNS variants.},
journal = {Eng. Appl. Artif. Intell.},
month = mar,
pages = {229–241},
numpages = {13},
keywords = {Beverage industry, Large neighborhood search, Long-term production planning, Mathematical programming, Multi-plant, Sensitivity analysis}
}

@article{10.1016/j.cie.2015.05.029,
author = {Onyeocha, Chukwunonyelum Emmanuel and Khoury, Joseph and Geraghty, John},
title = {Evaluation of multi-product lean manufacturing systems with setup and erratic demand},
year = {2015},
issue_date = {September 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {87},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2015.05.029},
doi = {10.1016/j.cie.2015.05.029},
abstract = {Production authorisation cards and basestock levels are lower in shared policy.The shared policy responds to demand variations quicker than the dedicated policy.BK-CONWIP has a higher flexibility than the alternatives in terms of WIP control.BK-CONWIP is the best performer of the three pull control strategies examined. The consideration of demand variability in Multi-Product Lean Manufacturing Environment (MPLME) is an innovation in production system engineering. Manufacturing systems that fail to recognise demand variability generate high Work-In-Process (WIP) and low throughput in MPLME. In response to demand variability, organisations allocate large quantities of Production Authorisation Cards (PAC). A large proportion of PAC results in a high WIP level. However, the Shared Kanban Allocation Policy (S-KAP) allows the distribution of PAC among part-types, which minimises WIP in MPLME. Nevertheless, some existing lean manufacturing control strategies referred as Pull Production Control Strategies (PPCS) that have shown improved performance in single-product systems failed to operate S-KAP. The recently developed BasestockKanban-CONWIP (BK-CONWIP) strategy has the capability of minimising WIP while maintaining low backlog and volume flexibility. This paper investigates the effects of erratic demand on the performance of PPCS in MPLME. It is shown that S-KAP BK-CONWIP outperforms other PPCS. Finally, it is feasible to design quick-response PPCS for MPLME under erratic demand.},
journal = {Comput. Ind. Eng.},
month = sep,
pages = {465–480},
numpages = {16},
keywords = {Erratic demand, Multi-product systems, Production authorisation cards, Production control strategies}
}

@inproceedings{10.1145/1052898.1052904,
author = {Zhang, Charles and Gao, Dapeng and Jacobsen, Hans-Arno},
title = {Towards just-in-time middleware architectures},
year = {2005},
isbn = {1595930426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1052898.1052904},
doi = {10.1145/1052898.1052904},
abstract = {Middleware becomes increasingly important in building distributed applications. Today, conventional middleware systems are designed, implemented, and packaged prior to their applications. We argue that with this middleware construction paradigm it is often difficult to meet the challenges imposed by application specific customization requirements. We propose to reverse this paradigm by automatically synthesizing middleware structures as the result of reasoning about the distribution needs of the user application of middleware. We term this type of post-postulated middleware Just-in-time middleware (JiM). In this paper, we present our initial design and present an evaluation of the JiM paradigm through Abacus, a CORBA middleware implementation based on the aspect oriented refactoring of an industrial strength object request broker. In addition, we present Arachne, the Abacus synthesizer, which integrates source analysis, feature inference, and implementation synthesis. Our evaluations show that, through automatic synthesis alone, Abacus is able to support diversified application domains with very flexible architectural compositions and versatile resource requirements as compared to conventional pre-postulated approaches.},
booktitle = {Proceedings of the 4th International Conference on Aspect-Oriented Software Development},
pages = {63–74},
numpages = {12},
keywords = {aspect oriented middleware, middleware architecture},
location = {Chicago, Illinois},
series = {AOSD '05}
}

@article{10.1007/s11227-014-1192-z,
author = {Santos, Andr\'{e} C. and Cardoso, Jo\~{a}o M. and Diniz, Pedro C. and Ferreira, Diogo R. and Petrov, Zlatko},
title = {A DSL for specifying run-time adaptations for embedded systems: an application to vehicle stereo navigation},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {70},
number = {3},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-014-1192-z},
doi = {10.1007/s11227-014-1192-z},
abstract = {The traditional approach for specifying adaptive behavior in embedded applications requires developers to engage in error-prone programming tasks. This results in long design cycles and in the inherent inability to explore and evaluate a wide variety of alternative adaptation behaviors, critical for systems exposed to dynamic operational and situational environments. In this paper, we introduce a domain-specific language (DSL) for specifying and implementing run-time adaptable application behavior. We illustrate our approach using a real-life stereo navigation application as a case study, highlighting the impact and benefits of dynamically adapting algorithm parameters. The experiments reveal our approach effective, as such run-time adaptations are easily specified in a higher level by the DSL, and thus at a lower programming effort than when using a general-purpose language such as C.},
journal = {J. Supercomput.},
month = dec,
pages = {1218–1248},
numpages = {31},
keywords = {Adaptable behavior, Domain-specific languages, Embedded systems, Run-time adaptations, Stereo navigation}
}

@inproceedings{10.1145/2103746.2103772,
author = {Hirzel, Martin and Gedik, Bugra},
title = {Streams that compose using macros that oblige},
year = {2012},
isbn = {9781450311182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103746.2103772},
doi = {10.1145/2103746.2103772},
abstract = {Since the end of frequency scaling, the programming languages community has started to embrace multi-core and even distributed systems. One paradigm that lends itself well to distribution is stream processing. In stream processing, an application consists of a directed graph of streams and operators, where streams are infinite sequences of data items, and operators fire in infinite loops to process data. This model directly exposes parallelism, requires no shared memory, and is a good match for several emerging application domains. Unfortunately, streaming languages have so far been lacking in abstraction. This paper introduces higher-order composite operators, which encapsulate stream subgraphs, and contracts, which specify pre- and post-conditions for composites. Composites are expanded at compile time, in a manner similar to macros. Their contractual obligations are also checked at compile-time. We build on existing work on macros and contracts to implement higher-order composites. The user-visible language features provide a consistent look-and-feel for the streaming language, whereas the underlying implementation provides high-quality static error messages and prevents accidental name capture.},
booktitle = {Proceedings of the ACM SIGPLAN 2012 Workshop on Partial Evaluation and Program Manipulation},
pages = {141–150},
numpages = {10},
keywords = {contracts, dataflow, macros, metaprogramming, stream processing},
location = {Philadelphia, Pennsylvania, USA},
series = {PEPM '12}
}

@article{10.1007/s10270-012-0227-2,
author = {Kraft, Stephan and Casale, Giuliano and Krishnamurthy, Diwakar and Greer, Des and Kilpatrick, Peter},
title = {Performance models of storage contention in cloud environments},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0227-2},
doi = {10.1007/s10270-012-0227-2},
abstract = {We propose simple models to predict the performance degradation of disk requests due to storage device contention in consolidated virtualized environments. Model parameters can be deduced from measurements obtained inside Virtual Machines (VMs) from a system where a single VM accesses a remote storage server. The parameterized model can then be used to predict the effect of storage contention when multiple VMs are consolidated on the same server. We first propose a trace-driven approach that evaluates a queueing network with fair share scheduling using simulation. The model parameters consider Virtual Machine Monitor level disk access optimizations and rely on a calibration technique. We further present a measurement-based approach that allows a distinct characterization of read/write performance attributes. In particular, we define simple linear prediction models for I/O request mean response times, throughputs and read/write mixes, as well as a simulation model for predicting response time distributions. We found our models to be effective in predicting such quantities across a range of synthetic and emulated application workloads.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {681–704},
numpages = {24},
keywords = {Performance modeling, Storage, Virtualization}
}

@article{10.1007/s10270-019-00735-y,
author = {Wolny, Sabine and Mazak, Alexandra and Carpella, Christine and Geist, Verena and Wimmer, Manuel},
title = {Thirteen years of SysML: a systematic mapping study},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00735-y},
doi = {10.1007/s10270-019-00735-y},
abstract = {The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML.},
journal = {Softw. Syst. Model.},
month = jan,
pages = {111–169},
numpages = {59},
keywords = {SysML, Systematic mapping study, Systems engineering}
}

@article{10.1016/j.infsof.2016.01.019,
author = {Arcaini, Paolo and Gargantini, Angelo and Riccobene, Elvinia and Vavassori, Paolo},
title = {A novel use of equivalent mutants for static anomaly detection in software artifacts},
year = {2017},
issue_date = {January 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {81},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.01.019},
doi = {10.1016/j.infsof.2016.01.019},
abstract = {Equivalent mutants are usually seen as an inconvenience in mutation analysis.We claim that equivalent mutants can be useful to detect and remove static anomalies.A process for detecting static anomalies is proposed.The process is based on mutation, equivalence checking, and quality measurement.The process is applicable to different kinds of software artifacts. Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis.Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed.Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement.Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level.Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {52–64},
numpages = {13},
keywords = {Equivalent mutant, Quality measure, Static anomaly}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {Self-adaptation, uncertainty, uncertainty models, uncertainty methods, unanticipated change, uncertainty challenges, survey}
}

@article{10.1016/j.infsof.2017.02.004,
author = {Ahmed, Bestoun S. and Gambardella, Luca M. and Afzal, Wasif and Zamli, Kamal Z.},
title = {Handling constraints in combinatorial interaction testing in the presence of multi objective particle swarm and multithreading},
year = {2017},
issue_date = {June 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {86},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.02.004},
doi = {10.1016/j.infsof.2017.02.004},
abstract = {ContextCombinatorial testing strategies have lately received a lot of attention as a result of their diverse applications. In its simple form, a combinatorial strategy can reduce several input parameters (configurations) of a system into a small set based on their interaction (or combination). In practice, the input configurations of software systems are subjected to constraints, especially in case of highly configurable systems. To implement this feature within a strategy, many difficulties arise for construction. While there are many combinatorial interaction testing strategies nowadays, few of them support constraints. ObjectiveThis paper presents a new strategy, to construct combinatorial interaction test suites in the presence of constraints. MethodThe design and algorithms are provided in detail. To overcome the multi-judgement criteria for an optimal solution, the multi-objective particle swarm optimisation and multithreading are used. The strategy and its associated algorithms are evaluated extensively using different benchmarks and comparisons. ResultsOur results are promising as the evaluation results showed the efficiency and performance of each algorithm in the strategy. The benchmarking results also showed that the strategy can generate constrained test suites efficiently as compared to state-of-the-art strategies. ConclusionThe proposed strategy can form a new way for constructing of constrained combinatorial interaction test suites. The strategy can form a new and effective base for future implementations.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {20–36},
numpages = {17},
keywords = {Constrained combinatorial interaction, Multi-objective particle swarm optimisation, Search-based software engineering, Test case design techniques, Test generation tools}
}

@article{10.1016/j.cor.2009.11.001,
author = {Ribas, Imma and Leisten, Rainer and Frami\~{n}an, Jose M.},
title = {Review: Review and classification of hybrid flow shop scheduling problems from a production system and a solutions procedure perspective},
year = {2010},
issue_date = {August, 2010},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {37},
number = {8},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2009.11.001},
doi = {10.1016/j.cor.2009.11.001},
abstract = {In this paper, an extensive review of recently published papers on hybrid flow shop (HFS) scheduling problems is presented. The papers are classified first according to the HFS characteristics and production limitations considered in the respective papers. This represents a new approach to the classification of papers in the HFS environment. Second, the papers have been classified according to the solution approach proposed. These two classification categories give a comprehensive overview on the state of the art of the problem and can guide the reader with respect to future research work.},
journal = {Comput. Oper. Res.},
month = aug,
pages = {1439–1454},
numpages = {16},
keywords = {Hybrid flow shop}
}

@article{10.1016/j.patcog.2014.11.007,
author = {Liu, Huawen and Ma, Zongjie and Zhang, Shichao and Wu, Xindong},
title = {Penalized partial least square discriminant analysis with ℓ 1 - norm for multi-label data},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2014.11.007},
doi = {10.1016/j.patcog.2014.11.007},
abstract = {Multi-label data are prevalent in real world. Due to its great potential applications, multi-label learning has now been receiving more and more attention from many fields. However, how to effectively exploit the correlations of variables and labels, and tackle the high-dimensional problems of data are two major challenging issues for multi-label learning. In this paper we make an attempt to cope with these two problems by proposing an effective multi-label learning algorithm. Specifically, we make use of the technique of partial least square discriminant analysis to identify a common latent space between the variable space and the label space of multi-label data. Moreover, considering the label space of the multi-label data is sparse, a l1-norm penalty is further performed to constrain the Y-loadings of the optimization problem of partial least squares, making them sparse. The merit of our method is that it can capture the correlations and perform dimension reduction at the same time. The experimental results conducted on eleven public data sets show that our method is promising and superior to the state-of-the-art multi-label classifiers in most cases. HighlightsA new and effective learning method for the multi-label data is proposed.The method exploits the sparse property of the label space fully.The method captures the correlations of the variables by using partial least squares discriminant analysis.Dimension reduction has also applied to the high-dimensional data.The sparse purpose is achieved by performing the l1-norm penalty.},
journal = {Pattern Recogn.},
month = may,
pages = {1724–1733},
numpages = {10},
keywords = {Dimension reduction, Discriminant analysis, Multi-label learning, Partial least squares, Sparse learning}
}

@inproceedings{10.1145/3386901.3388939,
author = {Rathore, Aditya Singh and Zhu, Weijin and Daiyan, Afee and Xu, Chenhan and Wang, Kun and Lin, Feng and Ren, Kui and Xu, Wenyao},
title = {SonicPrint: a generally adoptable and secure fingerprint biometrics in smart devices},
year = {2020},
isbn = {9781450379540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386901.3388939},
doi = {10.1145/3386901.3388939},
abstract = {The advent of smart devices has caused unprecedented security and privacy concerns to its users. Although the fingerprint technology is a go-to biometric solution in high-impact applications (e.g., smart-phone security, monetary transactions and international-border verification), the existing fingerprint scanners are vulnerable to spoofing attacks via fake-finger and cannot be employed across smart devices (e.g., wearables) due to hardware constraints. We propose SonicPrint that extends fingerprint identification beyond smartphones to any smart device without the need for traditional fingerprint scanners. SonicPrint builds on the fingerprint-induced sonic effect (FiSe) caused by a user swiping his fingertip on smart devices and the resulting property, i.e., different users' fingerprint would result in distinct FiSe. As the first exploratory study, extensive experiments verify the above property with 31 participants over four different swipe actions on five different types of smart devices with even partial fingerprints. SonicPrint achieves up to a 98% identification accuracy on smartphone and an equal-error-rate (EER) less than 3% for smartwatch and headphones. We also examine and demonstrate the resilience of SonicPrint against fingerprint phantoms and replay attacks. A key advantage of SonicPrint is that it leverages the already existing microphones in smart devices, requiring no hardware modifications. Compared with other biometrics including physiological patterns and passive sensing, SonicPrint is a low-cost, privacy-oriented and secure approach to identify users across smart devices of unique form-factors.},
booktitle = {Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services},
pages = {121–134},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {MobiSys '20}
}

@inproceedings{10.1145/1028976.1028992,
author = {Zhang, Charles and Jacobsen, Hans-Arno},
title = {Resolving feature convolution in middleware systems},
year = {2004},
isbn = {1581138318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1028976.1028992},
doi = {10.1145/1028976.1028992},
abstract = {Middleware provides simplicity and uniformity for the development of distributed applications. However, the modularity of the architecture of middleware is starting to disintegrate and to become complicated due to the interaction of too many orthogonal concerns imposed from a wide range of application requirements. This is not due to bad design but rather due to the limitations of the conventional architectural decomposition methodologies. We introduce the principles of horizontal decomposition (HD) which addresses this problem with a mixed-paradigm middleware architecture. HD provides guidance for the use of conventional decomposition methods to implement the core functionalities of middleware and the use of aspect orientation to address its orthogonal properties. Our evaluation of the horizontal decomposition principles focuses on refactoring major middleware functionalities into aspects in order to modularize and isolate them from the core architecture. New versions of the middleware platform can be created through combining the core and the flexible selection of middleware aspects such as IDL data types, the oneway invocation style, the dynamic messaging style, and additional character encoding schemes. As a result, the primary functionality of the middleware is supported with a much simpler architecture and enhanced performance. Moreover, customization and configuration of the middleware for a wide-range of requirements becomes possible.},
booktitle = {Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {188–205},
numpages = {18},
keywords = {aspect oriented middleware, middleware architecture},
location = {Vancouver, BC, Canada},
series = {OOPSLA '04}
}

@article{10.1016/j.scico.2010.11.013,
author = {Lincke, Jens and Appeltauer, Malte and Steinert, Bastian and Hirschfeld, Robert},
title = {An open implementation for context-oriented layer composition in ContextJS},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {76},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2010.11.013},
doi = {10.1016/j.scico.2010.11.013},
abstract = {Context-oriented programming (COP) provides dedicated support for defining and composing variations to a basic program behavior. A variation, which is defined within a layer, can be de-/activated for the dynamic extent of a code block. While this mechanism allows for control flow-specific scoping, expressing behavior adaptations can demand alternative scopes. For instance, adaptations can depend on dynamic object structure rather than control flow. We present scenarios for behavior adaptation and identify the need for new scoping mechanisms. The increasing number of scoping mechanisms calls for new language abstractions representing them. We suggest to open the implementation of scoping mechanisms so that developers can extend the COP language core according to their specific needs. Our open implementation moves layer composition into objects to be affected and with that closer to the method dispatch to be changed. We discuss the implementation of established COP scoping mechanisms using our approach and present new scoping mechanisms developed for our enhancements to Lively Kernel.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {1194–1209},
numpages = {16},
keywords = {Context-oriented programming, ContextJS, Dynamic adaptation, Open implementations, Scope}
}

@inproceedings{10.1007/978-3-642-39038-8_28,
author = {Auerbach, Josh and Bacon, Dave F. and Cheng, Perry and Fink, Steve and Rabbah, Rodric},
title = {The shape of things to run: compiling complex stream graphs to reconfigurable hardware in lime},
year = {2013},
isbn = {9783642390371},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39038-8_28},
doi = {10.1007/978-3-642-39038-8_28},
abstract = {Reconfigurable hardware can deliver impressive performance for some applications, when a highly static hardware design closely matches application logic. Obliged to express efficient static hardware structures, hardware designers cannot currently employ abstractions using dynamic features of modern programming languages.We present the design and implementation of new features in the Lime programming language that admit construction of stream graphs of arbitrary shape using the expressive power of an imperative, object-oriented language. The Lime programmer marks computations destined for hardware, and the compiler statically checks these computations for repeatable structure. If the check succeeds, the system guarantees it can extract the static structure needed for hardware synthesis.We describe the language design in detail and present case studies of 10 Lime benchmarks, each successfully synthesized to a Xilinx Virtex 5 FPGA.},
booktitle = {Proceedings of the 27th European Conference on Object-Oriented Programming},
pages = {679–706},
numpages = {28},
location = {Montpellier, France},
series = {ECOOP'13}
}

@inproceedings{10.5555/2813767.2813787,
author = {Kaestle, Stefan and Achermann, Reto and Roscoe, Timothy and Harris, Tim},
title = {Shoal: smart allocation and replication of memory for parallel programs},
year = {2015},
isbn = {9781931971225},
publisher = {USENIX Association},
address = {USA},
abstract = {Modern NUMA multi-core machines exhibit complex latency and throughput characteristics, making it hard to allocate memory optimally for a given program's access patterns. However, sub-optimal allocation can significantly impact performance of parallel programs.We present an array abstraction that allows data placement to be automatically inferred from program analysis, and implement the abstraction in Shoal, a runtime library for parallel programs on NUMA machines. In Shoal, arrays can be automatically replicated, distributed, or partitioned across NUMA domains based on annotating memory allocation statements to indicate access patterns. We further show how such annotations can be automatically provided by compilers for high-level domainspecific languages (for example, the Green-Marl graph language). Finally, we show how Shoal can exploit additional hardware such as programmable DMA copy engines to further improve parallel program performance.We demonstrate significant performance benefits from automatically selecting a good array implementation based on memory access patterns and machine characteristics. We present two case-studies: (i) Green-Marl, a graph analytics workload using automatically annotated code based on information extracted from the high-level program and (ii) a manually-annotated version of the PARSEC Streamcluster benchmark.},
booktitle = {Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference},
pages = {263–276},
numpages = {14},
location = {Santa Clara, CA},
series = {USENIX ATC '15}
}

@article{10.1016/j.jss.2016.03.068,
author = {Triantafyllidis, Konstantinos and Aslam, Waqar and Bondarev, Egor and Lukkien, Johan J. and de With, Peter H.N.},
title = {ProMARTES},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.068},
doi = {10.1016/j.jss.2016.03.068},
abstract = {Cycle-accurate performance analysis open-source tool for CBRTDS.Fusion of network and processing analysis methods.Support both analytical and simulation analysis techniques.Application of the proposed framework to an autonomously navigating robot system.Proposal of an effective combination of scheduling and simulation analysis techniques. This paper proposes a cycle-accurate performance analysis method for real-time component-based distributed systems (CB-RTDS). The method involves the following phases: (a) profiling SW components at cycle execution level and modeling the obtained performance measurements in MARTE-compatible component resource models, (b) guided composition of the system architecture from available SW and HW components, (c) automated generation of a system model, specifying both computation and network loads, and (d) performance analysis (scheduling, simulation and network analysis) of the composed system model. The method is demonstrated for a real-world case study of 3 autonomously navigating robots with advanced sensing capabilities. The case study is challenging because of the SW/HW mapping, real-time requirements and data synchronization among multiple nodes. This case-study proved that, thanks to the adopted low-level performance metrics, we are able to obtain accurate performance predictions of both computation and network delays. Moreover, the combination of analytical and simulation analysis methods enables the computation of both the guaranteed Worst Case Execution Time (WCET) and the detailed execution time-line data for real-time tasks. As a result, the analysis yields the identification of an optimal architecture, with respect to real-time deadlines, robustness and system costs. The paper main contributions are the cycle-accurate performance analysis workflow and supportive open-source ProMARTES tool-chain, both incorporating a network prediction model in all the performance analysis phases.},
journal = {J. Syst. Softw.},
month = jul,
pages = {450–470},
numpages = {21},
keywords = {Analysis, Distributed, Performance, Real-time, Scheduling, Simulation}
}

@article{10.1007/s10922-007-9083-8,
author = {Boutaba, Raouf and Aib, Issam},
title = {Policy-based Management: A Historical Perspective},
year = {2007},
issue_date = {Dec 2007},
publisher = {Plenum Press},
address = {USA},
volume = {15},
number = {4},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-007-9083-8},
doi = {10.1007/s10922-007-9083-8},
abstract = {This paper traces the history of policy-based management and how it evolved from the first security models dating back to the late 1960's until today's more elaborate frameworks, languages, and policy-based management tools. The focus will be on providing a synthesized chronicle of the evolution of ideas and research trends rather than on surveying the various specification formalisms, frameworks, and application domains of policy-based management.},
journal = {J. Netw. Syst. Manage.},
month = dec,
pages = {447–480},
numpages = {34},
keywords = {Policy history, Policy-based management, Policy-based networking}
}

@article{10.1145/2533685,
author = {Stol, Klaas-Jan and Avgeriou, Paris and Babar, Muhammad Ali and Lucas, Yan and Fitzgerald, Brian},
title = {Key factors for adopting inner source},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2533685},
doi = {10.1145/2533685},
abstract = {A number of organizations have adopted Open Source Software (OSS) development practices to support or augment their software development processes, a phenomenon frequently referred to as Inner Source. However the adoption of Inner Source is not a straightforward issue. Many organizations are struggling with the question of whether Inner Source is an appropriate approach to software development for them in the first place. This article presents a framework derived from the literature on Inner Source, which identifies nine important factors that need to be considered when implementing Inner Source. The framework can be used as a probing instrument to assess an organization on these nine factors so as to gain an understanding of whether or not Inner Source is suitable. We applied the framework in three case studies at Philips Healthcare, Neopost Technologies, and Rolls-Royce, which are all large organizations that have either adopted Inner Source or were planning to do so. Based on the results presented in this article, we outline directions for future research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {18},
numpages = {35},
keywords = {Case study, framework, inner source, open-source development practices}
}

@article{10.1177/1094342017718068,
author = {Videau, Brice and Pouget, Kevin and Genovese, Luigi and Deutsch, Thierry and Komatitsch, Dimitri and Desprez, Fr\'{e}d\'{e}ric and M\'{e}haut, Jean-Fran\c{c}ois},
title = {BOAST},
year = {2018},
issue_date = {1 2018},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {32},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342017718068},
doi = {10.1177/1094342017718068},
abstract = {The portability of real high-performance computing HPC applications on new platforms is an open and very delicate problem. Especially, the performance portability of the underlying computing kernels is problematic as they need to be tuned for each and every platform the application encounters. This article presents BOAST, a metaprogramming framework dedicated to computing kernels. BOAST allows the description of a kernel and its possible optimizations using a domain-specific language. BOAST runtime will then compare the different versions'performance as well as verify their exactness. BOAST is applied to three use cases: a Laplace kernel in OpenCL and two HPC applications BigDFT electronic density computation and SPECFEM3D seismic and wave propagation.},
journal = {Int. J. High Perform. Comput. Appl.},
month = jan,
pages = {28–44},
numpages = {17},
keywords = {Code generation, autotuning, genericity, high-performance computing, nonregression, portability, productivity and software design, testing}
}

@article{10.1007/s10796-021-10220-x,
author = {Tabim, Ver\^{o}nica Maurer and Ayala, N\'{e}stor Fabi\'{a}n and Frank, Alejandro G.},
title = {Implementing Vertical Integration in the Industry 4.0 Journey: Which Factors Influence the Process of Information Systems Adoption?},
year = {2021},
issue_date = {Oct 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-021-10220-x},
doi = {10.1007/s10796-021-10220-x},
abstract = {One of the key principles of Industry 4.0 is the implementation of vertical integration, which considers the integration of information systems from different hierarchical levels in a company to support decision-making with real-time data flows. Companies face challenges when they want to implement vertical integration, which is not trivial due to the risks inherent to the decision stages of adoption. We investigate the main factors influencing the different stages of adoption of vertical integration to provide a clearer view of what managers should consider at each stage. We adopt a multi-case study approach based on the investigation of ten companies that followed this adoption process. We develop a framework with 22 factors deployed in the three stages of decision (knowledge, persuasion, and final decision) and three main dimensions of analysis: technology, organization, and environment. We analyze the potential tensions between these factors and show how managers should balance such factors during the decision stages.},
journal = {Information Systems Frontiers},
month = nov,
pages = {1615–1632},
numpages = {18},
keywords = {Industry 4.0, Vertical integration, Information systems, Smart Manufacturing, Technology adoption}
}

@article{10.1007/s00607-013-0338-9,
author = {Bertolino, Antonia and Inverardi, Paola and Muccini, Henry},
title = {Software architecture-based analysis and testing: a look into achievements and future challenges},
year = {2013},
issue_date = {August    2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {95},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-013-0338-9},
doi = {10.1007/s00607-013-0338-9},
journal = {Computing},
month = aug,
pages = {633–648},
numpages = {16}
}

@inproceedings{10.5555/2666795.2666811,
author = {Weyns, Danny and Iftikhar, M. Usman and Malek, Sam and Andersson, Jesper},
title = {Claims and supporting evidence for self-adaptive systems: a literature study},
year = {2012},
isbn = {9781467317870},
publisher = {IEEE Press},
abstract = {Despite the vast body of work on self-adaption, no systematic study has been performed on the claims associated with self-adaptation and the evidence that exists for these claims. As such an insight is crucial for researchers and engineers, we performed a literature study of the research results from SEAMS since 2006 and the associated Dagstuhl seminar in 2008. The study shows that the primary claims of self-adaptation are improved flexibility, reliability, and performance of the system. On the other hand, the tradeoffs implied by self-adaptation have not received much attention. Evidence is obtained from basic examples, or simply lacking. Few systematic empirical studies have been performed, and no industrial evidence is reported. From the study, we offer the following recommendations to move the field forward: to improve evaluation, researchers should make their assessment methods, tools and data publicly available; to deal with poor discussion of limitations, conferences/workshops should require an explicit section on limitations in engineering papers; to improve poor treatment of tradeoffs, this aspect should be an explicit subject of reviews; and finally, to enhance industrial validation, the best academy-industry efforts could be formally recognized by the community.},
booktitle = {Proceedings of the 7th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {89–98},
numpages = {10},
location = {Zurich, Switzerland},
series = {SEAMS '12}
}

@inproceedings{10.5555/3049877.3049879,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {23–37},
numpages = {15},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.5555/1929101.1929110,
author = {Morin, Brice and Klein, Jacques and Kienzle, J\"{o}rg and J\'{e}z\'{e}quel, Jean-Marc},
title = {Flexible model element introduction policies for aspect-oriented modeling},
year = {2010},
isbn = {3642161286},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Aspect-Oriented Modeling techniques make it possible to use model transformation to achieve advanced separation of concerns within models. Applying aspects that introduce model elements into a base model in the context of large, potentially composite models is nevertheless tricky: when a pointcut model matches several join points within the base model, it is not clear whether the introduced element should be instantiated once for each match, once within each composite, once for the whole model, or based on a more elaborate criteria. This paper argues that in order to enable a modeler to write semantically correct aspects for large, composite models, an aspect weaver must support a flexible instantiation policy for model element introduction. Example models highlighting the need for such a mechanism are shown, and details of how such policies can be implemented are presented.},
booktitle = {Proceedings of the 13th International Conference on Model Driven Engineering Languages and Systems: Part II},
pages = {63–77},
numpages = {15},
location = {Oslo, Norway},
series = {MODELS'10}
}

@book{10.5555/2821575,
author = {Nielsen, Jakob},
title = {Usability Engineering},
year = {1994},
isbn = {9780080520292},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Written by the author of the best-selling HyperText &amp; HyperMedia, this book is an excellent guide to the methods of usability engineering. The book provides the tools needed to avoid usability surprises and improve product quality. Step-by-step information on which method to use at various stages during the development lifecycle are included, along with detailed information on how to run a usability test and the unique issues relating to international usability. * Emphasizes cost-effective methods that developers can implement immediately * Instructs readers about which methods to use when, throughout the development lifecycle, which ultimately helps in cost-benefit analysis. * Shows readers how to avoid the four most frequently listed reasons for delay in software projects. * Includes detailed information on how to run a usability test. * Covers unique issues of international usability. * Features an extensive bibliography allowing readers to find additional information. * Written by an internationally renowned expert in the field and the author of the best-selling HyperText &amp; HyperMedia. Table of Contents Executive Summary. What is Usability Generations of User Interfaces. The Usability Engineering Lifecycle. Usability Heuristics. Usability Testing. Usability Assessment Methods Beyond Testing. Interface Standards. International User Interfaces. Future Developments. Appendix A: Exercises. Appendix B: Bibliography. Author Index. Subject Index.}
}

@inproceedings{10.5555/3291291.3291314,
author = {Wehling, Kenny and Wille, David and Seidl, Christoph and Schaefer, Ina},
title = {Reducing variability of technically related software systems in large-scale IT landscapes},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {The number of software systems in a company typically grows with the business requirements. Therefore, IT landscapes in large companies can consist of hundreds or thousands of different software systems. As the evolution of such large-scale landscapes is often uncoordinated, they commonly comprise different groups of related software systems using a common core technology (e.g., Java Web-Application) implemented by a variety of architectural components (e.g., different application servers or databases). This leads to increased costs and higher effort for maintaining and evolving these software systems and the entire IT landscape. To alleviate these problems, the variability of such technically related software systems has to be reduced. For this purpose, experts have to assess and evaluate restructuring potentials in order to take appropriate restructuring decisions. As a manual analysis requires high effort and is not feasible for large-scale IT landscapes, experts face a major challenge. To overcome this challenge, we introduce a novel approach to automatically support experts in taking reasonable restructuring decisions. By providing automated methods for assessing, evaluating and simulating restructuring potentials, experts are capable of reducing the variability of related software systems in large-scale IT landscapes. We show suitability of our approach by expert interviews and an industrial case study with architectures of real-world software systems.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {224–235},
numpages = {12},
keywords = {restructuring, software systems, technology architecture, variability},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3442391.3442396,
author = {Sobernig, Stefan and Lessenich, Olaf},
title = {v1e: A Kernel for Domain-specific Textual Variability Modelling Languages},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442396},
doi = {10.1145/3442391.3442396},
abstract = {v1e is a language kernel for textual variability modelling built on top of the language-development system DjDSL. As a language kernel, v1e provides a minimal but extensible set of abstractions to implement language families for textual variability modelling. v1e provides for a small and versatile abstract syntax to encode variability models using multiplicity constraints and canonical semantics. v1e offers built-in analysis support via BDDs, such as configuration validation. A derived language becomes realised as a collection of extensions dependent on the language kernel. We showcase the design and implementation of a v1e-based implementation of TVL. We conclude the paper by pointing out current limitations (e.g., representing attributed variability models) and future directions (e.g., analysis support beyond BDD).},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {4},
numpages = {7},
keywords = {domain-specific modelling, language family, language kernel, language product line, modelling framework, variability modelling},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {Design smell detection, Machine learning, Software metrics, Project context information, God class}
}

@article{10.1145/3038926,
author = {Soetens, Quinten David and Robbes, Romain and Demeyer, Serge},
title = {Changes as First-Class Citizens: A Research Perspective on Modern Software Tooling},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3038926},
doi = {10.1145/3038926},
abstract = {Software must evolve to keep up with an ever-changing context, the real world. We discuss an emergent trend in software evolution research revolving around the central notion that drives evolution: Change. By reifying change, and by modelling it as a first-class entity, researchers can now analyse the complex phenomenon known as software evolution with an unprecedented degree of accuracy. We present a Systematic Mapping Study of 86 articles to give an overview on the state of the art in this area of research and present a roadmap with open issues and future directions.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {18},
numpages = {38},
keywords = {Systematic mapping study, atomic change operations, change distilling, change recording, fine-grained changes, fine-grained edit operations}
}

@article{10.1007/s10270-020-00831-4,
author = {Klikovits, Stefan and Buchs, Didier},
title = {Pragmatic reuse for DSML development: Composing a DSL for hybrid CPS modeling},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00831-4},
doi = {10.1007/s10270-020-00831-4},
abstract = {By bridging the semantic gap, domain-specific language (DSLs) serve an important role in the conquest to allow domain experts to model their systems themselves. In this publication we present a case study of the development of the Continuous REactive SysTems language (CREST), a DSL for hybrid systems modeling. The language focuses on the representation of continuous resource flows such as water, electricity, light or heat. Our methodology follows a very pragmatic approach, combining the syntactic and semantic principles of well-known modeling means such as hybrid automata, data-flow languages and architecture description languages into a coherent language. The borrowed aspects have been carefully combined and formalised in a well-defined operational semantics. The DSL provides two concrete syntaxes: CREST diagrams, a graphical language that is easily understandable and serves as a model basis, and crestdsl, an internal DSL implementation that supports rapid prototyping—both are geared towards usability and clarity. We present the DSL’s semantics, which thoroughly connect the various language concerns into an executable formalism that enables sound simulation and formal verification in crestdsl, and discuss the lessons learned throughout the project.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {837–866},
numpages = {30},
keywords = {Cyber-physical systems, Domain-specific language, Modeling, Simulation, Verification}
}

@inbook{10.5555/2172290.2172300,
author = {Lohmann, Daniel and Spinczyk, Olaf and Schr\"{o}der-Preikschat, Wolfgang},
title = {Lean and efficient system software product lines: where aspects beat objects},
year = {2006},
isbn = {3540488901},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software development in the domain of embedded and deeply embedded systems is dominated by cost pressure and extremely limited hardware resources. As a result, modern concepts for separation of concerns and software reuse are widely ignored, as developers worry about the thereby induced memory and performance overhead. Especially object-oriented programming (OOP) is still little in demand. For the development of highly configurable fine-grained system software product lines, however, separation of concerns (SoC) is a crucial property. As the overhead of object-orientation is not acceptable in this domain, we propose aspect-oriented programming (AOP) as an alternative. Compared to OOP, AOP makes it possible to reach similar or even better separation of concerns with significantly smaller memory footprints. In a case study for an embedded system product line the memory costs for SoC could be reduced from 148–236% to 2–10% by using AOP instead of OOP.},
booktitle = {Transactions on Aspect-Oriented Software Development II},
pages = {227–255},
numpages = {29}
}

@article{10.5555/2773202.2773211,
author = {\v{S}tuikys, Vytautas and Dama\v{s}evi\v{c}ius, Robertas},
title = {Equivalent Transformations of Heterogeneous Meta-Programs},
year = {2013},
issue_date = {April 2013},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {2},
issn = {0868-4952},
abstract = {We consider a generalization of heterogeneous meta-programs by (1) introducing an extra level of abstraction within the meta-program structure, and (2) meta-program transformations. We define basic terms, formalize transformation tasks, consider properties of meta-program transformations and rules to manage complexity through the following transformation processes: (1) reverse transformation, when a correct one-stage meta-program M1 is transformed into the equivalent two-stage meta-meta-program M2; (2) two-stage forward transformations, when M2 is transformed into a set of meta-programs, and each meta-program is transformed into a set of target programs. The results are as follows: (a) formalization of the transformation processes within the heterogeneous meta-programming paradigm; (b) introduction and approval of equivalent transformations of meta-programs into meta-meta-programs and vice versa; (c) introduction of metrics to evaluate complexity of meta-specifications. The results are approved by examples, theoretical reasoning and experiments.},
journal = {Informatica},
month = apr,
pages = {315–337},
numpages = {23},
keywords = {Generalization, Meta-Program Complexity, Meta-Programming, Transformation}
}

@proceedings{10.1145/2998407,
title = {ITSLE 2016: Proceedings of the 1st Industry Track on Software Language Engineering},
year = {2016},
isbn = {9781450346467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1007/s11704-013-3094-6,
author = {Millo, Jean-Vivien and Mallet, Fr\'{e}d\'{e}ric and Coadou, Anthony and Ramesh, S.},
title = {Scenario-based verification in presence of variability using a synchronous approach},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {7},
number = {5},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-013-3094-6},
doi = {10.1007/s11704-013-3094-6},
abstract = {This paper presents a new model of scenarios, dedicated to the specification and verification of system behaviours in the context of software product lines (SPL). We draw our inspiration from some techniques that are mostly used in the hardware community, and we show how they could be applied to the verification of software components. We point out the benefits of synchronous languages and models to bridge the gap between both worlds.},
journal = {Front. Comput. Sci.},
month = oct,
pages = {650–672},
numpages = {23},
keywords = {Esterel, UML MARTE, feature interaction, scenario, variability, verification}
}

@article{10.1016/j.jss.2015.11.025,
author = {van Angeren, Joey and Alves, Carina and Jansen, Slinger},
title = {Can we ask you to collaborate? Analyzing app developer relationships in commercial platform ecosystems},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.11.025},
doi = {10.1016/j.jss.2015.11.025},
abstract = {Comparative case studies of commercial platform ecosystems of Google and Microsoft.There is substantial variation in the network topology of the ecosystems studied.Lower entry barriers will be related to an increase in the number of complementors.The use of a partnership model will be related to greater network density.Customer demand for related applications will be related to greater network density. Previous studies have emphasized the necessity for software platform owners to govern their platform ecosystem in order to create durable opportunities for themselves and the app developers that surround the platform. To date, platform ecosystems have been widely analyzed from the perspective of platform owners. However, how and to what extent app developers collaborate with their peers needs to be investigated further. In this article, we study the interfirm relationships among app developers in commercial platform ecosystems and explore the causes of variation in the network structure of these ecosystems. By means of a comparative study of four commercial platform ecosystems of Google (Google Apps and Google Chrome) and Microsoft (Microsoft Office365 and Internet Explorer), we illustrate substantial variation in the extent to which app developers initiated interfirm relationships. Further, we analyze how the degree of enforced entry barriers to the app store, the use of a partnership model, and the domain of the software platform that underpins the ecosystem affect the properties of these commercial platform ecosystems. We present subsequent explanations as a set of propositions that can be tested in future empirical research.},
journal = {J. Syst. Softw.},
month = mar,
pages = {430–445},
numpages = {16},
keywords = {Case study, Interfirm network analysis, Software ecosystem}
}

@article{10.1016/j.asoc.2009.11.009,
author = {Avigad, Gideon and Moshaiov, Amiram},
title = {Simultaneous concept-based evolutionary multi-objective optimization},
year = {2011},
issue_date = {January, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {11},
number = {1},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2009.11.009},
doi = {10.1016/j.asoc.2009.11.009},
abstract = {In contrast to traditional multi-objective problems the concept-based version of such problems involves sets of particular solutions, which represent predefined conceptual solutions. This paper addresses the concept-based multi-objective problem by proposing two novel multi objective evolutionary algorithms. It also compares two major search approaches.The suggested algorithms deal with resource sharing among concepts, and within each concept, while simultaneously evolving concepts towards a Pareto front by way of their representing sets. The introduced algorithms, which use a simultaneous search approach, are compared with a sequential one. For this purpose concept-based performance indicators are suggested and used. The comparison study includes both the computational time and the quality of the concept-based front representation. Finally, the effect on the computational time of both the concept fitness evaluation time and concept optimality, for both the sequential and simultaneous approaches, is highlighted.},
journal = {Appl. Soft Comput.},
month = jan,
pages = {193–207},
numpages = {15},
keywords = {Co-evolution, Competing sub-populations, Conceptual design, Crowding, Elitism, Engineering design, Fitness sharing, Multi-objective optimization, Niching, Parallel GA, Species}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Adaptive Boosting, Cyber security, Ensemble Learning, Extreme Learning Machine, Fractional Polynomial Kernels, KDD Cup 1999, Kernel Selection, Machine Learning, Multiclass Classification, Multiple Kernel Learning, Network Intrusion Detection}
}

@article{10.1016/j.jss.2010.09.042,
author = {Rabiser, Rick and O'Leary, P\'{a}draig and Richardson, Ita},
title = {Key activities for product derivation in software product lines},
year = {2011},
issue_date = {February, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.09.042},
doi = {10.1016/j.jss.2010.09.042},
abstract = {More and more organizations adopt software product lines to leverage extensive reuse and deliver a multitude of benefits such as increased quality and productivity and a decrease in cost and time-to-market of their software development. When compared to the vast amount of research on developing product lines, relatively little work has been dedicated to the actual use of product lines to derive individual products, i.e., the process of product derivation. Existing approaches to product derivation have been developed independently for different aims and purposes. While the definition of a general approach applicable to every domain may not be possible, it would be interesting for researchers and practitioners to know which activities are common in existing approaches, i.e., what are the key activities in product derivation. In this paper we report on how we compared two product derivation approaches developed by the authors in two different, independent research projects. Both approaches independently sought to identify product derivation activities, one through a process reference model and the other through a tool-supported derivation approach. Both approaches have been developed and validated in research industry collaborations with different companies. Through the comparison of the approaches we identify key product derivation activities. We illustrate the activities' importance with examples from industry collaborations. To further validate the activities, we analyze three existing product derivation approaches for their support for these activities. The validation provides evidence that the identified activities are relevant to product derivation and we thus conclude that they should be considered (e.g., as a checklist) when developing or evaluating a product derivation approach.},
journal = {J. Syst. Softw.},
month = feb,
pages = {285–300},
numpages = {16},
keywords = {Process, Product derivation, Software product lines}
}

@inproceedings{10.1145/1988008.1988028,
author = {Kattepur, Ajay and Sen, Sagar and Baudry, Benoit and Benveniste, Albert and Jard, Claude},
title = {Pairwise testing of dynamic composite services},
year = {2011},
isbn = {9781450305754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988008.1988028},
doi = {10.1145/1988008.1988028},
abstract = {Online services encapsulate enterprises, people, software systems and often operate in poorly understood environments. Using such services in tandem to predictably orchestrate a complex task is one of the principal challenges of service-oriented computing. A composite service orchestration soliciting multiple atomic services is plagued by a number of sources of variation. For instance, availability of an atomic service and its response time are two important sources of variation. Moreover, the number of possible variations in a composite service increases exponentially with increase in the number of atomic services. Testing such a composite service presents a crucial challenge as its often very expensive to exhaustively examine the variation space. Can we effectively test the dynamic behavior of a composite service using only a subset of these variations? This is the question that intrigues us. In this paper, we first model composite service variability as a feature diagram (FD) that captures all valid configurations of its orchestration. Second, we apply pairwise testing to sample the set of all possible configurations to obtain a concise subset. Finally, we test the composite service for selected pairwise configurations for a variety of QoS metrics such as response time, data quality, and availability. Using two case studies, Car crash crisis management and eHealth management, we demonstrate that pairwise generation effectively samples the full range of QoS variations in a dynamic orchestration. The pairwise sampling technique eliminates over 99% redundancy in configurations, while still calling all atomic services at least once. We rigorously evaluate pairwise testing for the criteria such as: a) ability to sample the extreme QoS metrics of the service b) stable behavior of the extracted configurations c) compact set of configurations that can help evaluate QoS tradeoffs and d) comparison with random sampling.},
booktitle = {Proceedings of the 6th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {138–147},
numpages = {10},
keywords = {pairwise testing, qos, web services},
location = {Waikiki, Honolulu, HI, USA},
series = {SEAMS '11}
}

@article{10.1016/j.jvlc.2007.07.003,
author = {Gokhale, Aniruddha and Kaul, Dimple and Kogekar, Arundhati and Gray, Jeff and Gokhale, Swapna},
title = {POSAML: A visual modeling language for middleware provisioning},
year = {2007},
issue_date = {August, 2007},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {18},
number = {4},
issn = {1045-926X},
url = {https://doi.org/10.1016/j.jvlc.2007.07.003},
doi = {10.1016/j.jvlc.2007.07.003},
abstract = {Next generation distributed applications are often hosted on heterogeneous platforms including different kinds of middleware. Due to the applications' growing functional complexity and their multiple quality of service (QoS) requirements, system developers are increasingly facing a substantial number of middleware provisioning challenges, which include configuring, optimizing and validating the middleware platforms for QoS properties. Traditional techniques for middleware provisioning tend to use non-intuitive, low-level and technology-specific approaches, which are tedious, error prone, and non-reusable across different technologies. Quite often the middleware provisioning activities are carried out by different actors without much interaction among them, which results in an iterative trial-and-error process to provisioning. Higher level abstractions, particularly those that use visual models, are effective in addressing these challenges. This paper describes the design of a visual modeling language called POSAML (pattern-oriented software architecture modeling language) and associated tools that provide an intuitive, higher level and unified framework for provisioning middleware platforms. POSAML provides visual modeling capabilities for middleware-independent configurations and optimizations while enabling automated middleware-specific validation of system QoS properties.},
journal = {J. Vis. Lang. Comput.},
month = aug,
pages = {359–377},
numpages = {19},
keywords = {Generative tools, Model-driven engineering, Visual domain-specific modeling languages}
}

@inproceedings{10.1145/2162049.2162072,
author = {Salvaneschi, Guido and Ghezzi, Carlo and Pradella, Matteo},
title = {ContextErlang: introducing context-oriented programming in the actor model},
year = {2012},
isbn = {9781450310925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2162049.2162072},
doi = {10.1145/2162049.2162072},
abstract = {Self-adapting systems are becoming widespread in emerging fields such as autonomic, mobile and ubiquitous computing. Context-oriented programming (COP) is a promising language-level solution for the implementation of context-aware, self-adaptive software. However, current COP approaches struggle to effectively manage the asynchronous nature of context provisioning. We argue that, to solve these issues, COP features should be designed to fit nicely in the concurrency model supported by the language. This work presents the design rationale of ContextErlang, which introduces COP in the Actor Model. We provide evidence that ContextErlang constitutes a viable solution to implement context-aware software in a highly concurrent and distributed setting. We discuss a case study and an evaluation of run-time performance.},
booktitle = {Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development},
pages = {191–202},
numpages = {12},
keywords = {Erlang, OTP platform, context-oriented programming, self-adaptive software},
location = {Potsdam, Germany},
series = {AOSD '12}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@article{10.1007/s10845-021-01817-9,
author = {Kang, SungKu and Jin, Ran and Deng, Xinwei and Kenett, Ron S.},
title = {Challenges of modeling and analysis in cybermanufacturing: a review from a machine learning and computation perspective},
year = {2021},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {2},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01817-9},
doi = {10.1007/s10845-021-01817-9},
abstract = {In Industry 4.0, smart manufacturing is facing its next stage, cybermanufacturing, founded upon advanced communication, computation, and control infrastructure. Cybermanufacturing will unleash the potential of multi-modal manufacturing data, and provide a new perspective called computation service, as a part of service-oriented architecture (SOA), where on-demand computation requests throughout manufacturing operations are seamlessly satisfied by data analytics and machine learning. However, the complexity of information technology infrastructure leads to fundamental challenges in modeling and analysis under cybermanufacturing, ranging from information-poor datasets to a lack of reproducibility of analytical studies. Nevertheless, existing reviews have focused on the overall architecture of cybermanufacturing/SOA or its technical components (e.g., communication protocol), rather than the potential bottleneck of computation service with respect to modeling and analysis. In this paper, we review the fundamental challenges with respect to modeling and analysis in cybermanufacturing. Then, we introduce the existing efforts in computation pipeline recommendation, which aims at identifying an optimal sequence of method options for data analytics/machine learning without time-consuming trial-and-error. We envision computation pipeline recommendation as a promising research field to address the fundamental challenges in cybermanufacturing. We also expect that computation pipeline recommendation can be a driving force to flexible and resilient manufacturing operations in the post-COVID-19 industry.},
journal = {J. Intell. Manuf.},
month = aug,
pages = {415–428},
numpages = {14},
keywords = {Computation pipelines, Cybermanufacturing, Industry 4.0, Machine learning, Manufacturing modeling and analysis}
}

@inproceedings{10.5555/1366804.1366902,
author = {Zhang, Rui and Oliveira, Bruno C. d. S. and Bivens, Alan and McKeever, Steve},
title = {Scalable problem localization for distributed systems: principles and practices},
year = {2007},
isbn = {9781595937575},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Problem localization is a critical part of providing crucial system management capabilities to modern distributed environments. One key open challenge is for problem localization solutions to scale for systems containing hundreds or even thousands of nodes, whilst still remaining fast enough to respond to rapid environment changes and sufficiently cost-effective to avoid overloading any management or application component. This paper meets the challenge by introducing two scalable frameworks applicable to a wide range of existing problem localization solutions: one based on a summary-driven, narrow-down procedure, the other through decomposing and decentralizing the problem localization process. Both frameworks, at their best, are able to achieve O(logN) problem localization time and O(1) per node communication load. The contrasting natures of both frameworks provide them with complimentary strengths that make them suitable for different scenarios in practice. We demonstrate our approaches in simulation settings and two real-world environments and show promising scalability benefits that can make a difference in system management operations.},
booktitle = {Proceedings of the 2nd International Conference on Scalable Information Systems},
articleno = {76},
numpages = {8},
keywords = {complexity, decentralization, distributed systems, hierarchy, problem localization, scalability},
location = {Suzhou, China},
series = {InfoScale '07}
}

@article{10.1007/s10664-018-9670-1,
author = {Ali, Nauman Bin and Engstr\"{o}m, Emelie and Taromirad, Masoumeh and Mousavi, Mohammad Reza and Minhas, Nasir Mehmood and Helgesson, Daniel and Kunze, Sebastian and Varshosaz, Mahsa},
title = {On the search for industry-relevant regression testing research},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9670-1},
doi = {10.1007/s10664-018-9670-1},
abstract = {Regression testing is a means to assure that a change in the software, or its execution environment, does not introduce new defects. It involves the expensive undertaking of rerunning test cases. Several techniques have been proposed to reduce the number of test cases to execute in regression testing, however, there is no research on how to assess industrial relevance and applicability of such techniques. We conducted a systematic literature review with the following two goals: firstly, to enable researchers to design and present regression testing research with a focus on industrial relevance and applicability and secondly, to facilitate the industrial adoption of such research by addressing the attributes of concern from the practitioners' perspective. Using a reference-based search approach, we identified 1068 papers on regression testing. We then reduced the scope to only include papers with explicit discussions about relevance and applicability (i.e. mainly studies involving industrial stakeholders). Uniquely in this literature review, practitioners were consulted at several steps to increase the likelihood of achieving our aim of identifying factors important for relevance and applicability. We have summarised the results of these consultations and an analysis of the literature in three taxonomies, which capture aspects of industrial-relevance regarding the regression testing techniques. Based on these taxonomies, we mapped 38 papers reporting the evaluation of 26 regression testing techniques in industrial settings.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2020–2055},
numpages = {36},
keywords = {Industrial relevance, Recommendations, Regression testing, Systematic literature review, Taxonomy}
}

@article{10.1016/j.jnca.2017.12.001,
author = {Dias de Assuno, Marcos and da Silva Veith, Alexandre and Buyya, Rajkumar},
title = {Distributed data stream processing and edge computing},
year = {2018},
issue_date = {February 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {103},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2017.12.001},
doi = {10.1016/j.jnca.2017.12.001},
abstract = {Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructure, wearable assistance, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and efficient manner. More recently, architecture has been proposed to use edge computing for data stream processing. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to fluctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make efficient resource management decisions based on current load. Elasticity becomes even more challenging in highly distributed environments comprising edge and cloud computing resources. This work examines some of these challenges and discusses solutions proposed in the literature to address them. HighlightsThe paper surveys state of the art on stream processing engines and mechanisms.The work describes how existing solutions exploit resource elasticity features of cloud computing in stream processing.It presents a gap analysis and future directions on stream processing on heterogeneous environments.},
journal = {J. Netw. Comput. Appl.},
month = feb,
pages = {1–17},
numpages = {17},
keywords = {Big Data, Cloud computing, Resource elasticity, Stream processing}
}

@inproceedings{10.1109/MEMCOD.2015.7340482,
title = {On the deployment problem of embedded systems},
year = {2015},
isbn = {9781509002375},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MEMCOD.2015.7340482},
doi = {10.1109/MEMCOD.2015.7340482},
abstract = {The quality of today's embedded systems e. g. in vehicles, airplanes, or automation plants is highly influenced by their architecture. In this context, we study the so-called deployment problem. The question is where (i. e., on which execution unit) to deploy which software application or which sensor/actuator shall be connected to which device in an automation plant. First, we introduce a domain-specific constraint and optimization language fitting the needs of our partners. Second, we investigate different approaches to tackle the deployment problem even for industrial size systems. Therefore, we present different solving strategies using (i) multi-objective evolutionary algorithms, (ii) SMT-based, and (iii) ILP-based solving approaches. Furthermore, a combination of the first two is used. We investigate the proposed methods and demonstrate their feasibility using two realistic systems: a civil flight control system (FCS), and a seawater desalination plant.},
booktitle = {Proceedings of the 2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign},
pages = {158–167},
numpages = {10},
series = {MEMOCODE '15}
}

@article{10.1016/j.jss.2010.07.049,
author = {Ullah, Muhammad Irfan and Ruhe, G\"{u}nther and Garousi, Vahid},
title = {Decision support for moving from a single product to a product portfolio in evolving software systems},
year = {2010},
issue_date = {December, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.07.049},
doi = {10.1016/j.jss.2010.07.049},
abstract = {Successful software systems continuously evolve to accommodate ever-changing needs of customers. Accommodating the feature requests of all the customers in a single product increases the risks and costs of software maintenance. A possible approach to mitigate these risks is to transition the evolving software system (ESS) from a single system to a portfolio of related product variants, each addressing a specific customers' segment. This evolution should be conducted such that the extent of modifications required in ESS's structure is reduced. The proposed method COPE+ uses preferences of customers on product features to generate multiple product portfolios each containing one product variant per segment of customers. Recommendations are given to the decision maker to update the product portfolios based on structural analysis of ESS. Product portfolios are compared with the ESS using statechart representations to identify the level of similarity in their behaviors. A proof of concept is presented by application to an open-source text editing system. Structural and behavioral analysis of candidate portfolios helped the decision maker to select one portfolio out of three candidates.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2496–2512},
numpages = {17},
keywords = {Behavioral analysis, Decision support, Open-source systems, Software product evolution, Software product lines, Software product management}
}

@inproceedings{10.1007/978-3-319-24912-4_17,
author = {Lackner, Hartmut},
title = {Model-Based Product Line Testing: Sampling Configurations for Optimal Fault Detection},
year = {2015},
isbn = {9783319249117},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24912-4_17},
doi = {10.1007/978-3-319-24912-4_17},
abstract = {Product line PL engineering is an emerging methodology for the development of variant-rich systems. As product lines are viable for this purpose, testing them is complicated in contrast to non-variable systems, as there is an increasing amount of possible products due to the number of features. The question of which products should be chosen for testing is still an ongoing challenge.We present coverage criteria for sampling configurations from reusable test cases. Such criteria are e.g. choosing as many different products as possible so each of the test cases can be executed once. The main contribution is an analysis of the resulting fault detection potential for the presented criteria. The analysis is supported by an example product line and a mutation system for assessing the fault detection capability. From the results of this example, we draw conclusions about the different coverage criteria.},
booktitle = {Proceedings of the 17th International SDL Forum on SDL 2015: Model-Driven Engineering for Smart Cities - Volume 9369},
pages = {238–251},
numpages = {14},
keywords = {Fault detection, Reusable software, Sampling, Testing}
}

@inproceedings{10.1145/2491411.2491446,
author = {Rubin, Julia and Chechik, Marsha},
title = {N-way model merging},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491446},
doi = {10.1145/2491411.2491446},
abstract = {Model merging is widely recognized as an essential step in a variety of software development activities. During the process of combining a set of related products into a product line or consolidating model views of multiple stakeholders, we need to merge multiple input models into one; yet, most of the existing approaches are applicable to merging only two models. In this paper, we define the n-way merge problem. We show that it can be reduced to the known and widely studied NP-hard problem of weighted set packing. Yet, the approximation solutions for that problem do not scale for real-sized software models. We thus evaluate alternative approaches of merging models that incrementally process input models in small subsets and propose our own algorithm that considerably improves precision over such approaches without sacrificing performance.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {301–311},
numpages = {11},
keywords = {Model merging, combining multiple models, weighted set packing},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1016/j.cie.2019.04.039,
author = {Zhang, Xianyu and Ming, Xinguo and Liu, Zhiwen and Qu, Yuanju and Yin, Dao},
title = {State-of-the-art review of customer to business (C2B) model},
year = {2019},
issue_date = {Jun 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.04.039},
doi = {10.1016/j.cie.2019.04.039},
journal = {Comput. Ind. Eng.},
month = jun,
pages = {207–222},
numpages = {16},
keywords = {Customer to business (C2B), Mass customization, Mass personalization, Personalized customization, Model innovation, Transformation and upgrading, C2B, C2B2M-MC, C2B2M-MP, C2M, CPM, C2M2S, CPM2S, CPR, PD, PM, SC, US, PR, PS, RTs, PC}
}

@proceedings{10.1145/2984043,
title = {SPLASH Companion 2016: Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/2048066.2048128,
author = {K\"{a}stner, Christian and Giarrusso, Paolo G. and Rendel, Tillmann and Erdweg, Sebastian and Ostermann, Klaus and Berger, Thorsten},
title = {Variability-aware parsing in the presence of lexical macros and conditional compilation},
year = {2011},
isbn = {9781450309400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048066.2048128},
doi = {10.1145/2048066.2048128},
abstract = {In many projects, lexical preprocessors are used to manage different variants of the project (using conditional compilation) and to define compile-time code transformations (using macros). Unfortunately, while being a simple way to implement variability, conditional compilation and lexical macros hinder automatic analysis, even though such analysis is urgently needed to combat variability-induced complexity. To analyze code with its variability, we need to parse it without preprocessing it. However, current parsing solutions use unsound heuristics, support only a subset of the language, or suffer from exponential explosion. As part of the TypeChef project, we contribute a novel variability-aware parser that can parse almost all unpreprocessed code without heuristics in practicable time. Beyond the obvious task of detecting syntax errors, our parser paves the road for further analysis, such as variability-aware type checking. We implement variability-aware parsers for Java and GNU C and demonstrate practicability by parsing the product line MobileMedia and the entire X86 architecture of the Linux kernel with 6065 variable features.},
booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {805–824},
numpages = {20},
keywords = {#ifdef, c, conditional compilation, linux, parsing, preprocessor, software product lines, variability},
location = {Portland, Oregon, USA},
series = {OOPSLA '11}
}

@article{10.1007/s10845-011-0585-6,
author = {Wu, Dazhong and Zhang, Linda L. and Jiao, Roger J. and Lu, Roberto F.},
title = {SysML-based design chain information modeling for variety management in production reconfiguration},
year = {2013},
issue_date = {June      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-011-0585-6},
doi = {10.1007/s10845-011-0585-6},
abstract = {Satisfying diverse customer needs leads to proliferation of product variants. It is imperative to model the coherence of functional, product and process varieties throughout the design chain. Based on a model-based systems engineering approach, this paper applies the Systems Modeling Language (SysML) to model design chain information. To support variety management decisions, the SysML-based information models are further implemented as a variety coding information system. A case study of switchgear enclosure production reconfiguration system demonstrates that SysML-based information modeling excels in conducting requirements, structural, behavioral and constraints analysis and in performing trade-off study. In addition, it maintains semantic coherence along the design chain, keeps traceability across different levels of abstraction, thus improving interoperability among heterogeneous tools.},
journal = {J. Intell. Manuf.},
month = jun,
pages = {575–596},
numpages = {22},
keywords = {Design chain management, Information modeling, Model-based systems engineering, Production reconfiguration, SysML, Variety management}
}

@inproceedings{10.5555/646667.700192,
author = {M\"{a}rtens, Holger and Rahm, Erhard and St\"{o}hr, Thomas},
title = {Dynamic Query Scheduling in Parallel Data Warehouses},
year = {2002},
isbn = {3540440496},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Data warehouse queries pose challenging performance problems that often necessitate the use of parallel database systems (PDBS). Although dynamic load balancing is of key importance in PDBS, to our knowledge it has not yet been investigated thoroughly for parallel data warehouses. In this study, we propose a scheduling strategy that simultaneously considers both processors and disks while utilizing the load balancing potential of a Shared Disk architecture. We compare the performance of this new method to several other approaches in a comprehensive simulation study, incorporating skew aspects and typical data warehouse features such as star schemas.},
booktitle = {Proceedings of the 8th International Euro-Par Conference on Parallel Processing},
pages = {321–331},
numpages = {11},
series = {Euro-Par '02}
}

@inproceedings{10.1145/3062341.3062366,
author = {Schneider, Scott and Wu, Kun-Lung},
title = {Low-synchronization, mostly lock-free, elastic scheduling for streaming runtimes},
year = {2017},
isbn = {9781450349888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3062341.3062366},
doi = {10.1145/3062341.3062366},
abstract = {We present the scalable, elastic operator scheduler in IBM Streams 4.2. Streams is a distributed stream processing system used in production at many companies in a wide range of industries. The programming language for Streams, SPL, presents operators, tuples and streams as the primary abstractions. A fundamental SPL optimization is operator fusion, where multiple operators execute in the same process. Streams 4.2 introduces automatic submission-time fusion to simplify application development and deployment. However, potentially thousands of operators could then execute in the same process, with no user guidance for thread placement. We needed a way to automatically figure out how many threads to use, with arbitrarily sized applications on a wide variety of hardware, and without any input from programmers. Our solution has two components. The first is a scalable operator scheduler that minimizes synchronization, locks and global data, while allowing threads to execute any operator and dynamically come and go. The second is an elastic algorithm to dynamically adjust the number of threads to optimize performance, using the principles of trusted measurements to establish trends. We demonstrate our scheduler's ability to scale to over a hundred threads, and our elasticity algorithm's ability to adapt to different workloads on an Intel Xeon system with 176 logical cores, and an IBM Power8 system with 184 logical cores.},
booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {648–661},
numpages = {14},
keywords = {elastic, lock-free, runtime scheduling, stream processing},
location = {Barcelona, Spain},
series = {PLDI 2017}
}

@inproceedings{10.3115/100964.1138530,
author = {Zue, Victor},
title = {Prosody, performance evaluation, databases, and ISAT},
year = {1989},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/100964.1138530},
doi = {10.3115/100964.1138530},
abstract = {This session began with a one-hour invited tutorial on prosody given by Dr. Janet Pierrehumbert of AT&amp;T Bell Laboratories, which was followed by several short reports on database collection and performance evaluation activities dealing with various aspects of the spoken speech system development effort. Issues and discussions on methods of formal evaluation were particularly relevant for natural language processing, where formal evaluation is in many respects an elusive goal.},
booktitle = {Proceedings of the Workshop on Speech and Natural Language},
pages = {5–36},
numpages = {32},
location = {Philadelphia, Pennsylvania},
series = {HLT '89}
}

@inproceedings{10.1145/2818000.2818009,
author = {Bogaerts, Jasper and Decat, Maarten and Lagaisse, Bert and Joosen, Wouter},
title = {Entity-Based Access Control: supporting more expressive access control policies},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2818009},
doi = {10.1145/2818000.2818009},
abstract = {Access control is an important part of security that restricts the actions that users can perform on resources. Policy models specify how these restrictions are formulated in policies. Over the last decades, we have seen several such models, including role-based access control and more recently, attribute-based access control. However, these models do not take into account the relationships between users, resources and entities and their corresponding properties. This limits the expressiveness of these models. In this work, we present Entity-Based Access Control (EBAC). EBAC introduces entities as a primary concept and takes into account both attributes and relationships to evaluate policies. In addition, we present Auctoritas. Auctoritas is a authorization system that provides a practical policy language and evaluation engine for EBAC. We find that EBAC increases the expressiveness of policies and fits the application domain well. Moreover, our evaluation shows that entity-based policies described in Auctoritas can be enforced with a low policy evaluation latency.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {291–300},
numpages = {10},
keywords = {ABAC, Access Control, Access Control Model, Attribute, Authorization, EBAC, Entity, Language, Policy Language, Relationship, XACML},
location = {Los Angeles, CA, USA},
series = {ACSAC '15}
}

@inproceedings{10.1145/1278480.1278620,
author = {Di Natale, Marco},
title = {Virtual platforms and timing analysis: status, challenges and future directions},
year = {2007},
isbn = {9781595936271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1278480.1278620},
doi = {10.1145/1278480.1278620},
abstract = {This paper outlines a methodology based on virtual platforms and timing analysis to perform the exploration and selection of automotive architecture solutions. The analysis provides a quantitative evaluation of architecture options with respect to para-functional metrics. The satisfaction of deadline constraints and the optimization with respect to latency on end-to-end computations is considered. In addition, we discuss to what degree existing standards, including OSEK and AUTOSAR and model-based development practices can support the development of time predictable software and what is required to move toward the desirable goal of timing isolation when integrating multiple applications into the same execution platform. Finally, the paper provides a quick glance at recent results in the the optimization of the software architecture for complex distributed systems with respect to worst case timing behavior.},
booktitle = {Proceedings of the 44th Annual Design Automation Conference},
pages = {551–555},
numpages = {5},
keywords = {design methodologies, embedded systems, real-time},
location = {San Diego, California},
series = {DAC '07}
}

@article{10.1007/s00500-018-3146-5,
author = {Nemati, Yaser and Alavidoost, Mohammad Hosein},
title = {A fuzzy bi-objective MILP approach to integrate sales, production, distribution and procurement planning in a FMCG supply chain},
year = {2019},
issue_date = {July      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {13},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3146-5},
doi = {10.1007/s00500-018-3146-5},
abstract = {Sales and operations are the heart of today's businesses, and the decisions made in these areas will intensively affect the financial performance, operational efficiency and service level of the whole organization. This manuscript is going to develop three multiobjective fuzzy mixed integer linear programming models of sales and operations planning process. Then, the performance of the fully integrated fuzzy model is compared to the similar crisp model, in terms of total supply chain's cost and customer service level. All the models are developed for a multisite manufacturing company, which is coping with different raw material suppliers and third-party logistics, distribution centers and customers with a wide range of product families. Finally, the models are applied to a real case in a FMCG manufacturing company in Iran. The final results approve the superiority of the fuzzy model over the crisp one. Furthermore, a sensitivity analysis is carried out to analyze the effect of some key factors on the benefits of the SC planning integration.},
journal = {Soft Comput.},
month = jul,
pages = {4871–4890},
numpages = {20},
keywords = {FMCG industry, Fuzzy mixed integer linear programming (f-MILP), Multiobjective modeling, Real-world case study, Sales and operations planning (S&amp;OP)}
}

@article{10.1016/j.ijinfomgt.2016.05.025,
author = {Khan, Saif Ur Rehman and Lee, Sai Peck and Ahmad, Raja Wasim and Akhunzada, Adnan and Chang, Victor},
title = {A survey on Test Suite Reduction frameworks and tools},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {36},
number = {6},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2016.05.025},
doi = {10.1016/j.ijinfomgt.2016.05.025},
abstract = {An extensive review on automated support for TSR.Presenting a thematic taxonomy to categorize the existing literature based on testing domains and corresponding parameters.Providing a detailed comparative analysis of TSR frameworks based on the devised taxonometric parameters.Synthesizing current state-of-the-art based on the underlying common philosophies.Highlighting several potential research issues in this field of study. Software testing is a widely accepted practice that ensures the quality of a System under Test (SUT). However, the gradual increase of the test suite size demands high portion of testing budget and time. Test Suite Reduction (TSR) is considered a potential approach to deal with the test suite size problem. Moreover, a complete automation support is highly recommended for software testing to adequately meet the challenges of a resource constrained testing environment. Several TSR frameworks and tools have been proposed to efficiently address the test-suite size problem. The main objective of the paper is to comprehensively review the state-of-the-art TSR frameworks to highlights their strengths and weaknesses. Furthermore, the paper focuses on devising a detailed thematic taxonomy to classify existing literature that helps in understanding the underlying issues and proof of concept. Moreover, the paper investigates critical aspects and related features of TSR frameworks and tools based on a set of defined parameters. We also rigorously elaborated various testing domains and approaches followed by the extant TSR frameworks. The results reveal that majority of TSR frameworks focused on randomized unit testing, and a considerable number of frameworks lacks in supporting multi-objective optimization problems. Moreover, there is no generalized framework, effective for testing applications developed in any programming domain. Conversely, Integer Linear Programming (ILP) based TSR frameworks provide an optimal solution for multi-objective optimization problems and improve execution time by running multiple ILP in parallel. The study concludes with new insights and provides an unbiased view of the state-of-the-art TSR frameworks. Finally, we present potential research issues for further investigation to anticipate efficient TSR frameworks.},
journal = {Int. J. Inf. Manag.},
month = dec,
pages = {963–975},
numpages = {13},
keywords = {Fault localization, Frameworks, Regression testing, Test Suite Reduction, Test suite optimization}
}

@inproceedings{10.1145/3302424.3303959,
author = {Rommel, Florian and Dietrich, Christian and Rodin, Michael and Lohmann, Daniel},
title = {Multiverse: Compiler-Assisted Management of Dynamic Variability in Low-Level System Software},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303959},
doi = {10.1145/3302424.3303959},
abstract = {System software, such as the Linux kernel, typically provides a high degree of versatility by means of static and dynamic variability. While static variability can be completely resolved at compile time, dynamic variation points come at a cost arising from extra tests and branches in the control flow. Kernel developers use it (a) only sparingly and (b) try to mitigate its overhead by run-time binary code patching, for which several problem/architecture-specific mechanisms have become part of the kernel.We think that means for highly efficient dynamic variability should be provided by the language and compiler instead and present multiverse, an extension to the C programming language and the GNU C compiler for this purpose. Multiverse is easy to apply and targets program-global configuration switches in the form of (de-)activatable features, integer-valued configurations, and rarely-changing program modes. At run time, multiverse removes the overhead of evaluating them on every invocation. Microbenchmark results from applying multiverse to performance-critical features of the Linux kernel, cPython, the musl C-library and GNU grep show that multiverse can not only replace and unify the existing mechanisms for run-time code patching, but may in some cases even improve their performance by adding new dynamic variability options.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {37},
numpages = {13},
keywords = {Linux, binary patching, compilers, dynamic variability, operating systems},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@article{10.1145/3376921,
author = {Renner, Bernd-Christian and Heitmann, Jan and Steinmetz, Fabian},
title = {ahoi: Inexpensive, Low-power Communication and Localization for Underwater Sensor Networks and μAUVs},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3376921},
doi = {10.1145/3376921},
abstract = {The recent development of small, cheap AUVs enables a plethora of underwater near- and inshore applications. Among these are monitoring of wind parks, detection of pollution sources, water-quality inspection, and the support of divers during disaster management. These tasks profit from online reporting, control, and AUV swarm interaction; yet they require underwater communication. Unfortunately, commercial devices are prohibitively expensive and typically closed-source, hampering their application in affordable products and research. Therefore, we developed the open-source ahoi acoustic modem. It is (i)&nbsp;small enough to be carried by micro AUVs, (ii)&nbsp;consumes little enough energy to not diminish operation times of its host, (iii)&nbsp;comes at an attractive unit cost below $600, (iv)&nbsp;can reliably communicate at distances of 150 m and more, and (v)&nbsp;supports ranging without additional hardware. Due to its modular build, the modem can be customized and is suitable as research platform to analyze, e.g., MAC and routing protocols. We conducted extensive real-world studies and present results of communication range, packet reception rate, ranging accuracy, and efficient and reliable self-localization. Finally, we draw conclusions regarding acoustic communication, ranging, and localization with inexpensive and low-power devices that go beyond a particular device. Our study, hence, encompasses general insights, observations, and recommendations.},
journal = {ACM Trans. Sen. Netw.},
month = jan,
articleno = {18},
numpages = {46},
keywords = {AUV, Acoustic, ahoi, communication, localization, modem, swarm, underwater}
}

@article{10.1145/2729974,
author = {Mkaouer, Wiem and Kessentini, Marouane and Shaout, Adnan and Koligheu, Patrice and Bechikh, Slim and Deb, Kalyanmoy and Ouni, Ali},
title = {Many-Objective Software Remodularization Using NSGA-III},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2729974},
doi = {10.1145/2729974},
abstract = {Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this article, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and reuse the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {17},
numpages = {45},
keywords = {Search-based software engineering, remodularization, software maintenance, software quality}
}

@inproceedings{10.1145/3365438.3410963,
author = {Alwidian, Sanaa and Amyot, Daniel},
title = {"Union is power": analyzing families of goal models using union models},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410963},
doi = {10.1145/3365438.3410963},
abstract = {A goal model family is a set of related goal models that conform to the same metamodel, with commonalities and variabilities between models. Such families stem from the evolution of initial models into several versions over time and/or the variation of models over the space dimension (e.g., products). In contexts where there are several versions/variations of a goal model, analyzing a set of related models with typical similarities, one model at a time, often involves redundant computations and may require repeated user assistance (e.g., for interactive analysis) and laborious activities. This paper proposes the use of union models as first-class artifacts to analyze families of goal models, in order to improve performance of language-specific analysis procedures. The paper empirically evaluates the performance gain resulting from adapting (or lifting) an existing analysis technique specific to the Goal-oriented Requirement Language (GRL) to a family of GRL models, all at once using a union model, compared to analyzing individual models. Our experiments show, based on the use of the IBM CPLEX optimizer, the usefulness and performance gains of using union models to perform a computationally expensive analysis, namely quantitative backward propagation, on families of GRL models.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {252–262},
numpages = {11},
keywords = {analysis, backward propagation, goal modeling, model family},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@article{10.1016/j.infsof.2010.05.003,
author = {Ali, Muhammad Sarmad and Ali Babar, Muhammad and Chen, Lianping and Stol, Klaas-Jan},
title = {A systematic review of comparative evidence of aspect-oriented programming},
year = {2010},
issue_date = {September, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.003},
doi = {10.1016/j.infsof.2010.05.003},
abstract = {Context: Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Objective: This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Method: We performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings. Results: Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported. Conclusion: AOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {871–887},
numpages = {17},
keywords = {Aspect-oriented programming, Evidence-based software engineering, Systematic literature review}
}

@article{10.1007/s10586-007-0049-0,
author = {Xie, Tao and Qin, Xiao},
title = {Stochastic scheduling for multiclass applications with availability requirements in heterogeneous clusters},
year = {2008},
issue_date = {March     2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {11},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-007-0049-0},
doi = {10.1007/s10586-007-0049-0},
abstract = {High availability plays an important role in heterogeneous clusters, where processors operate at different speeds and are not continuously available for processing. Existing scheduling algorithms designed for heterogeneous clusters do not factor in availability. We address in this paper the stochastic scheduling problem for heterogeneous clusters with availability constraints. Each node in a heterogeneous cluster is modeled by its speed and availability, and different classes of tasks submitted to the cluster are characterized by their execution times and availability requirements. To incorporate availability and heterogeneity into stochastic scheduling, we introduce metrics to quantify availability and heterogeneity in the context of multiclass tasks. A stochastic scheduling algorithm SSAC (  s tochastic  s cheduling with  a vailability  c onstraints) is then proposed to improve availability of heterogeneous clusters while reducing average response time of tasks. Experimental results show that our algorithm achieves a good trade-off between availability and responsiveness.},
journal = {Cluster Computing},
month = mar,
pages = {33–43},
numpages = {11},
keywords = {Availability, Heterogeneous clusters, Multiclass application, Stochastic scheduling}
}

@inproceedings{10.1145/3205368.3205377,
author = {Grelck, Clemens and Sarris, Nikolaos},
title = {Towards Compiling SAC for the Xeon Phi Knights Corner and Knights Landing Architectures: Strategies and Experiments},
year = {2017},
isbn = {9781450363433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205368.3205377},
doi = {10.1145/3205368.3205377},
abstract = {Xeon Phi is the common brand name of Intel's Many Integrated Core (MIC) architecture. The first commercially available generation Knights Corner and the second generation Knights Landing form a middle ground between modestly parallel desktop and standard server processor architectures and the massively parallel GPGPU architectures.In this paper we explore various compilation strategies for the purely functional data-parallel array language SAC (Single Assignment C) to support both MIC architectures in the presence of entirely resource- and target-agnostic source code. Our particular interest lies in doing so with limited, or entirely without, user knowledge about the target architecture. We report on a series of experiments involving two classical benchmarks, Matrix Multiplication and Gaussian Blur, that demonstrate the level of performance that can be expected from compilation of abstract, purely functional source code to the Xeon Phi family of architectures.},
booktitle = {Proceedings of the 29th Symposium on the Implementation and Application of Functional Programming Languages},
articleno = {9},
numpages = {12},
keywords = {Array processing, Single Assignment C, Xeon Phi, automatic parallelisation, multithreading},
location = {Bristol, United Kingdom},
series = {IFL '17}
}

@article{10.1016/j.jss.2013.11.1096,
author = {Lochau, Malte and Lity, Sascha and Lachmann, Remo and Schaefer, Ina and Goltz, Ursula},
title = {Delta-oriented model-based integration testing of large-scale systems},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.11.1096},
doi = {10.1016/j.jss.2013.11.1096},
abstract = {Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain.},
journal = {J. Syst. Softw.},
month = may,
pages = {63–84},
numpages = {22},
keywords = {Large-scale systems, Model-based testing, Regression testing, Variable software architectures}
}

@inproceedings{10.1145/2588555.2594514,
author = {Psaroudakis, Iraklis and Athanassoulis, Manos and Olma, Matthaios and Ailamaki, Anastasia},
title = {Reactive and proactive sharing across concurrent analytical queries},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2594514},
doi = {10.1145/2588555.2594514},
abstract = {Today an ever increasing amount of data is collected and analyzed by researchers, businesses, and scientists in data warehouses (DW). In addition to the data size, the number of users and applications querying data grows exponentially. The increasing concurrency is itself a challenge in query execution, but also introduces an opportunity favoring synergy between concurrent queries. Traditional execution engines of DW follows a query-centric approach, where each query is optimized and executed independently. On the other hand, workloads with increased concurrency have several queries with common parts of data and work, creating the opportunity for sharing among concurrent queries. Sharing can be reactive to the inherently existing sharing opportunities, or proactive by redesigning query operators to maximize the sharing opportunities. This demonstration showcases the impact of proactive and reactive sharing by comparing and integrating representative state-of-the-art techniques: Simultaneous Pipelining (SP), for reactive sharing, which shares intermediate results of common sub-plans, and Global Query Plans (GQP) for proactive sharing, which build and evaluate a single query plan with shared operators. We visually demonstrate, in an interactive interface, the behavior of both sharing approaches on top of a state-of-the-art storage engine using the original prototypes. We show that pull-based sharing for SP eliminates the serialization point imposed by the original push-based approach. Then, we compare, through a sensitivity analysis, the performance of SP and GQP. Finally, we show that SP can improve the performance of GQP for a query mix with common sub-plans.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {889–892},
numpages = {4},
keywords = {CJOIN, QPipe, data sharing, data warehouses, global query plans, proactive sharing, query processing, reactive sharing, shared pages list, simultaneous pipelining, work sharing},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.5555/1218112.1218455,
author = {Quadt, Daniel},
title = {Simulation-based scheduling of parallel wire-bonders with limited clamp&amp;paddles},
year = {2006},
isbn = {1424405017},
publisher = {Winter Simulation Conference},
abstract = {We present a scheduling procedure for the wire-bonding operation of a semiconductor assembly facility. The wire-bonding operation typically consists of a large number of unrelated parallel machines and is typically one of the bottlenecks in an assembly facility. The scheduling procedure is able to handle setup times, limited fixtures (clamp&amp;paddles) and non-zero machine ready-times (initial work in progress). It is based on a simulator that generates a schedule and a Simulated Annealing approach to optimize the schedule. Some preliminary results from an implementation in a large assembly facility are given.},
booktitle = {Proceedings of the 38th Conference on Winter Simulation},
pages = {1887–1892},
numpages = {6},
location = {Monterey, California},
series = {WSC '06}
}

@inproceedings{10.1109/HUST.2014.7,
author = {Evans, Todd and Barth, William L. and Browne, James C. and DeLeon, Robert L. and Furlani, Thomas R. and Gallo, Steven M. and Jones, Matthew D. and Patra, Abani K.},
title = {Comprehensive resource use monitoring for HPC systems with TACC stats},
year = {2014},
isbn = {9781467367554},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/HUST.2014.7},
doi = {10.1109/HUST.2014.7},
abstract = {This paper reports on a comprehensive, fully automated resource use monitoring package, TACC Stats, which enables both consultants, users and other stakeholders in an HPC system to systematically and actively identify jobs/applications that could benefit from expert support and to aid in the diagnosis of software and hardware issues. TACC Stats continuously collects and analyzes resource usage data for every job run on a system and differs significantly from conventional profilers because it requires no action on the part of the user or consultants---it is always collecting data on every node for every job. TACC Stats is open source and downloadable, configurable and compatible with general Linux-based computing platforms, and extensible to new CPU architectures and hardware devices. It is meant to provide a comprehensive resource usage monitoring solution. In addition to describing TACC Stats, the paper illustrates its application to identifying production jobs which have inefficient resource use characteristics.},
booktitle = {Proceedings of the First International Workshop on HPC User Support Tools},
pages = {13–21},
numpages = {9},
location = {New Orleans, Louisiana},
series = {HUST '14}
}

@inproceedings{10.1007/978-3-031-15116-3_8,
author = {Lima dos Santos, Edilton and Fortz, Sophie and Schobbens, Pierre-Yves and Perrouin, Gilles},
title = {Behavioral Maps: Identifying Architectural Smells in&nbsp;Self-adaptive Systems at&nbsp;Runtime},
year = {2021},
isbn = {978-3-031-15115-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-15116-3_8},
doi = {10.1007/978-3-031-15116-3_8},
abstract = {Self-adaptive systems (SAS) change their behavior and structure at runtime, depending on environmental changes and reconfiguration plans and goals. Such systems combine architectural fragments or solutions in their (re)configuration process. However, this process may negatively impact the system’s architectural qualities, exhibiting architectural bad smells (ABS). Also, some smells may appear in only particular runtime conditions. This issue is challenging to detect due to the combinatorial explosion of interactions amongst features. We initially proposed the notion of Behavioral Map to explore architectural issues at runtime. This extended study applies the Behavioral Map to analyze the ABS in self-adaptive systems at runtime. In particular, we look for Cyclic Dependency, Extraneous Connector, Hub-Like Dependency, and Oppressed Monitor ABS in various runtime adaptations in the Smart Home Environment (SHE) framework, Adasim, and mRUBiS systems developed in Java. The results indicate that runtime ABS identification is required to fully capture SAS architectural qualities because the ABS are feature-dependent, and their number is highly variable for each adaptation. We have observed that some ABS appears in all runtime adaptations, some in only a few. However, some ABS only appear in the publish-subscribe architecture, such as Extraneous Connector and Oppressed Monitor smell. We discuss the reasons behind these architectural smells for each system and motivate the need for targeted ABS analyses in SAS.},
booktitle = {Software Architecture: 15th European Conference, ECSA 2021 Tracks and Workshops; V\"{a}xj\"{o}, Sweden, September 13–17, 2021, Revised Selected Papers},
pages = {159–180},
numpages = {22},
keywords = {Architectural smells, Dynamic software product lines, Runtime validation, Self-adaptive systems, Behavioral maps},
location = {V\"{a}xj\"{o}, Sweden}
}

@article{10.1147/JRD.2010.2092173,
author = {Hampapur, A. and Cao, H. and Davenport, A. and Dong, W. S. and Fenhagen, D. and Feris, R. S. and Goldszmidt, G. and Jiang, Z. B. and Kalagnanam, J. and Kumar, T. and Li, H. and Liu, X. and Mahatma, S. and Pankanti, S. and Pelleg, D. and Sun, W. and Taylor, M. and Tian, C. H. and Wasserkrug, S. and Xie, L. and Lodhi, M. and Kiely, C. and Butturff, K. and Desjardins, L.},
title = {Analytics-driven asset management},
year = {2011},
issue_date = {January/March 2011},
publisher = {IBM Corp.},
address = {USA},
volume = {55},
number = {1 &amp; 2},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2010.2092173},
doi = {10.1147/JRD.2010.2092173},
abstract = {Asset-intensive businesses across industries rely on physical assets to deliver services to their customers, and effective asset management is critical to the businesses. Today, businesses may make use of enterprise asset-management (EAM) solutions for many asset-related processes, ranging from the core asset-management functions to maintenance, inventory, contracts, warranties, procurement, and customer-service management. While EAM solutions have transformed the operational aspects of asset management through data capture and process automation, the decision-making process with respect to assets still heavily relies on institutional knowledge and anecdotal insights. Analytics-driven asset management is an approach that makes use of advanced analytics and optimization technologies to transform the vast amounts of data from asset management, metering, and sensor systems into actionable insight, foresight, and prescriptions that can guide decisions involving strategic and tactical assets, as well as customer and business models.},
journal = {IBM J. Res. Dev.},
month = jan,
pages = {138–156},
numpages = {19}
}

@article{10.5555/1326366.1326532,
author = {Manitz, Michael},
title = {Queueing-model based analysis of assembly lines with finite buffers and general service times},
year = {2008},
issue_date = {August, 2008},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {35},
number = {8},
issn = {0305-0548},
abstract = {In this paper, we study the production process on multi-stage assembly lines. These production systems comprise simple processing as well as assembly stations. At the latter, workpieces from two or more input stations have to be merged to form a new one for further processing. As the flow of material is asynchronous with stochastic processing times at each station, queueing effects arise as long as buffers provide waiting room. We consider finite buffer capacities and generally distributed processing times. Processing is a service operation to customer items in the sense of a queueing system. The arrival stream of customer items is generated by processing parts at a predecessor station. This paper describes an approximation procedure for determining the throughput of such an assembly line. Exact solutions are not available in this case. For performance evaluation, a decomposition approach is used. The two-station subsystems are analyzed by G/G/1/N stopped-arrival queueing models. In this heuristic approach, the virtual arrival and service rates, and the squared coefficients of variation of these subsystems are determined. A system of decomposition equations which are solved iteratively is presented. Any solution to this system of equations indicates estimated values for the subsystems' unknown parameters. The quality of the presented approximation procedure is tested against the results of various simulation experiments. Scope and purpose: We consider assembly lines, i.e. flow production systems with a convergent flow of material and synchronization constraints. By considering assembly operations, our paper generalizes the work of Buzacott, Liu and Shanthikumar [Multistage flow line analysis with the stopped arrival queue model. IIE Transactions 1995;27:444-55], in which flow lines as series production systems are analyzed. It also extends the work of Helber [Performance analysis of flow lines with non-linear flow of material, Lecture notes in economics and mathematical systems, vol. 473, Berlin, Heidelberg, New York: Springer; 1999] and Jeong and Kim [Performance analysis of assembly/disassembly systems with unreliable machines and random processing times. IIE Transactions 1998; 30(1):41-53], who analyze assembly systems, by considering general service times. Station failures can be incorporated with the completion time concept proposed by Gaver [A waiting line with interrupted service, including priorities. Journal of the Royal Statistical Society 1962;24(2):73-90. [47]]. The comparison to various simulation results shows that the queueing-model based approach presented in this paper yields quite good approximations of the throughput.},
journal = {Comput. Oper. Res.},
month = aug,
pages = {2520–2536},
numpages = {17},
keywords = {Assembly lines, Assembly stations, Decomposition, Flow production, G/G/1/N stopped-arrival queueing system, Queueing models, Synchronization constraint, Throughput}
}

@article{10.1145/1516533.1516538,
author = {Salehie, Mazeiar and Tahvildari, Ladan},
title = {Self-adaptive software: Landscape and research challenges},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/1516533.1516538},
doi = {10.1145/1516533.1516538},
abstract = {Software systems dealing with distributed applications in changing environments normally require human supervision to continue operation in all conditions. These (re-)configuring, troubleshooting, and in general maintenance tasks lead to costly and time-consuming procedures during the operating phase. These problems are primarily due to the open-loop structure often followed in software development. Therefore, there is a high demand for management complexity reduction, management automation, robustness, and achieving all of the desired quality requirements within a reasonable cost and time range during operation. Self-adaptive software is a response to these demands; it is a closed-loop system with a feedback loop aiming to adjust itself to changes during its operation. These changes may stem from the software system's self (internal causes, e.g., failure) or context (external events, e.g., increasing requests from users). Such a system is required to monitor itself and its context, detect significant changes, decide how to react, and act to execute such decisions. These processes depend on adaptation properties (called self-* properties), domain characteristics (context information or models), and preferences of stakeholders. Noting these requirements, it is widely believed that new models and frameworks are needed to design self-adaptive software. This survey article presents a taxonomy, based on concerns of adaptation, that is, how, what, when and where, towards providing a unified view of this emerging area. Moreover, as adaptive systems are encountered in many disciplines, it is imperative to learn from the theories and models developed in these other areas. This survey article presents a landscape of research in self-adaptive software by highlighting relevant disciplines and some prominent research projects. This landscape helps to identify the underlying research gaps and elaborates on the corresponding challenges.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {14},
numpages = {42},
keywords = {Adaptation processes, research challenges, self-adaptive software, self-properties, survey}
}

@article{10.1016/j.jksuci.2018.09.005,
author = {Mukherjee, Rajendrani and Patnaik, K. Sridhar},
title = {A survey on different approaches for software test case prioritization},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {9},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.09.005},
doi = {10.1016/j.jksuci.2018.09.005},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = nov,
pages = {1041–1054},
numpages = {14},
keywords = {Regression, Prioritization, Techniques, Program, Fault, Coverage}
}

@article{10.1016/j.compeleceng.2017.08.004,
author = {Alfrez, Germn H. and Pelechano, Vicente},
title = {Achieving autonomic Web service compositions with models at runtime},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.08.004},
doi = {10.1016/j.compeleceng.2017.08.004},
abstract = {Several exceptional situations may arise in the complex, heterogeneous, and changing contexts where Web service operations run. For instance, a Web service operation may have greatly increased its execution time or may have become unavailable. The contribution of this article is to provide a tool-supported framework to guide autonomic adjustments of context-aware service compositions using models at runtime. During execution, when problematic events arise in the context, models are used by an autonomic architecture to guide changes of the service composition. Under the closed-world assumption, the possible context events are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations arising in uncertain contexts where service compositions run. Therefore, the proposed framework also covers the dynamic evolution of service compositions to deal with unexpected events in the open world. An evaluation demonstrates that our framework is efficient during dynamic adjustments.},
journal = {Comput. Electr. Eng.},
month = oct,
pages = {332–352},
numpages = {21},
keywords = {Autonomic computing, Dynamic adaptation, Dynamic evolution, Dynamic software product lines, Models at runtime, Web service compositions}
}

@article{10.1016/j.eswa.2020.113808,
author = {Mohsin, Hufsa and Shi, Chongyang},
title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113808},
doi = {10.1016/j.eswa.2020.113808},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {15},
keywords = {Bug triaging, Defect localization, Self-paced learning, Bug report analysis, Bug classification}
}

@article{10.1007/s10845-016-1250-x,
author = {Lu, Yuqian and Wang, Hongqiang and Xu, Xun},
title = {ManuService ontology: a product data model for service-oriented business interactions in a cloud manufacturing environment},
year = {2019},
issue_date = {January   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-016-1250-x},
doi = {10.1007/s10845-016-1250-x},
abstract = {The ever-increasing distributed, networked and crowd-sourced cloud environment imposes the need of a service-oriented product data model for explicit representation of service requests in global manufacturing-service networks. The work in this paper aims to develop such a description framework for products based on semantic web technologies to facilitate the make-to-individual production strategy in a cloud manufacturing environment. A brief discussion on the requirements of a product data model in cloud manufacturing and research on product data modelling is given in the first part. A systematic ontology development methodology is then proposed and elaborated. The ontology called ManuService has been developed, consisting of all necessary concepts for description of products in a service-oriented business environment. These concepts include product specifications, quality constraints, manufacturing processes, organisation information, cost expectations, logistics requirements, and etcetera. ManuService ontology provides a module-based, reconfigurable, privacy-enhanced and standardised approach to modelling customised manufacturing service requests. An industrial case is presented to demonstrate possible applications using ManuService ontology. Comprehensive discussions are given thereafter, including a pilot application of a software package for semantic-based product design and a semantic web-based module for intelligent knowledge-based decision-making based on ManuService. ManuService forms the basis for collaborative service-oriented business interactions, intelligent and secure service provision in cloud manufacturing environment.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {317–334},
numpages = {18},
keywords = {Cloud manufacturing, Make-to-individual, Ontology, Product data model, Semantic web, Service-oriented manufacturing}
}

@article{10.1023/B:APIN.0000033637.51909.04,
author = {Xiao, Luo and Wissmann, Dieter and Brown, Michael and Jablonski, Stephan},
title = {Information Extraction from the Web: System and Techniques},
year = {2004},
issue_date = {September-October 2004},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {0924-669X},
url = {https://doi.org/10.1023/B:APIN.0000033637.51909.04},
doi = {10.1023/B:APIN.0000033637.51909.04},
abstract = {Information Extraction (IE) systems that can exploit the vast source of textual information that is the internet would provide a revolutionary step forward in terms of delivering large volumes of content cheaply and precisely, thus enabling a wide range of new knowledge driven applications and services. However, despite this enormous potential, few IE systems have successfully made the transition from laboratory to commercial application. The reason may be a purely practical one—to build useable, scaleable IE systems requires bringing together a range of different technologies as well as providing clear and reproducible guidelines as to how to collectively configure and deploy those technologies.This paper is an attempt to address these issues. The paper focuses on two primary goals. Firstly, we show that an information extraction system which is used for real world applications and different domains can be built using some autonomous, corporate components (agents). Such a system has some advanced properties: clear separation to different extraction tasks and steps, portability to multiple application domain, trainability, extensibility, etc. Secondly, we show that machine learning and, in particular, learning in different ways and at different levels, can be used to build practical IE systems. We show that carefully selecting the right machine learning technique for the right task and selective sampling can be used to reduce the human effort required to annotate examples for building such systems.},
journal = {Applied Intelligence},
month = sep,
pages = {195–224},
numpages = {30},
keywords = {information extraction, internet applications, knowledge acquisition, machine learning, methodology and design}
}

@article{10.1016/j.compind.2011.05.001,
author = {M\"{o}nch, Lars and Lendermann, Peter and McGinnis, Leon F. and Schirrmann, Arnd},
title = {A survey of challenges in modelling and decision-making for discrete event logistics systems},
year = {2011},
issue_date = {August, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {62},
number = {6},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2011.05.001},
doi = {10.1016/j.compind.2011.05.001},
abstract = {In this paper, we consider discrete event logistics systems (DELS). DELS are networks of resources through which material flow. They have been the subject of a large body of analytic research. A huge variety of specific models exists that generally require application by model and/or solution experts to answer narrowly-defined logistics questions about inventory, sourcing, scheduling, routing, etc. It has, however, proven difficult to integrate these models in any comprehensive way into information systems for logistics because of the lack of conceptual alignment between the models produced by researchers and the information systems deployed in practice with which they should be integrated. In this paper, we systematically identify several challenges for DELS research. We analyse the root of the difficulties for applying academic research results in DELS, and indicate some potential future research directions.},
journal = {Comput. Ind.},
month = aug,
pages = {557–567},
numpages = {11},
keywords = {Challenges, Decision-making, Future research directions, Logistics, Modelling, Survey}
}

@inproceedings{10.1109/ASE.2015.35,
author = {W\"{o}lfl, Andreas and Siegmund, Norbert and Apel, Sven and Kosch, Harald and Krautlager, Johann and Weber-Urbina, Guillermo},
title = {Generating qualifiable avionics software: an experience report},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.35},
doi = {10.1109/ASE.2015.35},
abstract = {We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {726–736},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.5555/2433508.2433931,
author = {Paju, Marja and Heilala, Juhani and Hentula, Markku and Heikkil\"{a}, Antti and Johansson, Bj\"{o}rn and Leong, Swee and Lyons, Kevin},
title = {Framework and indicators for a sustainable manufacturing mapping methodology},
year = {2010},
isbn = {9781424498642},
publisher = {Winter Simulation Conference},
abstract = {Increasing numbers of companies in the manufacturing industry have identified market potential for implementing sustainable and green manufacturing. Yet, current sustainability assessment tools for companies are complicated, requiring vast amounts of data and technical expertise to use them. Value Stream Mapping (VSM) is founded on lean practices, and it uses a simple method to analyze various types of material, energy, and information flow needed to bring products and services to the end-customer. The objective of this paper is to introduce and illustrate the application of a VSM-based assessment, termed as Sustainable Manufacturing Mapping (SMM). SMM takes chosen sustainability indicators into consideration and is based on VSM, Life Cycle Assessment (LCA), and Discrete Event Simulation (DES). The main phases of SMM include goal definition, identification of the sustainability indicators, and modeling the current and future state process maps. In this paper, some example indicators were identified and an SMM process map was generated for illustrative purposes.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3411–3422},
numpages = {12},
location = {Baltimore, Maryland},
series = {WSC '10}
}

@article{10.1016/j.knosys.2019.105185,
author = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Xie, Shengli and Su, Chun-Yi},
title = {Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105185},
doi = {10.1016/j.knosys.2019.105185},
journal = {Know.-Based Syst.},
month = feb,
numpages = {10},
keywords = {Graph-regularization, Semi-supervised learning, Multi-view clustering, Non-negative matrix factorization}
}

@article{10.1007/s00500-018-3608-9,
author = {Xu, Qingzhen and Li, Miao and Yu, Mengjing},
title = {Learning to rank with relational graph and pointwise constraint for cross-modal retrieval},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {19},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3608-9},
doi = {10.1007/s00500-018-3608-9},
abstract = {Cross-modal retrieval (i.e., image–query–text or text–query–image) is a hot research topic for multimedia information retrieval, but the heterogeneity gap between different modalities generates a critical challenge for multimodal data. Some researchers regard the cross-modal retrieval as a leaning to rank task, and they usually consider to measure similarity between two different modalities in the embedding shared subspace. However, previous methods almost pay more attention to construct a discriminative objective function to optimize common space, ignoring to exploit correlation between the single modality. In this paper, we consider the cross-modal retrieval task, from the perspective of optimizing ranking model, as a listwise ranking problem, and propose a novel method called learning to rank with relational graph and pointwise constraint ($$ {text{LR}}^{2} {text{GP}} $$). In $$ {text{LR}}^{2} {text{GP}} $$, we first propose a discriminative ranking model, which makes use of the relation between the single modality to improve ranking performance so as to learn an optimal embedding common subspace. Then, a pointwise constraint is introduced in the low-dimension embedding subspace to make up for the real loss in the training phase since listwise method introduced merely considers directly optimize latent permutation from the perspective of the overall. Finally, a dynamic interpolation algorithm, which gradually transits from pointwise and pairwise to listwise learning, is selected to deal with the problem of fusion  of loss function reasonable. Experiments on the benchmark datasets about Wikipedia and Pascal demonstrate the effectiveness for proposed method.},
journal = {Soft Comput.},
month = oct,
pages = {9413–9427},
numpages = {15},
keywords = {Cross-modal retrieval, Ranking model, Single modality, Pointwise constraint, Interpolation algorithm}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1155/2021/8603515,
author = {Nagy, L\'{a}szl\'{o} and Ruppert, Tam\'{a}s and Abonyi, J\'{a}nos and Javaid, Muhammad},
title = {Ontology-Based Analysis of Manufacturing Processes: Lessons Learned from the Case Study of Wire Harness Production},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1076-2787},
url = {https://doi.org/10.1155/2021/8603515},
doi = {10.1155/2021/8603515},
abstract = {Effective information management is critical for the development of manufacturing processes. This paper aims to provide an overview of ontologies that can be utilized in building Industry 4.0 applications. The main contributions of the work are that it highlights ontologies that are suitable for manufacturing management and recommends the multilayer-network-based interpretation and analysis of ontology-based databases. This article not only serves as a reference for engineers and researchers on ontologies but also presents a reproducible industrial case study that describes the ontology-based model of a wire harness assembly manufacturing process.},
journal = {Complex.},
month = jan,
numpages = {21}
}

@article{10.1007/s10270-011-0219-7,
author = {Mohagheghi, Parastoo and Gilani, Wasif and Stefanescu, Alin and Fernandez, Miguel A. and Nordmoen, Bj\o{}rn and Fritzsche, Mathias},
title = {Where does model-driven engineering help? Experiences from three industrial cases},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0219-7},
doi = {10.1007/s10270-011-0219-7},
abstract = {There have been few experience reports from industry on how Model-Driven Engineering (MDE) is applied and what the benefits are. This paper summarizes the experiences of three large industrial participants in a European research project with the objective of developing techniques and tools for applying MDE on the development of large and complex software systems. The participants had varying degrees of previous experience with MDE. They found MDE to be particularly useful for providing abstractions of complex systems at multiple levels or from different viewpoints, for the development of domain-specific models that facilitate communication with non-technical experts, for the purposes of simulation and testing, and for the consumption of models for analysis, such as performance-related decision support and system design improvements. From the industrial perspective, a methodology is considered to be useful and cost-efficient if it is possible to reuse solutions in multiple projects or products. However, developing reusable solutions required extra effort and sometimes had a negative impact on the performance of tools. While the companies identified several benefits of MDE, merging different tools with one another in a seamless development environment required several transformations, which increased the required implementation effort and complexity. Additionally, user-friendliness of tools and the provision of features for managing models of complex systems were identified as crucial for a wider industrial adoption of MDE.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {619–639},
numpages = {21},
keywords = {Complex systems, Domain-specific language, Eclipse, Experience report, Model-driven engineering, Simulation}
}

@inproceedings{10.5555/645340.650234,
author = {Tao, Yufei and Papadias, Dimitris and Zhang, Jun},
title = {Aggregate Processing of Planar Points},
year = {2002},
isbn = {3540433244},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Aggregate window queries return summarized information about objects that fall inside a query rectangle (e.g., the number of objects instead of their concrete ids). Traditional approaches for processing such queries usually retrieve considerable extra information, thus compromising the processing cost. The paper addresses this problem for planar points from both theoretical and practical points of view. We show that, an aggregate window query can be answered in logarithmic worst-case time by an indexing structure called the aP-tree. Next we study the practical behavior of the aP-tree and propose efficient cost models that predict the structure size and actual query cost. Extensive experiments show that the aP-tree, while involving more space consumption, accelerates query processing by up to an order of magnitude compared to a specialized method based on R-trees. Furthermore, our cost models are accurate and can be employed for the selection of the most appropriate method, balancing the space and query time tradeoff.},
booktitle = {Proceedings of the 8th International Conference on Extending Database Technology: Advances in Database Technology},
pages = {682–700},
numpages = {19},
series = {EDBT '02}
}

@article{10.1007/s42979-020-00323-8,
author = {Shafik, Wasswa and Matinkhah, S. Mojtaba and Ghasemzadeh, Mohammad},
title = {Theoretical Understanding of Deep Learning in UAV Biomedical Engineering Technologies Analysis},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {6},
url = {https://doi.org/10.1007/s42979-020-00323-8},
doi = {10.1007/s42979-020-00323-8},
abstract = {The unmanned aerial vehicles (UAVs) emerged into a promising research trend within the recurrent year where current and future networks are to use enhanced connectivity in these digital immigrations in different fields like medical, communication, search, and rescue operations among others. The current technologies are using fixed base stations to operate on-site and off-site in the fixed position with its associated problems like poor connectivity. This opens gates for the UAVs technology to be used as a mobile alternative to increase accessibility with a fifth-generation (5G) connectivity that focuses on increased availability and connectivity. There has been less usage of wireless technologies in the medical field. This paper first presents a study on deep learning to medical field application in general, and provides detailed steps that are involved in the multi-armed bandit approach in solving UAV biomedical engineering technologies devices and medical exploration to exploitation dilemma. The paper further presents a detailed description of the bandit network applicability to achieve close optimal medical engineered devices’ performance and efficiency. The simulated results depicted that a multi-armed bandit problem approach can be applied in optimizing the performance of any medical networked device issue compared to the Thompson sampling, Bayesian algorithm, and ε-greedy algorithm. The results obtained further illustrated the optimized utilization of biomedical engineering technologies systems achieving thus close optimal performance on the average period through deep learning of realistic medical situations.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {13},
keywords = {Deep learning, Biomedical technology, Unmanned aerial vehicles}
}

@article{10.1016/j.compind.2011.04.004,
author = {Marchetta, Mart\'{\i}n G. and Mayer, Fr\'{e}d\'{e}rique and Forradellas, Raymundo Q.},
title = {A reference framework following a proactive approach for Product Lifecycle Management},
year = {2011},
issue_date = {September, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {62},
number = {7},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2011.04.004},
doi = {10.1016/j.compind.2011.04.004},
abstract = {Product Lifecycle Management (PLM) has been identified as a key concept within manufacturing industries for improving product quality, time-to-market and costs. Previous works on this field are focused on processes, functions and information models, and those aimed at putting more intelligence on products are related to specific parts of the product lifecycle (e.g. supply chain management, shop floor control). Therefore, there is a lack of a holistic approach to PLM, putting more intelligence on products through the complete lifecycle. In this paper, a PLM framework supported by a proactive approach based on intelligent agents is proposed. The developed model aims at being a first step toward a reference framework for PLM, and complements past works on both product information and business process models (BPM), by putting proactivity on product's behavior. An example of an instantiation of the reference framework is presented as a case study.},
journal = {Comput. Ind.},
month = sep,
pages = {672–683},
numpages = {12},
keywords = {Concurrent Engineering, Intelligent Agent, PLM, Proactive Product, Virtual Enterprise}
}

@inproceedings{10.1609/aaai.v33i01.33019005,
author = {Wu, Xiang and Huang, Huaibo and Patel, Vishal M. and He, Ran and Sun, Zhenan},
title = {Disentangled variational representation for heterogeneous face recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019005},
doi = {10.1609/aaai.v33i01.33019005},
abstract = {Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1105},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.cie.2006.09.003,
author = {Matthews, Jason and Singh, Baljinder and Mullineux, Glen and Medland, Tony},
title = {Constraint-based approach to investigate the process flexibility of food processing equipment},
year = {2006},
issue_date = {December, 2006},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {51},
number = {4},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2006.09.003},
doi = {10.1016/j.cie.2006.09.003},
abstract = {Over the last decade the UK food processing industry has become increasing competitive. This leads the sector to maintain high numbers of product variations. Although some of these products are stable over long periods, others are short lived or seasonal. The ability to handle both the complexity of process and large variations in product format creates extreme difficulties in ensuring that the existing manufacturing, handling and packaging equipment has the process flexibility to cope. This paper presents an approach for investigating the performance envelopes of machines utilizing a constraint modelling environment. The approach aims to provide the engineer with enhanced understanding of the range of functionality of a given machine and provides the possibility of redesign to process variant product.},
journal = {Comput. Ind. Eng.},
month = dec,
pages = {809–820},
numpages = {12},
keywords = {Constraint-based modelling, Design knowledge, Food processing, Machine analysis, Process flexibility}
}

@article{10.5555/1370314.1370590,
author = {Benoit, Anne and Robert, Yves},
title = {Mapping pipeline skeletons onto heterogeneous platforms},
year = {2008},
issue_date = {June, 2008},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {68},
number = {6},
issn = {0743-7315},
abstract = {Mapping applications onto parallel platforms is a challenging problem, that becomes even more difficult when platforms are heterogeneous - nowadays a standard assumption. A high-level approach to parallel programming not only eases the application developer's task, but it also provides additional information which can help realize an efficient mapping of the application. In this paper, we discuss the mapping of pipeline skeletons onto different types of platforms: Fully Homogeneous platforms with identical processors and interconnection links; Communication Homogeneous platforms, with identical links but different-speed processors; and finally, Fully Heterogeneous platforms. We assume that a pipeline stage must be mapped on a single processor, and we establish new theoretical complexity results for different mapping policies: a mapping can be required to be one-to-one (a processor is assigned at most one stage), or interval-based (a processor is assigned an interval of consecutive stages), or fully general. In particular, we show that determining the optimal interval-based mapping is NP-hard for Communication Homogeneous platforms, and this result assesses the complexity of the well-known chains-to-chains problem for different-speed processors. We provide several efficient polynomial heuristics for the most important policy/platform combination, namely interval-based mappings on Communication Homogeneous platforms. These heuristics are compared to the optimal result provided by the formulation of the problem in terms of the solution of an integer linear program, for small problem instances.},
journal = {J. Parallel Distrib. Comput.},
month = jun,
pages = {790–808},
numpages = {19},
keywords = {Algorithmic skeletons, Complexity results, Heterogeneous clusters, Heuristics, Pipeline, Scheduling}
}

@article{10.1016/j.asoc.2020.106121,
author = {Leelathakul, Nutthanon and Rimcharoen, Sunisa},
title = {Generating Kranok patterns with an interactive evolutionary algorithm},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106121},
doi = {10.1016/j.asoc.2020.106121},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {17},
keywords = {Interactive evolutionary algorithm, Thai drawing, Kranok pattern, B\'{e}zier curve, Generated art}
}

@book{10.5555/2901596,
author = {Talia, Domenico and Trunfio, Paolo and Marozzo, Fabrizio},
title = {Data Analysis in the Cloud: Models, Techniques and Applications},
year = {2015},
isbn = {0128028815},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
edition = {1st},
abstract = {Data Analysis in the Cloud introduces and discusses models, methods, techniques, and systems to analyze the large number of digital data sources available on the Internet using the computing and storage facilities of the cloud. Coverage includes scalable data mining and knowledge discovery techniques together with cloud computing concepts, models, and systems. Specific sections focus on map-reduce and NoSQL models. The book also includes techniques for conducting high-performance distributed analysis of large data on clouds. Finally, the book examines research trends such as Big Data pervasive computing, data-intensive exascale computing, and massive social network analysis.Introduces data analysis techniques and cloud computing conceptsDescribes cloud-based models and systems for Big Data analyticsProvides examples of the state-of-the-art in cloud data analysisExplains how to develop large-scale data mining applications on cloudsOutlines the main research trends in the area of scalable Big Data analysis}
}

@inproceedings{10.1109/ASE.2015.47,
author = {Li, Yi and Rubin, Julia and Chechik, Marsha},
title = {Semantic slicing of software version histories},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.47},
doi = {10.1109/ASE.2015.47},
abstract = {Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a specific subset of the change history, "inheriting" additional, unwanted functionality.In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We refer to our approach, CSLICER, as semantic slicing of version histories. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and instantiate it in a specific implementation for Java projects managed in Git. We evaluate the correctness and effectiveness of our approach on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {686–696},
numpages = {11},
keywords = {dependency, software changes, version history},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1007/978-3-642-36757-1_3,
author = {Shamsaei, Azalia and Amyot, Daniel and Pourshahid, Alireza and Braun, Edna and Yu, Eric and Mussbacher, Gunter and Tawhid, Rasha and Cartwright, Nick},
title = {An approach to specify and analyze goal model families},
year = {2012},
isbn = {9783642367564},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36757-1_3},
doi = {10.1007/978-3-642-36757-1_3},
abstract = {Goal-oriented languages have been used for years to model and reason about functional, non-functional, and legal requirements. It is however difficult to develop and maintain these models, especially when many models overlap with each other. This becomes an even bigger challenge when a single, generic model is used to capture a family of related goal models but different evaluations are required for each individual family member. In this work, we use ITU-T's Goal-oriented Requirement Language (GRL) and the jUCMNav tool to illustrate the problem and to formulate a solution that exploits the flexibility of standard GRL. In addition, we report on our recent experience on the modeling of aerodrome regulations. We demonstrate the usefulness of specifying families of goal models to address challenges associated with the maintenance of models used in the regulatory domain. We finally define and illustrate a new tool-supported algorithm used to evaluate individual goal models that are members of the larger family model.},
booktitle = {Proceedings of the 7th International Conference on System Analysis and Modeling: Theory and Practice},
pages = {34–52},
numpages = {19},
keywords = {URN, goal modeling, goal-oriented requirement language, key performance indicator, legal compliance, tools, variability},
location = {Innsbruck, Austria},
series = {SAM'12}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1016/j.jss.2012.09.016,
author = {Lucena, Carlos and Nunes, Ingrid},
title = {Contributions to the emergence and consolidation of Agent-oriented Software Engineering},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.09.016},
doi = {10.1016/j.jss.2012.09.016},
abstract = {Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between MAS and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an AOSE community. This paper presents the history of AOSE at LES by discussing the sub-areas of MAS Software Engineering research and development that have been focus of the LES research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and MASs with a concern for scalability of solutions.},
journal = {J. Syst. Softw.},
month = apr,
pages = {890–904},
numpages = {15},
keywords = {Agent-oriented Software Engineering, LES, Multiagent systems, PUC-Rio, SBES 25 years}
}

@article{10.1007/s10845-013-0815-1,
author = {Fan, Beibei and Qi, Guoning and Hu, Xiaomei and Yu, Tao},
title = {A network methodology for structure-oriented modular product platform planning},
year = {2015},
issue_date = {June      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-013-0815-1},
doi = {10.1007/s10845-013-0815-1},
journal = {J. Intell. Manuf.},
month = jun,
pages = {553–570},
numpages = {18},
keywords = {Breadth first search, Generic-modules connection network, Main structure, Modular product platform, Parts connection network}
}

@article{10.1007/s10664-014-9306-z,
author = {Petersen, Kai and Gencel, Cigdem and Asghari, Negin and Betz, Stefanie},
title = {An elicitation instrument for operationalising GQM+Strategies (GQM+S-EI)},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9306-z},
doi = {10.1007/s10664-014-9306-z},
abstract = {A recent approach for measurement program planning, GQM+Strategies, provides an important extension to existing approaches linking measurements and improvement activities to strategic goals and ways to achieve these goals. There is a need for instruments aiding in eliciting information from stakeholders to use GQM+Strategies. The success of GQM+Strategies highly depends on accurately identifying goals, strategies and information needs from stakeholders. The research aims at providing an instrument (called GQM+S-EI), aiding practitioners to accurately elicit information needed by GQM+Strategies (capturing goals, strategies and information needs). The research included two phases. In the first phase, using action research method, the GQM+S-EI was designed in three iterations in Ericsson AB. Thereafter, a case study was conducted to evaluate whether the information elicited with the designed instrument following the defined process was accurate and complete. We identified that the industry requires elicitation instruments that are capable to elicit information from stakeholders, not having to know about the concepts (e.g. goals and strategies). The case study results showed that our proposed instrument is capable of accurately and completely capturing the needed information from the stakeholders. We conclude that GQM+S-EI can be used for accurately and completely eliciting the information needed by goal driven measurement frameworks. The instrument has been successfully transferred to Ericsson AB for measurement program planning.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {968–1005},
numpages = {38},
keywords = {Action research, Case study, GQM+Strategies, Goal driven measurement, Goal elicitation, Information need, Strategy elicitation}
}

@article{10.1016/j.patcog.2016.11.018,
author = {Ma, Xiaolong and Zhu, Xiatian and Gong, Shaogang and Xie, Xudong and Hu, Jianming and Lam, Kin-Man and Zhong, Yisheng},
title = {Person re-identification by unsupervised video matching},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {65},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.018},
doi = {10.1016/j.patcog.2016.11.018},
journal = {Pattern Recogn.},
month = may,
pages = {197–210},
numpages = {14},
keywords = {Person re-identification, Action recognition, Gait recognition, Video matching, Temporal sequence matching, Spatio-temporal pyramids, Time shift}
}

@inproceedings{10.5555/285730.285780,
author = {Lam, Jimmy and Delosme, Jean-Marc},
title = {Performance of a new annealing schedule},
year = {1988},
isbn = {0818688645},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {A new simulated annealing schedule has been developed; its application to the standard cell placement and the traveling salesman problems results in a two to twenty-four times speedup over annealing schedules currently available in the literature. Since it uses only statistical quantities, the annealing schedule is applicable to general combinatorial optimization problems.},
booktitle = {Proceedings of the 25th ACM/IEEE Design Automation Conference},
pages = {306–311},
numpages = {6},
location = {Atlantic City, New Jersey, USA},
series = {DAC '88}
}

@article{10.1007/s10664-015-9364-x,
author = {Passos, Leonardo and Teixeira, Leopoldo and Dintzner, Nicolas and Apel, Sven and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof and Borba, Paulo and Guo, Jianmei},
title = {Coevolution of variability models and related software artifacts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9364-x},
doi = {10.1007/s10664-015-9364-x},
abstract = {Variant-rich software systems offer a large degree of customization, allowing users to configure the target system according to their preferences and needs. Facing high degrees of variability, these systems often employ variability models to explicitly capture user-configurable features (e.g., systems options) and the constraints they impose. The explicit representation of features allows them to be referenced in different variation points across different artifacts, enabling the latter to vary according to specific feature selections. In such settings, the evolution of variability models interplays with the evolution of related artifacts, requiring the two to evolve together, or coevolve. Interestingly, little is known about how such coevolution occurs in real-world systems, as existing research has focused mostly on variability evolution as it happens in variability models only. Furthermore, existing techniques supporting variability evolution are usually validated with randomly-generated variability models or evolution scenarios that do not stem from practice. As the community lacks a deep understanding of how variability evolution occurs in real-world systems and how it relates to the evolution of different kinds of software artifacts, it is not surprising that industry reports existing tools and solutions ineffective, as they do not handle the complexity found in practice. Attempting to mitigate this overall lack of knowledge and to support tool builders with insights on how variability models coevolve with other artifact types, we study a large and complex real-world variant-rich software system: the Linux kernel. Specifically, we extract variability-coevolution patterns capturing changes in the variability model of the Linux kernel with subsequent changes in Makefiles and C source code. From the analysis of the patterns, we report on findings concerning evolution principles found in the kernel, and we reveal deficiencies in existing tools and theory when handling changes captured by our patterns.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1744–1793},
numpages = {50},
keywords = {Evolution, Linux, Patterns, Software product lines, Variability}
}

@inproceedings{10.5555/646972.713520,
author = {Bratthall, Lars and Hasselberg, Johan and Hoffman, Brad and Korendo, Zbigniew and Schilli, Bruno and Gundersen, Lars},
title = {Component Certification - What is the Value?},
year = {2002},
isbn = {3540002340},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Component-based software is becoming increasingly popular as a means to create value through improved integration across multiple parts of a plant or business. However, sometimes components that are supposed to be integrated cannot be integrated in the same way that the user envisions at time of acquiring the component. Certification of components is one way of ensuring that components adhere to certain standards for integration. This study presents findings from two case studies assessing the value of one particular certification program from ABB, called Industrial IT Enabled. A method for facilitating complex decision-making, Incomplete Pairwise Comparison (IPC), has been used to identify the relative value of Industrial IT Enabled for customers, as well as for ABB itself. Results indicate that the certification provides practically significant added value to customers, as well as to ABB. It is believed that these results, to some extent, can be valid in other similar certification programs.},
booktitle = {Proceedings of the 4th International Conference on Product Focused Software Process Improvement},
pages = {119–133},
numpages = {15},
series = {PROFES '02}
}

@article{10.1007/s10664-020-09910-y,
author = {Wang, Yingying and Kadiyala, Harshavardhan and Rubin, Julia},
title = {Promises and challenges of microservices: an exploratory study},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09910-y},
doi = {10.1007/s10664-020-09910-y},
abstract = {Microservice-based architecture is a SOA-inspired principle of building complex systems as a composition of small, loosely coupled components that communicate with each other using language-agnostic APIs. This architectural principle is now becoming increasingly popular in industry due to its advantages, such as greater software development agility and improved scalability of deployed applications. In this work, we aim at collecting and categorizing best practices, challenges, and some existing solutions for these challenges employed by practitioners successfully developing microservice-based applications for commercial use. Specifically, we focus our study on “mature” teams developing microservice-based applications for at least two years, explicitly excluding “newcomers” to the field. We conduct a broad, mixed-method study that includes in-depth interviews with 21 practitioners and a follow-up online survey with 37 respondents, covering 37 companies in total. Our study shows that, in several cases, practitioners opt to deviate from the “standard” advice, e.g., instead of splitting microservices by business capabilities, they focus on resource consumption and intended deployment infrastructure. Some also choose to refrain from using multiple programming languages for implementing their microservices, as that practice hinders reuse opportunities. In fact, our study participants identified robust and shared infrastructural support established early on in the development process as one of the main factors contributing to their success. They also identified several pressing challenges related to the efficient managing of common code across services and the support of product variants. The results of our study can benefit practitioners who are interested to learn from each other, borrow successful ideas, and avoid common mistakes. It can also inform researchers and inspire novel solutions to some of the identified challenges.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {44},
keywords = {Microservices, Cloud-native applications, Development practices, Empirical study}
}

@inproceedings{10.5555/3155562.3155664,
author = {Krismayer, Thomas and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Mining constraints for event-based monitoring in systems of systems},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {826–831},
numpages = {6},
keywords = {Constraint mining, event-based monitoring, systems of systems},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@article{10.1145/2925990,
author = {Copil, Georgiana and Moldovan, Daniel and Truong, Hong-Linh and Dustdar, Schahram},
title = {rSYBL: A Framework for Specifying and Controlling Cloud Services Elasticity},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/2925990},
doi = {10.1145/2925990},
abstract = {Cloud applications can benefit from the on-demand capacity of cloud infrastructures, which offer computing and data resources with diverse capabilities, pricing, and quality models. However, state-of-the-art tools mainly enable the user to specify “if-then-else” policies concerning resource usage and size, resulting in a cumbersome specification process that lacks expressiveness for enabling the control of complex multilevel elasticity requirements.In this article, first we propose SYBL, a novel language for specifying elasticity requirements at multiple levels of abstraction. Second, we design and develop the rSYBL framework for controlling cloud services at multiple levels of abstractions. To enforce user-specified requirements, we develop a multilevel elasticity control mechanism enhanced with conflict resolution. rSYBL supports different cloud providers and is highly extensible, allowing service providers or developers to define their own connectors to the desired infrastructures or tools. We validate it through experiments with two distinct services, evaluating rSYBL over two distinct cloud infrastructures, and showing the importance of multilevel elasticity control.},
journal = {ACM Trans. Internet Technol.},
month = aug,
articleno = {18},
numpages = {20},
keywords = {Cloud computing, control, elasticity, elasticity requirements}
}

@article{10.1016/j.jss.2009.08.032,
author = {Tang, Antony and Avgeriou, Paris and Jansen, Anton and Capilla, Rafael and Ali Babar, Muhammad},
title = {A comparative study of architecture knowledge management tools},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.08.032},
doi = {10.1016/j.jss.2009.08.032},
abstract = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools.},
journal = {J. Syst. Softw.},
month = mar,
pages = {352–370},
numpages = {19},
keywords = {Architectural design, Architectural knowledge management tool, Design rationale}
}

@book{10.5555/2815511,
author = {Wu, Caesar and Buyya, Rajkumar},
title = {Cloud Data Centers and Cost Modeling: A Complete Guide To Planning, Designing and Building a Cloud Data Center},
year = {2015},
isbn = {012801413X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development}
}

@proceedings{10.1145/2986012,
title = {Onward! 2016: Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2016},
isbn = {9781450340762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1006/jpdc.1997.1413,
author = {Chakrabarti, Soumen and Demmel, James and Yelick, Katherine},
title = {Models and Scheduling Algorithms for Mixed Data and Task Parallel Programs},
year = {1997},
issue_date = {Dec. 15, 1997},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {47},
number = {2},
issn = {0743-7315},
url = {https://doi.org/10.1006/jpdc.1997.1413},
doi = {10.1006/jpdc.1997.1413},
abstract = {An increasing number of scientific programs exhibit two forms of parallelism, often in a nested fashion. At the outer level, the application comprises coarse-grained task parallelism, with dependencies between tasks reflected by an acyclic graph. At the inner level, each node of the graph is a data-parallel operation on arrays. Designers of languages, compilers, and runtime systems are building mechanisms to support such applications by providing processor groups and array remapping capabilities. In this paper we explore how to supplement these mechanisms with policy. What properties of an application, its data size, and the parallel machine determine the maximum potential gains from using both kinds of parallelism? It turns out that large gains can be expected only for specific task graph structures. For such applications, what are practical and effective ways to allocate processors to the nodes of the task graph? In principle one could solve the NP-complete problem of finding the best possible allocation of arbitrary processor subsets to nodes in the task graph. Instead of this, our analysis and simulations show that a simpleswitchedscheduling paradigm, which alternates between pure task and pure data parallelism, provides nearly optimal performance for the task graphs considered here. Furthermore, our scheme is much simpler to implement, has less overhead than the optimal allocation, and would be attractive even if the optimal allocation was free to compute. To evaluate switching in real applications, we implemented a switching task scheduler in the parallel numerical library ScaLAPACK and used it in a nonsymmetric eigenvalue program. Even for fairly large input sizes, the efficiency improves by factors of 1.5 on the Intel Paragon and 2.5 on the IBM SP-2. The remapping and scheduling overhead is negligible, between 0.5 and 5%.},
journal = {J. Parallel Distrib. Comput.},
month = dec,
pages = {168–184},
numpages = {17}
}

@inproceedings{10.1145/1188966.1188976,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2006},
publisher = {IBM Corp.},
address = {USA},
url = {https://doi.org/10.1145/1188966.1188976},
doi = {10.1145/1188966.1188976},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {7–es},
location = {Toronto, Ontario, Canada},
series = {CASCON '06}
}

@article{10.1016/j.aei.2011.06.004,
author = {Verhagen, Wim J. C. and Bermell-Garcia, Pablo and van Dijk, Reinier E. C. and Curran, Richard},
title = {A critical review of Knowledge-Based Engineering: An identification of research challenges},
year = {2012},
issue_date = {January, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {26},
number = {1},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2011.06.004},
doi = {10.1016/j.aei.2011.06.004},
abstract = {Knowledge-Based Engineering (KBE) is a research field that studies methodologies and technologies for capture and re-use of product and process engineering knowledge. The objective of KBE is to reduce time and cost of product development, which is primarily achieved through automation of repetitive design tasks while capturing, retaining and re-using design knowledge. Published research on KBE is not very extensive and also quite dispersed; this paper is an effort to collect and review existing literature on KBE. A total of 50 research contributions have been analysed. The main objectives of this analysis are to identify the theoretical foundations of KBE and to identify research issues within KBE, pointing out the challenges and pitfalls that currently prohibit a wider adoption of KBE while suggesting avenues for further research. Key findings include (a) the necessity for improved methodological support and adherence, (b) better transparency and traceability of knowledge, (c) the necessity for a quantitative framework to assess the viability and success of KBE development and implementation projects, and (d) the opportunity to move towards mass customization approaches through distributed deployment of KBE in the extended enterprise.},
journal = {Adv. Eng. Inform.},
month = jan,
pages = {5–15},
numpages = {11},
keywords = {Intelligent systems, KBE, Knowledge, Knowledge Engineering, Knowledge-Based Engineering, Review}
}

@article{10.1145/2932707,
author = {Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.},
title = {Computational Health Informatics in the Big Data Age: A Survey},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2932707},
doi = {10.1145/2932707},
abstract = {The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {12},
numpages = {36},
keywords = {4V challenges, Big data analytics, clinical decision support, computational health informatics, data mining, machine learning, survey}
}

@article{10.1007/s10664-015-9401-9,
author = {Jonsson, Leif and Borg, Markus and Broman, David and Sandahl, Kristian and Eldh, Sigrid and Runeson, Per},
title = {Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9401-9},
doi = {10.1007/s10664-015-9401-9},
abstract = {Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial application and that it outperforms the use of individual classifiers for bug assignment, reaching prediction accuracies from 50 % to 89 % when large training sets are used. In addition, we show how old training data can decrease the prediction accuracy of bug assignment. We advice industry to use SG for bug assignment in proprietary contexts, using at least 2,000 bug reports for training. Finally, we highlight the importance of not solely relying on results from cross-validation when evaluating automated bug assignment.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1533–1578},
numpages = {46},
keywords = {Bug assignment, Bug reports, Classification, Ensemble learning, Industrial scale; Large scale, Machine learning}
}

@article{10.1016/j.jss.2013.02.061,
author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and Mcminn, Phil},
title = {An orchestrated survey of methodologies for automated software test case generation},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.02.061},
doi = {10.1016/j.jss.2013.02.061},
abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1978–2001},
numpages = {24},
keywords = {Adaptive random testing, Combinatorial testing, Model-based testing, Orchestrated survey, Search-based software testing, Software testing, Symbolic execution, Test automation, Test case generation}
}

@article{10.1016/j.cie.2016.09.023,
author = {Addo-Tenkorang, Richard and Helo, Petri T.},
title = {Big data applications in operations/supply-chain management},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.09.023},
doi = {10.1016/j.cie.2016.09.023},
abstract = {Harnessing optimum value from industrial data increased in the last two decades.A detailed review of "big data" application in operations/SC management processes.Proposed (Value-adding - V5) framework for operation/SC management. PurposeBig data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of "big data." Therefore, this paper attempts to thoroughly investigate "big data," its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of "big data" and its extension into "big data II"/IoT-value-adding perspectives by proposing a value-adding framework. Methodology/research approachThe research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of "big data" applications between 2010 and 2015. Findings/resultsThe four main attributes or factors identified with "big data" include - big data development sources (Variety - V1), big data acquisition (Velocity - V2), big data storage (Volume - V3), and finally big data analysis (Veracity - V4). However, the study of "big data" has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding - V5) - "Big Data cloud computing perspective/Internet of Things (IoT)". Hence, the four Vs of "big data" is now expanded into five Vs. Originality/value of researchThis paper presents original literature review research discussing "big data" issues, trends and perspectives in operations/supply-chain management in order to propose "Big data II" (IoT - Value-adding) framework. This proposed framework is supposed or assumed to be an extension of "big data" in a value-adding perspective, thus proposing that "big data" be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.},
journal = {Comput. Ind. Eng.},
month = nov,
pages = {528–543},
numpages = {16},
keywords = {Big data - applications and analysis, Cloud computing, Internet of Things (IoT), Master database management, Operations/supply-chain management}
}

@article{10.1016/j.scico.2014.02.009,
title = {A feature model of actor, agent, functional, object, and procedural programming languages},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {98},
number = {P2},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2014.02.009},
doi = {10.1016/j.scico.2014.02.009},
abstract = {The number of programming languages is large and steadily increasing. However, little structured information and empirical evidence is available to help software engineers assess the suitability of a language for a particular development project or software architecture.We argue that these shortages are partly due to a lack of high-level, objective programming language feature assessment criteria: existing advice to practitioners is often based on ill-defined notions of 'paradigms' 3, p. xiii and 'orientation', while researchers lack a shared common basis for generalisation and synthesis of empirical results.This paper presents a feature model constructed from the programmer's perspective, which can be used to precisely compare general-purpose programming languages in the actor-oriented, agent-oriented, functional, object-oriented, and procedural categories. The feature model is derived from the existing literature on general concepts of programming, and validated with concrete mappings of well-known languages in each of these categories. The model is intended to act as a tool for both practitioners and researchers, to facilitate both further high-level comparative studies of programming languages, and detailed investigations of feature usage and efficacy in specific development contexts. A survey of existing programming language comparisons and comparison techniques.Definitions of actor, agent, functional, object, and procedural programming concepts.A feature model of general-purpose programming languages.Mappings from five languages (C, Erlang, Haskell, Jason, and Java) to this model.},
journal = {Sci. Comput. Program.},
month = feb,
pages = {120–139},
numpages = {20}
}

@article{10.1145/3134844,
author = {Pradhan, Subhav and Dubey, Abhishek and Khare, Shweta and Nannapaneni, Saideep and Gokhale, Aniruddha and Mahadevan, Sankaran and Schmidt, Douglas C. and Lehofer, Martin},
title = {CHARIOT: Goal-Driven Orchestration Middleware for Resilient IoT Systems},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3134844},
doi = {10.1145/3134844},
abstract = {An emerging trend in Internet of Things (IoT) applications is to move the computation (cyber) closer to the source of the data (physical). This paradigm is often referred to as edge computing. If edge resources are pooled together, they can be used as decentralized shared resources for IoT applications, providing increased capacity to scale up computations and minimize end-to-end latency. Managing applications on these edge resources is hard, however, due to their remote, distributed, and (possibly) dynamic nature, which necessitates autonomous management mechanisms that facilitate application deployment, failure avoidance, failure management, and incremental updates. To address these needs, we present CHARIOT, which is orchestration middleware capable of autonomously managing IoT systems consisting of edge resources and applications.CHARIOT implements a three-layer architecture. The topmost layer comprises a system description language, the middle layer comprises a persistent data storage layer and the corresponding schema to store system information, and the bottom layer comprises a management engine that uses information stored persistently to formulate constraints that encode system properties and requirements, thereby enabling the use of satisfiability modulo theory solvers to compute optimal system (re)configurations dynamically at runtime. This article describes the structure and functionality of CHARIOT and evaluates its efficacy as the basis for a smart parking system case study that uses sensors to manage parking spaces.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jun,
articleno = {16},
numpages = {37},
keywords = {Autonomous management, cyber-physical systems, orchestration middleware, resilience at the edge}
}

@article{10.1016/j.scico.2010.02.001,
author = {Apel, Sven and Lengauer, Christian and M\"{o}ller, Bernhard and K\"{a}stner, Christian},
title = {An algebraic foundation for automatic feature-based program synthesis},
year = {2010},
issue_date = {November, 2010},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {75},
number = {11},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2010.02.001},
doi = {10.1016/j.scico.2010.02.001},
abstract = {Feature-Oriented Software Development provides a multitude of formalisms, methods, languages, and tools for building variable, customizable, and extensible software. Along different lines of research, different notions of a feature have been developed. Although these notions have similar goals, no common basis for evaluation, comparison, and integration exists. We present a feature algebra that captures the key ideas of feature orientation and that provides a common ground for current and future research in this field, on which also alternative options can be explored. Furthermore, our algebraic framework is meant to serve as a basis for the development of the technology of automatic feature-based program synthesis and architectural metaprogramming.},
journal = {Sci. Comput. Program.},
month = nov,
pages = {1022–1047},
numpages = {26},
keywords = {Architectural metaprogramming, Automatic feature-based program synthesis, Feature algebra, Feature composition, Feature structure tree, Feature-oriented software development, Quantification, Quark model, Superimposition, Weaving}
}

@article{10.1145/2339118.2339120,
author = {Abd-El-Malek, Michael and Wachs, Matthew and Cipar, James and Sanghi, Karan and Ganger, Gregory R. and Gibson, Garth A. and Reiter, Michael K.},
title = {File system virtual appliances: Portable file system implementations},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/2339118.2339120},
doi = {10.1145/2339118.2339120},
abstract = {File system virtual appliances (FSVAs) address the portability headaches that plague file system (FS) developers. By packaging their FS implementation in a virtual machine (VM), separate from the VM that runs user applications, they can avoid the need to port the file system to each operating system (OS) and OS version. A small FS-agnostic proxy, maintained by the core OS developers, connects the FSVA to whatever OS the user chooses. This article describes an FSVA design that maintains FS semantics for unmodified FS implementations and provides desired OS and virtualization features, such as a unified buffer cache and VM migration. Evaluation of prototype FSVA implementations in Linux and NetBSD, using Xen as the virtual machine manager (VMM), demonstrates that the FSVA architecture is efficient, FS-agnostic, and able to insulate file system implementations from OS differences that would otherwise require explicit porting.},
journal = {ACM Trans. Storage},
month = sep,
articleno = {9},
numpages = {26},
keywords = {Operating systems, file systems, virtual machines}
}

@article{10.1007/s10851-021-01035-1,
author = {Pascal, Barbara and Vaiter, Samuel and Pustelnik, Nelly and Abry, Patrice},
title = {Automated Data-Driven Selection of the Hyperparameters for Total-Variation-Based Texture Segmentation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {63},
number = {7},
issn = {0924-9907},
url = {https://doi.org/10.1007/s10851-021-01035-1},
doi = {10.1007/s10851-021-01035-1},
abstract = {Penalized least squares are widely used in signal and image processing. Yet, it suffers from a major limitation since it requires fine-tuning of the regularization parameters. Under assumptions on the noise probability distribution, Stein-based approaches provide unbiased estimator of the quadratic risk. The Generalized Stein Unbiased Risk Estimator is revisited to handle correlated Gaussian noise without requiring to invert the covariance matrix. Then, in order to avoid expansive grid search, it is necessary to design algorithmic scheme minimizing the quadratic risk with respect to regularization parameters. This work extends the Stein’s Unbiased GrAdient estimator of the Risk of Deledalle et al. (SIAM J Imaging Sci 7(4):2448–2487, 2014) to the case of correlated Gaussian noise, deriving a general automatic tuning of regularization parameters. First, the theoretical asymptotic unbiasedness of the gradient estimator is demonstrated in the case of general correlated Gaussian noise. Then, the proposed parameter selection strategy is particularized to fractal texture segmentation, where problem formulation naturally entails inter-scale and spatially correlated noise. Numerical assessment is provided, as well as discussion of the practical issues.},
journal = {J. Math. Imaging Vis.},
month = sep,
pages = {923–952},
numpages = {30},
keywords = {Regularization parameters tuning, SURE, Estimation, Gaussian noise, Texture, segmentation, Algorithmic differentiation}
}

@book{10.5555/2505465,
author = {Schmidt, Richard},
title = {Software Engineering: Architecture-driven Software Development},
year = {2013},
isbn = {0124077684},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Engineering: Architecture-driven Software Development is the first comprehensive guide to the underlying skills embodied in the IEEE's Software Engineering Body of Knowledge (SWEBOK) standard. Standards expert Richard Schmidt explains the traditional software engineering practices recognized for developing projects for government or corporate systems. Software engineering education often lacks standardization, with many institutions focusing on implementation rather than design as it impacts product architecture. Many graduates join the workforce with incomplete skills, leading to software projects that either fail outright or run woefully over budget and behind schedule. Additionally, software engineers need to understand system engineering and architecture-the hardware and peripherals their programs will run on. This issue will only grow in importance as more programs leverage parallel computing, requiring an understanding of the parallel capabilities of processors and hardware. This book gives both software developers and system engineers key insights into how their skillsets support and complement each other. With a focus on these key knowledge areas, Software Engineering offers a set of best practices that can be applied to any industry or domain involved in developing software products. A thorough, integrated compilation on the engineering of software products, addressing the majority of the standard knowledge areas and topics Offers best practices focused on those key skills common to many industries and domains that develop software Learn how software engineering relates to systems engineering for better communication with other engineering professionals within a project environment}
}

@inproceedings{10.5555/2980539.2980703,
author = {Rosales, R\'{o}mer and Sclaroff, Stan},
title = {Learning body pose via Specialized Maps},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion.},
booktitle = {Proceedings of the 15th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {1263–1270},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}

@article{10.1007/s00766-021-00358-0,
author = {Barrett, Don and Mazzuchi, Thomas and Sarkani, Shahram},
title = {A quantitative comparison of the effects of modeling approaches on system verification using a controlled challenge problem},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-021-00358-0},
doi = {10.1007/s00766-021-00358-0},
abstract = {To reduce program risks, engineering methods capitalizing on modeling and machine assistance have been extensively investigated within systems engineering (and more specifically requirements engineering) literature over the past 20&nbsp;years. However, there are few quantitative comparisons between model-based approaches and legacy document-centric approaches. Studies have shown that the lack of data regarding improvements of modeling has decelerated the adoption of model-based practices. To help address this gap, the authors conducted a screening experiment to compare the effects of modeling on an engineer’s ability to determine if a system has met its originating requirements. First, a notional acquisition program was created based on an unmanned aerial system, including originating requirements as well as both document and model-based design artifacts. Requirements were captured in both a traditional requirements document and a goal oriented requirements engineering model. System implementation data were capturing in both traditional document artifacts and a SysML model. Participants of varying experience levels used randomized combinations of document and model-based approaches to determine whether the notional system met its requirements. The experiment measured the review duration and accuracy, permitting analysis of the effects of model-based approaches for both requirements and system implementation data. The results of the experiment showed that use of a requirements model did not statistically effect the review. A system implementation model was shown to improve novice participants’ reviews, but did not statistically effect experienced participants. The results of this study should inform future research on the use of models, particularly the return on the modeling investment.},
journal = {Requir. Eng.},
month = dec,
pages = {557–580},
numpages = {24},
keywords = {Model-based systems engineering, Requirements engineering, Goal oriented requirements engineering, SysML, Requirements model}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1016/j.jss.2021.111037,
author = {Echeverr\'{\i}a, Jorge and Font, Jaime and P\'{e}rez, Francisca and Cetina, Carlos},
title = {Comparison of search strategies for feature location in software models},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111037},
doi = {10.1016/j.jss.2021.111037},
journal = {J. Syst. Softw.},
month = nov,
numpages = {21},
keywords = {Feature location in models, Search strategies}
}

@article{10.1109/TCBB.2013.30,
author = {Li, Yifeng and Ngom, Alioune},
title = {Nonnegative Least-Squares Methods for the Classification of High-Dimensional Biological Data},
year = {2013},
issue_date = {March 2013},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {10},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2013.30},
doi = {10.1109/TCBB.2013.30},
abstract = {Microarray data can be used to detect diseases and predict responses to therapies through classification models. However, the high dimensionality and low sample size of such data result in many computational problems such as reduced prediction accuracy and slow classification speed. In this paper, we propose a novel family of nonnegative least-squares classifiers for high-dimensional microarray gene expression and comparative genomic hybridization data. Our approaches are based on combining the advantages of using local learning, transductive learning, and ensemble learning, for better prediction performance. To study the performances of our methods, we performed computational experiments on 17 well-known data sets with diverse characteristics. We have also performed statistical comparisons with many classification techniques including the well-performing SVM approach and two related but recent methods proposed in literature. Experimental results show that our approaches are faster and achieve generally a better prediction performance over compared methods.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {447–456},
numpages = {10},
keywords = {Medicine, algorithms, classifier design and evaluation}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@article{10.1016/j.ins.2021.08.072,
author = {Wang, Xiaokang and Wang, Huiwen and Wang, Zhichao and Lu, Shan and Fan, Ying},
title = {Risk spillover network structure learning for correlated financial assets: A directed acyclic graph approach},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {580},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.08.072},
doi = {10.1016/j.ins.2021.08.072},
journal = {Inf. Sci.},
month = nov,
pages = {152–173},
numpages = {22},
keywords = {Directed acyclic graph, Partial least squares, Risk spillover, Variable screening}
}

@book{10.1145/3191315,
editor = {Kifer, Michael and Liu, Yanhong Annie},
title = {Declarative Logic Programming: Theory, Systems, and Applications},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {20},
abstract = {Logic Programming (LP) is at the nexus of knowledge representation, AI, mathematical logic, databases, and programming languages. It allows programming to be more declarative, by specifying “what” to do instead of “how” to do it. This field is fascinating and intellectually stimulating due to the fundamental interplay among theory, systems, and applications brought about by logic. Several books cover the basics of LP but they focus mostly on the Prolog language. There is generally a lack of accessible collections of articles covering the key aspects of LP, such as the well-founded vs. stable semantics for negation, constraints, object-oriented LP, updates, probabilistic LP, and implementation methods, including top-down vs. bottom-up evaluation and tabling.For systems, the situation is even less satisfactory, lacking expositions of LP inference machinery that supports tabling and other state-of-the-art implementation techniques. There is also a dearth of articles about systems that support truly declarative languages, especially those that tie into first-order logic, mathematical programming, and constraint programming. Also rare are surveys of challenging application areas of LP, such as bioinformatics, natural language processing, verification, and planning, as well as analysis of LP applications based on language abstractions and implementations methods.The goal of this book is to help fill in the void in the literature with state-of-the-art surveys on key aspects of LP. Much attention was paid to making these surveys accessible to researchers, practitioners, and graduate students alike.}
}

@inproceedings{10.1145/3239372.3239388,
author = {Kusmenko, Evgeny and Rumpe, Bernhard and Schneiders, Sascha and von Wenckstern, Michael},
title = {Highly-Optimizing and Multi-Target Compiler for Embedded System Models: C++ Compiler Toolchain for the Component and Connector Language EmbeddedMontiArc},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239388},
doi = {10.1145/3239372.3239388},
abstract = {Component and Connector (C&amp;C) models, with their corresponding code generators, are widely used by large automotive manufacturers to develop new software functions for embedded systems interacting with their environment; C&amp;C example applications are engine control, remote parking pilots, and traffic sign assistance. This paper presents a complete toolchain to design and compile C&amp;C models to highly-optimized code running on multiple targets including x86/x64, ARM and WebAssembly. One of our contributions are algebraic and threading optimizations to increase execution speed for computationally expensive tasks. A further contribution is an extensive case study with over 50 experiments. This case study compares the runtime speed of the generated code using different compilers and mathematical libraries. These experiments showed that programs produced by our compiler are at least two times faster than the ones compiled by MATLAB/Simulink for machine learning applications such as image clustering for object detection. Additionally, our compiler toolchain provides a complete model-based testing framework and plug-in points for middleware integration. We make all materials including models and toolchains electronically available for inspection and further research.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {447–457},
numpages = {11},
keywords = {code generation, model-driven software engineering},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@article{10.1007/s11042-015-3105-4,
author = {Lopatka, K. and Kotus, J. and Czyzewski, A.},
title = {Detection, classification and localization of acoustic events in the presence of background noise for acoustic surveillance of hazardous situations},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {17},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-015-3105-4},
doi = {10.1007/s11042-015-3105-4},
abstract = {Evaluation of sound event detection, classification and localization of hazardous acoustic events in the presence of background noise of different types and changing intensities is presented. The methods for discerning between the events being in focus and the acoustic background are introduced. The classifier, based on a Support Vector Machine algorithm, is described. The set of features and samples used for the training of the classifier are introduced. The sound source localization algorithm based on the analysis of multichannel signals from the Acoustic Vector Sensor is presented. The methods are evaluated in an experiment conducted in the anechoic chamber, in which the representative events are played together with noise of differing intensity. The results of detection, classification and localization accuracy with respect to the Signal to Noise Ratio are discussed. The results show that the recognition and localization accuracy are strongly dependent on the acoustic conditions. We also found that the engineered algorithms provide a sufficient robustness in moderately intense noise in order to be applied to practical audio-visual surveillance systems.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {10407–10439},
numpages = {33},
keywords = {Audio surveillance, Sound detection, Sound source localization}
}

@article{10.1177/0037549712455849,
author = {Dobre, Ciprian},
title = {Simulation analysis of data processing activities in Compact Muon Solenoid physics},
year = {2012},
issue_date = {December  2012},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
volume = {88},
number = {12},
issn = {0037-5497},
url = {https://doi.org/10.1177/0037549712455849},
doi = {10.1177/0037549712455849},
abstract = {The scale, complexity and worldwide geographical spread of the Large Hadron Collider (LHC) computing and data analysis problems are unprecedented in scientific research. The complexity of processing and accessing this data is increased substantially by the size and global span of the major experiments, combined with the limited wide-area network bandwidth available. This paper discusses the latest generation of the MONARC (MOdels of Networked Analysis at Regional Centers) simulation framework, as a design and modeling tool for large-scale distributed systems applied to high-energy physics experiments. We present a simulation study designed to evaluate the capabilities of the current real-world distributed infrastructures deployed to support existing LHC physics analysis processes and the means by which the experiments band together to meet the technical challenges posed by the storage, access and computing requirements of LHC data analysis. The Compact Muon Solenoid (CMS) experiment, in particular, uses a general-purpose detector to investigate a wide range of physics. We present a simulation study designed to evaluate the capability of its underlying distributed processing infrastructure to support the physics analysis processes. The results, made possible by the MONARC model, demonstrate that the LHC infrastructures are well suited to support the data processes envisioned by the CMS computing model.},
journal = {Simulation},
month = dec,
pages = {1438–1455},
numpages = {18},
keywords = {Compact Muon Solenoid, large-scale distributed systems, simulation model}
}

@article{10.1016/j.neucom.2015.04.005,
author = {Yu, Jun and Guo, Yukun and Tao, Dapeng and Wan, Jian},
title = {Human pose recovery by supervised spectral embedding},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.04.005},
doi = {10.1016/j.neucom.2015.04.005},
abstract = {In this paper we propose a subspace learning algorithm based on supervised manifold learning techniques to address the problem of inferring 3D human poses from monocular video frames. Low-dimensional representations of visual features are computed via spectral embedding, regularized by the pairwise relationship of poses for simultaneously preserving the locality in the feature space and taking account of similarities in the pose space. To deal with the "out-of-sample" problem, we obtain a global linear projection from the embedding whereby the Euclidean distances between transformed feature vectors can faithfully reflect the corresponding pose distances. To retrieve the most similar candidate from the exemplar database, weighted sum of Euclidean distances of features is employed to achieve better accuracy instead of simply summing up the squared distances of all feature types. The experimental results on HumanEva dataset validate the efficacy of our proposed method.},
journal = {Neurocomput.},
month = oct,
pages = {301–308},
numpages = {8},
keywords = {Human pose estimation, Spectral embedding, k-NN regression}
}

@inproceedings{10.1145/2635868.2635919,
author = {Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves and Dawagne, Bruno and Leucker, Martin},
title = {Counterexample guided abstraction refinement of product-line behavioural models},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635919},
doi = {10.1145/2635868.2635919},
abstract = {The model-checking problem for Software Products Lines (SPLs) is harder than for single systems: variability constitutes a new source of complexity that exacerbates the state-explosion problem. Abstraction techniques have successfully alleviated state explosion in single-system models. However, they need to be adapted to SPLs, to take into account the set of variants that produce a counterexample. In this paper, we apply CEGAR (Counterexample-Guided Abstraction Refinement) and we design new forms of abstraction specifically for SPLs. We carry out experiments to evaluate the efficiency of our new abstractions. The results show that our abstractions, combined with an appropriate refinement strategy, hold the potential to achieve large reductions in verification time, although they sometimes perform worse. We discuss in which cases a given abstraction should be used.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {190–201},
numpages = {12},
keywords = {Abstraction, CEGAR, Model Checking, Software Product Lines},
location = {Hong Kong, China},
series = {FSE 2014}
}

@article{10.5555/2796290.2796292,
author = {Cheng, LiYing and Wen, Decheng and Sun, Zhongrui},
title = {Strategic Design of the Purchase System Toward R&amp;D Supply Chain Based on SNA},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {3},
issn = {1935-5726},
abstract = {In order to make the strategy for research and development R&amp;D purchase system better serve the personalization of material requirements in R&amp;D process, the authors propose to develop a strategy set which will satisfy the internal as well as external constraints simultaneously. Social network analysis is used to analyze the vertical and horizontal relationships among the project, department and enterprise layers. Through a case study, the authors display the regulatory relationship of participants under given organization pattern and supply chain configuration. To disclose the restricted equilibrium mechanism of participants involved, changes under different strategies are compared, which can assist enterprises to enforce the decision making and to improve the R&amp;D purchase system ability. The authors outline some of the managerial implications arising from the research findings at the end of this paper.},
journal = {Int. J. Inf. Syst. Supply Chain Manag.},
month = jul,
pages = {27–43},
numpages = {17}
}

@inproceedings{10.1145/2512349.2512821,
author = {Mujibiya, Adiyan and Cao, Xiang and Tan, Desney S. and Morris, Dan and Patel, Shwetak N. and Rekimoto, Jun},
title = {The sound of touch: on-body touch and gesture sensing based on transdermal ultrasound propagation},
year = {2013},
isbn = {9781450322713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2512349.2512821},
doi = {10.1145/2512349.2512821},
abstract = {Recent work has shown that the body provides an interesting interaction platform. We propose a novel sensing technique based on transdermal low-frequency ultrasound propagation. This technique enables pressure-aware continuous touch sensing as well as arm-grasping hand gestures on the human body. We describe the phenomena we leverage as well as the system that produces ultrasound signals on one part of the body and measures this signal on another. The measured signal varies according to the measurement location, forming distinctive propagation profiles which are useful to infer on-body touch locations and on-body gestures. We also report on a series of experimental studies with 20 participants that characterize the signal, and show robust touch and gesture classification along the forearm.},
booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},
pages = {189–198},
numpages = {10},
keywords = {gestures, on-body sensing, skin, ultrasound propagation},
location = {St. Andrews, Scotland, United Kingdom},
series = {ITS '13}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@inproceedings{10.1109/SERP4IoT.2019.00012,
author = {Asici, Tansu Zafer and Karaduman, Burak and Eslampanah, Raheleh and Challenger, Moharram and Denil, Joachim and Vangheluwe, Hans},
title = {Applying model driven engineering techniques to the development of contiki-based IoT systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SERP4IoT.2019.00012},
doi = {10.1109/SERP4IoT.2019.00012},
abstract = {The huge variety of smart devices and their communication models increases the development complexity of embedded software for the Internet of Things. As a consequence, development of these systems becomes more complex, error-prone, and costly. To tackle this problem, in this study, a model-driven approach is proposed for the development of Contiki-based IoT systems. To this end, the Contiki metamodel available in the literature is extended to include elements of WiFi connectivity modules (such as ESP8266), IoT Log Manager, and information processing components (such as Raspberry Pi). Based on this new metamodel, a domain-specific modeling environment is developed in which visual symbols are used and static semantics (representing system constraints) are defined. Also, the architectural code for the computing components of the IoT system such as Contiki, ESP8266, and RaspberryPi are generated from the developer's instance model. Finally, a Smart Fire Detection system is used to evaluate this study. By modeling the Contiki-based IoT system, we support model-driven development of the system, including WSN motes and sink nodes (with ContikiOS), WiFi modules and information processing components.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {25–32},
numpages = {8},
keywords = {contikios, embedded software, internet of things (IoT), model-driven engineering (MDE), smart fire detection system, wireless sensor network},
location = {Montreal, Quebec, Canada},
series = {SERP4IoT '19}
}

@article{10.1016/j.tcs.2017.09.029,
author = {Dimovski, Aleksandar S.},
title = {Verifying annotated program families using symbolic game semantics},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {706},
number = {C},
issn = {0304-3975},
url = {https://doi.org/10.1016/j.tcs.2017.09.029},
doi = {10.1016/j.tcs.2017.09.029},
abstract = {Many software systems are today built as program families. They permit users to derive a custom program (variant) by selecting suitable configuration options at compile time according to their requirements. Many such program families are safety critical. However, most existing verification techniques are designed to work on the level of single programs. Their application to program families would require to verify each variant in isolation, in a brute force fashion. This approach does not scale in practice due to the (potentially) huge number of possible variants.In this paper, we propose an efficient game semantics based approach for verification of open program families, i.e. program families with undefined components (identifiers). We use symbolic representation of algorithmic game semantics, where symbolic values for inputs are used instead of concrete ones. In this way, we can compactly represent program families with infinite integers as so-called (finite state) featured symbolic automata. Specifically designed model checking algorithms are then employed to uniformly verify safety of all programs (variants) from a family at once using a single compact model and to pinpoint those programs that are unsafe (respectively, safe). We present a prototype tool implementing this approach, and we illustrate its practicality with several examples.},
journal = {Theor. Comput. Sci.},
month = jan,
pages = {35–53},
numpages = {19},
keywords = {Algorithmic game semantics, Model checking algorithms, Program families, Symbolic automata}
}

@inproceedings{10.1007/978-3-642-34059-8_1,
author = {Broy, Manfred and Cengarle, Mar\'{\i}a Victoria and Geisberger, Eva},
title = {Cyber-physical systems: imminent challenges},
year = {2012},
isbn = {9783642340581},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34059-8_1},
doi = {10.1007/978-3-642-34059-8_1},
abstract = {A German project is presented which was initiated in order to analyse the potential and risks associated with Cyber-Physical Systems. These have been recognised as the next wave of innovation in information and communication technology. Cyber-Physical Systems are herein understood in a very broad sense as the integration of embedded systems with global networks such as the Internet. The survey aims at deepening understanding the impact of those systems at technological and economical level as well as at political and sociological level. The goal of the study is to collect arguments for decision makers both in business and politics to take actions in research, legislation and business development.},
booktitle = {Proceedings of the 17th Monterey Conference on Large-Scale Complex IT Systems: Development, Operation and Management},
pages = {1–28},
numpages = {28},
location = {Oxford, UK}
}

@article{10.1145/3104028,
author = {Pahl, Claus and Jamshidi, Pooyan and Zimmermann, Olaf},
title = {Architectural Principles for Cloud Software},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3104028},
doi = {10.1145/3104028},
abstract = {A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service orientation is agreed on in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {17},
numpages = {23},
keywords = {Cloud computing, adaptive system, architectural style, cloud-native, control theory, devops, microservice, model-based controller, software architecture, uncertainty}
}

@article{10.1016/j.rcim.2016.06.002,
title = {Optimal custom design of both symmetric and unsymmetrical hexapod robots for aeronautics applications},
year = {2017},
issue_date = {April 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {44},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.06.002},
doi = {10.1016/j.rcim.2016.06.002},
abstract = {The Stewart parallel mechanism is used in various applications due to its high load-carrying capacity, accuracy and stiffness, such as flight simulation, spaceship aligning, radar and satellite antenna orientation, rehabilitation applications, parallel machine tools. However, the use of such parallel robots is not widespread due to three factors: the limited workspace, the singularity configurations existing inside the workspace, and the high cost. In this work, an approach to support the design of a cost-effective Stewart platform-based mechanism for specific applications and to facilitate the choice of suitable components (e.g., linear actuators and base and mobile plates) is presented. The optimal design proposed in this work has multiple objectives. In detail, it intends to maximize the payload and minimize the forces at each leg needed to counteract external forces applied to the mobile platform during positioning or manufacturing, or, in general, during specific applications. The approach also aims at avoiding reduction of the robot workspace through a kinematic optimization. Both symmetric and unsymmetrical geometries have been analysed to show how the optimal design approach can lead to effective results with different robot configurations. Moreover, these objectives are achieved through a dynamic optimization and several optimization algorithms were compared in terms of defined performance indexes. HighlightsA software tool for optimal design of hexapod robots is proposed.The tool is based on Inverse Dynamics computation implemented in Matlab.A Genetic Algorithm is proposed and compared with other optimization methods.The method is tested with the optimal design of an hexapod robot for aeronautics applications.},
journal = {Robot. Comput.-Integr. Manuf.},
month = apr,
pages = {1–16},
numpages = {16}
}

@article{10.1016/j.datak.2011.01.005,
author = {Li, Chen and Reichert, Manfred and Wombacher, Andreas},
title = {Editorial: Mining business process variants: Challenges, scenarios, algorithms},
year = {2011},
issue_date = {May, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {70},
number = {5},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2011.01.005},
doi = {10.1016/j.datak.2011.01.005},
abstract = {During the last years a new generation of process-aware information systems has emerged, which enables process model configurations at buildtime as well as process instance changes during runtime. Respective model adaptations result in a large number of model variants that are derived from the same process model, but slightly differ in structure. Generally, such model variants are expensive to configure and maintain. In this paper we address two scenarios for learning from process model adaptations and for discovering a reference model out of which the variants can be configured with minimum efforts. The first one is characterized by a reference process model and a collection of related process variants. The goal is to improve the original reference process model such that it fits better to the variant models. The second scenario comprises a collection of process variants, while the original reference model is unknown; i.e., the goal is to ''merge'' these variants into a new reference process model. We suggest two algorithms that are applicable in both scenarios, but have their pros and cons. We provide a systematic comparison of the two algorithms and further contrast them with conventional process mining techniques. Comparison results indicate good performance of our algorithms and also show that specific techniques are needed for learning from process configurations and adaptations. Finally, we provide results from a case study in automotive industry in which we successfully applied our algorithms.},
journal = {Data Knowl. Eng.},
month = may,
pages = {409–434},
numpages = {26},
keywords = {Process change, Process configuration, Process mining, Process variant}
}

@inproceedings{10.1109/FOSE.2007.14,
author = {France, Robert and Rumpe, Bernhard},
title = {Model-driven Development of Complex Software: A Research Roadmap},
year = {2007},
isbn = {0769528295},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOSE.2007.14},
doi = {10.1109/FOSE.2007.14},
abstract = {The term Model-Driven Engineering (MDE) is typically used to describe software development approaches in which abstract models of software systems are created and systematically transformed to concrete implementations. In this paper we give an overview of current research in MDE and discuss some of the major challenges that must be tackled in order to realize the MDE vision of software development. We argue that full realizations of the MDE vision may not be possible in the near to medium-term primarily because of the wicked problems involved. On the other hand, attempting to realize the vision will provide insights that can be used to significantly reduce the gap between evolving software complexity and the technologies used to manage complexity.},
booktitle = {2007 Future of Software Engineering},
pages = {37–54},
numpages = {18},
series = {FOSE '07}
}

@article{10.1007/s10009-014-0301-x,
author = {Wong, Peter Y. and Bubel, Richard and Boer, Frank S. and G\'{o}mez-Zamalloa, Miguel and Gouw, Stijn and H\"{a}hnle, Reiner and Meinke, Karl and Sindhu, Muddassar Azam},
title = {Testing abstract behavioral specifications},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-014-0301-x},
doi = {10.1007/s10009-014-0301-x},
abstract = {We present a range of testing techniques for the Abstract Behavioral Specification (ABS) language and apply them to an industrial case study. ABS is a formal modeling language for highly variable, concurrent, component-based systems. The nature of these systems makes them susceptible to the introduction of subtle bugs that are hard to detect in the presence of steady adaptation. While static analysis techniques are available for an abstract language such as ABS, testing is still indispensable and complements analytic methods. We focus on fully automated testing techniques including black-box and glass-box test generation as well as runtime assertion checking, which are shown to be effective in an industrial setting.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {107–119},
numpages = {13},
keywords = {Automated testing, Black-box testing, Glass-box testing, Industrial case study, Runtime assertion checking}
}

@article{10.1016/j.eswa.2010.09.146,
author = {Cai, Zhiqiang and Sun, Shudong and Si, Shubin and Yannou, Bernard},
title = {Identifying product failure rate based on a conditional Bayesian network classifier},
year = {2011},
issue_date = {May, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.09.146},
doi = {10.1016/j.eswa.2010.09.146},
abstract = {Research highlights CBN introduces the conditional independence relationships among attribute variables. CBN provides an effective approach to classify the failure rate rank of products. CBN increases the classification accuracy. CBN makes an acceptable balance between classifier complexity and performance. To identify the product failure rate grade under diverse configuration and operation conditions, a new conditional Bayesian networks (CBN) model is brought forward. By indicating the conditional independence relationship between attribute variables given the target variable, this model could provide an effective approach to classify the grade of failure rate. Furthermore, on the basis of the CBN model, the procedure of building product failure rate grade classifier is elaborated with modeling and application. At last, a case study is carried out and the results show that, with comparison to other Bayesian networks classifiers and traditional decision tree C4.5, the CBN model not only increases the total classification accuracy, but also reduces the complexity of network structure.},
journal = {Expert Syst. Appl.},
month = may,
pages = {5036–5043},
numpages = {8},
keywords = {Bayesian network, Classifier, Conditional independence, Failure rate, Maintenance management}
}

@inproceedings{10.1007/11767718_30,
author = {Regnell, Bj\"{o}rn and Olsson, Hans O. and Mossberg, Staffan},
title = {Assessing requirements compliance scenarios in system platform subcontracting},
year = {2006},
isbn = {3540346821},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11767718_30},
doi = {10.1007/11767718_30},
abstract = {In the mobile industry, system platforms are offered to device developers to enable rapid product development while sharing expensive technology development investments. This paper presents a framework for assessment of requirements engineering collaboration related to statements-of-compliance negotiation in platform subcontracting. The framework includes a classification of platform compliance scenarios and results from analysis of interviews with engineers at two collaborating companies, a device vendor and a platform vendor. Case study findings particular to the compliance scenarios of the framework are provided. The purpose of the framework is to provide a basis for process improvement in collaborative requirements engineering.},
booktitle = {Proceedings of the 7th International Conference on Product-Focused Software Process Improvement},
pages = {362–376},
numpages = {15},
location = {Amsterdam, The Netherlands},
series = {PROFES'06}
}

@article{10.1016/j.infsof.2019.03.004,
author = {Duran, Mustafa Berk and Mussbacher, Gunter},
title = {Reusability in goal modeling: A systematic literature review},
year = {2019},
issue_date = {Jun 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {110},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.004},
doi = {10.1016/j.infsof.2019.03.004},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {156–173},
numpages = {18},
keywords = {Goal model, Reuse, Context, Requirements reuse, Model-driven requirements engineering, Systematic literature review}
}

@article{10.1016/S0166-3615(05)00151-X,
title = {Subject Index},
year = {2005},
issue_date = {December, 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {56},
number = {8–9},
issn = {0166-3615},
url = {https://doi.org/10.1016/S0166-3615(05)00151-X},
doi = {10.1016/S0166-3615(05)00151-X},
journal = {Comput. Ind.},
month = dec,
pages = {1022–1024},
numpages = {3}
}

@proceedings{10.1145/2935323,
title = {ARRAY 2016: Proceedings of the 3rd ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
year = {2016},
isbn = {9781450343848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Santa Barbara, CA, USA}
}

@article{10.1016/j.aei.2007.10.001,
author = {Saridakis, K. M. and Dentsoras, A. J.},
title = {Soft computing in engineering design - A review},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {22},
number = {2},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2007.10.001},
doi = {10.1016/j.aei.2007.10.001},
abstract = {The present paper surveys the application of soft computing (SC) techniques in engineering design. Within this context, fuzzy logic (FL), genetic algorithms (GA) and artificial neural networks (ANN), as well as their fusion are reviewed in order to examine the capability of soft computing methods and techniques to effectively address various hard-to-solve design tasks and issues. Both these tasks and issues are studied in the first part of the paper accompanied by references to some results extracted from a survey performed for in some industrial enterprises. The second part of the paper makes an extensive review of the literature regarding the application of soft computing (SC) techniques in engineering design. Although this review cannot be collectively exhaustive, it may be considered as a valuable guide for researchers who are interested in the domain of engineering design and wish to explore the opportunities offered by fuzzy logic, artificial neural networks and genetic algorithms for further improvement of both the design outcome and the design process itself. An arithmetic method is used in order to evaluate the review results, to locate the research areas where SC has already given considerable results and to reveal new research opportunities.},
journal = {Adv. Eng. Inform.},
month = apr,
pages = {202–221},
numpages = {20},
keywords = {Engineering design, Fuzzy logic, Genetic algorithm, Neural networks, Soft computing}
}

@inproceedings{10.5555/3038718.3038778,
author = {Sohrabi, Shirin and Udrea, Octavian and Ranganathan, Anand and Riabov, Anton V.},
title = {HTN planning for the composition of stream processing applications},
year = {2013},
publisher = {AAAI Press},
abstract = {Goal-driven automated composition of software components is an important problem with applications in Web service composition and stream processing systems. The popular approach to address this problem is to build the composition automatically using AI planning. However, it is shown that some of these planning approaches may neither be feasible nor scalable for many large-scale flow-based applications. Recent advances have proven that the automated composition problem can take advantage of expert knowledge describing the many ways in which different reusable components can be composed. This knowledge can be represented using an extensible composition template or pattern. In prior work, a flow pattern language called Cascade and its corresponding specialized planner have shown the best performance in these domains. In this paper, we propose the use of Hierarchical Task Network (HTN) planning for the composition of stream processing applications. To this end, we propose an automated approach of creating an HTN-based problem from the Cascade representation of the flow patterns. The resulting technique not only allows us to use the HTN planning paradigm and its many advantages including added expressivity but also enables optimization and customization of composition with respect to preferences and constraints. Further, we propose and develop a lookahead heuristic and show that it significantly reduces the planning time. We have performed extensive experimentation with stream processing applications and evaluated applicability and performance of our approach.},
booktitle = {Proceedings of the Twenty-Third International Conference on International Conference on Automated Planning and Scheduling},
pages = {443–451},
numpages = {9},
location = {Rome, Italy},
series = {ICAPS'13}
}

@article{10.1155/2021/1057371,
author = {Yang, Yinghui and cheikhrouhou, omar},
title = {The Potential Energy of Artificial Intelligence Technology in University Education Reform from the Perspective of Communication Science},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/1057371},
doi = {10.1155/2021/1057371},
abstract = {In today’s rapid development of science and technology, science is everywhere in people’s lives, and science communication is everywhere. Science and communication are not only not far away but also very close. Since machine learning algorithms with deep learning as a theme have achieved great success in the fields of vision and speech recognition, as well as the large amount of data resources that cloud computing, big data, and other technologies can provide, the development speed of artificial intelligence has been greatly improved, and it has had a significant impact in various industries in the society, and the country has put forward the concept of intelligent education for this purpose. However, there have been few systematic discussions on the combination of artificial intelligence with education and teaching. Therefore, this article uses artificial intelligence technology to study the potential energy space of artificial intelligence technology in college education reform from the perspective of science communication, designs and implements an online education platform for colleges and universities, and conducts a trial of platform use in a domestic college and universities. Some teachers and students conduct a satisfaction survey after the platform is used, and the conclusions show that whether in the teacher group or the student group, most teachers and students are relatively satisfied with the online education platform designed in this article. The reform of college education includes many aspects. This article is a research study on the form of college education, changing from traditional offline education to online platform education. This research can provide a certain reference for the reform of college education.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {7}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@article{10.1007/s10664-010-9135-7,
author = {Garvin, Brady J. and Cohen, Myra B. and Dwyer, Matthew B.},
title = {Evaluating improvements to a meta-heuristic search for constrained interaction testing},
year = {2011},
issue_date = {February  2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-010-9135-7},
doi = {10.1007/s10664-010-9135-7},
abstract = {Combinatorial interaction testing (CIT) is a cost-effective sampling technique for discovering interaction faults in highly-configurable systems. Constrained CIT extends the technique to situations where some features cannot coexist in a configuration, and is therefore more applicable to real-world software. Recent work on greedy algorithms to build CIT samples now efficiently supports these feature constraints. But when testing a single system configuration is expensive, greedy techniques perform worse than meta-heuristic algorithms, because greedy algorithms generally need larger samples to exercise the same set of interactions. On the other hand, current meta-heuristic algorithms have long run times when feature constraints are present. Neither class of algorithm is suitable when both constraints and the cost of testing configurations are important factors. Therefore, we reformulate one meta-heuristic search algorithm for constructing CIT samples, simulated annealing, to more efficiently incorporate constraints. We identify a set of algorithmic changes and experiment with our modifications on 35 realistic constrained problems and on a set of unconstrained problems from the literature to isolate the factors that improve performance. Our evaluation determines that the optimizations reduce run time by a factor of 90 and accomplish the same coverage objectives with even fewer system configurations. Furthermore, the new version compares favorably with greedy algorithms on real-world problems, and, though our modifications were aimed at constrained problems, it shows similar advantages when feature constraints are absent.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {61–102},
numpages = {42},
keywords = {Configurable software, Constrained combinatorial interaction testing, Search based software engineering}
}

@article{10.1016/j.jmva.2014.10.003,
author = {Luo, Ruiyan and Qi, Xin},
title = {Sparse wavelet regression with multiple predictive curves},
year = {2015},
issue_date = {February 2015},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0047-259X},
url = {https://doi.org/10.1016/j.jmva.2014.10.003},
doi = {10.1016/j.jmva.2014.10.003},
abstract = {With the advance of techniques, more and more complicated data are extracted and recorded. In this paper, functional regression models with a scalar response and multiple predictive curves are considered. We transform the functional regression models to multiple linear regression models by using the discrete wavelet transformation. When the number of predictive curves is big, the multiple linear regression model usually has much bigger number of features than the sample size. We apply our correlation-based sparse regression method to the resulted high dimensional regression model. The novel feature of our sparse method is that we impose sparsity penalty on the direction of the estimate of the coefficient vector instead of the estimate itself, and only the direction of the estimate is determined by an optimization problem. The estimation consistency of the coefficient curve for the functional regression model is obtained when both the sample size and the number of curves go to infinity. The effects of the discrete observations are discussed. We compare our method with both functional regression methods and other wavelet based sparse regression methods on both simulated data and four real data sets, including the cases of single and multiple predictive curves. The results indicate that sparse wavelet regression methods are better in extracting local features and our method has good predictive performances in all scenarios.},
journal = {J. Multivar. Anal.},
month = feb,
pages = {33–49},
numpages = {17},
keywords = {Functional linear model, Sparse regression, Wavelet transformation, primary62J05, secondary62G0562G20}
}

@article{10.1016/j.engappai.2008.10.016,
author = {Barton, Richard and Thomas, Andrew},
title = {Implementation of intelligent systems, enabling integration of SMEs to high-value supply chain networks},
year = {2009},
issue_date = {September, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {6},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2008.10.016},
doi = {10.1016/j.engappai.2008.10.016},
abstract = {In the face of increasing competition from low-cost economies, European manufacturing companies are focusing on optimisation of operational activities to remain competitive. Previous work has identified how companies can optimise implementation of specialist technology to improve production capability; however increasing demands in service requirements such as customisability and flexibility are often negating the localised gains in capability. Supply chain management has become an increasingly important aspect of operations improvement to ensure support throughout the product realisation process. The key to creating a supply chain capable of this rapid response and high level of adaptability is integration of intelligent systems and management capabilities. A site-visit-based survey and characterisation of small and medium enterprises (SMEs), comprising actual or potential supply chain components, reveals that even those with well developed capabilities and attitudes to adopting production technologies are largely not proactive with technology adoption targeting these needs. A review of requirements for SMEs to achieve such competitive supply chain capabilities reveals a hierarchy of technical expertise to be developed. This is presented as an implementation strategy for staged introduction of these tools and techniques with a view to establishing high-value supply chains capable of withstanding business pressures from developing economies.},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {929–938},
numpages = {10},
keywords = {Agility, Integration, Intelligent systems, SME, Supply chain management, Survey}
}

@article{10.1016/j.dss.2012.06.002,
author = {Luo, X. G. and Kwong, C. K. and Tang, J. F. and Tu, Y. L.},
title = {Optimal product positioning with consideration of negative utility effect on consumer choice rule},
year = {2012},
issue_date = {December, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {1},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.06.002},
doi = {10.1016/j.dss.2012.06.002},
abstract = {In most studies related to product positioning, probabilistic consumer choice rules assume that a product always gains some market share no matter how small a product's utility value is or even if the utility value is negative. Some researchers have considered this problem in multidimensional-scaling-based model or share-of-surplus choice rule. In this study, we consider this problem for multinomial logit rule by introducing a piecewise function and establishing a conjoint-analysis-based one-step optimization model for product positioning. Interval analysis is applied to obtain the optimal price of the new product from the model, and the mathematical properties of the profit-maximizing model are analyzed. An interval-analysis-embedded Tabu Search (TS) algorithm is developed for solving the model. An industrial application employing the proposed model and the interval-analysis-based enumeration method is presented and sensitivity analysis is performed. An experiment for randomly created large-scale product positioning problems is carried out to evaluate the feasibility of the proposed TS algorithm.},
journal = {Decis. Support Syst.},
month = dec,
pages = {402–413},
numpages = {12},
keywords = {Consumer choice rule, Interval analysis, Product positioning, Tabu search}
}

@article{10.1145/1118890.1118892,
author = {Mernik, Marjan and Heering, Jan and Sloane, Anthony M.},
title = {When and how to develop domain-specific languages},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/1118890.1118892},
doi = {10.1145/1118890.1118892},
abstract = {Domain-specific languages (DSLs) are languages tailored to a specific application domain. They offer substantial gains in expressiveness and ease of use compared with general-purpose programming languages in their domain of application. DSL development is hard, requiring both domain knowledge and language development expertise. Few people have both. Not surprisingly, the decision to develop a DSL is often postponed indefinitely, if considered at all, and most DSLs never get beyond the application library stage.Although many articles have been written on the development of particular DSLs, there is very limited literature on DSL development methodologies and many questions remain regarding when and how to develop a DSL. To aid the DSL developer, we identify patterns in the decision, analysis, design, and implementation phases of DSL development. Our patterns improve and extend earlier work on DSL design patterns. We also discuss domain analysis tools and language development systems that may help to speed up DSL development. Finally, we present a number of open problems.},
journal = {ACM Comput. Surv.},
month = dec,
pages = {316–344},
numpages = {29},
keywords = {Domain-specific language, application language, domain analysis, language development system}
}

@article{10.1016/j.eswa.2010.03.061,
author = {Kulak, Osman and Cebi, Selcuk and Kahraman, Cengiz},
title = {Review: Applications of axiomatic design principles: A literature review},
year = {2010},
issue_date = {September, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {9},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.03.061},
doi = {10.1016/j.eswa.2010.03.061},
abstract = {Although there have been quite a number of theoretical and practical studies where axiomatic design (AD) principles have been used in the last few years, there is a lack of a comprehensive literature survey which evaluates and classifies these papers. This study provides a recognizable overview of literature on AD principles from the past 20 years and introduces a novel classification scheme covering 63 papers. Each article was classified into four main groups, namely the type of the axiom, the application area, the method, and the evaluation type. Findings of our paper indicate that most of the studies in the literature are application-based papers which use typically the independence axiom. While product design is put forward in the application area, the crisp approach is widely used as an evaluation type. A rise in the fuzzy evaluation based research studies using the information axiom for multi-attribute decision making problems has also been noticed.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {6705–6717},
numpages = {13},
keywords = {Axiomatic design, Independence axiom, Information axiom}
}

@article{10.1017/S0269888910000299,
title = {From the journals???},
year = {2011},
issue_date = {February 2011},
publisher = {Cambridge University Press},
address = {USA},
volume = {26},
number = {1},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888910000299},
doi = {10.1017/S0269888910000299},
journal = {Knowl. Eng. Rev.},
month = feb,
pages = {73–97},
numpages = {25}
}

@article{10.1016/j.jss.2016.03.067,
author = {Bauer, Veronika and Vetro', Antonio},
title = {Comparing reuse practices in two large software-producing companies},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.067},
doi = {10.1016/j.jss.2016.03.067},
abstract = {We compare two empirical investigations on software reuse at two large companies.We analyzed 108 survey responses and 35 h of interviews with 30 participants.Homogeneous, coherent settings produce clear benefits in development and maintenance.We identify coherence between culture and approach as important reuse success factor.Systematic reuse in heterogeneous contexts requires structured decision support. ContextReuse can improve productivity and maintainability in software development. Research has proposed a wide range of methods and techniques. Are these successfully adopted in practice__ __ ObjectiveWe propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies. MethodWe compare and interpret the study results with a focus on reuse practices, effects, and context. ResultsBoth companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. Reusable entities are retrieved from a central repository, if present. Otherwise, direct communication with trusted colleagues is crucial for access.Reuse processes remain implicit and reflect the development style. In a homogeneous infrastructure-supported context, participants strongly agreed on higher development pace and less maintenance effort as reuse benefits. In a heterogeneous context with fragmented infrastructure, these benefits did not materialize.Neither case reports statistically significant evidence of negative side effects of reuse nor inhibitors. In both cases, a lack of reuse led to duplicate implementations. ConclusionTechnological advances have improved the way reuse concepts can be applied in practice. Homogeneity in development process and tool support seem necessary preconditions. Developing and adopting adequate reuse strategies in heterogeneous contexts remains challenging.},
journal = {J. Syst. Softw.},
month = jul,
pages = {545–582},
numpages = {38},
keywords = {Empirical, Software engineering, Software reuse, Survey research, Technology transfer}
}

@article{10.1145/1022494.1022576,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Backmatter (Report abstracts, Paper abstracts, Calendar of Future Events, Call for Participation, Keynotes, Workshops, Tutorials)},
year = {2004},
issue_date = {September 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1022494.1022576},
doi = {10.1145/1022494.1022576},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {42},
numpages = {16}
}

@article{10.1016/j.compind.2009.09.007,
author = {Gao, Fei and Xiao, Gang and Simpson, Timothy W.},
title = {Identifying functional modules using generalized directed graphs: Definition and application},
year = {2010},
issue_date = {April, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {3},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2009.09.007},
doi = {10.1016/j.compind.2009.09.007},
abstract = {We extend traditional directed graphs to generalized directed graphs, making them capable of representing function structures as graphs. In a generalized directed graph, vectors are used to denote the edges, which are pairs of sub-function vertices connected by a relationship, and elements of the vectors indicate different types of flow (i.e., material, energy, or signal) on which sub-functions operate. Based on these definitions, we formalize the three heuristics proposed by Stone et al. into rules to identify functional modules in function structures: (1) sequential flow rule, (2) parallel flow rule and (3) flow transformation rule. The arithmetic for identifying functional modules based on these formalized rules is developed, and a computer-aided software tool is created to facilitate this process. Finally, the proposed approach is applied to a function structure for a power screwdriver, and the results compare favorably to those obtained using the three heuristics.},
journal = {Comput. Ind.},
month = apr,
pages = {260–269},
numpages = {10},
keywords = {Conceptual design, Design methodology, Generalized directed graph, Product development, Product modeling}
}

@article{10.1007/s10270-010-0176-6,
author = {Prout, Adam and Atlee, Joanne M. and Day, Nancy A. and Shaker, Pourya},
title = {Code generation for a family of executable modelling notations},
year = {2012},
issue_date = {May       2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-010-0176-6},
doi = {10.1007/s10270-010-0176-6},
abstract = {We are investigating semantically configurable model-driven engineering (MDE). The goal of this work is a modelling environment that supports flexible, configurable modelling notations, in which specifiers can configure the semantics of notations to suit their needs and yet still have access to the types of analysis tools and code generators normally associated with MDE. In this paper, we describe semantically configurable code generation for a family of behavioural modelling notations. The family includes variants of statecharts, process algebras, Petri Nets, and SDL88. The semantics of this family is defined using template semantics, which is a parameterized structured operational semantics in which parameters represent semantic variation points. A specific notation is derived by instantiating the family's template semantics with parameter values that specify semantic choices. We have developed a code-generator generator (CGG) that creates a suitable Java code generator for a subset of derivable modelling notations. Our prototype CGG supports 26 semantics parameters, 89 parameter values, and 7 composition operators. As a result, we are able to produce code generators for a sizable family of modelling notations, though at present the performance of our generated code is about an order of magnitude slower than that produced by commercial-grade generators.},
journal = {Softw. Syst. Model.},
month = may,
pages = {251–272},
numpages = {22},
keywords = {Code generation, Model-driven engineering}
}

@article{10.14778/2994509.2994521,
author = {Borovica-Gaji\'{c}, Renata and Appuswamy, Raja and Ailamaki, Anastasia},
title = {Cheap data analytics using cold storage devices},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994521},
doi = {10.14778/2994509.2994521},
abstract = {Enterprise databases use storage tiering to lower capital and operational expenses. In such a setting, data waterfalls from an SSD-based high-performance tier when it is "hot" (frequently accessed) to a disk-based capacity tier and finally to a tape-based archival tier when "cold" (rarely accessed). To address the unprecedented growth in the amount of cold data, hardware vendors introduced new devices named Cold Storage Devices (CSD) explicitly targeted at cold data workloads. With access latencies in tens of seconds and cost/GB as low as $0.01/GB/month, CSD provide a middle ground between the low-latency (ms), high-cost, HDD-based capacity tier, and high-latency (min to h), low-cost, tape-based, archival tier.Driven by the price/performance aspect of CSD, this paper makes a case for using CSD as a replacement for both capacity and archival tiers of enterprise databases. Although CSD offer major cost savings, we show that current database systems can suffer from severe performance drop when CSD are used as a replacement for HDD due to the mismatch between design assumptions made by the query execution engine and actual storage characteristics of the CSD. We then build a CSD-driven query execution framework, called Skipper, that modifies both the database execution engine and CSD scheduling algorithms to be aware of each other. Using results from our implementation of the architecture based on PostgreSQL and OpenStack Swift, we show that Skipper is capable of completely masking the high latency overhead of CSD, thereby opening up CSD for wider adoption as a storage tier for cheap data analytics over cold data.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1029–1040},
numpages = {12}
}

@inproceedings{10.5555/2830865.2830876,
author = {Skelin, Mladen and Geilen, Marc and Catthoor, Francky and Hendseth, Sverre},
title = {Parametrized dataflow scenarios},
year = {2015},
isbn = {9781467380799},
publisher = {IEEE Press},
abstract = {The FSM-based scenario-aware dataflow (FSM-SADF) model of computation has been introduced to facilitate the analysis of dynamic streaming applications. FSM-SADF interprets application's execution as an execution of a sequence of static modes of operation called scenarios. Each scenario is modeled using a synchronous dataflow (SDF) graph (SDFG), while a finite-state machine (FSM) is used to encode scenario occurrence patterns. However, FSM-SADF can precisely capture only those dynamic applications whose behaviors can be abstracted into a reasonably sized set of scenarios (coarse-grained dynamism). Nevertheless, in many cases, the application may exhibit thousands or even millions of behaviours (fine-grained dynamism). In this work, we generalize the concept of FSM-SADF to one that is able to model dynamic applications exhibiting fine-grained dynamism. We achieve this by applying parametrization to the FSM-SADF's base model, i.e. SDF, and defining scenarios over parametrized SDFGs. We refer to the extension as parametrized FSM-SADF (PFSM-SADF). Thereafter, we present a novel and a fully parametric analysis technique that allows us to derive tight worst-case performance (throughput and latency) guarantees for PFSM-SADF specifications. We evaluate our approach on a realistic case-study from the multimedia domain.},
booktitle = {Proceedings of the 12th International Conference on Embedded Software},
pages = {95–104},
numpages = {10},
keywords = {max-plus algebra, parametrized dataflow, scenario-aware dataflow, synchronous dataflow, worst-case performance},
location = {Amsterdam, The Netherlands},
series = {EMSOFT '15}
}

@article{10.1016/j.cie.2018.11.018,
author = {Ijadi Maghsoodi, Abteen and Mosavat, Mojan and Hafezalkotob, Ashkan and Hafezalkotob, Arian},
title = {Hybrid hierarchical fuzzy group decision-making based on information axioms and BWM: Prototype design selection},
year = {2019},
issue_date = {Jan 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2018.11.018},
doi = {10.1016/j.cie.2018.11.018},
journal = {Comput. Ind. Eng.},
month = jan,
pages = {788–804},
numpages = {17},
keywords = {Best-Worst Method (BWM), Fuzzy sets, Multi-Criteria Decision-Making (MCDM), Fuzzy Axiomatic Design (FAD), Group decision-making, Loudspeaker prototype selection}
}

@article{10.1016/j.jss.2013.01.058,
author = {Garcia, Alessandro},
title = {Editorial: Software Engineering in Brazil: Retrospective and prospective views},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.01.058},
doi = {10.1016/j.jss.2013.01.058},
journal = {J. Syst. Softw.},
month = apr,
pages = {869–871},
numpages = {3}
}

@article{10.1007/s00450-012-0232-2,
author = {Souza, V\'{\i}tor E. and Lapouchnian, Alexei and Angelopoulos, Konstantinos and Mylopoulos, John},
title = {Requirements-driven software evolution},
year = {2013},
issue_date = {November  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-012-0232-2},
doi = {10.1007/s00450-012-0232-2},
abstract = {It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement  R  is violated more than  N  times in a week, it should be relaxed to a less demanding one  R  . Such  evolution requirements  play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature.},
journal = {Comput. Sci.},
month = nov,
pages = {311–329},
numpages = {19},
keywords = {Adaptive systems, Evolution, Modeling, Requirements, Requirements engineering}
}

@article{10.1016/j.jss.2014.08.024,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Cost-effective test suite minimization in product lines using search techniques},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.024},
doi = {10.1016/j.jss.2014.08.024},
abstract = {We define five cost-effectiveness measures for product line test minimization.We define a sound fitness function based on the cost-effectiveness measures.Ten search algorithms are empirically evaluated using an industrial case study.The scalability of the search algorithms is assessed using 500 artificial problems.Random-Weighted GA performs the best and can solve a wide range of problems. Cost-effective testing of a product in a product line requires obtaining a set of relevant test cases from the entire test suite via test selection and minimization techniques. In this paper, we particularly focus on test minimization for product lines, which identifies and eliminates redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. However, such minimization may result in the minimized test suite with low test coverage, low fault revealing capability, low priority test cases, and require more time than the allowed testing budget (e.g., time) as compared to the original test suite. To deal with the above issues, we formulated the minimization problem as a search problem and defined a fitness function considering various optimization objectives based on the above issues. To assess the performance of our fitness function, we conducted an extensive empirical evaluation by investigating the fitness function with three weight-based Genetic Algorithms (GAs) and seven multi-objective search algorithms using an industrial case study and 500 artificial problems inspired from the industrial case study. The results show that Random-Weighted Genetic Algorithm (RWGA) significantly outperforms the other algorithms since RWGA can balance all the objectives together by dynamically updating weights during each generation. Based on the results of our empirical evaluation, we also implemented a tool called TEst Minimization using Search Algorithms (TEMSA) to support test minimization using various search algorithms in the context of product lines.},
journal = {J. Syst. Softw.},
month = may,
pages = {370–391},
numpages = {22},
keywords = {Product line, Search algorithm, Test suite minimization}
}

@inproceedings{10.1145/336512.336556,
author = {Lutz, Robyn R.},
title = {Software engineering for safety: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336556},
doi = {10.1145/336512.336556},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {213–226},
numpages = {14},
keywords = {future directions, software engineering, software safety},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@book{10.5555/2876104,
author = {Hamid, Arabnia R. and Tran, Quoc Nam},
title = {Emerging Trends in Computational Biology, Bioinformatics, and Systems Biology: Algorithms and Software Tools},
year = {2015},
isbn = {0128025085},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Emerging Trends in Computational Biology, Bioinformatics, and Systems Biology discusses the latest developments in all aspects of computational biology, bioinformatics, and systems biology and the application of data-analytics and algorithms, mathematical modeling, and simu- lation techniques. Discusses the development and application of data-analytical and theoretical methods, mathematical modeling, and computational simulation techniques to the study of biological and behavioral systems, including applications in cancer research, computational intelligence and drug design, high-performance computing, and biology, as well as cloud and grid computing for the storage and access of big data sets. Presents a systematic approach for storing, retrieving, organizing, and analyzing biological data using software tools with applications to general principles of DNA/RNA structure, bioinformatics and applications, genomes, protein structure, and modeling and classification, as well as microarray analysis. Provides a systems biology perspective, including general guidelines and techniques for obtaining, integrating, and analyzing complex data sets from multiple experimental sources using computational tools and software. Topics covered include phenomics, genomics, epigenomics/epigenetics, metabolomics, cell cycle and checkpoint control, and systems biology and vaccination research. Explains how to effectively harness the power of Big Data tools when data sets are so large and complex that it is difficult to process them using conventional database management systems or traditional data processing applications.Discusses the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological and behavioral systems.Presents a systematic approach for storing, retrieving, organizing and analyzing biological data using software tools with applications. Provides a systems biology perspective including general guidelines and techniques for obtaining, integrating and analyzing complex data sets from multiple experimental sources using computational tools and software.}
}

@book{10.1145/3477355,
editor = {Jones, Cliff B. and Misra, Jayadev},
title = {Theories of Programming: The Life and Works of Tony Hoare},
year = {2021},
isbn = {9781450387286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {39},
abstract = {Sir Tony Hoare has had an enormous influence on computer science, from the Quicksort algorithm to the science of software development, concurrency and program verification. His contributions have been widely recognised: He was awarded the ACM’s Turing Award in 1980, the Kyoto Prize from the Inamori Foundation in 2000, and was knighted for “services to education and computer science” by Queen Elizabeth II of England in 2000.This book presents the essence of his various works—the quest for effective abstractions—both in his own words as well as chapters written by leading experts in the field, including many of his research collaborators. In addition, this volume contains biographical material, his Turing award lecture, the transcript of an interview and some of his seminal papers.Hoare’s foundational paper “An Axiomatic Basis for Computer Programming”, presented his approach, commonly known as Hoare Logic, for proving the correctness of programs by using logical assertions. Hoare Logic and subsequent developments have formed the basis of a wide variety of software verification efforts. Hoare was instrumental in proposing the Verified Software Initiative, a cooperative international project directed at the scientific challenges of large-scale software verification, encompassing theories, tools and experiments.Tony Hoare’s contributions to the theory and practice of concurrent software systems are equally impressive. The process algebra called Communicating Sequential Processes (CSP) has been one of the fundamental paradigms, both as a mathematical theory to reason about concurrent computation as well as the basis for the programming language occam. CSP served as a framework for exploring several ideas in denotational semantics such as powerdomains, as well as notions of abstraction and refinement. It is the basis for a series of industrial-strength tools which have been employed in a wide range of applications.This book also presents Hoare’s work in the last few decades. These works include a rigorous approach to specifications in software engineering practice, including procedural and data abstractions, data refinement, and a modular theory of designs. More recently, he has worked with collaborators to develop Unifying Theories of Programming (UTP). Their goal is to identify the common algebraic theories that lie at the core of sequential, concurrent, reactive and cyber-physical computations. Theories of Programming: The Life and Works of Tony Hoare’ is available as a printed book (DOI: ) and an on-line version. In addition to the book itself, a number of on-line resources might be of interest to readers:
A bibliography of Tony Hoare’s papers with clickable DOIs/URLs where available (ACM: INSERT URL)Appendix E of the book provides links to talks and interviews featuring Tony Hoare ()The Oxford archive of Hoare’s manuscripts:  
Supplementary Material: Tony Hoare’ is a PDF of additional material (not included in the book) containing the following:
Stories from a Life in Interesting Times (A transcription by Jayadev Misra of Tony Hoare’s acceptance speech for the 2000 Kyoto prize)Tony Hoare’s Heidelberg comments: (A transcription by Margaret Gray of Tony Hoare’s part in the 2020 Heidelberg event)Milestones in Tony’s Life and Work: A ‘cv’ of Tony Hoare prepared by Margaret GrayExtended version - ’Bernard Sufrin: Teaching at Belfast and Oxford’}
}

@inproceedings{10.1145/1273463.1273482,
author = {Cohen, Myra B. and Dwyer, Matthew B. and Shi, Jiangfan},
title = {Interaction testing of highly-configurable systems in the presence of constraints},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273482},
doi = {10.1145/1273463.1273482},
abstract = {Combinatorial interaction testing (CIT) is a method to sample configurations of a software system systematically for testing. Many algorithms have been developed that create CIT samples, however few have considered the practical concerns that arise when adding constraints between combinations of options. In this paper, we survey constraint handling techniques in existing algorithms and discuss the challenges that they present. We examine two highly-configurable software systems to quantify the nature of constraints in real systems. We then present a general constraint representation and solving technique that can be integrated with existing CIT algorithms and compare two constraint-enhanced algorithm implementations with existing CIT tools to demonstrate feasibility.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {129–139},
numpages = {11},
keywords = {SAT, combinatorial interaction testing, constraints, covering arrays},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@inproceedings{10.1145/2896839.2896845,
author = {Karim, Muhammad Rezaul and Al Alam, S. M. Didar and Kabeer, Shaikh Jeeshan and Ruhe, G\"{u}nther and Baluta, Basil and Mahmud, Shafquat},
title = {Applying data analytics towards optimized issue management: an industrial case study},
year = {2016},
isbn = {9781450341547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896839.2896845},
doi = {10.1145/2896839.2896845},
abstract = {This document describes our experience of applying data analytics at Plexina, a leading IT company working in the healthcare domain. The main goal of the project was to identify factors currently affecting issue management and to make analytics based suggestions for optimizing the process. Various statistical and machine learning techniques were applied on a data set extracted from six releases of Plexina, containing more than 666 issues. Statistical techniques successfully identified the various factors that leads to estimation inaccuracy related to issues as well as identified the hidden relationships existing among various variables. The employed predictive analytic models was also successful to some extent, in predicting effort estimation related inaccuracy associated with the issues. The insights provided by the entire data analytics study can be of great help to product managers or the developers to make more informed decisions. In addition, the guidelines presented in this paper based on the lessons learnt can be applied to other data analytics and academia-industry collaboration project.},
booktitle = {Proceedings of the 4th International Workshop on Conducting Empirical Studies in Industry},
pages = {7–13},
numpages = {7},
keywords = {case study, data analytics, industry-academia collaboration, issue management},
location = {Austin, Texas},
series = {CESI '16}
}

@article{10.1016/j.scico.2012.01.005,
author = {Autili, Marco and Di Benedetto, Paolo and Inverardi, Paola},
title = {A hybrid approach for resource-based comparison of adaptable Java applications},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.01.005},
doi = {10.1016/j.scico.2012.01.005},
abstract = {During the last decade, context-awareness and adaptation have been receiving significant attention in many research areas. For application developers, the heterogeneity of resource-constrained mobile terminals creates serious problems for the development of mobile applications able to run properly on a large number of different devices. Thus, resource awareness plays a crucial role when developing such applications. It identifies the capability of being aware of the resources offered by an execution environment, in order to decide whether that environment is suited to receive and execute the application. Within this line of research, we propose Chameleon, a framework that provides both an integrated development environment and a proper context-aware support to adaptable Java applications for limited devices. In this paper we present the novel hybrid (from static to dynamic) analysis approach that Chameleon uses for inspecting (adaptable) Java programs with respect to their resource consumption in a given execution environment. This analysis permits to quantitatively compare alternative versions of the same program. The analysis is based on a resource model for specifying resource provisions and consumptions, and a parametric transition system that performs the actual analysis.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {987–1009},
numpages = {23},
keywords = {Adaptable applications, Analysis, Tool support}
}

@article{10.1016/j.jss.2012.02.044,
author = {Weidlich, Matthias and Mendling, Jan and Weske, Mathias},
title = {Propagating changes between aligned process models},
year = {2012},
issue_date = {August, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.02.044},
doi = {10.1016/j.jss.2012.02.044},
abstract = {There is a wide variety of drivers for business process modelling initiatives, reaching from organisational redesign to the development of information systems. Consequently, a common business process is often captured in multiple models that overlap in content due to serving different purposes. Business process management aims at flexible adaptation to changing business needs. Hence, changes of business processes occur frequently and have to be incorporated in the respective process models. Once a process model is changed, related process models have to be updated accordingly, despite the fact that those process models may only be loosely coupled. In this article, we introduce an approach that supports change propagation between related process models. Given a change in one process model, we leverage the behavioural abstraction of behavioural profiles for corresponding activities in order to determine a change region in another model. Our approach is able to cope with changes in pairs of models that are not related by hierarchical refinement and show behavioural inconsistencies. We evaluate the applicability of our approach with two real-world process model collections. To this end, we either deduce change operations from different model revisions or rely on synthetic change operations.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1885–1898},
numpages = {14},
keywords = {Behavioural analysis, Change propagation, Model synchronisation, Process model alignment}
}

@article{10.1016/S0166-3615(05)00150-8,
title = {Author Index},
year = {2005},
issue_date = {December, 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {56},
number = {8–9},
issn = {0166-3615},
url = {https://doi.org/10.1016/S0166-3615(05)00150-8},
doi = {10.1016/S0166-3615(05)00150-8},
journal = {Comput. Ind.},
month = dec,
pages = {1016–1021},
numpages = {6}
}

@book{10.1145/2534860,
author = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
title = {Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science},
year = {2013},
isbn = {9781450323093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@article{10.1016/j.infsof.2015.02.001,
author = {de Magalh\~{a}es, Cleyton V.C. and da Silva, Fabio Q.B. and Santos, Ronnie E.S. and Suassuna, Marcos},
title = {Investigations about replication of empirical studies in software engineering},
year = {2015},
issue_date = {August 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {64},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.02.001},
doi = {10.1016/j.infsof.2015.02.001},
abstract = {ContextTwo recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012). ObjectiveIn this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research. MethodWe applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996-2012 complemented by manual and automatic search procedures that collected articles published in 2013. ResultsWe analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012. ConclusionsReplication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {76–101},
numpages = {26},
keywords = {Empirical studies, Experiments, Mapping study, Replications, Software engineering, Systematic literature review}
}

@inbook{10.5555/1793854.1793861,
author = {Alves, Vander and Matos, Pedro and Cole, Leonardo and Vasconcelos, Alexandre and Borba, Paulo and Ramalho, Geber},
title = {Extracting and evolving code in product lines with aspect-oriented programming},
year = {2007},
isbn = {3540770410},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For some organizations, the proactive approach to product lines may be inadequate due to prohibitively high investment and risks. As an alternative, the extractive and the reactive approaches are incremental, offering moderate costs and risks, and therefore sometimes may be more appropriate. However, combining these two approaches demands a more detailed process at the implementation level. This paper presents a method and a tool for extracting a product line and evolving it, relying on a strategy that uses refactorings expressed in terms of simpler programming laws. The approach is evaluated with a case study in the domain of games for mobile devices, where variations are handled with aspect-oriented constructs.},
booktitle = {Transactions on Aspect-Oriented Software Development IV},
pages = {117–142},
numpages = {26}
}

@article{10.1016/j.is.2015.06.003,
author = {Alam, Khubaib Amjad and Ahmad, Rodina and Akhunzada, Adnan and Nasir, Mohd Hairul Nizam Md and Khan, Samee U.},
title = {Impact analysis and change propagation in service-oriented enterprises},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {54},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2015.06.003},
doi = {10.1016/j.is.2015.06.003},
abstract = {ContextThe adoption of Service-oriented Architecture (SOA) and Business Process Management (BPM) is fairly recent. The major concern is now shifting towards the maintenance and evolution of service-based business information systems. Moreover, these systems are highly dynamic and frequent changes are anticipated across multiple levels of abstraction. Impact analysis and change propagation are identified as potential research areas in this regard. ObjectiveThe aim of this study is to systematically review extant research on impact analysis and propagation in the BPM and SOA domains. Identifying, categorizing and synthesizing relevant solutions are the main study objectives. MethodThrough careful review and screening, we identified 60 studies relevant to 4 research questions. Two classification schemes served to comprehend and analyze the anatomy of existing solutions. BPM is considered at the business level for business operations and processes, while SOA is considered at the service level as deployment architecture. We focused on both horizontal and vertical impacts of changes across multiple abstraction layers. ResultsImpact analysis solutions were mainly divided into dependency analysis, traceability analysis and history mining. Dependency analysis is the most frequently adopted technique followed by traceability analysis. Further categorization of dependency analysis indicates that graph-based techniques are extensively used, followed by formal dependency modeling. While considering hierarchical coverage, inter-process and inter-service change analyses have received considerable attention from the research community, whereas bottom-up analysis has been the most neglected research area. The majority of change propagation solutions are top-down and semi-automated. ConclusionsThis study concludes with new insight suggestions for future research. Although, the evolution of service-based systems is becoming of grave concern, existing solutions in this field are less mature. Studies on hierarchical change impact are scarce. Complex relationships of services with business processes and semantic dependencies are poorly understood and require more attention from the research community.},
journal = {Inf. Syst.},
month = dec,
pages = {43–73},
numpages = {31},
keywords = {BPM, CIA, Change propagation, Dependency analysis, MSR, SOA, SOC, Semantic annotation, Systematic literature review (SLR), Web service}
}

@article{10.1504/ijbis.2019.103793,
author = {Santos, Carlos Habekost Dos and Thom, Lucin\'{e}ia Heloisa and Cota, \'{E}rika and Fantinato, Marcelo},
title = {Supporting BPMN tool developers through meta-algorithms},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {32},
number = {4},
issn = {1746-0972},
url = {https://doi.org/10.1504/ijbis.2019.103793},
doi = {10.1504/ijbis.2019.103793},
abstract = {Business process model and notation (BPMN) provides an extensive set of notational elements, such as activities, events and gateways, which enable the representation of a wide variety of business processes. The purpose of this paper is to propose an approach to develop logical models (referred here as 'meta-algorithms'), whose goal is to express the content described in the textual rules, for each BPMN element, considering the BPMN2.0.2 specification. We identified that there is completeness and correctness in this approach, applying decision tables and control-flow graphs to checking the proposed meta-algorithms. In addition, we applied two surveys, considering a potential audience for the proposed meta-algorithms in order to check users' acceptance. For validation results, we identified that only textual rules are not enough to implement the notational elements for 84% of the survey participants.},
journal = {Int. J. Bus. Inf. Syst.},
month = jan,
pages = {460–488},
numpages = {28},
keywords = {BPMN, tool development, notational elements, meta-algorithms, business information systems, developer, business rule}
}

@article{10.1016/j.cie.2011.12.011,
author = {Hasan, Mohd. Asif and Sarkis, Joseph and Shankar, Ravi},
title = {Agility and production flow layouts: An analytical decision analysis},
year = {2012},
issue_date = {May, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {62},
number = {4},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2011.12.011},
doi = {10.1016/j.cie.2011.12.011},
abstract = {The term 'agile manufacturing' has referred to operational aspects of a manufacturing company concerning their ability to produce customized products at mass production prices and with short lead times. A core issue faced within agile manufacturing is the need for appropriate and supporting production and operations systems. Many design dimensions of agility and agile manufacturing exist. To help attain this goal for integrating the many design dimensions, operations infrastructure and capacity must be carefully planned to manage production flow, and thus production layout planning takes on an increasingly important role. Given the importance of these dimensions in response to agility, this paper seeks to make a contribution by providing insights into a decision aid for evaluating production flow layouts that support and enhance the agile manufacture of products. Layout design has a significant impact on the performance of a manufacturing or service industry system and has been an active research area for many decades. Strategic evaluation of production layouts requires consideration of both qualitative and quantitative factors (managerial, organizational, and technical). This paper makes use of the Analytical Network Process (ANP) which captures interdependencies among different criteria, sub-criteria and dimensions, an evident characteristic of production flow layouts in complex agile manufacturing environments. An application case study exemplifying the practical usefulness of this type of model describes how management, after implementation of the model, made a mid-course correction related to the production layout initially selected.},
journal = {Comput. Ind. Eng.},
month = may,
pages = {898–907},
numpages = {10},
keywords = {Agile manufacturing, Agility, Analytical Network Process (ANP), Layout, Production flow layout, Production layout}
}

@article{10.14778/3007263.3007272,
author = {Jacques-Silva, Gabriela and Zheng, Fang and Debrunner, Daniel and Wu, Kun-Lung and Dogaru, Victor and Johnson, Eric and Spicer, Michael and Sariy\"{u}ce, Ahmet Erdem},
title = {Consistent regions: guaranteed tuple processing in IBM streams},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007272},
doi = {10.14778/3007263.3007272},
abstract = {Guaranteed tuple processing has become critically important for many streaming applications. This paper describes how we enabled IBM Streams, an enterprise-grade stream processing system, to provide data processing guarantees. Our solution goes from language-level abstractions to a runtime protocol. As a result, with a couple of simple annotations at the source code level, IBM Streams developers can define consistent regions, allowing any subgraph of their streaming application to achieve guaranteed tuple processing. At runtime, a consistent region periodically executes a variation of the Chandy-Lamport snapshot algorithm to establish a consistent global state for that region. The coupling of consistent states with data replay enables guaranteed tuple processing.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1341–1352},
numpages = {12}
}

@article{10.1145/2723872.2723876,
author = {Howard, Heidi and Schwarzkopf, Malte and Madhavapeddy, Anil and Crowcroft, Jon},
title = {Raft Refloated: Do We Have Consensus?},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/2723872.2723876},
doi = {10.1145/2723872.2723876},
abstract = {The Paxos algorithm is famously difficult to reason about and even more so to implement, despite having been synonymous with distributed consensus for over a decade. The recently proposed Raft protocol lays claim to being a new, understandable consensus algorithm, improving on Paxos without making compromises in performance or correctness.In this study, we repeat the Raft authors' performance analysis. We developed a clean-slate implementation of the Raft protocol and built an event-driven simulation framework for prototyping it on experimental topologies. We propose several optimizations to the Raft protocol and demonstrate their effectiveness under contention. Finally, we empirically validate the correctness of the Raft protocol invariants and evaluate Raft's understandability claims.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jan,
pages = {12–21},
numpages = {10}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@inbook{10.1145/3191315.3191321,
author = {De Cat, Broes and Bogaerts, Bart and Bruynooghe, Maurice and Janssens, Gerda and Denecker, Marc},
title = {Predicate logic as a modeling language: the IDP system},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191321},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {279–323},
numpages = {45}
}

@article{10.1016/j.robot.2009.03.006,
author = {Cherubini, A. and Giannone, F. and Iocchi, L. and Lombardo, M. and Oriolo, G.},
title = {Policy gradient learning for a humanoid soccer robot},
year = {2009},
issue_date = {July, 2009},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {57},
number = {8},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2009.03.006},
doi = {10.1016/j.robot.2009.03.006},
abstract = {In humanoid robotic soccer, many factors, both at low-level (e.g., vision and motion control) and at high-level (e.g., behaviors and game strategies), determine the quality of the robot performance. In particular, the speed of individual robots, the precision of the trajectory, and the stability of the walking gaits, have a high impact on the success of a team. Consequently, humanoid soccer robots require fine tuning, especially for the basic behaviors. In recent years, machine learning techniques have been used to find optimal parameter sets for various humanoid robot behaviors. However, a drawback of learning techniques is time consumption: a practical learning method for robotic applications must be effective with a small amount of data. In this article, we compare two learning methods for humanoid walking gaits based on the Policy Gradient algorithm. We demonstrate that an extension of the classic Policy Gradient algorithm that takes into account parameter relevance allows for better solutions when only a few experiments are available. The results of our experimental work show the effectiveness of the policy gradient learning method, as well as its higher convergence rate, when the relevance of parameters is taken into account during learning.},
journal = {Robot. Auton. Syst.},
month = jul,
pages = {808–818},
numpages = {11},
keywords = {Humanoid robotics, Machine learning, Motion control}
}

@inproceedings{10.1145/1868294.1868300,
author = {Sincero, Julio and Tartler, Reinhard and Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang},
title = {Efficient extraction and analysis of preprocessor-based variability},
year = {2010},
isbn = {9781450301541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868294.1868300},
doi = {10.1145/1868294.1868300},
abstract = {The C Preprocessor (CPP) is the tool of choice for the implementation of variability in many large-scale configurable software projects. Linux, probably the most-configurable piece of software ever, employs more than 10,000 preprocessor variables for this purpose. However, this de-facto variability tends to be "hidden in the code"; which on the long term leads to variability defects, such as dead code or inconsistencies with respect to the intended (modeled) variability of the software. This calls for tool support for the efficient extraction of (and reasoning over) CPP-based variability.We suggest a novel approach to extract CPP-based variability. Our tool transforms CPP-based variability in O(n) complexity into a propositional formula that "mimics" all valid effects of conditional compilation and can be analyzed with standard SAT or BDD packages.Our evaluation results demonstrate the scalability and practicability of the approach. A dead-block-analysis on the complete Linux source tree takes less than 30 minutes; we thereby have revealed 60 dead blocks, 2 of which meanwhile have been confirmed as new (and long-lasting) bugs; the rest is still under investigation.},
booktitle = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering},
pages = {33–42},
numpages = {10},
keywords = {conditional compilation, linux, variability},
location = {Eindhoven, The Netherlands},
series = {GPCE '10}
}

@article{10.1016/j.dss.2006.02.005,
author = {Mohan, Kannan and Jain, Radhika and Ramesh, Balasubramaniam},
title = {Knowledge networking to support medical new product development},
year = {2007},
issue_date = {August, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {43},
number = {4},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2006.02.005},
doi = {10.1016/j.dss.2006.02.005},
abstract = {New product development (NPD) in the pharmaceutical industry is very knowledge intensive. Knowledge generated and used during medical NPD processes is fragmented and distributed across various phases and artifacts. Many challenges in medical NPD can be addressed by the integration of this fragmented knowledge. We propose the creation and use of knowledge networks to address these challenges. Based on a case study conducted in a leading pharmaceutical company, we have developed a knowledge framework that represents knowledge fragments that need to be integrated to support medical NPD. We have also developed a prototype system that supports knowledge integration using knowledge networks. We illustrate the capabilities of the system through scenarios drawn from the case study. Qualitative validation of our approach is also presented.},
journal = {Decis. Support Syst.},
month = aug,
pages = {1255–1273},
numpages = {19},
keywords = {Healthcare, Knowledge integration, Knowledge networks, New product development, Pharmaceutical knowledge management}
}

@article{10.1007/s00766-013-0198-z,
author = {Ghezzi, Carlo and Menghi, Claudio and Molzam Sharifloo, Amir and Spoletini, Paola},
title = {On requirement verification for evolving Statecharts specifications},
year = {2014},
issue_date = {September 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0198-z},
doi = {10.1007/s00766-013-0198-z},
abstract = {Software development processes have been evolving from rigid, pre-specified, and sequential to incremental, and iterative. This evolution has been dictated by the need to accommodate evolving user requirements and reduce the delay between design decision and feedback from users. Formal verification techniques, however, have largely ignored this evolution and even when they made enormous improvements and found significant uses in practice, like in the case of model checking, they remained confined into the niches of safety-critical systems. Model checking verifies if a system's model  $$mathcal{M}$$ M  satisfies a set of requirements, formalized as a set of logic properties  $$Phi$$  . Current model-checking approaches, however, implicitly rely on the assumption that both the  complete  model  $$mathcal{M}$$ M  and the whole set of properties  $$Phi$$   are fully specified when verification takes place. Very often, however,  $$mathcal{M}$$ M  is subject to change because its development is iterative and its definition evolves through stages of incompleteness, where alternative design decisions are explored, typically to evaluate some quality trade-offs. Evolving systems specifications of this kind ask for novel verification approaches that tolerate incompleteness and support incremental analysis of alternative designs for certain functionalities. This is exactly the focus of this paper, which develops an incremental model-checking approach for evolving Statecharts. Statecharts have been chosen both because they are increasingly used in practice natively support model refinements.},
journal = {Requir. Eng.},
month = sep,
pages = {231–255},
numpages = {25},
keywords = {Agile development, Formal verification, Incremental verification, Model checking, Software modeling, Statecharts}
}

@inbook{10.1145/3191315.3191323,
author = {Dal Pal\`{u}, Alessandro and Dovier, Agostino and Formisano, Andrea and Pontelli, Enrico},
title = {Exploring life: answer set programming in bioinformatics},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191323},
abstract = {This chapter provides a broad overview of howlogic programming, and more specifically Answer Set Programming (ASP), can be used to model and solve some popular and challenging classes of problems in the general domain of bioinformatics. In particular, the chapter explores the use of ASP in Genomics studies, such as Haplotype inference and Phylogenetic inference, in Structural studies, such as RNA secondary structure prediction and Protein structure prediction, and in Systems Biology. The chapter offers a brief introduction to biology and bioinformatics and working ASP code fragments for the various problems investigated. The chapter serves a dual role: (1) it offers a declarative characterization of a number of core problems in bioinformatics, making them easily understandable; and (2) it provides an "entry point" to the extensive literature on the use of logic-based methods to address such bioinformatics problems, by offering the basic modeling of the problem and pointers to the relevant literature.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {359–412},
numpages = {54}
}

@article{10.4018/jismd.2012100102,
author = {van de Weerd, Inge and Mirandolle, Dominique and Brinkkemper, Sjaak},
title = {Situational Fit in Incremental Method Engineering},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100102},
doi = {10.4018/jismd.2012100102},
abstract = {Almost all software vendors use methods and techniques in their software development and production processes. In order to improve the maturity of their processes, an incremental method engineering approach can be followed to adapt and improve methods and techniques. In order to ensure the suitability of selected methods, the authors propose the concept of situational fit to balance environmental characteristics, company characteristics, and information system development methods. They carried out a case study to illustrate the process of incremental method engineering. Furthermore, the authors performed a quantitative analysis on a data set of 38 companies to evaluate the use of situational fit.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {27–45},
numpages = {19},
keywords = {Incremental Method Engineering, Production Processes, Situational Factors, Situational Fit, Software Development, Software Product Management, Vendors}
}

@book{10.5555/2742301,
author = {Preim, Bernhard and Botha, Charl P.},
title = {Visual Computing for Medicine: Theory, Algorithms, and Applications},
year = {2013},
isbn = {9780124159792},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includes algorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques from research, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field.}
}

@article{10.1147/JRD.2013.2243551,
author = {Biem, A. and Feng, H. and Riabov, A. V. and Turaga, D. S.},
title = {Real-time analysis and management of big time-series data},
year = {2013},
issue_date = {May/July 2013},
publisher = {IBM Corp.},
address = {USA},
volume = {57},
number = {3–4},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2013.2243551},
doi = {10.1147/JRD.2013.2243551},
abstract = {The ability to process and analyze large volumes of time-series data is in increasing demand in various domains including health care, finance, energy and utilities, transportation, and cybersecurity. Despite the broad use of time-series data worldwide, the design of a system to easily manage, analyze, and visualize large multidimensional time series, with dimensions on the order of hundreds of thousands, is still a challenging endeavor. This paper describes the Streaming Time-Series Analysis and Management (STAM) system as a solution to this problem. STAM provides the capability to glean actionable information from continuously changing time series with thousands of dimensions, in real time. STAM exploits the IBM InfoSphere® Streams platform and allows for general-purpose large-scale time-series analytics for applications including anomaly detection, modeling, smoothing, forecasting, and tracking. In addition, the system provides user-friendly tools for managing, deploying, and initiating analytics on large-scale data streams of interest, and provides a web-based graphical visualization interface that allows highlighting of events of interest with interactive menus. In this paper, we describe the system and illustrate its use in a large-scale system-monitoring application.},
journal = {IBM J. Res. Dev.},
month = may,
articleno = {1},
numpages = {1}
}

@inproceedings{10.1145/2967973.2968606,
author = {Grewe, Sylvia and Erdweg, Sebastian and Raulf, Michael and Mezini, Mira},
title = {Exploration of language specifications by compilation to first-order logic},
year = {2016},
isbn = {9781450341486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967973.2968606},
doi = {10.1145/2967973.2968606},
abstract = {Exploration of language specifications helps to discover errors and inconsistencies early during the development of a programming language. We propose exploration of language specifications via application of existing automated first-order theorem provers (ATPs). To this end, we translate language specifications and exploration tasks to first-order logic, which many ATPs accept as input. However, there are several different strategies for compiling a language specification to first-order logic, and even small variations in the translation may have a large impact on the time it takes ATPs to find proofs.In this paper, we present a systematic empirical study on how to best compile language specifications to first-order logic such that existing ATPs can solve typical exploration tasks efficiently. We have developed a compiler product line that implements 36 different compilation strategies and used it to feed language specifications to 4 existing first-order theorem provers. As a benchmark, we developed a language specification for typed SQL with 50 exploration goals. Our study empirically confirms that the choice of a compilation strategy in general greatly influences prover performance and shows which strategies are advantageous for prover performance.},
booktitle = {Proceedings of the 18th International Symposium on Principles and Practice of Declarative Programming},
pages = {104–117},
numpages = {14},
keywords = {declarative languages, domain-specific languages, first-order theorem proving, formal specification, type systems},
location = {Edinburgh, United Kingdom},
series = {PPDP '16}
}

@article{10.1007/s10845-013-0749-7,
author = {Vin, Emmanuelle and Delchambre, Alain},
title = {Generalized cell formation: iterative versus simultaneous resolution with grouping genetic algorithm},
year = {2014},
issue_date = {Oct 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-013-0749-7},
doi = {10.1007/s10845-013-0749-7},
abstract = {For each industrial, lean manufacturing is "The method" to improve productivity and reduce cost. One of the tools for lean is cellular manufacturing. This technique reduces the factory to several small entities which are easier to manage. The algorithm proposed in this paper is based on a simultaneous resolution of two interdependent problems. These two problems emerge when the flexibility is used during the production process. This paper proves the efficiency of the simultaneous resolution comparing to the sequential resolution with iterations. To compare only the resolution method, a unique grouping genetic algorithm is adapted to be used in both cases.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1113–1124},
numpages = {12},
keywords = {Cellular, Flow analysis, Genetic algorithm, Lean manufacturing}
}

@inproceedings{10.1145/1632149.1632165,
author = {Munga, Neeshal and Fogwill, Thomas and Williams, Quentin},
title = {The adoption of open source software in business models: a Red Hat and IBM case study},
year = {2009},
isbn = {9781605586434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1632149.1632165},
doi = {10.1145/1632149.1632165},
abstract = {Free / Libre open source software (FLOSS/OSS) has gained increasing popularity and utilisation in commercial and governmental organisations. Universities like Harvard and Stanford now offer courses on open source as a business and also on how businesses can compete with open source firms. However, very little research has been published in regards to the influence of OSS on business strategies; the use of OSS as a viable business or its value proposition within a commercial entity. The business model, a powerful tool for extracting economic value from the potential of technological innovation, clearly plays an important role in the success of a business. In this paper we investigate the role of open source in the business models of Red Hat and IBM and describe how OSS has contributed to their success. A framework recently developed by some of the authors is used to evaluate and identify the key factors important to the integration of OSS strategies into traditional business models.},
booktitle = {Proceedings of the 2009 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists},
pages = {112–121},
numpages = {10},
keywords = {business models, case study, open source software},
location = {Vanderbijlpark, Emfuleni, South Africa},
series = {SAICSIT '09}
}

@inproceedings{10.1145/2335484.2335519,
author = {Paschke, Adrian and Vincent, Paul and Alves, Alex and Moxey, Catherine},
title = {Tutorial on advanced design patterns in event processing},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335519},
doi = {10.1145/2335484.2335519},
abstract = {We introduce a reference architecture for event processing, as defined by the EPTS reference architecture (RA) working group. An event processing reference architecture allows users to quickly create event processing solutions that adhere to known stakeholder requirements and architectural qualities. The focus in this paper is on the EPTS reference architecture description of the functional view which is supported by a mapping of its functions into design patterns as means to derive and prove these architectural descriptions to be usable solutions for recurring best practice implementations in common CEP languages.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {324–334},
numpages = {11},
keywords = {complex event processing, design patterns, reference architecture},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/775832.775943,
author = {Magarshack, Philippe and Paulin, Pierre G.},
title = {System-on-chip beyond the nanometer wall},
year = {2003},
isbn = {1581136889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775832.775943},
doi = {10.1145/775832.775943},
abstract = {In this paper, we analyze the emerging trends in the design of complex Systems-on-a-Chip for nanometer-scale semiconductor technologies and their impact on design automation requirements, from the perspective of a broad range SoC supplier.We present our vision of some of the key changes that will emerge in the next five years. This vision is characterized by two major paradigm changes. The first is that SoC design will become divided into four mostly non-overlapping distinct abstraction levels. Very different competences and design automation tools will be needed at each level.The second paradigm change is the emergence of domain-specific S/W programmable SoC platforms consisting of large, heterogeneous sets of embedded processors. These will be complemented by embedded reconfigurable hardware and networks-on-chip. A key enabler for the effective us of these flexible SoC platforms, is a high-level parallel programming model supporting automatic specification-to-platform mapping.},
booktitle = {Proceedings of the 40th Annual Design Automation Conference},
pages = {419–424},
numpages = {6},
keywords = {design automation tools, embedded software technologies, multi-processor systems, network-on-chip, reconfigurable systems, system-on-chip},
location = {Anaheim, CA, USA},
series = {DAC '03}
}

@inproceedings{10.1145/1811212.1811216,
author = {Brauer, J\"{o}rg and Noll, Thomas and Schlich, Bastian},
title = {Interval analysis of microcontroller code using abstract interpretation of hardware and software},
year = {2010},
isbn = {9781450300841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1811212.1811216},
doi = {10.1145/1811212.1811216},
abstract = {Static analysis is often performed on source code where intervals -- possibly the most widely used numeric abstract domain -- have successfully been used as a program abstraction for decades. Binary code on microcontroller platforms, however, is different from high-level code in that data is frequently altered using bitwise operations and the results of operations often depend on the hardware configuration. We describe a method that combines word- and bit-level interval analysis and integrates a hardware model by means of abstract interpretation in order to handle these peculiarities. Moreover, we show that this method proves powerful enough to derive invariants that could so far only be verified using computationally more expensive techniques such as model checking.},
booktitle = {Proceedings of the 13th International Workshop on Software &amp; Compilers for Embedded Systems},
articleno = {3},
numpages = {10},
keywords = {abstract interpretation, binary code, embedded systems, interval analysis, static analysis},
location = {St. Goar, Germany},
series = {SCOPES '10}
}

@article{10.1016/j.csl.2012.01.008,
author = {Li, Ming and Han, Kyu J. and Narayanan, Shrikanth},
title = {Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
year = {2013},
issue_date = {January, 2013},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {27},
number = {1},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.01.008},
doi = {10.1016/j.csl.2012.01.008},
abstract = {The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively.},
journal = {Comput. Speech Lang.},
month = jan,
pages = {151–167},
numpages = {17},
keywords = {Age recognition, Formant, GMM, Gender recognition, Harmonic structure, Maximum likelihood linear regression, Pitch, Polynomial expansion, Prosodic features, SVM, Score level fusion, Sparse representation, UBM weight posterior probability supervectors}
}

@inproceedings{10.1145/1370175.1370181,
author = {Oreizy, Peyman and Medvidovic, Nenad and Taylor, Richard N.},
title = {Runtime software adaptation: framework, approaches, and styles},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370175.1370181},
doi = {10.1145/1370175.1370181},
abstract = {Our ICSE 1998 paper showed how an application can be adapted at runtime by manipulating its architectural model. In particular, our paper demonstrated the beneficial role of (1) software connectors in aiding runtime change, (2) an explicit architectural model fielded with the system and used as the basis for runtime change, and (3) architectural style in providing both structural and behavioral constraints over runtime change. This paper examines runtime evolution in the decade hence. A broad framework for studying and describing evolution is introduced that serves to unify the wide range of work now found in the field of dynamic software adaptation. This paper also looks to the future, identifying what we believe to be highly promising directions.},
booktitle = {Companion of the 30th International Conference on Software Engineering},
pages = {899–910},
numpages = {12},
keywords = {architectural styles, autonomic computing, software adaptation, software architecture, software evolution},
location = {Leipzig, Germany},
series = {ICSE Companion '08}
}

@book{10.5555/1564784,
author = {Wang},
title = {System-on-Chip Test Architectures: Nanometer  Design for Testability},
year = {2007},
isbn = {9780080556802},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Modern electronics testing has a legacy of more than 40 years. The introduction of new technologies, especially nanometer technologies with 90nm or smaller geometry, has allowed the semiconductor industry to keep pace with the increased performance-capacity demands from consumers. As a result, semiconductor test costs have been growing steadily and typically amount to 40% of today's overall product cost. This book is a comprehensive guide to new VLSI Testing and Design-for-Testability techniques that will allow students, researchers, DFT practitioners, and VLSI designers to master quickly System-on-Chip Test architectures, for test debug and diagnosis of digital, memory, and analog/mixed-signal designs. KEY FEATURES * Emphasizes VLSI Test principles and Design for Testability architectures, with numerous illustrations/examples. * Most up-to-date coverage available, including Fault Tolerance, Low-Power Testing, Defect and Error Tolerance, Network-on-Chip (NOC) Testing, Software-Based Self-Testing, FPGA Testing, MEMS Testing, and System-In-Package (SIP) Testing, which are not yet available in any testing book. * Covers the entire spectrum of VLSI testing and DFT architectures, from digital and analog, to memory circuits, and fault diagnosis and self-repair from digital to memory circuits. * Discusses future nanotechnology test trends and challenges facing the nanometer design era; promising nanotechnology test techniques, including Quantum-Dots, Cellular Automata, Carbon-Nanotubes, and Hybrid Semiconductor/Nanowire/Molecular Computing. * Practical problems at the end of each chapter for students.}
}

@article{10.1145/352029.352035,
author = {van Deursen, Arie and Klint, Paul and Visser, Joost},
title = {Domain-specific languages: an annotated bibliography},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/352029.352035},
doi = {10.1145/352029.352035},
abstract = {We survey the literature available on the topic of domain-specific languages as used for the construction and maintenance of software systems. We list a selection of 75 key publications in the area, and provide a summary for each of the papers. Moreover, we discuss terminology, risks and benefits, example domain-specific languages, design methodologies, and implementation techniques.},
journal = {SIGPLAN Not.},
month = jun,
pages = {26–36},
numpages = {11}
}

@article{10.1016/j.aei.2012.02.010,
author = {Yin, C. -G. and Ma, Y. -S.},
title = {Parametric feature constraint modeling and mapping in product development},
year = {2012},
issue_date = {August, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {26},
number = {3},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2012.02.010},
doi = {10.1016/j.aei.2012.02.010},
abstract = {This paper presents the exploration and application of a feature-based design methodology within mechanical product development cycles. Based on a review of the feature technology and previous research work, this paper focuses on the modeling of intricate relations among features of different design aspects. A concept of feature parameter map that leads to a constraint mapping method is proposed. Further, features are classified into different levels; and information management for product lifecycle support is considered. The application of this method is demonstrated with the conceptual design and optimization of a gearbox as the study case. In addition, an extended feature system for product development was explored. With a spreadsheet package and a computer aided design (CAD) tool, the product model generation, change management and final optimization of the case assembly including its bulk shape, have been achieved. Two important information chains were used to address the aspect of ''design for post-manufacture services'' with concurrent engineering consideration, i.e. a field installation pattern and a set of wrapping dimensions for product transport packaging. In order to demonstrate the feasibility of change management, a different product derivative model was regenerated by adopting and changing the values of a main conceptual feature. The result is promising.},
journal = {Adv. Eng. Inform.},
month = aug,
pages = {539–552},
numpages = {14},
keywords = {Assembly feature (AF), Concurrent engineering (CE), Engineering informatics, Feature-based design (FBD), Product lifecycle management (PLM)}
}

@techreport{10.5555/886615,
author = {C. M., Holloway},
title = {Lfm2000 - Fifth NASA Langley Formal Methods Workshop},
year = {2000},
publisher = {NASA Langley Technical Report Server},
abstract = {This is the proceedings of Lfm2000: Fifth NASA Langley Formal Methods Workshop. The workshop was held June 13-15, 2000, in Williamsburg, Virginia. See the web site http://shemesh.larc.nasa.gov/lfm2000/ for complete information about the event.}
}

@article{10.1145/1218776.1218777,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Miscellaneous material)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1218776.1218777},
doi = {10.1145/1218776.1218777},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {0},
numpages = {36}
}

@inproceedings{10.5555/646781.705926,
author = {Barth, Barbara and Butler, Gregory and Czarnecki, Krzysztof and Eisenecker, Ulrich W.},
title = {Generative Programming},
year = {2001},
isbn = {3540436758},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This report describes the results of a one-day workshop on Generative Programming (GP) at ECOOP'01. The goal of the workshop was to discuss the state-of-the-art of generative programming, share experience, consolidate successful techniques, discuss the relation of GP to object-oriented programming and other emerging approaches such as Aspect-Oriented Programming or Multi-dimensional Decomposition, and identify open issues for future work. This report gives a summary of the workshop contributions, debates, and the identified future directions.},
booktitle = {Proceedings of the Workshops on Object-Oriented Technology},
pages = {135–149},
numpages = {15},
series = {ECOOP '01}
}

@book{10.5555/1805911,
author = {Cypher, Allen and Dontcheva, Mira and Lau, Tessa and Nichols, Jeffrey},
title = {No Code Required: Giving Users Tools to Transform the Web},
year = {2010},
isbn = {012381541X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Revolutionary tools are emerging from research labs that enable all computer users to customize and automate their use of the Web without learning how to program. No Code Required takes cutting edge material from academic and industry leaders - the people creating these tools -- and presents the research, development, application, and impact of a variety of new and emerging systems. *The first book since Web 2.0 that covers the latest research, development, and systems emerging from HCI research labs on end user programming tools *Featuring contributions from the creators of Adobe s Zoetrope and Intel s Mash Maker, discussing test results, implementation, feedback, and ways forward in this booming area *Companion Web site features video demonstrations of each system (http://www.elsevierdirect.com/v2/companion.jsp ISBN=9780123815415) Table of Contents Introduction End User Programming on the Web Allen Cypher (IBM) Why We Customize the Web Robert Miller (MIT) I. End User Programming Languages for the Web Sloppy Programming Greg Little (MIT) Mixing the reactive with the personal: Opportunities for end user programming in Personal information management (system) Max Van Kleek (MIT) Going beyond PBD: A Play-by-Play and Mixed-initiative Approach (system) Hyuckchul Jung (Institute for Human and Machine Cognition) Rewriting the Web with Chickenfoot (system) Robert Miller (MIT) A Goal-Oriented Web Browser (system) Alexander Faaborg (Mozilla) II. Systems and Applications Clip, Connect, Clone: Combining Application Elements to Build Custom Interfaces for Information Access (system) Jun Fujima (Hokkaido) Mash Maker (system) Robert Ennals (Intel) Collaborative scripting on the web (system) Tessa Lau (IBM) Programming by a Sample: Rapidly Creating Web Applications with d.mix (system) Bj rn Hartmann (Stanford) Highlight: End User Mobilization of Existing Web Sites (system) Jeffrey Nichols (IBM) Subjunctive Interfaces for the Web Aran Lunzer (University of Copenhagen) From Web Summaries to Search Templates: Automation for Personal Web Content (system) Mira Dontcheva (Adobe Systems) Access to the Temporal Web Through Zoetrope (system) Eytan Adar (University of Washington) Enabling End Users to Independently Build Accessibility into the Web Jeffrey Bigham (University of Washington) Social Accessibility: A Collaborative Approach For Improving Web Accessibility (system) Yevgen Borodin (Stony Brook) III. Data Management and Interoperability A World Wider than the Web: End User Programming Across Multiple Domains (system) Will Haines (SRI) Knowing What You're Talking About: Natural Language Programming of a Multi-Player Online Game (system) Henry Lieberman (MIT) IV. User Studies Mashups for Web-Active End Users Nan Zang (Penn State) Mashed layers and muddled models: debugging mashup applications M. Cameron Jones (Yahoo!) Reuse in the world of end-user programmers Christopher Scaffidi (CMU) Using Web Search to Write Programs Joel Brandt (Stanford)}
}

@proceedings{10.1145/2951913,
title = {ICFP 2016: Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nara, Japan}
}

@book{10.5555/2974978,
author = {Rabinowitz, Harold and Vogel, Suzanne},
title = {The Manual of Scientific Style: A Guide for Authors, Editors, and Researchers},
year = {2009},
isbn = {9780080557960},
publisher = {Academic Press, Inc.},
address = {USA},
abstract = {Much like the Chicago Manual of Style, The Manual of Scientific Style addresses all stylistic matters in the relevant disciplines of physical and biological science, medicine, health, and technology. It presents consistent guidelines for text, data, and graphics, providing a comprehensive and authoritative style manual that can be used by the professional scientist, science editor, general editor, science writer, and researcher. Scientific disciplines treated independently, with notes where variances occur in the same linguistic areas Organization and directives designed to assist readers in finding the precise usage rule or convention A focus on American usage in rules and formulations with noted differences between American and British usage Differences in the various levels of scientific discourse addressed in a variety of settings in which science writing appears Instruction and guidance on the means of improving clarity, precision, and effectiveness of science writing, from its most technical to its most popular Table of Contents Part I. General Style Manuscript Preparation General Style Units of Measurement Citation of References Presentation of Data and Figures Part II. References, Citations and Quotations Standards for Clear and Proper Attribution Standard Citation Formats Text Sources Audiovisual Media Electronic Sources Part III. Style Issues for Specific Disciplines Mathematics Physics Chemistry Earth and Environmental Science Life Science Medicine Civil, Mechanical and Electrical Engineering Computer Science and Information Science Appendices Scientific Organizations and Publications: Standard Abbreviations Classification Schemes in Science and Technology Standard Abbreviation Dictionary Difficult and Troublesome Terms and Words Comparative Standards for Shared Terms and Conventions BibliographyIndex}
}

@article{10.1145/1658349.1658355,
author = {Frintrop, Simone and Rome, Erich and Christensen, Henrik I.},
title = {Computational visual attention systems and their cognitive foundations: A survey},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/1658349.1658355},
doi = {10.1145/1658349.1658355},
abstract = {Based on concepts of the human visual system, computational visual attention systems aim to detect regions of interest in images. Psychologists, neurobiologists, and computer scientists have investigated visual attention thoroughly during the last decades and profited considerably from each other. However, the interdisciplinarity of the topic holds not only benefits but also difficulties: Concepts of other fields are usually hard to access due to differences in vocabulary and lack of knowledge of the relevant literature. This article aims to bridge this gap and bring together concepts and ideas from the different research areas. It provides an extensive survey of the grounding psychological and biological research on visual attention as well as the current state of the art of computational systems. Furthermore, it presents a broad range of applications of computational attention systems in fields like computer vision, cognitive systems, and mobile robotics. We conclude with a discussion on the limitations and open questions in the field.},
journal = {ACM Trans. Appl. Percept.},
month = jan,
articleno = {6},
numpages = {39},
keywords = {Visual attention, biologically motivated computer vision, regions of interest, robot vision, saliency}
}

@inproceedings{10.1145/1667239.1667254,
author = {Manocha, Dinesh and Calamia, Paul and Lin, Ming C. and Manocha, Dinesh and Savioja, Lauri and Tsingos, Nicolas},
title = {Interactive sound rendering},
year = {2009},
isbn = {9781450379380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1667239.1667254},
doi = {10.1145/1667239.1667254},
abstract = {An overview of algorithmic and software technologies related to interactive sound rendering. The course lectures cover three main topics: physically based techniques to synthesize sounds generated from colliding objects or liquid sounds, efficient computation of sound propagation paths based on reflection or diffraction paths and converting those paths into audible sound, exploiting the computational capabilities of current multi-core commodity processors for real-time sound propagation and sound rendering for gaming and interactive applications. The presentations include audio demonstrations that show the meaning of various processing components in practice.},
booktitle = {ACM SIGGRAPH 2009 Courses},
articleno = {15},
numpages = {338},
location = {New Orleans, Louisiana},
series = {SIGGRAPH '09}
}

@techreport{10.1145/2594148,
title = {Computing Curricula 1991: Report of the ACM/IEEE-CS Joint Curriculum Task Force},
year = {1991},
isbn = {089793817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This report contains curricular recommendations for baccalaureate programs in the discipline of computing, which includes programs with the titles "computer science," "computer engineering," "computer science and engineering," and other similar titles. These recommendations provide a uniform basis for curriculum design across all segments of the educational community---schools and colleges of engineering, arts and sciences, and liberal arts. This report is the first comprehensive undergraduate curriculum report to be endorsed by the two major professional societies in the computing discipline---the Association for Computing Machinery and the Computer Society of the IEEE.}
}

@inproceedings{10.1145/800008.808038,
author = {Slamecka, Vladimir},
title = {Conference abstracts},
year = {1977},
isbn = {9781450373739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800008.808038},
doi = {10.1145/800008.808038},
abstract = {One problem in computer program testing arises when errors are found and corrected after a portion of the tests have run properly. How can it be shown that a fix to one area of the code does not adversely affect the execution of another area? What is needed is a quantitative method for assuring that new program modifications do not introduce new errors into the code. This model considers the retest philosophy that every program instruction that could possibly be reached and tested from the modified code be retested at least once. The problem is how to determine the minimum number of test cases to be rerun. The process first involves generating the test case dependency matrix and the reachability matrix. Using the test case dependency matrix and the appropriate rows of the reachability matrix, a 0-1 integer program can be specified. The solution of the integer program yields the minimum number of test cases to be rerun, and the coefficients of the objective function identify which specific test cases to rerun.},
booktitle = {Proceedings of the 5th Annual ACM Computer Science Conference},
pages = {1–36},
numpages = {36},
series = {CSC '77}
}

@book{10.5555/1102008,
author = {Bauer, F. L. and De Remer, F. L. and Griffiths, M. and Hill, U. and Horning, J. J. and Koster, C. H. A. and McKeeman, W. M. and Poole, P. C. and Waite, W. M. and Eickel, J. and Goos, G. and Hartmanis, J.},
title = {Compiler construction: an advanced course},
year = {1974},
isbn = {3540069585},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Advanced Course took place from March 4 to 15, 1974 and was organized by the Mathematical Institute of the Technical University of Munich and the Leibniz Computing Center of the Bavarian Academy of Sciences, in co-operation with the European Communities, sponsored by the Ministry for Research and Technology of the Federal Republic of Germany and by the European Research Office, London.}
}

@book{10.5555/1481584,
author = {Landy, Gene K. and Mastrobattista, Amy J.},
title = {The IT / Digital Legal Companion: A Comprehensive Business Guide to Software, IT, Internet, Media and IP Law},
year = {2008},
isbn = {1597492566},
publisher = {Syngress Publishing},
abstract = {To compete effectively in digital business markets, you need to understand how the law affects your digital technology business. The contents include detailed plain English business and legal guidance on:* Intellectual Property for Digital Business* Digital Contract Fundamentals* Open Source* Development and Consulting* Software as a Service* Software Licensing and Distribution* Web and Internet Agreements* Privacy* Digital Multimedia Content and Distribution* IT Standards* Web and Mobile Technology and Content Deals* Video Game Deals* International Distribution* Legal Affairs Management* Forms Appendix in the book and downloadable online 38 sample forms for deals and transactions and for the WebThe content goes from the basics to advanced topics such as off-shoring, anti-circumvention, open source business models, user-created content, reverse engineering, mobile media distribution, web and game development, mash-ups, web widgets, and massively multiplayer games.This book is designed to empower you to:* Understand the interaction between law, money and technology* Obtain and exploit a portfolio of IP assets* Build and reinforce positive relationships with other companies* Leverage your technologies* Manage risks in markets with many uncertainties* Make better deals and close deals more quickly* Act more decisively and confidently in legal mattersThis book will make you much smarter about spotting issues, perceiving risk, thinking strategically, setting priorities and using legal services effectively and efficiently. This is the most comprehensive layperson's book on the subject.Key Features:* A "need-to-know" legal companion for those competing in digital business markets in the US and around the world* Points out the relationship between legal issues and business strategies, needs and goals* Based on 15 years of legal practice in all aspects of digital, IT, software and computer law}
}

