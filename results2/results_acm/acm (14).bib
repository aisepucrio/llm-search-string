@inproceedings{10.1145/3030207.3030216,
author = {Valov, Pavel and Petkovich, Jean-Christophe and Guo, Jianmei and Fischmeister, Sebastian and Czarnecki, Krzysztof},
title = {Transferring Performance Prediction Models Across Different Hardware Platforms},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030216},
doi = {10.1145/3030207.3030216},
abstract = {Many software systems provide configuration options relevant to users, which are often called features. Features influence functional properties of software systems as well as non-functional ones, such as performance and memory consumption. Researchers have successfully demonstrated the correlation between feature selection and performance. However, the generality of these performance models across different hardware platforms has not yet been evaluated.We propose a technique for enhancing generality of performance models across different hardware environments using linear transformation. Empirical studies on three real-world software systems show that our approach is computationally efficient and can achieve high accuracy (less than 10% mean relative error) when predicting system performance across 23 different hardware platforms. Moreover, we investigate why the approach works by comparing performance distributions of systems and structure of performance models across different platforms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {39–50},
numpages = {12},
keywords = {linear transformation, model transfer, performance modelling, regression trees},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1109/ASE.2013.6693089,
author = {Guo, Jianmei and Czarnecki, Krzysztof and Apely, Sven and Siegmundy, Norbert and Wasowski, Andrzej},
title = {Variability-aware performance prediction: a statistical learning approach},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693089},
doi = {10.1109/ASE.2013.6693089},
abstract = {Configurable software systems allow stakeholders to derive program variants by selecting features. Understanding the correlation between feature selections and performance is important for stakeholders to be able to derive a program variant that meets their requirements. A major challenge in practice is to accurately predict performance based on a small sample of measured variants, especially when features interact. We propose a variability-aware approach to performance prediction via statistical learning. The approach works progressively with random samples, without additional effort to detect feature interactions. Empirical results on six real-world case studies demonstrate an average of 94% prediction accuracy based on small random samples. Furthermore, we investigate why the approach works by a comparative analysis of performance distributions. Finally, we compare our approach to an existing technique and guide users to choose one or the other in practice.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {301–311},
numpages = {11},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@inproceedings{10.1145/3205651.3208267,
author = {Xuan, Jifeng and Gu, Yongfeng and Ren, Zhilei and Jia, Xiangyang and Fan, Qingna},
title = {Genetic configuration sampling: learning a sampling strategy for fault detection of configurable systems},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208267},
doi = {10.1145/3205651.3208267},
abstract = {A highly-configurable system provides many configuration options to diversify application scenarios. The combination of these configuration options results in a large search space of configurations. This makes the detection of configuration-related faults extremely hard. Since it is infeasible to exhaust every configuration, several methods are proposed to sample a subset of all configurations to detect hidden faults. Configuration sampling can be viewed as a process of repeating a pre-defined sampling action to the whole search space, such as the one-enabled or pair-wise strategy.In this paper, we propose genetic configuration sampling, a new method of learning a sampling strategy for configuration-related faults. Genetic configuration sampling encodes a sequence of sampling actions as a chromosome in the genetic algorithm. Given a set of known configuration-related faults, genetic configuration sampling evolves the sequence of sampling actions and applies the learnt sequence to new configuration data. A pilot study on three highly-configurable systems shows that genetic configuration sampling performs well among nine sampling strategies in comparison.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1624–1631},
numpages = {8},
keywords = {software configurations, highly-configurable systems, genetic improvement, fault detection, configuration sampling},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/3106237.3106238,
author = {Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
title = {Using bad learners to find good configurations},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106238},
doi = {10.1145/3106237.3106238},
abstract = {Finding the optimally performing configuration of a software system for a given setting is often challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, building an accurate performance model can be very expensive (and is often infeasible in practice). The central insight of this paper is that exact performance values (e.g., the response time of a software system) are not required to rank configurations and to identify the optimal one. As shown by our experiments, performance models that are cheap to learn but inaccurate (with respect to the difference between actual and predicted performance) can still be used rank configurations and hence find the optimal configuration. This novel rank-based approach allows us to significantly reduce the cost (in terms of number of measurements of sample configuration) as well as the time required to build performance models. We evaluate our approach with 21 scenarios based on 9 software systems and demonstrate that our approach is beneficial in 16 scenarios; for the remaining 5 scenarios, an accurate model can be built by using very few samples anyway, without the need for a rank-based approach.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {Sampling, SBSE, Rank-based method, Performance Prediction},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3442391.3442410,
author = {Pett, Tobias and Krieter, Sebastian and Runge, Tobias and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {Stability of Product-Line Samplingin Continuous Integration},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442410},
doi = {10.1145/3442391.3442410},
abstract = {Companies strive to implement continuous integration into their development process to ensure the quality of their systems. Regression testing within the CI process considers the efficient re-test of systems after changes. However, even with regression testing, it is not feasible to test all configurations from a highly-configurable software system due to the combinatorial-explosion problem. Numerous sampling algorithms have been proposed that aim at computing a considerably smaller yet sufficiently representative set of configurations to be tested. Those algorithms are typically evaluated with regard to efficiency (i.e., number of configurations in a sample and computational effort for generating a sample) and effectiveness (i.e., feature-interaction coverage or number of faults detected). In this paper, we argue that a further crucial characteristic of sampling algorithms is their tendency to produce similar configurations when applied consecutively to an evolving configurable system. We propose sampling stability as a new evaluation criterion for sampling algorithms. We present a procedure to compute the sampling stability of sampling algorithms based on the similarity between consecutive samples. In our evaluation, we compare the sampling stability of multiple established t-wise sampling algorithms on large real-world systems.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {18},
numpages = {9},
keywords = {sampling, product-line evolution, product lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@inproceedings{10.1145/3417990.3421263,
author = {Pett, Tobias and Eichhorn, Domenik and Schaefer, Ina},
title = {Risk-based compatibility analysis in automotive systems engineering},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421263},
doi = {10.1145/3417990.3421263},
abstract = {Software is the new leading factor for innovation in the automotive industry. With the increase of software in road vehicles new business models, such as after-sale updates (i.e., Function-on-Demand) and Over-the-Air-Updates come into focus of manufacturers. When updating a road vehicle in the field, it is required to ensure functional safety. An update shall not influence existing functionality and break its safety. Hence, it must be compatible with the existing software. The compatibility of an update is ensured by testing. However, testing all variants of a highly configurable system, such as a modern car's software, is infeasible, due to the combinatorial explosion. To address this problem, in this paper, we propose a risk-based change-impact analysis to identify system variants relevant for retesting after an update. We combine existing concepts from product sampling, risk-based testing, and configuration prioritization and apply them to automotive architectures. For validating our concept, we use the Body Comfort System case study from the automotive industry. Our evaluation reveals that the concept backed by tool support may reduce testing effort by identifying and prioritizing incompatible variants wrt to a system update.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {34},
numpages = {10},
keywords = {risk-based analysis, configurable systems, automotive engineering},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {performance, fourier transform, feature interactions, boolean functions},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.5555/2873826.2874006,
author = {Hervieu, Aymeric and Marijan, Dusica and Gotlieb, Arnaud and Baudry, Benoit},
title = {Practical minimization of pairwise-covering test configurations using constraint programming},
year = {2016},
issue_date = {March 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {71},
number = {C},
issn = {0950-5849},
abstract = {Context: Testing highly-configurable software systems is challenging due to a large number of test configurations that have to be carefully selected in order to reduce the testing effort as much as possible, while maintaining high software quality. Finding the smallest set of valid test configurations that ensure sufficient coverage of the system's feature interactions is thus the objective of validation engineers, especially when the execution of test configurations is costly or time-consuming. However, this problem is NP-hard in general and approximation algorithms have often been used to address it in practice.Objective: In this paper, we explore an alternative exact approach based on constraint programming that will allow engineers to increase the effectiveness of configuration testing while keeping the number of configurations as low as possible.Method: Our approach consists in using a (time-aware) minimization algorithm based on constraint programming. Given the amount of time, our solution generates a minimized set of valid test configurations that ensure coverage of all pairs of feature values (a.k.a. pairwise coverage). The approach has been implemented in a tool called PACOGEN.Results: PACOGEN was evaluated on 224 feature models in comparison with the two existing tools that are based on a greedy algorithm. For 79% of 224 feature models, PACOGEN generated up to 60% fewer test configurations than the competitor tools. We further evaluated PACOGEN in the case study of an industrial video conferencing product line with a feature model of 169 features, and found 60% fewer configurations compared with the manual approach followed by test engineers. The set of test configurations generated by PACOGEN decreased the time required by test engineers in manual test configuration by 85%, increasing the feature-pairs coverage at the same time.Conclusion: Our experimental evaluation concluded that optimal time-aware minimization of pairwise-covering test configurations is efficiently addressed using constraint programming techniques.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {129–146},
numpages = {18},
keywords = {Variability testing, Highly-configurable software systems, Constraint programming}
}

@inproceedings{10.1145/3442391.3442411,
author = {Fischer, Stefan and Ramler, Rudolf and Klammer, Claus and Rabiser, Rick},
title = {Testing of Highly Configurable Cyber-Physical Systems – A Multiple Case Study},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442411},
doi = {10.1145/3442391.3442411},
abstract = {Cyber-physical systems, i.e., systems that seamlessly integrate computation and physical components, are typically highly-configurable systems. Testing such systems is particularly challenging because they comprise a large number of heterogeneous components that can be configured and combined in different ways. Despite a plethora of work investigating software testing in general and software product line testing in particular, variability in tests and how industry does actually manage testing highly configurable cyber-physical systems is not well understood. In this paper, we report the results of a multiple case study we conducted with three companies developing and maintaining highly-configurable cyber-physical systems focusing on their testing practices, with a particular focus on how they manage variability in tests. We conclude that experienced-based selection of configurations for testing is currently predominant. Variability modeling techniques are not utilized and the dependencies between configuration options are only partially modeled at best. However, the companies are aware of the situation and have the need and desire to cover more configuration combinations by automated tests. This in turn raises many questions, which might also be of interest to the scientific community and motivate future research.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {19},
numpages = {10},
keywords = {variability testing, interview, industry case, configuration testing},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s10664-018-9670-1,
author = {Ali, Nauman Bin and Engstr\"{o}m, Emelie and Taromirad, Masoumeh and Mousavi, Mohammad Reza and Minhas, Nasir Mehmood and Helgesson, Daniel and Kunze, Sebastian and Varshosaz, Mahsa},
title = {On the search for industry-relevant regression testing research},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9670-1},
doi = {10.1007/s10664-018-9670-1},
abstract = {Regression testing is a means to assure that a change in the software, or its execution environment, does not introduce new defects. It involves the expensive undertaking of rerunning test cases. Several techniques have been proposed to reduce the number of test cases to execute in regression testing, however, there is no research on how to assess industrial relevance and applicability of such techniques. We conducted a systematic literature review with the following two goals: firstly, to enable researchers to design and present regression testing research with a focus on industrial relevance and applicability and secondly, to facilitate the industrial adoption of such research by addressing the attributes of concern from the practitioners' perspective. Using a reference-based search approach, we identified 1068 papers on regression testing. We then reduced the scope to only include papers with explicit discussions about relevance and applicability (i.e. mainly studies involving industrial stakeholders). Uniquely in this literature review, practitioners were consulted at several steps to increase the likelihood of achieving our aim of identifying factors important for relevance and applicability. We have summarised the results of these consultations and an analysis of the literature in three taxonomies, which capture aspects of industrial-relevance regarding the regression testing techniques. Based on these taxonomies, we mapped 38 papers reporting the evaluation of 26 regression testing techniques in industrial settings.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2020–2055},
numpages = {36},
keywords = {Taxonomy, Systematic literature review, Regression testing, Recommendations, Industrial relevance}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@article{10.1145/3231593,
author = {Ruotsalo, Tuukka and Peltonen, Jaakko and Eugster, Manuel J. A. and G\l{}owacka, Dorota and Flor\'{e}en, Patrik and Myllym\"{a}ki, Petri and Jacucci, Giulio and Kaski, Samuel},
title = {Interactive Intent Modeling for Exploratory Search},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231593},
doi = {10.1145/3231593},
abstract = {Exploratory search requires the system to assist the user in comprehending the information space and expressing evolving search intents for iterative exploration and retrieval of information. We introduce interactive intent modeling, a technique that models a user’s evolving search intents and visualizes them as keywords for interaction. The user can provide feedback on the keywords, from which the system learns and visualizes an improved intent estimate and retrieves information. We report experiments comparing variants of a system implementing interactive intent modeling to a control system. Data comprising search logs, interaction logs, essay answers, and questionnaires indicate significant improvements in task performance, information retrieval performance over the session, information comprehension performance, and user experience. The improvements in retrieval effectiveness can be attributed to the intent modeling and the effect on users’ task performance, breadth of information comprehension, and user experience are shown to be dependent on a richer visualization. Our results demonstrate the utility of combining interactive modeling of search intentions with interactive visualization of the models that can benefit both directing the exploratory search process and making sense of the information space. Our findings can help design personalized systems that support exploratory information seeking and discovery of novel information.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {44},
numpages = {46},
keywords = {user intent modeling, Proactive search}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/2541596.2541601,
author = {Rusitschka, Sebnem and Doblander, Christoph and Goebel, Christoph and Jacobsen, Hans-Arno},
title = {Adaptive middleware for real-time prescriptive analytics in large scale power systems},
year = {2013},
isbn = {9781450325509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541596.2541601},
doi = {10.1145/2541596.2541601},
abstract = {The increased digitalization of power systems poses both opportunities and challenges for system operators. GPS time-synchronized high-resolution data streams emanating from measurement devices distributed over a wide area enable the detection of disturbances and the real-time monitoring of consequences as they are evolving, such as undamped oscillations. Processing these data streams is not possible with state-of-the-art SCADA systems that poll data asynchronously at much lower time intervals. Moreover, real-time analysis on fresh streaming data at the enterprise level is an unresolved challenge. In this paper we propose an adaptive middleware concept that can make better use of available data processing resources by enabling distributed computation both on the enterprise and on the field level. We apply the concept of linked data to provide a map for moving the computation to the data it requires for analysis. If based on the IEC 61850 standard semantic data model, the linked data concept additionally yields location and domain awareness that can be leveraged for real-time prescriptive analytics in the field. Another advantage of the proposed adaptive middleware is the abstraction of computational resources: Analytical programs can be written once and then be used to process historical data residing on servers on the enterprise level as well on the distributed devices that originated the data to enable fast analysis of events as they are unfolding.},
booktitle = {Proceedings of the Industrial Track of the 13th ACM/IFIP/USENIX International Middleware Conference},
articleno = {5},
numpages = {6},
keywords = {power system automation, middleware, distributed computing, data analytics, IEC 61850},
location = {Beijing, China},
series = {Middleware Industry '13}
}

@inproceedings{10.1007/978-3-642-34091-8_5,
author = {Dragon, Ralf and Fenzi, Michele and Siberski, Wolf and Rosenhahn, Bodo and Ostermann, J\"{o}rn},
title = {Towards feature-based situation assessment for airport apron video surveillance},
year = {2011},
isbn = {9783642340901},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34091-8_5},
doi = {10.1007/978-3-642-34091-8_5},
abstract = {We present a feature-based surveillance pipeline which, in contrast to traditional image-based methods, allows to learn a detailed description of the observed background as well as of foreground objects. The pipeline consists of motion segmentation of feature trajectories and subsequent tracking-by-recognition with updates. Furthermore, 3D object representations are learned in order to extract the 3D object pose of a later object recognition. Finally, we show how such sufficiently reliable information is inputted into a reasoning system comparing actual and nominal condition of an airport apron. By this, automatic situation assessment becomes possible in a manageable and reliable way.},
booktitle = {Proceedings of the 15th International Conference on Theoretical Foundations of Computer Vision: Outdoor and Large-Scale Real-World Scene Analysis},
pages = {110–130},
numpages = {21},
location = {Dagstuhl Castle, Germany}
}

@article{10.1016/j.jss.2016.09.045,
author = {Parejo, Jos\'{e} A. and S\'{a}nchez, Ana B. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio and Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Multi-objective test case prioritization in highly configurable systems},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.045},
doi = {10.1016/j.jss.2016.09.045},
abstract = {A multi-objective test case prioritization real-world case study is presented.Seven objective functions based on functional and non-functional data are proposed.Comparison of the effectiveness of 63 combinations of up to three objectives.NSGA-II evolutionary algorithm to solve the multi-objective prioritization problem.Multi-objective prioritization is more effective than mono-objective approaches. Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non-functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
journal = {J. Syst. Softw.},
month = dec,
pages = {287–310},
numpages = {24},
keywords = {Variability, Test case prioritization, Highly-configurable systems, Automated software testing}
}

@book{10.5555/1543376,
author = {Jacob, Bruce and Ng, Spencer and Wang, David},
title = {Memory Systems: Cache, DRAM, Disk},
year = {2007},
isbn = {0123797519},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Is your memory hierarchy stopping your microprocessor from performing at the high level it should be? Memory Systems: Cache, DRAM, Disk shows you how to resolve this problem. The book tells you everything you need to know about the logical design and operation, physical design and operation, performance characteristics and resulting design trade-offs, and the energy consumption of modern memory hierarchies. You learn how to to tackle the challenging optimization problems that result from the side-effects that can appear at any point in the entire hierarchy.As a result you will be able to design and emulate the entire memory hierarchy. . Understand all levels of the system hierarchy -Xcache, DRAM, and disk. . Evaluate the system-level effects of all design choices. . Model performance and energy consumption for each component in the memory hierarchy.}
}

