@inproceedings{10.1007/978-3-030-90785-3_16,
author = {Wang, Baoping and Wang, Wennan and Zhu, Linkai and Liu, Wenjian},
title = {Research on Cross-Project Software Defect Prediction Based on Machine Learning},
year = {2021},
isbn = {978-3-030-90784-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90785-3_16},
doi = {10.1007/978-3-030-90785-3_16},
abstract = {In recent years, machine learning technology has developed vigorously. The research on software defect prediction in the field of software engineering is increasingly adopting various algorithms of machine learning. This article has carried out a systematic literature review on the field of defect prediction. First, this article studies the development process of defect prediction, from correlation to prediction model. then this article studies the development process of cross-project defect prediction based on machine learning algorithms (naive Bayes, decision tree, random forest, neural network, etc.). Finally, this paper looks forward to the research difficulties and future directions of software defect prediction, such as imbalance in classification, cost of data labeling, and cross-project data distribution.},
booktitle = {Advances in Web-Based Learning – ICWL 2021: 20th International Conference, ICWL 2021, Macau, China, November 13–14, 2021, Proceedings},
pages = {160–165},
numpages = {6},
keywords = {Machine learning, Software defect prediction model, Metric},
location = {Macau, China}
}

@inproceedings{10.1007/978-3-030-87007-2_27,
author = {Aladics, Tam\'{a}s and J\'{a}sz, Judit and Ferenc, Rudolf},
title = {Bug Prediction Using Source Code Embedding Based on Doc2Vec},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_27},
doi = {10.1007/978-3-030-87007-2_27},
abstract = {Bug prediction is a resource demanding task that is hard to automate using static source code analysis. In many fields of computer science, machine learning has proven to be extremely useful in tasks like this, however, for it to work we need a way to use source code as input. We propose a simple, but meaningful representation for source code based on its abstract syntax tree and the Doc2Vec embedding algorithm. This representation maps the source code to a fixed length vector which can be used for various upstream tasks – one of which is bug prediction. We measured this approach’s validity by itself and its effectiveness compared to bug prediction based solely on code metrics. We also experimented on numerous machine learning approaches to check the connection between different embedding parameters with different machine learning models. Our results show that this representation provides meaningful information as it improves the bug prediction accuracy in most cases, and is always at least as good as only using code metrics as features.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {382–397},
numpages = {16},
keywords = {Source code embedding, Code metrics, Bug prediction, Java, Doc2Vec},
location = {Cagliari, Italy}
}

@article{10.1016/j.jss.2021.111026,
author = {Zhu, Kun and Ying, Shi and Zhang, Nana and Zhu, Dandan},
title = {Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111026},
doi = {10.1016/j.jss.2021.111026},
journal = {J. Syst. Softw.},
month = oct,
numpages = {25},
keywords = {Software defect prediction, Metaheuristic feature selection, Whale optimization algorithm, Convolutional neural network, Kernel extreme learning machine}
}

@inproceedings{10.1145/3368926.3369711,
author = {Ha, Duy-An and Chen, Ting-Hsuan and Yuan, Shyan-Ming},
title = {Unsupervised methods for Software Defect Prediction},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369711},
doi = {10.1145/3368926.3369711},
abstract = {Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {49–55},
numpages = {7},
keywords = {Community Structure Detection, Machine Learning, Software Defect Prediction, Software Engineering, Unsupervised Learning},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3474124.3474127,
author = {Rajnish, Kumar and Bhattacharjee, Vandana and Chandrabanshi, Vishnu},
title = {Applying Cognitive and Neural Network Approach over Control Flow Graph for Software Defect Prediction},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474127},
doi = {10.1145/3474124.3474127},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {13–17},
numpages = {5},
keywords = {CFGs, Cognitive Complexity, Cognitive Measures, Graph Convolutional Network, Neural Network, Software Defect Prediction},
location = {Noida, India},
series = {IC3-2021}
}

@inproceedings{10.1145/3416508.3417114,
author = {Aljamaan, Hamoud and Alazba, Amal},
title = {Software defect prediction using tree-based ensembles},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417114},
doi = {10.1145/3416508.3417114},
abstract = {Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Bagging, Boosting, Classification, Ensemble Learning, Machine Learning, Prediction, Software Defect},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/3387940.3391463,
author = {Omri, Safa and Sinz, Carsten},
title = {Deep Learning for Software Defect Prediction: A Survey},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391463},
doi = {10.1145/3387940.3391463},
abstract = {Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {209–214},
numpages = {6},
keywords = {deep learning, machine learning, software defect prediction, software quality assurance, software testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1016/j.infsof.2021.106664,
author = {Yao, Jingxiu and Shepperd, Martin},
title = {The impact of using biased performance metrics on software defect prediction research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106664},
doi = {10.1016/j.infsof.2021.106664},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Software engineering, Machine learning, Software defect prediction, Computational experiment, Classification metrics}
}

@inproceedings{10.1007/978-3-030-29551-6_23,
author = {Miholca, Diana-Lucia and Czibula, Gabriela},
title = {Software Defect Prediction Using a Hybrid Model Based on Semantic Features Learned from the Source Code},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_23},
doi = {10.1007/978-3-030-29551-6_23},
abstract = {Software defect prediction has extensive applicability thus being a very active research area in Search-Based Software Engineering. A high proportion of the software defects are caused by violated couplings. In this paper, we investigate the relevance of semantic coupling in assessing the software proneness to defects. We propose a hybrid classification model combining Gradual Relational Association Rules with Artificial Neural Networks, which detects the defective software entities based on semantic features automatically learned from the source code. The experiments we have performed led to results that confirm the interplay between conceptual coupling and software defects proneness.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {262–274},
numpages = {13},
keywords = {Software defect prediction, Machine learning, Conceptual coupling},
location = {Athens, Greece}
}

@inproceedings{10.1109/ICSE43902.2021.00050,
author = {Shrikanth, N. C. and Majumder, Suvodeep and Menzies, Tim},
title = {Early Life Cycle Software Defect Prediction: Why? How?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00050},
doi = {10.1109/ICSE43902.2021.00050},
abstract = {Many researchers assume that, for software analytics, "more data is better." We write to show that, at least for learning defect predictors, this may not be true.To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models.We hope these results inspire other researchers to adopt a "simplicity-first" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for "short cuts" that can simplify the analysis.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {448–459},
numpages = {12},
keywords = {analytics, defect prediction, early, sampling},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s11063-020-10355-z,
author = {Niu, Liang and Wan, Jianwu and Wang, Hongyuan and Zhou, Kaiwei},
title = {Cost-sensitive Dictionary Learning for Software Defect Prediction},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10355-z},
doi = {10.1007/s11063-020-10355-z},
abstract = {In recent years, software defect prediction has been recognized as a cost-sensitive learning problem. To deal with the unequal misclassification losses resulted by different classification errors, some cost-sensitive dictionary learning methods have been proposed recently. Generally speaking, these methods usually define the misclassification costs to measure the unequal losses and then propose to minimize the cost-sensitive reconstruction loss by embedding the cost information into the reconstruction function of dictionary learning. Although promising performance has been achieved, their cost-sensitive reconstruction functions are not well-designed. In addition, no sufficient attentions are paid to the coding coefficients which can also be helpful to reduce the reconstruction loss. To address these issues, this paper proposes a new cost-sensitive reconstruction loss function and introduces an additional cost-sensitive discrimination regularization for the coding coefficients. Both the two terms are jointly optimized in a unified cost-sensitive dictionary learning framework. By doing so, we can achieve the minimum reconstruction loss and thus obtain a more cost-sensitive dictionary for feature encoding of test data. In the experimental part, we have conducted extensive experiments on twenty-five software projects from four benchmark datasets of NASA, AEEEM, ReLink and Jureczko. The results, in comparison with ten state-of-the-art software defect prediction methods, demonstrate the effectiveness of learned cost-sensitive dictionary for software defect prediction.},
journal = {Neural Process. Lett.},
month = dec,
pages = {2415–2449},
numpages = {35},
keywords = {Software defect prediction, Cost-sensitive, Dictionary learning, Discrimination}
}

@article{10.1155/2019/6230953,
author = {Fan, Guisheng and Diao, Xuyang and Yu, Huiqun and Yang, Kang and Chen, Liqiong and Vitiello, Autilia},
title = {Software Defect Prediction via Attention-Based Recurrent Neural Network},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1058-9244},
url = {https://doi.org/10.1155/2019/6230953},
doi = {10.1155/2019/6230953},
abstract = {In order to improve software reliability, software defect prediction is applied to the process of software maintenance to identify potential bugs. Traditional methods of software defect prediction mainly focus on designing static code metrics, which are input into machine learning classifiers to predict defect probabilities of the code. However, the characteristics of these artificial metrics do not contain the syntactic structures and semantic information of programs. Such information is more significant than manual metrics and can provide a more accurate predictive model. In this paper, we propose a framework called defect prediction via attention-based recurrent neural network (DP-ARNN). More specifically, DP-ARNN first parses abstract syntax trees (ASTs) of programs and extracts them as vectors. Then it encodes vectors which are used as inputs of DP-ARNN by dictionary mapping and word embedding. After that, it can automatically learn syntactic and semantic features. Furthermore, it employs the attention mechanism to further generate significant features for accurate defect prediction. To validate our method, we choose seven open-source Java projects in Apache, using F1-measure and area under the curve (AUC) as evaluation criteria. The experimental results show that, in average, DP-ARNN improves the F1-measure by 14% and AUC by 7% compared with the state-of-the-art methods, respectively.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@article{10.1016/j.neucom.2019.11.067,
author = {Qiao, Lei and Li, Xuesong and Umer, Qasim and Guo, Ping},
title = {Deep learning based software defect prediction},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {385},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.067},
doi = {10.1016/j.neucom.2019.11.067},
journal = {Neurocomput.},
month = apr,
pages = {100–110},
numpages = {11},
keywords = {Software defect prediction, Deep learning, Software quality, Software metrics, Robustness evaluation}
}

@article{10.1007/s11334-021-00399-2,
author = {Suresh Kumar, P. and Behera, H. S. and Nayak, Janmenjoy and Naik, Bighnaraj},
title = {Bootstrap aggregation ensemble learning-based reliable approach for software defect prediction by using characterized code feature},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-021-00399-2},
doi = {10.1007/s11334-021-00399-2},
abstract = {To ensure software quality, software defect prediction plays a prominent role for the software developers and practitioners. Software defect prediction can assist us with distinguishing software defect modules and enhance the software quality. In present days, many supervised machine learning algorithms have proved their efficacy to identify defective modules. However, those are limited to prove their major significance due to the limitations such as the adaptation of parameters with the environment and complexity. So, it is important to develop a key methodology to improve the efficiency of the prediction module. In this paper, an ensemble learning technique called&nbsp;Bootstrap&nbsp;aggregating has been proposed for software defect prediction object-oriented modules. The proposed method's accuracy, recall, precision, F-measure, and AUC-ROC efficiency were compared to those of many qualified machine learning algorithms. Simulation results and performance comparison are evident that the proposed method outperformed well compared to other approaches.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {355–379},
numpages = {25},
keywords = {Ensemble learning, Software defect prediction, Software reliability, Machine learning}
}

@inproceedings{10.1007/978-3-030-58817-5_45,
author = {Balogun, Abdullateef O. and Lafenwa-Balogun, Fatimah B. and Mojeed, Hammed A. and Adeyemo, Victor E. and Akande, Oluwatobi N. and Akintola, Abimbola G. and Bajeh, Amos O. and Usman-Hamza, Fatimah E.},
title = {SMOTE-Based Homogeneous Ensemble Methods for Software Defect Prediction},
year = {2020},
isbn = {978-3-030-58816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58817-5_45},
doi = {10.1007/978-3-030-58817-5_45},
abstract = {Class imbalance is a prevalent problem in machine learning which affects the prediction performance of classification algorithms. Software Defect Prediction (SDP) is no exception to this latent problem. Solutions such as data sampling and ensemble methods have been proposed to address the class imbalance problem in SDP. This study proposes a combination of Synthetic Minority Oversampling Technique (SMOTE) and homogeneous ensemble (Bagging and Boosting) methods for predicting software defects. The proposed approach was implemented using Decision Tree (DT) and Bayesian Network (BN) as base classifiers on defects datasets acquired from NASA software corpus. The experimental results showed that the proposed approach outperformed other experimental methods. High accuracy of 86.8% and area under operating receiver characteristics curve value of 0.93% achieved by the proposed technique affirmed its ability to differentiate between the defective and non-defective labels without bias.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part VI},
pages = {615–631},
numpages = {17},
keywords = {Software Defect Prediction, Class imbalance, Data sampling, Ensemble methods},
location = {Cagliari, Italy}
}

@article{10.1016/j.jss.2021.111038,
author = {Eken, Beyza and Tosun, Ayse},
title = {Investigating the performance of personalized models for software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111038},
doi = {10.1016/j.jss.2021.111038},
journal = {J. Syst. Softw.},
month = nov,
numpages = {17},
keywords = {Personalized, Change-level, Defect prediction, Software recommendation systems}
}

@inproceedings{10.1007/978-3-030-86472-9_28,
author = {Shakhovska, Natalya and Yakovyna, Vitaliy},
title = {Feature Selection and Software Defect Prediction by Different Ensemble Classifiers},
year = {2021},
isbn = {978-3-030-86471-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86472-9_28},
doi = {10.1007/978-3-030-86472-9_28},
abstract = {Software defect prediction can improve its quality and is actively studied during the last decade. This paper focuses on the improvement of software defect prediction accuracy by proper feature selection techniques and using ensemble classifier. The software code metrics were used to predict the defective modules. JM1 public NASA dataset from PROMISE Software Engineering Repository was used in this study. Boruta, ACE, regsubsets and simple correlation are used for feature selection. The results of selection are formed based on hard voting of all features selectors. A new stacking classifier for software defects prediction is presented in this paper. The stacking classifier for defects prediction algorithm is based on combination of 5 weak classifiers. Random forest algorithm is used to combine the predictions. The obtained prediction accuracy was up to 96.26%.},
booktitle = {Database and Expert Systems Applications: 32nd International Conference, DEXA 2021, Virtual Event, September 27–30, 2021, Proceedings, Part I},
pages = {307–313},
numpages = {7},
keywords = {Ensemble of classifiers, Feature selection, Software defect analysis}
}

@article{10.1049/iet-sen.2019.0149,
author = {Deng, Jiehan and Lu, Lu and Qiu, Shaojian},
title = {Software defect prediction via LSTM},
year = {2020},
issue_date = {August 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {14},
number = {4},
url = {https://doi.org/10.1049/iet-sen.2019.0149},
doi = {10.1049/iet-sen.2019.0149},
abstract = {Software quality plays an important role in the software lifecycle. Traditional software defect prediction approaches mainly focused on using hand‐crafted features to detect defects. However, like human languages, programming languages contain rich semantic and structural information, and the cause of defective code is closely related to its context. Failing to catch this significant information, the performance of traditional approaches is far from satisfactory. In this study, the authors leveraged a long short‐term memory (LSTM) network to automatically learn the semantic and contextual features from the source code. Specifically, they first extract the program's Abstract Syntax Trees (ASTs), which is made up of AST nodes, and then evaluate what and how much information they can preserve for several node types. They traverse the AST of each file and fed them into the LSTM network to automatically the semantic and contextual features of the program, which is then used to determine whether the file is defective. Experimental results on several opensource projects showed that the proposed LSTM method is superior to the state‐of‐the‐art methods.},
journal = {IET Software},
month = aug,
pages = {443–450},
numpages = {8},
keywords = {feature extraction, learning (artificial intelligence), public domain software, program diagnostics, program debugging, software quality, recurrent neural nets, trees (mathematics), program abstract syntax trees, AST node sequence, semantic features, contextual features, LSTM, software quality, software lifecycle, software defect prediction approaches, machine learning techniques, programming languages, human languages, structural information, defective code, long short-term memory network, open source projects, numerical vectors, word embedding techniques}
}

@article{10.1016/j.eswa.2021.114637,
author = {Jin, Cong},
title = {Cross-project software defect prediction based on domain adaptation learning and optimization},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114637},
doi = {10.1016/j.eswa.2021.114637},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {14},
keywords = {Software defect prediction, Optimization, Domain adaptation, Cross-project defect prediction, Improved quantum particle swarm optimization}
}

@phdthesis{10.5555/AAI28389525,
author = {Rahman, Ashiqur},
advisor = {R, Cordy, James},
title = {Software Defect Prediction Using Rich Contextualized Language Use Vectors},
year = {2020},
isbn = {9798708779250},
publisher = {Queen's University (Canada)},
abstract = {Context. Software defect prediction aims to find defect prone source code, and thus reduce the effort, time and cost involved with ensuring the quality of software systems. Both code and non-code metrics are commonly used in this process to train machine learning algorithms to predict software defects. Studies have shown that such metrics-based approaches are failing to give satisfactory results, and have reached a performance ceiling. This thesis explores the idea of using code profiles as an alternative to traditional metrics to predict software defects. This code profile-based method proves to be more promising than traditional metrics-based approaches.Aims. This thesis aims to improve software defect prediction using code profiles as feature variables in place of traditional metrics. Software code profiles encode the density of language feature use and the context of such use in Rich Contextualized Language Use Vectors (RCLUVs) by analysing the parse tree of the source code. This thesis explores whether code profiles can be used to train machine learning algorithms, and compares the performance of the derived models to traditional metrics-based approaches.Methods. To achieve these aims the learning curves of several machine learning algorithms are analyzed, and the performance of the derived models are evaluated against traditional metrics-based approaches. Two benchmark bug datasets, the Eclipse bug dataset and the Github bug database, are used to train the models.Results. The learning curves of the models show machine learning algorithms can learn from RCLUV-based code profiles. Performance evaluation against existing metrics-based approaches reveals that the code profile-based approach is more promising than traditional metrics-based approaches. However, the predictive performance of both metrics and code profile-based approaches drops in cross-version predictions.Conclusions. Unlike traditional metrics-based approaches, this thesis uses vectors generated by analyzing language feature use from the parse trees of source code as feature variables to train machine learning algorithms. Experimental results using learning algorithms encourages us to use software code profiles as an alternative to traditional metrics to predict software defects.},
note = {AAI28389525}
}

@article{10.1007/s11219-016-9353-3,
author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
title = {Software defect prediction: do different classifiers find the same defects?},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9353-3},
doi = {10.1007/s11219-016-9353-3},
abstract = {During the last 10 years, hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall. We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers. We perform a sensitivity analysis to compare the performance of Random Forest, Na\"{\i}ve Bayes, RPart and SVM classifiers when predicting defects in NASA, open source and commercial datasets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty of each classifier is compared. Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others. Our results confirm that a unique subset of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Given our results, we conclude that classifier ensembles with decision-making strategies not based on majority voting are likely to perform best in defect prediction.},
journal = {Software Quality Journal},
month = jun,
pages = {525–552},
numpages = {28},
keywords = {Machine learning, Prediction modelling, Software defect prediction}
}

@article{10.1016/j.neucom.2019.05.100,
author = {Huo, Xuan and Li, Ming},
title = {On cost-effective software defect prediction: Classification or ranking?},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {363},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.100},
doi = {10.1016/j.neucom.2019.05.100},
journal = {Neurocomput.},
month = oct,
pages = {339–350},
numpages = {12},
keywords = {Software mining, Software defect prediction, Ranking model, Classification model}
}

@inproceedings{10.1007/978-3-030-58802-1_25,
author = {Ronchieri, Elisabetta and Canaparo, Marco and Belgiovine, Mauro},
title = {Software Defect Prediction on Unlabelled Datasets: A Comparative Study},
year = {2020},
isbn = {978-3-030-58801-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58802-1_25},
doi = {10.1007/978-3-030-58802-1_25},
abstract = {Background: Defect prediction on unlabelled datasets is a challenging and widespread problem in software engineering. Machine learning is of great value in this context because it provides techniques - called unsupervised - that are applicable to unlabelled datasets. Objective: This study aims at comparing various approaches employed over the years on unlabelled datasets to predict the defective modules, i.e. the ones which need more attention in the testing phase. Our comparison is based on the measurement of performance metrics and on the real defective information derived from software archives. Our work leverages a new dataset that has been obtained by extracting and preprocessing its metrics from a C++ software. Method: Our empirical study has taken advantage of CLAMI with its improvement CLAMI+ that we have applied on high energy physics software datasets. Furthermore, we have used clustering techniques such as the K-means algorithm to find potentially critical modules. Results: Our experimental analysis have been carried out on 1 open source project with 34 software releases. We have applied 17 ML techniques to the labelled datasets obtained by following the CLAMI and CLAMI+ approaches. The two approaches have been evaluated by using different performance metrics, our results show that CLAMI+ performs better than CLAMI. The predictive average accuracy metric is around 95% for 4 ML techniques (4 out of 17) that show a Kappa statistic greater than 0.80. We applied K-means on the same dataset and obtained 2 clusters labelled according to the output of CLAMI and CLAMI+. Conclusion: Based on the results of the different statistical tests, we conclude that no significant performance differences have been found in the selected classification techniques.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part II},
pages = {333–353},
numpages = {21},
keywords = {Unlabelled dataset, Defect prediction, Unsupervised methods, Machine learning},
location = {Cagliari, Italy}
}

@article{10.1016/j.jss.2018.06.025,
author = {\"{O}zak\i{}nc\i{}, Rana and Tarhan, Ay\c{c}a},
title = {Early software defect prediction: A systematic map and review},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.06.025},
doi = {10.1016/j.jss.2018.06.025},
journal = {J. Syst. Softw.},
month = oct,
pages = {216–239},
numpages = {24},
keywords = {Early defect prediction, Software defect, Software quality, Prediction model, Systematic mapping, Systematic literature review}
}

@article{10.1007/s10515-016-0194-x,
author = {Zhang, Zhi-Wu and Jing, Xiao-Yuan and Wang, Tie-Jian},
title = {Label propagation based semi-supervised learning for software defect prediction},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-016-0194-x},
doi = {10.1007/s10515-016-0194-x},
abstract = {Software defect prediction can automatically predict defect-prone software modules for efficient software test in software engineering. When the previous defect labels of modules are limited, predicting the defect-prone modules becomes a challenging problem. In static software defect prediction, there exist the similarity among software modules, a software module can be approximated by a sparse representation of the other part of the software modules, and class-imbalance problem, the number of defect-free modules is much larger than that of defective ones. In this paper, we propose to use graph based semi-supervised learning technique to predict software defect. By using Laplacian score sampling strategy for the labeled defect-free modules, we construct a class-balance labeled training dataset firstly. And then, we use a nonnegative sparse algorithm to compute the nonnegative sparse weights of a relationship graph which serve as clustering indicators. Lastly, on the nonnegative sparse graph, we use a label propagation algorithm to iteratively predict the labels of unlabeled software modules. We thus propose a nonnegative sparse graph based label propagation approach for software defect classification and prediction, which uses not only few labeled data but also abundant unlabeled ones to improve the generalization capability. We vary the size of labeled software modules from 10 to 30 % of all the datasets in the widely used NASA projects. Experimental results show that the NSGLP outperforms several representative state-of-the-art semi-supervised software defect prediction methods, and it can fully exploit the characteristics of static code metrics and improve the generalization capability of the software defect prediction model.},
journal = {Automated Software Engg.},
month = mar,
pages = {47–69},
numpages = {23},
keywords = {Label propagation, Nonnegative sparse graph, Nonnegative sparse graph based label propagation (NSGLP), Semi-supervised learning, Software defect prediction}
}

@article{10.1016/j.jss.2017.03.044,
author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
title = {Source code metrics},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.044},
doi = {10.1016/j.jss.2017.03.044},
abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
journal = {J. Syst. Softw.},
month = jun,
pages = {164–197},
numpages = {34},
keywords = {Aspect-oriented metrics, Feature-oriented metrics, Object-oriented metrics, Software metrics, Source code metrics, Systematic mapping study}
}

@article{10.1016/j.asoc.2015.04.045,
author = {Arar, \"{O}mer Faruk and Ayan, K\"{u}r\c{s}at},
title = {Software defect prediction using cost-sensitive neural network},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {33},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.04.045},
doi = {10.1016/j.asoc.2015.04.045},
abstract = {Software defect prediction model was built by Artificial Neural Network (ANN).ANN connection weights were optimized by Artificial Bee Colony (ABC).Parametric cost-sensitivity feature was added to ANN by using a new error function.Model was applied to five publicly available datasets from the NASA repository.Results were compared with other cost-sensitive and non-cost-sensitive studies. The software development life cycle generally includes analysis, design, implementation, test and release phases. The testing phase should be operated effectively in order to release bug-free software to end users. In the last two decades, academicians have taken an increasing interest in the software defect prediction problem, several machine learning techniques have been applied for more robust prediction. A different classification approach for this problem is proposed in this paper. A combination of traditional Artificial Neural Network (ANN) and the novel Artificial Bee Colony (ABC) algorithm are used in this study. Training the neural network is performed by ABC algorithm in order to find optimal weights. The False Positive Rate (FPR) and False Negative Rate (FNR) multiplied by parametric cost coefficients are the optimization task of the ABC algorithm. Software defect data in nature have a class imbalance because of the skewed distribution of defective and non-defective modules, so that conventional error functions of the neural network produce unbalanced FPR and FNR results. The proposed approach was applied to five publicly available datasets from the NASA Metrics Data Program repository. Accuracy, probability of detection, probability of false alarm, balance, Area Under Curve (AUC), and Normalized Expected Cost of Misclassification (NECM) are the main performance indicators of our classification approach. In order to prevent random results, the dataset was shuffled and the algorithm was executed 10 times with the use of n-fold cross-validation in each iteration. Our experimental results showed that a cost-sensitive neural network can be created successfully by using the ABC optimization algorithm for the purpose of software defect prediction.},
journal = {Appl. Soft Comput.},
month = aug,
pages = {263–277},
numpages = {15},
keywords = {Artificial Bee Colony, Artificial Neural Network, Cost-sensitive classification, Machine learning, Software defect prediction, Software quality}
}

@article{10.1016/j.infsof.2021.106662,
author = {Feng, Shuo and Keung, Jacky and Yu, Xiao and Xiao, Yan and Zhang, Miao},
title = {Investigation on the stability of SMOTE-based oversampling techniques in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106662},
doi = {10.1016/j.infsof.2021.106662},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Software defect prediction, Class imbalance, Oversampling, SMOTE, Empirical Software Engineering}
}

@article{10.1007/s10515-021-00289-8,
author = {Ali, Aftab and Khan, Naveed and Abu-Tair, Mamun and Noppen, Joost and McClean, Sally and McChesney, Ian},
title = {Discriminating features-based cost-sensitive approach for software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00289-8},
doi = {10.1007/s10515-021-00289-8},
abstract = {Correlated quality metrics extracted from a source code repository can be utilized to design a model to automatically predict defects in a software system. It is obvious that the extracted metrics will result in a highly unbalanced data, since the number of defects in a good quality software system should be far less than the number of normal instances. It is also a fact that the selection of the best discriminating features significantly improves the robustness and accuracy of a prediction model. Therefore, the contribution of this paper is twofold, first it selects the best discriminating features that help in accurately predicting a defect in a software component. Secondly, a cost-sensitive logistic regression and decision tree ensemble-based prediction models are applied to the best discriminating features for precisely predicting a defect in a software component. The proposed models are compared with the most recent schemes in the literature in terms of accuracy, area under the curve, and recall. The models are evaluated using 11 datasets and it is evident from the results and analysis that the performance of the proposed prediction models outperforms the schemes in the literature.},
journal = {Automated Software Engg.},
month = nov,
numpages = {18},
keywords = {Software bugs/defects, Machine learning models, Discriminating features, Cost-sensitivity, AUC, Recall}
}

@article{10.1016/j.neucom.2021.05.043,
author = {Harzevili, Nima Shiri and Alizadeh, Sasan H.},
title = {Analysis and modeling conditional mutual dependency of metrics in software defect prediction using latent variables},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {460},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.05.043},
doi = {10.1016/j.neucom.2021.05.043},
journal = {Neurocomput.},
month = oct,
pages = {309–330},
numpages = {22},
keywords = {Software defect prediction Software metrics, Naive Bayes classifier, Latent variable, 00–01, 99–00}
}

@inbook{10.5555/1985688.1985695,
author = {Pu\l{}awski, \L{}ukasz},
title = {Software defect prediction based on source code metrics time series},
year = {2011},
isbn = {9783642183010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Source code metrics have been proved to be reliable indicators of the vulnerability of the source code to defects. Typically, a source code unit with high value of a certain metric is considered to be badly structured and thus error-prone. However, analysis of source code change history shows that there are cases when source files with low values of metrics still turn out to be defective. Instead of introducing new metrics for such cases, I investigate the possibility of estimating the vulnerability of source code units to defects on the basis of the history of the values of selected well-known metrics. The experiments show that we can efficiently identify bad source code units just by looking at the history of metrics, coming from only a few revisions that precede the actual resolution of the defect.},
booktitle = {Transactions on Rough Sets XIII},
pages = {104–120},
numpages = {17}
}

@article{10.1007/s10664-020-09861-4,
author = {Morasca, Sandro and Lavazza, Luigi},
title = {On the assessment of software defect prediction models via ROC curves},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09861-4},
doi = {10.1007/s10664-020-09861-4},
abstract = {Software defect prediction models are classifiers often built by setting a threshold t on a defect proneness model, i.e., a scoring function. For instance, they classify a software module non-faulty if its defect proneness is below t and positive otherwise. Different values of t may lead to different defect prediction models, possibly with very different performance levels. Receiver Operating Characteristic (ROC) curves provide an overall assessment of a defect proneness model, by taking into account all possible values of t and thus all defect prediction models that can be built based on it. However, using a defect proneness model with a value of t is sensible only if the resulting defect prediction model has a performance that is at least as good as some minimal performance level that depends on practitioners’ and researchers’ goals and needs. We introduce a new approach and a new performance metric (the Ratio of Relevant Areas) for assessing a defect proneness model by taking into account only the parts of a ROC curve corresponding to values of t for which defect proneness models have higher performance than some reference value. We provide the practical motivations and theoretical underpinnings for our approach, by: 1) showing how it addresses the shortcomings of existing performance metrics like the Area Under the Curve and Gini’s coefficient; 2) deriving reference values based on random defect prediction policies, in addition to deterministic ones; 3) showing how the approach works with several performance metrics (e.g., Precision and Recall) and their combinations; 4) studying misclassification costs and providing a general upper bound for the cost related to the use of any defect proneness model; 5) showing the relationships between misclassification costs and performance metrics. We also carried out a comprehensive empirical study on real-life data from the SEACRAFT repository, to show the differences between our metric and the existing ones and how more reliable and less misleading our metric can be.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3977–4019},
numpages = {43},
keywords = {Software defect prediction model, Software defect proneness, ROC, Thresholds, AUC, Gini}
}

@inproceedings{10.1109/ASE.2019.00071,
author = {Gong, Lina and Jiang, Shujuan and Wang, Rongcun and Jiang, Li},
title = {Empirical evaluation of the impact of class overlap on software defect prediction},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00071},
doi = {10.1109/ASE.2019.00071},
abstract = {Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {698–709},
numpages = {12},
keywords = {K-Means clustering, class overlap, machine learning, software defect prediction},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1007/978-3-031-08421-8_41,
author = {Giorgio, Lazzarinetti and Nicola, Massarenti and Fabio, Sgr\`{o} and Andrea, Salafia},
title = {Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_41},
doi = {10.1007/978-3-031-08421-8_41},
abstract = {Recent advances in information technology has led to an increasing number of applications to be developed and maintained daily by product teams. Ensuring that a software application works as expected and that it is absent of bugs requires a lot of time and resources. Thanks to the recent adoption of DevOps methodologies, it is often the case where code commits and application builds are centralized and standardized. Thanks to this new approach, it is now possible to retrieve log and build data to ease the development and management operations of product teams. However, even if such approaches include code control to detect unit or integration errors, they do not check for the presence of logical bugs that can raise after code builds. For such reasons in this work we propose a framework for continuous defect prediction based on machine learning algorithms trained on a publicly available dataset. The framework is composed of a machine learning model for detecting the presence of logical bugs in code on the basis of the available data generated by DevOps tools and a dashboard to monitor the software projects status. We also describe the serverless architecture we designed for hosting the aforementioned framework.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {591–606},
numpages = {16},
keywords = {Continuous defect prediction, Machine learning, DevOps, Continuous integration}
}

@inproceedings{10.1145/3377811.3380389,
author = {Chen, Jinyin and Hu, Keke and Yu, Yue and Chen, Zhuangzhi and Xuan, Qi and Liu, Yi and Filkov, Vladimir},
title = {Software visualization and deep transfer learning for effective software defect prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380389},
doi = {10.1145/3377811.3380389},
abstract = {Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learning-based classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be custom-tailored to effectively build most accurate models.To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {578–589},
numpages = {12},
keywords = {cross-project defect prediction, deep transfer learning, self-attention, software visualization, within-project defect prediction},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1504/ijcat.2020.110428,
author = {Bai, Xue and Zhou, Hua and Yang, Hongji and Wang, Dong},
title = {Connecting historical changes for cross-version software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {63},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.110428},
doi = {10.1504/ijcat.2020.110428},
abstract = {In the whole software life cycle, software defects are inevitable and increase the cost of software development and evolution. Cross-Version Software Defect Prediction (CVSDP) aims at learning the defect patterns from the historical data of previous software versions to distinguish buggy software modules from clean ones. In CVSDP, metrics are intrinsic properties associated with the external manifestation of defects. However, traditional software defect measures ignore the sequential information of changes during software evolution process which may play a crucial role in CVSDP. Therefore, researchers tried to connect traditional metrics across versions as a new kind of evolution metrics. This study proposes a new way to connect historical sequence of metrics based on change sequence named HCSM and designs a novel deep learning algorithm GDNN as a classifier to process it. Compared to the traditional metrics approaches and other relevant approaches, the proposed approach fits in projects with stable and orderly defect control trend.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {371–383},
numpages = {12},
keywords = {software testing, cross-version defect prediction, software metrics, historical change sequences, deep learning, DNN, deep neural networks, gate recurrent unit}
}

@article{10.1007/s00500-021-06096-3,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {An empirical study toward dealing with noise and class imbalance issues in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06096-3},
doi = {10.1007/s00500-021-06096-3},
abstract = {The quality of the defect datasets is a critical issue in the domain of software defect prediction (SDP). These datasets are obtained through the mining of software repositories. Recent studies claim over the quality of the defect dataset. It is because of inconsistency between bug/clean fix keyword in fault reports and the corresponding link in the change management logs. Class Imbalance (CI) problem is also a big challenging issue in SDP models. The defect prediction method trained using noisy and imbalanced data leads to inconsistent and unsatisfactory results. Combined analysis over noisy instances and CI problem needs to be required. To the best of our knowledge, there are insufficient studies that have been done over such aspects. In this paper, we deal with the impact of noise and CI problem on five baseline SDP models; we manually added the various noise level (0–80%) and identified its impact on the performance of those SDP models. Moreover, we further provide guidelines for the possible range of tolerable noise for baseline models. We have also suggested the SDP model, which has the highest noise tolerable ability and outperforms over other classical methods. The True Positive Rate (TPR) and False Positive Rate (FPR) values of the baseline models reduce between 20–30% after adding 10–40% noisy instances. Similarly, the ROC (Receiver Operating Characteristics) values of SDP models reduce to 40–50%. The suggested model leads to avoid noise between 40–60% as compared to other traditional models.},
journal = {Soft Comput.},
month = nov,
pages = {13465–13492},
numpages = {28},
keywords = {Software testing, Software fault prediction, Class imbalance, Noisy instance, Machine learning, Software metrics, Fault proneness}
}

@article{10.1016/j.infsof.2018.02.003,
author = {Mahmood, Zaheed and Bowes, David and Hall, Tracy and Lane, Peter C.R. and Petri\'{c}, Jean},
title = {Reproducibility and replicability of software defect prediction studies},
year = {2018},
issue_date = {Jul 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {99},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.02.003},
doi = {10.1016/j.infsof.2018.02.003},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {148–163},
numpages = {16},
keywords = {Replication, Reproducibility, Software defect prediction}
}

@inproceedings{10.1145/3374549.3374553,
author = {Zong, Liang},
title = {Classification Based Software Defect Prediction Model for Finance Software System - An Industry Study},
year = {2020},
isbn = {9781450376495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374549.3374553},
doi = {10.1145/3374549.3374553},
abstract = {Automated software defect prediction is an important and fundamental activity in the domain of software development. Successful software defect prediction can save testing effort thus reduce the time and cost for software development. However, software systems for finance company are inherently large and complex with numerous interfaces with other systems. Thus, identifying and selecting a good model and a set of features is important but challenging problem. In our paper, we first define the problem we want to solve. Then we propose a prediction model based on binary classification and a set of novel features, which is more specific for finance software systems. We collected 15 months real production data and labelled it as our dataset. The experiment shows our model and features can give a better prediction accuracy for finance systems. In addition, we demonstrate how our prediction model helps improve our production quality further. Unlike other research papers, our proposal focuses to solve problem in real finance industry.},
booktitle = {Proceedings of the 2019 3rd International Conference on Software and E-Business},
pages = {60–65},
numpages = {6},
keywords = {Faulty change, Finance system, Machine learning, Software defect prediction},
location = {Tokyo, Japan},
series = {ICSEB '19}
}

@inproceedings{10.1145/3475716.3475790,
author = {Wang, Song and Wang, Junjie and Nam, Jaechang and Nagappan, Nachiappan},
title = {Continuous Software Bug Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475790},
doi = {10.1145/3475716.3475790},
abstract = {Background: Many software bug prediction models have been proposed and evaluated on a set of well-known benchmark datasets. We conducted pilot studies on the widely used benchmark datasets and observed common issues among them. Specifically, most of existing benchmark datasets consist of randomly selected historical versions of software projects, which poses non-trivial threats to the validity of existing bug prediction studies since the real-world software projects often evolve continuously. Yet how to conduct software bug prediction in the real-world continuous software development scenarios is not well studied.Aims: In this paper, to bridge the gap between current software bug prediction practice and real-world continuous software development, we propose new approaches to conduct bug prediction in real-world continuous software development regarding model building, updating, and evaluation.Method: For model building, we propose ConBuild, which leverages distributional characteristics of bug prediction data to guide the training version selection. For model updating, we propose ConUpdate, which leverages the evolution of distributional characteristics of bug prediction data between versions to guide the reuse or update of bug prediction models in continuous software development. For model evaluation, we propose ConEA, which leverages the evolution of buggy probability of files between versions to conduct effort-aware evaluation.Results: Experiments on 120 continuously release versions that span across six large-scale open-source software systems show the practical value of our approaches.Conclusions: This paper provides new insights and guidelines for conducting software bug prediction in the context of continuous software development.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {14},
numpages = {12},
keywords = {Empirical software engineering, continuous software development, software defect prediction, software quality},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/3238147.3240469,
author = {Qu, Yu and Liu, Ting and Chi, Jianlei and Jin, Yangxu and Cui, Di and He, Ancheng and Zheng, Qinghua},
title = {node2defect: using network embedding to improve software defect prediction},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240469},
doi = {10.1145/3238147.3240469},
abstract = {Network measures have been proved to be useful in predicting software defects. Leveraging the dependency relationships between software modules, network measures can capture various structural features of software systems. However, existing studies have relied on user-defined network measures (e.g., degree statistics or centrality metrics), which are inflexible and require high computation cost, to describe the structural features. In this paper, we propose a new method called node2defect which uses a newly proposed network embedding technique, node2vec, to automatically learn to encode dependency network structure into low-dimensional vector spaces to improve software defect prediction. Specifically, we firstly construct a program's Class Dependency Network. Then node2vec is used to automatically learn structural features of the network. After that, we combine the learned features with traditional software engineering features, for accurate defect prediction. We evaluate our method on 15 open source programs. The experimental results show that in average, node2defect improves the state-of-the-art approach by 9.15% in terms of F-measure.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {844–849},
numpages = {6},
keywords = {Software defect, defect prediction, network embedding, software metrics},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1504/ijcse.2020.106871,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {A benchmarking framework using nonlinear manifold detection techniques for software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {21},
number = {4},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2020.106871},
doi = {10.1504/ijcse.2020.106871},
abstract = {Prediction of software defects in time improves quality and helps in locating the defect-prone areas accurately. Although earlier considerable methods were applied, actually none of those measures was found to be fool-proof and accurate. Hence, a newer framework includes nonlinear manifold detection model, and its algorithm originated for defect prediction using different techniques of nonlinear manifold detection (nonlinear MDs) along with 14 different machine learning techniques (MLTs) on eight defective software datasets. A critical analysis cum exhaustive comparative estimation revealed that nonlinear manifold detection model has a more accurate and effective impact on defect prediction as compared to feature selection techniques. The outcome of the experiment was statistically tested by Friedman and post hoc analysis using Nemenyi test, which validates that hidden Markov model (HMM) along with nonlinear manifold detection model outperforms and is significantly different from MLTs.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {593–614},
numpages = {21},
keywords = {dimensionality reduction, feature selection, Friedman test, machine learning, Nemenyi test, nonlinear manifold detection, software defect prediction, post hoc analysis}
}

@inproceedings{10.1007/978-3-030-87007-2_18,
author = {G\'{a}l, P\'{e}ter},
title = {Bug Prediction Capability of Primitive Enthusiasm Metrics},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_18},
doi = {10.1007/978-3-030-87007-2_18},
abstract = {Bugs in software development life cycle are unavoidable. Manually finding these bugs is not always the most effective way. To aid this, various bug prediction approaches which are using code metrics are developed and are also still in active development.In a previous work, the Primitive Enthusiasm code metrics were introduced to add detection capabilities for Primitive Obsession code smells. This paper explores the usability of the Primitive Enthusiasm metrics in a bug prediction scenario. To evaluate the new metrics, an existing source code bug data set was used. The correlation between existing metrics and the new PE metrics was investigated. Furthermore the effectiveness of bug prediction is investigated by building training models with and without the new metrics. Using a cross-project and a project version-based evaluation the results show that adding PE metrics can be beneficial for bug prediction.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {246–262},
numpages = {17},
keywords = {Bug prediction, Code metrics, Primitive obsession, Primitive enthusiasm, Static analysis},
location = {Cagliari, Italy}
}

@article{10.1016/j.neucom.2019.03.076,
author = {Zhao, Linchang and Shang, Zhaowei and Zhao, Ling and Zhang, Taiping and Tang, Yuan Yan},
title = {Software defect prediction via cost-sensitive Siamese parallel fully-connected neural networks},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {352},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.076},
doi = {10.1016/j.neucom.2019.03.076},
journal = {Neurocomput.},
month = aug,
pages = {64–74},
numpages = {11},
keywords = {Siamese parallel fully-connected networks, Cost-sensitive learning, Deep learning, Few-shot learning, Software defect prediction}
}

@article{10.1007/s11219-016-9342-6,
author = {Chen, Lin and Fang, Bin and Shang, Zhaowei and Tang, Yuanyan},
title = {Tackling class overlap and imbalance problems in software defect prediction},
year = {2018},
issue_date = {March     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9342-6},
doi = {10.1007/s11219-016-9342-6},
abstract = {Software defect prediction (SDP) is a promising solution to save time and cost in the software testing phase for improving software quality. Numerous machine learning approaches have proven effective in SDP. However, the unbalanced class distribution in SDP datasets could be a problem for some conventional learning methods. In addition, class overlap increases the difficulty for the predictors to learn the defective class accurately. In this study, we propose a new SDP model which combines class overlap reduction and ensemble imbalance learning to improve defect prediction. First, the neighbor cleaning method is applied to remove the overlapping non-defective samples. The whole dataset is then randomly under-sampled several times to generate balanced subsets so that multiple classifiers can be trained on these data. Finally, these individual classifiers are assembled with the AdaBoost mechanism to build the final prediction model. In the experiments, we investigated nine highly unbalanced datasets selected from a public software repository and confirmed that the high rate of overlap between classes existed in SDP data. We assessed the performance of our proposed model by comparing it with other state-of-the-art methods including conventional SDP models, imbalance learning and data cleaning methods. Test results and statistical analysis show that the proposed model provides more reasonable defect prediction results and performs best in terms of G-mean and AUC among all tested models.},
journal = {Software Quality Journal},
month = mar,
pages = {97–125},
numpages = {29},
keywords = {Class imbalance, Class overlap, Machine learning, Software defect prediction}
}

@article{10.1007/s10515-020-00277-4,
author = {Esteves, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Understanding machine learning software defect predictions},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00277-4},
doi = {10.1007/s10515-020-00277-4},
abstract = {Software defects are well-known in software development and might cause several problems for users and developers aside. As a result, researches employed distinct techniques to mitigate the impacts of these defects in the source code. One of the most notable techniques focuses on defect prediction using machine learning methods, which could support developers in handling these defects before they are introduced in the production environment. These studies provide alternative approaches to predict the likelihood of defects. However, most of these works concentrate on predicting defects from a vast set of software features. Another key issue with the current literature is the lack of a satisfactory explanation of the reasons that drive the software to a defective state. Specifically, we use a tree boosting algorithm (XGBoost) that receives as input a training set comprising records of easy-to-compute characteristics of each module and outputs whether the corresponding module is defect-prone. To exploit the link between predictive power and model explainability, we propose a simple model sampling approach that finds accurate models with the minimum set of features. Our principal idea is that features not contributing to increasing the predictive power should not be included in the model. Interestingly, the reduced set of features helps to increase model explainability, which is important to provide information to developers on features related to each module of the code which is more defect-prone. We evaluate our models on diverse projects within Jureczko datasets, and we show that (i) features that contribute most for finding best models may vary depending on the project and (ii) it is possible to find effective models that use few features leading to better understandability. We believe our results are useful to developers as we provide the specific software features that influence the defectiveness of selected projects.},
journal = {Automated Software Engg.},
month = dec,
pages = {369–392},
numpages = {24},
keywords = {Software defects, Explainable models, Jureczko datasets, SHAP values}
}

@article{10.5555/2684939.2684969,
author = {Ma, Ying and Pan, Weiwei and Zhu, Shunzhi and Yin, Huayi and Luo, Jian},
title = {An improved semi-supervised learning method for software defect prediction},
year = {2014},
issue_date = {September 2014},
publisher = {IOS Press},
address = {NLD},
volume = {27},
number = {5},
issn = {1064-1246},
abstract = {This paper presents an improved semi-supervised learning approach for defect prediction involving class imbalanced and limited labeled data problem. This approach employs random under-sampling technique to resample the original training set and updating training set in each round for co-train style algorithm. It makes the defect predictor more practical for real applications, by combating these problems. In comparison with conventional machine learning approaches, our method has significant superior performance. Experimental results also show that with the proposed learning approach, it is possible to design better method to tackle the class imbalanced problem in semi-supervised learning.},
journal = {J. Intell. Fuzzy Syst.},
month = sep,
pages = {2473–2480},
numpages = {8},
keywords = {Class Imbalance, Co-Train, Defect Prediction, Random Sampling, Semi-Supervised Learning}
}

@article{10.1016/j.asoc.2016.06.023,
author = {Mesquita, Diego P.P. and Rocha, Lincoln S. and Gomes, Joo Paulo P. and Rocha Neto, Ajalmar R.},
title = {Classification with reject option for software defect prediction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.06.023},
doi = {10.1016/j.asoc.2016.06.023},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose the use of classification with reject option for software defect prediction (SDP) as a way to incorporate additional knowledge in the SDP process.We propose two variants of the extreme learning machine with reject option.It is proposed an ELM with reject option for imbalanced datasets.The proposed method is tested on five real world software datasets.An example is shown to illustrate how the rejected software modules can be further analyzed to improve the final SDP accuracy. ContextSoftware defect prediction (SDP) is an important task in software engineering. Along with estimating the number of defects remaining in software systems and discovering defect associations, classifying the defect-proneness of software modules plays an important role in software defect prediction. Several machine-learning methods have been applied to handle the defect-proneness of software modules as a classification problem. This type of yes or no decision is an important drawback in the decision-making process and if not precise may lead to misclassifications. To the best of our knowledge, existing approaches rely on fully automated module classification and do not provide a way to incorporate extra knowledge during the classification process. This knowledge can be helpful in avoiding misclassifications in cases where system modules cannot be classified in a reliable way. ObjectiveWe seek to develop a SDP method that (i) incorporates a reject option in the classifier to improve the reliability in the decision-making process; and (ii) makes it possible postpone the final decision related to rejected modules for an expert analysis or even for another classifier using extra domain knowledge. MethodWe develop a SDP method called rejoELM and its variant, IrejoELM. Both methods were built upon the weighted extreme learning machine (ELM) with reject option that makes it possible postpone the final decision of non-classified modules, the rejected ones, to another moment. While rejoELM aims to maximize the accuracy for a rejection rate, IrejoELM maximizes the F-measure. Hence, IrejoELM becomes an alternative for classification with reject option for imbalanced datasets. ResultsrejoEM and IrejoELM are tested on five datasets of source code metrics extracted from real world open-source software projects. Results indicate that rejoELM has an accuracy for several rejection rates that is comparable to some state-of-the-art classifiers with reject option. Although IrejoELM shows lower accuracies for several rejection rates, it clearly outperforms all other methods when the F-measure is used as a performance metric. ConclusionIt is concluded that rejoELM is a valid alternative for classification with reject option problems when classes are nearly equally represented. On the other hand, IrejoELM is shown to be the best alternative for classification with reject option on imbalanced datasets. Since SDP problems are usually characterized as imbalanced learning problems, the use of IrejoELM is recommended.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1085–1093},
numpages = {9},
keywords = {Classification with reject option, Extreme learning machines, Software defect prediction}
}

@article{10.1007/s11277-017-5069-3,
author = {Dong, Feng and Wang, Junfeng and Li, Qi and Xu, Guoai and Zhang, Shaodong},
title = {Defect Prediction in Android Binary Executables Using Deep Neural Network},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-017-5069-3},
doi = {10.1007/s11277-017-5069-3},
abstract = {Software defect prediction locates defective code to help developers improve the security of software. However, existing studies on software defect prediction are mostly limited to the source code. Defect prediction for Android binary executables (called apks) has never been explored in previous studies. In this paper, we propose an explorative study of defect prediction in Android apks. We first propose smali2vec, a new approach to generate features that capture the characteristics of smali (decompiled files of apks) files in apks. Smali2vec extracts both token and semantic features of the defective files in apks and such comprehensive features are needed for building accurate prediction models. Then we leverage deep neural network (DNN), which is one of the most common architecture of deep learning networks, to train and build the defect prediction model in order to achieve accuracy. We apply our defect prediction model to more than 90,000 smali files from 50 Android apks and the results show that our model could achieve an AUC (the area under the receiver operating characteristic curve) of 85.98% and it is capable of predicting defects in apks. Furthermore, the DNN is proved to have a better performance than the traditional shallow machine learning algorithms (e.g., support vector machine and naive bayes) used in previous studies. The model has been used in our practical work and helped locate many defective files in apks.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2261–2285},
numpages = {25},
keywords = {Android binary executables, Deep neural network, Machine learning, Mobile security, Software defect prediction}
}

@article{10.1007/s11227-019-03051-w,
author = {NezhadShokouhi, Mohammad Mahdi and Majidi, Mohammad Ali and Rasoolzadegan, Abbas},
title = {Software defect prediction using over-sampling and feature extraction based on Mahalanobis distance},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {1},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-03051-w},
doi = {10.1007/s11227-019-03051-w},
abstract = {As the size of software projects becomes larger, software defect prediction (SDP) will play a key role in allocating testing resources reasonably, reducing testing costs, and speeding up the development process. Most SDP methods have used machine learning techniques based on common software metrics such as Halstead and McCabe’s cyclomatic. Datasets produced by these metrics usually do not follow Gaussian distribution, and also, they have overlaps in defect and non-defect classes. In addition, in many of software defect datasets, the number of defective modules (minority class) is considerably less than non-defective modules (majority class). In this situation, the performance of machine learning methods is reduced dramatically. Therefore, we first need to create a balance between minority and majority classes and then transfer the samples into a new space in which pair samples with same class (must-link set) are near to each other as close as possible and pair samples with different classes (cannot-link) stay as far as possible. To achieve the mentioned objectives, in this paper, Mahalanobis distance in two manners will be used. First, the minority class is oversampled based on the Mahalanobis distance such that generated synthetic data are more diverse from other minority data, and minority class distribution is not changed significantly. Second, a feature extraction method based on Mahalanobis distance metric learning is used which try to minimize distances of sample pairs in must-links and maximize the distance of sample pairs in cannot-links. To demonstrate the effectiveness of the proposed method, we performed some experiments on 12 publicly available datasets which are collected NASA repositories and compared its result by some powerful previous methods. The performance is evaluated in F-measure, G-Mean, and Matthews correlation coefficient. Generally, the proposed method has better performance as compared to the mentioned methods.},
journal = {J. Supercomput.},
month = jan,
pages = {602–635},
numpages = {34},
keywords = {Software defect prediction, Software metrics, Mahalanobis distance, Over-sampling, Feature extraction}
}

@article{10.1016/j.neucom.2018.04.090,
author = {Malhotra, Ruchika and Kamal, Shine},
title = {An empirical study to investigate oversampling methods for improving software defect prediction using imbalanced data},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {343},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.090},
doi = {10.1016/j.neucom.2018.04.090},
journal = {Neurocomput.},
month = may,
pages = {120–140},
numpages = {21},
keywords = {Defect prediction, Imbalanced data, Oversampling methods, MetaCost learners, Machine learning techniques, Procedural metrics}
}

@article{10.1145/3468744.3468751,
author = {Pinzger, Martin and Giger, Emanuel and Gall, Harald C.},
title = {Comparing fine-grained source code changes and code churn for bug prediction - A retrospective},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3468744.3468751},
doi = {10.1145/3468744.3468751},
abstract = {More than two decades ago, researchers started to mine the data stored in software repositories to help software developers in making informed decisions for developing and testing software systems. Bug prediction was one of the most promising and popular research directions that uses the data stored in software repositories to predict the bug-proneness or number of bugs in source files. On that topic and as part of Emanuel's PhD studies, we submitted a paper with the title Comparing fine-grained source code changes and code churn for bug prediction [8] to the 8th Working Conference on Mining Software Engineering, held 2011 in beautiful Honolulu, Hawaii. Ten years later, it got selected as one of the finalists to receive the MSR 2021 Most Influential Paper Award. In the following, we provide a retrospective on our work, describing the road to publishing this paper, its impact in the field of bug prediction, and the road ahead.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {21–23},
numpages = {3}
}

@article{10.1504/ijista.2019.102667,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {Statistical assessment of nonlinear manifold detection-based software defect prediction techniques},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {6},
issn = {1740-8865},
url = {https://doi.org/10.1504/ijista.2019.102667},
doi = {10.1504/ijista.2019.102667},
abstract = {Prediction of software defects has immense importance for obtaining desired outcome at minimised cost and so attracted researchers working on this topic applying various techniques, which were not found fully effective. Software datasets comprise of redundant features that hinder effective application of techniques resulting inappropriate defect prediction. Hence, it requires newer application of nonlinear manifold detection techniques (nonlinear MDTs) that has been examined for accurate prediction of defects at lesser time and cost using different classification techniques. In this work, we analysed and tested the effect of nonlinear MDTs to find out accurate and best classification technique for all datasets. Comparison has been made between the results of without or with nonlinear MDTs and paired two-tailed T-test has been performed for statistical testing and verifying the performance of classifiers using nonlinear MDTs on all datasets. Outcome revealed that among all nonlinear MDTs, FastMVU makes most accurate prediction of software defects.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {579–605},
numpages = {26},
keywords = {dimensionality reduction, fast maximum variance unfolding, FastMVU, machine learning, manifold detection, nonlinear, promise repository, software defect prediction}
}

@article{10.1007/s10515-015-0179-1,
author = {Wang, Tiejian and Zhang, Zhiwu and Jing, Xiaoyuan and Zhang, Liqiang},
title = {Multiple kernel ensemble learning for software defect prediction},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-015-0179-1},
doi = {10.1007/s10515-015-0179-1},
abstract = {Software defect prediction aims to predict the defect proneness of new software modules with the historical defect data so as to improve the quality of a software system. Software historical defect data has a complicated structure and a marked characteristic of class-imbalance; how to fully analyze and utilize the existing historical defect data and build more precise and effective classifiers has attracted considerable researchers' interest from both academia and industry. Multiple kernel learning and ensemble learning are effective techniques in the field of machine learning. Multiple kernel learning can map the historical defect data to a higher-dimensional feature space and make them express better, and ensemble learning can use a series of weak classifiers to reduce the bias generated by the majority class and obtain better predictive performance. In this paper, we propose to use the multiple kernel learning to predict software defect. By using the characteristics of the metrics mined from the open source software, we get a multiple kernel classifier through ensemble learning method, which has the advantages of both multiple kernel learning and ensemble learning. We thus propose a multiple kernel ensemble learning (MKEL) approach for software defect classification and prediction. Considering the cost of risk in software defect prediction, we design a new sample weight vector updating strategy to reduce the cost of risk caused by misclassifying defective modules as non-defective ones. We employ the widely used NASA MDP datasets as test data to evaluate the performance of all compared methods; experimental results show that MKEL outperforms several representative state-of-the-art defect prediction methods.},
journal = {Automated Software Engg.},
month = dec,
pages = {569–590},
numpages = {22},
keywords = {Ensemble learning, Multiple kernel ensemble learning (MKEL), Multiple kernel learning, Software defect prediction}
}

@inproceedings{10.1145/3239576.3239622,
author = {Yang, Zhao and Qian, Hongbing},
title = {Automated Parameter Tuning of Artificial Neural Networks for Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239622},
doi = {10.1145/3239576.3239622},
abstract = {Defect prediction can help predict defect-prone software modules and improve the efficiency and accuracy of defect location and repair, which plays an extremely important role in software quality assurance. Artificial Neural Networks (ANNs), a family of powerful machine learning regression or classification models, have been widely applied for defect prediction. However, the performance of these models will be degraded if they use suboptimal default parameter settings (e.g., the number of units in the hidden layer). This paper utilizes an automated parameter tuning technique-Caret to optimize parameter settings. In our study, 30 datasets are downloaded from the Tera-PROMISE Repository. According to the characteristics of the datasets, we select key features (metrics) as predictors to train defect prediction models. The experiment applies feed-forward, single hidden layer artificial neural network as classifier to build different defect prediction models respectively with optimized parameter settings and with default parameter settings. Confusion matrix and ROC curve are used for evaluating the quality of the models above. The results show that the models trained with optimized parameter settings outperform the models trained with default parameter settings. Hence, we suggest that researchers should pay attention to tuning parameter settings by Caret for ANNs instead of using suboptimal default settings if they select ANNs for training models in the future defect prediction studies.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {203–209},
numpages = {7},
keywords = {Artificial Neural Networks, Automated Parameter Tuning, Metrics, Software defect prediction},
location = {Chengdu, China},
series = {ICAIP '18}
}

@article{10.1007/s11219-018-9436-4,
author = {Ji, Haijin and Huang, Song and Wu, Yaning and Hui, Zhanwei and Zheng, Changyou},
title = {A new weighted naive Bayes method based on information diffusion for software defect prediction},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9436-4},
doi = {10.1007/s11219-018-9436-4},
abstract = {Software defect prediction (SDP) plays a significant part in identifying the most defect-prone modules before software testing and allocating limited testing resources. One of the most commonly used classifiers in SDP is naive Bayes (NB). Despite the simplicity of the NB classifier, it can often perform better than more complicated classification models. In NB, the features are assumed to be equally important, and the numeric features are assumed to have a normal distribution. However, the features often do not contribute equivalently to the classification, and they usually do not have a normal distribution after performing a Kolmogorov-Smirnov test; this may harm the performance of the NB classifier. Therefore, this paper proposes a new weighted naive Bayes method based on information diffusion (WNB-ID) for SDP. More specifically, for the equal importance assumption, we investigate six weight assignment methods for setting the feature weights and then choose the most suitable one based on the F-measure. For the normal distribution assumption, we apply the information diffusion model (IDM) to compute the probability density of each feature instead of the acquiescent probability density function of the normal distribution. We carry out experiments on 10 software defect data sets of three types of projects in three different programming languages provided by the PROMISE repository. Several well-known classifiers and ensemble methods are included for comparison. The final experimental results demonstrate the effectiveness and practicability of the proposed method.},
journal = {Software Quality Journal},
month = sep,
pages = {923–968},
numpages = {46},
keywords = {Software defect prediction, Naive Bayes, Feature weighting, Information diffusion}
}

@article{10.1007/s42979-020-0119-4,
author = {Khuat, Thanh Tung and Le, My Hanh},
title = {Evaluation of Sampling-Based Ensembles of Classifiers on Imbalanced Data for Software Defect Prediction Problems},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {2},
url = {https://doi.org/10.1007/s42979-020-0119-4},
doi = {10.1007/s42979-020-0119-4},
abstract = {Defect prediction in software projects plays a crucial role to reduce quality-based risk and increase the capability of detecting faulty program modules. Hence, classification approaches to anticipate software defect proneness based on static code characteristics have become a hot topic with a great deal of attention in recent years. While several novel studies show that the use of a single classifier causes the performance bottleneck, ensembles of classifiers might effectively enhance classification performance compared to a single classifier. However, the class imbalance property of software defect data severely hinders the classification efficiency of ensemble learning. To cope with this problem, resampling methods are usually combined into ensemble models.
This paper empirically assesses the importance of sampling with regard to ensembles of various classifiers on imbalanced data in software defect prediction problems. Extensive experiments with the combination of seven different kinds of classification algorithms, three sampling methods, and two balanced data learning schemata were conducted over ten datasets. Empirical results indicated the positive effects of combining sampling techniques and the ensemble learning model on the performance of defect prediction regarding datasets with imbalanced class distributions.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {16},
keywords = {Software defect prediction, Random undersampling, Random oversampling, SMOTE, Data balancing, Ensemble learning, Imbalanced data}
}

@article{10.1007/s00500-021-06254-7,
author = {Sotto-Mayor, Bruno and Kalech, Meir},
title = {Cross-project smell-based defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {22},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06254-7},
doi = {10.1007/s00500-021-06254-7},
abstract = {Defect prediction is a technique introduced to optimize the testing phase of the software development pipeline by predicting which components in the software may contain defects. Its methodology trains a classifier with data regarding a set of features measured on each component from the target software project to predict whether the component may be defective or not. However, suppose the defective information is not available in the training set. In that case, we need to rely on an alternate approach that uses the training set of external projects to train the classifier. This approached is called cross-project defect prediction. Bad code smells are a category of features that have been previously explored in defect prediction and have been shown to be a good predictor of defects. Code smells are patterns of poor development in the code and indicate flaws in its design and implementation. Although they have been previously studied in the context of defect prediction, they have not been studied as features for cross-project defect prediction. In our experiment, we train defect prediction models for 100 projects to evaluate the predictive performance of the bad code smells. We implemented four cross-project approaches known in the literature and compared the performance of 37 smells with 56 code metrics, commonly used for defect prediction. The results show that the cross-project defect prediction models trained with code smells significantly improved 6.50% on the ROC AUC compared against the code metrics.},
journal = {Soft Comput.},
month = nov,
pages = {14171–14181},
numpages = {11},
keywords = {Cross-project defect prediction, Defect prediction, Code smell, Mining software repositories, Software quality, Software engineering}
}

@article{10.1504/ijcat.2019.100297,
author = {Jayanthi, R. and Florence, M. Lilly},
title = {Improved Bayesian regularisation using neural networks based on feature selection for software defect prediction},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {60},
number = {3},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2019.100297},
doi = {10.1504/ijcat.2019.100297},
abstract = {Demand for software-based applications has grown drastically in various real-time applications. However, software testing schemes have been developed which include manual and automatic testing. Manual testing requires human effort and chances of error may still affect the quality of software. To overcome this issue, automatic software testing techniques based on machine learning techniques have been developed. In this work, we focus on the machine learning scheme for early prediction of software defects using Levenberg-Marquardt algorithm (LM), Back Propagation (BP) and Bayesian Regularisation (BR) techniques. Bayesian regularisation achieves better performance in terms of bug prediction. However, this performance can be enhanced further. Hence, we developed a novel approach for attribute selection-based feature selection technique to improve the performance of BR classification. An extensive study is carried out with the PROMISE repository where we considered KC1 and JM1 datasets. Experimental study shows that the proposed approach achieves better performance in predicting the defects in software.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {225–241},
numpages = {16},
keywords = {defect prediction model, machine learning techniques, software defect prediction, software metrics, gradient descent optimisation, gradient-based approach, feature subset selection, cross entropy error function, adaptive computation process}
}

@article{10.1016/j.procs.2015.02.161,
author = {Arora, Ishani and Tetarwal, Vivek and Saha, Anju},
title = {Open Issues in Software Defect Prediction},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.161},
doi = {10.1016/j.procs.2015.02.161},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {906–912},
numpages = {7},
keywords = {data mining, defect prediction, machine learning, software quality, software testing}
}

@article{10.1007/s11390-019-1958-0,
author = {Chen, Xiang and Zhang, Dun and Cui, Zhan-Qi and Gu, Qing and Ju, Xiao-Lin},
title = {DP-Share: Privacy-Preserving Software Defect Prediction Model Sharing Through Differential Privacy},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1958-0},
doi = {10.1007/s11390-019-1958-0},
abstract = {In current software defect prediction (SDP) research, most previous empirical studies only use datasets provided by PROMISE repository and this may cause a threat to the external validity of previous empirical results. Instead of SDP dataset sharing, SDP model sharing is a potential solution to alleviate this problem and can encourage researchers in the research community and practitioners in the industrial community to share more models. However, directly sharing models may result in privacy disclosure, such as model inversion attack. To the best of our knowledge, we are the first to apply differential privacy (DP) to privacy-preserving SDP model sharing and then propose a novel method DP-Share, since DP mechanisms can prevent this attack when the privacy budget is carefully selected. In particular, DP-Share first performs data preprocessing for the dataset, such as over-sampling for minority instances (i.e., defective modules) and conducting discretization for continuous features to optimize privacy budget allocation. Then, it uses a novel sampling strategy to create a set of training sets. Finally it constructs decision trees based on these training sets and these decision trees can form a random forest (i.e., model). The last phase of DP-Share uses Laplace and exponential mechanisms to satisfy the requirements of DP. In our empirical studies, we choose nine experimental subjects from real software projects. Then, we use AUC (area under ROC curve) as the performance measure and holdout as our model validation technique. After privacy and utility analysis, we find that DP-Share can achieve better performance than a baseline method DF-Enhance in most cases when using the same privacy budget. Moreover, we also provide guidelines to effectively use our proposed method. Our work attempts to fill the research gap in terms of differential privacy for SDP, which can encourage researchers and practitioners to share more SDP models and then effectively advance the state of the art of SDP.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1020–1038},
numpages = {19},
keywords = {software defect prediction, model sharing, differential privacy, cross project defect prediction, empirical study}
}

@inproceedings{10.1145/3106237.3106257,
author = {Fu, Wei and Menzies, Tim},
title = {Revisiting unsupervised learning for defect prediction},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106257},
doi = {10.1145/3106237.3106257},
abstract = {Collecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore "unsupervised" approaches to quality prediction that does not require labelled data. An alternate technique is to use "supervised" approaches that learn models from project data labelled with, say, "defective" or "not-defective". Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects. At FSE-16, Yang et al. reported startling results where unsupervised defect predictors outperformed supervised predictors for effort-aware just-in-time defect prediction. If confirmed, these results would lead to a dramatic simplification of a seemingly complex task (data mining) that is widely explored in the software engineering literature. This paper repeats and refutes those results as follows. (1) There is much variability in the efficacy of the Yang et al. predictors so even with their approach, some supervised data is required to prune weaker predictors away. (2) Their findings were grouped across N projects. When we repeat their analysis on a project-by-project basis, supervised predictors are seen to work better. Even though this paper rejects the specific conclusions of Yang et al., we still endorse their general goal. In our our experiments, supervised predictors did not perform outstandingly better than unsupervised ones for effort-aware just-in-time defect prediction. Hence, they may indeed be some combination of unsupervised learners to achieve comparable performance to supervised ones. We therefore encourage others to work in this promising area.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {72–83},
numpages = {12},
keywords = {data analytics for software engineering, defect prediction, empirical studies, software repository mining},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1007/978-3-030-59003-1_27,
author = {Shakhovska, Natalya and Yakovyna, Vitaliy and Kryvinska, Natalia},
title = {An Improved Software Defect Prediction Algorithm Using Self-organizing Maps Combined with Hierarchical Clustering and Data Preprocessing},
year = {2020},
isbn = {978-3-030-59002-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59003-1_27},
doi = {10.1007/978-3-030-59003-1_27},
abstract = {An improved software defects prediction algorithm based on combination of Kohonen map and hierarchical clustering is presented in this paper. The need for software reliability assessment and analysis growths rapidly due to increasing dependence of our day-to-day life on software-controlled devices and systems. Software reliability prediction is the only tool available at early stage of software development lifecycle when the debugging cost risk of faulty operation is minimal. Artificial intelligence and machine learning in particular are promising techniques to solve this task. Various classification methods have been used previously to build software defect prediction models, ranging from simple, like logistic regression, to advanced methods, e.g. multivariate adaptive regression splicing. However, the available literature still does not allow to make unambiguous conclusion concerning the choice of the best classifier and trying different dimensions to overcome potential bias is suggested. The purpose of the paper is to analyze the software code metrics to find dependences be-tween software module’s defect-proneness and its metrics. JM1 public NASA dataset from PROMISE Software Engineering Repository was used in this study. To increase the classification accuracy, we combine self-organizing maps with hierarchical clustering and data preprocessing.},
booktitle = {Database and Expert Systems Applications: 31st International Conference, DEXA 2020, Bratislava, Slovakia, September 14–17, 2020, Proceedings, Part I},
pages = {414–424},
numpages = {11},
keywords = {Prediction algorithm, Hierarchical clustering, Software defect analysis},
location = {Bratislava, Slovakia}
}

@article{10.4018/IJOSSP.2017100102,
author = {Akour, Mohammed and Melhem, Wasen Yahya},
title = {Software Defect Prediction Using Genetic Programming and Neural Networks},
year = {2017},
issue_date = {October 2017},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {4},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2017100102},
doi = {10.4018/IJOSSP.2017100102},
abstract = {This article describes how classification methods on software defect prediction is widely researched due to the need to increase the software quality and decrease testing efforts. However, findings of past researches done on this issue has not shown any classifier which proves to be superior to the other. Additionally, there is a lack of research that studies the effects and accuracy of genetic programming on software defect prediction. To find solutions for this problem, a comparative software defect prediction experiment between genetic programming and neural networks are performed on four datasets from the NASA Metrics Data repository. Generally, an interesting degree of accuracy is detected, which shows how the metric-based classification is useful. Nevertheless, this article specifies that the application and usage of genetic programming is highly recommended due to the detailed analysis it provides, as well as an important feature in this classification method which allows the viewing of each attributes impact in the dataset.},
journal = {Int. J. Open Source Softw. Process.},
month = oct,
pages = {32–51},
numpages = {20},
keywords = {Classification, Genetic Algorithm, Genetic Programming, Machine learning, Nasa Metrics, Neural Networks, Software Defect Prediction, Testing}
}

@inproceedings{10.5555/3432601.3432618,
author = {Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Report on evaluation experiments using different machine learning techniques for defect prediction},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {123–132},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1007/978-3-030-87007-2_14,
author = {Peng\H{o}, Edit},
title = {Examining the Bug Prediction Capabilities of Primitive Obsession Metrics},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_14},
doi = {10.1007/978-3-030-87007-2_14},
abstract = {Bug prediction is an approach that helps make bug detection more automated during software development. Based on a bug dataset a prediction model is built to locate future bugs. Bug datasets contain information about previous defects in the code, process metrics, or source code metrics, etc. As code smells can indicate potential flaws in the source code, they can be used for bug prediction as well.In our previous work, we introduced several source code metrics to detect and describe the occurrence of Primitive Obsession in Java. This paper is a further study on three of the Primitive Obsession metrics. We integrated them into an existing, source code metrics-based bug dataset, and studied the effectiveness of the prediction built upon it. We performed a 10 fold cross-validation on the whole dataset and a cross-project validation as well. We compared the new models with the results of the original dataset. While the cross-validation showed no significant change, in the case of the cross-project validation, we have found that the amount of improvement exceeded the amount of deterioration by 5%. Furthermore, the variance added to the dataset was confirmed by correlation and PCA calculations.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {185–200},
numpages = {16},
keywords = {Bug prediction, Code smells, Primitive obsession, Static analysis, Refactoring},
location = {Cagliari, Italy}
}

@article{10.1155/2021/4997459,
author = {Li, Zhen and Li, Tong and Wu, YuMei and Yang, Liu and Miao, Hong and Wang, DongSheng and Precup, Radu-Emil},
title = {Software Defect Prediction Based on Hybrid Swarm Intelligence and Deep Learning},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/4997459},
doi = {10.1155/2021/4997459},
abstract = {In order to improve software quality and testing efficiency, this paper implements the prediction of software defects based on deep learning. According to the respective advantages and disadvantages of the particle swarm algorithm and the wolf swarm algorithm, the two algorithms are mixed to realize the complementary advantages of the algorithms. At the same time, the hybrid algorithm is used in the search of model hyperparameter optimization, the loss function of the model is used as the fitness function, and the collaborative search ability of the swarm intelligence population is used to find the global optimal solution in multiple local solution spaces. Through the analysis of the experimental results of six data sets, compared with the traditional hyperparameter optimization method and a single swarm intelligence algorithm, the model using the hybrid algorithm has higher and better indicators. And, under the processing of the autoencoder, the performance of the model has been further improved.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {17}
}

@inproceedings{10.1145/3377811.3380403,
author = {Tabassum, Sadia and Minku, Leandro L. and Feng, Danyi and Cabral, George G. and Song, Liyan},
title = {An investigation of cross-project learning in online just-in-time software defect prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380403},
doi = {10.1145/3377811.3380403},
abstract = {Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {554–565},
numpages = {12},
keywords = {class imbalance, concept drift, cross-project learning, online learning, software defect prediction, transfer learning, verification latency},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.ins.2018.02.027,
author = {Miholca, Diana-Lucia and Czibula, Gabriela and Czibula, Istvan Gergely},
title = {A novel approach for software defect prediction through hybridizing gradual relational association rules with artificial neural networks},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {441},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.02.027},
doi = {10.1016/j.ins.2018.02.027},
abstract = {The growing complexity of software projects requires increasing consideration of their analysis and testing. Identifying defective software entities is essential for software quality assurance and it also improves activities related to software testing. In this study, we developed a novel supervised classification method called HyGRAR for software defect prediction. HyGRAR is a non-linear hybrid model that combines gradual relational association rule mining and artificial neural networks to discriminate between defective and non-defective software entities. Experiments performed based on 10 open-source data sets demonstrated the excellent performance of the HYGRAR classifier. HyGRAR performed better than most of the previously proposed approaches for software defect prediction in performance evaluations using the same data sets.},
journal = {Inf. Sci.},
month = may,
pages = {152–170},
numpages = {19},
keywords = {Artificial neural network, Gradual relational association rule, Machine learning, Software defect prediction}
}

@article{10.1016/j.knosys.2015.09.035,
author = {Li, Weiwei and Huang, Zhiqiu and Li, Qing},
title = {Three-way decisions based software defect prediction},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.09.035},
doi = {10.1016/j.knosys.2015.09.035},
abstract = {Based on a two-stage classification method and a two-stage ranking method on three-way decisions, this paper introduces a three-way decisions framework for cost-sensitive software defect prediction. For the classification problem in software defect prediction, traditional two-way decisions methods usually generate a higher classification error and more decision cost. Here, a two-stage classification method that integrates three-way decisions and ensemble learning to predict software defect is proposed. Experimental results on NASA data sets show that our method can obtain a higher accuracy and a lower decision cost. For the ranking problem in software defect prediction, a two-stage ranking method is introduced. In the first stage, all software modules are classified into three different regions based on three-way decisions. A dominance relation rough set based ranking algorithm is next applied to rank the modules in each region. Comparison experiments with 6 other ranking methods present that our proposed method can obtain a better result on FPA measure.},
journal = {Know.-Based Syst.},
month = jan,
pages = {263–274},
numpages = {12},
keywords = {Software defect classification, Software defect ranking, Three-way decisions}
}

@inproceedings{10.1145/3180374.3181331,
author = {Li, Yuting and Su, Jianmin and Yang, Xiaoxing},
title = {Multi-Objective vs. Single-Objective Approaches for Software Defect Prediction},
year = {2018},
isbn = {9781450354318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180374.3181331},
doi = {10.1145/3180374.3181331},
abstract = {Software defect prediction employs attributes of software modules to identify defect-prone modules and thus improves software reliability by allocating testing resources more efficiently. Realizing that single-objective methods might be insufficient for solving defect prediction problems, some researchers have proposed multi-objective learning approaches, and proved better performance of multi-objective than single-objective methods. However, existing compared single-objective methods optimize a completely different goal from goals of multi-objective approaches, which might lead to bias. In this paper, we compare a multi-objective approach that optimizes two objectives and a single-objective approach that directly optimizes a trade-off of the two objectives, in order to further investigate the comparison of multi-objective and single-objective approaches. The conclusion will help to appropriately choose multi-objective or single-objective learning approaches for defect prediction.},
booktitle = {Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {122–127},
numpages = {6},
keywords = {Multi-objective learning, cost, effectiveness, single-objective learning, software defect prediction},
location = {Wuhan, China},
series = {ICMSS 2018}
}

@article{10.1007/s10515-011-0092-1,
author = {Li, Ming and Zhang, Hongyu and Wu, Rongxin and Zhou, Zhi-Hua},
title = {Sample-based software defect prediction with active and semi-supervised learning},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0092-1},
doi = {10.1007/s10515-011-0092-1},
abstract = {Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice.},
journal = {Automated Software Engg.},
month = jun,
pages = {201–230},
numpages = {30},
keywords = {Active semi-supervised learning, Machine learning, Quality assurance, Sampling, Software defect prediction}
}

@article{10.1007/s10664-021-09991-3,
author = {Tahir, Amjed and Bennin, Kwabena E. and Xiao, Xun and MacDonell, Stephen G.},
title = {Does class size matter? An in-depth assessment of the effect of class size in software defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09991-3},
doi = {10.1007/s10664-021-09991-3},
abstract = {In the past 20 years, defect prediction studies have generally acknowledged the effect of class size on software prediction performance. To quantify the relationship between object-oriented (OO) metrics and defects, modelling has to take into account the direct, and potentially indirect, effects of class size on defects. However, some studies have shown that size cannot be simply controlled or ignored, when building prediction models. As such, there remains a question whether, and when, to control for class size. This study provides a new in-depth examination of the impact of class size on the relationship between OO metrics and software defects or defect-proneness. We assess the impact of class size on the number of defects and defect-proneness in software systems by employing a regression-based mediation (with bootstrapping) and moderation analysis to investigate the direct and indirect effect of class size in count and binary defect prediction. Our results show that the size effect is not always significant for all metrics. Of the seven OO metrics we investigated, size consistently has significant mediation impact only on the relationship between Coupling Between Objects (CBO) and defects/defect-proneness, and a potential moderation impact on the relationship between Fan-out and defects/defect-proneness. Other metrics show mixed results, in that they are significant for some systems but not for others. Based on our results we make three recommendations. One, we encourage researchers and practitioners to examine the impact of class size for the specific data they have in hand and through the use of the proposed statistical mediation/moderation procedures. Two, we encourage empirical studies to investigate the indirect effect of possible additional variables in their models when relevant. Three, the statistical procedures adopted in this study could be used in other empirical software engineering research to investigate the influence of potential mediators/moderators.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {38},
keywords = {Defect prediction, Class size, Metrics, Software quality}
}

@inproceedings{10.1145/2896387.2900324,
author = {Rahman, Md. Habibur and Sharmin, Sadia and Sarwar, Sheikh Muhammad and Shoyaib, Mohammad},
title = {Software Defect Prediction Using Feature Space Transformation},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900324},
doi = {10.1145/2896387.2900324},
abstract = {In software quality estimation research, software defect prediction is a key topic. A defect prediction model is generally constructed using a variety of software attributes and each attribute may have positive, negative or neutral effect on a specific model. Selection of an optimal set of attributes for model development remains a vital yet unexplored issue. In this paper, we have introduced a new feature space transformation process with a normalization technique to improve the defect prediction accuracy. We proposed a feature space transformation technique and classify the instances using Support Vector Machine (SVM) with its histogram intersection kernel. The proposed method is evaluated using the data sets from NASA metric data repository and its application demonstrates acceptable accuracy.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {72},
numpages = {6},
keywords = {Attribute selection, Feature space transformation, Software defect prediction},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1007/978-3-030-87007-2_26,
author = {Szamosv\"{o}lgyi, Zsolt J\'{a}nos and V\'{a}radi, Endre Tam\'{a}s and T\'{o}th, Zolt\'{a}n and J\'{a}sz, Judit and Ferenc, Rudolf},
title = {Assessing Ensemble Learning Techniques in Bug Prediction},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_26},
doi = {10.1007/978-3-030-87007-2_26},
abstract = {The application of ensemble learning techniques is continuously increasing, since they have proven to be superior over traditional machine learning techniques in various domains. These algorithms could be employed for bug prediction purposes as well. Existing studies investigated the performance of ensemble learning techniques only for PROMISE and the NASA MDP public datasets; however, it is important to evaluate the ensemble learning techniques on additional public datasets in order to test the generalizability of the techniques. We investigated the performance of the two most widely-used ensemble learning techniques AdaBoost and Bagging on the Unified Bug Dataset, which encapsulates 3 class level public bug datasets in a uniformed format with a common set of software product metrics used as predictors. Additionally, we investigated the effect of using 3 different resampling techniques on the dataset. Finally, we studied the performance of using Decision Tree and Na\"{\i}ve Bayes as the weak learners in the ensemble learning. We also fine tuned the parameters of the weak learners to have the best possible end results.We experienced that AdaBoost with Decision Tree weak learner outperformed other configurations. We could achieve 54.61% F-measure value (81.96% Accuracy, 50.92% Precision, 58.90% Recall) with the configuration of 300 estimators and 0.05 learning rate. Based on the needs, one can apply RUS resampling to get a recall value up&nbsp;to 75.14% (of course losing precision at the same time).},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {368–381},
numpages = {14},
keywords = {AdaBoost, Bug prediction, Resampling, Unified bug dataset},
location = {Cagliari, Italy}
}

@article{10.1016/j.infsof.2017.11.008,
author = {Tong, Haonan and Liu, Bin and Wang, Shihai},
title = {Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning},
year = {2018},
issue_date = {Apr 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {96},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.11.008},
doi = {10.1016/j.infsof.2017.11.008},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {94–111},
numpages = {18},
keywords = {Software defect prediction, Stacked denoising autoencoders, Ensemble learning, Software metrics, Deep learning}
}

@article{10.1016/j.infsof.2019.07.003,
author = {Zhou, Tianchi and Sun, Xiaobing and Xia, Xin and Li, Bin and Chen, Xiang},
title = {Improving defect prediction with deep forest},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.003},
doi = {10.1016/j.infsof.2019.07.003},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {204–216},
numpages = {13},
keywords = {Software defect prediction, Deep forest, Cascade strategy, Empirical evaluation}
}

@inproceedings{10.4108/icst.bict.2014.257871,
author = {Malhotra, Ruchika and Raje, Rajeev},
title = {An empirical comparison of machine learning techniques for software defect prediction},
year = {2014},
isbn = {9781631900532},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.bict.2014.257871},
doi = {10.4108/icst.bict.2014.257871},
abstract = {Software systems are exposed to various types of defects. The timely identification of defective classes is essential in early phases of software development to reduce the cost of testing the software. This will guide the software practitioners and researchers for planning of the proper allocation of testing resources. Software metrics can be used in conjunction with defect data to develop models for predicting defective classes. There have been various machine learning techniques proposed in the literature for analyzing complex relationships and extracting useful information from problems in less time. However, more studies comparing these techniques are needed to provide evidence so that confidence is established on the performance of one technique over the other. In this paper we address four issues (i) comparison of the machine learning techniques over unpopular used data sets (ii) use of inappropriate performance measures for measuring the performance of defect prediction models (iii) less use of statistical tests and (iv) validation of models from the same data set from which they are trained. To resolve these issues, in this paper, we compare 18 machine learning techniques for investigating the effect of Object-Oriented metrics on defective classes. The results are validated on six releases of the 'MMS' application package of recent widely used mobile operating system -- Android. The overall results of the study indicate the predictive capability of the machine learning techniques and an endorsement of one particular ML technique to predict defects.},
booktitle = {Proceedings of the 8th International Conference on Bioinspired Information and Communications Technologies},
pages = {320–327},
numpages = {8},
keywords = {defect prediction, empirical validation, machine learning, object-oriented metrics},
location = {Boston, Massachusetts},
series = {BICT '14}
}

@inproceedings{10.5555/3432601.3432619,
author = {Jahanshahi, Hadi and Cevik, Mucahit and Ba\c{s}ar, Ay\c{s}e},
title = {Moving from cross-project defect prediction to heterogeneous defect prediction: a partial replication study},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software defect prediction heavily relies on the metrics collected from software projects. Earlier studies often used machine learning techniques to build, validate, and improve bug prediction models using either a set of metrics collected within a project or across different projects. However, techniques applied and conclusions derived by those models are restricted by how identical those metrics are. Knowledge coming from those models will not be extensible to a target project if no sufficient overlapping metrics have been collected in the source projects. To explore the feasibility of transferring knowledge across projects without common labeled metrics, we systematically integrated Heterogeneous Defect Prediction (HDP) by replicating and validating the obtained results. Our main goal is to extend prior research and explore the feasibility of HDP and finally to compare its performance with that of its predecessor, Cross-Project Defect Prediction. We construct an HDP model on different publicly available datasets. Moreover, we propose a new ensemble voting approach in the HDP context to utilize the predictive power of multiple available datasets. The result of our experiment is comparable to that of the original study. However, we also explored the feasibility of HDP in real cases. Our results shed light on the infeasibility of many cases for the HDP algorithm due to its sensitivity to the parameter selection. In general, our analysis gives a deep insight into why and how to perform transfer learning from one domain to another, and in particular, provides a set of guidelines to help researchers and practitioners to disseminate knowledge to the defect prediction domain.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {133–142},
numpages = {10},
keywords = {defect prediction, heterogeneous metrics, software quality, transfer learning},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1109/ICSE.2019.00076,
author = {Cabral, George G. and Minku, Leandro L. and Shihab, Emad and Mujahid, Suhaib},
title = {Class imbalance evolution and verification latency in just-in-time software defect prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00076},
doi = {10.1109/ICSE.2019.00076},
abstract = {Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that rebuild classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency - the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {666–676},
numpages = {11},
keywords = {class imbalance, concept drift, ensembles, online learning, software defect prediction, verification latency},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1016/j.jss.2019.03.012,
author = {Ni, Chao and Chen, Xiang and Wu, Fangfang and Shen, Yuxiang and Gu, Qing},
title = {An empirical study on pareto based multi-objective feature selection for software defect prediction},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.012},
doi = {10.1016/j.jss.2019.03.012},
journal = {J. Syst. Softw.},
month = jun,
pages = {215–238},
numpages = {24},
keywords = {xx-xx, xx-xx, Software defect prediction, Search based software engineering, Feature selection, Multi-Objective optimization, Empirical study}
}

@article{10.1016/j.asoc.2017.05.043,
author = {Arar, mer Faruk and Ayan, Krat},
title = {A feature dependent Naive Bayes approach and its application to the software defect prediction problem},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.05.043},
doi = {10.1016/j.asoc.2017.05.043},
abstract = {Display Omitted In Naive Bayes, features are assumed to be independent and have equal weight. But, In practice, features are interrelated.In this study, features are included for calculation as pairs using the proposed Feature Dependent Naive Bayes (FDNB) method.Eight data sets from the NASA PROMISE repository were used for the software defect prediction problem.Results were compared with other modified NBs. Increased classification performance was found after use of the proposed FDNB. Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other and that their weights are equally important. However, in practice, features may be interrelated. In that case, such assumptions may cause a dramatic decrease in performance. In this study, by following preprocessing steps, a Feature Dependent Naive Bayes (FDNB) classification method is proposed. Features are included for calculation as pairs to create dependence between one another. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized NASA PROMISE data sets. The obtained results show that this new method is more successful than the standard Naive Bayes approach and that it has a competitive performance with other feature-weighting techniques. A further aim of this study is to demonstrate that to be reliable, a learning model must be constructed by using only training data, as otherwise misleading results arise from the use of the entire data set.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {197–209},
numpages = {13},
keywords = {Data mining, Discretization, Feature independence, Naive Bayes, Software defect prediction}
}

@article{10.4018/IJOSSP.2018010101,
author = {Kakkar, Misha and Jain, Sarika and Bansal, Abhay and Grover, P.S.},
title = {Combining Data Preprocessing Methods With Imputation Techniques for Software Defect Prediction},
year = {2018},
issue_date = {January 2018},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2018010101},
doi = {10.4018/IJOSSP.2018010101},
abstract = {Software Defect Prediction SDP models are used to predict, whether software is clean or buggy using the historical data collected from various software repositories. The data collected from such repositories may contain some missing values. In order to estimate missing values, imputation techniques are used, which utilizes the complete observed values in the dataset. The objective of this study is to identify the best-suited imputation technique for handling missing values in SDP dataset. In addition to identifying the imputation technique, the authors have investigated for the most appropriate combination of imputation technique and data preprocessing method for building SDP model. In this study, four combinations of imputation technique and data preprocessing methods are examined using the improved NASA datasets. These combinations are used along with five different machine-learning algorithms to develop models. The performance of these SDP models are then compared using traditional performance indicators. Experiment results show that among different imputation techniques, linear regression gives the most accurate imputed value. The combination of linear regression with correlation based feature selector outperforms all other combinations. To validate the significance of data preprocessing methods with imputation the findings are applied to open source projects. It was concluded that the result is in consistency with the above conclusion.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {1–19},
numpages = {19},
keywords = {Feature Selection, Instance Selection, Missing Value Imputation, Software Defect Prediction}
}

@article{10.1155/2021/5069016,
author = {Balogun, Abdullateef O. and Basri, Shuib and Mahamad, Saipunidzam and Capretz, Luiz Fernando and Imam, Abdullahi Abubakar and Almomani, Malek A. and Adeyemo, Victor E. and Kumar, Ganesh and Dourado, Ant\'{o}nio},
title = {A Novel Rank Aggregation-Based Hybrid Multifilter Wrapper Feature Selection Method in Software Defect Prediction},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/5069016},
doi = {10.1155/2021/5069016},
abstract = {The high dimensionality of software metric features has long been noted as a data quality problem that affects the performance of software defect prediction (SDP) models. This drawback makes it necessary to apply feature selection (FS) algorithm(s) in SDP processes. FS approaches can be categorized into three types, namely, filter FS (FFS), wrapper FS (WFS), and hybrid FS (HFS). HFS has been established as superior because it combines the strength of both FFS and WFS methods. However, selecting the most appropriate FFS (filter rank selection problem) for HFS is a challenge because the performance of FFS methods depends on the choice of datasets and classifiers. In addition, the local optima stagnation and high computational costs of WFS due to large search spaces are inherited by the HFS method. Therefore, as a solution, this study proposes a novel rank aggregation-based hybrid multifilter wrapper feature selection (RAHMFWFS) method for the selection of relevant and irredundant features from software defect datasets. The proposed RAHMFWFS is divided into two stepwise stages. The first stage involves a rank aggregation-based multifilter feature selection (RMFFS) method that addresses the filter rank selection problem by aggregating individual rank lists from multiple filter methods, using a novel rank aggregation method to generate a single, robust, and non-disjoint rank list. In the second stage, the aggregated ranked features are further preprocessed by an enhanced wrapper feature selection (EWFS) method based on a dynamic reranking strategy that is used to guide the feature subset selection process of the HFS method. This, in turn, reduces the number of evaluation cycles while amplifying or maintaining its prediction performance. The feasibility of the proposed RAHMFWFS was demonstrated on benchmarked software defect datasets with Na\"{\i}ve Bayes and Decision Tree classifiers, based on accuracy, the area under the curve (AUC), and F-measure values. The experimental results showed the effectiveness of RAHMFWFS in addressing filter rank selection and local optima stagnation problems in HFS, as well as the ability to select optimal features from SDP datasets while maintaining or enhancing the performance of SDP models. To conclude, the proposed RAHMFWFS achieved good performance by improving the prediction performances of SDP models across the selected datasets, compared to existing state-of-the-arts HFS methods.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.1145/2568225.2568320,
author = {Jing, Xiao-Yuan and Ying, Shi and Zhang, Zhi-Wu and Wu, Shan-Shan and Liu, Jin},
title = {Dictionary learning based software defect prediction},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568320},
doi = {10.1145/2568225.2568320},
abstract = {In order to improve the quality of a software system, software defect prediction aims to automatically identify defective software modules for efficient software test. To predict software defect, those classification methods with static code attributes have attracted a great deal of attention. In recent years, machine learning techniques have been applied to defect prediction. Due to the fact that there exists the similarity among different software modules, one software module can be approximately represented by a small proportion of other modules. And the representation coefficients over the pre-defined dictionary, which consists of historical software module data, are generally sparse. In this paper, we propose to use the dictionary learning technique to predict software defect. By using the characteristics of the metrics mined from the open source software, we learn multiple dictionaries (including defective module and defective-free module sub-dictionaries and the total dictionary) and sparse representation coefficients. Moreover, we take the misclassification cost issue into account because the misclassification of defective modules generally incurs much higher risk cost than that of defective-free ones. We thus propose a cost-sensitive discriminative dictionary learning (CDDL) approach for software defect classification and prediction. The widely used datasets from NASA projects are employed as test data to evaluate the performance of all compared methods. Experimental results show that CDDL outperforms several representative state-of-the-art defect prediction methods.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {414–423},
numpages = {10},
keywords = {Software defect prediction, cost-sensitive discriminative dictionary learning (CDDL), dictionary learning, sparse representation},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1109/MSR.2019.00017,
author = {Dam, Hoa Khanh and Pham, Trang and Ng, Shien Wee and Tran, Truyen and Grundy, John and Ghose, Aditya and Kim, Taeksu and Kim, Chul-Joo},
title = {Lessons learned from using a deep tree-based model for software defect prediction in practice},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00017},
doi = {10.1109/MSR.2019.00017},
abstract = {Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {46–57},
numpages = {12},
keywords = {deep learning, defect prediction},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1002/smr.2330,
author = {Shi, Ke and Lu, Yang and Liu, Guangliang and Wei, Zhenchun and Chang, Jingfei},
title = {MPT‐embedding: An unsupervised representation learning of code for software defect prediction},
year = {2021},
issue_date = {April 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {33},
number = {4},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2330},
doi = {10.1002/smr.2330},
abstract = {Software project defect prediction can help developers allocate debugging resources. Existing software defect prediction models are usually based on machine learning methods, especially deep learning. Deep learning‐based methods tend to build end‐to‐end models that directly use source code‐based abstract syntax trees (ASTs) as input. They do not pay enough attention to the front‐end data representation. In this paper, we propose a new framework to represent source code called multiperspective tree embedding (MPT‐embedding), which is an unsupervised representation learning method. MPT‐embedding parses the nodes of ASTs from multiple perspectives and encodes the structural information of a tree into a vector sequence. Experiments on both cross‐project defect prediction (CPDP) and within‐project defect prediction (WPDP) show that, on average, MPT‐embedding provides improvements over the state‐of‐the‐art method.Source code‐based automatic representations are more objective and accurate than traditional handcrafted metrics. This article proposed a new framework to represent code called multiperspective tree embedding (MPT‐embedding), which is an unsupervised representation learning method. MPT‐embedding parses the nodes of ASTs from multiple perspectives and encodes the structural information of a tree into a vector sequence. Experiments on the tasks of defect prediction show the effectiveness of the model.


image
image},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {20},
keywords = {deep learning, defect prediction, representation learning, tree embedding}
}

@article{10.1016/j.asoc.2021.107870,
author = {Kabir, Md Alamgir and Keung, Jacky and Turhan, Burak and Bennin, Kwabena Ebo},
title = {Inter-release defect prediction with feature selection using temporal chunk-based learning: An empirical study},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107870},
doi = {10.1016/j.asoc.2021.107870},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {17},
keywords = {Software defect prediction, Inter-release defect prediction, Feature selection}
}

@article{10.1007/s00500-018-3546-6,
author = {Khuat, Thanh Tung and Le, My Hanh},
title = {Binary teaching–learning-based optimization algorithm with a new update mechanism for sample subset optimization in software defect prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {20},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3546-6},
doi = {10.1007/s00500-018-3546-6},
abstract = {Software defect prediction has gained considerable attention in recent years. A broad range of computational methods has been developed for accurate prediction of faulty modules based on code and design metrics. One of the challenges in training classifiers is the highly imbalanced class distribution in available datasets, leading to an undesirable bias in the prediction performance for the minority class. Data sampling is a widespread technique to tackle this problem. However, traditional sampling methods, which depend mainly on random resampling from a given dataset, do not take advantage of useful information available in training sets, such as sample quality and representative instances. To cope with this limitation, evolutionary undersampling methods are usually used for identifying an optimal sample subset for the training dataset. This paper proposes a binary teaching–learning- based optimization algorithm employing a distribution-based solution update rule, namely BTLBOd, to generate a balanced subset of highly valuable examples. This subset is then applied to train a classifier for reliable prediction of potentially defective modules in a software system. Each individual in BTLBOd includes two vectors: a real-valued vector generated by the distribution-based update mechanism, and a binary vector produced from the corresponding real vector by a proposed mapping function. Empirical results showed that the optimal sample subset produced by BTLBOd might ameliorate the classification accuracy of the predictor on highly imbalanced software defect data. Obtained results also demonstrated the superior performance of the proposed sampling method compared to other popular sampling techniques.},
journal = {Soft Comput.},
month = oct,
pages = {9919–9935},
numpages = {17},
keywords = {Teaching–learning-based optimization, Binary teaching–learning-based optimization, Distribution-based update, Sample subset optimization, Imbalanced learning, Software defect prediction}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3239576.3239607,
author = {Du, Yuntao and Zhang, Lu and Shi, Jiahao and Tang, Jingjuan and Yin, Ying},
title = {Feature-Grouping-Based Two Steps Feature Selection Algorithm in Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239607},
doi = {10.1145/3239576.3239607},
abstract = {In order to improve the effect of software defect prediction, many algorithms including feature selection, have been proposed. Based on Wrapper and Filter hybrid framework, a feature-grouping-based feature selection algorithm is proposed in this paper. The algorithm is composed of two steps. In the first step, in order to remove the redundant features, we group the features according to the redundancy between the features. The symmetry uncertainty is used as the constant indicator of the correlation and the FCBF-based grouping algorithm is used to group the features. In the second step, a subset of the features are selected from each group to form the final subset of features. Many classical methods select the representative feature from each group. We consider that when the number of intra-group features is large, the representative features are not enough to reflect the information in this group. Therefore, we require that at least one feature be selected within each group, in this step, the PSO algorithm is used for Searching Randomly from each group. We tested on the open source NASA and PROMISE data sets. Using three kinds of classifier. Compared to the other methods tested in this article, our method resulted in 90% improvement in the predictive performance of 30 sets of results on 10 data sets. Compared with the algorithms without feature selection, the AUC values of this method in the Logistic regression, Naive Bayesian, and K-neighbor classifiers are improved by 5.94% and 4.69% And 8.05%. The FCBF algorithm can also be regarded as a kind of first performing feature grouping. Compared with the FCBF algorithm, the AUC values of this method are improved by 4.78%, 6.41% and 4.4% on the basis of Logistic regression, Naive Bayes and K-neighbor. We can also see that for the FCBF-based grouping algorithm, it could be better to choose a characteristic cloud from each group than to choose a representative one.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {173–178},
numpages = {6},
keywords = {FCBF-based grouping algorithm, Feature grouping, Intra-group feature selection, PSO, Software defect prediction},
location = {Chengdu, China},
series = {ICAIP '18}
}

@article{10.1016/j.ins.2013.12.031,
author = {Czibula, Gabriela and Marian, Zsuzsanna and Czibula, Istvan Gergely},
title = {Software defect prediction using relational association rule mining},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {264},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.12.031},
doi = {10.1016/j.ins.2013.12.031},
abstract = {This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source NASA datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal.},
journal = {Inf. Sci.},
month = apr,
pages = {260–278},
numpages = {19},
keywords = {Association rule, Data mining, Defect prediction, Software engineering}
}

@article{10.1016/j.neunet.2019.05.022,
author = {Zhao, Haitao and Lai, Zhihui and Chen, Yudong},
title = {Global-and-local-structure-based neural network for fault detection},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {118},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.05.022},
doi = {10.1016/j.neunet.2019.05.022},
journal = {Neural Netw.},
month = oct,
pages = {43–53},
numpages = {11},
keywords = {Statistical process monitoring, Fault detection, Feedforward neural network, Principal component analysis, Dimension reduction}
}

@article{10.1016/j.knosys.2015.10.009,
author = {Rana, Zeeshan Ali and Mian, M. Awais and Shamail, Shafay},
title = {Improving Recall of software defect prediction models using association mining},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.10.009},
doi = {10.1016/j.knosys.2015.10.009},
abstract = {Use of software product metrics in defect prediction studies highlights the utility of these metrics. Public availability of software defect data based on the product metrics has resulted in the development of defect prediction models. These models experience a limitation in learning Defect-prone (D) modules because the available datasets are imbalanced. Most of the datasets are dominated by Not Defect-prone (ND) modules as compared to D modules. This affects the ability of classification models to learn the D modules more accurately. This paper presents an association mining based approach that allows the defect prediction models to learn D modules in imbalanced datasets. The proposed algorithm preprocesses data by setting specific metric values as missing and improves the prediction of D modules. The proposed algorithm has been evaluated using 5 public datasets. A Naive Bayes (NB) classifier has been developed before and after the proposed preprocessing. It has been shown that Recall of the classifier after the proposed preprocessing has improved. Stability of the approach has been tested by experimenting the algorithm with different number of bins. The results show that the algorithm has resulted in up to 40% performance gain.},
journal = {Know.-Based Syst.},
month = dec,
pages = {1–13},
numpages = {13},
keywords = {Association mining, Imbalanced data, Improving Recall, Naive Bayes, PROMISE repository, Software defect prediction}
}

@article{10.1007/s10664-021-09984-2,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Weighted software metrics aggregation and its application to defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09984-2},
doi = {10.1007/s10664-021-09984-2},
abstract = {It is a well-known practice in software engineering to aggregate software metrics to assess software artifacts for various purposes, such as their maintainability or their proneness to contain bugs. For different purposes, different metrics might be relevant. However, weighting these software metrics according to their contribution to the respective purpose is a challenging task. Manual approaches based on experts do not scale with the number of metrics. Also, experts get confused if the metrics are not independent, which is rarely the case. Automated approaches based on supervised learning require reliable and generalizable training data, a ground truth, which is rarely available. We propose an automated approach to weighted metrics aggregation that is based on unsupervised learning. It sets metrics scores and their weights based on probability theory and aggregates them. To evaluate the effectiveness, we conducted two empirical studies on defect prediction, one on ca. 200 000 code changes, and another ca. 5 000 software classes. The results show that our approach can be used as an agnostic unsupervised predictor in the absence of a ground truth.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {34},
keywords = {Software assessment, Quantitative methods, Defect prediction, Software metrics, Aggregation, Weighting}
}

@article{10.1016/j.asoc.2017.01.050,
author = {Maua, Goran and Galinac Grbac, Tihana},
title = {Co-evolutionary multi-population genetic programming for classification in software defect prediction},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.01.050},
doi = {10.1016/j.asoc.2017.01.050},
abstract = {Evolving diverse ensembles using genetic programming has recently been proposed for classification problems with unbalanced data. Population diversity is crucial for evolving effective algorithms. Multilevel selection strategies that involve additional colonization and migration operations have shown better performance in some applications. Therefore, in this paper, we are interested in analysing the performance of evolving diverse ensembles using genetic programming for software defect prediction with unbalanced data by using different selection strategies. We use colonization and migration operators along with three ensemble selection strategies for the multi-objective evolutionary algorithm. We compare the performance of the operators for software defect prediction datasets with varying levels of data imbalance. Moreover, to generalize the results, gain a broader view and understand the underlying effects, we replicated the same experiments on UCI datasets, which are often used in the evolutionary computing community. The use of multilevel selection strategies provides reliable results with relatively fast convergence speeds and outperforms the other evolutionary algorithms that are often used in this research area and investigated in this paper. This paper also presented a promising ensemble strategy based on a simple convex hull approach and at the same time it raised the question whether ensemble strategy based on the whole population should also be investigated.},
journal = {Appl. Soft Comput.},
month = jun,
pages = {331–351},
numpages = {21},
keywords = {Classification, Coevolution, Genetic programming, Software defect prediction}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {Software defect prediction, Deep feature representation, Triplet loss, Weighted cross-entropy loss, Deep neural network, 00-01, 99-00}
}

@article{10.1007/s11219-020-09515-0,
author = {Ferenc, Rudolf and T\'{o}th, Zolt\'{a}n and Lad\'{a}nyi, Gergely and Siket, Istv\'{a}n and Gyim\'{o}thy, Tibor},
title = {A public unified bug dataset for java and its assessment regarding metrics and bug prediction},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09515-0},
doi = {10.1007/s11219-020-09515-0},
abstract = {Bug datasets have been created and used by many researchers to build and validate novel bug prediction models. In this work, our aim is to collect existing public source code metric-based bug datasets and unify their contents. Furthermore, we wish to assess the plethora of collected metrics and the capabilities of the unified bug dataset in bug prediction. We considered 5 public datasets and we downloaded the corresponding source code for each system in the datasets and performed source code analysis to obtain a common set of source code metrics. This way, we produced a unified bug dataset at class and file level as well. We investigated the diversion of metric definitions and values of the different bug datasets. Finally, we used a decision tree algorithm to show the capabilities of the dataset in bug prediction. We found that there are statistically significant differences in the values of the original and the newly calculated metrics; furthermore, notations and definitions can severely differ. We compared the bug prediction capabilities of the original and the extended metric suites (within-project learning). Afterwards, we merged all classes (and files) into one large dataset which consists of 47,618 elements (43,744 for files) and we evaluated the bug prediction model build on this large dataset as well. Finally, we also investigated cross-project capabilities of the bug prediction models and datasets. We made the unified dataset publicly available for everyone. By using a public unified dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.},
journal = {Software Quality Journal},
month = dec,
pages = {1447–1506},
numpages = {60},
keywords = {Bug dataset, Code metrics, Static code analysis, Bug prediction}
}

@inproceedings{10.1007/978-3-030-91452-3_12,
author = {Amasaki, Sousuke and Aman, Hirohisa and Yokogawa, Tomoyuki},
title = {Searching for Bellwether Developers for Cross-Personalized Defect Prediction},
year = {2021},
isbn = {978-3-030-91451-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91452-3_12},
doi = {10.1007/978-3-030-91452-3_12},
abstract = {Context: Recent progress in the use of commit data for software defect prediction has driven research on personalized defect prediction. An idea applying one personalized model to another developer came in for seeking an alternative model predicting better than one’s own model. A question arose whether such exemplary developer (bellwether) existed as observed in traditional defect prediction. Objective: To investigate whether bellwether developers existed and how they behaved. Method: Experiments were conducted on 9 OSS projects. Models based on active developers in a project were compared with each other to seek bellwethers, whose models beaten models of the other active developers. Their performance was evaluated with new unseen data from the other active developers and the remaining non-active developers. Results: Bellwether developers were identified in all nine projects. Their performance on new unseen data from the other active developers was not higher than models learned by those developers. The bellwether was only a practical choice for the non-active developers. Conclusion: Bellwethers were a useful prediction model for the non-active developers but not for the other active developers.},
booktitle = {Product-Focused Software Process Improvement: 22nd International Conference, PROFES 2021, Turin, Italy, November 26, 2021, Proceedings},
pages = {183–198},
numpages = {16},
keywords = {Personalized defect prediction, Transfer learning, Bellwether effect},
location = {Turin, Italy}
}

@article{10.1016/j.jss.2016.09.001,
author = {Andreou, Andreas S. and Chatzis, Sotirios P.},
title = {Software defect prediction using doubly stochastic Poisson processes driven by stochastic belief networks},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.001},
doi = {10.1016/j.jss.2016.09.001},
abstract = {This research aims at better addressing the challenges related with software defect prediction.We develop a novel Bayesian inference approach driven from appropriate metrics.Formulation of our method is based on a doubly stochastic homogeneous Poisson process.Our model better learns from data with multiple modes in their distributions.We evaluate generalization across software classes, subsequent releases, and projects. Accurate prediction of software defects is of crucial importance in software engineering. Software defect prediction comprises two major procedures: (i) Design of appropriate software metrics to represent characteristic software system properties; and (ii) development of effective regression models for count data, allowing for accurate prediction of the number of software defects. Although significant research effort has been devoted to software metrics design, research in count data regression has been rather limited. More specifically, most used methods have not been explicitly designed to tackle the problem of metrics-driven software defect counts prediction, thus postulating irrelevant assumptions, such as (log-)linearity of the modeled data. In addition, a lack of simple and efficient algorithms for posterior computation has made more elaborate hierarchical Bayesian approaches appear unattractive in the context of software defect prediction. To address these issues, in this paper we introduce a doubly stochastic Poisson process for count data regression, the failure log-rate of which is driven by a novel latent space stochastic feedforward neural network. Our approach yields simple and efficient updates for its complicated conditional distributions by means of sampling importance resampling and error backpropagation. We exhibit the efficacy of our approach using publicly available and benchmark datasets.},
journal = {J. Syst. Softw.},
month = dec,
pages = {72–82},
numpages = {11},
keywords = {Doubly stochastic Poisson process, Sampling importance resampling, Software defect prediction, Stochastic belief network}
}

@article{10.1145/3384517,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {A Defect Estimator for Source Code: Linking Defect Reports with Programming Constructs Usage Metrics},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3384517},
doi = {10.1145/3384517},
abstract = {An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost.We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python.The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {12},
numpages = {35},
keywords = {AI in software engineering, Maintaining software, automated software engineering, software defect prediction, software faults and failures, software metrics, source code mining}
}

@inproceedings{10.1145/3368089.3417062,
author = {Suh, Alexander},
title = {Adapting bug prediction models to predict reverted commits at Wayfair},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417062},
doi = {10.1145/3368089.3417062},
abstract = {Researchers have proposed many algorithms to predict software bugs. Given a software entity (e.g., a file or method), these algorithms predict whether the entity is bug-prone. However, since these algorithms cannot identify specific bugs, this does not tend to be particularly useful in practice. In this work, we adapt this prior work to the related problem of predicting whether a commit is likely to be reverted. Given the batch nature of continuous integration deployment at scale, this allows developers to find time-sensitive bugs in production more quickly. The models in this paper are based on features extracted from the revision history of a codebase that are typically used in bug prediction. Our experiments, performed on the three main repositories for the Wayfair website, show that our models can rank reverted commits above 80% of non-reverted commits on average. Moreover, when given to Wayfair developers, our models reduce the amount of time needed to find certain kinds of bugs by 55%. Wayfair continues to use our findings and models today to help find bugs during software deployments.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1251–1262},
numpages = {12},
keywords = {reverted commits, software defect prediction, software deployment},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1007/978-3-030-78609-0_28,
author = {Sun, Ying and Sun, Yanfei and Wu, Fei and Jing, Xiao-Yuan},
title = {Deep Adversarial Learning Based Heterogeneous Defect Prediction},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_28},
doi = {10.1007/978-3-030-78609-0_28},
abstract = {Cross-project defect prediction (CPDP) is a hot study that predicts defects in the new project by utilizing the model trained on the data from other projects. However, existing CPDP methods usually assume that source and target projects have the same metrics. Heterogeneous defect prediction (HDP) is proposed and has attracted increasing attention, which refers to the metric sets from source and target projects are different in CPDP. HDP conducts prediction model using the instances with heterogeneous metrics from external projects and then use this model to predict defect-prone software instances in source project. However, building HDP methods is challenging including the distribution difference between source and target projects with heterogeneous metrics. In this paper, we propose a Deep adversarial learning based HDP (DHDP) approach. DHDP leverages deep neural network to learn nonlinear transformation for each project to obtain common feature represent, which the heterogeneous data from different projects can be compared directly. DHDP consists of two parts: a discriminator and a classifier that compete with each other. A classifier tries to minimize the similarity across classes and maximize the inter-class similarity. A discriminator tries to distinguish the source of instances that is source or target project on the common feature space. Expensive experiments are performed on 10 public projects from two datasets in terms of F-measure and G-measure. The experimental results show that DHDP gains superior prediction performance improvement compared to a range of competing methods.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {326–337},
numpages = {12},
keywords = {Adversarial learning, Metric learning, Heterogeneous defect prediction},
location = {Dublin, Ireland}
}

@article{10.1016/j.asoc.2020.106686,
author = {Haouari, Ahmed Taha and Souici-Meslati, Labiba and Atil, Fadila and Meslati, Djamel},
title = {Empirical comparison and evaluation of Artificial Immune Systems in inter-release software fault prediction},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {96},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106686},
doi = {10.1016/j.asoc.2020.106686},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {18},
keywords = {Artificial Immune Systems, Software defect prediction, Inter-projects fault prediction, Artificial Immune Recognition System, Neural Network}
}

@article{10.1007/s10664-012-9218-8,
author = {Okutan, Ahmet and Y\i{}ld\i{}z, Olcay Taner},
title = {Software defect prediction using Bayesian networks},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9218-8},
doi = {10.1007/s10664-012-9218-8},
abstract = {There are lots of different software metrics discovered and used for defect prediction in the literature. Instead of dealing with so many metrics, it would be practical and easy if we could determine the set of metrics that are most important and focus on them more to predict defectiveness. We use Bayesian networks to determine the probabilistic influential relationships among software metrics and defect proneness. In addition to the metrics used in Promise data repository, we define two more metrics, i.e. NOD for the number of developers and LOCQ for the source code quality. We extract these metrics by inspecting the source code repositories of the selected Promise data repository data sets. At the end of our modeling, we learn the marginal defect proneness probability of the whole software system, the set of most effective metrics, and the influential relationships among metrics and defectiveness. Our experiments on nine open source Promise data repository data sets show that response for class (RFC), lines of code (LOC), and lack of coding quality (LOCQ) are the most effective metrics whereas coupling between objects (CBO), weighted method per class (WMC), and lack of cohesion of methods (LCOM) are less effective metrics on defect proneness. Furthermore, number of children (NOC) and depth of inheritance tree (DIT) have very limited effect and are untrustworthy. On the other hand, based on the experiments on Poi, Tomcat, and Xalan data sets, we observe that there is a positive correlation between the number of developers (NOD) and the level of defectiveness. However, further investigation involving a greater number of projects is needed to confirm our findings.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {154–181},
numpages = {28},
keywords = {Bayesian networks, Defect prediction}
}

@inproceedings{10.1145/3460319.3464819,
author = {Zeng, Zhengran and Zhang, Yuqun and Zhang, Haotian and Zhang, Lingming},
title = {Deep just-in-time defect prediction: how far are we?},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464819},
doi = {10.1145/3460319.3464819},
abstract = {Defect prediction aims to automatically identify potential defective code with minimal human intervention and has been widely studied in the literature. Just-in-Time (JIT) defect prediction focuses on program changes rather than whole programs, and has been widely adopted in continuous testing. CC2Vec, state-of-the-art JIT defect prediction tool, first constructs a hierarchical attention network (HAN) to learn distributed vector representations of both code additions and deletions, and then concatenates them with two other embedding vectors representing commit messages and overall code changes extracted by the existing DeepJIT approach to train a model for predicting whether a given commit is defective. Although CC2Vec has been shown to be the state of the art for JIT defect prediction, it was only evaluated on a limited dataset and not compared with all representative baselines. Therefore, to further investigate the efficacy and limitations of CC2Vec, this paper performs an extensive study of CC2Vec on a large-scale dataset with over 310,370 changes (8.3 X larger than the original CC2Vec dataset). More specifically, we also empirically compare CC2Vec against DeepJIT and representative traditional JIT defect prediction techniques. The experimental results show that CC2Vec cannot consistently outperform DeepJIT, and neither of them can consistently outperform traditional JIT defect prediction. We also investigate the impact of individual traditional defect prediction features and find that the added-line-number feature outperforms other traditional features. Inspired by this finding, we construct a simplistic JIT defect prediction approach which simply adopts the added-line-number feature with the logistic regression classifier. Surprisingly, such a simplistic approach can outperform CC2Vec and DeepJIT in defect prediction, and can be 81k X/120k X faster in training/testing. Furthermore, the paper also provides various practical guidelines for advancing JIT defect prediction in the near future.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {427–438},
numpages = {12},
keywords = {Deep Learning, Just-In-Time Prediction, Software Defect Prediction},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1007/s10664-021-09965-5,
author = {Qu, Yu and Yin, Heng},
title = {Evaluating network embedding techniques’ performances in software bug prediction},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09965-5},
doi = {10.1007/s10664-021-09965-5},
abstract = {Software bug prediction techniques can be very helpful in testing and code inspection. Over the past decade, network measures have been successfully used in bug prediction. Following the same intuition, recently, researchers started using network embedding techniques in bug prediction. However, existing studies only evaluated the Skip-gram and CBOW models with random walk. Considering network embedding is a fast-developing research direction, it is important to evaluate other network embedding techniques’ performances in bug prediction. Moreover, existing studies have not investigated the application and performance of network embedding in effort-aware bug prediction, which is thought to be a more realistic scenario that evaluates the cost effectiveness of bug prediction models. In this paper, we conduct an extensive empirical study to evaluate network embedding algorithms in bug prediction by utilizing and extending node2defect, a newly proposed bug prediction model that combines the embedded vectors with traditional software engineering metrics through concatenation. Experiments are conducted based on seven network embedding algorithms, two effort-aware models, and 13 open-source Java systems. Experimental results show that node2defect outperforms traditional metrics by + 14.64% in terms of MCC score, and by + 7.51% to + 16.57% in effort-aware bug prediction. More interestingly, when combined with CBS + , the embedded vectors alone can achieve the best performance. Among different network embedding algorithms, the newly proposed algorithm ProNE has the best performance.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {44},
keywords = {Software bug, Bug prediction, Network embedding, Network representation learning, Effort-aware bug prediction, Empirical study}
}

@inproceedings{10.1145/2961111.2962620,
author = {Shippey, Thomas and Hall, Tracy and Counsell, Steve and Bowes, David},
title = {So You Need More Method Level Datasets for Your Software Defect Prediction? Voil\`{a}!},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962620},
doi = {10.1145/2961111.2962620},
abstract = {Context: Defect prediction research is based on a small number of defect datasets and most are at class not method level. Consequently our knowledge of defects is limited. Identifying defect datasets for prediction is not easy and extracting quality data from identified datasets is even more difficult. Goal: Identify open source Java systems suitable for defect prediction and extract high quality fault data from these datasets. Method: We used the Boa to identify candidate open source systems. We reduce 50,000 potential candidates down to 23 suitable for defect prediction using a selection criteria based on the system's software repository and its defect tracking system. We use an enhanced SZZ algorithm to extract fault information and calculate metrics using JHawk. Result: We have produced 138 fault and metrics datasets for the 23 identified systems. We make these datasets (the ELFF datasets) and our data extraction tools freely available to future researchers. Conclusions: The data we provide enables future studies to proceed with minimal effort. Our datasets significantly increase the pool of systems currently being used in defect analysis studies.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {12},
numpages = {6},
keywords = {Boa, Data Mining, Defect Prediction, Defect linking, Defects},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1007/978-3-319-49586-6_11,
author = {Siers, Michael J. and Islam, Md Zahidul},
title = {Addressing Class Imbalance and Cost Sensitivity in Software Defect Prediction by Combining Domain Costs and Balancing Costs},
year = {2016},
isbn = {978-3-319-49585-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-49586-6_11},
doi = {10.1007/978-3-319-49586-6_11},
abstract = {Effective methods for identification of software defects help minimize the business costs of software development. Classification methods can be used to perform software defect prediction. When cost-sensitive methods are used, the predictions are optimized for business cost. The data sets used as input for these methods typically suffer from the class imbalance problem. That is, there are many more defect-free code examples than defective code examples to learn from. This negatively impacts the classifier’s ability to correctly predict defective code examples. Cost-sensitive classification can also be used to mitigate the affects of the class imbalance problem by setting the costs to reflect the level of imbalance in the training data set. Through an experimental process, we have developed a method for combining these two different types of costs. We demonstrate that by using our proposed approach, we can produce more cost effective predictions than several recent cost-sensitive methods used for software defect prediction. Furthermore, we examine the software defect prediction models built by our method and present the discovered insights.},
booktitle = {Advanced Data Mining and Applications: 12th International Conference, ADMA 2016, Gold Coast, QLD, Australia, December 12-15, 2016, Proceedings},
pages = {156–171},
numpages = {16},
keywords = {Cost sensitive, Software defect prediction, Class imbalance},
location = {Gold Coast, Australia}
}

@phdthesis{10.5555/AAI28414631,
author = {Gatling, Teia C. and Blackburn, Timothy},
advisor = {Oluwatomi, Adetunji, and Amirhossein, Etemadi,},
title = {Applying Documentation Metrics in Cross Version Defect Prediction Modeling},
year = {2021},
isbn = {9798708754837},
publisher = {The George Washington University},
abstract = {Software documentation such as documented Application Programming Interface (API) and comments embedded with the software code aid in faster debugging of software defects. The existence of this documentation is used as a measurement of software quality. Over the last 40 years, a series of object-oriented metrics-based defect prediction models have been successfully developed. However, documentation metrics in combination with object-oriented metrics in software defect prediction modeling has not been explored in predicting defects. By leveraging a publicly available GitHub dataset that contains both documentation and object-oriented metrics and applying the cross version defect prediction approach, this research determined documentation metrics impact on defects across a project and developed a predictive model using these metric in combination with baseline metrics to improve model performance. As a result, Boosting ensemble method returned improved model performance when combining the documentation metrics with commonly used object-oriented code metrics. Likewise, the Random Forest returned an improved model when using a feature subset. Random Forest using a subset of metrics provided the most promising results with F-Measure performance improvement of 8.9 percent. The results of this research highlight quantitatively the impact documentation metrics have on software defect prediction and that model performance can improve when identifying a subset of metrics. The results also demonstrate the use of data from three previous versions versus solely using the latest version, the models perform within an average of five percentage points of each other. This knowledge can be leveraged by managers to enhance the application of documentation throughout the lifecycle of software.},
note = {AAI28414631}
}

@inproceedings{10.1145/3183440.3183449,
author = {Eken, Beyza},
title = {Assessing personalized software defect predictors},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183449},
doi = {10.1145/3183440.3183449},
abstract = {Software defect prediction models guide developers and testers to identify defect prone software modules in fewer time and effort, compared to manual inspections of the source code. The state-of-the-art predictors on publicly available software engineering data could catch around 70% of the defects. While early studies mostly utilize static code properties of the software, recent studies incorporate the people factor into the prediction models, such as the number of developers that touched a code unit, the experience of the developer, and interaction and cognitive behaviors of developers. Those information could give a stronger clue about the defect-prone parts because they could explain defect injection patterns in software development. Personalization has been emerging in many other systems such as social platforms, web search engines such that people get customized recommendations based on their actions, profiles and interest. Following this point of view, customization in defect prediction with respect to each developer would increase predictions' accuracy and usefulness than traditional, general models. In this thesis, we focus on building a personalized defect prediction framework that gives instant feedback to the developer at change level, based on historical defect and change data. Our preliminary analysis of the personalized prediction models of 121 developers in six open source projects indicate that, a personalized approach is not always the best model when compared to general models built for six projects. Other factors such as project characteristics, developer's historical data, the context and frequency of contributions, and/or development methodologies might affect which model to consider in practice. Eventually, this topic is open to improvement with further empirical studies on each of these factors.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {488–491},
numpages = {4},
keywords = {bug prediction, customization, personalized defect prediction},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.eswa.2019.113156,
author = {Majd, Amirabbas and Vahidi-Asl, Mojtaba and Khalilian, Alireza and Poorsarvi-Tehrani, Pooria and Haghighi, Hassan},
title = {SLDeep: Statement-level software defect prediction using deep-learning model on static code features},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {147},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113156},
doi = {10.1016/j.eswa.2019.113156},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {14},
keywords = {Defect, Software fault proneness, Machine learning, Fault prediction model, Software metric}
}

@article{10.1016/j.jss.2019.110493,
author = {Pascarella, Luca and Palomba, Fabio and Bacchelli, Alberto},
title = {On the performance of method-level bug prediction: A negative result},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110493},
doi = {10.1016/j.jss.2019.110493},
journal = {J. Syst. Softw.},
month = mar,
numpages = {15},
keywords = {Defect prediction, Empirical software engineering, Mining software repositories}
}

@article{10.1016/j.infsof.2011.09.007,
author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
title = {Transfer learning for cross-company software defect prediction},
year = {2012},
issue_date = {March, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.007},
doi = {10.1016/j.infsof.2011.09.007},
abstract = {Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Different distribution, Machine learning, Naive Bayes, Software defect prediction, Transfer learning}
}

@inproceedings{10.1145/2875913.2875944,
author = {Qing, He and Biwen, Li and Beijun, Shen and Xia, Yong},
title = {Cross-Project Software Defect Prediction Using Feature-Based Transfer Learning},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875944},
doi = {10.1145/2875913.2875944},
abstract = {Cross-project defect prediction is taken as an effective means of predicting software defects when the data shortage exists in the early phase of software development. Unfortunately, the precision of cross-project defect prediction is usually poor, largely because of the differences between the reference and the target projects. Having realized the project differences, this paper proposes CPDP, a feature-based transfer learning approach to cross-project defect prediction. The core insight of CPDP is to (1) filter and transfer highly-correlated data based on data samples in the target projects, and (2) evaluate and choose learning schemas for transferring data sets. Models are then built for predicting defects in the target projects. We have also conducted an evaluation of the proposed approach on PROMISE datasets. The evaluation results show that, the proposed approach adapts to cross-project defect prediction in that f-measure of 81.8% of projects can get improved, and AUC of 54.5% projects improved. It also achieves similar f-measure and AUC as some inner-project defect prediction approaches.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {74–82},
numpages = {9},
keywords = {cross-project defect prediction, feature-based transfer, transfer learning},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1145/3172871.3172872,
author = {Kumar, Lov and Sureka, Ashish},
title = {Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172872},
doi = {10.1145/3172871.3172872},
abstract = {Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {11},
keywords = {Aging Related Bugs, Empirical Software Engineering, Feature Selection Techniques, Imbalance Learning, Machine Learning, Predictive Modeling, Software Maintenance, Source Code Metrics},
location = {Hyderabad, India},
series = {ISEC '18}
}

@article{10.1145/3467895,
author = {Falessi, Davide and Ahluwalia, Aalok and Penta, Massimiliano DI},
title = {The Impact of Dormant Defects on Defect Prediction: A Study of 19 Apache Projects},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3467895},
doi = {10.1145/3467895},
abstract = {Defect prediction models can be beneficial to prioritize testing, analysis, or code review activities, and has been the subject of a substantial effort in academia, and some applications in industrial contexts. A necessary precondition when creating a defect prediction model is the availability of defect data from the history of projects. If this data is noisy, the resulting defect prediction model could result to be unreliable. One of the causes of noise for defect datasets is the presence of “dormant defects,” i.e., of defects discovered several releases after their introduction. This can cause a class to be labeled as defect-free while it is not, and is, therefore “snoring.” In this article, we investigate the impact of snoring on classifiers' accuracy and the effectiveness of a possible countermeasure, i.e., dropping too recent data from a training set. We analyze the accuracy of 15 machine learning defect prediction classifiers, on data from more than 4,000 defects and 600 releases of 19 open source projects from the Apache ecosystem. Our results show that on average across projects (i) the presence of dormant defects decreases the recall of defect prediction classifiers, and (ii) removing from the training set the classes that in the last release are labeled as not defective significantly improves the accuracy of the classifiers. In summary, this article provides insights on how to create defects datasets by mitigating the negative effect of dormant defects on defect prediction.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {4},
numpages = {26},
keywords = {Defect prediction, fix-inducing changes, dataset bias}
}

@article{10.1007/s11219-020-09538-7,
author = {Eken, Beyza and Palma, Francis and Ay\c{s}e, Ba\c{s}ar and Ay\c{s}e, Tosun},
title = {An empirical study on the effect of community smells on bug prediction},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09538-7},
doi = {10.1007/s11219-020-09538-7},
abstract = {Community-aware metrics through socio-technical developer networks or organizational structures have already been studied in the&nbsp;software bug prediction field. Community smells are also proposed to identify communication and collaboration patterns in developer communities. Prior work reports a statistical association between community smells and code smells identified in software modules. We investigate the contribution of community smells on predicting bug-prone classes and compare their contribution with that of code smell-related information and state-of-the-art process metrics. We conduct our empirical analysis on ten open-source projects with varying sizes, buggy and smelly class ratios. We build seven different bug prediction models to answer three RQs: a baseline model including a state-of-the-art metric set used, three models incorporating a particular metric set, namely community smells, code smells, code smell intensity, into the baseline, and three models incorporating a combination of smell-related metrics into the baseline. The performance of these models is reported in terms of recall, false positive rates, F-measure and AUC and statistically compared using Scott–Knott ESD tests. Community smells improve the prediction performance of a baseline model by up to 3% in terms of AUC, while code smell intensity improves the baseline models by up to 40% in terms of F-measure and up to 17% in terms of AUC. The conclusions are significantly influenced by the validation strategies used, algorithms and the selected projects’ data characteristics. While the code smell intensity metric captures the most information about technical flaws in predicting bug-prone classes, the community smells also contribute to bug prediction models by revealing communication and collaboration flaws in software development teams. Future research is needed to capture the communication patterns through multiple channels and to understand whether socio-technical flaws could be used in a cross-project bug prediction setting.},
journal = {Software Quality Journal},
month = mar,
pages = {159–194},
numpages = {36},
keywords = {Community smells, Bug prediction, Mining software repositories}
}

@article{10.1016/j.procs.2018.05.115,
author = {Singh, Ajmer and Bhatia, Rajesh and Singhrova, Anita},
title = {Taxonomy of machine learning algorithms in software fault prediction using object oriented metrics},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.115},
doi = {10.1016/j.procs.2018.05.115},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {993–1001},
numpages = {9},
keywords = {Software fault prediction, Object Oriented Testing, Object Oriented Coupling, software faults prediction, machine learning}
}

@inproceedings{10.1145/2491411.2494581,
author = {Zhang, Hongyu and Cheung, S. C.},
title = {A cost-effectiveness criterion for applying software defect prediction models},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2494581},
doi = {10.1145/2491411.2494581},
abstract = {Ideally, software defect prediction models should help organize software quality assurance (SQA) resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first. In this paper, we study the cost-effectiveness of applying defect prediction models in SQA and propose a basic cost-effectiveness criterion. The criterion implies that defect prediction models should be applied with caution. We also propose a new metric FN/(FN+TN) to measure the cost-effectiveness of a defect prediction model.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {643–646},
numpages = {4},
keywords = {Defect prediction, cost effectiveness, evaluation metrics},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/3273934.3273938,
author = {Amasaki, Sousuke},
title = {Cross-Version Defect Prediction using Cross-Project Defect Prediction Approaches: Does it work?},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273938},
doi = {10.1145/3273934.3273938},
abstract = {Background: Specifying and removing defects before release deserve extra cost for the success of software projects. Long-running projects experience multiple releases, and it is a natural choice to adopt cross-version defect prediction (CVDP) that uses information from older versions. A past study shows that feeding multi older versions data may have a positive influence on the performance. The study also suggests that cross-project defect prediction (CPDP) may fit the situation but one CPDP approach was only examined.Aims: To investigate whether feeding multiple older versions data is effective for CVDP using CPDP approaches. The investigation also involves performance comparisons of the CPDP approaches under CVDP situation. Method: We chose a style of replication of the comparative study on CPDP approaches by Herbold et al. under CVDP situation. Results: Feeding multiple older versions had a positive effect for more than a half CPDP approaches. However, almost all of the CPDP approaches did not perform significantly better than a simple rule-based prediction. Although the best CPDP approach could work better than it and with-in project defect prediction, we found no effect of feeding multiple older versions for it. Conclusions: Feeding multiple older versions could improve CPDP approaches under CVDP situation. However, it did not work for the best CPDP approach in the study.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {32–41},
numpages = {10},
keywords = {Comparative Study, Cross-Project Defect Prediction, Cross-Version Defect Prediction},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1007/978-3-030-16142-2_17,
author = {Li, Heng-Yi and Li, Ming and Zhou, Zhi-Hua},
title = {Towards One Reusable Model for Various Software Defect Mining Tasks},
year = {2019},
isbn = {978-3-030-16141-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-16142-2_17},
doi = {10.1007/978-3-030-16142-2_17},
abstract = {Software defect mining is playing an important role in software quality assurance. Many deep neural network based models have been proposed for software defect mining tasks, and have pushed forward the state-of-the-art mining performance. These deep models usually require a huge amount of task-specific source code for training to capture the code functionality to mine the defects. But such requirement is often hard to be satisfied in practice. On the other hand, lots of free source code and corresponding textual explanations are publicly available in the open source software repositories, which is potentially useful in modeling code functionality. However, no previous studies ever leverage these resources to help defect mining tasks. In this paper, we propose a novel framework to learn one reusable deep model for code functional representation using the huge amount of publicly available task-free source code as well as their textual explanations. And then reuse it for various software defect mining tasks. Experimental results on three major defect mining tasks with real world datasets indicate that by reusing this model in specific tasks, the mining performance outperforms its counterpart that learns deep models from scratch, especially when the training data is insufficient.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019, Proceedings, Part III},
pages = {212–224},
numpages = {13},
keywords = {Software defect mining, Machine learning, Model reuse},
location = {Macau, China}
}

@inproceedings{10.1145/3475716.3475791,
author = {Gesi, Jiri and Li, Jiawei and Ahmed, Iftekhar},
title = {An Empirical Examination of the Impact of Bias on Just-in-time Defect Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475791},
doi = {10.1145/3475716.3475791},
abstract = {Background: Just-In-Time (JIT) defect prediction models predict if a commit will introduce defects in the future. DeepJIT and CC2Vec are two state-of-the-art JIT Deep Learning (DL) techniques. Usually, defect prediction techniques are evaluated, treating all training data equally. However, data is usually imbalanced not only in terms of the overall class label (e.g., defect and non-defect) but also in terms of characteristics such as File Count, Edit Count, Multiline Comments, Inward Dependency Sum etc. Prior research has investigated the impact of class imbalance on prediction technique's performance but not the impact of imbalance of other characteristics. Aims: We aim to explore the impact of different commit related characteristic's imbalance on DL defect prediction. Method: We investigated different characteristic's impact on the overall performance of DeepJIT and CC2Vec. We also propose a Siamese network based few-shot learning framework for JIT defect prediction (SifterJIT) combining Siamese network and DeepJIT. Results: Our results show that DeepJIT and CC2Vec lose out on the performance by around 20% when trained and tested on imbalanced data. However, SifterJIT can outperform state-of-the-art DL techniques with an average of 8.65% AUC score, 11% precision, and 6% F1-score improvement. Conclusions: Our results highlight that dataset imbalanced in terms of commit characteristics can significantly impact prediction performance, and few-shot learning based techniques can help alleviate the situation.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {7},
numpages = {12},
keywords = {Deep learning, defect prediction, few-shot learning, software engineering},
location = {Bari, Italy},
series = {ESEM '21}
}

@article{10.1007/s10664-021-09996-y,
author = {Laaber, Christoph and Basmaci, Mikael and Salza, Pasquale},
title = {Predicting unstable software benchmarks using static source code features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09996-y},
doi = {10.1007/s10664-021-09996-y},
abstract = {Software benchmarks are only as good as the performance measurements they yield. Unstable benchmarks show high variability among repeated measurements, which causes uncertainty about the actual performance and complicates reliable change assessment. However, if a benchmark is stable or unstable only becomes evident after it has been executed and its results are available. In this paper, we introduce a machine-learning-based approach to predict a benchmark’s stability without having to execute it. Our approach relies on 58 statically-computed source code features, extracted for benchmark code and code called by a benchmark, related to (1) meta information, e.g., lines of code (LOC), (2) programming language elements, e.g., conditionals or loops, and (3) potentially performance-impacting standard library calls, e.g., file and network input/output (I/O). To assess our approach’s effectiveness, we perform a large-scale experiment on 4,461 Go benchmarks coming from 230 open-source software (OSS) projects. First, we assess the prediction performance of our machine learning models using 11 binary classification algorithms. We find that Random Forest performs best with good prediction performance from 0.79 to 0.90, and 0.43 to 0.68, in terms of AUC and MCC, respectively. Second, we perform feature importance analyses for individual features and feature categories. We find that 7 features related to meta-information, slice usage, nested loops, and synchronization application programming interfaces (APIs) are individually important for good predictions; and that the combination of all features of the called source code is paramount for our model, while the combination of features of the benchmark itself is less important. Our results show that although benchmark stability is affected by more than just the source code, we can effectively utilize machine learning models to predict whether a benchmark will be stable or not ahead of execution. This enables spending precious testing time on reliable benchmarks, supporting developers to identify unstable benchmarks during development, allowing unstable benchmarks to be repeated more often, estimating stability in scenarios where repeated benchmark execution is infeasible or impossible, and warning developers if new benchmarks or existing benchmarks executed in new environments will be unstable.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {53},
keywords = {Performance testing, Software benchmarking, Performance variability, Source code features, Machine learning for software engineering, Go}
}

@inproceedings{10.1007/978-3-030-78612-0_5,
author = {Xu, Haitao and Duan, Ruifeng and Yang, Shengsong and Guo, Lei},
title = {An Empirical Study on Data Sampling for Just-in-Time Defect Prediction},
year = {2021},
isbn = {978-3-030-78611-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78612-0_5},
doi = {10.1007/978-3-030-78612-0_5},
abstract = {In this paper, the impact of Data Sampling on Just-in-Time defect prediction is explored. We find that there is a significant negative relationship between the class imbalance ratio of the dataset and the performance of the instant software defect prediction model. Secondly although most software defect data are not as unbalanced as expected, a moderate degree of imbalance is sufficient to affect the performance of traditional learning. This means that if the training data for immediate software defects show moderate or more severe imbalances, one need not expect good defect prediction performance and the data sampling approach to balancing the training data can improve the performance of the model. Finally, the empirical approach shows that although the under-sampling method slightly improves model performance, the different sampling methods do not have a substantial impact on the evaluation of immediate software defect prediction models.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part II},
pages = {54–69},
numpages = {16},
keywords = {Data sampling, Just-in-time defect, Empirical study},
location = {Dublin, Ireland}
}

@article{10.1016/j.asoc.2016.08.002,
author = {ztrk, Muhammed Maruf and Zengin, Ahmet},
title = {How repeated data points affect bug prediction performance},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.002},
doi = {10.1016/j.asoc.2016.08.002},
abstract = {Graphical abstractDisplay Omitted HighlightsPresents a novel pre-processing algorithm for defect data sets.Discusses the effects of the use of low level metrics in bug prediction.Compares repeated data points industrial versus open-source projects.Provides tips to obtain better bug prediction results. In defect prediction studies, open-source and real-world defect data sets are frequently used. The quality of these data sets is one of the main factors affecting the validity of defect prediction methods. One of the issues is repeated data points in defect prediction data sets. The main goal of the paper is to explore how low-level metrics are derived. This paper also presents a cleansing algorithm that removes repeated data points from defect data sets. The method was applied on 20 data sets, including five open source sets, and area under the curve (AUC) and precision performance parameters have been improved by 4.05% and 6.7%, respectively. In addition, this work discusses how static code metrics should be used in bug prediction. The study provides tips to obtain better defect prediction results.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1051–1061},
numpages = {11},
keywords = {Bug prediction, Repeated data, Software metrics}
}

@article{10.1007/s10664-018-9679-5,
author = {Kondo, Masanari and Bezemer, Cor-Paul and Kamei, Yasutaka and Hassan, Ahmed E. and Mizuno, Osamu},
title = {The impact of feature reduction techniques on defect prediction models},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9679-5},
doi = {10.1007/s10664-018-9679-5},
abstract = {Defect prediction is an important task for preserving software quality. Most prior work on defect prediction uses software features, such as the number of lines of code, to predict whether a file or commit will be defective in the future. There are several reasons to keep the number of features that are used in a defect prediction model small. For example, using a small number of features avoids the problem of multicollinearity and the so-called `curse of dimensionality'. Feature selection and reduction techniques can help to reduce the number of features in a model. Feature selection techniques reduce the number of features in a model by selecting the most important ones, while feature reduction techniques reduce the number of features by creating new, combined features from the original features. Several recent studies have investigated the impact of feature selection techniques on defect prediction. However, there do not exist large-scale studies in which the impact of multiple feature reduction techniques on defect prediction is investigated. In this paper, we study the impact of eight feature reduction techniques on the performance and the variance in performance of five supervised learning and five unsupervised defect prediction models. In addition, we compare the impact of the studied feature reduction techniques with the impact of the two best-performing feature selection techniques (according to prior work). The following findings are the highlights of our study: (1) The studied correlation and consistency-based feature selection techniques result in the best-performing supervised defect prediction models, while feature reduction techniques using neural network-based techniques (restricted Boltzmann machine and autoencoder) result in the best-performing unsupervised defect prediction models. In both cases, the defect prediction models that use the selected/generated features perform better than those that use the original features (in terms of AUC and performance variance). (2) Neural network-based feature reduction techniques generate features that have a small variance across both supervised and unsupervised defect prediction models. Hence, we recommend that practitioners who do not wish to choose a best-performing defect prediction model for their data use a neural network-based feature reduction technique.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1925–1963},
numpages = {39},
keywords = {Defect prediction, Feature reduction, Feature selection, Neural network, Restricted Boltzmann machine}
}

@article{10.1504/IJBIC.2018.092808,
title = {An improved twin support vector machine based on multi-objective cuckoo search for software defect prediction},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {4},
issn = {1758-0366},
url = {https://doi.org/10.1504/IJBIC.2018.092808},
doi = {10.1504/IJBIC.2018.092808},
abstract = {Recently, software defect prediction SDP has drawn much attention as software size becomes larger and consumers hold higher reliability expectations. The premise of SDP is to guide the detection of software bugs and to conserve computational resources. However, in prior research, data imbalances among software defect modules were largely ignored to focus instead on how to improve defect prediction accuracy. In this paper, a novel SDP model based on twin support vector machines TSVM and a multi-objective cuckoo search MOCS is proposed, called MOCSTSVM. We set the probability of detection and the probability of false alarm as the SDP objectives. We use TSVM to predict defected modules and employ MOCS to optimise TSVM for this dual-objective optimisation problem. To test our approach, we conduct a series of experiments on a public dataset from the PROMISE repository. The experimental results demonstrate that our approach achieves good performance compared with other SDP models.},
journal = {Int. J. Bio-Inspired Comput.},
month = jan,
pages = {282–291},
numpages = {10}
}

@inproceedings{10.1007/978-3-030-30952-7_16,
author = {Liu, Xinyue and Li, Yanhui},
title = {Is Bigger Data Better for Defect Prediction: Examining the Impact of Data Size on Supervised and Unsupervised Defect Prediction},
year = {2019},
isbn = {978-3-030-30951-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30952-7_16},
doi = {10.1007/978-3-030-30952-7_16},
abstract = {Defect prediction could help software practitioners to predict the future occurrence of bugs in the software code regions. In order to improve the accuracy of defect prediction, dozens of supervised and unsupervised methods have been put forward and achieved good results in this field. One limiting factor of defect prediction is that the data size of defect data is not big, which restricts the scope of application with defect prediction models. In this study, we try to construct bigger defect datasets by merging available datasets with the same measurement dimension and check whether bigger data will bring better defect prediction performance with supervised and unsupervised models or not. The results of our experiment reveal that larger-scale dataset doesn’t bring improvements of both supervised and unsupervised classifiers.},
booktitle = {Web Information Systems and Applications: 16th International Conference, WISA 2019, Qingdao, China, September 20-22, 2019, Proceedings},
pages = {138–150},
numpages = {13},
keywords = {Defect prediction, Supervised, Classifier, Data size},
location = {Qingdao, China}
}

@article{10.1007/s00521-021-05811-3,
author = {Mehta, Sweta and Patnaik, K. Sridhar},
title = {Improved prediction of software defects using ensemble machine learning techniques},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {16},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05811-3},
doi = {10.1007/s00521-021-05811-3},
abstract = {Software testing process is a crucial part in software development. Generally the errors made by developers get fixed at a later stage of the software development process. This increases the impact of the defect. To prevent this, defects need to be predicted during the initial days of the software development, which in turn helps in efficient utilization of the testing resources. Defect prediction process involves classification of software modules into defect prone and non-defect prone. This paper aims to reduce the impact of two major issues faced during defect prediction, i.e., data imbalance and high dimensionality of the defect datasets. In this research work, various software metrics are evaluated using feature selection techniques such as Recursive Feature Elimination (RFE), Correlation-based feature selection, Lasso, Ridge, ElasticNet and Boruta. Logistic Regression, Decision Trees, K-nearest neighbor, Support Vector Machines and Ensemble Learning are some of the algorithms in machine learning that have been used in combination with the feature extraction and feature selection techniques for classifying the modules in software as defect prone and non-defect prone. The proposed model uses combination of Partial Least Square (PLS) Regression and RFE for dimension reduction which is further combined with Synthetic Minority Oversampling Technique due to the imbalanced nature of the used datasets. It has been observed that XGBoost and Stacking Ensemble technique gave best results for all the datasets with defect prediction accuracy more than 0.9 as compared to algorithms used in the research work.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {10551–10562},
numpages = {12},
keywords = {Defect prediction, Dimension reduction, Data imbalance, Machine learning algorithms, XGBoost, Stacking ensemble classifier}
}

@inproceedings{10.1145/3412841.3442019,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Tang, Yutian and Fan, Ming and Catolino, Gemma},
title = {Just-in-time defect prediction for Android apps via imbalanced deep learning model},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442019},
doi = {10.1145/3412841.3442019},
abstract = {Android mobile apps have played important roles in our daily life and work. To meet new requirements from users, the mobile apps encounter frequent updates, which involves in a large quantity of code commits. Previous studies proposed to apply Just-in-Time (JIT) defect prediction for mobile apps to timely identify whether new code commits can introduce defects into apps, aiming to assure the quality of mobile apps. In general, the number of defective commit instances is much fewer than that of clean ones, in other words, the defect data is class imbalanced. In this work, we propose a novel Imbalanced Deep Learning model, called IDL, to conduct JIT defect prediction task for Android mobile apps. More specifically, we introduce a state-of-the-art cost-sensitive cross-entropy loss function into the deep neural network to learn the high-level feature representation, in which the loss function alleviates the class imbalance issue by taking the prior probability of the two types of classes into account. We conduct experiments on a benchmark defect data consisting of 12 Android mobile apps. The results of rigorous experiments show that our proposed IDL model performs significantly better than 23 comparative imbalanced learning methods in terms of Matthews correlation coefficient performance indicator.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1447–1454},
numpages = {8},
keywords = {JIT defect prediction, imbalanced learning, mobile apps},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1016/j.asoc.2016.04.032,
author = {Malhotra, Ruchika},
title = {An empirical framework for defect prediction using machine learning techniques with Android software},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.04.032},
doi = {10.1016/j.asoc.2016.04.032},
abstract = {Display Omitted Use of appropriate and large number data sets for comparing 18 ML techniques for defect prediction using object-oriented metrics.Effective performance of the predicted models assessed using appropriate performance measures.Reliability of the results evaluated using statistical test and post-hoc analysis.Validating the predicted models using inter-release validation on various releases of seven application packages of Android software. ContextSoftware defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results. ObjectiveThe main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets. MethodIn this study we use object-oriented metrics for predicting defective classes using 18 machine-learning techniques. The proposed framework has been applied to seven application packages of well known, widely used Android operating system viz. Contact, MMS, Bluetooth, Email, Calendar, Gallery2 and Telephony. The results are validated using 10-fold and inter-release validation methods. The reliability and significance of the results are evaluated using statistical test and post-hoc analysis. ResultsThe results show that the area under the curve measure for Nave Bayes, LogitBoost and Multilayer Perceptron is above 0.7 in most of the cases. The results also depict that the difference between the ML techniques is statistically significant. However, it is also proved that the Support Vector Machines based techniques such as Support Vector Machines and voted perceptron do not possess the predictive capability for predicting defects. ConclusionThe results confirm the predictive capability of various ML techniques for developing defect prediction models. The results also confirm the superiority of one ML technique over the other ML techniques. Thus, the software engineers can use the results obtained from this study in the early phases of the software development for identifying defect-prone classes of given software.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1034–1050},
numpages = {17},
keywords = {Inter-release validation, Machine-learning, Object-oriented metrics, Software defect proneness, Statistical tests}
}

@inproceedings{10.1145/3127005.3127017,
author = {Osman, Haidar and Ghafari, Mohammad and Nierstrasz, Oscar and Lungu, Mircea},
title = {An Extensive Analysis of Efficient Bug Prediction Configurations},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127017},
doi = {10.1145/3127005.3127017},
abstract = {Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable.Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction configurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the field where each aspect is investigated in isolation.Method: We use a cost-aware evaluation scheme to evaluate 60 different bug prediction configuration combinations on five open source Java projects.Results: We find out that the best choices for building a cost-effective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these configuration options results in the most efficient bug predictor across all subject systems.Conclusions: We demonstrate a strong evidence for the interplay among bug prediction configurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate efficient bug predictors.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {Bug Prediction, Effort-Aware Evaluation},
location = {Toronto, Canada},
series = {PROMISE}
}

@article{10.1007/s10664-020-09878-9,
author = {Bangash, Abdul Ali and Sahar, Hareem and Hindle, Abram and Ali, Karim},
title = {On the time-based conclusion stability of cross-project defect prediction models},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09878-9},
doi = {10.1007/s10664-020-09878-9},
abstract = {Researchers in empirical software engineering often make claims based on observable data such as defect reports. Unfortunately, in many cases, these claims are generalized beyond the data sets that have been evaluated. Will the researcher’s conclusions hold a year from now for the same software projects? Perhaps not. Recent studies show that in the area of Software Analytics, conclusions over different data sets are usually inconsistent. In this article, we empirically investigate whether conclusions in the area of cross-project defect prediction truly exhibit stability throughout time or not. Our investigation applies a time-aware evaluation approach where models are trained only on the past, and evaluations are executed only on the future. Through this time-aware evaluation, we show that depending on which time period we evaluate defect predictors, their performance, in terms of F-Score, the area under the curve (AUC), and Mathews Correlation Coefficient (MCC), varies and their results are not consistent. The next release of a product, which is significantly different from its prior release, may drastically change defect prediction performance. Therefore, without knowing about the conclusion stability, empirical software engineering researchers should limit their claims of performance within the contexts of evaluation, because broad claims about defect prediction performance might be contradicted by the next upcoming release of a product under analysis.},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5047–5083},
numpages = {37},
keywords = {Conclusion stability, Defect prediction, Time-aware evaluation}
}

@article{10.1007/s10462-017-9563-5,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {A study on software fault prediction techniques},
year = {2019},
issue_date = {February  2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9563-5},
doi = {10.1007/s10462-017-9563-5},
abstract = {Software fault prediction aims to identify fault-prone software modules by using some underlying properties of the software project before the actual testing process begins. It helps in obtaining desired software quality with optimized cost and effort. Initially, this paper provides an overview of the software fault prediction process. Next, different dimensions of software fault prediction process are explored and discussed. This review aims to help with the understanding of various elements associated with fault prediction process and to explore various issues involved in the software fault prediction. We search through various digital libraries and identify all the relevant papers published since 1993. The review of these papers are grouped into three classes: software metrics, fault prediction techniques, and data quality issues. For each of the class, taxonomical classification of different techniques and our observations have also been presented. The review and summarization in the tabular form are also given. At the end of the paper, the statistical analysis, observations, challenges, and future directions of software fault prediction have been discussed.},
journal = {Artif. Intell. Rev.},
month = feb,
pages = {255–327},
numpages = {73},
keywords = {Fault prediction techniques, Software fault datasets, Software fault prediction, Software metrics, Taxonomic classification}
}

@article{10.1155/2021/5558561,
author = {Shao, Yanli and Zhao, Jingru and Wang, Xingqi and Wu, Weiwei and Fang, Jinglong and Gao, Honghao},
title = {Research on Cross-Company Defect Prediction Method to Improve Software Security},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5558561},
doi = {10.1155/2021/5558561},
abstract = {As the scale and complexity of software increase, software security issues have become the focus of society. Software defect prediction (SDP) is an important means to assist developers in discovering and repairing potential defects that may endanger software security in advance and improving software security and reliability. Currently, cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) are widely studied to improve the defect prediction performance, but there are still problems such as inconsistent metrics and large differences in data distribution between source and target projects. Therefore, a new CCDP method based on metric matching and sample weight setting is proposed in this study. First, a clustering-based metric matching method is proposed. The multigranularity metric feature vector is extracted to unify the metric dimension while maximally retaining the information contained in the metrics. Then use metric clustering to eliminate metric redundancy and extract representative metrics through principal component analysis (PCA) to support one-to-one metric matching. This strategy not only solves the metric inconsistent and redundancy problem but also transforms the cross-company heterogeneous defect prediction problem into a homogeneous problem. Second, a sample weight setting method is proposed to transform the source data distribution. Wherein the statistical source sample frequency information is set as an impact factor to increase the weight of source samples that are more similar to the target samples, which improves the data distribution similarity between the source and target projects, thereby building a more accurate prediction model. Finally, after the above two-step processing, some classical machine learning methods are applied to build the prediction model, and 12 project datasets in NASA and PROMISE are used for performance comparison. Experimental results prove that the proposed method has superior prediction performance over other mainstream CCDP methods.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {19}
}

@inproceedings{10.1145/2601248.2601294,
author = {Rodriguez, Daniel and Herraiz, Israel and Harrison, Rachel and Dolado, Javier and Riquelme, Jos\'{e} C.},
title = {Preliminary comparison of techniques for dealing with imbalance in software defect prediction},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601294},
doi = {10.1145/2601248.2601294},
abstract = {Imbalanced data is a common problem in data mining when dealing with classification problems, where samples of a class vastly outnumber other classes. In this situation, many data mining algorithms generate poor models as they try to optimize the overall accuracy and perform badly in classes with very few samples. Software Engineering data in general and defect prediction datasets are not an exception and in this paper, we compare different approaches, namely sampling, cost-sensitive, ensemble and hybrid approaches to the problem of defect prediction with different datasets preprocessed differently. We have used the well-known NASA datasets curated by Shepperd et al. There are differences in the results depending on the characteristics of the dataset and the evaluation metrics, especially if duplicates and inconsistencies are removed as a preprocessing step.Further Results and replication package: http://www.cc.uah.es/drg/ease14},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {43},
numpages = {10},
keywords = {data quality, defect prediction, imbalanced data},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.1049/sfw2.12053,
author = {Huang, Qingan and Ma, Le and Jiang, Siyu and Wu, Guobin and Song, Hengjie and Jiang, Libiao and Zheng, Chunyun},
title = {A cross‐project defect prediction method based on multi‐adaptation and nuclear norm},
year = {2021},
issue_date = {April 2022},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {16},
number = {2},
url = {https://doi.org/10.1049/sfw2.12053},
doi = {10.1049/sfw2.12053},
abstract = {Cross‐project defect prediction (CPDP) is an important research direction in software defect prediction. Traditional CPDP methods based on hand‐crafted features ignore the semantic information in the source code. Existing CPDP methods based on the&nbsp;deep learning model may not fully consider the differences among projects. Additionally, these methods may not accurately classify the samples near the classification boundary. To solve these problems, the authors propose a model based on multi‐adaptation and nuclear norm (MANN) to deal with samples in projects. The feature of samples were embedded into the multi‐core Hilbert space for distribution and the multi‐kernel maximum mean discrepancy method was utilised to reduce differences among projects. More importantly, the nuclear norm module was constructed, which improved the discriminability and diversity of the target sample by calculating and maximizing the nuclear norm of the target sample in the process of domain adaptation, thus improving the performance of MANN. Finally, extensive experiments were conducted on 11 sizeable open‐source projects. The results indicate&nbsp;that the proposed method exceeds the state of the art under the widely used metrics.},
journal = {IET Software},
month = dec,
pages = {200–213},
numpages = {14},
keywords = {neural nets, software quality, software reliability, unsupervised learning}
}

@article{10.1007/s11219-021-09553-2,
author = {Wu, Jie and Wu , Yingbo and Niu, Nan and Zhou, Min},
title = {MHCPDP: multi-source heterogeneous cross-project defect prediction via multi-source transfer learning and autoencoder},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09553-2},
doi = {10.1007/s11219-021-09553-2},
abstract = {Heterogeneous cross-project defect prediction (HCPDP) is aimed at building a defect prediction model for the target project by reusing datasets from source projects, where the source project datasets and target project dataset have different features. Most existing HCPDP methods only remove redundant or unrelated features without exploring the underlying features of cross-project datasets. Additionally, when the&nbsp;transfer learning method is used in HCPDP, these methods ignore the negative effect of transfer learning. In this paper, we propose a novel HCPDP method called multi-source heterogeneous cross-project defect prediction (MHCPDP). To reduce the gap between the target datasets and the source datasets, MHCPDP uses the autoencoder to extract the intermediate features from the original datasets instead of simply removing redundant and unrelated features and adopts a modified autoencoder algorithm to make instance selection for eliminating irrelevant instances from the source domain datasets. Furthermore, by incorporating multiple source projects to increase the number of source datasets, MHCPDP develops a multi-source transfer learning algorithm to reduce the impact of negative transfers and upgrade the performance of the classifier. We comprehensively evaluate MHCPDP on five open source datasets; our experimental results show that MHCPDP not only has significant improvement in two performance metrics but also overcomes the shortcomings of the conventional HCPDP methods.},
journal = {Software Quality Journal},
month = jun,
pages = {405–430},
numpages = {26},
keywords = {Autoencoder, Heterogeneous cross-project defect prediction, Multi-source transfer learning, Modified autoencoder}
}

@inproceedings{10.1145/3379247.3379278,
author = {Ahmed, Md. Razu and Ali, Md. Asraf and Ahmed, Nasim and Zamal, Md. Fahad Bin and Shamrat, F.M. Javed Mehedi},
title = {The Impact of Software Fault Prediction in Real-World Application: An Automated Approach for Software Engineering},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379278},
doi = {10.1145/3379247.3379278},
abstract = {Software fault prediction and proneness has long been considered as a critical issue for the tech industry and software professionals. In the traditional techniques, it requires previous experience of faults or a faulty module while detecting the software faults inside an application. An automated software fault recovery models enable the software to significantly predict and recover software faults using machine learning techniques. Such ability of the feature makes the software to run more effectively and reduce the faults, time and cost. In this paper, we proposed a software defect predictive development models using machine learning techniques that can enable the software to continue its projected task. Moreover, we used different prominent evaluation benchmark to evaluate the model's performance such as ten-fold cross-validation techniques, precision, recall, specificity, f 1 measure, and accuracy. This study reports a significant classification performance of 98-100% using SVM on three defect datasets in terms of f1 measure. However, software practitioners and researchers can attain independent understanding from this study while selecting automated task for their intended application.},
booktitle = {Proceedings of 2020 6th International Conference on Computing and Data Engineering},
pages = {247–251},
numpages = {5},
keywords = {Defect prediction, Machine learning, Software engineering, Software fault},
location = {Sanya, China},
series = {ICCDE '20}
}

@inproceedings{10.1109/RAISE.2019.00016,
author = {Humphreys, Jack and Dam, Hoa Khanh},
title = {An explainable deep model for defect prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RAISE.2019.00016},
doi = {10.1109/RAISE.2019.00016},
abstract = {Self attention transformer encoders represent an effective method for sequence to class prediction tasks as they can disentangle long distance dependencies and have many regularising effects. We achieve results substantially better than state of the art in one such task, namely, defect prediction and with many added benefits. Existing techniques do not normalise for correlations that are inversely proportional to the usefulness of the prediction but do, in fact, go further, specifically exploiting these features which is tantamount to data leakage. Our model is end-to-end trainable and has the potential capability to explain its prediction. This explainability provides insights and potential causes of a model's decisions, the absence of which has stopped defect prediction from gaining any traction in industry.},
booktitle = {Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {49–55},
numpages = {7},
keywords = {deep learning, defect prediction},
location = {Montreal, Quebec, Canada},
series = {RAISE '19}
}

@article{10.3233/KES-200029,
author = {Singh, Pradeep and Verma, Shrish},
title = {ACO based comprehensive model for software fault prediction},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {1},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-200029},
doi = {10.3233/KES-200029},
abstract = {The comprehensive models can be used for software quality modelling which involves prediction of low-quality modules using interpretable rules. Such comprehensive model can guide the design and testing team to focus on the poor quality modules, thereby, limited resources allocated for software quality inspection can be targeted only towards modules that are likely to be defective. Ant Colony Optimization (ACO) based learner is one potential way to obtain rules that can classify the software modules faulty and not faulty. This paper investigates ACO based mining approach with ROC based rule quality updation to constructs a rule-based software fault prediction model with useful metrics. We have also investigated the effect of feature selection on ACO based and other benchmark algorithms. We tested the proposed method on several publicly available software fault data sets. We compared the performance of ACO based learning with the results of three benchmark classifiers on the basis of area under the receiver operating characteristic curve. The evaluation of performance measure proves that the ACO based learner outperforms other benchmark techniques.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {63–71},
numpages = {9},
keywords = {Software metric, fault prediction, ACO}
}

@article{10.1007/s00521-021-06158-5,
author = {Nevendra, Meetesh and Singh, Pradeep},
title = {Defect count prediction via metric-based convolutional neural network},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {22},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06158-5},
doi = {10.1007/s00521-021-06158-5},
abstract = {With the increasing complexity and volume of the software, the number of defects in software modules is also increasing consistently, which affects the quality and delivery of software in time and budget. To improve the software quality and timely allocation of resources, defects should be detected at the initial phases of the software development life cycle. However, the existing defect prediction methodology based on high-dimensional and limited data only focuses on predicting defective modules. In contrast, the number of defects present in the software module has not been explored so far, especially using deep neural network. Also, whether deep learning could enhance the performance of defect count prediction is still uninvestigated. To fill this gap, we proposed an improved Convolutional Neural Network model, called metrics-based convolutional neural network (MB-CNN), which combines the advantages of appropriate metrics and an improved CNN method by introducing dropout for regularization between convolutions and dense layer. The proposed method predicts the presented defect count in the software module for homogeneous scenarios as within-version and cross-version. The experimental results show that, on average, across the fourteen real-world defect datasets, the proposed approach improves Li’s CNN architecture by 31% in within-version prediction and 28% in cross-version prediction. Moreover, the Friedman ranking test and Wilcoxon nonparametric test reveal the usefulness of our proposed approach over ten benchmark learning algorithms to predict defect count.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {15319–15344},
numpages = {26},
keywords = {Software defect count prediction, Deep learning, CNN, Cross-project}
}

@article{10.1016/j.eswa.2009.12.056,
author = {Zheng, Jun},
title = {Cost-sensitive boosting neural networks for software defect prediction},
year = {2010},
issue_date = {June, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {6},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.12.056},
doi = {10.1016/j.eswa.2009.12.056},
abstract = {Software defect predictors which classify the software modules into defect-prone and not-defect-prone classes are effective tools to maintain the high quality of software products. The early prediction of defect-proneness of the modules can allow software developers to allocate the limited resources on those defect-prone modules such that high quality software can be produced on time and within budget. In the process of software defect prediction, the misclassification of defect-prone modules generally incurs much higher cost than the misclassification of not-defect-prone ones. Most of the previously developed predication models do not consider this cost issue. In this paper, three cost-sensitive boosting algorithms are studied to boost neural networks for software defect prediction. The first algorithm based on threshold-moving tries to move the classification threshold towards the not-fault-prone modules such that more fault-prone modules can be classified correctly. The other two weight-updating based algorithms incorporate the misclassification costs into the weight-update rule of boosting procedure such that the algorithms boost more weights on the samples associated with misclassified defect-prone modules. The performances of the three algorithms are evaluated by using four datasets from NASA projects in terms of a singular measure, the Normalized Expected Cost of Misclassification (NECM). The experimental results suggest that threshold-moving is the best choice to build cost-sensitive software defect prediction models with boosted neural networks among the three algorithms studied, especially for the datasets from projects developed by object-oriented language.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {4537–4543},
numpages = {7},
keywords = {Adaboost, Cost-sensitive, Neural networks, Software defect}
}

@inproceedings{10.1007/978-3-030-79463-7_35,
author = {Kawalerowicz, Marcin and Madeyski, Lech},
title = {Continuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project},
year = {2021},
isbn = {978-3-030-79462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79463-7_35},
doi = {10.1007/978-3-030-79463-7_35},
abstract = {We explain the idea of Continuous Build Outcome Prediction (CBOP) practice that uses classification to label the possible build results (success or failure) based on historical data and metrics (features) derived from the software repository. Additionally, we present a preliminary empirical evaluation of CBOP in a real live software project. In a small-n repeated-measure with two conditions and replicates experiment, we study whether CBOP will reduce the Failed Build Ratio (FBR). Surprisingly, the result of the study indicates a slight increase in FBR while using the CBOP, although the effect size is very small. A plausible explanation of the revealed phenomenon may come from the authority principle, which is rarely discussed in the software engineering context in general, and AI-supported software development practices in particular.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part II},
pages = {412–425},
numpages = {14},
keywords = {Software defect prediction, Agile experimentation, Continuous integration, Machine learning},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1145/3092566,
author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092566},
doi = {10.1145/3092566},
abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {56},
numpages = {36},
keywords = {Software vulnerability analysis, data-mining, machine-learning, review, software security, software vulnerability discovery, survey}
}

@article{10.1007/s10796-013-9430-0,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri and Wald, Randall},
title = {A comparative study of iterative and non-iterative feature selection techniques for software defect prediction},
year = {2014},
issue_date = {November  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {5},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-013-9430-0},
doi = {10.1007/s10796-013-9430-0},
abstract = {Two important problems which can affect the performance of classification models are high-dimensionality (an overabundance of independent features in the dataset) and imbalanced data (a skewed class distribution which creates at least one class with many fewer instances than other classes). To resolve these problems concurrently, we propose an iterative feature selection approach, which repeated applies data sampling (in order to address class imbalance) followed by feature selection (in order to address high-dimensionality), and finally we perform an aggregation step which combines the ranked feature lists from the separate iterations of sampling. This approach is designed to find a ranked feature list which is particularly effective on the more balanced dataset resulting from sampling while minimizing the risk of losing data through the sampling step and missing important features. To demonstrate this technique, we employ 18 different feature selection algorithms and Random Undersampling with two post-sampling class distributions. We also investigate the use of sampling and feature selection without the iterative step (e.g., using the ranked list from a single iteration, rather than combining the lists from multiple iterations), and compare these results from the version which uses iteration. Our study is carried out using three groups of datasets with different levels of class balance, all of which were collected from a real-world software system. All of our experiments use four different learners and one feature subset size. We find that our proposed iterative feature selection approach outperforms the non-iterative approach.},
journal = {Information Systems Frontiers},
month = nov,
pages = {801–822},
numpages = {22},
keywords = {Class imbalance, Date sampling, High dimensionality, Iterative feature selection, Software defect prediction}
}

@article{10.1007/s10664-019-09736-3,
author = {Kondo, Masanari and German, Daniel M. and Mizuno, Osamu and Choi, Eun-Hye},
title = {The impact of context metrics on just-in-time defect prediction},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09736-3},
doi = {10.1007/s10664-019-09736-3},
abstract = {Traditional just-in-time defect prediction approaches have been using changed lines of software to predict defective-changes in software development. However, they disregard information around the changed lines. Our main hypothesis is that such information has an impact on the likelihood that the change is defective. To take advantage of this information in defect prediction, we consider n-lines (n = 1,2,…) that precede and follow the changed lines (which we call context lines), and propose metrics that measure them, which we call “Context Metrics.” Specifically, these context metrics are defined as the number of words/keywords in the context lines. In a large-scale empirical study using six open source software projects, we compare the performance of using our context metrics, traditional code churn metrics (e.g., the number of modified subsystems), our extended context metrics which measure not only context lines but also changed lines, and combination metrics that use two extended context metrics at a prediction model for defect prediction. The results show that context metrics that consider the context lines of added-lines achieve the best median value in all cases in terms of a statistical test. Moreover, using few number of context lines is suitable for context metric that considers words, and using more number of context lines is suitable for context metric that considers keywords. Finally, the combination metrics of two extended context metrics significantly outperform all studied metrics in all studied projects w. r. t. the area under the receiver operation characteristic curve (AUC) and Matthews correlation coefficient (MCC).},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {890–939},
numpages = {50},
keywords = {Just-in-time defect prediction, Defect prediction, Source code changes, Context lines, Changed lines, Indentation metrics, Code churn metrics}
}

@article{10.1016/j.compeleceng.2018.02.043,
author = {Choudhary, Garvit Rajesh and Kumar, Sandeep and Kumar, Kuldeep and Mishra, Alok and Catal, Cagatay},
title = {Empirical analysis of change metrics for software fault prediction},
year = {2018},
issue_date = {Apr 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2018.02.043},
doi = {10.1016/j.compeleceng.2018.02.043},
journal = {Comput. Electr. Eng.},
month = apr,
pages = {15–24},
numpages = {10},
keywords = {Software fault prediction, Eclipse, Change log, Metrics, Software quality, Defect prediction}
}

@article{10.1016/j.asoc.2016.05.038,
author = {Serdar Bier, M. and Diri, Banu},
title = {Defect prediction for Cascading Style Sheets},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.038},
doi = {10.1016/j.asoc.2016.05.038},
abstract = {Graphical abstractDisplay Omitted Testing is a crucial activity in software development. However exhaustive testing of a given software is impossible in practice because projects have serious time and budget limitations. Therefore, software testing teams need guidance about which modules they should focus on. Defect prediction techniques are useful for this situation because they let testers to identify and focus on defect prone parts of software. These techniques are essential for software teams, because they help teams to efficiently allocate their precious resources in testing phase. Software defect prediction has been an active research area in recent years. Researchers in this field have been using different types of metrics in their prediction models. However, value of extracting static code metrics for style sheet languages has been ignored until now. User experience is a very important part of web applications and its mostly provided using Cascading Style Sheets (CSS). In this research, our aim is to improve defect prediction performance for web applications by utilizing metrics generated from CSS code. We generated datasets from four open source web applications to conduct our experiments. Defect prediction is then performed using three different well-known machine learning algorithms. The results revealed that static code metrics based defect prediction techniques can be performed effectively to improve quality of CSS code in web applications. Therefore we recommend utilizing domain-specific characteristics of applications in defect prediction as they result in significantly high prediction performance with low costs.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1078–1084},
numpages = {7},
keywords = {Defect prediction, Software Metrics, Software quality, Web sites}
}

@article{10.1007/s10489-021-02346-x,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {Software fault prediction based on the dynamic selection of learning technique: findings from the eclipse project study},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {12},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02346-x},
doi = {10.1007/s10489-021-02346-x},
abstract = {An effective software fault prediction (SFP) model could help developers in the quick and prompt detection of faults and thus help enhance the overall reliability and quality of the software project. Variations in the prediction performance of learning techniques for different software systems make it difficult to select a suitable learning technique for fault prediction modeling. The evaluation of previously presented SFP approaches has shown that single machine learning-based models failed to provide the best accuracy in any context, highlighting the need to use multiple techniques to build the SFP model. To solve this problem, we present and discuss a software fault prediction approach based on selecting the most appropriate learning techniques from a set of competitive and accurate learning techniques for building a fault prediction model. In work, we apply the discussed SFP approach for the five Eclipse project datasets and nine Object-oriented (OO) project datasets and report the findings of the experimental study. We have used different performance measures, i.e., AUC, accuracy, sensitivity, and specificity, to assess the discussed approach’s performance. Further, we have performed a cost-benefit analysis to evaluate the economic viability of the approach. Results showed that the presented approach predicted the software’s faults effectively for the used accuracy, AUC, sensitivity, and specificity measures with the highest achieved values of 0.816, 0.835, 0.98, and 0.903 for AUC, accuracy, sensitivity, and specificity, respectively. The cost-benefit analysis of the approach showed that it could help reduce the overall software testing cost.},
journal = {Applied Intelligence},
month = dec,
pages = {8945–8960},
numpages = {16},
keywords = {Software fault prediction, Eclipse project, Dynamic selection, Cost-benefit analysis, Machine learning techniques}
}

@inproceedings{10.1145/2786805.2786814,
author = {Nam, Jaechang and Kim, Sunghun},
title = {Heterogeneous defect prediction},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786814},
doi = {10.1145/2786805.2786814},
abstract = {Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {508–519},
numpages = {12},
keywords = {Defect prediction, heterogeneous metrics, quality assurance},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3449365.3449384,
author = {Malhotra, Ruchika and Budhiraja, Anmol and Kumar Singh, Abhinav and Ghoshal, Ishani},
title = {A Novel Feature Selection Approach based on Binary Particle Swarm Optimization and Ensemble Learning for Heterogeneous Defect Prediction},
year = {2021},
isbn = {9781450388108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449365.3449384},
doi = {10.1145/3449365.3449384},
abstract = {Software defect prediction is an integral part of the software development process. Defect prediction helps focus on the grey areas beforehand, thus saving the considerable amount of money that is otherwise wasted in finding and fixing the faults once the software is already in production. One of the popular areas of defect prediction in recent years is Heterogeneous Defect Prediction, which predicts defects in a target project using a source project with different metrics. Through our paper, we provide a novel feature selection based approach, En-BPSO, based on binary particle swarm optimization, coupled with majority voting ensemble classifier based fitness function for heterogeneous defect prediction. The datasets we are using are MORPH and SOFTLAB. The results show that the En-BPSO method provides the highest Friedman mean rank amongst all the feature selection methods used for comparison. En-BPSO technique also helps us dynamically determine the optimal number of features to build an accurate heterogeneous defect prediction model.},
booktitle = {Proceedings of the 2021 3rd Asia Pacific Information Technology Conference},
pages = {115–121},
numpages = {7},
keywords = {Binary Particle Swarm Optimization, Defect Prediction, Ensemble Learning, Feature Selection, Heterogeneous Metrics},
location = {Bangkok, Thailand},
series = {APIT '21}
}

@article{10.5555/3197793.3197817,
author = {Shukla, Swapnil and Radhakrishnan, T. and Muthukumaran, K. and Neti, Lalita Bhanu},
title = {Multi-objective cross-version defect prediction},
year = {2018},
issue_date = {March     2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {6},
issn = {1432-7643},
abstract = {Defect prediction models help software project teams to spot defect-prone source files of software systems. Software project teams can prioritize and put up rigorous quality assurance (QA) activities on these predicted defect-prone files to minimize post-release defects so that quality software can be delivered. Cross-version defect prediction is building a prediction model from the previous version of a software project to predict defects in the current version. This is more practical than the other two ways of building models, i.e., cross-project prediction model and cross- validation prediction models, as previous version of same software project will have similar parameter distribution among files. In this paper, we formulate cross-version defect prediction problem as a multi-objective optimization problem with two objective functions: (a) maximizing recall by minimizing misclassification cost and (b) maximizing recall by minimizing cost of QA activities on defect prone files. The two multi-objective defect prediction models are compared with four traditional machine learning algorithms, namely logistic regression, na\"{\i}ve Bayes, decision tree and random forest. We have used 11 projects from the PROMISE repository consisting of a total of 41 different versions of these projects. Our findings show that multi-objective logistic regression is more cost-effective than single-objective algorithms.},
journal = {Soft Comput.},
month = mar,
pages = {1959–1980},
numpages = {22},
keywords = {Cost-effectiveness, Cross-version defect prediction, Misclassification cost, Multi-objective optimization, Search-based software engineering}
}

@article{10.1007/s10489-020-01935-6,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {An empirical study of ensemble techniques for software fault prediction},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01935-6},
doi = {10.1007/s10489-020-01935-6},
abstract = {Previously, many researchers have performed analysis of various techniques for the software fault prediction (SFP). Oddly, the majority of such studies have shown the limited prediction capability and their performance for given software fault datasets was not persistent. In contrast to this, recently, ensemble techniques based SFP models have shown promising and improved results across different software fault datasets. However, many new as well as improved ensemble techniques have been introduced, which are not explored for SFP. Motivated by this, the paper performs an investigation on ensemble techniques for SFP. We empirically assess the performance of seven ensemble techniques namely, Dagging, Decorate, Grading, MultiBoostAB, RealAdaBoost, Rotation Forest, and Ensemble Selection. We believe that most of these ensemble techniques are not used before for SFP. We conduct a series of experiments on the benchmark fault datasets and use three distinct classification algorithms, namely, naive Bayes, logistic regression, and J48 (decision tree) as base learners to the ensemble techniques. Experimental analysis revealed that rotation forest with J48 as the base learner achieved the highest precision, recall, and G-mean 1 values of 0.995, 0.994, and 0.994, respectively and Decorate achieved the highest AUC value of 0.986. Further, results of statistical tests showed used ensemble techniques demonstrated a statistically significant difference in their performance among the used ones for SFP. Additionally, the cost-benefit analysis showed that SFP models based on used ensemble techniques might be helpful in saving software testing cost and effort for twenty out of twenty-eight used fault datasets.},
journal = {Applied Intelligence},
month = jun,
pages = {3615–3644},
numpages = {30},
keywords = {Software fault prediction, Ensemble techniques, PROMISE data repository, Empirical analysis}
}

@inproceedings{10.1145/3377811.3380360,
author = {Li, Ke and Xiang, Zilin and Chen, Tao and Wang, Shuo and Tan, Kay Chen},
title = {Understanding the automated parameter optimization on transfer learning for cross-project defect prediction: an empirical study},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380360},
doi = {10.1145/3377811.3380360},
abstract = {Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {566–577},
numpages = {12},
keywords = {automated parameter optimization, classification techniques, cross-project defect prediction, transfer learning},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/ESEM.2017.48,
author = {Yan, Meng and Fang, Yicheng and Lo, David and Xia, Xin and Zhang, Xiaohong},
title = {File-level defect prediction: unsupervised vs. supervised models},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.48},
doi = {10.1109/ESEM.2017.48},
abstract = {Background: Software defect models can help software quality assurance teams to allocate testing or code review resources. A variety of techniques have been used to build defect prediction models, including supervised and unsupervised methods. Recently, Yang et al. [1] surprisingly find that unsupervised models can perform statistically significantly better than supervised models in effort-aware change-level defect prediction. However, little is known about relative performance of unsupervised and supervised models for effort-aware file-level defect prediction. Goal: Inspired by their work, we aim to investigate whether a similar finding holds in effort-aware file-level defect prediction. Method: We replicate Yang et al.'s study on PROMISE dataset with totally ten projects. We compare the effectiveness of unsupervised and supervised prediction models for effort-aware file-level defect prediction. Results: We find that the conclusion of Yang et al. [1] does not hold under within-project but holds under cross-project setting for file-level defect prediction. In addition, following the recommendations given by the best unsupervised model, developers needs to inspect statistically significantly more files than that of supervised models considering the same inspection effort (i.e., LOC). Conclusions: (a) Unsupervised models do not perform statistically significantly better than state-of-art supervised model under within-project setting, (b) Unsupervised models can perform statistically significantly better than state-of-art supervised model under cross-project setting, (c) We suggest that not only LOC but also number of files needed to be inspected should be considered when evaluating effort-aware file-level defect prediction models.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {344–353},
numpages = {10},
keywords = {effort-aware defect prediction, inspection effort, replication study},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@article{10.1016/j.infsof.2019.04.013,
author = {Rahman, Akond and Williams, Laurie},
title = {Source code properties of defective infrastructure as code scripts},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.013},
doi = {10.1016/j.infsof.2019.04.013},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {148–163},
numpages = {16},
keywords = {Configuration as code, Continuous deployment, Defect prediction, Devops, Empirical study, Infrastructure as code, Puppet}
}

@inproceedings{10.5555/3340730.3340750,
author = {Catolino, Gemma and Di Nucci, Dario and Ferrucci, Filomena},
title = {Cross-project just-in-time bug prediction for mobile apps: an empirical assessment},
year = {2019},
publisher = {IEEE Press},
abstract = {Bug Prediction is an activity aimed at identifying defect-prone source code entities that allows developers to focus testing efforts on specific areas of software systems. Recently, the research community proposed Just-in-Time (JIT) Bug Prediction with the goal of detecting bugs at commit-level. While this topic has been extensively investigated in the context of traditional systems, to the best of our knowledge, only a few preliminary studies assessed the performance of the technique in a mobile environment, by applying the metrics proposed by Kamei et al. in a within-project scenario. The results of these studies highlighted that there is still room for improvement. In this paper, we faced this problem to understand (i) which Kamei et al.'s metrics are useful in the mobile context, (ii) if different classifiers impact the performance of cross-project JIT bug prediction models and (iii) whether the application of ensemble techniques improves the capabilities of the models. To carry out the experiment, we first applied a feature selection technique, i.e., InfoGain, to filter relevant features and avoid models multicollinearity. Then, we assessed and compared the performance of four different well-known classifiers and four ensemble techniques. Our empirical study involved 14 apps and 42,543 commits extracted from the Commit Guru platform. The results show that Naive Bayes achieves the best performance with respect to the other classifiers and in some cases outperforms some well-known ensemble techniques.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {99–110},
numpages = {12},
keywords = {JIT bug prediction, empirical study, metrics},
location = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@article{10.1145/3470006,
author = {Nikanjam, Amin and Braiek, Houssem Ben and Morovati, Mohammad Mehdi and Khomh, Foutse},
title = {Automatic Fault Detection for Deep Learning Programs Using Graph Transformations},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3470006},
doi = {10.1145/3470006},
abstract = {Nowadays, we are witnessing an increasing demand in both corporates and academia for exploiting Deep Learning (DL) to solve complex real-world problems. A DL program encodes the network structure of a desirable DL model and the process by which the model learns from the training dataset. Like any software, a DL program can be faulty, which implies substantial challenges of software quality assurance, especially in safety-critical domains. It is therefore crucial to equip DL development teams with efficient fault detection techniques and tools. In this article, we propose NeuraLint, a model-based fault detection approach for DL programs, using meta-modeling and graph transformations. First, we design a meta-model for DL programs that includes their base skeleton and fundamental properties. Then, we construct a graph-based verification process that covers 23 rules defined on top of the meta-model and implemented as graph transformations to detect faults and design inefficiencies in the generated models (i.e., instances of the meta-model). First, the proposed approach is evaluated by finding faults and design inefficiencies in 28 synthesized examples built from common problems reported in the literature. Then NeuraLint successfully finds 64 faults and design inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts and GitHub repositories. The results show that NeuraLint effectively detects faults and design issues in both synthesized and real-world examples with a recall of 70.5% and a precision of 100%. Although the proposed meta-model is designed for feedforward neural networks, it can be extended to support other neural network architectures such as recurrent neural networks. Researchers can also expand our set of verification rules to cover more types of issues in DL programs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {14},
numpages = {27},
keywords = {Graph transformations, model-based verification, deep learning, fault detection}
}

@inproceedings{10.1007/978-3-030-38961-1_8,
author = {Sun, Yuanyuan and Xu, Lele and Guo, Lili and Li, Ye and Wang, Yongming},
title = {A Comparison Study of VAE and GAN for Software Fault Prediction},
year = {2019},
isbn = {978-3-030-38960-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38961-1_8},
doi = {10.1007/978-3-030-38961-1_8},
abstract = {Software fault is an unavoidable problem in software project. How to predict software fault to enhance safety and reliability of system is worth studying. In recent years, deep learning has been widely used in the fields of image, text and voice. However it is seldom applied in the field of software fault prediction. Considering the ability of deep learning, we select the deep learning techniques of VAE and GAN for software fault prediction and compare the performance of them. There is one salient feature of software fault data. The proportion of non-fault data is well above the proportion of fault data. Because of the imbalanced data, it is difficult to get high accuracy to predict software fault. As we known, VAE and GAN are able to generate synthetic samples that obey the distribution of real data. We try to take advantage of their power to generate new fault samples in order to improve the accuracy of software fault prediction. The architectures of VAE and GAN are designed to fit for the high dimensional software fault data. New software fault samples are generated to balance the software fault datasets in order to get better performance for software fault prediction. The models of VAE and GAN are trained on GPU TITAN X. SMOTE is also adopted in order to compare the performance with VAE and GAN. The results in the experiment show that VAE and GAN are useful techniques for software fault prediction and VAE has better performance than GAN on this issue.},
booktitle = {Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9–11, 2019, Proceedings, Part II},
pages = {82–96},
numpages = {15},
keywords = {Deep learning, VAE, GAN, Software fault prediction},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1007/978-3-030-82153-1_23,
author = {Tang, Gaigai and Zhang, Long and Yang, Feng and Meng, Lianxiao and Cao, Weipeng and Qiu, Meikang and Ren, Shuangyin and Yang, Lin and Wang, Huiqiang},
title = {Interpretation of Learning-Based Automatic Source Code Vulnerability Detection Model Using LIME},
year = {2021},
isbn = {978-3-030-82152-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82153-1_23},
doi = {10.1007/978-3-030-82153-1_23},
abstract = {The existing advanced automatic vulnerability detection methods based on source code are mainly learning-based, such as machine learning and deep learning. These models can capture the vulnerability pattern through learning, which is more automatic and intelligent. However, the outputs of many learning-based vulnerability detection models are unexplainable, even though they usually show high accuracy. It’s meaningful to verify the credibility of the models so that we can better understand and use them in practice. To alleviate the above issue, we use an interpretation method called LIME to explain the learning-based automatic vulnerability detection model. For one thing, the preprocessing methods are all interpretable, including symbolization and vector representation, where the Bag of words model is chosen for source code vector representation. For another, the vulnerability detection models we select are based on Logistic Regression and Bi-LSTM. The former is interpretable, which is used to verify the effectiveness of LIME in the field of source code vulnerability detection. The latter is unexplained that is interpreted by LIME to its credibility on source code vulnerability detection. The experimental results show that LIME can effectively explain the learning-based automatic vulnerability detection model. Moreover, we find that under the condition of local interpretation, the predictions of the model based on Bi-LSTM are credible.},
booktitle = {Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14–16, 2021, Proceedings, Part III},
pages = {275–286},
numpages = {12},
keywords = {Vulnerability detection, Machine learning, Logistic regression, Bi-LSTM, LIME, Model interpretation},
location = {Tokyo, Japan}
}

@article{10.1016/j.jss.2016.02.015,
author = {Rana, Rakesh and Staron, Miroslaw and Berger, Christian and Hansson, J\"{o}rgen and Nilsson, Martin and Meding, Wilhelm},
title = {Analyzing defect inflow distribution and applying Bayesian inference method for software defect prediction in large software projects},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.015},
doi = {10.1016/j.jss.2016.02.015},
abstract = {Defect inflow distribution of 14 large projects from industry &amp; OSS is analyzed.6 standard distributions are evaluated for their ability to fit the defect inflow.12 out of 14 projects defect inflow data was described best by beta distribution.Historical projects information is useful for early defect prediction using Bayesian inference method. Tracking and predicting quality and reliability is a major challenge in large and distributed software development projects. A number of standard distributions have been successfully used in reliability engineering theory and practice, common among these for modeling software defect inflow being exponential, Weibull, beta and Non-Homogeneous Poisson Process (NHPP). Although standard distribution models have been recognized in reliability engineering practice, their ability to fit defect data from proprietary and OSS software projects is not well understood. Lack of knowledge about underlying defect inflow distribution also leads to difficulty in applying Bayesian based inference methods for software defect prediction. In this paper we explore the defect inflow distribution of total of fourteen large software projects/release from two industrial domain and open source community. We evaluate six standard distributions for their ability to fit the defect inflow data and also assess which information criterion is practical for selecting the distribution with best fit. Our results show that beta distribution provides the best fit to the defect inflow data for all industrial projects as well as majority of OSS projects studied. In the paper we also evaluate how information about defect inflow distribution from historical projects is applied for modeling the prior beliefs/experience in Bayesian analysis which is useful for making software defect predictions early during the software project lifecycle.},
journal = {J. Syst. Softw.},
month = jul,
pages = {229–244},
numpages = {16},
keywords = {Defect Inflow, SRGM, Software}
}

@article{10.1007/s10664-016-9468-y,
author = {Herbold, Steffen and Trautsch, Alexander and Grabowski, Jens},
title = {Global vs. local models for cross-project defect prediction},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9468-y},
doi = {10.1007/s10664-016-9468-y},
abstract = {Although researchers invested significant effort, the performance of defect prediction in a cross-project setting, i.e., with data that does not come from the same project, is still unsatisfactory. A recent proposal for the improvement of defect prediction is using local models. With local models, the available data is first clustered into homogeneous regions and afterwards separate classifiers are trained for each homogeneous region. Since the main problem of cross-project defect prediction is data heterogeneity, the idea of local models is promising. Therefore, we perform a conceptual replication of the previous studies on local models with a focus on cross-project defect prediction. In a large case study, we evaluate the performance of local models and investigate their advantages and drawbacks for cross-project predictions. To this aim, we also compare the performance with a global model and a transfer learning technique designed for cross-project defect predictions. Our findings show that local models make only a minor difference in comparison to global models and transfer learning for cross-project defect prediction. While these results are negative, they provide valuable knowledge about the limitations of local models and increase the validity of previously gained research results.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1866–1902},
numpages = {37},
keywords = {Cross-project, Defect prediction, Local models}
}

@inproceedings{10.1145/2372251.2372285,
author = {Giger, Emanuel and D'Ambros, Marco and Pinzger, Martin and Gall, Harald C.},
title = {Method-level bug prediction},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372285},
doi = {10.1145/2372251.2372285},
abstract = {Researchers proposed a wide range of approaches to build effective bug prediction models that take into account multiple aspects of the software development process. Such models achieved good prediction performance, guiding developers towards those parts of their system where a large share of bugs can be expected. However, most of those approaches predict bugs on file-level. This often leaves developers with a considerable amount of effort to examine all methods of a file until a bug is located. This particular problem is reinforced by the fact that large files are typically predicted as the most bug-prone. In this paper, we present bug prediction models at the level of individual methods rather than at file-level. This increases the granularity of the prediction and thus reduces manual inspection efforts for developers. The models are based on change metrics and source code metrics that are typically used in bug prediction. Our experiments---performed on 21 Java open-source (sub-)systems---show that our prediction models reach a precision and recall of 84% and 88%, respectively. Furthermore, the results indicate that change metrics significantly outperform source code metrics.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {171–180},
numpages = {10},
keywords = {code metrics, fine-grained source code changes, method-level bug prediction},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1145/2931037.2931039,
author = {Bowes, David and Hall, Tracy and Harman, Mark and Jia, Yue and Sarro, Federica and Wu, Fan},
title = {Mutation-aware fault prediction},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931039},
doi = {10.1145/2931037.2931039},
abstract = {We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ≤ 0.05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments.},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {330–341},
numpages = {12},
keywords = {Empirical Study, Mutation Testing, Software Defect Prediction, Software Fault Prediction, Software Metrics},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3194104.3194112,
author = {Di Nucci, Dario and Palomba, Fabio and De Lucia, Andrea},
title = {Evaluating the adaptive selection of classifiers for cross-project bug prediction},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194112},
doi = {10.1145/3194104.3194112},
abstract = {Bug prediction models are used to locate source code elements more likely to be defective. One of the key factors influencing their performances is related to the selection of a machine learning method (a.k.a., classifier) to use when discriminating buggy and non-buggy classes. Given the high complementarity of stand-alone classifiers, a recent trend is the definition of ensemble techniques, which try to effectively combine the predictions of different stand-alone machine learners. In a recent work we proposed ASCI, a technique that dynamically selects the right classifier to use based on the characteristics of the class on which the prediction has to be done. We tested it in a within-project scenario, showing its higher accuracy with respect to the Validation and Voting strategy. In this paper, we continue on the line of research, by (i) evaluating ASCI in a global and local cross-project setting and (ii) comparing its performances with those achieved by a stand-alone and an ensemble baselines, namely Naive Bayes and Validation and Voting, respectively. A key finding of our study shows that ASCI is able to perform better than the other techniques in the context of cross-project bug prediction. Moreover, despite local learning is not able to improve the performances of the corresponding models in most cases, it is able to improve the robustness of the models relying on ASCI.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {48–54},
numpages = {7},
keywords = {bug prediction, cross-project, ensemble classifiers},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@inproceedings{10.1145/3412841.3442020,
author = {Hosseini, Seyedrebvar and Turhan, Burak},
title = {A comparison of similarity based instance selection methods for cross project defect prediction},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442020},
doi = {10.1145/3412841.3442020},
abstract = {Context: Previous studies have shown that training data instance selection based on nearest neighborhood (NN) information can lead to better performance in cross project defect prediction (CPDP) by reducing heterogeneity in training datasets. However, neighborhood calculation is computationally expensive and approximate methods such as Locality Sensitive Hashing (LSH) can be as effective as exact methods. Aim: We aim at comparing instance selection methods for CPDP, namely LSH, NN-filter, and Genetic Instance Selection (GIS). Method: We conduct experiments with five base learners, optimizing their hyper parameters, on 13 datasets from PROMISE repository in order to compare the performance of LSH with benchmark instance selection methods NN-Filter and GIS. Results: The statistical tests show six distinct groups for F-measure performance. The top two group contains only LSH and GIS benchmarks whereas the bottom two groups contain only NN-Filter variants. LSH and GIS favor recall more than precision. In fact, for precision performance only three significantly distinct groups are detected by the tests where the top group is comprised of NN-Filter variants only. Recall wise, 16 different groups are identified where the top three groups contain only LSH methods, four of the next six are GIS only and the bottom five contain only NN-Filter. Finally, NN-Filter benchmarks never outperform the LSH counterparts with the same base learner, tuned or non-tuned. Further, they never even belong to the same rank group, meaning that LSH is always significantly better than NN-Filter with the same learner and settings. Conclusions: The increase in performance and the decrease in computational overhead and runtime make LSH a promising approach. However, the performance of LSH is based on high recall and in environments where precision is considered more important NN-Filter should be considered.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1455–1464},
numpages = {10},
keywords = {approximate near neighbour, cross project defect prediction, instance selection, locality sensitive hashing, search based optimisation},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1007/978-3-030-33709-4_5,
author = {Tasnim Cynthia, Shamse and Rasul, Md. Golam and Ripon, Shamim},
title = {Effect of Feature Selection in Software Fault Detection},
year = {2019},
isbn = {978-3-030-33708-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33709-4_5},
doi = {10.1007/978-3-030-33709-4_5},
abstract = {The quality of software is enormously affected by the faults associated with it. Detection of faults at a proper stage in software development is a challenging task and plays a vital role in the quality of the software. Machine learning is, now a days, a commonly used technique for fault detection and prediction. However, the effectiveness of the fault detection mechanism is impacted by the number of attributes in the publicly available datasets. Feature selection is the process of selecting a subset of all the features that are most influential to the classification and it is a challenging task. This paper thoroughly investigates the effect of various feature selection techniques on software fault classification by using NASA’s some benchmark publicly available datasets. Various metrics are used to analyze the performance of the feature selection techniques. The experiment discovers that the most important and relevant features can be selected by the adopted feature selection techniques without sacrificing the performance of fault detection.},
booktitle = {Multi-Disciplinary Trends in Artificial Intelligence: 13th International Conference, MIWAI 2019, Kuala Lumpur, Malaysia, November 17–19, 2019, Proceedings},
pages = {52–63},
numpages = {12},
keywords = {Fault detection, Feature selection, Feature classification},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1504/ijcsm.2021.117600,
author = {Hammad, Mustafa},
title = {Classifying defective software projects based on machine learning and complexity metrics},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {4},
issn = {1752-5055},
url = {https://doi.org/10.1504/ijcsm.2021.117600},
doi = {10.1504/ijcsm.2021.117600},
abstract = {Software defects can lead to software failures or errors at any time. Therefore, software developers and engineers spend a lot of time and effort in order to find possible defects. This paper proposes an automatic approach to predict software defects based on machine learning algorithms. A set of complexity measures values are used to train the classifier. Three public datasets were used to evaluate the ability of mining complexity measures for different software projects to predict possible defects. Experimental results showed that it is possible to min software complexity to build a defect prediction model with a high accuracy rate.},
journal = {Int. J. Comput. Sci. Math.},
month = jan,
pages = {401–412},
numpages = {11},
keywords = {software defects, defect prediction, software metrics, machine learning, complexity, neural networks, na\"{\i}ve Bayes, decision trees, SVM, support vector machine}
}

@inproceedings{10.5555/1332044.1332090,
author = {Catal, Cagatay and Diri, Banu},
title = {Software defect prediction using artificial immune recognition system},
year = {2007},
publisher = {ACTA Press},
address = {USA},
abstract = {Predicting fault-prone modules for software development projects enables companies to reach high reliable systems and minimizes necessary budget, personnel and resource to be allocated to achieve this goal. Researchers have investigated various statistical techniques and machine learning algorithms until now but most of them applied their models to the different datasets which are not public or used different criteria to decide the best predictor model. Artificial Immune Recognition System is a supervised learning algorithm which has been proposed in 2001 for the classification problems and its performance for UCI datasets (University of California machine learning repository) is remarkable.In this paper, we propose a novel software defect prediction model by applying Artificial Immune Recognition System (AIRS) along with the Correlation-Based Feature Selection (CFS) technique. In order to evaluate the performance of the proposed model, we apply it to the five NASA public defect datasets and compute G-mean 1, G-mean 2 and F-measure values to discuss the effectiveness of the model. Experimental results show that AIRS has a great potential for software defect prediction and AIRS along with CFS technique provides relatively better prediction for large scale projects which consist of many modules.},
booktitle = {Proceedings of the 25th Conference on IASTED International Multi-Conference: Software Engineering},
pages = {285–290},
numpages = {6},
keywords = {artificial immune recognition system (AIRS) and correlation-based feature selection, immune systems, quality prediction, software defect prediction},
location = {Innsbruck, Austria},
series = {SE'07}
}

@article{10.1007/s11219-015-9287-1,
author = {Ryu, Duksan and Jang, Jong-In and Baik, Jongmoon},
title = {A transfer cost-sensitive boosting approach for cross-project defect prediction},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-015-9287-1},
doi = {10.1007/s11219-015-9287-1},
abstract = {Software defect prediction has been regarded as one of the crucial tasks to improve software quality by effectively allocating valuable resources to fault-prone modules. It is necessary to have a sufficient set of historical data for building a predictor. Without a set of sufficient historical data within a company, cross-project defect prediction (CPDP) can be employed where data from other companies are used to build predictors. In such cases, a transfer learning technique, which extracts common knowledge from source projects and transfers it to a target project, can be used to enhance the prediction performance. There exists the class imbalance problem, which causes difficulties for the learner to predict defects. The main impacts of imbalanced data under cross-project settings have not been investigated in depth. We propose a transfer cost-sensitive boosting method that considers both knowledge transfer and class imbalance for CPDP when given a small amount of labeled target data. The proposed approach performs boosting that assigns weights to the training instances with consideration of both distributional characteristics and the class imbalance. Through comparative experiments with the transfer learning and the class imbalance learning techniques, we show that the proposed model provides significantly higher defect detection accuracy while retaining better overall performance. As a result, a combination of transfer learning and class imbalance learning is highly effective for improving the prediction performance under cross-project settings. The proposed approach will help to design an effective prediction model for CPDP. The improved defect prediction performance could help to direct software quality assurance activities and reduce costs. Consequently, the quality of software can be managed effectively.},
journal = {Software Quality Journal},
month = mar,
pages = {235–272},
numpages = {38},
keywords = {Boosting, Class imbalance, Cost-sensitive learning, Cross-project defect prediction, Software defect prediction, Transfer learning}
}

@article{10.1145/3409331,
author = {Wang, Wenhan and Li, Ge and Shen, Sijie and Xia, Xin and Jin, Zhi},
title = {Modular Tree Network for Source Code Representation Learning},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3409331},
doi = {10.1145/3409331},
abstract = {Learning representation for source code is a foundation of many program analysis tasks. In recent years, neural networks have already shown success in this area, but most existing models did not make full use of the unique structural information of programs. Although abstract syntax tree (AST)-based neural models can handle the tree structure in the source code, they cannot capture the richness of different types of substructure in programs. In this article, we propose a modular tree network that dynamically composes different neural network units into tree structures based on the input AST. Different from previous tree-structural neural network models, a modular tree network can capture the semantic differences between types of AST substructures. We evaluate our model on two tasks: program classification and code clone detection. Our model achieves the best performance compared with state-of-the-art approaches in both tasks, showing the advantage of leveraging more elaborate structure information of the source code.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {31},
numpages = {23},
keywords = {Deep learning, code clone detection, neural networks, program classification}
}

@article{10.1007/s11390-019-1959-z,
author = {Xu, Zhou and Pang, Shuai and Zhang, Tao and Luo, Xia-Pu and Liu, Jin and Tang, Yu-Tian and Yu, Xiao and Xue, Lei},
title = {Cross Project Defect Prediction via Balanced Distribution Adaptation Based Transfer Learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1959-z},
doi = {10.1007/s11390-019-1959-z},
abstract = {Defect prediction assists the rational allocation of testing resources by detecting the potentially defective software modules before releasing products. When a project has no historical labeled defect data, cross project defect prediction (CPDP) is an alternative technique for this scenario. CPDP utilizes labeled defect data of an external project to construct a classification model to predict the module labels of the current project. Transfer learning based CPDP methods are the current mainstream. In general, such methods aim to minimize the distribution differences between the data of the two projects. However, previous methods mainly focus on the marginal distribution difference but ignore the conditional distribution difference, which will lead to unsatisfactory performance. In this work, we use a novel balanced distribution adaptation (BDA) based transfer learning method to narrow this gap. BDA simultaneously considers the two kinds of distribution differences and adaptively assigns different weights to them. To evaluate the effectiveness of BDA for CPDP performance, we conduct experiments on 18 projects from four datasets using six indicators (i.e., F-measure, g-means, Balance, AUC, EARecall, and EAF-measure). Compared with 12 baseline methods, BDA achieves average improvements of 23.8%, 12.5%, 11.5%, 4.7%, 34.2%, and 33.7% in terms of the six indicators respectively over four datasets.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1039–1062},
numpages = {24},
keywords = {cross-project defect prediction, transfer learning, balancing distribution, effort-aware indicator}
}

@article{10.1007/s10515-019-00259-1,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Zhu, Xiaoke and Zhang, Hongyu and Xu, Baowen and Ying, Shi},
title = {Heterogeneous defect prediction with two-stage ensemble learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00259-1},
doi = {10.1007/s10515-019-00259-1},
abstract = {Heterogeneous defect prediction (HDP) refers to predicting defect-prone software modules in one project (target) using heterogeneous data collected from other projects (source). Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect data: (1) data could be linear inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Two-Stage Ensemble Learning (TSEL) approach to HDP, which contains two stages: ensemble multi-kernel domain adaptation (EMDA) stage and ensemble data sampling (EDS) stage. In the EMDA stage, we develop an Ensemble Multiple Kernel Correlation Alignment (EMKCA) predictor, which combines the advantage of multiple kernel learning and domain adaptation techniques. In the EDS stage, we employ RESample with replacement (RES) technique to learn multiple different EMKCA predictors and use average ensemble to combine them together. These two stages create an ensemble of defect predictors. Extensive experiments on 30 public projects show that the proposed TSEL approach outperforms a range of competing methods. The improvement is 20.14–33.92% in AUC, 36.05–54.78% in f-measure, and 5.48–19.93% in balance, respectively.},
journal = {Automated Software Engg.},
month = sep,
pages = {599–651},
numpages = {53},
keywords = {Heterogeneous defect prediction, Two-stage ensemble learning, Linear inseparability, Multiple kernel learning, Class imbalance, Data sampling, Domain adaptation}
}

@inproceedings{10.1145/2979779.2979783,
author = {Maheshwari, Suchi and Agarwal, Sonali},
title = {Three-way decision based Defect Prediction for Object Oriented Software},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979783},
doi = {10.1145/2979779.2979783},
abstract = {Early prediction of defective software module plays critical role in the software project development to reduce the overall development time, budgets and increases the customer satisfaction. The bug prediction based on two-way classification method classifies the software module as defective or non-defective. This method provides good accuracy measure but this metric is not sufficient in case if misclassification cost is concerned. Classifying the defective module as non-defective will lead to higher cost of entire software project at the end. In this study, three-way decision based classification method and Random Forest ensemble are used to predict the defect in Object Oriented Software to reduce the misclassification cost which will lead to avoid the cost overrun. The eclipse bug prediction dataset is used and experimental results show that the decision cost is reduced and accuracy is increased using our proposed method.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {4},
numpages = {6},
keywords = {Eclipse Bug Prediction dataset, Na\"{\i}ve Bayes, Random Forest, Software defect prediction, Three-way decision},
location = {Bikaner, India},
series = {AICTC '16}
}

@inproceedings{10.1145/3412841.3442027,
author = {Tsimpourlas, Foivos and Rajan, Ajitha and Allamanis, Miltiadis},
title = {Supervised learning over test executions as a test oracle},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442027},
doi = {10.1145/3412841.3442027},
abstract = {The challenge of automatically determining the correctness of test executions is referred to as the test oracle problem and is a key remaining issue for automated testing. The paper aims at solving the test oracle problem in a scalable and accurate way. To achieve this, we use supervised learning over test execution traces. We label a small fraction of the execution traces with their verdict of pass or fail. We use the labelled traces to train a neural network (NN) model to learn to distinguish runtime patterns for passing versus failing executions for a given program.We evaluate our approach using case studies from different application domains - 1. Module from Ethereum Blockchain, 2. Module from PyTorch deep learning framework, 3. Microsoft SEAL encryption library components and 4. Sed stream editor. We found the classification models for all subject programs resulted in high precision, recall and specificity, averaging to 89%, 88% and 92% respectively, while only training with an average 15% of the total traces. Our experiments show that the proposed NN model is promising as a test oracle and is able to learn runtime patterns to distinguish test executions for systems and tests from different application domains.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1521–1531},
numpages = {11},
keywords = {execution trace, neural networks, software testing, test oracle},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/2040660.2040688,
author = {Wahyudin, Dindin and Ramler, Rudolf and Biffl, Stefan},
title = {A framework for defect prediction in specific software project contexts},
year = {2008},
isbn = {9783642223853},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software defect prediction has drawn the attention of many researchers in empirical software engineering and software maintenance due to its importance in providing quality estimates and to identify the needs for improvement from project management perspective. However, most defect prediction studies seem valid primarily in a particular context and little concern is given on how to find out which prediction model is well suited for a given project context. In this paper we present a framework for conducting software defect prediction as aid for the project manager in the context of a particular project or organization. The framework has been aligned with practitioners' requirements and is supported by our findings from a systematical literature review on software defect prediction. We provide a guide to the body of existing studies on defect prediction by mapping the results of the systematic literature review to the framework.},
booktitle = {Proceedings of the Third IFIP TC 2 Central and East European Conference on Software Engineering Techniques},
pages = {261–274},
numpages = {14},
keywords = {metric-based defect prediction, software defect prediction, systematical literature review},
location = {Brno, Czech Republic},
series = {CEE-SET'08}
}

@inproceedings{10.1145/3377812.3390806,
author = {Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
title = {Open-vocabulary models for source code},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3390806},
doi = {10.1145/3377812.3390806},
abstract = {Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale.In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work, and outperforms the state of the art. To our knowledge, this is the largest NLM for code that has been reported.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {294–295},
numpages = {2},
keywords = {byte-pair encoding, naturalness of code, neural language models},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1007/s10664-017-9516-2,
author = {Zhang, Feng and Keivanloo, Iman and Zou, Ying},
title = {Data Transformation in Cross-project Defect Prediction},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9516-2},
doi = {10.1007/s10664-017-9516-2},
abstract = {Software metrics rarely follow a normal distribution. Therefore, software metrics are usually transformed prior to building a defect prediction model. To the best of our knowledge, the impact that the transformation has on cross-project defect prediction models has not been thoroughly explored. A cross-project model is built from one project and applied on another project. In this study, we investigate if cross-project defect prediction is affected by applying different transformations (i.e., log and rank transformations, as well as the Box-Cox transformation). The Box-Cox transformation subsumes log and other power transformations (e.g., square root), but has not been studied in the defect prediction literature. We propose an approach, namely Multiple Transformations (MT), to utilize multiple transformations for cross-project defect prediction. We further propose an enhanced approach MT+ to use the parameter of the Box-Cox transformation to determine the most appropriate training project for each target project. Our experiments are conducted upon three publicly available data sets (i.e., AEEEM, ReLink, and PROMISE). Comparing to the random forest model built solely using the log transformation, our MT+ approach improves the F-measure by 7, 59 and 43% for the three data sets, respectively. As a summary, our major contributions are three-fold: 1) conduct an empirical study on the impact that data transformation has on cross-project defect prediction models; 2) propose an approach to utilize the various information retained by applying different transformation methods; and 3) propose an unsupervised approach to select the most appropriate training project for each target project.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {3186–3218},
numpages = {33},
keywords = {Box-cox, Data transformation, Defect prediction, Software metrics}
}

@article{10.1134/S0005117921080014,
author = {Mukhachev, P. A. and Sadretdinov, T. R. and Pritykin, D. A. and Ivanov, A. B. and Solov’ev, S. V.},
title = {Modern Machine Learning Methods for Telemetry-Based Spacecraft Health Monitoring},
year = {2021},
issue_date = {Aug 2021},
publisher = {Plenum Press},
address = {USA},
volume = {82},
number = {8},
issn = {0005-1179},
url = {https://doi.org/10.1134/S0005117921080014},
doi = {10.1134/S0005117921080014},
journal = {Autom. Remote Control},
month = aug,
pages = {1293–1320},
numpages = {28},
keywords = {data mining, anomaly detection, flight control, technical diagnostics, telemetry data}
}

@inproceedings{10.1145/1985441.1985456,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
title = {Comparing fine-grained source code changes and code churn for bug prediction},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985456},
doi = {10.1145/1985441.1985456},
abstract = {A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified (LM) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes (SCC) for bug prediction. SCC captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of SCC and LM. The results show that SCC outperforms LM for learning bug prediction models.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {83–92},
numpages = {10},
keywords = {code churn, nonlinear regression, prediction models, software bugs, source code changes},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1109/ICSE.2019.00086,
author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},
title = {A novel neural source code representation based on abstract syntax tree},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00086},
doi = {10.1109/ICSE.2019.00086},
abstract = {Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {783–794},
numpages = {12},
keywords = {abstract syntax tree, code classification, code clone detection, neural network, source code representation},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1016/j.eswa.2015.03.013,
author = {\"{O}zt\"{u}rk, Muhammed Maruf and Cavusoglu, Unal and Zengin, Ahmet},
title = {A novel defect prediction method for web pages using k-means++},
year = {2015},
issue_date = {November 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {19},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.03.013},
doi = {10.1016/j.eswa.2015.03.013},
abstract = {Presents a novel defect clustering method.Shed new light to defect prediction methods.Depicts the prominence of k-means++ for software testing.Unveils the density of error rates of web elements. With the increase of the web software complexity, defect detection and prevention have become crucial processes in the software industry. Over the past decades, defect prediction research has reported encouraging results for reducing software product costs. Despite promising results, these researches have hardly been applied to web based systems using clustering algorithms. An appropriate implementation of the clustering in defect prediction may facilitate to estimate defects in a web page source code. One of the widely used clustering algorithms is k-means whose derived versions such as k-means++ show good performance on large-data sets. Here, we present a new defect clustering method using k-means++ for web page source codes. According to the experimental results, almost half of the defects are detected in the middle of web pages. k-means++ is significantly better than the other four clustering algorithms in three criteria on four data set. We also tested our method on four classifiers and the results have shown that after the clustering, Linear Discriminant Analysis is, in general, better than the other three classifiers.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6496–6506},
numpages = {11},
keywords = {Defect prediction, Fault clustering, Software testing, k-means++}
}

@inproceedings{10.1145/2896839.2896843,
author = {Koroglu, Yavuz and Sen, Alper and Kutluay, Doruk and Bayraktar, Akin and Tosun, Yalcin and Cinar, Murat and Kaya, Hasan},
title = {Defect prediction on a legacy industrial software: a case study on software with few defects},
year = {2016},
isbn = {9781450341547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896839.2896843},
doi = {10.1145/2896839.2896843},
abstract = {Context: Building defect prediction models for software projects is helpful for reducing the effort in locating defects. In this paper, we share our experiences in building a defect prediction model for a large industrial software project. We extract product and process metrics to build models and show that we can build an accurate defect prediction model even when 4% of the software is defective.Objective: Our goal in this project is to integrate a defect predictor into the continuous integration (CI) cycle of a large software project and decrease the effort in testing.Method: We present our approach in the form of an experience report. Specifically, we collected data from seven older versions of the software project and used additional features to predict defects of current versions. We compared several classification techniques including Naive Bayes, Decision Trees, and Random Forest and resampled our training data to present the company with the most accurate defect predictor.Results: Our results indicate that we can focus testing efforts by guiding the test team to only 8% of the software where 53% of actual defects can be found. Our model has 90% accuracy.Conclusion: We produce a defect prediction model with high accuracy for a software with defect rate of 4%. Our model uses Random Forest, that which we show has more predictive power than Naive Bayes, Logistic Regression and Decision Trees in our case.},
booktitle = {Proceedings of the 4th International Workshop on Conducting Empirical Studies in Industry},
pages = {14–20},
numpages = {7},
keywords = {defect prediction, experience report, feature selection, process metrics, random forest},
location = {Austin, Texas},
series = {CESI '16}
}

@inproceedings{10.1109/MSR.2017.20,
author = {Patil, Sangameshwar},
title = {Concept-based classification of software defect reports},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.20},
doi = {10.1109/MSR.2017.20},
abstract = {Automatic identification of the defect type from the textual description of a software defect can significantly speedup as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects.In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the "semantic similarity" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {182–186},
numpages = {5},
keywords = {explicit semantic analysis, mining software respositories, software defect classification, text data mining},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1007/978-3-030-82136-4_25,
author = {Kerestely, Arpad and Baicoianu, Alexandra and Bocu, Razvan},
title = {A Research Study on Running Machine Learning Algorithms on Big Data with Spark},
year = {2021},
isbn = {978-3-030-82135-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82136-4_25},
doi = {10.1007/978-3-030-82136-4_25},
abstract = {The design and implementation of proactive fault diagnosis systems concerning the bearings during their manufacturing process requires the selection of robust representation learning techniques, which belong to the broader scope of the machine learning techniques. Particular systems, such as those that are based on machine learning libraries like Scikit-learn, favor the actual processing of the data, while essentially disregarding relevant computational parameters, such as the speed of the data processing, or the consideration of scalability as an important design and implementation feature. This paper describes an integrated machine learning-based data analytics system, which processes the large amounts of data that are generated by the bearings manufacturing processes using a multinode cluster infrastructure. The data analytics system uses an optimally configured and deployed Spark environment. The proposed data analytics system is thoroughly assessed using a large dataset that stores real manufacturing data, which is generated by the respective bearings manufacturing processes. The performance assessment demonstrates that the described approach ensures the timely and scalable processing of the data. This achievement is relevant, as it exceeds the processing capabilities of significant existing data analytics systems.},
booktitle = {Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14–16, 2021, Proceedings, Part I},
pages = {307–318},
numpages = {12},
keywords = {Big data, High performance computing, Spark, Fault detection, Representation techniques, Machine learning},
location = {Tokyo, Japan}
}

@article{10.1016/j.asoc.2014.11.023,
author = {Malhotra, Ruchika},
title = {A systematic review of machine learning techniques for software fault prediction},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {27},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2014.11.023},
doi = {10.1016/j.asoc.2014.11.023},
abstract = {Reviews studies from 1991-2013 to assess application of ML techniques for SFP.Identifies seven categories of the ML techniques.Identifies 64 studies to answer the established research questions.Selects primary studies according to the quality assessment of the studies.Systematic literature review performs the following:Summarize ML techniques for SFP models.Assess performance accuracy and capability of ML techniques for constructing SFP models.Provide comparison between the ML and statistical techniques.Provide comparison of performance accuracy of different ML techniques.Summarize the strength and weakness of the ML techniques.Provides future guidelines to software practitioners and researchers. BackgroundSoftware fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. MethodIn this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. ResultsIn this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. ConclusionBased on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work.},
journal = {Appl. Soft Comput.},
month = feb,
pages = {504–518},
numpages = {15},
keywords = {Machine learning, Software fault proneness, Systematic literature review}
}

@inproceedings{10.1145/3183399.3183402,
author = {Koch, Patrick and Schekotihin, Konstantin and Jannach, Dietmar and Hofer, Birgit and Wotawa, Franz and Schmitz, Thomas},
title = {Combining spreadsheet smells for improved fault prediction},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183402},
doi = {10.1145/3183399.3183402},
abstract = {Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {25–28},
numpages = {4},
keywords = {fault prediction, spreadsheet QA, spreadsheet smells},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@article{10.1016/j.infsof.2013.09.001,
author = {Finlay, Jacqui and Pears, Russel and Connor, Andy M.},
title = {Data stream mining for predicting software build outcomes using source code metrics},
year = {2014},
issue_date = {February, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.09.001},
doi = {10.1016/j.infsof.2013.09.001},
abstract = {Context: Software development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process. Objective: This paper explores the concepts of representing a software development project as a process that results in the creation of a data stream. It also describes the extraction of metrics from the Jazz repository and the application of data stream mining techniques to identify useful metrics for predicting build success or failure. Method: This research is a systematic study using the Hoeffding Tree classification method used in conjunction with the Adaptive Sliding Window (ADWIN) method for detecting concept drift by applying the Massive Online Analysis (MOA) tool. Results: The results indicate that only a relatively small number of the available measures considered have any significance for predicting the outcome of a build over time. These significant measures are identified and the implication of the results discussed, particularly the relative difficulty of being able to predict failed builds. The Hoeffding Tree approach is shown to produce a more stable and robust model than traditional data mining approaches. Conclusion: Overall prediction accuracies of 75% have been achieved through the use of the Hoeffding Tree classification method. Despite this high overall accuracy, there is greater difficulty in predicting failure than success. The emergence of a stable classification tree is limited by the lack of data but overall the approach shows promise in terms of informing software development activities in order to minimize the chance of failure.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {183–198},
numpages = {16},
keywords = {Concept drift detection, Data stream mining, Hoeffding tree, Jazz, Software metrics, Software repositories}
}

@inproceedings{10.1145/3338906.3341462,
author = {Caulo, Maria},
title = {A taxonomy of metrics for software fault prediction},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341462},
doi = {10.1145/3338906.3341462},
abstract = {In the field of Software Fault Prediction (SFP), researchers exploit software metrics to build predictive models using machine learning and/or statistical techniques. SFP has existed for several decades and the number of metrics used has increased dramatically. Thus, the need for a taxonomy of metrics for SFP arises firstly to standardize the lexicon used in this field so that the communication among researchers is simplified and then to organize and systematically classify the used metrics. In this doctoral symposium paper, I present my ongoing work which aims not only to build such a taxonomy as comprehensive as possible, but also to provide a global understanding of the metrics for SFP in terms of detailed information: acronym(s), extended name, univocal description, granularity of the fault prediction (e.g., method and class), category, and research papers in which they were used.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1144–1147},
numpages = {4},
keywords = {software fault prediction, software metrics, taxonomy},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1016/j.knosys.2021.107541,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {DNNAttention: A deep neural network and attention based architecture for cross project defect number prediction},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {233},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107541},
doi = {10.1016/j.knosys.2021.107541},
journal = {Know.-Based Syst.},
month = dec,
numpages = {30},
keywords = {Cross project defect prediction, Deep neural network, Attention layer, Long short term memory (LSTM), Software defect number prediction}
}

@inproceedings{10.1007/978-3-030-62822-2_12,
author = {Zhou, Yangxi and Zhu, Yan and Chen, Liangyu},
title = {Software Defect-Proneness Prediction with Package Cohesion and Coupling Metrics Based on Complex Network Theory},
year = {2020},
isbn = {978-3-030-62821-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62822-2_12},
doi = {10.1007/978-3-030-62822-2_12},
abstract = {Driven by functionality requirements, software codes are increasingly inflated, and invocations between codes are frequent and random. This makes it difficult for programmers to be thoughtful when modifying code, increasing the risk of defects. In an object-oriented software system, packages take the role of a middle tier that aggregates classes and limits class access. However, as the software system evolves, the logic and correctness of packages are weakened. In this paper, we explore the relation between package metrics and object-oriented software defect-proneness. We use two metrics of package cohesion and coupling based on complex network theory to verify the impact of code structure on software quality. On six Java software systems, the experimental result shows that the cohesion and coupling metrics play a positive role in software defect prediction, and they can correctly and effectively evaluate package organization structure. Meanwhile, our study confirms that compliance with the design principle of high cohesion and low coupling can reduce the risk of software defect-proneness and improve software quality.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 6th International Symposium, SETTA 2020, Guangzhou, China, November 24–27, 2020, Proceedings},
pages = {186–201},
numpages = {16},
keywords = {Software package, Defect proneness, Cohesion and coupling metrics},
location = {Guangzhou, China}
}

@article{10.1007/s10664-011-9173-9,
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
title = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9173-9},
doi = {10.1007/s10664-011-9173-9},
abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {531–577},
numpages = {47},
keywords = {Change metrics, Defect prediction, Source code metrics}
}

@article{10.1016/j.infsof.2008.04.008,
author = {Chang, Ching-Pao and Chu, Chih-Ping and Yeh, Yu-Fang},
title = {Integrating in-process software defect prediction with association mining to discover defect pattern},
year = {2009},
issue_date = {February, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.04.008},
doi = {10.1016/j.infsof.2008.04.008},
abstract = {Rather than detecting defects at an early stage to reduce their impact, defect prevention means that defects are prevented from occurring in advance. Causal analysis is a common approach to discover the causes of defects and take corrective actions. However, selecting defects to analyze among large amounts of reported defects is time consuming, and requires significant effort. To address this problem, this study proposes a defect prediction approach where the reported defects and performed actions are utilized to discover the patterns of actions which are likely to cause defects. The approach proposed in this study is adapted from the Action-Based Defect Prediction (ABDP), an approach uses the classification with decision tree technique to build a prediction model, and performs association rule mining on the records of actions and defects. An action is defined as a basic operation used to perform a software project, while a defect is defined as software flaws and can arise at any stage of the software process. The association rule mining finds the maximum rule set with specific minimum support and confidence and thus the discovered knowledge can be utilized to interpret the prediction models and software process behaviors. The discovered patterns then can be applied to predict the defects generated by the subsequent actions and take necessary corrective actions to avoid defects. The proposed defect prediction approach applies association rule mining to discover defect patterns, and multi-interval discretization to handle the continuous attributes of actions. The proposed approach is applied to a business project, giving excellent prediction results and revealing the efficiency of the proposed approach. The main benefit of using this approach is that the discovered defect patterns can be used to evaluate subsequent actions for in-process projects, and reduce variance of the reported data resulting from different projects. Additionally, the discovered patterns can be used in causal analysis to identify the causes of defects for software process improvement.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {375–384},
numpages = {10},
keywords = {Association rule, Multi-interval discretization, Software defect prediction}
}

@article{10.1007/s10664-018-9661-2,
author = {Huang, Qiao and Xia, Xin and Lo, David},
title = {Revisiting supervised and unsupervised models for effort-aware just-in-time defect prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9661-2},
doi = {10.1007/s10664-018-9661-2},
abstract = {Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (i.e., LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find about 12% - 27% more defective changes than a state-of-the-art supervised model (i.e., EALR) when using different evaluation settings. This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.’s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners’ patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS+, which leverages the idea of both EALR and LT. We investigate the performance of CBS+ using three different evaluation settings, including time-wise cross-validation, 10-times 10-fold cross-validation and cross-project validation. When compared with EALR, CBS+ detects about 15% - 26% more defective changes, while keeping the number of context switches and initial false alarms close to those of EALR. When compared with LT, the number of defective changes detected by CBS+ is comparable to LT’s result, while CBS+ significantly reduces context switches and initial false alarms before first success. Finally, we discuss how to balance the tradeoff between the number of inspected defects and context switches, and present the implications of our findings for practitioners and researchers.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2823–2862},
numpages = {40},
keywords = {Defect prediction, Evaluation metrics, Research bias}
}

@article{10.1016/j.eswa.2020.114022,
author = {Rauber, Thomas Walter and da Silva Loca, Antonio Luiz and Boldt, Francisco de Assis and Rodrigues, Alexandre Loureiros and Varej\~{a}o, Fl\'{a}vio Miguel},
title = {An experimental methodology to evaluate machine learning methods for fault diagnosis based on vibration signals},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114022},
doi = {10.1016/j.eswa.2020.114022},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {18},
keywords = {Fault detection, CWRU bearing fault database, Performance criteria, Classification, Pattern recognition, Machine learning}
}

@inproceedings{10.1145/2723742.2723754,
author = {Muthukumaran, K. and Rallapalli, Akhila and Murthy, N. L. Bhanu},
title = {Impact of Feature Selection Techniques on Bug Prediction Models},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723754},
doi = {10.1145/2723742.2723754},
abstract = {Several change metrics and source code metrics have been introduced and proved to be effective features in building bug prediction models. Researchers performed comparative studies of bug prediction models built using the individual metrics as well as combination of these metrics. In this paper, we investigate whether the prediction accuracy of bug prediction models is improved by applying feature selection techniques. We explore if there is one algorithm amongst ten popular feature selection algorithms that consistently fares better than others across sixteen bench marked open source projects. We also study whether the metrics in best feature subset are consistent across projects.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {120–129},
numpages = {10},
keywords = {Bug prediction, Feature selection, Software Quality},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/2884781.2884804,
author = {Wang, Song and Liu, Taiyue and Tan, Lin},
title = {Automatically learning semantic features for defect prediction},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884804},
doi = {10.1145/2884781.2884804},
abstract = {Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models.To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs).Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {297–308},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/1868328.1868342,
author = {Jureczko, Marian and Madeyski, Lech},
title = {Towards identifying software project clusters with regard to defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868342},
doi = {10.1145/1868328.1868342},
abstract = {Background: This paper describes an analysis that was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary study perfomed before showed the need for a further in-depth analysis in order to identify project clusters.Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency.Method: Hierarchical and k-means clustering, as well as Kohonen's neural network was used to find groups of similar projects. The obtained clusters were investigated with the discriminant analysis. For each of the identified group a statistical analysis has been conducted in order to distinguish whether this group really exists. Two defect prediction models were created for each of the identified groups. The first one was based on the projects that belong to a given group, and the second one - on all the projects. Then, both models were applied to all versions of projects from the investigated group. If the predictions from the model based on projects that belong to the identified group are significantly better than the all-projects model (the mean values were compared and statistical tests were used), we conclude that the group really exists.Results: Six different clusters were identified and the existence of two of them was statistically proven: 1) cluster proprietary B -- T=19, p=0.035, r=0.40; 2) cluster proprietary/open - t(17)=3.18, p=0.05, r=0.59. The obtained effect sizes (r) represent large effects according to Cohen's benchmark, which is a substantial finding.Conclusions: The two identified clusters were described and compared with results obtained by other researchers. The results of this work makes next step towards defining formal methods of reuse defect prediction models by identifying groups of projects within which the same defect prediction model may be used. Furthermore, a method of clustering was suggested and applied.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {9},
numpages = {10},
keywords = {clustering, defect prediction, design metrics, size metrics},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/3439961.3439979,
author = {Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Predicting Software Defects with Explainable Machine Learning},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439979},
doi = {10.1145/3439961.3439979},
abstract = {Most software systems must evolve to cope with stakeholders’ requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-to-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {18},
numpages = {10},
keywords = {NASA datasets, SHAP values, explainable models, software defects},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.5555/1881763.1881787,
author = {Liu, Guang-Jie and Wang, Wen-Yong},
title = {Research an educational software defect prediction model based on SVM},
year = {2010},
isbn = {3642145329},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We must pay attention and find defects, defects through the prediction to quantify the quality management and quality in order to achieve this goal, requires an estimate of the various defect detection process. Software defects are the departure of software are products' anticipative function. This paper collecting the data of the software defects, then, using the SVM model the predictive values are gained analyzing the predictive results, software are organizations can improve software control measure software process and allocate testing resources effectively.},
booktitle = {Proceedings of the Entertainment for Education, and 5th International Conference on E-Learning and Games},
pages = {215–222},
numpages = {8},
keywords = {SVM, educational software, software defect, software lifecycle},
location = {Changchun, China},
series = {Edutainment'10}
}

@article{10.1007/s11219-019-09460-7,
author = {Qin, Fangyun and Wan, Xiaohui and Yin, Beibei},
title = {An empirical study of factors affecting cross-project aging-related bug prediction with TLAP},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09460-7},
doi = {10.1007/s11219-019-09460-7},
abstract = {Software aging is a phenomenon in which long-running software systems show an increasing failure rate and/or progressive performance degradation. Due to their nature, Aging-Related Bugs (ARBs) are hard to discover during software testing and are also challenging to reproduce. Therefore, automatically predicting ARBs before software release can help developers reduce ARB impact or avoid ARBs. Many bug prediction approaches have been proposed, and most of them show effectiveness in within-project prediction settings. However, due to the low presence and reproducing difficulty of ARBs, it is usually hard to collect sufficient training data to build an accurate prediction model. A recent work proposed a method named Transfer Learning based Aging-related bug Prediction (TLAP) for performing cross-project ARB prediction. Although this method considerably improves cross-project ARB prediction performance, it has been observed that its prediction result is affected by several key factors, such as the normalization methods, kernel functions, and machine learning classifiers. Therefore, this paper presents the first empirical study to examine the impact of these factors on the effectiveness of cross-project ARB prediction in terms of single-factor pattern, bigram pattern, and triplet pattern and validates the results with the Scott-Knott test technique. We find that kernel functions and classifiers are key factors affecting the effectiveness of cross-project ARB prediction, while normalization methods do not show statistical influence. In addition, the order of values in three single-factor patterns is maintained in three bigram patterns and one triplet pattern to a large extent. Similarly, the order of values in the three bigram patterns is also maintained in the triplet pattern.},
journal = {Software Quality Journal},
month = mar,
pages = {107–134},
numpages = {28},
keywords = {Aging-related bugs, Software aging, Cross-project, Empirical study}
}

@inproceedings{10.1145/3196321.3196331,
author = {Xu, Zhou and Li, Shuai and Tang, Yutian and Luo, Xiapu and Zhang, Tao and Liu, Jin and Xu, Jun},
title = {Cross version defect prediction with representative data via sparse subset selection},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196331},
doi = {10.1145/3196321.3196331},
abstract = {Software defect prediction aims at detecting the defect-prone software modules by mining historical development data from software repositories. If such modules are identified at the early stage of the development, it can save large amounts of resources. Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules of the current version. However, software development is a constantly-evolving process which leads to the data distribution differences across versions within the same project. The distribution differences will degrade the performance of the classification model. In this paper, we approach this issue by leveraging a state-of-the-art Dissimilarity-based Sparse Subset Selection (DS3) method. This method selects a representative module subset from the prior version based on the pairwise dissimilarities between the modules of two versions and assigns each module of the current version to one of the representative modules. These selected modules can well represent the modules of the current version, thus mitigating the distribution differences. We evaluate the effectiveness of DS3 for CVDP performance on total 40 cross-version pairs from 56 versions of 15 projects with three traditional and two effort-aware indicators. The extensive experiments show that DS3 outperforms three baseline methods, especially in terms of two effort-aware indicators.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {132–143},
numpages = {12},
keywords = {cross version defect prediction, pairwise dissimilarities, representative data, sparse subset selection},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/2884781.2884857,
author = {Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, Ahmed E. and Matsumoto, Kenichi},
title = {Automated parameter optimization of classification techniques for defect prediction models},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884857},
doi = {10.1145/2884781.2884857},
abstract = {Defect prediction models are classifiers that are trained to identify defect-prone software modules. Such classifiers have configurable parameters that control their characteristics (e.g., the number of trees in a random forest classifier). Recent studies show that these classifiers may underperform due to the use of suboptimal default parameter settings. However, it is impractical to assess all of the possible settings in the parameter spaces. In this paper, we investigate the performance of defect prediction models where Caret --- an automated parameter optimization technique --- has been applied. Through a case study of 18 datasets from systems that span both proprietary and open source domains, we find that (1) Caret improves the AUC performance of defect prediction models by as much as 40 percentage points; (2) Caret-optimized classifiers are at least as stable as (with 35% of them being more stable than) classifiers that are trained using the default settings; and (3) Caret increases the likelihood of producing a top-performing classifier by as much as 83%. Hence, we conclude that parameter settings can indeed have a large impact on the performance of defect prediction models, suggesting that researchers should experiment with the parameters of the classification techniques. Since automated parameter optimization techniques like Caret yield substantially benefits in terms of performance improvement and stability, while incurring a manageable additional computational cost, they should be included in future defect prediction studies.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {321–332},
numpages = {12},
keywords = {classification techniques, experimental design, parameter optimization, software defect prediction},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1155/2020/8858010,
author = {Shen, Zhidong and Chen, Si and Coppolino, Luigi},
title = {A Survey of Automatic Software Vulnerability Detection, Program Repair, and Defect Prediction Techniques},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1939-0114},
url = {https://doi.org/10.1155/2020/8858010},
doi = {10.1155/2020/8858010},
abstract = {Open source software has been widely used in various industries due to its openness and flexibility, but it also brings potential software security problems. Together with the large-scale increase in the number of software and the increase in complexity, the traditional manual methods to deal with these security issues are inefficient and cannot meet the current cyberspace security requirements. Therefore, it is an important research topic for researchers in the field of software security to develop more intelligent technologies to apply to potential security issues in software. The development of deep learning technology has brought new opportunities for the study of potential security issues in software, and researchers have successively proposed many automation methods. In this paper, these automation technologies are evaluated and analysed in detail from three aspects: software vulnerability detection, software program repair, and software defect prediction. At the same time, we point out some problems of these research methods, give corresponding solutions, and finally look forward to the application prospect of deep learning technology in automated software vulnerability detection, automated program repair, and automated defect prediction.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {16}
}

@inproceedings{10.1109/ASE.2013.6693087,
author = {Jiang, Tian and Tan, Lin and Kim, Sunghun},
title = {Personalized defect prediction},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693087},
doi = {10.1109/ASE.2013.6693087},
abstract = {Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance.This paper proposes personalized defect prediction--building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java--the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {279–289},
numpages = {11},
keywords = {change classification, machine learning, personalized defect prediction, software reliability},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ unleashed: an open implementation of the SZZ algorithm - featuring example usage in a study of just-in-time bug prediction for the Jenkins project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {SZZ, defect prediction, issue tracking, mining software repositories},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1109/ESEM.2011.29,
author = {Li, Lianfa and Leung, Hareton},
title = {Mining Static Code Metrics for a Robust Prediction of Software Defect-Proneness},
year = {2011},
isbn = {9780769546049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ESEM.2011.29},
doi = {10.1109/ESEM.2011.29},
abstract = {Defect-proneness prediction is affected by multiple aspects including sampling bias, non-metric factors, uncertainty of models etc. These aspects often contribute to prediction uncertainty and result in variance of prediction. This paper proposes two methods of data mining static code metrics to enhance defect-proneness prediction. Given little non-metric or qualitative information extracted from software codes, we first suggest to use a robust unsupervised learning method, shared nearest neighbors (SNN) to extract the similarity patterns of the code metrics. These patterns indicate similar characteristics of the components of the same cluster that may result in introduction of similar defects. Using the similarity patterns with code metrics as predictors, defect-proneness prediction may be improved. The second method uses the Occam's windows and Bayesian model averaging to deal with model uncertainty: first, the datasets are used to train and cross-validate multiple learners and then highly qualified models are selected and integrated into a robust prediction. From a study based on 12 datasets from NASA, we conclude that our proposed solutions can contribute to a better defect-proneness prediction.},
booktitle = {Proceedings of the 2011 International Symposium on Empirical Software Engineering and Measurement},
pages = {207–214},
numpages = {8},
keywords = {data mining, defect-proneness, robust prediction, software quality, uncertainty},
series = {ESEM '11}
}

@inproceedings{10.1109/IROS51168.2021.9636868,
author = {Gu, Haoyuan and Hu, Hanjiang and Wang, Hesheng and Chen, Weidong},
title = {Soft Manipulator Fault Detection and Identification Using ANC-based LSTM},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636868},
doi = {10.1109/IROS51168.2021.9636868},
abstract = {Timely fault detection and identification (FDI) of soft manipulators are critical in the design of surgical systems to improve reliability. However, due to the intrinsic compliance of soft manipulators, their end effectors vibrate during the dynamic control process, which introduces noise into the measured signals and makes FDI of soft manipulators challenging. This paper proposes a novel method to accomplish these tasks based on Long Short Term Memory (LSTM) recurrent neural network. Based on LSTM network, a new Attention-based Noise Compensation (ANC) module is proposed to enable the network to filter the noise merged with signals input in a self-supervision manner. Moreover, weighted cross entropy loss is introduced to balance the normal and faulty samples in the training set. Of the 9930 samples presented to the model, 9489 are correctly diagnosed in less than 1.0 second, which implies that the method can learn the spatial and temporal dependence of the signals and distinguish the healthy modes from the faulty ones. Finally, we compare the ANC-based method with the vanilla LSTM method and the state-of-art Bruin et al. method. From the comparison, we conclude that the ANC-based method proposed in this paper not only shortens the time cost of the FDI process but also suppresses the sensitivity of diagnosis results to noise. Source code, pre-trained models and dataset are available on https://github.com/IRMVLab/ANC-LSTM-fault-detection.},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {1702–1707},
numpages = {6},
location = {Prague, Czech Republic}
}

@article{10.1007/s10515-017-0220-7,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Wu, Fei and Zhu, Xiaoke and Xu, Baowen and Ying, Shi},
title = {Cost-sensitive transfer kernel canonical correlation analysis for heterogeneous defect prediction},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0220-7},
doi = {10.1007/s10515-017-0220-7},
abstract = {Cross-project defect prediction (CPDP) refers to predicting defects in a target project using prediction models trained from historical data of other source projects. And CPDP in the scenario where source and target projects have different metric sets is called heterogeneous defect prediction (HDP). Recently, HDP has received much research interest. Existing HDP methods only consider the linear correlation relationship among the features (metrics) of the source and target projects, and such models are insufficient to evaluate nonlinear correlation relationship among the features. So these methods may suffer from the linearly inseparable problem in the linear feature space. Furthermore, existing HDP methods do not take the class imbalance problem into consideration. Unfortunately, the imbalanced nature of software defect datasets increases the learning difficulty for the predictors. In this paper, we propose a new cost-sensitive transfer kernel canonical correlation analysis (CTKCCA) approach for HDP. CTKCCA can not only make the data distributions of source and target projects much more similar in the nonlinear feature space, where the learned features have favorable separability, but also utilize the different misclassification costs for defective and defect-free classes to alleviate the class imbalance problem. We perform the Friedman test with Nemenyi's post-hoc statistical test and the Cliff's delta effect size test for the evaluation. Extensive experiments on 28 public projects from five data sources indicate that: (1) CTKCCA significantly performs better than the related CPDP methods; (2) CTKCCA performs better than the related state-of-the-art HDP methods.},
journal = {Automated Software Engg.},
month = jun,
pages = {201–245},
numpages = {45},
keywords = {Class imbalance, Cost-sensitive learning, Heterogeneous defect prediction, Kernel canonical correlation analysis, Transfer learning}
}

@article{10.5555/3271870.3271878,
title = {Software fault prediction using firefly algorithm},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {6},
number = {3–4},
issn = {1758-8715},
abstract = {The software fault prediction SFP literature has shown an immense growth of the research studies involving the artificial neural network ANN based fault prediction models. However, the default gradient descent back propagation neural networks BPNNs have a high risk of getting stuck in the local minima of the search space. A class of nature inspired computing methods overcomes this disadvantage of BPNNs and has helped ANNs to evolve into a class of adaptive ANN. In this work, we propose a hybrid SFP model built using firefly algorithm FA and artificial neural network ANN, along with an empirical comparison with GA and PSO based evolutionary methods in optimising the connection weights of ANN. Seven different datasets were involved and MSE and the confusion matrix parameters were used for performance evaluation. The results have shown that FA-ANN model has performed better than the genetic and particle swarm optimised ANN fault prediction models.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {356–377},
numpages = {22}
}

@inproceedings{10.1007/978-3-030-91265-9_11,
author = {Wei, Shaozhi and Mo, Ran and Xiong, Pu and Zhang, Siyuan and Zhao, Yang and Li, Zengyang},
title = {Predicting and Monitoring Bug-Proneness at the Feature Level},
year = {2021},
isbn = {978-3-030-91264-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91265-9_11},
doi = {10.1007/978-3-030-91265-9_11},
abstract = {Enabling quick feature modification and delivery is important for a project’s success. Obtaining early estimates of software features’ bug-proneness is helpful for effectively allocating resources to the bug-prone features requiring further fixes. Researchers have proposed various studies on bug prediction at different granularity levels, such as class level, package level, method level, etc. However, there exists little work building predictive models at the feature level. In this paper, we investigated how to predict bug-prone features and monitor their evolution. More specifically, we first identified a project’s features and their involved files. Next, we collected a suite of code metrics and selected a relevant set of metrics as attributes to be used for six machine learning algorithms to predict bug-prone features. Through our evaluation, we have presented that using the machine learning algorithms with an appropriate set of code metrics, we can build effective models of bug prediction at the feature level. Furthermore, we build regression models to monitor growth trends of bug-prone features, which shows how these features accumulate bug-proneness over time.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 7th International Symposium, SETTA 2021, Beijing, China, November 25–27, 2021, Proceedings},
pages = {201–218},
numpages = {18},
keywords = {Code metrics, Machine learning, Feature bug prediction},
location = {Beijing, China}
}

@inproceedings{10.1109/RAISE.2019.00010,
author = {Ferenc, Rudolf and Hegedundefineds, P\'{e}ter and Gyimesi, P\'{e}ter and Antal, G\'{a}bor and B\'{a}n, D\'{e}nes and Gyim\'{o}thy, Tibor},
title = {Challenging machine learning algorithms in predicting vulnerable JavaScript functions},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RAISE.2019.00010},
doi = {10.1109/RAISE.2019.00010},
abstract = {The rapid rise of cyber-crime activities and the growing number of devices threatened by them place software security issues in the spotlight. As around 90% of all attacks exploit known types of security issues, finding vulnerable components and applying existing mitigation techniques is a viable practical approach for fighting against cyber-crime. In this paper, we investigate how the state-of-the-art machine learning techniques, including a popular deep learning algorithm, perform in predicting functions with possible security vulnerabilities in JavaScript programs.We applied 8 machine learning algorithms to build prediction models using a new dataset constructed for this research from the vulnerability information in public databases of the Node Security Project and the Snyk platform, and code fixing patches from GitHub. We used static source code metrics as predictors and an extensive grid-search algorithm to find the best performing models. We also examined the effect of various re-sampling strategies to handle the imbalanced nature of the dataset.The best performing algorithm was KNN, which created a model for the prediction of vulnerable functions with an F-measure of 0.76 (0.91 precision and 0.66 recall). Moreover, deep learning, tree and forest based classifiers, and SVM were competitive with F-measures over 0.70. Although the F-measures did not vary significantly with the re-sampling strategies, the distribution of precision and recall did change. No re-sampling seemed to produce models preferring high precision, while resampling strategies balanced the IR measures.},
booktitle = {Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {8–14},
numpages = {7},
keywords = {JavaScript, code metrics, dataset, deep learning, machine learning, vulnerability},
location = {Montreal, Quebec, Canada},
series = {RAISE '19}
}

@inproceedings{10.1145/3460319.3464844,
author = {Dutta, Saikat and Selvam, Jeeva and Jain, Aryaman and Misailovic, Sasa},
title = {TERA: optimizing stochastic regression tests in machine learning projects},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464844},
doi = {10.1145/3460319.3464844},
abstract = {The stochastic nature of many Machine Learning (ML) algorithms makes testing of ML tools and libraries challenging. ML algorithms allow a developer to control their accuracy and run-time through a set of hyper-parameters, which are typically manually selected in tests. This choice is often too conservative and leads to slow test executions, thereby increasing the cost of regression testing.  We propose TERA, the first automated technique for reducing the cost of regression testing in Machine Learning tools and libraries(jointly referred to as projects) without making the tests more flaky. TERA solves the problem of exploring the trade-off space between execution time of the test and its flakiness as an instance of Stochastic Optimization over the space of algorithm hyper-parameters. TERA presents how to leverage statistical convergence-testing techniques to estimate the level of flakiness of the test for a specific choice of hyper-parameters during optimization.  We evaluate TERA on a corpus of 160 tests selected from 15 popular machine learning projects. Overall, TERA obtains a geo-mean speedup of 2.23x over the original tests, for the minimum passing probability threshold of 99%. We also show that the new tests did not reduce fault detection ability through a mutation study and a study on a set of 12 historical build failures in studied projects.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {413–426},
numpages = {14},
keywords = {Bayesian Optimization, Machine Learning, Software Testing, Test Optimization},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1016/j.compeleceng.2021.107362,
author = {P, Gouthaman and Sankaranarayanan, Suresh},
title = {Prediction of Risk Percentage in Software Projects by Training Machine Learning Classifiers},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107362},
doi = {10.1016/j.compeleceng.2021.107362},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {9},
keywords = {Software model, Agile, Waterfall, Evolutionary, Incremental, Machine learning, Risk prediction}
}

@inproceedings{10.1145/3127005.3127015,
author = {Amasaki, Sousuke},
title = {On Applicability of Cross-project Defect Prediction Method for Multi-Versions Projects},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127015},
doi = {10.1145/3127005.3127015},
abstract = {Context: Cross-project defect prediction (CPDP) research has been popular, and many CPDP methods have been proposed so far. As the straightforward use of Cross-project (CP) data was useless, those methods filter, weigh, and adapt CP data for a target project data. This idea would also be useful for a project having past defect data. Objective: To evaluate the applicability of CPDP methods for multi-versions projects. The evaluation focused on the relationship between the performance change and the proximity of older release data to a target project. Method: We conducted experiments that compared the predictive performance between using older version data with and without Nearest Neighbor (NN) filter, a classic CPDP method. Results: NN-filter could not make clear differences in predictive performance. Conclusions: NN-filter was not helpful for improving predictive performance with older release data.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {93–96},
numpages = {4},
keywords = {Cross-Project, Defect Prediction, Experiment},
location = {Toronto, Canada},
series = {PROMISE}
}

@article{10.1155/2021/6662932,
author = {Gupta, Mansi and Rajnish, Kumar and Bhattacharjee, Vandana and Gou, Jianping},
title = {Impact of Parameter Tuning for Optimizing Deep Neural Network Models for Predicting Software Faults},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/6662932},
doi = {10.1155/2021/6662932},
abstract = {Deep neural network models built by the appropriate design decisions are crucial to obtain the desired classifier performance. This is especially desired when predicting fault proneness of software modules. When correctly identified, this could help in reducing the testing cost by directing the efforts more towards the modules identified to be fault prone. To be able to build an efficient deep neural network model, it is important that the parameters such as number of hidden layers, number of nodes in each layer, and training details such as learning rate and regularization methods be investigated in detail. The objective of this paper is to show the importance of hyperparameter tuning in developing efficient deep neural network models for predicting fault proneness of software modules and to compare the results with other machine learning algorithms. It is shown that the proposed model outperforms the other algorithms in most cases.},
journal = {Sci. Program.},
month = jan,
numpages = {17}
}

@inproceedings{10.1145/3127005.3127013,
author = {Valdivia-Garcia, Harold and Nagappan, Meiyappan},
title = {The Characteristics of False-Negatives in File-level Fault Prediction},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127013},
doi = {10.1145/3127005.3127013},
abstract = {Over the years, a plethora of works has proposed more and more sophisticated machine learning techniques to improve fault prediction models. However, past studies using product metrics from closed-source projects, found a ceiling effect in the performance of fault prediction models. On the other hand, other studies have shown that process metrics are significantly better than product metrics for fault prediction. In our case study therefore we build models that include both product and process metrics taken together. We find that the ceiling effect found in prior studies exists even when we consider process metrics. We then qualitatively investigate the bug reports, source code files, and commit information for the bugs in the files that are false-negative in our fault prediction models trained using product and process metrics. Surprisingly, our qualitative analysis shows that bugs related to false-negative files and true-positive files are similar in terms of root causes, impact and affected components, and consequently such similarities might be exploited to enhance fault prediction models.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {Code Metrics, Post-release Defects, Process Metrics},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1145/2915970.2916007,
author = {Petri\'{c}, Jean and Bowes, David and Hall, Tracy and Christianson, Bruce and Baddoo, Nathan},
title = {The jinx on the NASA software defect data sets},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2916007},
doi = {10.1145/2915970.2916007},
abstract = {Background: The NASA datasets have previously been used extensively in studies of software defects. In 2013 Shepperd et al. presented an essential set of rules for removing erroneous data from the NASA datasets making this data more reliable to use.Objective: We have now found additional rules necessary for removing problematic data which were not identified by Shepperd et al.Results: In this paper, we demonstrate the level of erroneous data still present even after cleaning using Shepperd et al.'s rules and apply our new rules to remove this erroneous data.Conclusion: Even after systematic data cleaning of the NASA MDP datasets, we found new erroneous data. Data quality should always be explicitly considered by researchers before use.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {13},
numpages = {5},
keywords = {data quality, machine learning, software defect prediction},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/2875913.2875922,
author = {Tang, Hao and Lan, Tian and Hao, Dan and Zhang, Lu},
title = {Enhancing Defect Prediction with Static Defect Analysis},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875922},
doi = {10.1145/2875913.2875922},
abstract = {In the software development process, how to develop better software at lower cost has been a major issue of concern. One way that helps is to find more defects as early as possible, on which defect prediction can provide effective guidance. The most popular defect prediction technique is to build defect prediction models based on machine learning. To improve the performance of defect prediction model, selecting appropriate features is critical. On the other hand, static analysis is usually used in defect detection. As static defect analyzers detects defects by matching some well-defined "defect patterns", its result is useful for locating defects. However, defect prediction and static defect analysis are supposed to be two parallel areas due to the differences in research motivation, solution and granularity.In this paper, we present a possible approach to improve the performance of defect prediction with the help of static analysis techniques. Specifically, we present to extract features based on defect patterns from static defect analyzers to improve the performance of defect prediction models. Based on this approach, we implemented a defect prediction tool and set up experiments to measure the effect of the features.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {43–51},
numpages = {9},
keywords = {Defect, code feature, defect pattern, machine learning, predictive model, static defect analyzer},
location = {Wuhan, China},
series = {Internetware '15}
}

@article{10.1007/s10664-014-9346-4,
author = {Ryu, Duksan and Choi, Okjoo and Baik, Jongmoon},
title = {Value-cognitive boosting with a support vector machine for cross-project defect prediction},
year = {2016},
issue_date = {February  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9346-4},
doi = {10.1007/s10664-014-9346-4},
abstract = {It is well-known that software defect prediction is one of the most important tasks for software quality improvement. The use of defect predictors allows test engineers to focus on defective modules. Thereby testing resources can be allocated effectively and the quality assurance costs can be reduced. For within-project defect prediction (WPDP), there should be sufficient data within a company to train any prediction model. Without such local data, cross-project defect prediction (CPDP) is feasible since it uses data collected from similar projects in other companies. Software defect datasets have the class imbalance problem increasing the difficulty for the learner to predict defects. In addition, the impact of imbalanced data on the real performance of models can be hidden by the performance measures chosen. We investigate if the class imbalance learning can be beneficial for CPDP. In our approach, the asymmetric misclassification cost and the similarity weights obtained from distributional characteristics are closely associated to guide the appropriate resampling mechanism. We performed the effect size A-statistics test to evaluate the magnitude of the improvement. For the statistical significant test, we used Wilcoxon rank-sum test. The experimental results show that our approach can provide higher prediction performance than both the existing CPDP technique and the existing class imbalance technique.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {43–71},
numpages = {29},
keywords = {Boosting, Class imbalance, Cross-project defect prediction, Transfer learning}
}

@inproceedings{10.1145/3468264.3468614,
author = {Cito, J\"{u}rgen and Dillig, Isil and Kim, Seohyun and Murali, Vijayaraghavan and Chandra, Satish},
title = {Explaining mispredictions of machine learning models using rule induction},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468614},
doi = {10.1145/3468264.3468614},
abstract = {While machine learning (ML) models play an increasingly prevalent role in many software engineering tasks, their prediction accuracy is often problematic. When these models do mispredict, it can be very difficult to isolate the cause. In this paper, we propose a technique that aims to facilitate the debugging process of trained statistical models. Given an ML model and a labeled data set, our method produces an interpretable characterization of the data on which the model performs particularly poorly. The output of our technique can be useful for understanding limitations of the training data or the model itself; it can also be useful for ensembling if there are multiple models with different strengths. We evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within Facebook.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {716–727},
numpages = {12},
keywords = {explainability, machine learning, rule induction},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3318216.3363305,
author = {Soualhia, Mbarka and Fu, Chunyan and Khomh, Foutse},
title = {Infrastructure fault detection and prediction in edge cloud environments},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363305},
doi = {10.1145/3318216.3363305},
abstract = {As an emerging 5G system component, edge cloud becomes one of the key enablers to provide services such us mission critical, IoT and content delivery applications. However, because of limited fail-over mechanisms in edge clouds, faults (e.g., CPU or HDD faults) are highly undesirable. When infrastructure faults occur in edge clouds, they can accumulate and propagate; leading to severe degradation of system and application performance. It is therefore crucial to identify these faults early on and mitigate them. In this paper, we propose a framework to detect and predict several faults at infrastructure-level of edge clouds using supervised machine learning and statistical techniques. The proposed framework is composed of three main components responsible for: (1) data pre-processing, (2) fault detection, and (3) fault prediction. The results show that the framework allows to timely detect and predict several faults online. For instance, using Support Vector Machine (SVM), Random Forest(RF) and Neural Network(NN)models, the framework is able to detect non-fatal CPU and HDD overload faults with an F1 score of more than 95%. For the prediction, the Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) have comparable accuracy at 96.47% vs. 96.88% for CPU-overload fault and 85.52% vs. 88.73% for network fault.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {222–235},
numpages = {14},
location = {Arlington, Virginia},
series = {SEC '19}
}

@inproceedings{10.1145/2499393.2499395,
author = {Herbold, Steffen},
title = {Training data selection for cross-project defect prediction},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499395},
doi = {10.1145/2499393.2499395},
abstract = {Software defect prediction has been a popular research topic in recent years and is considered as a means for the optimization of quality assurance activities. Defect prediction can be done in a within-project or a cross-project scenario. The within-project scenario produces results with a very high quality, but requires historic data of the project, which is often not available. For the cross-project prediction, the data availability is not an issue as data from other projects is readily available, e.g., in repositories like PROMISE. However, the quality of the defect prediction results is too low for practical use. Recent research showed that the selection of appropriate training data can improve the quality of cross-project defect predictions. In this paper, we propose distance-based strategies for the selection of training data based on distributional characteristics of the available data. We evaluate the proposed strategies in a large case study with 44 data sets obtained from 14 open source projects. Our results show that our training data selection strategy improves the achieved success rate of cross-project defect predictions significantly. However, the quality of the results still cannot compete with within-project defect prediction.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {6},
numpages = {10},
keywords = {cross-project prediction, defect-prediction, machine learning},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@article{10.4018/IJSI.2021070105,
author = {Jo, Jun-Hyuk and Lee, Jihyun and Jaffari, Aman and Kim, Eunmi},
title = {Fault Localization With Data Flow Information and an Artificial Neural Network},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {3},
issn = {2166-7160},
url = {https://doi.org/10.4018/IJSI.2021070105},
doi = {10.4018/IJSI.2021070105},
abstract = {Fault localization is a technique for identifying the exact source code line with faults. It typically requires a lot of time and cost because, to locate the fault, a developer must track the execution of the failed program line by line. To reduce the fault localization efforts, many methods have been proposed. However, their localized suspicious code range is wide, and their fault localization effect is not high. To cope with this limitation, this paper computes the degree of fault suspiciousness of statements by using an artificial neural network and information of the executed test case, such as statement coverage, execution result, and definition-use pair. Compared to the approach that uses only statement coverage as input data for training an artificial neural network, the experiment results show higher accuracy in 15 types of faults out of 29 real fault types in the approach that the definition-use pair included.},
journal = {Int. J. Softw. Innov.},
month = jul,
pages = {66–78},
numpages = {13},
keywords = {Artificial Neural Network, Data Flow Coverage, Definition-Use, Du-Pair, Fault Localization, Fault Suspiciousness, Software Testing, Software Verification}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, G\"{o}rkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review}
}

@article{10.1016/j.infsof.2013.05.002,
author = {Rodriguez, Daniel and Ruiz, Roberto and Riquelme, Jose C. and Harrison, Rachel},
title = {A study of subgroup discovery approaches for defect prediction},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.002},
doi = {10.1016/j.infsof.2013.05.002},
abstract = {Context: Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective: In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method: We describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results: The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions: The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1810–1822},
numpages = {13},
keywords = {Defect prediction, Imbalanced datasets, Rules, Subgroup discovery}
}

@article{10.5555/3057337.3057441,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {A decision tree logic based recommendation system to select software fault prediction techniques},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {99},
number = {3},
issn = {0010-485X},
abstract = {Identifying a reliable fault prediction technique is the key requirement for building effective fault prediction model. It has been found that the performance of fault prediction techniques is highly dependent on the characteristics of the fault dataset. To mitigate this issue, researchers have evaluated and compared a plethora of fault prediction techniques by varying the context in terms of domain information, characteristics of input data, complexity, etc. However, the lack of an accepted benchmark makes it difficult to select fault prediction technique for a particular context of prediction. In this paper, we present a recommendation system that facilitates the selection of appropriate technique(s) to build fault prediction model. First, we have reviewed the literature to elicit the various characteristics of the fault dataset and the appropriateness of the machine learning and statistical techniques for the identified characteristics. Subsequently, we have formalized our findings and built a recommendation system that helps in the selection of fault prediction techniques. We performed an initial appraisal of our presented system and found that proposed recommendation system provides useful hints in the selection of the fault prediction techniques.},
journal = {Computing},
month = mar,
pages = {255–285},
numpages = {31},
keywords = {68N30 Mathematical aspects of software engineering (specification, Decision tree, Recommendation system, Software fault prediction, Software fault prediction techniques, etc.), metrics, requirements, verification}
}

@inproceedings{10.1145/3383219.3383243,
author = {Pham, Van and Lokan, Chris and Kasmarik, Kathryn},
title = {A Better Set of Object-Oriented Design Metrics for Within-Project Defect Prediction},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383243},
doi = {10.1145/3383219.3383243},
abstract = {Background: Using design metrics to predict fault-prone elements of a software design can help to focus attention on classes that need redesign and more extensive testing. However, some design metrics have been pointed out to be theoretically invalid, and the usefulness of some metrics is questioned.Aim: To identify a set of object-oriented metrics that are theoretically valid, and useful for identifying fault-prone classes in a design.Method: Drawing on four well-known sets of design metrics (CK, LK, MOOD and QMOOD), we propose a consolidated set of metrics that covers many aspects of object-oriented software design. We conduct two experiments, first using a single large system and then considering successive releases of that system, to compare the usefulness of the consolidated set with the other four sets for within-project prediction of fault-prone classes.Results: Both experiments suggest the consolidated set is effective at identifying fault-prone classes, outperforming the other metric sets (though at a cost of more false alarms).Conclusion: This paper adds to knowledge about the usefulness of existing sets of design metrics for within-project defect prediction, and identifies a consolidated set of metrics that is more effective than the existing sets at identifying fault-prone classes.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {230–239},
numpages = {10},
keywords = {Object-oriented software design, design metrics, fault-proneness prediction},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.5555/3507788.3507798,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Garcon, Sylvain and Tauseef, Qasim},
title = {Failure prediction using machine learning in IBM WebSphere liberty continuous integration environment},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The growing complexity and dependencies of software have increased the importance of testing to ensure that frequent changes do not adversely affect existing functionality. Moreover, continuous integration comes with unique challenges associated with maintaining a stable build environment. Several studies have shown that the testing environment becomes more efficient with proper test case prioritization techniques. However, an application's dynamic behavior makes it challenging to derive test case prioritization techniques for achieving optimal results. With the advance of machine learning, the context of an application execution can be analyzed to select and prioritize test suites more efficiently.Test suite prioritization techniques aim to reorder test suites' execution to deliver high quality, maintainable software at lower costs to meet specific objectives such as revealing failures earlier. The state-of-the-art techniques on test prioritization in a continuous integration environment focus on relatively small, single-language, unit-tested projects. This paper compares and analyzes Machine learning-based test suite prioritization technique on two large-scale dataset collected from a continuous integration environment Google and IBM respectively. We optimize hyperparameters and report on experiments' findings by using different machine learning algorithms for test suite prioritization. Our optimized algorithms prioritize test suites with 93% accuracy on average and require 20% fewer test suites to detect 80% of the failures than the test suites prioritized randomly.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {CI, continuous integration, machine learning, test prioritization},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1016/j.cose.2021.102459,
author = {Zhao, Jinxiong and Guo, Sensen and Mu, Dejun},
title = {DouBiGRU-A: Software defect detection algorithm based on attention mechanism and double BiGRU},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {111},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102459},
doi = {10.1016/j.cose.2021.102459},
journal = {Comput. Secur.},
month = dec,
numpages = {10},
keywords = {DouBiGRU-A, Software defect detection, Vulnerability identification, Flawfinder, RATS}
}

@inproceedings{10.1145/3377816.3381734,
author = {Byun, Taejoon and Rayadurgam, Sanjai},
title = {Manifold for machine learning assurance},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381734},
doi = {10.1145/3377816.3381734},
abstract = {The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure---a manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-time monitoring provides an independent means to assess trustability of the target system's output.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {97–100},
numpages = {4},
keywords = {machine learning testing, neural networks, variational autoencoder},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@inproceedings{10.1109/MOBILESoft.2017.14,
author = {Rahman, Akond and Pradhan, Priysha and Partho, Asif and Williams, Laurie},
title = {Predicting Android application security and privacy risk with static code metrics},
year = {2017},
isbn = {9781538626696},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MOBILESoft.2017.14},
doi = {10.1109/MOBILESoft.2017.14},
abstract = {Android applications pose security and privacy risks for end-users. These risks are often quantified by performing dynamic analysis and permission analysis of the Android applications after release. Prediction of security and privacy risks associated with Android applications at early stages of application development, e.g. when the developer (s) are writing the code of the application, might help Android application developers in releasing applications to end-users that have less security and privacy risk. The goal of this paper is to aid Android application developers in assessing the security and privacy risk associated with Android applications by using static code metrics as predictors. In our paper, we consider security and privacy risk of Android application as how susceptible the application is to leaking private information of end-users and to releasing vulnerabilities. We investigate how effectively static code metrics that are extracted from the source code of Android applications, can be used to predict security and privacy risk of Android applications. We collected 21 static code metrics of 1,407 Android applications, and use the collected static code metrics to predict security and privacy risk of the applications. As the oracle of security and privacy risk, we used Androrisk, a tool that quantifies the amount of security and privacy risk of an Android application using analysis of Android permissions and dynamic analysis. To accomplish our goal, we used statistical learners such as, radial-based support vector machine (r-SVM). For r-SVM, we observe a precision of 0.83. Findings from our paper suggest that with proper selection of static code metrics, r-SVM can be used effectively to predict security and privacy risk of Android applications.},
booktitle = {Proceedings of the 4th International Conference on Mobile Software Engineering and Systems},
pages = {149–153},
numpages = {5},
keywords = {Android application, code metrics, prediction, security and privacy risk},
location = {Buenos Aires, Argentina},
series = {MOBILESoft '17}
}

@inproceedings{10.5555/1671248.1671311,
author = {Tosun, Ayse and Bener, Ayse},
title = {Reducing false alarms in software defect prediction by decision threshold optimization},
year = {2009},
isbn = {9781424448425},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software defect data has an imbalanced and highly skewed class distribution. The misclassification costs of two classes are not equal nor are known. It is critical to find the optimum bound, i.e. threshold, which would best separate defective and defect-free classes in software data. We have applied decision threshold optimization on Na\"{\i}ve Bayes classifier in order to find the optimum threshold for software defect data. ROC analyses show that decision threshold optimization significantly decreases false alarms (on the average by 11%) without changing probability of detection rates.},
booktitle = {Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
pages = {477–480},
numpages = {4},
series = {ESEM '09}
}

@inproceedings{10.1145/1868328.1868350,
author = {Zhang, Hongyu and Nelson, Adam and Menzies, Tim},
title = {On the value of learning from defect dense components for software defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868350},
doi = {10.1145/1868328.1868350},
abstract = {BACKGROUND: Defect predictors learned from static code measures can isolate code modules with a higher than usual probability of defects.AIMS: To improve those learners by focusing on the defect-rich portions of the training sets.METHOD: Defect data CM1, KC1, MC1, PC1, PC3 was separated into components. A subset of the projects (selected at random) were set aside for testing. Training sets were generated for a NaiveBayes classifier in two ways. In sample the dense treatment, the components with higher than the median number of defective modules were used for training. In the standard treatment, modules from any component were used for training. Both samples were run against the test set and evaluated using recall, probability of false alarm, and precision. In addition, under sampling and over sampling was performed on the defect data. Each method was repeated in a 10-by-10 cross-validation experiment.RESULTS: Prediction models learned from defect dense components out-performed standard method, under sampling, as well as over sampling. In statistical rankings based on recall, probability of false alarm, and precision, models learned from dense components won 4--5 times more often than any other method, and also lost the least amount of times.CONCLUSIONS: Given training data where most of the defects exist in small numbers of components, better defect predictors can be trained from the defect dense components.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {14},
numpages = {9},
keywords = {ceiling effect, defect dense components, defect prediction, sampling},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1109/ASE.2009.76,
author = {Shivaji, Shivkumar and Jr., E. James Whitehead and Akella, Ram and Kim, Sunghun},
title = {Reducing Features to Improve Bug Prediction},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.76},
doi = {10.1109/ASE.2009.76},
abstract = {Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {600–604},
numpages = {5},
keywords = {Reliability, Machine Learning, Feature Selection, Bug prediction},
series = {ASE '09}
}

@inproceedings{10.1145/3368089.3418538,
author = {\v{C}egi\v{n}, J\'{a}n},
title = {Machine learning based test data generation for safety-critical software},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418538},
doi = {10.1145/3368089.3418538},
abstract = {Unit testing focused on Modified Condition/Decision Coverage (MC/DC) criterion is essential in development safety-critical systems. However, design of test data that meets the MC/DC criterion currently needs detailed manual analysis of branching conditions in units under test by test engineers. Multiple state-of-art approaches exist with proven usage even in industrial projects. However, these approaches have multiple shortcomings, one of them being the Path explosion problem which has not been fully solved yet. Machine learning methods as meta-heuristic approximations can model behaviour of programs that are hard to test using traditional approaches, where the Path explosion problem does occur and thus could solve the limitations of the current state-of-art approaches. I believe, motivated by an ongoing collaboration with an industrial partner, that the machine learning methods could be combined with existing approaches to produce an approach suitable for testing of safety-critical projects.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1678–1681},
numpages = {4},
keywords = {unit testing, test data generation, machine learning, MC/DC criterion},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.asoc.2016.08.025,
author = {Erturk, Ezgi and Akcapinar Sezer, Ebru},
title = {Iterative software fault prediction with a hybrid approach},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.025},
doi = {10.1016/j.asoc.2016.08.025},
abstract = {Display Omitted To make software fault prediction (SFP) more beneficial, it should be into service at the beginning of the project.A novel prediction methodology based on existing methods (i.e. FIS, ANN) are proposed here.Version based development of software projects are considered to design an iterative prediction approach.Proposed methodology is developed as Eclipse plugin.Experiments show that proposed methodology gives promising results to use SFP in daily routine of software development phases. In this study, we consider a software fault prediction task that can assist a developer during the lifetime of a project. We aim to improve the performance of software fault prediction task while keeping it as applicable. Initial predictions are constructed by Fuzzy Inference Systems (FISs), whereas subsequent predictions are performed by data-driven methods. In this paper, an Artificial Neural Network and Adaptive Neuro Fuzzy Inference System are employed. We propose an iterative prediction model that begins with a FIS when no data are available for the software project and continues with a data-driven method when adequate data become available. To prove the usability of this iterative prediction approach, software fault prediction experiments are performed using expert knowledge for the initial version and information about previous versions for subsequent versions. The datasets employed in this paper comprise different versions of Ant, jEdit, Camel, Xalan, Log4j and Lucene projects from the PROMISE repository. The metrics of the models are common object-oriented metrics, such as coupling between objects, weighted methods per class and response for a class. The results of the models are evaluated according to the receiver operating characteristics with the area under the curve approach. The results indicate that the iterative software fault prediction is successful and can be transformed into a tool that can automatically locate fault-prone modules due to its well-organized information flow. We also implement the proposed methodology as a plugin for the Eclipse environment.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1020–1033},
numpages = {14},
keywords = {Adaptive neuro fuzzy inference system, Artificial neural network, Fuzzy inference systems, Iterative prediction, Software fault prediction}
}

@inproceedings{10.5555/3507788.3507810,
author = {Korlepara, Piyush and Grigoriou, Marios and Kontogiannis, Kostas and Brealey, Chris and Giammaria, Alberto},
title = {Combining domain expert knowledge and machine learning for the identification of error prone files},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Identifying as early as possible fault prone modules in order to facilitate continuous delivery in large software systems, has been an area where significant attention has been paid over the past few years. Recent efforts consider source code metrics and process metrics for training machine learning models to predict whether a software source code file is fault prone or not. In such prediction frameworks the accuracy of the trained model relies heavily on the features selected and the profiles of the metrics used for training the model which are unique to each system. Furthermore, these models act as black-boxes, where the end-user does not know how a specific prediction was reached. In this paper, we propose an approach which allows for domain expert knowledge to be combined with machine learning in order to yield fault-proneness prediction models that both exhibit high levels of recall and at the same time are able to provide explanations to the developers as to how and why these predictions were reached. For this paper we apply two rule-based inferencing techniques namely, Fuzzy reasoning, and Markov Logic Networks. The main contribution of this work is that it allows for expert developers to identify in the form of if-then rules domain logic that pertains to the fault-proneness of a source code file in the specific system being analysed. Results obtained from 19 open source systens indicate that MLNs perform better than Fuzzy Logic models and that project-customized rules achieve better results than generic rules. Furthermore, results indicate that its possible to compile a common set of rules that yields consistently acceptable results across different projects.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {153–162},
numpages = {10},
keywords = {software repositories, process metrics, fault-proneness prediction, continuous software engineering},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1016/j.jss.2019.110486,
author = {Barbez, Antoine and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {A machine-learning based ensemble method for anti-patterns detection},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110486},
doi = {10.1016/j.jss.2019.110486},
journal = {J. Syst. Softw.},
month = mar,
numpages = {11},
keywords = {Software quality, Anti-patterns, Machine learning, Ensemble methods}
}

@article{10.1007/s42979-021-00872-6,
author = {Sakhrawi, Zaineb and Sellami, Asma and Bouassida, Nadia},
title = {Software Enhancement Effort Prediction Using Machine-Learning Techniques: A Systematic Mapping Study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {6},
url = {https://doi.org/10.1007/s42979-021-00872-6},
doi = {10.1007/s42979-021-00872-6},
abstract = {Accurate prediction of software enhancement effort is a key success in software project management. To increase the accuracy of estimates, several proposals used machine-learning (ML) techniques for predicting the software project effort. However, there is no clear evidence for determining which techniques to select for predicting more accurate effort within the context of enhancement projects. This paper aims to present a systematic mapping study (SMS) related to the use of ML techniques for predicting software enhancement effort (SEME). A SMS was performed by reviewing relevant papers from 1995 through 2020. We followed well-known guidelines. We selected 30 relevant studies; 19 from journals and 11 conferences proceedings through 4 search engines. Some of the key findings indicate that (1) there is relatively little activity in the area of SEME, (2) most of the successful studies cited focused on regression problems for enhancement maintenance effort prediction, (3) SEME is the dependent variable the most commonly used in software enhancement project planning, and the enhancement size (or the functional change size) is the most used independent variables, (4) several private datasets were used in the selected studies, and there is a growing demand for the use of commonly published datasets, and (5) only single models were employed for SEME prediction. Results indicate that much more work is needed to develop repositories in all prediction models. Based on the findings obtained in this SMS, estimators should be aware that SEME using ML techniques as part of non-algorithmic models demonstrated increased accuracy prediction over the algorithmic models. The use of ML techniques generally provides a reasonable accuracy when using the enhancement functional size as independent variables.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {15},
keywords = {Systematic mapping study (SMS), Functional change (FC), Software enhancement effort (SEME) prediction, Machine learning (ML)}
}

@inproceedings{10.1109/ICSE43902.2021.00138,
author = {Wang, Song and Shrestha, Nishtha and Subburaman, Abarna Kucheri and Wang, Junjie and Wei, Moshi and Nagappan, Nachiappan},
title = {Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00138},
doi = {10.1109/ICSE43902.2021.00138},
abstract = {Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically-orientated and have fundamentally different nature and construction from general software projects.In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely-used machine learning libraries with two popular unit test case generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1548–1560},
numpages = {13},
keywords = {testing machine learning libraries, test case generation, Empirical software engineering},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1007/978-3-030-79463-7_36,
author = {Kawalerowicz, Marcin and Madeyski, Lech},
title = {Jaskier: A Supporting Software Tool for&nbsp;Continuous Build Outcome Prediction Practice},
year = {2021},
isbn = {978-3-030-79462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79463-7_36},
doi = {10.1007/978-3-030-79463-7_36},
abstract = {Continuous Defect Prediction (CDP) is an assisting software development practice that combines Software Defect Prediction (SDP) with machine learning aided modelling and continuous developer feedback. Jaskier is a set of software tools developed under the supervision and with the participation of the authors of the article that implements a lightweight version of CDP called Continuous Build Outcome Prediction (CBOP). CBOP uses classification to label the possible build results based on historical data and metrics derived from the software repository. This paper contains a detailed description of the tool that was already started to be used in the production environment of a real software project where the CBOP practice is being evaluated.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part II},
pages = {426–438},
numpages = {13},
keywords = {Software defect prediction, Continuous integration},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1504/ijict.2019.103203,
author = {Singh, Jagannath and Mohapatra, Durga Prashad},
title = {Source code-based context-sensitive dynamic slicing of web applications},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {4},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2019.103203},
doi = {10.1504/ijict.2019.103203},
abstract = {Web applications are broadly utilised for spreading business around the globe. To meet the necessities of the huge numbers of users or customers, the web applications must have better quality and robustness than any other applications where the number of users is limited. Program slicing is found to be useful in improving program understanding, analysis, testing and maintenance. This paper presents a context-sensitive slicing technique for web applications. In this paper, a new intermediate representation called web dependence graph (WDG) is proposed for representing all dependencies that may present in a web application. We have proposed a context-sensitive web slicing (CSWS) algorithm for computation of slices of a given web application using the WDG. A tool is developed for automatic generation of the WDG for a given web application and computation of slices. During our literature survey, we noticed that majority of the automatic graph generation tools are mainly based on byte-code whereas our tool uses the dependency analysis from the source code of the given program. Using our tool WDG, we compared the performance of our proposed CSWS algorithm for slicing with other closely related slicing techniques.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {391–418},
numpages = {27},
keywords = {program slicing, JSP application, source code analysis, context sensitive, dynamic slicing}
}

@article{10.3233/JIFS-181998,
author = {Jahan, Hosney and Feng, Ziliang and Mahmud, S.M. Hasan and Dong, Penglin},
title = {Version specific test case prioritization approach based on artificial neural network},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {36},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-181998},
doi = {10.3233/JIFS-181998},
abstract = {Regression testing involves validating a software system after modification to ensure that the previous bugs have been fixed and no new error has been raised. Finding faults early and increasing the fault detection rate are the main objectives of regression testing. A common technique involves re-executing the whole test suite, which is time consuming. Test case prioritization aims to schedule the test cases in an order that could achieve the regression testing goals early in the testing phase. Recently, machine learning techniques have been extensively used in regression testing to make it more effective and efficient. In this paper, we propose and investigate whether an Artificial Neural Network (ANN)-based approach can improve the version specific test case prioritization approach. The proposed approach utilizes the combination of test cases complexity information and software modification information with an ANN, for early detection of critical faults. Three new factors have been proposed, based on which an ANN is trained and finally it can automatically assign priorities to new test cases. The proposed approach is empirically evaluated with two software applications. Effectiveness metrics, such as fault detection rate, accuracy, precision, and recall are examined. The results suggest that the proposed approach is both effective and feasible.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6181–6194},
numpages = {14},
keywords = {Regression testing, test case prioritization, artificial neural network, fault detection capability}
}

@article{10.1145/3183339,
author = {Zhou, Yuming and Yang, Yibiao and Lu, Hongmin and Chen, Lin and Li, Yanhui and Zhao, Yangyang and Qian, Junyan and Xu, Baowen},
title = {How Far We Have Progressed in the Journey? An Examination of Cross-Project Defect Prediction},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3183339},
doi = {10.1145/3183339},
abstract = {Background. Recent years have seen an increasing interest in cross-project defect prediction (CPDP), which aims to apply defect prediction models built on source projects to a target project. Currently, a variety of (complex) CPDP models have been proposed with a promising prediction performance.Problem. Most, if not all, of the existing CPDP models are not compared against those simple module size models that are easy to implement and have shown a good performance in defect prediction in the literature.Objective. We aim to investigate how far we have really progressed in the journey by comparing the performance in defect prediction between the existing CPDP models and simple module size models.Method. We first use module size in the target project to build two simple defect prediction models, ManualDown and ManualUp, which do not require any training data from source projects. ManualDown considers a larger module as more defect-prone, while ManualUp considers a smaller module as more defect-prone. Then, we take the following measures to ensure a fair comparison on the performance in defect prediction between the existing CPDP models and the simple module size models: using the same publicly available data sets, using the same performance indicators, and using the prediction performance reported in the original cross-project defect prediction studies.Result. The simple module size models have a prediction performance comparable or even superior to most of the existing CPDP models in the literature, including many newly proposed models.Conclusion. The results caution us that, if the prediction performance is the goal, the real progress in CPDP is not being achieved as it might have been envisaged. We hence recommend that future studies should include ManualDown/ManualUp as the baseline models for comparison when developing new CPDP models to predict defects in a complete target project.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {1},
numpages = {51},
keywords = {unsupervised, supervised, model, cross-project, Defect prediction}
}

@inproceedings{10.1145/3434581.3434619,
author = {Bao, Yang and Rui, Guosheng and Zhang, Song},
title = {A Unsupervised Learning System of Aeroengine Predictive Maintenance Based on Cluster Analysis},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434619},
doi = {10.1145/3434581.3434619},
abstract = {In this paper, a new cluster analysis system of predictive maintenance is proposed. The aim is to perform predictive maintenance on aero-engines under unsupervised conditions and reduce the cost of traditional periodic maintenance. Using this system and the proposed maintenance strategy to verify the subset from C-MAPSS dataset, the results show that the system obtains 40% extra uptime than regular maintenance. Under the theoretical limit, up to 60% of extra uptime can be obtained. The results show that the system can effectively increase uptime and reduce costs, which is a good supplement to the existing predictive maintenance.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {187–191},
numpages = {5},
keywords = {warning system, unsupervised learning, cluster analysis, Predictive maintenance},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/3368089.3409723,
author = {She, Dongdong and Krishna, Rahul and Yan, Lu and Jana, Suman and Ray, Baishakhi},
title = {MTFuzz: fuzzing with a multi-task neural network},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409723},
doi = {10.1145/3368089.3409723},
abstract = {Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model.In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e.,predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2\texttimes{} more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {737–749},
numpages = {13},
keywords = {Multi-task learning, Machine learning, Fuzzing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2884781.2884839,
author = {Zhang, Feng and Zheng, Quan and Zou, Ying and Hassan, Ahmed E.},
title = {Cross-project defect prediction using a connectivity-based unsupervised classifier},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884839},
doi = {10.1145/2884781.2884839},
abstract = {Defect prediction on projects with limited historical data has attracted great interest from both researchers and practitioners. Cross-project defect prediction has been the main area of progress by reusing classifiers from other projects. However, existing approaches require some degree of homogeneity (e.g., a similar distribution of metric values) between the training projects and the target project. Satisfying the homogeneity requirement often requires significant effort (currently a very active area of research).An unsupervised classifier does not require any training data, therefore the heterogeneity challenge is no longer an issue. In this paper, we examine two types of unsupervised classifiers: a) distance-based classifiers (e.g., k-means); and b) connectivity-based classifiers. While distance-based unsupervised classifiers have been previously used in the defect prediction literature with disappointing performance, connectivity-based classifiers have never been explored before in our community.We compare the performance of unsupervised classifiers versus supervised classifiers using data from 26 projects from three publicly available datasets (i.e., AEEEM, NASA, and PROMISE). In the cross-project setting, our proposed connectivity-based classifier (via spectral clustering) ranks as one of the top classifiers among five widely-used supervised classifiers (i.e., random forest, naive Bayes, logistic regression, decision tree, and logistic model tree) and five unsupervised classifiers (i.e., k-means, partition around medoids, fuzzy C-means, neural-gas, and spectral clustering). In the within-project setting (i.e., models are built and applied on the same project), our spectral classifier ranks in the second tier, while only random forest ranks in the first tier. Hence, connectivity-based unsupervised classifiers offer a viable solution for cross and within project defect predictions.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {309–320},
numpages = {12},
keywords = {unsupervised, spectral clustering, heterogeneity, graph mining, defect prediction, cross-project},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.asoc.2019.02.008,
author = {Juneja, Kapil},
title = {A fuzzy-filtered neuro-fuzzy framework for software fault prediction for inter-version and inter-project evaluation},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {77},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.02.008},
doi = {10.1016/j.asoc.2019.02.008},
journal = {Appl. Soft Comput.},
month = apr,
pages = {696–713},
numpages = {18},
keywords = {Defect Prediction, Inter project, Intra project, Classification, Fuzzy}
}

@article{10.1016/j.future.2019.09.009,
author = {Lopes, F\'{a}bio and Agnelo, Jo\~{a}o and Teixeira, C\'{e}sar A. and Laranjeiro, Nuno and Bernardino, Jorge},
title = {Automating orthogonal defect classification using machine learning algorithms},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.09.009},
doi = {10.1016/j.future.2019.09.009},
journal = {Future Gener. Comput. Syst.},
month = jan,
pages = {932–947},
numpages = {16},
keywords = {Text classification, Machine learning, Orthogonal defect classification, Bug reports, Software defects}
}

@article{10.1016/j.asoc.2016.04.009,
author = {Ryu, Duksan and Baik, Jongmoon},
title = {Effective multi-objective nave Bayes learning for cross-project defect prediction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.04.009},
doi = {10.1016/j.asoc.2016.04.009},
abstract = {Display Omitted We propose novel multi-objective learning techniques considering the class imbalance context for cross-project defect prediction.The proposed approaches (i.e., MONB and MONBNN) show the better diversity compared to existing multi-objective prediction models.The proposed approaches show the similar prediction performance compared to within-project defect prediction models. Software defect prediction predicts fault-prone modules which will be tested thoroughly. Thereby, limited quality control resources can be allocated effectively on them. Without sufficient local data, defects can be predicted via cross-project defect prediction (CPDP) utilizing data from other projects to build a classifier. Software defect datasets have the class imbalance problem, indicating the defect class has much fewer instances than the non-defect class does. Unless defect instances are predicted correctly, software quality could be degraded. In this context, a classifier requires to provide high accuracy of the defect class without severely worsening the accuracy of the non-defect class. This class imbalance principle seamlessly connects to the purpose of the multi-objective (MO) optimization in that MO predictive models aim at balancing many of the competing objectives. In this paper, we target to identify effective multi-objective learning techniques under cross-project (CP) environments. Three objectives are devised considering the class imbalance context. The first objective is to maximize the probability of detection (PD). The second objective is to minimize the probability of false alarm (PF). The third objective is to maximize the overall performance (e.g., Balance). We propose novel MO naive Bayes learning techniques modeled by a Harmony Search meta-heuristic algorithm. Our approaches are compared with single-objective models, other existing MO models and within-project defect prediction models. The experimental results show that the proposed approaches are promising. As a result, they can be effectively applied to satisfy various prediction needs under CP settings.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1062–1077},
numpages = {16},
keywords = {Class imbalance, Cross-project defect prediction, Harmony Search, Multi-objective optimization, Search-based software engineering}
}

@inproceedings{10.1145/2851613.2851788,
author = {das D\^{o}res, Silvia N. and Alves, Luciano and Ruiz, Duncan D. and Barros, Rodrigo C.},
title = {A meta-learning framework for algorithm recommendation in software fault prediction},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851788},
doi = {10.1145/2851613.2851788},
abstract = {Software fault prediction is a significant part of software quality assurance and it is commonly used to detect faulty software modules based on software measurement data. Several machine learning based approaches have been proposed for generating predictive models from collected data, although none has become standard given the specificities of each software project. Hence, we believe that recommending the best algorithm for each project is much more important and useful than developing a single algorithm for being used in any project. For achieving that goal, we propose in this paper a novel framework for recommending machine learning algorithms that is capable of automatically identifying the most suitable algorithm according to the software project that is being considered. Our solution, namely SFP-MLF, makes use of the meta-learning paradigm in order to learn the best learner for a particular project. Results show that the SFP-MLF framework provides both the best single algorithm recommendation and also the best ranking recommendation for the software fault prediction problem.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1486–1491},
numpages = {6},
keywords = {algorithm recommendation, machine learning, meta-learning, software fault prediction, software quality},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3377811.3380384,
author = {Sotiropoulos, Thodoris and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {Practical fault detection in puppet programs},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380384},
doi = {10.1145/3377811.3380384},
abstract = {Puppet is a popular computer system configuration management tool. By providing abstractions that model system resources it allows administrators to set up computer systems in a reliable, predictable, and documented fashion. Its use suffers from two potential pitfalls. First, if ordering constraints are not correctly specified whenever a Puppet resource depends on another, the non-deterministic application of resources can lead to race conditions and consequent failures. Second, if a service is not tied to its resources (through the notification construct), the system may operate in a stale state whenever a resource gets modified. Such faults can degrade a computing infrastructure's availability and functionality.We have developed an approach that identifies these issues through the analysis of a Puppet program and its system call trace. Specifically, a formal model for traces allows us to capture the interactions of Puppet resources with the file system. By analyzing these interactions we identify (1) resources that are related to each other (e.g., operate on the same file), and (2) resources that should act as notifiers so that changes are correctly propagated. We then check the relationships from the trace's analysis against the program's dependency graph: a representation containing all the ordering constraints and notifications declared in the program. If a mismatch is detected, our system reports a potential fault.We have evaluated our method on a large set of popular Puppet modules, and discovered 92 previously unknown issues in 33 modules. Performance benchmarking shows that our approach can analyze in seconds real-world configurations with a magnitude measured in thousands of lines and millions of system calls.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {26–37},
numpages = {12},
keywords = {notifiers, ordering relationships, program analysis, puppet, system calls},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.csi.2017.02.003,
title = {An empirical analysis of the effectiveness of software metrics and fault prediction model for identifying faulty classes},
year = {2017},
issue_date = {August 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2017.02.003},
doi = {10.1016/j.csi.2017.02.003},
abstract = {Software fault prediction models are used to predict faulty modules at the very early stage of software development life cycle. Predicting fault proneness using source code metrics is an area that has attracted several researchers' attention. The performance of a model to assess fault proneness depends on the source code metrics which are considered as the input for the model. In this work, we have proposed a framework to validate the source code metrics and identify a suitable set of source code metrics with the aim to reduce irrelevant features and improve the performance of the fault prediction model. Initially, we applied a t-test analysis and univariate logistic regression analysis to each source code metric to evaluate their potential for predicting fault proneness. Next, we performed a correlation analysis and multivariate linear regression stepwise forward selection to find the right set of source code metrics for fault prediction. The obtained set of source code metrics are considered as the input to develop a fault prediction model using a neural network with five different training algorithms and three different ensemble methods. The effectiveness of the developed fault prediction models are evaluated using a proposed cost evaluation framework. We performed experiments on fifty six Open Source Java projects. The experimental results reveal that the model developed by considering the selected set of source code metrics using the suggested source code metrics validation framework as the input achieves better results compared to all other metrics. The experimental results also demonstrate that the fault prediction model is best suitable for projects with faulty classes less than the threshold value depending on fault identification efficiency (low 48.89%, median- 39.26%, and high 27.86%). HighlightsFault prediction improve the effectiveness of software quality assurance activities.This paper focus on building an effective fault prediction tool.Fault prediction model using ANN and ensemble methods.We perform experiments on 56 Open Source Java projects.Fault prediction model is best suitable for projects with faulty classes less than the threshold value.},
journal = {Comput. Stand. Interfaces},
month = aug,
pages = {1–32},
numpages = {32}
}

@article{10.1007/s11219-014-9241-7,
author = {Madeyski, Lech and Jureczko, Marian},
title = {Which process metrics can significantly improve defect prediction models? An empirical study},
year = {2015},
issue_date = {September 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9241-7},
doi = {10.1007/s11219-014-9241-7},
abstract = {The knowledge about the software metrics which serve as defect indicators is vital for the efficient allocation of resources for quality assurance. It is the process metrics, although sometimes difficult to collect, which have recently become popular with regard to defect prediction. However, in order to identify rightly the process metrics which are actually worth collecting, we need the evidence validating their ability to improve the product metric-based defect prediction models. This paper presents an empirical evaluation in which several process metrics were investigated in order to identify the ones which significantly improve the defect prediction models based on product metrics. Data from a wide range of software projects (both, industrial and open source) were collected. The predictions of the models that use only product metrics (simple models) were compared with the predictions of the models which used product metrics, as well as one of the process metrics under scrutiny (advanced models). To decide whether the improvements were significant or not, statistical tests were performed and effect sizes were calculated. The advanced defect prediction models trained on a data set containing product metrics and additionally Number of Distinct Committers (NDC) were significantly better than the simple models without NDC, while the effect size was medium and the probability of superiority (PS) of the advanced models over simple ones was high (  $$p=.016$$ p = . 016 ,  $$r=-.29$$ r = - . 29 ,  $$hbox {PS}=.76$$ PS = . 76 ), which is a substantial finding useful in defect prediction. A similar result with slightly smaller PS was achieved by the advanced models trained on a data set containing product metrics and additionally all of the investigated process metrics (  $$p=.038$$ p = . 038 ,  $$r=-.29$$ r = - . 29 ,  $$hbox {PS}=.68$$ PS = . 68 ). The advanced models trained on a data set containing product metrics and additionally Number of Modified Lines (NML) were significantly better than the simple models without NML, but the effect size was small (  $$p=.038$$ p = . 038 ,  $$r=.06$$ r = . 06 ). Hence, it is reasonable to recommend the NDC process metric in building the defect prediction models.},
journal = {Software Quality Journal},
month = sep,
pages = {393–422},
numpages = {30},
keywords = {Defect prediction models, Process metrics, Product metrics, Software defect prediction, Software metrics}
}

@inproceedings{10.1145/1370750.1370759,
author = {Ratzinger, Jacek and Sigmund, Thomas and Gall, Harald C.},
title = {On the relation of refactorings and software defect prediction},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370759},
doi = {10.1145/1370750.1370759},
abstract = {This paper analyzes the influence of evolution activities such as refactoring on software defects. In a case study of five open source projects we used attributes of software evolution to predict defects in time periods of six months. We use versioning and issue tracking systems to extract 110 data mining features, which are separated into refactoring and non-refactoring related features. These features are used as input into classification algorithms that create prediction models for software defects. We found out that refactoring related features as well as non-refactoring related features lead to high quality prediction models. Additionally, we discovered that refactorings and defects have an inverse correlation: The number of software defects decreases, if the number of refactorings increased in the preceding time period. As a result, refactoring should be a significant part of both bug fixes and other evolutionary changes to reduce software defects.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {35–38},
numpages = {4},
keywords = {software evolution, software analysis, mining},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.1145/2597073.2597078,
author = {Zhang, Feng and Mockus, Audris and Keivanloo, Iman and Zou, Ying},
title = {Towards building a universal defect prediction model},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597078},
doi = {10.1145/2597073.2597078},
abstract = {To predict files with defects, a suitable prediction model must be built for a software project from either itself (within-project) or other projects (cross-project). A universal defect prediction model that is built from the entire set of diverse projects would relieve the need for building models for an individual project. A universal model could also be interpreted as a basic relationship between software metrics and defects. However, the variations in the distribution of predictors pose a formidable obstacle to build a universal model. Such variations exist among projects with different context factors (e.g., size and programming language). To overcome this challenge, we propose context-aware rank transformations for predictors. We cluster projects based on the similarity of the distribution of 26 predictors, and derive the rank transformations using quantiles of predictors for a cluster. We then fit the universal model on the transformed data of 1,398 open source projects hosted on SourceForge and GoogleCode. Adding context factors to the universal model improves the predictive power. The universal model obtains prediction performance comparable to the within-project models and yields similar results when applied on five external projects (one Apache and four Eclipse projects). These results suggest that a universal defect prediction model may be an achievable goal.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {182–191},
numpages = {10},
keywords = {rank transformation, quality, large scale, defect prediction, defect, context factors, bug, Universal defect prediction model},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1109/ESEM.2017.50,
author = {Bennin, Kwabena Ebo and Keung, Jacky and Monden, Akito and Phannachitta, Passakorn and Mensah, Solomon},
title = {The significant effects of data sampling approaches on software defect prioritization and classification},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.50},
doi = {10.1109/ESEM.2017.50},
abstract = {Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to unbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {364–373},
numpages = {10},
keywords = {statistical significance, sampling methods, imbalanced data, empirical software engineering, defect prediction},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/2908812.2908938,
author = {Panichella, Annibale and Alexandru, Carol V. and Panichella, Sebastiano and Bacchelli, Alberto and Gall, Harald C.},
title = {A Search-based Training Algorithm for Cost-aware Defect Prediction},
year = {2016},
isbn = {9781450342063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908812.2908938},
doi = {10.1145/2908812.2908938},
abstract = {Research has yielded approaches to predict future defects in software artifacts based on historical information, thus assisting companies in effectively allocating limited development resources and developers in reviewing each others' code changes. Developers are unlikely to devote the same effort to inspect each software artifact predicted to contain defects, since the effort varies with the artifacts' size (cost) and the number of defects it exhibits (effectiveness). We propose to use Genetic Algorithms (GAs) for training prediction models to maximize their cost-effectiveness. We evaluate the approach on two well-known models, Regression Tree and Generalized Linear Model, and predict defects between multiple releases of six open source projects. Our results show that regression models trained by GAs significantly outperform their traditional counterparts, improving the cost-effectiveness by up to 240%. Often the top 10% of predicted lines of code contain up to twice as many defects.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
pages = {1077–1084},
numpages = {8},
keywords = {machine learning, genetic algorithm, defect prediction},
location = {Denver, Colorado, USA},
series = {GECCO '16}
}

@article{10.1016/j.eswa.2008.12.028,
author = {Turhan, Burak and Kocak, Gozde and Bener, Ayse},
title = {Data mining source code for locating software bugs},
year = {2009},
issue_date = {August, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {6},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.12.028},
doi = {10.1016/j.eswa.2008.12.028},
abstract = {In a large software system knowing which files are most likely to be fault-prone is valuable information for project managers. They can use such information in prioritizing software testing and allocating resources accordingly. However, our experience shows that it is difficult to collect and analyze fine-grained test defects in a large and complex software system. On the other hand, previous research has shown that companies can safely use cross-company data with nearest neighbor sampling to predict their defects in case they are unable to collect local data. In this study we analyzed 25 projects of a large telecommunication system. To predict defect proneness of modules we trained models on publicly available Nasa MDP data. In our experiments we used static call graph based ranking (CGBR) as well as nearest neighbor sampling for constructing method level defect predictors. Our results suggest that, for the analyzed projects, at least 70% of the defects can be detected by inspecting only (i) 6% of the code using a Na\"{\i}ve Bayes model, (ii) 3% of the code using CGBR framework.},
journal = {Expert Syst. Appl.},
month = aug,
pages = {9986–9990},
numpages = {5},
keywords = {Case study, Defect prediction, Software bugs, Software testing}
}

@article{10.1007/s11704-020-9441-1,
author = {Sun, Xiaobing and Zhou, Tianchi and Wang, Rongcun and Duan, Yucong and Bo, Lili and Chang, Jianming},
title = {Experience report: investigating bug fixes in machine learning frameworks/libraries},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {6},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-020-9441-1},
doi = {10.1007/s11704-020-9441-1},
abstract = {Machine learning (ML) techniques and algorithms have been successfully and widely used in various areas including software engineering tasks. Like other software projects, bugs are also common in ML projects and libraries. In order to more deeply understand the features related to bug fixing in ML projects, we conduct an empirical study with 939 bugs from five ML projects by manually examining the bug categories, fixing patterns, fixing scale, fixing duration, and types of maintenance. The results show that (1) there are commonly seven types of bugs in ML programs; (2) twelve fixing patterns are typically used to fix the bugs in ML programs; (3) 68.80% of the patches belong to micro-scale-fix and small-scale-fix; (4) 66.77% of the bugs in ML programs can be fixed within one month; (5) 45.90% of the bug fixes belong to corrective activity from the perspective of software maintenance. Moreover, we perform a questionnaire survey and send them to developers or users of ML projects to validate the results in our empirical study. The results of our empirical study are basically consistent with the feedback from developers. The findings from the empirical study provide useful guidance and insights for developers and users to effectively detect and fix bugs in ML projects.},
journal = {Front. Comput. Sci.},
month = dec,
numpages = {16},
keywords = {bug fixing, machine learning project, empirical study, questionnaire survey}
}

@article{10.1007/s10664-015-9400-x,
author = {Kamei, Yasutaka and Fukushima, Takafumi and Mcintosh, Shane and Yamashita, Kazuhiro and Ubayashi, Naoyasu and Hassan, Ahmed E.},
title = {Studying just-in-time defect prediction using cross-project models},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9400-x},
doi = {10.1007/s10664-015-9400-x},
abstract = {Unlike traditional defect prediction models that identify defect-prone modules, Just-In-Time (JIT) defect prediction models identify defect-inducing changes. As such, JIT defect models can provide earlier feedback for developers, while design decisions are still fresh in their minds. Unfortunately, similar to traditional defect models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this limitation in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from other projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT models in a cross-project context. Through an empirical study on 11 open source projects, we find that while JIT models rarely perform well in a cross-project context, their performance tends to improve when using approaches that: (1) select models trained using other projects that are similar to the testing project, (2) combine the data of several other projects to produce a larger pool of training data, and (3) combine the models of several other projects to produce an ensemble model. Our findings empirically confirm that JIT models learned using other projects are a viable solution for projects with limited historical data. However, JIT models tend to perform best in a cross-project context when the data used to learn them are carefully selected.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2072–2106},
numpages = {35},
keywords = {Defect prediction, Empirical study, Just-in-time prediction}
}

@inproceedings{10.1145/2025113.2025156,
author = {Lee, Taek and Nam, Jaechang and Han, DongGyun and Kim, Sunghun and In, Hoh Peter},
title = {Micro interaction metrics for defect prediction},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025156},
doi = {10.1145/2025113.2025156},
abstract = {There is a common belief that developers' behavioral interaction patterns may affect software quality. However, widely used defect prediction metrics such as source code metrics, change churns, and the number of previous defects do not capture developers' direct interactions. We propose 56 novel micro interaction metrics (MIMs) that leverage developers' interaction information stored in the Mylyn data. Mylyn is an Eclipse plug-in, which captures developers' interactions such as file editing and selection events with time spent. To evaluate the performance of MIMs in defect prediction, we build defect prediction (classification and regression) models using MIMs, traditional metrics, and their combinations. Our experimental results show that MIMs significantly improve defect classification and regression accuracy.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {311–321},
numpages = {11},
keywords = {mylyn, micro interaction metrics, defect prediction},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@article{10.1007/s10515-011-0091-2,
author = {Liparas, Dimitris and Angelis, Lefteris and Feldt, Robert},
title = {Applying the Mahalanobis-Taguchi strategy for software defect diagnosis},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0091-2},
doi = {10.1007/s10515-011-0091-2},
abstract = {The Mahalanobis-Taguchi (MT) strategy combines mathematical and statistical concepts like Mahalanobis distance, Gram-Schmidt orthogonalization and experimental designs to support diagnosis and decision-making based on multivariate data. The primary purpose is to develop a scale to measure the degree of abnormality of cases, compared to "normal" or "healthy" cases, i.e. a continuous scale from a set of binary classified cases. An optimal subset of variables for measuring abnormality is then selected and rules for future diagnosis are defined based on them and the measurement scale. This maps well to problems in software defect prediction based on a multivariate set of software metrics and attributes. In this paper, the MT strategy combined with a cluster analysis technique for determining the most appropriate training set, is described and applied to well-known datasets in order to evaluate the fault-proneness of software modules. The measurement scale resulting from the MT strategy is evaluated using ROC curves and shows that it is a promising technique for software defect diagnosis. It compares favorably to previously evaluated methods on a number of publically available data sets. The special characteristic of the MT strategy that it quantifies the level of abnormality can also stimulate and inform discussions with engineers and managers in different defect prediction situations.},
journal = {Automated Software Engg.},
month = jun,
pages = {141–165},
numpages = {25},
keywords = {Fault-proneness, Mahalanobis-Taguchi strategy, Software defect prediction, Software testing}
}

@article{10.1007/s10664-020-09881-0,
author = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea and Humbatova, Nargiz and Weiss, Michael and Tonella, Paolo},
title = {Testing machine learning based systems: a systematic mapping},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09881-0},
doi = {10.1007/s10664-020-09881-0},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5193–5254},
numpages = {62},
keywords = {Systematic mapping, Systematic review, Software testing, Machine learning}
}

@article{10.1016/j.jss.2016.05.015,
author = {Chen, Tse-Hsun and Shang, Weiyi and Nagappan, Meiyappan and Hassan, Ahmed E. and Thomas, Stephen W.},
title = {Topic-based software defect explanation},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.05.015},
doi = {10.1016/j.jss.2016.05.015},
abstract = {Some topics are more defect-prone than others.Defect-prone topics are likely to remain so over time.Our topic-based metrics provide additional defect explanatory to baseline metrics.Our metrics outperform state-of-the-art topic-based cohesion and coupling metrics. Researchers continue to propose metrics using measurable aspects of software systems to understand software quality. However, these metrics largely ignore the functionality, i.e., the conceptual concerns, of software systems. Such concerns are the technical concepts that reflect the systems business logic. For instance, while lines of code may be a good general measure for defects, a large file responsible for simple I/O tasks is likely to have fewer defects than a small file responsible for complicated compiler implementation details. In this paper, we study the effect of concerns on software quality. We use a statistical topic modeling approach to approximate software concerns as topics (related words in source code). We propose various metrics using these topics to help explain the file defect-proneness. Case studies on multiple versions of Firefox, Eclipse, Mylyn, and NetBeans show that (i) some topics are more defect-prone than others; (ii) defect-prone topics tend to remain so over time; (iii) our topic-based metrics provide additional explanatory power for software quality over existing structural and historical metrics; and (iv) our topic-based cohesion metric outperforms state-of-the-art topic-based cohesion and coupling metrics in terms of defect explanatory power, while being simpler to implement and more intuitive to interpret.},
journal = {J. Syst. Softw.},
month = jul,
pages = {79–106},
numpages = {28},
keywords = {Code quality, Cohesion, Coupling, LDA, Metrics, Topic modeling}
}

@inproceedings{10.1145/2786805.2804429,
author = {Kim, Mijung and Nam, Jaechang and Yeon, Jaehyuk and Choi, Soonhwang and Kim, Sunghun},
title = {REMI: defect prediction for efficient API testing},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2804429},
doi = {10.1145/2786805.2804429},
abstract = {Quality assurance for common APIs is important since the the reliability of APIs affects the quality of other systems using the APIs. Testing is a common practice to ensure the quality of APIs, but it is a challenging and laborious task especially for industrial projects. Due to a large number of APIs with tight time constraints and limited resources, it is hard to write enough test cases for all APIs. To address these challenges, we present a novel technique, REMI that predicts high risk APIs in terms of producing potential bugs. REMI allows developers to write more test cases for the high risk APIs. We evaluate REMI on a real-world industrial project, Tizen-wearable, and apply REMI to the API development process at Samsung Electronics. Our evaluation results show that REMI predicts the bug-prone APIs with reasonable accuracy (0.681 f-measure on average). The results also show that applying REMI to the Tizen-wearable development process increases the number of bugs detected, and reduces the resources required for executing test cases.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {990–993},
numpages = {4},
keywords = {Quality Assurance, Defect Prediction, API Testing},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@phdthesis{10.5555/AAI28256855,
author = {Wu, Baijun and Arun, Lakhotia, and Anthony, Maida, and Miao, Jin,},
advisor = {Sheng, Chen,},
title = {Using Machine Learning to Improve Programming Error Reporting},
year = {2020},
isbn = {9798519181839},
publisher = {University of Louisiana at Lafayette},
abstract = {The main purpose of this research is to explore applying machine learning to improve programming error reporting. In the first part of this dissertation, I present the empirical study about how type error were fixed and what students did. The investigation results demonstrate that current error debugging support is far from sufficient in practice, where the located error causes for more than 50% of type errors are incorrect and the corresponding change suggestions are ineffective. I provide a fundamental understanding of why existing error debuggers do not work well for nonstructural type errors. To address this issue, a machine learning-base type error debugger, Learnskell, is developed. The evaluations results show that Learnskell could locate the error causes for nonstructural type errors several times more accurate than the state-of-the-art tools. In the second part, I study how to precisely infer error specifications in C. Error specifications, which specify the value range that each function returns to indicate failures, are widely used to check and propagate errors for the sake of reliability and security. I propose a general method, MLPEx, that can automatically generate error specifications by analyzing only the source code. MLPEx utilizes the idea of transfer learning, and therefore requires zero manual efforts to label data during the learning process. Error specifications are useful to detect bugs. As one example, I present how the results of MLPEx can be used to find new error handling bugs in real-world projects.},
note = {AAI28256855}
}

@article{10.1007/s10664-015-9376-6,
author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
title = {The impact of tangled code changes on defect prediction models},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9376-6},
doi = {10.1007/s10664-015-9376-6},
abstract = {When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets--in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {303–336},
numpages = {34},
keywords = {Untangling, Defect prediction, Data noise}
}

@inproceedings{10.1109/ASE.2019.00164,
author = {Zhang, Kevin},
title = {A machine learning based approach to identify SQL injection vulnerabilities},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00164},
doi = {10.1109/ASE.2019.00164},
abstract = {This paper presents a machine learning classifier designed to identify SQL injection vulnerabilities in PHP code. Both classical and deep learning based machine learning algorithms were used to train and evaluate classifier models using input validation and sanitization features extracted from source code files. On ten-fold cross validations a model trained using Convolutional Neural Network(CNN) achieved the highest precision (95.4%), while a model based on Multilayer Perceptron (MLP) achieved the highest recall (63.7%) and the highest fmeasure (0.746).},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1286–1288},
numpages = {3},
keywords = {vulnerability, prediction model, deep learning, SQL injection},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1609/aaai.v33i01.33014910,
author = {Shi, Shu-Ting and Li, Ming and Lo, David and Thung, Ferdian and Huo, Xuan},
title = {Automatic code review by learning the revision of source code},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014910},
doi = {10.1609/aaai.v33i01.33014910},
abstract = {Code review is the process of manual inspection on the revision of the source code in order to find out whether the revised source code eventually meets the revision requirements. However, manual code review is time-consuming, and automating such the code review process will alleviate the burden of code reviewers and speed up the software maintenance process. To construct the model for automatic code review, the characteristics of the revisions of source code (i.e., the difference between the two pieces of source code) should be properly captured and modeled. Unfortunately, most of the existing techniques can easily model the overall correlation between two pieces of source code, but not for the "difference" between two pieces of source code. In this paper, we propose a novel deep model named DACE for automatic code review. Such a model is able to learn revision features by contrasting the revised hunks from the original and revised source code with respect to the code context containing the hunks. Experimental results on six open source software projects indicate by learning the revision features, DACE can outperform the competing approaches in automatic code review.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {603},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.5555/2819009.2819026,
author = {Tan, Ming and Tan, Lin and Dara, Sashank and Mayeux, Caleb},
title = {Online defect prediction for imbalanced data},
year = {2015},
publisher = {IEEE Press},
abstract = {Many defect prediction techniques are proposed to improve software reliability. Change classification predicts defects at the change level, where a change is the modifications to one file in a commit. In this paper, we conduct the first study of applying change classification in practice.We identify two issues in the prediction process, both of which contribute to the low prediction performance. First, the data are imbalanced---there are much fewer buggy changes than clean changes. Second, the commonly used cross-validation approach is inappropriate for evaluating the performance of change classification. To address these challenges, we apply and adapt online change classification, resampling, and updatable classification techniques to improve the classification performance.We perform the improved change classification techniques on one proprietary and six open source projects. Our results show that these techniques improve the precision of change classification by 12.2-89.5% or 6.4--34.8 percentage points (pp.) on the seven projects. In addition, we integrate change classification in the development process of the proprietary project. We have learned the following lessons: 1) new solutions are needed to convince developers to use and believe prediction results, and prediction results need to be actionable, 2) new and improved classification algorithms are needed to explain the prediction results, and insensible and unactionable explanations need to be filtered or refined, and 3) new techniques are needed to improve the relatively low precision.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {99–108},
numpages = {10},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1007/s10664-020-09874-z,
author = {Moslehi, Parisa and Adams, Bram and Rilling, Juergen},
title = {A feature location approach for mapping application features extracted from crowd-based screencasts to source code},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09874-z},
doi = {10.1007/s10664-020-09874-z},
abstract = {Crowd-based multimedia documents such as screencasts have emerged as a source for documenting requirements, the workflow and implementation issues of open source and agile software projects. For example, users can show and narrate how they manipulate an application’s GUI to perform a certain functionality, or a bug reporter could visually explain how to trigger a bug or a security vulnerability. Unfortunately, the streaming nature of programming screencasts and their binary format limit how developers can interact with a screencast’s content. In this research, we present an automated approach for mining and linking the multimedia content found in screencasts to their relevant software artifacts and, more specifically, to source code. We apply LDA-based mining approaches that take as input a set of screencast artifacts, such as GUI text and spoken word, to make the screencast content accessible and searchable to users and to link it to their relevant source code artifacts. To evaluate the applicability of our approach, we report on results from case studies that we conducted on existing WordPress and Mozilla Firefox screencasts. We found that our automated approach can significantly speed up the feature location process. For WordPress, we find that our approach using screencast speech and GUI text can successfully link relevant source code files within the top 10 hits of the result set with median Reciprocal Rank (RR) of 50% (rank 2) and 100% (rank 1). In the case of Firefox, our approach can identify relevant source code directories within the top 100 hits using screencast speech and GUI text with the median RR = 20%, meaning that the first true positive is ranked 5 or higher in more than 50% of the cases. Also, source code related to the frontend implementation that handles high-level or GUI-related aspects of an application is located with higher accuracy. We also found that term frequency rebalancing can further improve the linking results when using less noisy scenarios or locating less technical implementation of scenarios. Investigating the results of using original and weighted screencast data sources (speech, GUI, speech and GUI) that can result in having the highest median RR values in both case studies shows that speech data is an important information source that can result in having RR of 100%.},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {4873–4926},
numpages = {54},
keywords = {Crowd-based documentation, Mining video content, Speech analysis, Feature location, Software traceability, Information extraction, Software documentation}
}

@inproceedings{10.1145/3338906.3338961,
author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Ji, Chao and Liu, Dewei and Xiang, Qilin and He, Chuan},
title = {Latent error prediction and fault localization for microservice applications by learning from system trace logs},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338961},
doi = {10.1145/3338906.3338961},
abstract = {In the production environment, a large part of microservice failures are related to the complex and dynamic interactions and runtime environments, such as those related to multiple instances, environmental configurations, and asynchronous interactions of microservices. Due to the complexity and dynamism of these failures, it is often hard to reproduce and diagnose them in testing environments. It is desirable yet still challenging that these failures can be detected and the faults can be located at runtime of the production environment to allow developers to resolve them efficiently. To address this challenge, in this paper, we propose MEPFL, an approach of latent error prediction and fault localization for microservice applications by learning from system trace logs. Based on a set of features defined on the system trace logs, MEPFL trains prediction models at both the trace level and the microservice level using the system trace logs collected from automatic executions of the target application and its faulty versions produced by fault injection. The prediction models thus can be used in the production environment to predict latent errors, faulty microservices, and fault types for trace instances captured at runtime. We implement MEPFL based on the infrastructure systems of container orchestrator and service mesh, and conduct a series of experimental studies with two opensource microservice applications (one of them being the largest open-source microservice application to our best knowledge). The results indicate that MEPFL can achieve high accuracy in intraapplication prediction of latent errors, faulty microservices, and fault types, and outperforms a state-of-the-art approach of failure diagnosis for distributed systems. The results also show that MEPFL can effectively predict latent errors caused by real-world fault cases.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {683–694},
numpages = {12},
keywords = {tracing, microservices, machine learning, fault localization, error prediction, debugging},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1007/s10664-015-9396-2,
author = {Zhang, Feng and Mockus, Audris and Keivanloo, Iman and Zou, Ying},
title = {Towards building a universal defect prediction model with rank transformed predictors},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9396-2},
doi = {10.1007/s10664-015-9396-2},
abstract = {Software defects can lead to undesired results. Correcting defects costs 50 % to 75 % of the total software development budgets. To predict defective files, a prediction model must be built with predictors (e.g., software metrics) obtained from either a project itself (within-project) or from other projects (cross-project). A universal defect prediction model that is built from a large set of diverse projects would relieve the need to build and tailor prediction models for an individual project. A formidable obstacle to build a universal model is the variations in the distribution of predictors among projects of diverse contexts (e.g., size and programming language). Hence, we propose to cluster projects based on the similarity of the distribution of predictors, and derive the rank transformations using quantiles of predictors for a cluster. We fit the universal model on the transformed data of 1,385 open source projects hosted on SourceForge and GoogleCode. The universal model obtains prediction performance comparable to the within-project models, yields similar results when applied on five external projects (one Apache and four Eclipse projects), and performs similarly among projects with different context factors. At last, we investigate what predictors should be included in the universal model. We expect that this work could form a basis for future work on building a universal model and would lead to software support tools that incorporate it into a regular development workflow.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2107–2145},
numpages = {39},
keywords = {Context factors, Defect prediction, Large-scale, Rank transformation, Software quality, Universal defect prediction model}
}

@article{10.1016/j.scico.2021.102713,
author = {Jain, Shivani and Saha, Anju},
title = {Improving performance with hybrid feature selection and ensemble machine learning techniques for code smell detection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102713},
doi = {10.1016/j.scico.2021.102713},
journal = {Sci. Comput. Program.},
month = dec,
numpages = {34},
keywords = {Code smell, Machine learning, Ensemble machine learning, Hybrid feature selection, Stacking}
}

@inproceedings{10.1145/1985793.1985950,
author = {Nguyen, Tung Thanh and Nguyen, Tien N. and Phuong, Tu Minh},
title = {Topic-based defect prediction (NIER track)},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985950},
doi = {10.1145/1985793.1985950},
abstract = {Defects are unavoidable in software development and fixing them is costly and resource-intensive. To build defect prediction models, researchers have investigated a number of factors related to the defect-proneness of source code, such as code complexity, change complexity, or socio-technical factors. In this paper, we propose a new approach that emphasizes on technical concerns/functionality of a system. In our approach, a software system is viewed as a collection of software artifacts that describe different technical concerns/-aspects. Those concerns are assumed to have different levels of defect-proneness, thus, cause different levels of defectproneness to the relevant software artifacts. We use topic modeling to measure the concerns in source code, and use them as the input for machine learning-based defect prediction models. Preliminary result on Eclipse JDT shows that the topic-based metrics have high correlation to the number of bugs (defect-proneness), and our topic-based defect prediction has better predictive performance than existing state-of-the-art approaches.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {932–935},
numpages = {4},
keywords = {topic modeling, defect prediction},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@article{10.1145/3428205,
author = {Wang, Yu and Wang, Ke and Gao, Fengjuan and Wang, Linzhang},
title = {Learning semantic program embeddings with graph interval neural network},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428205},
doi = {10.1145/3428205},
abstract = {Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous (i.e. do not take advantage of the program-specific graph characteristics) and expensive (i.e. require heavy information exchange among nodes in the graph) message-passing procedure, GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN), to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, GINN focuses exclusively on intervals (generally manifested in looping construct) for mining the feature representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning to large graphs.  We evaluate GINN for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based bug detector and Facebook Infer, arguably the state-of-the-art static analysis tool, to scan the codebase of 20 highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer. We have reported 38 bugs GINN caught to developers, among which 11 have been fixed and 12 have been confirmed (fix pending). GINN has shown to be a general, powerful deep neural network for learning precise, semantic program embeddings.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {137},
numpages = {27},
keywords = {Program embeddings, Null pointer dereference detection, Intervals, Graph neural networks, Control-flow graphs}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00022,
author = {Panichella, Annibale and Liem, Cynthia C. S.},
title = {What are we really testing in mutation testing for machine learning? a critical reflection},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00022},
doi = {10.1109/ICSE-NIER52604.2021.00022},
abstract = {Mutation testing is a well-established technique for assessing a test suite's quality by injecting artificial faults into production code. In recent years, mutation testing has been extended to machine learning (ML) systems, and deep learning (DL) in particular; researchers have proposed approaches, tools, and statistically sound heuristics to determine whether mutants in DL systems are killed or not. However, as we will argue in this work, questions can be raised to what extent currently used mutation testing techniques in DL are actually in line with the classical interpretation of mutation testing. We observe that ML model development resembles a test-driven development (TDD) process, in which a training algorithm ('programmer') generates a model (program) that fits the data points (test data) to labels (implicit assertions), up to a certain threshold. However, considering proposed mutation testing techniques for ML systems under this TDD metaphor, in current approaches, the distinction between production and test code is blurry, and the realism of mutation operators can be challenged. We also consider the fundamental hypotheses underlying classical mutation testing: the competent programmer hypothesis and coupling effect hypothesis. As we will illustrate, these hypotheses do not trivially translate to ML system development, and more conscious and explicit scoping and concept mapping will be needed to truly draw parallels. Based on our observations, we propose several action points for better alignment of mutation testing techniques for ML with paradigms and vocabularies of classical mutation testing.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {66–70},
numpages = {5},
keywords = {software testing, mutation testing, mutation operators, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@inproceedings{10.1145/3318299.3318345,
author = {Li, ZhanJun and Shao, Yan},
title = {A Survey of Feature Selection for Vulnerability Prediction Using Feature-based Machine Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318345},
doi = {10.1145/3318299.3318345},
abstract = {This paper summarized the basic process of software vulnerability prediction using feature-based machine learning for the first time. In addition to sorting out the related types and basis of vulnerability features definition, the advantages and disadvantages of different methods are compared. Finally, this paper analyzed the difficulties and challenges in this research field, and put forward some suggestions for future work.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {36–42},
numpages = {7},
keywords = {machine learning, feature, Software vulnerability prediction},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/2972958.2972964,
author = {Hosseini, Seyedrebvar and Turhan, Burak and M\"{a}ntyl\"{a}, Mika},
title = {Search Based Training Data Selection For Cross Project Defect Prediction},
year = {2016},
isbn = {9781450347723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972958.2972964},
doi = {10.1145/2972958.2972964},
abstract = {Context: Previous studies have shown that steered training data or dataset selection can lead to better performance for cross project defect prediction (CPDP). On the other hand, data quality is an issue to consider in CPDP.Aim: We aim at utilising the Nearest Neighbor (NN)-Filter, embedded in a genetic algorithm, for generating evolving training datasets to tackle CPDP, while accounting for potential noise in defect labels.Method: We propose a new search based training data (i.e., instance) selection approach for CPDP called GIS (Genetic Instance Selection) that looks for solutions to optimize a combined measure of F-Measure and GMean, on a validation set generated by (NN)-filter. The genetic operations consider the similarities in features and address possible noise in assigned defect labels. We use 13 datasets from PROMISE repository in order to compare the performance of GIS with benchmark CPDP methods, namely (NN)-filter and naive CPDP, as well as with within project defect prediction (WPDP).Results: Our results show that GIS is significantly better than (NN)-Filter in terms of F-Measure (p -- value ≪ 0.001, Cohen's d = 0.697) and GMean (p -- value ≪ 0.001, Cohen's d = 0.946). It also outperforms the naive CPDP approach in terms of F-Measure (p -- value ≪ 0.001, Cohen's d = 0.753) and GMean (p -- value ≪ 0.001, Cohen's d = 0.994). In addition, the performance of our approach is better than that of WPDP, again considering F-Measure (p -- value ≪ 0.001, Cohen's d = 0.227) and GMean (p -- value ≪ 0.001, Cohen's d = 0.595) values.Conclusions: We conclude that search based instance selection is a promising way to tackle CPDP. Especially, the performance comparison with the within project scenario encourages further investigation of our approach. However, the performance of GIS is based on high recall in the expense of low precision. Using different optimization goals, e.g. targeting high precision, would be a future direction to investigate.},
booktitle = {Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {3},
numpages = {10},
keywords = {Cross Project Defect Prediction, Genetic Algorithms, Instance Selection, Search Based Optimization, Training Data Selection},
location = {Ciudad Real, Spain},
series = {PROMISE 2016}
}

@inproceedings{10.1145/1540438.1540462,
author = {English, Michael and Exton, Chris and Rigon, Irene and Cleary, Brendan},
title = {Fault detection and prediction in an open-source software project},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540462},
doi = {10.1145/1540438.1540462},
abstract = {Software maintenance continues to be a time and resource intensive activity. Any efforts that help to address the maintenance bottleneck within the software lifecycle are welcome. One area where such efforts are useful is in the identification of the parts of the source-code of a software system that are most likely to contain faults and thus require changes. We have carried out an empirical study where we have merged information from the CVS repository and the Bugzilla database for an open-source software project to investigate whether or not parts of the source-code are faulty, the number and severity of faults and the number and types of changes associated with parts of the system. We present an analysis of this information, showing that Pareto's Law holds and we evaluate the usefulness of the Chidamber and Kemerer metrics for identifying the fault-prone classes in the system analysed.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {17},
numpages = {11},
keywords = {empirical study, fault prediction, metrics, open source, regression, software quality},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@article{10.1007/s10664-020-09808-9,
author = {Agrawal, Amritanshu and Menzies, Tim and Minku, Leandro L. and Wagner, Markus and Yu, Zhe},
title = {Better software analytics via “DUO”: Data mining algorithms using/used-by optimizers},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09808-9},
doi = {10.1007/s10664-020-09808-9},
abstract = {This paper claims that a new field of empirical software engineering research and practice is emerging: data mining using/used-by optimizers for empirical studies, or DUO. For example, data miners can generate models that are explored by optimizers. Also, optimizers can advise how to best adjust the control parameters of a data miner. This combined approach acts like an agent leaning over the shoulder of an analyst that advises “ask this question next” or “ignore that problem, it is not relevant to your goals”. Further, those agents can help us build “better” predictive models, where “better” can be either greater predictive accuracy or faster modeling time (which, in turn, enables the exploration of a wider range of options). We also caution that the era of papers that just use data miners is coming to an end. Results obtained from an unoptimized data miner can be quickly refuted, just by applying an optimizer to produce a different (and better performing) model. Our conclusion, hence, is that for software analytics it is possible, useful and necessary to combine data mining and optimization using DUO.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {2099–2136},
numpages = {38},
keywords = {Evolutionary algorithms, Optimization, Data mining, Software analytics}
}

@article{10.1016/j.infsof.2013.02.009,
author = {Radjenovi\'{c}, Danijel and Heri\v{c}ko, Marjan and Torkar, Richard and \v{Z}ivkovi\v{c}, Ale\v{s}},
title = {Software fault prediction metrics},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.009},
doi = {10.1016/j.infsof.2013.02.009},
abstract = {ContextSoftware metrics may be used in fault prediction models to improve software quality by predicting fault location. ObjectiveThis paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics' selection and performance. MethodThis systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties. ResultsObject-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer's (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics. ConclusionMore studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1397–1418},
numpages = {22},
keywords = {Systematic literature review, Software metric, Software fault prediction}
}

@inproceedings{10.1145/3324884.3416621,
author = {Shen, Weijun and Li, Yanhui and Chen, Lin and Han, Yuanlei and Zhou, Yuming and Xu, Baowen},
title = {Multiple-boundary clustering and prioritization to promote neural network retraining},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416621},
doi = {10.1145/3324884.3416621},
abstract = {With the increasing application of deep learning (DL) models in many safety-critical scenarios, effective and efficient DL testing techniques are much in demand to improve the quality of DL models. One of the major challenges is the data gap between the training data to construct the models and the testing data to evaluate them. To bridge the gap, testers aim to collect an effective subset of inputs from the testing contexts, with limited labeling effort, for retraining DL models.To assist the subset selection, we propose Multiple-Boundary Clustering and Prioritization (MCP), a technique to cluster test samples into the boundary areas of multiple boundaries for DL models and specify the priority to select samples evenly from all boundary areas, to make sure enough useful samples for each boundary reconstruction. To evaluate MCP, we conduct an extensive empirical study with three popular DL models and 33 simulated testing contexts. The experiment results show that, compared with state-of-the-art baseline methods, on effectiveness, our approach MCP has a significantly better performance by evaluating the improved quality of retrained DL models; on efficiency, MCP also has the advantages in time costs.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {410–422},
numpages = {13},
keywords = {deep learning, multiple-boundary, neural network, retraining, software testing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3196398.3196431,
author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
title = {Deep learning similarities from different representations of source code},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196431},
doi = {10.1145/3196398.3196431},
abstract = {Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from different representations.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {542–553},
numpages = {12},
keywords = {code similarities, deep learning, neural networks},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.4018/IJITSA.2021010104,
author = {Shatnawi, Raed and Mishra, Alok},
title = {An Empirical Study on Software Fault Prediction Using Product and Process Metrics},
year = {2021},
issue_date = {Jan 2021},
publisher = {IGI Global},
address = {USA},
volume = {14},
number = {1},
issn = {1935-570X},
url = {https://doi.org/10.4018/IJITSA.2021010104},
doi = {10.4018/IJITSA.2021010104},
abstract = {Product and process metrics are measured from the development and evolution of software. Metrics are indicators of software fault-proneness and advanced models using machine learning can be provided to the development team to select modules for further inspection. Most fault-proneness classifiers were built from product metrics. However, the inclusion of process metrics adds evolution as a factor to software quality. In this work, the authors propose a process metric measured from the evolution of software to predict fault-proneness in software models. The process metrics measures change-proneness of modules (classes and interfaces). Classifiers are trained and tested for five large open-source systems. Classifiers were built using product metrics alone and using a combination of product and the proposed process metric. The classifiers evaluation shows improvements whenever the process metrics were used. Evolution metrics are correlated with quality of software and helps in improving software quality prediction for future releases.},
journal = {Int. J. Inf. Technol. Syst. Appoach},
month = jan,
pages = {62–78},
numpages = {17},
keywords = {CK Metrics, Process Metrics, Product Metrics, Software Fault}
}

@article{10.1504/IJKEDM.2013.059319,
author = {Chaturvedi, K. K. and Singh, V. B.},
title = {Bug prediction using entropy-based measures},
year = {2013},
issue_date = {February 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {4},
issn = {1755-2087},
url = {https://doi.org/10.1504/IJKEDM.2013.059319},
doi = {10.1504/IJKEDM.2013.059319},
abstract = {In the available literature, researchers have proposed and implemented a plethora of bug prediction approaches, which vary in terms of accuracy, complexity and the input data they require, but very few of them has predicted the number of bugs in the software based on the entropy or the complexity of code changes. To use the entropy of code change as a bug predictor, firstly, the history of complexity metric HCM defined with different decay weight and decay models were assigned to it Hassan, 2009. But, they did not propose any method to find out the value of decay rate/factor. In this paper, we proposed a new weight to HCM, a method to find out the value of decay rate/factor and proposed some novel decay-based methods. We have applied simple linear regression SLR and support vector regression SVR to predict the bugs based on existing and proposed methods of HCM. We have also studied the performance of different complexity of code changes entropy-based bug prediction approaches on the basis of various performance measures using four subsystems of Mozilla project. We found that decay models for SVR show better results in comparison with SLR.},
journal = {Int. J. Knowl. Eng. Data Min.},
month = feb,
pages = {266–291},
numpages = {26}
}

@article{10.1016/j.eswa.2014.10.025,
author = {Erturk, Ezgi and Sezer, Ebru Akcapinar},
title = {A comparison of some soft computing methods for software fault prediction},
year = {2015},
issue_date = {March 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.025},
doi = {10.1016/j.eswa.2014.10.025},
abstract = {Software fault prediction is implemented with ANN, SVM and ANFIS.First ANFIS implementation is applied to solve fault prediction problem.Parameters are discussed in neuro fuzzy approach.Experiments show that the application of ANFIS to the software fault prediction problem is highly reasonable. The main expectation from reliable software is the minimization of the number of failures that occur when the program runs. Determining whether software modules are prone to fault is important because doing so assists in identifying modules that require refactoring or detailed testing. Software fault prediction is a discipline that predicts the fault proneness of future modules by using essential prediction metrics and historical fault data. This study presents the first application of the Adaptive Neuro Fuzzy Inference System (ANFIS) for the software fault prediction problem. Moreover, Artificial Neural Network (ANN) and Support Vector Machine (SVM) methods, which were experienced previously, are built to discuss the performance of ANFIS. Data used in this study are collected from the PROMISE Software Engineering Repository, and McCabe metrics are selected because they comprehensively address the programming effort. ROC-AUC is used as a performance measure. The results achieved were 0.7795, 0.8685, and 0.8573 for the SVM, ANN and ANFIS methods, respectively.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1872–1879},
numpages = {8},
keywords = {Adaptive neuro fuzzy systems, Artificial Neural Networks, McCabe metrics, Software fault prediction, Support Vector Machines}
}

@article{10.1155/2021/6612342,
author = {Li, Yao and Dourado, Ant\'{o}nio},
title = {A Fault Prediction and Cause Identification Approach in Complex Industrial Processes Based on Deep Learning},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/6612342},
doi = {10.1155/2021/6612342},
abstract = {Faults occurring in the production line can cause many losses. Predicting the fault events before they occur or identifying the causes can effectively reduce such losses. A modern production line can provide enough data to solve the problem. However, in the face of complex industrial processes, this problem will become very difficult depending on traditional methods. In this paper, we propose a new approach based on a deep learning (DL) algorithm to solve the problem. First, we regard these process data as a spatial sequence according to the production process, which is different from traditional time series data. Second, we improve the long short-term memory (LSTM) neural network in an encoder-decoder model to adapt to the branch structure, corresponding to the spatial sequence. Meanwhile, an attention mechanism (AM) algorithm is used in fault detection and cause identification. Third, instead of traditional biclassification, the output is defined as a sequence of fault types. The approach proposed in this article has two advantages. On the one hand, treating data as a spatial sequence rather than a time sequence can overcome multidimensional problems and improve prediction accuracy. On the other hand, in the trained neural network, the weight vectors generated by the AM algorithm can represent the correlation between faults and the input data. This correlation can help engineers identify the cause of faults. The proposed approach is compared with some well-developed fault diagnosing methods in the Tennessee Eastman process. Experimental results show that the approach has higher prediction accuracy, and the weight vector can accurately label the factors that cause faults.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/1774088.1774612,
author = {Sami, Ashkan and Fakhrahmad, Seyed Mostafa},
title = {Design-level metrics estimation based on code metrics},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774612},
doi = {10.1145/1774088.1774612},
abstract = {Fault detection based on mining code and design metrics has been an active research area for many years. Basically "module"-based metrics for source code and design level are calculated or obtained and data mining is used to build predictor models. However, in many projects due to organizational or software process models, design level metrics are not available and/or accurate. It has been shown that performance of these classifiers or predictors decline if only source code features are used for training them. Based on best of our know knowledge no set of rule to estimate design level metrics based on code level metrics has been presented since it is believed that design level metrics have additional information and cannot be estimated without access to design artifacts. In this study we present a fuzzy modeling system to find and present these relationships for projects presented in NASA Metrics Data Repository (MDP) datasets. Interestingly, we could find a set of empirical rules that govern all the projects regardless of size, programming language and software development methodology. Comparison of fault detectors built based on estimated design metrics with actual design metrics on various projects showed a very small difference in accuracy of classifiers and validated our hypothesis that estimation of design metrics based on source code attributes can become a practical exercise.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2531–2535},
numpages = {5},
keywords = {software metrics, software defect prediction, parameter estimation, fuzzy classification, approximate dependencies},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/3338906.3342484,
author = {Moghadam, Mahshid Helali},
title = {Machine learning-assisted performance testing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342484},
doi = {10.1145/3338906.3342484},
abstract = {Automated testing activities like automated test case generation imply a reduction in human effort and cost, with the potential to impact the test coverage positively. If the optimal policy, i.e., the course of actions adopted, for performing the intended test activity could be learnt by the testing system, i.e., a smart tester agent, then the learnt policy could be reused in analogous situations which leads to even more efficiency in terms of required efforts. Performance testing under stress execution conditions, i.e., stress testing, which involves providing extreme test conditions to find the performance breaking points, remains a challenge, particularly for complex software systems. Some common approaches for generating stress test conditions are based on source code or system model analysis, or use-case based design approaches. However, source code or precise system models might not be easily available for testing. Moreover, drawing a precise performance model is often difficult, particularly for complex systems. In this research, I have used model-free reinforcement learning to build a self-adaptive autonomous stress testing framework which is able to learn the optimal policy for stress test case generation without having a model of the system under test. The conducted experimental analysis shows that the proposed smart framework is able to generate the stress test conditions for different software systems efficiently and adaptively without access to performance models.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1187–1189},
numpages = {3},
keywords = {Test case generation, Stress testing, Reinforcement learning, Performance testing, Autonomous testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/1985793.1985859,
author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
title = {Dealing with noise in defect prediction},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985859},
doi = {10.1145/1985793.1985859},
abstract = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises.This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {481–490},
numpages = {10},
keywords = {noise resistance, defect prediction, data quality, buggy files, buggy changes},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/3324884.3416617,
author = {Li, Ke and Xiang, Zilin and Chen, Tao and Tan, Kay Chen},
title = {BiLO-CPDP: bi-level programming for automated model discovery in cross-project defect prediction},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416617},
doi = {10.1145/3324884.3416617},
abstract = {Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {573–584},
numpages = {12},
keywords = {automated parameter optimization, classification techniques, configurable software and tool, cross-project defect prediction, transfer learning},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1109/ICTAI.2010.27,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan and Seliya, Naeem},
title = {Attribute Selection and Imbalanced Data: Problems in Software Defect Prediction},
year = {2010},
isbn = {9780769542638},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2010.27},
doi = {10.1109/ICTAI.2010.27},
abstract = {The data mining and machine learning community is often faced with two key problems: working with imbalanced data and selecting the best features for machine learning. This paper presents a process involving a feature selection technique for selecting the important attributes and a data sampling technique for addressing class imbalance. The application domain of this study is software engineering, more specifically, software quality prediction using classification models. When using feature selection and data sampling together, different scenarios should be considered. The four possible scenarios are: (1) feature selection based on original data, and modeling (defect prediction) based on original data; (2) feature selection based on original data, and modeling based on sampled data; (3) feature selection based on sampled data, and modeling based on original data; and (4) feature selection based on sampled data, and modeling based on sampled data. The research objective is to compare the software defect prediction performances of models based on the four scenarios. The case study consists of nine software measurement data sets obtained from the PROMISE software project repository. Empirical results suggest that feature selection based on sampled data performs significantly better than feature selection based on original data, and that defect prediction models perform similarly regardless of whether the training data was formed using sampled or original data.},
booktitle = {Proceedings of the 2010 22nd IEEE International Conference on Tools with Artificial Intelligence - Volume 01},
pages = {137–144},
numpages = {8},
keywords = {data sampling, defect prediction, feature selection, software measurements},
series = {ICTAI '10}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s10515-021-00285-y,
author = {Goyal, Somya},
title = {Predicting the Defects using Stacked Ensemble Learner with Filtered Dataset},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00285-y},
doi = {10.1007/s10515-021-00285-y},
abstract = {Software defect prediction is a crucial software project management activity to enhance the software quality. It aids the development team to forecast about which modules need extra attention for testing; which part of software is more prone to errors and faults; before the commencement of testing phase. It helps to reduce the testing cost and hence the overall development cost of the software. Though, it ensures in-time delivery of good quality end-product, but there is one major hinderance in making this prediction. This is the class imbalance issue in the training data. Data imbalance in class distribution adversely affects the performance of classifiers. This paper proposes a K-nearest neighbour (KNN) filtering-based data pre-processing technique for stacked ensemble classifier to handle class imbalance issue. First, nearest neighbour-based filtering is applied to filter out the overlapped data-points to reduce Imbalanced Ratio, then, the processed data with static code metrics is supplied to stacked ensemble for prediction. The stacking is achieved with five base classifiers namely Artificial Neural Network, Decision Tree, Na\"{\i}ve Bayes, K-nearest neighbour (KNN) and Support Vector Machine. A comparative analysis among 30 classifiers (5 data pre-processing techniques * 6 prediction techniques) is made. In the experiments, five public datasets from NASA repository namely CM1, JM1, KC1, KC2 and PC1 are used. In total 150 prediction models (5 data pre-processing techniques * 6 classification techniques * 5 datasets) are proposed and their performances are assessed in terms of measures namely Receiver Operator Curve, Area under the Curve and accuracy. The statistical analysis shows that proposed stacked ensemble classifier with KNN filtering performs best among all the predictors independent of datasets.},
journal = {Automated Software Engg.},
month = nov,
numpages = {81},
keywords = {Software quality, Defect prediction, Data pre-processing, Class imbalance, Artificial neural networks (ANN), Stacked ensembles, Decision trees, Nearest neighbour, Support vector machine, ROC and AUC}
}

@article{10.1016/j.procs.2015.02.154,
author = {Mahajan, Rohit and Gupta, Sunil Kumar and Bedi, Rajeev Kumar},
title = {Design of Software Fault Prediction Model Using BR Technique},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.154},
doi = {10.1016/j.procs.2015.02.154},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {849–858},
numpages = {10},
keywords = {Back Propagation (BPA) algorithm ;Bayesian Regularization(BR)algorithml, Levenberg-Marquardt (LM)algorithm, Neural network, public dataset ;}
}

@article{10.1016/j.infsof.2019.106214,
author = {Alsolai, Hadeel and Roper, Marc},
title = {A systematic literature review of machine learning techniques for software maintainability prediction},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106214},
doi = {10.1016/j.infsof.2019.106214},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {25},
keywords = {Systematic literature review, Software maintainability prediction, Machine learning, Metric, Dataset}
}

@article{10.1007/s10664-016-9473-1,
author = {Mahmoud, Anas and Bradshaw, Gary},
title = {Semantic topic models for source code analysis},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9473-1},
doi = {10.1007/s10664-016-9473-1},
abstract = {Topic modeling techniques have been recently applied to analyze and model source code. Such techniques exploit the textual content of source code to provide automated support for several basic software engineering activities. Despite these advances, applications of topic modeling in software engineering are frequently suboptimal. This can be attributed to the fact that current state-of-the-art topic modeling techniques tend to be data intensive. However, the textual content of source code, embedded in its identifiers, comments, and string literals, tends to be sparse in nature. This prevents classical topic modeling techniques, typically used to model natural language texts, to generate proper models when applied to source code. Furthermore, the operational complexity and multi-parameter calibration often associated with conventional topic modeling techniques raise important concerns about their feasibility as data analysis models in software engineering. Motivated by these observations, in this paper we propose a novel approach for topic modeling designed for source code. The proposed approach exploits the basic assumptions of the cluster hypothesis and information theory to discover semantically coherent topics in software systems. Ten software systems from different application domains are used to empirically calibrate and configure the proposed approach. The usefulness of generated topics is empirically validated using human judgment. Furthermore, a case study that demonstrates thet operation of the proposed approach in analyzing code evolution is reported. The results show that our approach produces stable, more interpretable, and more expressive topics than classical topic modeling techniques without the necessity for extensive parameter calibration.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1965–2000},
numpages = {36},
keywords = {Clustering, Information theory, Topic modeling}
}

@article{10.1007/s10836-021-05966-w,
author = {Wang, Qi and Ouyang, Yiming and Lu, Yingchun and Liang, Huaguo and Zhu, Dakai},
title = {Neural Network-based Online Fault Diagnosis in Wireless-NoC Systems},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {37},
number = {4},
issn = {0923-8174},
url = {https://doi.org/10.1007/s10836-021-05966-w},
doi = {10.1007/s10836-021-05966-w},
abstract = {The recent development of wireless Network-on-Chip (WiNoC) by introducing wireless interface in traditional wired NoC has significantly increased the performance of NoC systems with higher bandwidth and low latency on-chip communication. However, the integration of more components (e.g., antenna and transceiver) on the chip also increases system complexity and makes it more susceptible to various failures. In this paper, we propose a run-time fault diagnosis mechanism based on neural network (NN) techniques, where both fully-connected (FC) and convolutional neural networks (CNN) are considered. For NoC with 2-D mesh topology that incorporates both wired and wireless interfaces, the FC and CNN neural networks for fault diagnosis and detection are presented. The NN models can be trained offline with collected traffic data from partially failed NoC with various faulty components. Then, at run-time, the NN models can be deployed on certain tiles in the NoC to detect and locate the faulty components using the run-time traffic data. Based on simulated traffic data, we have evaluated the proposed NN-based mechanism under different fault scenarios (e.g., type, location and number of faulty components). The results show that, CNN models outperform FC neural networks with higher fault diagnosis rates. CNN can successfully identify up to 81.2% faults when there is only one faulty component on the NoC with different traffic patterns. The accuracy decreases when there are more faulty components and higher traffic loads.},
journal = {J. Electron. Test.},
month = aug,
pages = {545–559},
numpages = {15},
keywords = {Artificial neural network, Fault diagnosis, Wireless network on Chip}
}

@article{10.1016/j.infsof.2019.01.008,
author = {Meqdadi, Omar and Alhindawi, Nouh and Alsakran, Jamal and Saifan, Ahmad and Migdadi, Hatim},
title = {Mining software repositories for adaptive change commits using machine learning techniques},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.008},
doi = {10.1016/j.infsof.2019.01.008},
journal = {Inf. Softw. Technol.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Code change metrics, Adaptive maintenance, Commit types, Maintenance classification, Machine learning}
}

@article{10.1007/s11277-017-5224-x,
author = {Wu, Yu-Chen and Feng, Jun-Wen},
title = {Development and Application of Artificial Neural Network},
year = {2018},
issue_date = {Sep 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-017-5224-x},
doi = {10.1007/s11277-017-5224-x},
abstract = {Artificial neural network is a very important part in the new industry of artificial intelligence. In China, there are many researches on artificial neural network and artificial intelligence are developing rapidly. Therefore, this paper reviews and summarizes artificial neural network, and hopes that readers can get a deeper understanding of artificial neural network. This paper first reviews the development history of artificial neural network and its related theory, and introduces four major characteristics of artificial neural network, such as the non-linear, non-limitative, non-qualitative and non-convex. Then it emphatically analyzes its application in information, medicine, economy, control, transportation and psychology. Finally, the future development trend of artificial neural network is prospected and summarized.},
journal = {Wirel. Pers. Commun.},
month = sep,
pages = {1645–1656},
numpages = {12},
keywords = {Application status analysis, Artificial neural network, Development history, Future development trend}
}

@inproceedings{10.5555/3049877.3049895,
author = {Mezouar, Mariam El and Zhang, Feng and Zou, Ying},
title = {Local versus global models for effort-aware defect prediction},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software entities (e.g., files or classes) do not have the same density of defects and therefore do not require the same amount of effort for inspection. With limited resources, it is critical to reveal as many defects as possible. To satisfy such need, effort-aware defect prediction models have been proposed. However, the performance of prediction models is commonly affected by a large amount of possible variability in the training data. Prior studies have inspected whether using a subset of the original training data (i.e., local models) could improve the performance of prediction models in the context of defect prediction and effort estimation in comparison with global models (i.e., trained on the whole dataset). However, no consensus has been reached and the comparison has not been performed in the context of effort-aware defect prediction.In this study, we compare local and global effort-aware defect prediction models using 15 projects from the widely used AEEEM and PROMISE datasets. We observe that although there is at least one local model that can outperform the global model, there always exists another local model that performs very poorly in all the projects. We further find that the poor performing local model is built on the subset of the training set with a low ratio of defective entities. By excluding such subset of the training set and building a local effort-aware model with the remaining training set, the local model usually underperforms the global model in 11 out of the 15 studied projects. A close inspection on the failure of local effort-aware models reveals that the major challenge comes from defective entities with small size (i.e., few lines of code), as such entities tend to be correctly predicted by the global model but missed by the local model. Further work should pay special attention to the small but defective entities.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {178–187},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@article{10.1007/s10664-011-9180-x,
author = {Ekanayake, Jayalath and Tappolet, Jonas and Gall, Harald C. and Bernstein, Abraham},
title = {Time variance and defect prediction in software projects},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9180-x},
doi = {10.1007/s10664-011-9180-x},
abstract = {It is crucial for a software manager to know whether or not one can rely on a bug prediction model. A wrong prediction of the number or the location of future bugs can lead to problems in the achievement of a project's goals. In this paper we first verify the existence of variability in a bug prediction model's accuracy over time both visually and statistically. Furthermore, we explore the reasons for such a high variability over time, which includes periods of stability and variability of prediction quality, and formulate a decision procedure for evaluating prediction models before applying them. To exemplify our findings we use data from four open source projects and empirically identify various project features that influence the defect prediction quality. Specifically, we observed that a change in the number of authors editing a file and the number of defects fixed by them influence the prediction quality. Finally, we introduce an approach to estimate the accuracy of prediction models that helps a project manager decide when to rely on a prediction model. Our findings suggest that one should be aware of the periods of stability and variability of prediction quality and should use approaches such as ours to assess their models' accuracy in advance.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {348–389},
numpages = {42},
keywords = {Concept drift, Decision tree learner, Defect prediction, Mining software repository, Time variance}
}

@inproceedings{10.1145/3395363.3397366,
author = {Dutta, Saikat and Shi, August and Choudhary, Rutvik and Zhang, Zhekun and Jain, Aryaman and Misailovic, Sasa},
title = {Detecting flaky tests in probabilistic and machine learning applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397366},
doi = {10.1145/3395363.3397366},
abstract = {Probabilistic programming systems and machine learning frameworks like Pyro, PyMC3, TensorFlow, and PyTorch provide scalable and efficient primitives for inference and training. However, such operations are non-deterministic. Hence, it is challenging for developers to write tests for applications that depend on such frameworks, often resulting in flaky tests – tests which fail non-deterministically when run on the same version of code.  In this paper, we conduct the first extensive study of flaky tests in this domain. In particular, we study the projects that depend on four frameworks: Pyro, PyMC3, TensorFlow-Probability, and PyTorch. We identify 75 bug reports/commits that deal with flaky tests, and we categorize the common causes and fixes for them. This study provides developers with useful insights on dealing with flaky tests in this domain.  Motivated by our study, we develop a technique, FLASH, to systematically detect flaky tests due to assertions passing and failing in different runs on the same code. These assertions fail due to differences in the sequence of random numbers in different runs of the same test. FLASH exposes such failures, and our evaluation on 20 projects results in 11 previously-unknown flaky tests that we reported to developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {211–224},
numpages = {14},
keywords = {Randomness, Probabilistic Programming, Non-Determinism, Machine Learning, Flaky tests},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1007/978-3-319-65340-2_44,
author = {Cunha, Pedro and Ferreira, Andr\'{e} and Cortez, Paulo},
title = {Mining Rational Team Concert Repositories: A&nbsp;Case Study on a Software Project},
year = {2017},
isbn = {978-3-319-65339-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-65340-2_44},
doi = {10.1007/978-3-319-65340-2_44},
abstract = {Software repositories are key to support the development of software. In this article, we present a Mining Software Repositories (MSR) approach that considered a two-year software project repository, set using the Rational Team Concert (RTC) tool. Such MSR was designed in terms of three main components: RTC data extraction, RTC data mining and design of RTC intelligence dashboard. In particular, we focus more on the data extraction component, although we also present mining and dashboard outcomes. Interesting results were achieved, revealing a potential of the proposed MSR to improve the software project planning/development agility and quality.},
booktitle = {Progress in Artificial Intelligence: 18th EPIA Conference on Artificial Intelligence, EPIA 2017, Porto, Portugal, September 5-8, 2017, Proceedings},
pages = {537–548},
numpages = {12},
keywords = {Software engineering, Data mining, Association rules},
location = {Porto, Portugal}
}

@article{10.1007/s00500-016-2284-x,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {An empirical study of some software fault prediction techniques for the number of faults prediction},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {24},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-016-2284-x},
doi = {10.1007/s00500-016-2284-x},
abstract = {During the software development process, prediction of the number of faults in software modules can be more helpful instead of predicting the modules being faulty or non-faulty. Such an approach may help in more focused software testing process and may enhance the reliability of the software system. Most of the earlier works on software fault prediction have used classification techniques for classifying software modules into faulty or non-faulty categories. The techniques such as Poisson regression, negative binomial regression, genetic programming, decision tree regression, and multilayer perceptron can be used for the prediction of the number of faults. In this paper, we present an experimental study to evaluate and compare the capability of six fault prediction techniques such as genetic programming, multilayer perceptron, linear regression, decision tree regression, zero-inflated Poisson regression, and negative binomial regression for the prediction of number of faults. The experimental investigation is carried out for eighteen software project datasets collected from the PROMISE data repository. The results of the investigation are evaluated using average absolute error, average relative error, measure of completeness, and prediction at level l measures. We also perform Kruskal---Wallis test and Dunn's multiple comparison test to compare the relative performance of the considered fault prediction techniques.},
journal = {Soft Comput.},
month = dec,
pages = {7417–7434},
numpages = {18},
keywords = {Dunn's multiple comparison test, Genetic programming, Kruskal---Wallis test, Multilayer perceptron, Software fault prediction, Zero-inflated Poisson regression}
}

@inproceedings{10.1109/ICSE43902.2021.00059,
author = {Wainakh, Yaza and Rauf, Moiz and Pradel, Michael},
title = {IdBench: Evaluating Semantic Representations of Identifier Names in Source Code},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00059},
doi = {10.1109/ICSE43902.2021.00059},
abstract = {Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of name-based analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., len and size, are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {562–573},
numpages = {12},
keywords = {source code, neural networks, identifiers, embeddings, benchmark},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ICPC.2019.00023,
author = {Pecorelli, Fabiano and Palomba, Fabio and Di Nucci, Dario and De Lucia, Andrea},
title = {Comparing heuristic and machine learning approaches for metric-based code smell detection},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00023},
doi = {10.1109/ICPC.2019.00023},
abstract = {Code smells represent poor implementation choices performed by developers when enhancing source code. Their negative impact on source code maintainability and comprehensibility has been widely shown in the past and several techniques to automatically detect them have been devised. Most of these techniques are based on heuristics, namely they compute a set of code metrics and combine them by creating detection rules; while they have a reasonable accuracy, a recent trend is represented by the use of machine learning where code metrics are used as predictors of the smelliness of code artefacts. Despite the recent advances in the field, there is still a noticeable lack of knowledge of whether machine learning can actually be more accurate than traditional heuristic-based approaches. To fill this gap, in this paper we propose a large-scale study to empirically compare the performance of heuristic-based and machine-learning-based techniques for metric-based code smell detection. We consider five code smell types and compare machine learning models with Decor, a state-of-the-art heuristic-based approach. Key findings emphasize the need of further research aimed at improving the effectiveness of both machine learning and heuristic approaches for code smell detection: while Decor generally achieves better performance than a machine learning baseline, its precision is still too low to make it usable in practice.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {93–104},
numpages = {12},
keywords = {machine learning, heuristics, empirical study, code smells detection},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/2491411.2491418,
author = {Rahman, Foyzur and Posnett, Daryl and Herraiz, Israel and Devanbu, Premkumar},
title = {Sample size vs. bias in defect prediction},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491418},
doi = {10.1145/2491411.2491418},
abstract = {Most empirical disciplines promote the reuse and sharing of datasets, as it leads to greater possibility of replication. While this is increasingly the case in Empirical Software Engineering, some of the most popular bug-fix datasets are now known to be biased. This raises two significant concerns: first, that sample bias may lead to underperforming prediction models, and second, that the external validity of the studies based on biased datasets may be suspect. This issue has raised considerable consternation in the ESE literature in recent years. However, there is a confounding factor of these datasets that has not been examined carefully: size. Biased datasets are sampling only some of the data that could be sampled, and doing so in a biased fashion; but biased samples could be smaller, or larger. Smaller data sets in general provide less reliable bases for estimating models, and thus could lead to inferior model performance. In this setting, we ask the question, what affects performance more, bias, or size? We conduct a detailed, large-scale meta-analysis, using simulated datasets sampled with bias from a high-quality dataset which is relatively free of bias. Our results suggest that size always matters just as much bias direction, and in fact much more than bias direction when considering information-retrieval measures such as AUCROC and F-score. This indicates that at least for prediction models, even when dealing with sampling bias, simply finding larger samples can sometimes be sufficient. Our analysis also exposes the complexity of the bias issue, and raises further issues to be explored in the future.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {147–157},
numpages = {11},
keywords = {size, defect prediction, bias},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1109/TSE.2012.43,
author = {Shivaji, Shivkumar and Whitehead, E. James and Akella, Ram and Kim, Sunghun},
title = {Reducing Features to Improve Code Change-Based Bug Prediction},
year = {2013},
issue_date = {April 2013},
publisher = {IEEE Press},
volume = {39},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2012.43},
doi = {10.1109/TSE.2012.43},
abstract = {Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {552–569},
numpages = {18}
}

@article{10.1016/j.infsof.2019.08.005,
author = {Bigonha, Mariza A.S. and Ferreira, Kecia and Souza, Priscila and Sousa, Bruno and Janu\'{a}rio, Marcela and Lima, Daniele},
title = {The usefulness of software metric thresholds for detection of bad smells and fault prediction},
year = {2019},
issue_date = {Nov 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.08.005},
doi = {10.1016/j.infsof.2019.08.005},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {79–92},
numpages = {14},
keywords = {Software metrics, Software quality, Thresholds, Detection strategies, Bad smell, Fault prediction}
}

@inproceedings{10.1145/3266003.3266004,
author = {de Santiago, Valdivino Alexandre and da Silva, Leoni Augusto Romain and de Andrade Neto, Pedro Ribeiro},
title = {Testing Environmental Models supported by Machine Learning},
year = {2018},
isbn = {9781450365550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266003.3266004},
doi = {10.1145/3266003.3266004},
abstract = {In this paper we present a new methodology, DaOBML, to test environmental models whose outputs are complex artifacts such as images (maps) or plots. Our approach suggests several test data generation techniques (Combinatorial Interaction Testing, Model-Based Testing, Random Testing) and digital image processing methods to drive the creation of Knowledge Bases (KBs). Considering such KBs and Machine Learning (ML) algorithms, a test oracle assigns the verdicts of new test data. Our methodology is supported by a tool and we applied it to models developed via the TerraME product. A controlled experiment was carried out and we conclude that Random Testing is the most feasible test data generation approach for developing the KBs, Artificial Neural Networks present the best performance out of six ML algorithms, and the larger the KB, in terms of size, the better.},
booktitle = {Proceedings of the III Brazilian Symposium on Systematic and Automated Software Testing},
pages = {3–12},
numpages = {10},
keywords = {Random Testing, Model-Based Testing, Machine Learning, Environmental Modeling, Empirical Software Engineering, Digital Image Processing, Combinatorial Interaction Testing},
location = {SAO CARLOS, Brazil},
series = {SAST '18}
}

@inproceedings{10.5555/2487085.2487161,
author = {Peters, Fayola and Menzies, Tim and Marcus, Andrian},
title = {Better cross company defect prediction},
year = {2013},
isbn = {9781467329361},
publisher = {IEEE Press},
abstract = {How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach?  This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects.  To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within- company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within- company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning.},
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
pages = {409–418},
numpages = {10},
location = {San Francisco, CA, USA},
series = {MSR '13}
}

@inproceedings{10.1145/3361242.3361243,
author = {Wang, Yuehuan and Li, Zenan and Xu, Jingwei and Yu, Ping and Ma, Xiaoxing},
title = {Fast Robustness Prediction for Deep Neural Network},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361243},
doi = {10.1145/3361242.3361243},
abstract = {Deep neural networks (DNNs) have achieved impressive performance in many difficult tasks. However, DNN models are essentially uninterpretable to humans, and unfortunately prone to adversarial attacks, which hinders their adoption in security and safety-critical scenarios. The robustness of a DNN model, which measures its stableness against adversarial attacks, becomes an important topic in both the machine learning and the software engineering communities. Analytical evaluation of DNN robustness is difficult due to the high-dimensionality of inputs, the huge amount of parameters, and the nonlinear network structure. In practice, the degree of robustness of DNNs is empirically approximated with adversarial searching, which is computationally expensive and cannot be applied in resource constrained settings such as embedded computing. In this paper, we propose to predict the robustness of a DNN model for each input with another DNN model, which takes the output of neurons of the former model as input. We train a regression model to encode the connections between output of the penultimate layer of a DNN model and its robustness. With this trained model, the robustness for an input can be predicted instantaneously. Experiments with MNIST and CIFAR10 datasets and LeNet, VGG and ResNet DNN models were conducted to evaluate the efficacy of the proposed approach. The results indicated that our approach achieved 0.05-0.21 mean absolute errors and significantly outperformed confidence and surprise adequacy-based approaches.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {11},
numpages = {10},
keywords = {Robustness, Prediction, Deep Neural Networks},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1145/3385032.3385657,
author = {Tiwari, Saurabh and Thakur, Jitendra Singh},
title = {A Report on Students Software Project Contest co-located with ISEC 2020},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385657},
doi = {10.1145/3385032.3385657},
abstract = {Student Software Project Contest (SSPC) at ISEC 2020 provides graduate and postgraduate students with the opportunity to showcase their software and system development skills by submitting their software project summary and video demonstrations aligned to the theme of ISEC 2020. The projects can be web-based, mobile applications, embedded systems, analysis tool and so on. We invited software project submissions from students who followed a systematic software development approach in their software projects. A total of 30 submissions have been received and five of them are selected and invited for the presentation (and demonstration) at the conference.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {25},
numpages = {1},
keywords = {Software Engineering, Software development, software tools and technologies, student software projects},
location = {Jabalpur, India},
series = {ISEC '20}
}

@article{10.1016/j.sysarc.2021.102298,
author = {Fern\'{a}ndez, Javier and Perez, Jon and Agirre, Irune and Allende, Imanol and Abella, Jaume and Cazorla, Francisco J.},
title = {Towards functional safety compliance of matrix–matrix multiplication for machine learning-based autonomous systems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2021.102298},
doi = {10.1016/j.sysarc.2021.102298},
journal = {J. Syst. Archit.},
month = dec,
numpages = {14},
keywords = {Machine learning, Functional safety, Error detection}
}

@inproceedings{10.1007/978-3-319-25945-1_9,
author = {Altinger, Harald and Herbold, Steffen and Grabowski, Jens and Wotawa, Franz},
title = {Novel Insights on Cross Project Fault Prediction Applied to Automotive Software},
year = {2015},
isbn = {9783319259444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-25945-1_9},
doi = {10.1007/978-3-319-25945-1_9},
abstract = {Defect prediction is a powerful tool that greatly helps focusing quality assurance efforts during development. In the case of the availability of fault data from a particular context, there are different ways of using such fault predictions in practice. Companies like Google, Bell Labs and Cisco make use of fault prediction, whereas its use within automotive industry has not yet gained a lot of attraction, although, modern cars require a huge amount of software to operate. In this paper, we want to contribute the adoption of fault prediction techniques for automotive software projects. Hereby we rely on a publicly available data set comprising fault data from three automotive software projects. When learning a fault prediction model from the data of one particular project, we achieve a remarkably high and nearly perfect prediction performance for the same project. However, when applying a cross-project prediction we obtain rather poor results. These results are rather surprising, because of the fact that the underlying projects are as similar as two distinct projects can possibly be within a certain application context. Therefore we investigate the reasons behind this observation through correlation and factor analyses techniques. We further report the obtained findings and discuss the consequences for future applications of Cross-Project Fault Prediction CPFP in the domain of automotive software.},
booktitle = {Proceedings of the 27th IFIP WG 6.1 International Conference on Testing Software and Systems - Volume 9447},
pages = {141–157},
numpages = {17},
keywords = {Automotive, Cross project fault prediction, Principal component analysis, Project fault prediction},
location = {Sharjah and Dubai, United Arab Emirates},
series = {ICTSS 2015}
}

@article{10.5555/3192182.3192194,
title = {An empirical approach for complexity reduction and fault prediction for software quality attribute},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {1–3},
issn = {1743-8195},
abstract = {Designing the high-quality software is a difficult one due to the high complexity and fault prone class. To reduce the complexity and predict the fault-prone class in the object orient software design proposed a new empirical approach. This proposed approach concentrates more on to increase the software quality in the object oriented programming structures. This technique will collect the dataset and metric values from CK-based metrics. And then complexity will be calculated based on the weighted approach. The fault prediction will be done, based on the low usage of the dataset and high complexity dataset. This helps to increase the software quality. In simulation section, the proposed approach has performed and analysed the parameters such as accuracy, fairness, recall, prediction rate and efficiency. The experimental results have shown that the proposed approach increases the prediction rate, accuracy and efficiency.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {177–187},
numpages = {11}
}

@article{10.1007/s10515-011-0090-3,
author = {He, Zhimin and Shu, Fengdi and Yang, Ye and Li, Mingshu and Wang, Qing},
title = {An investigation on the feasibility of cross-project defect prediction},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0090-3},
doi = {10.1007/s10515-011-0090-3},
abstract = {Software defect prediction helps to optimize testing resources allocation by identifying defect-prone modules prior to testing. Most existing models build their prediction capability based on a set of historical data, presumably from the same or similar project settings as those under prediction. However, such historical data is not always available in practice. One potential way of predicting defects in projects without historical data is to learn predictors from data of other projects. This paper investigates defect predictions in the cross-project context focusing on the selection of training data. We conduct three large-scale experiments on 34 data sets obtained from 10 open source projects. Major conclusions from our experiments include: (1) in the best cases, training data from other projects can provide better prediction results than training data from the same project; (2) the prediction results obtained using training data from other projects meet our criteria for acceptance on the average level, defects in 18 out of 34 cases were predicted at a Recall greater than 70% and a Precision greater than 50%; (3) results of cross-project defect predictions are related with the distributional characteristics of data sets which are valuable for training data selection. We further propose an approach to automatically select suitable training data for projects without historical data. Prediction results provided by the training data selected by using our approach are comparable with those provided by training data from the same project.},
journal = {Automated Software Engg.},
month = jun,
pages = {167–199},
numpages = {33},
keywords = {Cross-project, Data characteristics, Defect prediction, Machine learning, Training data}
}

@inproceedings{10.1609/aaai.v33i01.33019446,
author = {Elmishali, Amir and Stern, Roni and Kalech, Meir},
title = {DeBGUer: a tool for bug prediction and diagnosis},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019446},
doi = {10.1609/aaai.v33i01.33019446},
abstract = {In this paper, we present the DeBGUer tool, a web-based tool for prediction and isolation of software bugs. DeBGUer is a partial implementation of the Learn, Diagnose, and Plan (LDP) paradigm, which is a recently introduced paradigm for integrating Artificial Intelligence (AI) in the software bug detection and correction process. In LDP, a diagnosis (DX) algorithm is used to suggest possible explanations – diagnoses – for an observed bug. If needed, a test planning algorithm is subsequently used to suggest further testing. Both diagnosis and test planning algorithms consider a fault prediction model, which associates each software component (e.g., class or method) with the likelihood that it contains a bug. DeBGUer implements the first two components of LDP, bug prediction (Learn) and bug diagnosis (Diagnose). It provides an easy-to-use web interface, and has been successfully tested on 12 projects.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1161},
numpages = {6},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3409501.3409543,
author = {Yan, Ziyue and Zong, Lu},
title = {Spatial Prediction of Housing Prices in Beijing Using Machine Learning Algorithms},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409543},
doi = {10.1145/3409501.3409543},
abstract = {The real estate industry places key influence on almost every aspect of social economy given its great financing capacity and prolonged upstream and downstream industry chain. Therefore, predicting housing prices is regarded as an emerging topic in the recent decades. Hedonic Regression and Machine Learning Algorithms are two main methods in this field. This study aims to explore the important explanatory features and determine an accurate mechanism to implement spatial prediction of housing prices in Beijing by incorporating a list of machine learning techniques, including XGBoost, linear regression, Random Forest Regression, Ridge and Lasso Model, bagging and boosting, based on the housing price and features data in Beijing, China. Our result shows that compared to traditional hedonic method, machine learning methods demonstrate significant improvements on the accuracy of estimation despite that they are more time-costly. Moreover, it is found that XGBoost is the most accurate model in explaining and prediciting the spatial dynamics of housing prices in Beijing.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {64–71},
numpages = {8},
keywords = {Spatial Modeling, Prediction, Machine Learning Algorithms, Housing Price},
location = {Qingdao, China},
series = {HPCCT &amp; BDAI '20}
}

@article{10.1007/s11334-015-0258-2,
author = {Abdi, Yousef and Parsa, Saeed and Seyfari, Yousef},
title = {A hybrid one-class rule learning approach based on swarm intelligence for software fault prediction},
year = {2015},
issue_date = {December  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-015-0258-2},
doi = {10.1007/s11334-015-0258-2},
abstract = {Software testing is a fundamental activity in the software development process aimed to determine the quality of software. To reduce the effort and cost of this process, defect prediction methods can be used to determine fault-prone software modules through software metrics to focus testing activities on them. Because of model interpretation and easily used by programmers and testers some recent studies presented classification rules to make prediction models. This study presents a rule-based prediction approach based on kernel k-means clustering algorithm and Distance based Multi-objective Particle Swarm Optimization (DSMOPSO). Because of discrete search space, we modified this algorithm and named it DSMOPSO-D. We prevent best global rules to dominate local rules by dividing the search space with kernel k-means algorithm and by taking different approaches for imbalanced and balanced clusters, we solved imbalanced data set problem. The presented model performance was evaluated by four publicly available data sets from the PROMISE repository and compared with other machine learning and rule learning algorithms. The obtained results demonstrate that our model presents very good performance, especially in large data sets.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {289–301},
numpages = {13},
keywords = {Classification rules, DSMOPSO-D, Fault prediction, Imbalanced data sets, Kernel k-means, Multi-objective particle swarm optimization}
}

@inproceedings{10.1145/3094243.3094245,
author = {Pang, Yulei and Xue, Xiaozhen and Wang, Huaying},
title = {Predicting Vulnerable Software Components through Deep Neural Network},
year = {2017},
isbn = {9781450352321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094243.3094245},
doi = {10.1145/3094243.3094245},
abstract = {Vulnerabilities need to be detected and removed from software. Although previous studies demonstrated the usefulness of employing prediction techniques in deciding about vulnerabilities of software components, the improvement of effectiveness of these prediction techniques is still a grand challenging research question. This paper employed a technique based on a deep neural network with rectifier linear units trained with stochastic gradient descent method and batch normalization, for predicting vulnerable software components. The features are defined as continuous sequences of tokens in source code files. Besides, a statistical feature selection algorithm is then employed to reduce the feature and search space. We evaluated the proposed technique based on some Java Android applications, and the results demonstrated that the proposed technique could predict vulnerable classes, i.e., software components, with high precision, accuracy and recall.},
booktitle = {Proceedings of the 2017 International Conference on Deep Learning Technologies},
pages = {6–10},
numpages = {5},
keywords = {vulnerability prediction, neural network, deep learning, Android},
location = {Chengdu, China},
series = {ICDLT '17}
}

@inproceedings{10.1145/2811411.2811544,
author = {Siebra, Clauirton A. and Mello, Michael A. B.},
title = {The importance of replications in software engineering: a case study in defect prediction},
year = {2015},
isbn = {9781450337380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811411.2811544},
doi = {10.1145/2811411.2811544},
abstract = {Prediction of defects in software is an important investigation area in software engineering, since such technique is able to return indications of parts of the code that are prone to contain problems. Thus, test teams can optimize the allocation of their resources by directing them to modules that are more defect-prone. The use of supervised learning is one of the approaches to support the design of prediction models. However, the erroneous use of training datasets can lead to poor models and, consequently, false results regarding accuracy. This work replicates important experiments of the area and shows how they could provide reliable results via the use of simple techniques of pre-processing. Based on the results, we discuss the importance of replications as method to find problems in current results and how this method is being motivated inside the software engineering area.},
booktitle = {Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems},
pages = {376–381},
numpages = {6},
keywords = {supervised learning, replication, defect prediction},
location = {Prague, Czech Republic},
series = {RACS '15}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00066,
author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {A replication of are machine learning cloud APIs used correctly},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00066},
doi = {10.1109/ICSE-Companion52605.2021.00066},
abstract = {This artifact aims to provide benchmark suite, data, and script used in our study "Are Machine Learning Cloud APIs Used Correctly?". We collected a suite of 360 non-trivial applications that use ML cloud APIs for manual study. We also developed checkers and tool to detect and fix API mis-uses. We hope this artifact can motivate and help future research to further tackle ML API mis-uses. All related data are available online.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {158–159},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2393596.2393669,
author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar},
title = {Recalling the "imprecision" of cross-project defect prediction},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393669},
doi = {10.1145/2393596.2393669},
abstract = {There has been a great deal of interest in defect prediction: using prediction models trained on historical data to help focus quality-control resources in ongoing development. Since most new projects don't have historical data, there is interest in cross-project prediction: using data from one project to predict defects in another. Sadly, results in this area have largely been disheartening. Most experiments in cross-project defect prediction report poor performance, using the standard measures of precision, recall and F-score. We argue that these IR-based measures, while broadly applicable, are not as well suited for the quality-control settings in which defect prediction models are used. Specifically, these measures are taken at specific threshold settings (typically thresholds of the predicted probability of defectiveness returned by a logistic regression model). However, in practice, software quality control processes choose from a range of time-and-cost vs quality tradeoffs: how many files shall we test? how many shall we inspect? Thus, we argue that measures based on a variety of tradeoffs, viz., 5%, 10% or 20% of files tested/inspected would be more suitable. We study cross-project defect prediction from this perspective. We find that cross-project prediction performance is no worse than within-project performance, and substantially better than random prediction!},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {61},
numpages = {11},
keywords = {inspection, fault prediction, empirical software engineering},
location = {Cary, North Carolina},
series = {FSE '12}
}

@article{10.1007/s10515-010-0069-5,
author = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ay\c{s}e},
title = {Defect prediction from static code features: current results, limitations, new approaches},
year = {2010},
issue_date = {December  2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0069-5},
doi = {10.1007/s10515-010-0069-5},
abstract = {Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective.Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection "AUC(pd, pf)"; i.e. the area under the curve of a probability of false alarm versus probability of detection.Accordingly, we explore changing the standard goal. Learners that maximize "AUC(effort, pd)" find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods.Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.},
journal = {Automated Software Engg.},
month = dec,
pages = {375–407},
numpages = {33},
keywords = {Defect prediction, Static code features, WHICH}
}

@article{10.1016/j.future.2019.11.042,
author = {Loreti, Daniela and Lippi, Marco and Torroni, Paolo},
title = {Parallelizing Machine Learning as a service for the end-user},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.11.042},
doi = {10.1016/j.future.2019.11.042},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {275–286},
numpages = {12},
keywords = {Machine Learning as a service, Parallelization, MapReduce}
}

@article{10.1007/s10270-020-00856-9,
author = {Pilarski, Sebastian and Staniszewski, Martin and Bryan, Matthew and Villeneuve, Frederic and Varr\'{o}, D\'{a}niel},
title = {Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime: For multi-disciplinary design and operation of gas turbines},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00856-9},
doi = {10.1007/s10270-020-00856-9},
abstract = {The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {685–709},
numpages = {25},
keywords = {Prediction-at-runtime, Machine learning, Neural networks, Automated deployment, Code generation, Gas turbine engines}
}

@inproceedings{10.1145/3482909.3482911,
author = {Santos, Sebasti\~{a}o and Silveira, Beatriz and Durelli, Vinicius and Durelli, Rafael and Souza, Simone and Delamaro, Marcio},
title = {On Using Decision Tree Coverage Criteria forTesting Machine Learning Models},
year = {2021},
isbn = {9781450385039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482909.3482911},
doi = {10.1145/3482909.3482911},
abstract = {Over the past decade, there has been a growing interest in applying machine learning (ML) to address a myriad of tasks. Owing to this interest, the adoption of ML-based systems has gone mainstream. However, this widespread adoption of ML-based systems poses new challenges for software testers that must improve the quality and reliability of these ML-based solutions. To cope with the challenges of testing ML-based systems, we propose novel test adequacy criteria based on decision tree models. Differently from the traditional approach to testing ML models, which relies on manual collection and labelling of data, our criteria leverage the internal structure of decision tree models to guide the selection of test inputs. Thus, we introduce decision tree coverage (DTC) and boundary value analysis (BVA) as approaches to systematically guide the creation of effective test data that exercises key structural elements of a given decision tree model. To evaluate these criteria, we carried out an experiment using 12 datasets. We measured the effectiveness of test inputs in terms of the difference in model’s behavior between the test input and the training data. The experiment results indicate that our testing criteria can be used to guide the generation of effective test data.},
booktitle = {Proceedings of the 6th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {1–9},
numpages = {9},
keywords = {Testing Criterion, Software Testing, Decision Tree},
location = {Joinville, Brazil},
series = {SAST '21}
}

@inproceedings{10.1145/2020390.2020405,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {An iterative semi-supervised approach to software fault prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020405},
doi = {10.1145/2020390.2020405},
abstract = {Background: Many statistical and machine learning techniques have been implemented to build predictive fault models. Traditional methods are based on supervised learning. Software metrics for a module and corresponding fault information, available from previous projects, are used to train a fault prediction model. This approach calls for a large size of training data set and enables the development of effective fault prediction models. In practice, data collection costs, the lack of data from earlier projects or product versions may make large fault prediction training data set unattainable. Small size of the training set that may be available from the current project is known to deteriorate the performance of the fault predictive model. In semi-supervised learning approaches, software modules with known or unknown fault content can be used for training.Aims: To implement and evaluate a semi-supervised learning approach in software fault prediction.Methods: We investigate an iterative semi-supervised approach to software quality prediction in which a base supervised learner is used within a semi-supervised application.Results: We varied the size of labeled software modules from 2% to 50% of all the modules in the project. After tracking the performance of each iteration in the semi-supervised algorithm, we observe that semi-supervised learning improves fault prediction if the number of initially labeled software modules exceeds 5%.Conclusion: The semi-supervised approach outperforms the corresponding supervised learning approach when both use random forest as base classification algorithm.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {15},
numpages = {10},
keywords = {fault prediction, semi-supervised learning},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/3266237.3266273,
author = {Braga, Rony\'{e}rison and Neto, Pedro Santos and Rab\^{e}lo, Ricardo and Santiago, Jos\'{e} and Souza, Matheus},
title = {A machine learning approach to generate test oracles},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266273},
doi = {10.1145/3266237.3266273},
abstract = {One of the essential activities for quality assurance in software development is the software testing. Studies report that Software Testing is one of the most costly activities in the development process, can reach up to 50 percent of its total cost. One of the great challenges of conducting software testing is related to the automation of a mechanism known as "test oracle". This work presents an approach based on machine learning (ML) for automation of the test oracle mechanism in software. The approach uses historical usage data from an application captured by inserting a capture component into the application under test. These data go through a Knowledge Discovery in Database step and are then used for training to generate an oracle suitable for the application under test. Four experiments were executed with web applications to evaluate the proposed approach. The first and second experiments were performed with a fictitious application, with faults inserted randomly in the first experiment, inserted by a developer in the second one and inserted by mutation tests in third one. The fourth experiment was carried out with a large real application in order to assure the results of the preliminary experiments. The experiments presented indications of the suitability of the approach to the solution of the problem.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {142–151},
numpages = {10},
keywords = {testing automation, test oracle, machine learning},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1007/s11219-020-09511-4,
author = {Moreno, Valent\'{\i}n and G\'{e}nova, Gonzalo and Parra, Eugenio and Fraga, Anabel},
title = {Application of machine learning techniques to the flexible assessment and improvement of requirements quality},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09511-4},
doi = {10.1007/s11219-020-09511-4},
abstract = {It is already common to compute quantitative metrics of requirements to assess their quality. However, the risk is to build assessment methods and tools that are both arbitrary and rigid in the parameterization and combination of metrics. Specifically, we show that a linear combination of metrics is insufficient to adequately compute a global measure of quality. In this work, we propose to develop a flexible method to assess and improve the quality of requirements that can be adapted to different contexts, projects, organizations, and quality standards, with a high degree of automation. The domain experts contribute with an initial set of requirements that they have classified according to their quality, and we extract their quality metrics. We then use machine learning techniques to emulate the implicit expert’s quality function. We provide also a procedure to suggest improvements in bad requirements. We compare the obtained rule-based classifiers with different machine learning algorithms, obtaining measurements of effectiveness around 85%. We show as well the appearance of the generated rules and how to interpret them. The method is tailorable to different contexts, different styles to write requirements, and different demands in quality. The whole process of inferring and applying the quality rules adapted to each organization is highly automated.},
journal = {Software Quality Journal},
month = dec,
pages = {1645–1674},
numpages = {30},
keywords = {Requirements quality, Machine learning, Automatic classification, Automatic improvement, Experts’ judgment, Flexible assessment}
}

@inproceedings{10.1109/ASE.2015.56,
author = {Nam, Jaechang and Kim, Sunghun},
title = {CLAMI: defect prediction on unlabeled datasets},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.56},
doi = {10.1109/ASE.2015.56},
abstract = {Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort.In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {452–463},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1145/3212695,
author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
title = {A Survey of Machine Learning for Big Code and Naturalness},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3212695},
doi = {10.1145/3212695},
abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {81},
numpages = {37},
keywords = {software engineering tools, machine learning, code naturalness, Big code}
}

@inproceedings{10.1145/2020390.2020406,
author = {Paikari, Elham and Sun, Bo and Ruhe, Guenther and Livani, Emadoddin},
title = {Customization support for CBR-based defect prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020406},
doi = {10.1145/2020390.2020406},
abstract = {Background: The prediction performance of a case-based reasoning (CBR) model is influenced by the combination of the following parameters: (i) similarity function, (ii) number of nearest neighbor cases, (iii) weighting technique used for attributes, and (iv) solution algorithm. Each combination of the above parameters is considered as an instantiation of the general CBR-based prediction method. The selection of an instantiation for a new data set with specific characteristics (such as size, defect density and language) is called customization of the general CBR method.Aims: For the purpose of defect prediction, we approach the question which combinations of parameters works best at which situation. Three more specific questions were studied:(RQ1) Does one size fit all? Is one instantiation always the best?(RQ2) If not, which individual and combined parameter settings occur most frequently in generating the best prediction results?(RQ3) Are there context-specific rules to support the customization?Method: In total, 120 different CBR instantiations were created and applied to 11 data sets from the PROMISE repository. Predictions were evaluated in terms of their mean magnitude of relative error (MMRE) and percentage Pred(α) of objects fulfilling a prediction quality level α. For the third research question, dependency network analysis was performed.Results: Most frequent parameter options for CBR instantiations were neural network based sensitivity analysis (as the weighting technique), un-weighted average (as the solution algorithm), and maximum number of nearest neighbors (as the number of nearest neighbors). Using dependency network analysis, a set of recommendations for customization was provided.Conclusion: An approach to support customization is provided. It was confirmed that application of context-specific rules across groups of similar data sets is risky and produces poor results.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {16},
numpages = {10},
keywords = {case-based reasoning, customization, defect prediction, dependency network analysis, instantiation},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.4018/IJSSCI.2016070102,
author = {Rashid, Ekbal},
title = {R4 Model for Case-Based Reasoning and Its Application for Software Fault Prediction},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {3},
issn = {1942-9045},
url = {https://doi.org/10.4018/IJSSCI.2016070102},
doi = {10.4018/IJSSCI.2016070102},
abstract = {Making R4 model effective and efficient I have introduced some new features, i.e., renovation of knowledgebase KBS and reducing the maintenance cost by removing the duplicate record from the KBS. Renovation of knowledgebase is the process of removing duplicate record stored in knowledgebase and adding world new problems along with world new solutions. This paper explores case-based reasoning and its applications for software quality improvement through early prediction of error patterns. It summarizes a variety of techniques for software quality prediction in the domain of software engineering. The system predicts the error level with respect to LOC and with respect to development time, and both affects the quality level. This paper also reviews four existing models of case-based reasoning CBR. The paper presents a work in which I have expanded our previous work Rashid et al., 2012. I have used different similarity measures to find the best method that increases reliability. The present work is also credited through introduction of some new terms like coefficient of efficiency, i.e., developer's ability.},
journal = {Int. J. Softw. Sci. Comput. Intell.},
month = jul,
pages = {19–38},
numpages = {20},
keywords = {Development Time, LOC, Machine Learning, Reliability, Similarity Function, Software Fault Prediction}
}

@article{10.1007/s11219-008-9053-8,
author = {Kastro, Yomi and Bener, Ay\c{s}e Basar},
title = {A defect prediction method for software versioning},
year = {2008},
issue_date = {December  2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-008-9053-8},
doi = {10.1007/s11219-008-9053-8},
abstract = {New methodologies and tools have gradually made the life cycle for software development more human-independent. Much of the research in this field focuses on defect reduction, defect identification and defect prediction. Defect prediction is a relatively new research area that involves using various methods from artificial intelligence to data mining. Identifying and locating defects in software projects is a difficult task. Measuring software in a continuous and disciplined manner provides many advantages such as the accurate estimation of project costs and schedules as well as improving product and process qualities. This study aims to propose a model to predict the number of defects in the new version of a software product with respect to the previous stable version. The new version may contain changes related to a new feature or a modification in the algorithm or bug fixes. Our proposed model aims to predict the new defects introduced into the new version by analyzing the types of changes in an objective and formal manner as well as considering the lines of code (LOC) change. Defect predictors are helpful tools for both project managers and developers. Accurate predictors may help reducing test times and guide developers towards implementing higher quality codes. Our proposed model can aid software engineers in determining the stability of software before it goes on production. Furthermore, such a model may provide useful insight for understanding the effects of a feature, bug fix or change in the process of defect detection.},
journal = {Software Quality Journal},
month = dec,
pages = {543–562},
numpages = {20},
keywords = {Defect prediction, Neural networks, Software defects}
}

@inproceedings{10.1145/2590748.2590755,
author = {Rathore, Santosh Singh and Gupta, Atul},
title = {A comparative study of feature-ranking and feature-subset selection techniques for improved fault prediction},
year = {2014},
isbn = {9781450327763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590748.2590755},
doi = {10.1145/2590748.2590755},
abstract = {The quality of a fault prediction model depends on the software metrics that are used to build the prediction model. Feature selection represents a process of selecting a subset of relevant features that may lead to build improved prediction models. Feature selection techniques can be broadly categorized into two subcategories: feature-ranking and feature-subset selection. In this paper, we present a comparative investigation of seven feature-ranking techniques and eight feature-subset selection techniques for improved fault prediction. The performance of these feature selection techniques is evaluated using two popular machine-learning classifiers: Naive Bayes and Random Forest, over fourteen software project's fault-datasets obtained from the PROMISE data repository. The performances were measured using F-measure and AUC values. Our results demonstrated that feature-ranking techniques produced better results compared to feature-subset selection techniques. Among, the feature-ranking techniques used in the study, InfoGain and PCA techniques provided the best performance over all the datasets, while for feature-subset selection techniques ClassifierSubsetEval and Logistic Regression produced better results against their peers.},
booktitle = {Proceedings of the 7th India Software Engineering Conference},
articleno = {7},
numpages = {10},
keywords = {wrappers, software metrics, filters, feature-ranking, feature selection, fault prediction},
location = {Chennai, India},
series = {ISEC '14}
}

@article{10.1016/j.dss.2015.02.003,
author = {Hu, Yong and Feng, Bin and Mo, Xizhu and Zhang, Xiangzhou and Ngai, E.W.T. and Fan, Ming and Liu, Mei},
title = {Cost-sensitive and ensemble-based prediction model for outsourced software project risk prediction},
year = {2015},
issue_date = {April 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {72},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2015.02.003},
doi = {10.1016/j.dss.2015.02.003},
abstract = {Nowadays software is mainly developed through outsourcing and it has become one of the most important business practice strategies for the software industry. However, outsourcing projects are often affiliated with high failure rate. Therefore to ensure success in outsourcing projects, past research has aimed to develop intelligent risk prediction models to evaluate the success rate and cost-effectiveness of software projects. In this study, we first summarized related work over the past 20years and observed that all existing prediction models assume equal misclassification costs, neglecting actual situations in the management of software projects. In fact, overlooking project failure is far more serious than the misclassification of a success-prone project as a failure. Moreover, ensemble learning, a technique well-recognized to improve prediction performance in other fields, has not yet been comprehensively studied in software project risk prediction. This study aims to close the research gaps by exploring cost-sensitive analysis and classifier ensemble methods. Comparative analysis with T-test on 60 different risk prediction models using 327 outsourced software project samples suggests that the ideal model is a homogeneous ensemble model of decision trees (DT) based on bagging. Interestingly, DT underperformed Support Vector Machine (SVM) in accuracy (i.e., assuming equal misclassification cost), but outperformed in cost-sensitive analysis under the proposed framework. In conclusion, this study proposes the first cost-sensitive and ensemble-based hybrid modeling framework (COSENS) for software project risk prediction. In addition, it establishes a new rigorous evaluation standard for assessing software risk prediction models by considering misclassification costs. Display Omitted The first cost-sensitive and ensemble framework to predict software project riskA comprehensive T-test method was used for rigorous performance comparison.A total of 60 models were built and compared based on 327 real project samples.Decision tree underperformed SVM in accuracy, but outperformed in cost analysis.A new rigorous model standard for software project risk analysis is established.},
journal = {Decis. Support Syst.},
month = apr,
pages = {11–23},
numpages = {13},
keywords = {Risk prediction, Risk management, Outsourced software project, Ensemble, Cost-sensitive, COSENS}
}

@article{10.1155/2020/7426461,
author = {Rahman, Md. Mostafizer and Watanobe, Yutaka and Nakamura, Keita and Bures, Miroslav},
title = {A Neural Network Based Intelligent Support Model for Program Code Completion},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1058-9244},
url = {https://doi.org/10.1155/2020/7426461},
doi = {10.1155/2020/7426461},
abstract = {In recent years, millions of source codes are generated in different languages on a daily basis all over the world. A deep neural network-based intelligent support model for source code completion would be a great advantage in software engineering and programming education fields. Vast numbers of syntax, logical, and other critical errors that cannot be detected by normal compilers continue to exist in source codes, and the development of an intelligent evaluation methodology that does not rely on manual compilation has become essential. Even experienced programmers often find it necessary to analyze an entire program in order to find a single error and are thus being forced to waste valuable time debugging their source codes. With this point in mind, we proposed an intelligent model that is based on long short-term memory (LSTM) and combined it with an attention mechanism for source code completion. Thus, the proposed model can detect source code errors with locations and then predict the correct words. In addition, the proposed model can classify the source codes as to whether they are erroneous or not. We trained our proposed model using the source code and then evaluated the performance. All of the data used in our experiments were extracted from Aizu Online Judge (AOJ) system. The experimental results obtained show that the accuracy in terms of error detection and prediction of our proposed model approximately is 62% and source code classification accuracy is approximately 96% which outperformed a standard LSTM and other state-of-the-art models. Moreover, in comparison to state-of-the-art models, our proposed model achieved an interesting level of success in terms of error detection, prediction, and classification when applied to long source code sequences. Overall, these experimental results indicate the usefulness of our proposed model in software engineering and programming education arena.},
journal = {Sci. Program.},
month = jan,
numpages = {18}
}

@inproceedings{10.1007/978-3-319-68935-7_14,
author = {Phan, Anh Viet and Nguyen, Minh Le and Bui, Lam Thu},
title = {SibStCNN and TBCNN + kNN-TED: New Models over Tree Structures for Source Code Classification},
year = {2017},
isbn = {978-3-319-68934-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-68935-7_14},
doi = {10.1007/978-3-319-68935-7_14},
abstract = {This paper aims to solve a software engineering problem by applying several approaches to exploit tree representations of programs. Firstly, we propose a new sibling-subtree convolutional neural network (SibStCNN), and combination models of tree-based neural networks and k-Nearest Neighbors (kNN) for source code classification. Secondly, we present a pruning tree technique to reduce data dimension and strengthen classifiers. The experiments show that the proposed models outperform other methods, and the pruning tree leads to not only a substantial reduction in execution time but also an increase in accuracy.},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2017: 18th International Conference, Guilin, China, October 30 – November 1, 2017, Proceedings},
pages = {120–128},
numpages = {9},
keywords = {Abstract Syntax Trees (AST), Convolutional Neural Networks (CNNs), k-Nearest Neighbors (kNNs)},
location = {Guilin, China}
}

@inproceedings{10.1145/2786805.2786813,
author = {Jing, Xiaoyuan and Wu, Fei and Dong, Xiwei and Qi, Fumin and Xu, Baowen},
title = {Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786813},
doi = {10.1145/2786805.2786813},
abstract = {Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {496–507},
numpages = {12},
keywords = {unified metric representation, company-specific metrics, common metrics, canonical correlation analysis (CCA), Heterogeneous cross-company defect prediction (HCCDP)},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1504/IJDATS.2016.075971,
author = {Erturk, Ezgi and Sezer, Ebru Akcapinar},
title = {Software fault prediction using Mamdani type fuzzy inference system},
year = {2016},
issue_date = {April 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {1},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2016.075971},
doi = {10.1504/IJDATS.2016.075971},
abstract = {High quality software requires the occurrence of minimum number of failures while software runs. Software fault prediction is the determining whether software modules are prone to fault or not. Identification of the modules or code segments which need detailed testing, editing or, reorganising can be possible with the help of software fault prediction systems. In literature, many studies present models for software fault prediction using some soft computing methods which use training/testing phases. As a result, they require historical data to build models. In this study, to eliminate this drawback, Mamdani type fuzzy inference system FIS is applied for the software fault prediction problem. Several FIS models are produced and assessed with ROC-AUC as performance measure. The results achieved are ranging between 0.7138 and 0.7304; they are encouraging us to try FIS with the different software metrics and data to demonstrate general FIS performance on this problem.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = apr,
pages = {14–28},
numpages = {15}
}

@inproceedings{10.1109/ICSE43902.2021.00109,
author = {Bui, Nghi D. Q. and Yu, Yijun and Jiang, Lingxiao},
title = {InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00109},
doi = {10.1109/ICSE43902.2021.00109},
abstract = {Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units.We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1186–1197},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3106237.3106290,
author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
title = {Are deep neural networks the best choice for modeling source code?},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106290},
doi = {10.1145/3106237.3106290},
abstract = {Current statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add &amp; remove text, and mix &amp; swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N-gram, as well as RNN, and LSTM deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N-gram models for source code can yield performance that surpasses even RNN and LSTM based deep-learning models.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {763–773},
numpages = {11},
keywords = {software tools, naturalness, language models},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2024445.2024455,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald},
title = {Using the gini coefficient for bug prediction in eclipse},
year = {2011},
isbn = {9781450308489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024445.2024455},
doi = {10.1145/2024445.2024455},
abstract = {The Gini coefficient is a prominent measure to quantify the inequality of a distribution. It is often used in the field of economy to describe how goods, e.g., wealth or farmland, are distributed among people. We use the Gini coefficient to measure code ownership by investigating how changes made to source code are distributed among the developer population. The results of our study with data from the Eclipse platform show that less bugs can be expected if a large share of all changes are accumulated, i.e., carried out, by relatively few developers.},
booktitle = {Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution},
pages = {51–55},
numpages = {5},
keywords = {gini coefficient, code ownership, bug prediction},
location = {Szeged, Hungary},
series = {IWPSE-EVOL '11}
}

@inproceedings{10.1145/3338906.3338937,
author = {Aggarwal, Aniya and Lohia, Pranay and Nagar, Seema and Dey, Kuntal and Saha, Diptikalyan},
title = {Black box fairness testing of machine learning models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338937},
doi = {10.1145/3338906.3338937},
abstract = {Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {625–635},
numpages = {11},
keywords = {Symbolic Execution, Local Explainability, Individual Discrimination, Fairness Testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1016/j.infsof.2021.106576,
author = {Cao, Sicong and Sun, Xiaobing and Bo, Lili and Wei, Ying and Li, Bin},
title = {         BGNN4VD: Constructing Bidirectional Graph Neural-Network for Vulnerability Detection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {136},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106576},
doi = {10.1016/j.infsof.2021.106576},
journal = {Inf. Softw. Technol.},
month = aug,
numpages = {11},
keywords = {Vulnerability detection, Bidirectional Graph Neural-Network, Code representation}
}

@article{10.1016/j.infsof.2010.06.006,
author = {Tosun, Ay\c{s}e and Bener, Ay\c{s}e and Turhan, Burak and Menzies, Tim},
title = {Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry},
year = {2010},
issue_date = {November, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.06.006},
doi = {10.1016/j.infsof.2010.06.006},
abstract = {Context: Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results. Objective: In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software. Method: We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naive Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naive Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost-benefit analysis to show that our approach can be efficiently put into practice. Results: Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naive Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold. Conclusion: Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naive Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1242–1257},
numpages = {16},
keywords = {Experience report, Na\"{\i}ve Bayes, Software defect prediction, Static code attributes}
}

@inproceedings{10.1145/2499393.2499397,
author = {Tass\'{e}, Jos\'{e}e},
title = {Using code change types in an analogy-based classifier for short-term defect prediction},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499397},
doi = {10.1145/2499393.2499397},
abstract = {Current approaches for defect prediction usually analyze files (or modules) and their development as work is done on a given release, to predict post-release defects. What is missing is an approach for predicting bugs to be detected in a more short-term interval, even within the development of a particular version. In this paper, we propose a defect predictor that looks into change bursts in a given file, analyzing the number of changes and their types, and then predict whether the file is likely to have a bug found within the next 3 months after that change burst. An analogy-based classifier is used for this task: the prediction is made based on comparisons with similar change bursts that occurred in other files. New metrics are described to capture the change type of a file (e.g., small local change, massive change all in one place, multiple changes scattered throughout the file).},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {5},
numpages = {4},
keywords = {analogy-based classifier, change burst, change type metrics, defect prediction, short-term prediction},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@article{10.1007/s00500-016-2316-6,
author = {Chinna Gounder Dhanajayan, Rajaganapathy and Appavu Pillai, Subramani},
title = {SLMBC: spiral life cycle model-based Bayesian classification technique for efficient software fault prediction and classification},
year = {2017},
issue_date = {January   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-016-2316-6},
doi = {10.1007/s00500-016-2316-6},
abstract = {Software fault prediction and classification plays a vital role in the software development process for assuring high quality and reliability of the software product. Earlier prediction of the fault-prone software modules enables timely correction of the faults and delivery of reliable product. Generally, the fuzzy logic, decision tree and neural networks are deployed for fault prediction. But these techniques suffer due to low accuracy and inconsistency. To overcome these issues, this paper proposes a spiral life cycle model-based Bayesian classification technique for efficient software fault prediction and classification. In this process, initially the dependent and independent software modules are identified. The spiral life cycle model is used for testing the software modules in each life cycle of the software development process. Bayesian classification is applied to classify the software modules as faulty module and non-faulty module, by using the probability distribution models. Robust similarity-aware clustering algorithm performs clustering of the faulty and non-faulty software modules based on the similarity measure of the features in the dataset. From the experimental results, it is observed that the proposed method enables accurate prediction and classification of the faulty modules. The proposed technique achieves higher accuracy, precision, recall, probability of detection, F-measure and lower error rate than the existing techniques. The misclassification rate of the proposed technique is found to be lower than the existing techniques. Hence, the reliability of the software development process can be improved.},
journal = {Soft Comput.},
month = jan,
pages = {403–415},
numpages = {13},
keywords = {Spiral life cycle model-based Bayesian classification technique (SLMBC), Spiral life cycle model, Software development, Segregate fault prediction algorithm, Robust similarity-aware clustering (RSC) algorithm, Bayesian classification}
}

@inproceedings{10.1145/3166094.3166114,
author = {Chistyakov, Alexander and Pripadchev, Artem and Radchenko, Irina},
title = {On development of a framework for massive source code analysis using static code analyzers},
year = {2017},
isbn = {9781450363969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3166094.3166114},
doi = {10.1145/3166094.3166114},
abstract = {Authors describe architecture and implementation of an automated source code analyzing system which uses pluggable static code analyzers. The paper presents a module for gathering and analyzing the source code massively in a detailed manner. Authors also compare existing static code analyzers for Python programming language. A common format of storing results of code analysis for subsequent processing is introduced. Also, authors discuss methods of statistical processing and visualizing of raw analysis data.},
booktitle = {Proceedings of the 13th Central &amp; Eastern European Software Engineering Conference in Russia},
articleno = {20},
numpages = {3},
keywords = {static analyzers, open source, code analysis},
location = {St. Petersburg, Russia},
series = {CEE-SECR '17}
}

@article{10.1145/3324916,
author = {Ren, Xiaoxue and Xing, Zhenchang and Xia, Xin and Lo, David and Wang, Xinyu and Grundy, John},
title = {Neural Network-based Detection of Self-Admitted Technical Debt: From Performance to Explainability},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3324916},
doi = {10.1145/3324916},
abstract = {Technical debt is a metaphor to reflect the tradeoff software engineers make between short-term benefits and long-term stability. Self-admitted technical debt (SATD), a variant of technical debt, has been proposed to identify debt that is intentionally introduced during software development, e.g., temporary fixes and workarounds. Previous studies have leveraged human-summarized patterns (which represent n-gram phrases that can be used to identify SATD) or text-mining techniques to detect SATD in source code comments. However, several characteristics of SATD features in code comments, such as vocabulary diversity, project uniqueness, length, and semantic variations, pose a big challenge to the accuracy of pattern or traditional text-mining-based SATD detection, especially for cross-project deployment. Furthermore, although traditional text-mining-based method outperforms pattern-based method in prediction accuracy, the text features it uses are less intuitive than human-summarized patterns, which makes the prediction results hard to explain. To improve the accuracy of SATD prediction, especially for cross-project prediction, we propose a Convolutional Neural Network-- (CNN) based approach for classifying code comments as SATD or non-SATD. To improve the explainability of our model’s prediction results, we exploit the computational structure of CNNs to identify key phrases and patterns in code comments that are most relevant to SATD. We have conducted an extensive set of experiments with 62,566 code comments from 10 open-source projects and a user study with 150 comments of another three projects. Our evaluation confirms the effectiveness of different aspects of our approach and its superior performance, generalizability, adaptability, and explainability over current state-of-the-art traditional text-mining-based methods for SATD classification.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {15},
numpages = {45},
keywords = {model generalizability, model explainability, model adaptability, cross project prediction, convolutional neural network, Self-admitted technical debt}
}

@inproceedings{10.1007/978-3-030-58817-5_66,
author = {Kumari, Madhu and Singh, Ujjawal Kumar and Sharma, Meera},
title = {Entropy Based Machine Learning Models for Software Bug Severity Assessment in Cross Project Context},
year = {2020},
isbn = {978-3-030-58816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58817-5_66},
doi = {10.1007/978-3-030-58817-5_66},
abstract = {There can be noise and uncertainty in the bug reports data as the bugs are reported by a heterogeneous group of users working across different countries. Bug description is an essential attribute that helps to predict other bug attributes, such as severity, priority, and time fixes. We need to consider the noise and confusion present in the text of the bug report, as it can impact the output of different machine learning techniques. Shannon entropy has been used in this paper to calculate summary uncertainty about the bug. Bug severity attribute tells about the type of impact the bug has on the functionality of the software. Correct bug severity estimation allows scheduling and repair bugs and hence help in resource and effort utilization. To predict the severity of the bug we need software project historical data to train the classifier. These training data are not always available in particular for new software projects. The solution which is called cross project prediction is to use the training data from other projects. Using bug priority, summary weight and summary entropy, we have proposed cross project bug severity assessment models. Results for proposed summary entropy based approach for bug severity prediction in cross project context show improved performance of the Accuracy and F-measure up to 70.23% and 93.72% respectively across all the machine learning techniques over existing work.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part VI},
pages = {939–953},
numpages = {15},
location = {Cagliari, Italy}
}

@phdthesis{10.5555/1237392,
author = {Kim, Sunghun},
advisor = {Whitehead, E. James},
title = {Adaptive bug prediction by analyzing project history},
year = {2006},
isbn = {9780542837180},
publisher = {University of California at Santa Cruz},
address = {USA},
abstract = {Finding and fixing software bugs is difficult, and many developers put significant effort into finding and fixing them. A project's software change history records the change that introduces a bug and the change that subsequently fixes it. This bug-introducing and bug-fix experience can be used to predict future bugs. This dissertation presents two bug prediction algorithms that adaptively analyze a project's change history: bug cache and change classification. The basic assumption of the bug cache approach is that the bugs do not occur in isolation, but rather in a burst of several related bugs. The bug cache exploits this locality by caching locations that are likely to have bugs. By consulting the bug cache, a developer can detect locations likely to be fault prone. This is useful for prioritizing verification and validation resources on the most bug prone files, functions, or methods. An evaluation of seven open source projects with more than 200,000 revisions shows that the bug cache selects 10% of the source code files; these files account for 73%--95% of future bugs. The change classification approach learns from previous buggy change patterns using two machine learning algorithms, Na\"{\i}ve Bayes and Support Vector Machine. After training on buggy change patterns, it predicts new unknown changes as either buggy or clean. As soon as changes are made, developers can use the predicted information to inspect the new changes, which are an average of 20 lines of code. After training on 12 open source projects, the change classification approach can, on average, classify buggy changes with 78% accuracy and 65% buggy change recall. By leveraging project history and learning the unique bug patterns of each project, both approaches can be used to find locations of bugs. This information can be used to increase software quality and reduce software development cost.},
note = {AAI3229992}
}

@inproceedings{10.1145/3193977.3193982,
author = {Saha, Prashanta and Kanewala, Upulee},
title = {Fault detection effectiveness of source test case generation strategies for metamorphic testing},
year = {2018},
isbn = {9781450357296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193977.3193982},
doi = {10.1145/3193977.3193982},
abstract = {Metamorphic testing is a well known approach to tackle the oracle problem in software testing. This technique requires the use of source test cases that serve as seeds for the generation of follow-up test cases. Systematic design of test cases is crucial for the test quality. Thus, source test case generation strategy can make a big impact on the fault detection effectiveness of metamorphic testing. Most of the previous studies on metamorphic testing have used either random test data or existing test cases as source test cases. There has been limited research done on systematic source test case generation for metamorphic testing. This paper provides a comprehensive evaluation on the impact of source test case generation techniques on the fault finding effectiveness of metamorphic testing. We evaluated the effectiveness of line coverage, branch coverage, weak mutation and random test generation strategies for source test case generation. The experiments are conducted with 77 methods from 4 open source code repositories. Our results show that by systematically creating source test cases, we can significantly increase the fault finding effectiveness of metamorphic testing. Further, in this paper we introduce a simple metamorphic testing tool called "METtester" that we use to conduct metamorphic testing on these methods.},
booktitle = {Proceedings of the 3rd International Workshop on Metamorphic Testing},
pages = {2–9},
numpages = {8},
keywords = {weak mutation, source test case generation, random testing, metamorphic testing, line coverage, branch coverage},
location = {Gothenburg, Sweden},
series = {MET '18}
}

@inproceedings{10.1145/3180155.3180197,
author = {Agrawal, Amritanshu and Menzies, Tim},
title = {Is "better data" better than "better data miners"? on the benefits of tuning SMOTE for defect prediction},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180197},
doi = {10.1145/3180155.3180197},
abstract = {We report and fix an important systematic error in prior studies that ranked classifiers for software analytics. Those studies did not (a) assess classifiers on multiple criteria and they did not (b) study how variations in the data affect the results. Hence, this paper applies (a) multi-performance criteria while (b) fixing the weaker regions of the training data (using SMOTUNED, which is an auto-tuning version of SMOTE). This approach leads to dramatically large increases in software defect predictions when applied in a 5*5 cross-validation study for 3,681 JAVA classes (containing over a million lines of code) from open source systems, SMOTUNED increased AUC and recall by 60% and 20% respectively. These improvements are independent of the classifier used to predict for defects. Same kind of pattern (improvement) was observed when a comparative analysis of SMOTE and SMOTUNED was done against the most recent class imbalance technique.In conclusion, for software analytic tasks like defect prediction, (1) data pre-processing can be more important than classifier choice, (2) ranking studies are incomplete without such pre-processing, and (3) SMOTUNED is a promising candidate for pre-processing.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1050–1061},
numpages = {12},
keywords = {SMOTE, classification, data analytics for software engineering, defect prediction, preprocessing, search based SE, unbalanced data},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.5555/2831120.2831126,
author = {Cook, Devin and Choe, Yung Ryn and Hamilton, John A.},
title = {Finding bugs in source code using commonly available development metadata},
year = {2015},
publisher = {USENIX Association},
address = {USA},
abstract = {Developers and security analysts have been using static analysis for a long time to analyze programs for defects and vulnerabilities. Generally a static analysis tool is run on the source code for a given program, flagging areas of code that need to be further inspected by a human analyst. These tools tend to work fairly well - every year they find many important bugs. These tools are more impressive considering the fact that they only examine the source code, which may be very complex. Now consider the amount of data available that these tools do not analyze. There are many additional pieces of information available that would prove useful for finding bugs in code, such as a history of bug reports, a history of all changes to the code, information about committers, etc. By leveraging all this additional data, it is possible to find more bugs with less user interaction, as well as track useful metrics such as number and type of defects injected by committer. This paper provides a method for leveraging development metadata to find bugs that would otherwise be difficult to find using standard static analysis tools. We showcase two case studies that demonstrate the ability to find new vulnerabilities in large and small software projects by finding new vulnerabilities in the cpython and Roundup open source projects.},
booktitle = {Proceedings of the 8th USENIX Conference on Cyber Security Experimentation and Test},
pages = {6},
numpages = {1},
location = {Washington, D.C.},
series = {CSET'15}
}

@article{10.4018/IJSI.2016100104,
author = {Saifan, Ahmad A. and Alsukhni, Emad and Alawneh, Hanadi and Sbaih, Ayat AL},
title = {Test Case Reduction Using Data Mining Technique},
year = {2016},
issue_date = {October 2016},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {4},
issn = {2166-7160},
url = {https://doi.org/10.4018/IJSI.2016100104},
doi = {10.4018/IJSI.2016100104},
abstract = {Software testing is a process of ratifying the functionality of software. It is a crucial area which consumes a great deal of time and cost. The time spent on testing is mainly concerned with testing large numbers of unreliable test cases. The authors' goal is to reduce the numbers and offer more reliable test cases, which can be achieved using certain selection techniques to choose a subset of existing test cases. The main goal of test case selection is to identify a subset of the test cases that are capable of satisfying the requirements as well as exposing most of the existing faults. The state of practice among test case selection heuristics is cyclomatic complexity and code coverage. The authors used clustering algorithm which is a data mining approach to reduce the number of test cases. Their approach was able to obtain 93 unique effective test cases out a total of 504.},
journal = {Int. J. Softw. Innov.},
month = oct,
pages = {56–70},
numpages = {15},
keywords = {Clustering, Coverage, Cyclomatic Complexity, Redundant Test Cases, Test Case Reduction}
}

@inproceedings{10.1145/3210459.3210469,
author = {Xiao, Yan and Keung, Jacky and Mi, Qing and Bennin, Kwabena E.},
title = {Bug Localization with Semantic and Structural Features using Convolutional Neural Network and Cascade Forest},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210469},
doi = {10.1145/3210459.3210469},
abstract = {Background: Correctly localizing buggy files for bug reports together with their semantic and structural information is a crucial task, which would essentially improve the accuracy of bug localization techniques. Aims: To empirically evaluate and demonstrate the effects of both semantic and structural information in bug reports and source files on improving the performance of bug localization, we propose CNN_Forest involving convolutional neural network and ensemble of random forests that have excellent performance in the tasks of semantic parsing and structural information extraction. Method: We first employ convolutional neural network with multiple filters and an ensemble of random forests with multi-grained scanning to extract semantic and structural features from the word vectors derived from bug reports and source files. And a subsequent cascade forest (a cascade of ensembles of random forests) is used to further extract deeper features and observe the correlated relationships between bug reports and source files. CNNLForest is then empirically evaluated over 10,754 bug reports extracted from AspectJ, Eclipse UI, JDT, SWT, and Tomcat projects. Results: The experiments empirically demonstrate the significance of including semantic and structural information in bug localization, and further show that the proposed CNN_Forest achieves higher Mean Average Precision and Mean Reciprocal Rank measures than the best results of the four current state-of-the-art approaches (NPCNN, LR+WE, DNNLOC, and BugLocator). Conclusion: CNNLForest is capable of defining the correlated relationships between bug reports and source files, and we empirically show that semantic and structural information in bug reports and source files are crucial in improving bug localization.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {101–111},
numpages = {11},
keywords = {word embedding, structural information, semantic information, convolutional neural network, cascade forest, bug localization},
location = {Christchurch, New Zealand},
series = {EASE '18}
}

@inproceedings{10.1145/2938503.2938553,
author = {Soltanifar, Behjat and Akbarinasaji, Shirin and Caglayan, Bora and Bener, Ayse Basar and Filiz, Asli and Kramer, Bryan M.},
title = {Software Analytics in Practice: A Defect Prediction Model Using Code Smells},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938553},
doi = {10.1145/2938503.2938553},
abstract = {In software engineering, maintainability is related to investigating the defects and their causes, correcting the defects and modifying the system to meet customer requirements. Maintenance is a time consuming activity within the software life cycle. Therefore, there is a need for efficiently organizing the software resources in terms of time, cost and personnel for maintenance activity. One way of efficiently managing maintenance resources is to predict defects that may occur after the deployment. Many researchers so far have built defect prediction models using different sets of metrics such as churn and static code metrics. However, hidden causes of defects such as code smells have not been investigated thoroughly. In this study we propose using data science and analytics techniques on software data to build defect prediction models. In order to build the prediction model we used code smells metrics, churn metrics and combination of churn and code smells metrics. The results of our experiments on two different software companies show that code smells is a good indicator of defect proneness of the software product. Therefore, we recommend that code smells metrics should be used to train a defect prediction model to guide the software maintenance team.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {148–155},
numpages = {8},
keywords = {Mining software repositories, Defect Prediction Model, Code Smells},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@book{10.5555/1972541,
author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
title = {Data Mining: Concepts and Techniques},
year = {2011},
isbn = {0123814790},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data}
}

@article{10.1155/2021/2922728,
author = {Iftikhar, Asim and Alam, Muhammad and Ahmed, Rizwan and Musa, Shahrulniza and Su’ud, Mazliham Mohd and Yi, Yugen},
title = {Risk Prediction by Using Artificial Neural Network in Global Software Development},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/2922728},
doi = {10.1155/2021/2922728},
abstract = {The demand for global software development is growing. The nonavailability of software experts at one place or a country is the reason for the increase in the scope of global software development. Software developers who are located in different parts of the world with diversified skills necessary for a successful completion of a project play a critical role in the field of software development. Using the skills and expertise of software developers around the world, one could get any component developed or any IT-related issue resolved. The best software skills and tools are dispersed across the globe, but to integrate these skills and tools together and make them work for solving real world problems is a challenging task. The discipline of risk management gives the alternative strategies to manage risks that the software experts are facing in today’s world of competitiveness. This research is an effort to predict risks related to time, cost, and resources those are faced by distributed teams in global software development environment. To examine the relative effect of these factors, in this research, neural network approaches like Levenberg–Marquardt, Bayesian Regularization, and Scaled Conjugate Gradient have been implemented to predict the responses of risks related to project time, cost, and resources involved in global software development. Comparative analysis of these three algorithms is also performed to determine the highest accuracy algorithms. The findings of this study proved that Bayesian Regularization performed very well in terms of the MSE (validation) criterion as compared with the Levenberg–Marquardt and Scaled Conjugate Gradient approaches.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {25}
}

@inproceedings{10.5555/2486788.2486838,
author = {Lewis, Chris and Lin, Zhongpeng and Sadowski, Caitlin and Zhu, Xiaoyan and Ou, Rong and Whitehead Jr., E. James},
title = {Does bug prediction support human developers? findings from a google case study},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {372–381},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1016/j.knosys.2014.10.017,
author = {Abaei, Golnoush and Selamat, Ali and Fujita, Hamido},
title = {An empirical study based on semi-supervised hybrid self-organizing map for software fault prediction},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {1},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2014.10.017},
doi = {10.1016/j.knosys.2014.10.017},
abstract = {Software testing is a crucial task during software development process with the potential to save time and budget by recognizing defects as early as possible and delivering a more defect-free product. To improve the testing process, fault prediction approaches identify parts of the system that are more defect prone. However, when the defect data or quality-based class labels are not identified or the company does not have similar or earlier versions of the software project, researchers cannot use supervised classification methods for defect detection. In order to detect defect proneness of modules in software projects with high accuracy and improve detection model generalization ability, we propose an automated software fault detection model using semi-supervised hybrid self-organizing map (HySOM). HySOM is a semi-supervised model based on self-organizing map and artificial neural network. The advantage of HySOM is the ability to predict the label of the modules in a semi-supervised manner using software measurement threshold values in the absence of quality data. In semi-supervised HySOM, the role of expert for identifying fault prone modules becomes less critical and more supportive. We have benchmarked the proposed model with eight industrial data sets from NASA and Turkish white-goods embedded controller software. The results show improvement in false negative rate and overall error rate in 80% and 60% of the cases respectively for NASA data sets. Moreover, we investigate the performance of the proposed model with other recent proposed methods. According to the results, our semi-supervised model can be used as an automated tool to guide testing effort by prioritizing the module's defects improving the quality of software development and software testing in less time and budget.},
journal = {Know.-Based Syst.},
month = jan,
pages = {28–39},
numpages = {12},
keywords = {artificial neural network, clustering, self-organizing maps, semi-supervised, software fault prediction, threshold}
}

@article{10.3233/KES-190421,
author = {Panigrahi, Rasmita and Kuanar, Sanjay K. and Kumar, Lov and Padhy, Neelamadhab and Satapathy, Suresh Chandra},
title = {Software reusability metrics prediction and cost estimation by using machine learning algorithms},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {23},
number = {4},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-190421},
doi = {10.3233/KES-190421},
abstract = {In this research, a highly robust and efficient software design optimization model has been proposed for object-oriented programming based software solutions while considering the importance of quality and reliability. Due to a piece of information that software component reusability has allowed cost and time-efficient software design. The software reusability metrics prediction and cost estimation play a vital role in the software industry. Software quality prediction is an important feature that can be achieved a novel machine learning approach. It is a process of gathering and analyzing recurring patterns in software metrics. Machine learning techniques play a crucial role in intelligent decision making and proactive forecasting. This paper focuses on analyzing software reusability and cost estimation metrics by providing the data set. In the present world software, cost estimation and reusability prediction problem has been resolved using various newly developed methods. This paper emphasizes to solve the novel machine learning algorithms as well as improved Output layer self-connection recurrent neural networks (OLSRNN) with kernel fuzzy c-means clustering (KFCM). The investigational results confirmed the competence of the proposed method for solving software reusability and cost estimation.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {317–328},
numpages = {12},
keywords = {Object-Oriented Metrics, software reusability metrics, machine learning techniques, software cost estimation}
}

@inproceedings{10.1145/1540438.1540448,
author = {Mende, Thilo and Koschke, Rainer},
title = {Revisiting the evaluation of defect prediction models},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540448},
doi = {10.1145/1540438.1540448},
abstract = {Defect Prediction Models aim at identifying error-prone parts of a software system as early as possible. Many such models have been proposed, their evaluation, however, is still an open question, as recent publications show.An important aspect often ignored during evaluation is the effort reduction gained by using such models. Models are usually evaluated per module by performance measures used in information retrieval, such as recall, precision, or the area under the ROC curve (AUC). These measures assume that the costs associated with additional quality assurance activities are the same for each module, which is not reasonable in practice. For example, costs for unit testing and code reviews are roughly proportional to the size of a module.In this paper, we investigate this discrepancy using optimal and trivial models. We describe a trivial model that takes only the module size measured in lines of code into account, and compare it to five classification methods. The trivial model performs surprisingly well when evaluated using AUC. However, when an effort-sensitive performance measure is used, it becomes apparent that the trivial model is in fact the worst.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {cost-sensitive performance measures, defect prediction},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@inproceedings{10.5555/3060832.3060845,
author = {Huo, Xuan and Li, Ming and Zhou, Zhi-Hua},
title = {Learning unified features from natural and programming languages for locating buggy source code},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Bug reports provide an effective way for end-users to disclose potential bugs hidden in a software system, while automatically locating the potential buggy source code according to a bug report remains a great challenge in software maintenance. Many previous studies treated the source code as natural language by representing both the bug report and source code based on bag-of-words feature representations, and correlate the bug report and source code by measuring similarity in the same lexical feature space. However, these approaches fail to consider the structure information of source code which carries additional semantics beyond the lexical terms. Such information is important in modeling program functionality. In this paper, we propose a novel convolutional neural network NP-CNN, which leverages both lexical and program structure information to learn unified features from natural language and source code in programming language for automatically locating the potential buggy source code according to bug report. Experimental results on widely-used software projects indicate that NP-CNN significantly outperforms the state-of-the-art methods in locating the buggy source files.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1606–1612},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.5555/2938006.2938019,
author = {Arcelli Fontana, Francesca and M\"{a}ntyl\"{a}, Mika V. and Zanoni, Marco and Marino, Alessandro},
title = {Comparing and experimenting machine learning techniques for code smell detection},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {3},
issn = {1382-3256},
abstract = {Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (&gt;96 %), and only a hundred training examples are needed to reach at least 95 % accuracy.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1143–1191},
numpages = {49},
keywords = {Benchmark for code smell detection, Code smells detection, Machine learning techniques}
}

@article{10.1016/j.asoc.2018.09.042,
author = {Xie, Yunyun and Li, Chaojie and Lv, Youjie and Yu, Chen},
title = {Predicting lightning outages of transmission lines using generalized regression neural network},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {78},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.09.042},
doi = {10.1016/j.asoc.2018.09.042},
journal = {Appl. Soft Comput.},
month = may,
pages = {438–446},
numpages = {9},
keywords = {Transmission line, Lightning outage, Outage prediction, Generalized regression neural network}
}

@article{10.1016/j.infsof.2020.106269,
author = {Mahdieh, Mostafa and Mirian-Hosseinabadi, Seyed-Hassan and Etemadi, Khashayar and Nosrati, Ali and Jalali, Sajad},
title = {Incorporating fault-proneness estimations into coverage-based test case prioritization methods},
year = {2020},
issue_date = {May 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {121},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106269},
doi = {10.1016/j.infsof.2020.106269},
journal = {Inf. Softw. Technol.},
month = may,
numpages = {12},
keywords = {Regression testing, Test case prioritization, Defect prediction, Machine learning, Bug history}
}

@inproceedings{10.1145/3395363.3404540,
author = {Tizpaz-Niari, Saeid and \v{C}ern\'{y}, Pavol and Trivedi, Ashutosh},
title = {Detecting and understanding real-world differential performance bugs in machine learning libraries},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404540},
doi = {10.1145/3395363.3404540},
abstract = {Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {189–199},
numpages = {11},
keywords = {Testing, ML Libraries, Differential Performance Bugs, Debugging},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1007/s10664-013-9285-5,
author = {Lucia, Andrea and Penta, Massimiliano and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
title = {Labeling source code with information retrieval methods: an empirical study},
year = {2014},
issue_date = {October   2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9285-5},
doi = {10.1007/s10664-013-9285-5},
abstract = {To support program comprehension, software artifacts can be labeled--for example within software visualization tools--with a set of representative words, hereby referred to as labels. Such labels can be obtained using various approaches, including Information Retrieval (IR) methods or other simple heuristics. They provide a bird-eye's view of the source code, allowing developers to look over software components fast and make more informed decisions on which parts of the source code they need to analyze in detail. However, few empirical studies have been conducted to verify whether the extracted labels make sense to software developers. This paper investigates (i) to what extent various IR techniques and other simple heuristics overlap with (and differ from) labeling performed by humans; (ii) what kinds of source code terms do humans use when labeling software artifacts; and (iii) what factors--in particular what characteristics of the artifacts to be labeled--influence the performance of automatic labeling techniques. We conducted two experiments in which we asked a group of students (38 in total) to label 20 classes from two Java software systems, JHotDraw and eXVantage. Then, we analyzed to what extent the words identified with an automated technique--including Vector Space Models, Latent Semantic Indexing (LSI), latent Dirichlet allocation (LDA), as well as customized heuristics extracting words from specific source code elements--overlap with those identified by humans. Results indicate that, in most cases, simpler automatic labeling techniques--based on the use of words extracted from class and method names as well as from class comments--better reflect human-based labeling. Indeed, clustering-based approaches (LSI and LDA) are more worthwhile to be used for source code artifacts having a high verbosity, as well as for artifacts requiring more effort to be manually labeled. The obtained results help to define guidelines on how to build effective automatic labeling techniques, and provide some insights on the actual usefulness of automatic labeling techniques during program comprehension tasks.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {1383–1420},
numpages = {38},
keywords = {Empirical studies, Information retrieval, Program comprehension, Software artifact labeling}
}

@article{10.1016/j.eswa.2008.10.027,
author = {Catal, Cagatay and Diri, Banu},
title = {A systematic review of software fault prediction studies},
year = {2009},
issue_date = {May, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.10.027},
doi = {10.1016/j.eswa.2008.10.027},
abstract = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
journal = {Expert Syst. Appl.},
month = may,
pages = {7346–7354},
numpages = {9},
keywords = {Automated fault prediction models, Expert systems, Machine learning, Method-level metrics, Public datasets}
}

@inproceedings{10.1145/1540438.1540466,
author = {Jiang, Yue and Cukic, Bojan},
title = {Misclassification cost-sensitive fault prediction models},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540466},
doi = {10.1145/1540438.1540466},
abstract = {Traditionally, software fault prediction models are built by assuming a uniform misclassification cost. In other words, cost implications of misclassifying a faulty module as fault free are assumed to be the same as the cost implications of misclassifying a fault free module as faulty. In reality, these two types of misclassification costs are rarely equal. They are project-specific, reflecting the characteristics of the domain in which the program operates. In this paper, using project information from a public repository, we analyze the benefits of techniques which incorporate misclassification costs in the development of software fault prediction models. We find that cost-sensitive learning does not provide operational points which outperform cost-insensitive classifiers. However, an advantage of cost-sensitive modeling is the explicit choice of the operational threshold appropriate for the cost differential.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {20},
numpages = {10},
keywords = {cost-sensitive, fault prediction, machine learning, misclassification cost},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@inproceedings{10.1145/2532443.2532461,
author = {Chen, Jiaqiang and Liu, Shulong and Chen, Xiang and Gu, Qing and Chen, Daoxu},
title = {Empirical studies on feature selection for software fault prediction},
year = {2013},
isbn = {9781450323697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532443.2532461},
doi = {10.1145/2532443.2532461},
abstract = {Classification based software fault prediction methods aim to classify the modules into either fault-prone or non-fault-prone. Feature selection is a preprocess step used to improve the data quality. However most of previous research mainly focus on feature relevance analysis, there is little work focusing on feature redundancy analysis. Therefore we propose a two-stage framework for feature selection to solve this issue. In particular, during the feature relevance phase, we adopt three different relevance measures to obtain the relevant feature subset. Then during the feature redundancy analysis phase, we use a cluster-based method to eliminate redundant features. To verify the effectiveness of our proposed framework, we choose typical real-world software projects, including Eclipse projects and NASA software project KC1. Final empirical result shows the effectiveness of our proposed framework.},
booktitle = {Proceedings of the 5th Asia-Pacific Symposium on Internetware},
articleno = {26},
numpages = {4},
keywords = {software fault prediction, relevance analysis, redundancy analysis, feature selection},
location = {Changsha, China},
series = {Internetware '13}
}

@inproceedings{10.1145/3419604.3419809,
author = {Miloudi, Chaymae and Cheikhi, Laila and Idri, Ali},
title = {A Review of Open Source Software Maintenance Effort Estimation},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419809},
doi = {10.1145/3419604.3419809},
abstract = {Open Source Software (OSS) is gaining interests of software engineering community as well as practitioners from industry with the growth of the internet. Studies in estimating maintenance effort (MEE) of such software product have been published in the literature in order to provide better estimation. The aim of this study is to provide a review of studies related to maintenance effort estimation for open source software (OSSMEE). To this end, a set of 60 primary empirical studies are selected from six electronic databases and a discussion is provided according to eight research questions (RQs) related to: publication year, publication source, datasets (OSS projects), metrics (independent variables), techniques, maintenance effort (dependent variable), validation methods, and accuracy criteria used in the empirical validation. This study has found that popular OSS projects have been used, Linear Regression, Na\"{\i}ve Bayes and k Nearest Neighbors were frequently used, and bug resolution was the most used regarding the estimation of maintenance effort for the future releases. A set of gaps are identified and recommendations for researchers are also provided.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {41},
numpages = {7},
keywords = {techniques, metrics, Review, Open source software, Maintenance effort estimation, Empirical, Datasets},
location = {Rabat, Morocco},
series = {SITA'20}
}

@article{10.1155/2019/8391425,
author = {Ren, Jiadong and Zheng, Zhangqi and Liu, Qian and Wei, Zhiyao and Yan, Huaizhi and Chen, Jiageng},
title = {A Buffer Overflow Prediction Approach Based on Software Metrics and Machine Learning},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1939-0114},
url = {https://doi.org/10.1155/2019/8391425},
doi = {10.1155/2019/8391425},
abstract = {Buffer overflow vulnerability is the most common and serious type of vulnerability in software today, as network security issues have become increasingly critical. To alleviate the security threat, many vulnerability mining methods based on static and dynamic analysis have been developed. However, the current analysis methods have problems regarding high computational time, low test efficiency, low accuracy, and low versatility. This paper proposed a software buffer overflow vulnerability prediction method by using software metrics and a decision tree algorithm. First, the software metrics were extracted from the software source code, and data from the dynamic data stream at the functional level was extracted by a data mining method. Second, a model based on a decision tree algorithm was constructed to measure multiple types of buffer overflow vulnerabilities at the functional level. Finally, the experimental results showed that our method ran in less time than SVM, Bayes, adaboost, and random forest algorithms and achieved 82.53% and 87.51% accuracy in two different data sets. The method presented in this paper achieved the effect of accurately predicting software buffer overflow vulnerabilities in C/C++ and Java programs.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/1414004.1414066,
author = {Tosun, Ayse and Turhan, Burak and Bener, Ayse},
title = {Ensemble of software defect predictors: a case study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414066},
doi = {10.1145/1414004.1414066},
abstract = {In this paper, we present a defect prediction model based on ensemble of classifiers, which has not been fully explored so far in this type of research. We have conducted several experiments on public datasets. Our results reveal that ensemble of classifiers considerably improve the defect detection capability compared to Naive Bayes algorithm. We also conduct a cost-benefit analysis for our ensemble, where it turns out that it is enough to inspect 32% of the code on the average, for detecting 76% of the defects.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {318–320},
numpages = {3},
keywords = {static code attributes, ensemble of classifiers, defect prediction},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@inproceedings{10.5555/3172077.3172102,
author = {Choi, Min-Je and Jeong, Sehun and Oh, Hakjoo and Choo, Jaegul},
title = {End-to-end prediction of buffer overruns from raw source code via neural memory networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Detecting buffer overruns from a source code is one of the most common and yet challenging tasks in program analysis. Current approaches based on rigid rules and handcrafted features are limited in terms of flexible applicability and robustness due to diverse bug patterns and characteristics existing in sophisticated real-world software programs. In this paper, we propose a novel, data-driven approach that is completely end-to-end without requiring any hand-crafted features, thus free from any program language-specific structural limitations. In particular, our approach leverages a recently proposed neural network model called memory networks that have shown the state-of-the-art performances mainly in question-answering tasks. Our experimental results using source code samples demonstrate that our proposed model is capable of accurately detecting different types of buffer overruns. We also present in-depth analyses on how a memory network can learn to understand the semantics in programming languages solely from raw source codes, such as tracing variables of interest, identifying numerical values, and performing their quantitative comparisons.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1546–1553},
numpages = {8},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3236024.3236082,
author = {Ma, Shiqing and Liu, Yingqi and Lee, Wen-Chuan and Zhang, Xiangyu and Grama, Ananth},
title = {MODE: automated neural network model debugging via state differential analysis and input selection},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236082},
doi = {10.1145/3236024.3236082},
abstract = {Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {175–186},
numpages = {12},
keywords = {Differential Analysis, Deep Neural Network, Debugging},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2950290.2950353,
author = {Yang, Yibiao and Zhou, Yuming and Liu, Jinping and Zhao, Yangyang and Lu, Hongmin and Xu, Lei and Xu, Baowen and Leung, Hareton},
title = {Effort-aware just-in-time defect prediction: simple unsupervised models could be better than supervised models},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950353},
doi = {10.1145/2950290.2950353},
abstract = {Unsupervised models do not require the defect data to build the prediction models and hence incur a low building cost and gain a wide application range. Consequently, it would be more desirable for practitioners to apply unsupervised models in effort-aware just-in-time (JIT) defect prediction if they can predict defect-inducing changes well. However, little is currently known on their prediction effectiveness in this context. We aim to investigate the predictive power of simple unsupervised models in effort-aware JIT defect prediction, especially compared with the state-of-the-art supervised models in the recent literature. We first use the most commonly used change metrics to build simple unsupervised models. Then, we compare these unsupervised models with the state-of-the-art supervised models under cross-validation, time-wise-cross-validation, and across-project prediction settings to determine whether they are of practical value. The experimental results, from open-source software systems, show that many simple unsupervised models perform better than the state-of-the-art supervised models in effort-aware JIT defect prediction.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {157–168},
numpages = {12},
keywords = {prediction, just-in-time, effort-aware, changes, Defect},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/1987875.1987888,
author = {Bi\c{c}er, Serdar and Bener, Ay\c{s}e Ba\c{s}ar and \c{C}a\u{g}layan, Bora},
title = {Defect prediction using social network analysis on issue repositories},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987888},
doi = {10.1145/1987875.1987888},
abstract = {People are the most important pillar of software development process. It is critical to understand how they interact with each other and how these interactions affect the quality of the end product in terms of defects. In this research we propose to include a new set of metrics, a.k.a. social network metrics on issue repositories in predicting defects. Social network metrics on issue repositories has not been used before to predict defect proneness of a software product. To validate our hypotheses we used two datasets, development data of IBM1 Rational ® Team Concert™ (RTC) and Drupal, to conduct our experiments. The results of the experiments revealed that compared to other set of metrics such as churn metrics using social network metrics on issue repositories either considerably decreases high false alarm rates without compromising the detection rates or considerably increases low prediction rates without compromising low false alarm rates. Therefore we recommend practitioners to collect social network metrics on issue repositories since people related information is a strong indicator of past patterns in a given team.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {63–71},
numpages = {9},
keywords = {social networks, network metrics, developer communication, defect prediction},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@article{10.1145/2579281.2579292,
author = {Bhasin, Harsh and Khanna, Esha},
title = {Neural network based black box testing},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579292},
doi = {10.1145/2579281.2579292},
abstract = {Black Box Testing is immensely important because the source code of a module is not always available. Enterprise Resource Planning systems are also tested using Black Box Testing wherein all the test cases are not equally important. The prioritization of these test cases would be helpful in case of premature termination of testing, due to lack of resources. This paper proposes a Neural Network based method to prioritize test cases. The paper also presents guidelines for prioritizing test cases. The technique has been tested using a financial management system and the results are encouraging. This paper paves way for applying Neural Network in Black Box Testing and presents a framework, which would help both researchers and practitioners.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6},
keywords = {test case design, prioritizing test cases, neural network, black box testing}
}

@inproceedings{10.1145/3236024.3236055,
author = {Hu, Gang and Zhu, Linjie and Yang, Junfeng},
title = {AppFlow: using machine learning to synthesize robust, reusable UI tests},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236055},
doi = {10.1145/3236024.3236055},
abstract = {UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–282},
numpages = {14},
keywords = {test synthesis, test reuse, mobile testing, machine learning, UI testing, UI recognition},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2222444.2222446,
author = {Abuasad, Arwa and Alsmadi, Izzat M.},
title = {The correlation between source code analysis change recommendations and software metrics},
year = {2012},
isbn = {9781450313278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2222444.2222446},
doi = {10.1145/2222444.2222446},
abstract = {Developing a software with high quality is a challenge that is continuously evolving. There are several methods to evaluate the quality in developed software products. In this paper, we evaluated the correlation between two software quality tools: Source Code Analysis (SCA) and software metrics. While both approaches are used to assess one or more attributes of the software, each one has its own goals and characteristics. In this paper, we evaluated the relation and correlation between SCA and software metrics. We used tools to fix warning detected by SCA tools. Software metrics are measured before and after the modifications of SCA recommended changes. Results showed that some specific metrics related to structure, complexity and maintainability metrics can be significantly impacted by SCA modifications. This is expected given the nature of proposed modifications from SCA tools.},
booktitle = {Proceedings of the 3rd International Conference on Information and Communication Systems},
articleno = {2},
numpages = {5},
keywords = {source code analysis, software mining, software faults, object oriented designs, design metrics, data mining, coupling and complexity metrics},
location = {Irbid, Jordan},
series = {ICICS '12}
}

@article{10.1504/ijaip.2019.101983,
author = {Kumar, Reddi Kiran and Rao, S.V. Achuta},
title = {Severity of defect: an optimised prediction},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {3–4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2019.101983},
doi = {10.1504/ijaip.2019.101983},
abstract = {To assure the quality of software an important activity is performed namely software defect prediction (SDP). Historical databases are used to detect software defects using different machine learning techniques. Conversely, there are disadvantages like testing becomes expensive, poor quality and so the product is unreliable for use. This paper classifies the severity of defects by using a method based on optimised neural network (NN). In full search space, a solution is found by many meta-heuristic optimisations and global search ability has been used. Hence, high-quality solutions are finding within a reasonable period of time. SDP performance is improved by the combination of meta-heuristic optimisation methods. For class imbalance problem, meta-heuristic optimisation methods such as genetic algorithm (GA) and shuffled frog leaping algorithm (SFLA) are applied. The above method is based on SFLA and the experimental outputs show that it can do better than Leven berg Marquardt based NN system (LM-NN).},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {334–345},
numpages = {11},
keywords = {software defect prediction, SDP, severity, neural network, Levenberg Marquardt, LM, shuffled frog and fuzzy classifier}
}

@inproceedings{10.1145/2393596.2393619,
author = {Caglayan, Bora and Misirli, Ayse Tosun and Calikli, Gul and Bener, Ayse and Aytac, Turgay and Turhan, Burak},
title = {Dione: an integrated measurement and defect prediction solution},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393619},
doi = {10.1145/2393596.2393619},
abstract = {We present an integrated measurement and defect prediction tool: Dione. Our tool enables organizations to measure, monitor, and control product quality through learning based defect prediction. Similar existing tools either provide data collection and analytics, or work just as a prediction engine. Therefore, companies need to deal with multiple tools with incompatible interfaces in order to deploy a complete measurement and prediction solution. Dione provides a fully integrated solution where data extraction, defect prediction and reporting steps fit seamlessly. In this paper, we present the major functionality and architectural elements of Dione followed by an overview of our demonstration.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {20},
numpages = {2},
keywords = {software tool, software defect prediction, measurement},
location = {Cary, North Carolina},
series = {FSE '12}
}

@article{10.1007/s10664-008-9103-7,
author = {Turhan, Burak and Menzies, Tim and Bener, Ay\c{s}e B. and Di Stefano, Justin},
title = {On the relative value of cross-company and within-company data for defect prediction},
year = {2009},
issue_date = {October   2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9103-7},
doi = {10.1007/s10664-008-9103-7},
abstract = {We propose a practical defect prediction approach for companies that do not track defect related data. Specifically, we investigate the applicability of cross-company (CC) data for building localized defect predictors using static code features. Firstly, we analyze the conditions, where CC data can be used as is. These conditions turn out to be quite few. Then we apply principles of analogy-based learning (i.e. nearest neighbor (NN) filtering) to CC data, in order to fine tune these models for localization. We compare the performance of these models with that of defect predictors learned from within-company (WC) data. As expected, we observe that defect predictors learned from WC data outperform the ones learned from CC data. However, our analyses also yield defect predictors learned from NN-filtered CC data, with performance close to, but still not better than, WC data. Therefore, we perform a final analysis for determining the minimum number of local defect reports in order to learn WC defect predictors. We demonstrate in this paper that the minimum number of data samples required to build effective defect predictors can be quite small and can be collected quickly within a few months. Hence, for companies with no local defect data, we recommend a two-phase approach that allows them to employ the defect prediction process instantaneously. In phase one, companies should use NN-filtered CC data to initiate the defect prediction process and simultaneously start collecting WC (local) data. Once enough WC data is collected (i.e. after a few months), organizations should switch to phase two and use predictors learned from WC data.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {540–578},
numpages = {39},
keywords = {Cross-company, Defect prediction, Learning, Metrics (product metrics), Nearest-neighbor filtering, Within-company}
}

@inproceedings{10.1145/1540438.1540453,
author = {Tosun, Ay\c{s}e and Turhan, Burak and Bener, Ay\c{s}e},
title = {Practical considerations in deploying AI for defect prediction: a case study within the Turkish telecommunication industry},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540453},
doi = {10.1145/1540438.1540453},
abstract = {We have conducted a study in a large telecommunication company in Turkey to employ a software measurement program and to predict pre-release defects. We have previously built such predictors using AI techniques. This project is a transfer of our research experience into a real life setting to solve a specific problem for the company: to improve code quality by predicting pre-release defects and efficiently allocating testing resources. Our results in this project have many practical implications that managers have started benefiting: code analysis, bug tracking, effective use of version management system and defect prediction. Using version history information, developers can find around 88% of the defects with 28% false alarms, compared to same detection rate with 50% false alarms without using historical data. In this paper we also shared in detail our experience in terms of the project steps (i.e. challenges and opportunities).},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {11},
numpages = {9},
keywords = {AI methods, experience report, prediction, software defect prediction, static code attributes},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@inproceedings{10.1145/3213846.3213858,
author = {Dwarakanath, Anurag and Ahuja, Manish and Sikand, Samarth and Rao, Raghotham M. and Bose, R. P. Jagadeesh Chandra and Dubash, Neville and Podder, Sanjay},
title = {Identifying implementation bugs in machine learning based image classifiers using metamorphic testing},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213858},
doi = {10.1145/3213846.3213858},
abstract = {We have recently witnessed tremendous success of Machine Learning (ML) in practical applications. Computer vision, speech recognition and language translation have all seen a near human level performance. We expect, in the near future, most business applications will have some form of ML. However, testing such applications is extremely challenging and would be very expensive if we follow today's methodologies. In this work, we present an articulation of the challenges in testing ML based applications. We then present our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based application. Empirical validation showed that our approach was able to catch 71% of the implementation bugs in the ML applications.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {118–128},
numpages = {11},
keywords = {Testing Machine Learning based applications, Metamorphic Testing},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1109/MSR.2019.00016,
author = {Hoang, Thong and Dam, Hoa Khanh and Kamei, Yasutaka and Lo, David and Ubayashi, Naoyasu},
title = {DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00016},
doi = {10.1109/MSR.2019.00016},
abstract = {Software quality assurance efforts often focus on identifying defective code. To find likely defective code early, change-level defect prediction - aka. Just-In-Time (JIT) defect prediction - has been proposed. JIT defect prediction models identify likely defective changes and they are trained using machine learning techniques with the assumption that historical changes are similar to future ones. Most existing JIT defect prediction approaches make use of manually engineered features. Unlike those approaches, in this paper, we propose an end-to-end deep learning framework, named DeepJIT, that automatically extracts features from commit messages and code changes and use them to identify defects. Experiments on two popular software projects (i.e., QT and OPENSTACK) on three evaluation settings (i.e., cross-validation, short-period, and long-period) show that the best variant of DeepJIT (DeepJIT-Combined), compared with the best performing state-of-the-art approach, achieves improvements of 10.36--11.02% for the project QT and 9.51--13.69% for the project OPENSTACK in terms of the Area Under the Curve (AUC).},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {34–45},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.5555/2010978.2010987,
author = {Halkidi, M. and Spinellis, D. and Tsatsaronis, G. and Vazirgiannis, M.},
title = {Data mining in software engineering},
year = {2011},
issue_date = {August 2011},
publisher = {IOS Press},
address = {NLD},
volume = {15},
number = {3},
issn = {1088-467X},
abstract = {The increased availability of data created as part of the software development process allows us to apply novel analysis techniques on the data and use the results to guide the process's optimization. In this paper we describe various data sources and discuss the principles and techniques of data mining as applied on software engineering data. Data that can be mined is generated by most parts of the development process: requirements elicitation, development analysis, testing, debugging, and maintenance. Based on this classification we survey the mining approaches that have been used and categorize them according to the corresponding parts of the development process and the task they assist. Thus the survey provides researchers with a concise overview of data mining techniques applied to software engineering data, and aids practitioners on the selection of appropriate data mining techniques for their work.},
journal = {Intell. Data Anal.},
month = aug,
pages = {413–441},
numpages = {29},
keywords = {Data mining techniques, KDD methods, mining software engineering data}
}

@inproceedings{10.1145/3234698.3234714,
author = {Benaddy, Mohamed and El Habil, Brahim and El Meslouhi, Othmane and Krit, Salah-Ddine},
title = {Recurrent neural network for software failure prediction},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234714},
doi = {10.1145/3234698.3234714},
abstract = {Software failure occurs when the software runs in an operational profile. Controlling failures in software require that one can predict problems early enough to take preventive action. The prediction of software failures is done by using the historical failures collected previously when they occur. To predict software failures, several models are proposed by researchers. In this paper, we present a recurrent neural network (RNN) to predict software failure using historical failure data. The proposed RNN is trained and tested using collected data from the literature; the obtained results are compared with other models and show that our proposed model gives very attractive prediction rates.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {16},
numpages = {8},
keywords = {Software Reliability, Software Failure Prediction, Recurrent Neural Networks, Neural Networks},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1145/2365324.2365335,
author = {Lu, Huihua and Cukic, Bojan},
title = {An adaptive approach with active learning in software fault prediction},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365335},
doi = {10.1145/2365324.2365335},
abstract = {Background: Software quality prediction plays an important role in improving the quality of software systems. By mining software metrics, predictive models can be induced that provide software managers with insights into quality problems they need to tackle as effectively as possible.Objective: Traditional, supervised learning approaches dominate software quality prediction. Resulting models tend to be project specific. On the other hand, in situations where there are no previous releases, supervised learning approaches are not very useful because large training data sets are needed to develop accurate predictive models.Method: This paper eases the limitations of supervised learning approaches and offers good prediction performance. We propose an adaptive approach in which supervised learning and active learning are coupled together. NaiveBayes classifier is used as the base learner.Results: We track the performance at each iteration of the adaptive learning algorithm and compare it with the performance of supervised learning. Our results show that proposed scheme provides good fault prediction performance over time, i.e., it eventually outperforms the corresponding supervised learning approach. On the other hand, adaptive learning classification approach reduces the variance in prediction performance in comparison with the corresponding supervised learning algorithm.Conclusion: The adaptive approach outperforms the corresponding supervised learning approach when both use Naive-Bayes as base learner. Additional research is needed to investigate whether this observation remains valid with other base classifiers.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {79–88},
numpages = {10},
keywords = {active learning, adaptive learning, software fault prediction},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/1370788.1370793,
author = {Jiang, Yue and Cuki, Bojan and Menzies, Tim and Bartlow, Nick},
title = {Comparing design and code metrics for software quality prediction},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370793},
doi = {10.1145/1370788.1370793},
abstract = {The prediction of fault-prone modules continues to attract interest due to the significant impact it has on software quality assurance. One of the most important goals of such techniques is to accurately predict the modules where faults are likely to hide as early as possible in the development lifecycle. Design, code, and most recently, requirements metrics have been successfully used for predicting fault-prone modules. The goal of this paper is to compare the performance of predictive models which use design-level metrics with those that use code-level metrics and those that use both. We analyze thirteen datasets from NASA Metrics Data Program which offer design as well as code metrics. Using a range of modeling techniques and statistical significance tests, we confirmed that models built from code metrics typically outperform design metrics based models. However, both types of models prove to be useful as they can be constructed in different project phases. Code-based models can be used to increase the performance of design-level models and, thus, increase the efficiency of assigning verification and validation activities late in the development lifecycle. We also conclude that models that utilize a combination of design and code level metrics outperform models which use either one or the other metric set.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {code metrics, design metrics, fault-proneness prediction, machine learning},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@inproceedings{10.5555/3172077.3172153,
author = {Huo, Xuan and Li, Ming},
title = {Enhancing the unified features to locate buggy files by exploiting the sequential nature of source code},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Bug reports provide an effective way for end-users to disclose potential bugs hidden in a software system, while automatically locating the potential buggy source files according to a bug report remains a great challenge in software maintenance. Many previous approaches represent bug reports and source code from lexical and structural information correlated their relevance by measuring their similarity, and recently a CNN-based model is proposed to learn the unified features for bug localization, which overcomes the difficulty in modeling natural and programming languages with different structural semantics. However, previous studies fail to capture the sequential nature of source code, which carries additional semantics beyond the lexical and structural terms and such information is vital in modeling program functionalities and behaviors. In this paper, we propose a novel model LS-CNN, which enhances the unified features by exploiting the sequential nature of source code. LS-CNN combines CNN and LSTM to extract semantic features for automatically identifying potential buggy source code according to a bug report. Experimental results on widely-used software projects indicate that LS-CNN significantly outperforms the state-of-the-art methods in locating buggy files.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1909–1915},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.5555/2486788.2486840,
author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
title = {It's not a bug, it's a feature: how misclassification impacts bug prediction},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8% of all bug reports to be misclassified---that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {392–401},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/1137983.1138012,
author = {Knab, Patrick and Pinzger, Martin and Bernstein, Abraham},
title = {Predicting defect densities in source code files with decision tree learners},
year = {2006},
isbn = {1595933972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137983.1138012},
doi = {10.1145/1137983.1138012},
abstract = {With the advent of open source software repositories the data available for defect prediction in source files increased tremendously. Although traditional statistics turned out to derive reasonable results the sheer amount of data and the problem context of defect prediction demand sophisticated analysis such as provided by current data mining and machine learning techniques.In this work we focus on defect density prediction and present an approach that applies a decision tree learner on evolution data extracted from the Mozilla open source web browser project. The evolution data includes different source code, modification, and defect measures computed from seven recent Mozilla releases. Among the modification measures we also take into account the change coupling, a measure for the number of change-dependencies between source files. The main reason for choosing decision tree learners, instead of for example neural nets, was the goal of finding underlying rules which can be easily interpreted by humans. To find these rules, we set up a number of experiments to test common hypotheses regarding defects in software entities. Our experiments showed, that a simple tree learner can produce good results with various sets of input data.},
booktitle = {Proceedings of the 2006 International Workshop on Mining Software Repositories},
pages = {119–125},
numpages = {7},
keywords = {defect prediction, decision tree learner, data mining},
location = {Shanghai, China},
series = {MSR '06}
}

@inproceedings{10.5555/1888258.1888293,
author = {Eichinger, Frank and Krogmann, Klaus and Klug, Roland and B\"{o}hm, Klemens},
title = {Software-defect localisation by mining dataflow-enabled call graphs},
year = {2010},
isbn = {364215879X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Defect localisation is essential in software engineering and is an important task in domain-specific data mining. Existing techniques building on call-graph mining can localise different kinds of defects. However, these techniques focus on defects that affect the controlflow and are agnostic regarding the dataflow. In this paper, we introduce dataflowenabled call graphs that incorporate abstractions of the dataflow. Building on these graphs, we present an approach for defect localisation. The creation of the graphs and the defect localisation are essentially data mining problems, making use of discretisation, frequent subgraph mining and feature selection. We demonstrate the defect-localisation qualities of our approach with a study on defects introduced into Weka. As a result, defect localisation now works much better, and a developer has to investigate on average only 1.5 out of 30 methods to fix a defect.},
booktitle = {Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part I},
pages = {425–441},
numpages = {17},
location = {Barcelona, Spain},
series = {ECML PKDD'10}
}

@article{10.1016/j.cl.2016.06.001,
author = {Milewicz, Reed and Pirkelbauer, Peter},
title = {Refinement of structural heuristics for model checking of concurrent programs through data mining},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {P2},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.06.001},
doi = {10.1016/j.cl.2016.06.001},
abstract = {Detecting concurrency bugs in multi-threaded programs through model-checking is complicated by the combinatorial explosion in the number of ways that different threads can be interleaved to produce different combinations of behaviors. At the same time, concurrency bugs tend to be limited in their scope and scale due to the way in which concurrent programs are designed, and making visible the rules that govern the relationships between threads can help us to better identify which interleavings are worth investigating. In this work, patterns of read-write sequences are mined from a single execution of the target program to produce a quantitative, categorical model of thread behaviors. This model is exploited by a novel structural heuristic. Experiments with a proof-of-concept implementation, built using Java Pathfinder and WEKA, demonstrate that this heuristic locates bugs faster and more reliably than a conventional counterpart. HighlightsThe paper explores how model checking can be hybridized with trace analysis.Traces are mined to produce a categorical model of thread behavior.A heuristic for race detection is retrofitted to exploit the behavior model.With proper parameterization, the novel heuristic finds bugs 8 faster.A novel mutation-based benchmark generation process is introduced for validation.},
journal = {Comput. Lang. Syst. Struct.},
month = jan,
pages = {170–188},
numpages = {19},
keywords = {Data mining, Model checking, Structural heuristics}
}

@inproceedings{10.1145/3330204.3330275,
author = {Luiz, Frederico Caram and de Oliveira Rodrigues, Bruno Rafael and Parreiras, Fernando Silva},
title = {Machine learning techniques for code smells detection: an empirical experiment on a highly imbalanced setup},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330275},
doi = {10.1145/3330204.3330275},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {65},
numpages = {8},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/2979779.2979811,
author = {Kaur, Ishleen and Bajpai, Neha},
title = {An Empirical Study on Fault Prediction using Token-Based Approach},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979811},
doi = {10.1145/2979779.2979811},
abstract = {Since exhaustive testing is not possible, prediction of fault prone modules can be used for prioritizing the components of a software system. Various approaches have been proposed for the prediction of fault prone modules. Most of them uses module metrics as quality estimators. In this study, we proposed a tokenbased approach and combine the metric evaluated from our approach with the module metrics to further improve the prediction results. We conducted the experiment on an open source project for evaluating the approach. The proposed approach is further compared with the existing fault prone filtering technique. The results show that the proposed approach is an improvement over fault prone filtering technique.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {32},
numpages = {7},
keywords = {software testing, software metrics, logistic regression, fault prone modules, fault, Classification},
location = {Bikaner, India},
series = {AICTC '16}
}

@article{10.1007/s10664-012-9230-z,
author = {Linares-V\'{a}squez, Mario and Mcmillan, Collin and Poshyvanyk, Denys and Grechanik, Mark},
title = {On using machine learning to automatically classify software applications into domain categories},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9230-z},
doi = {10.1007/s10664-012-9230-z},
abstract = {Software repositories hold applications that are often categorized to improve the effectiveness of various maintenance tasks. Properly categorized applications allow stakeholders to identify requirements related to their applications and predict maintenance problems in software projects. Manual categorization is expensive, tedious, and laborious --- this is why automatic categorization approaches are gaining widespread importance. Unfortunately, for different legal and organizational reasons, the applications' source code is often not available, thus making it difficult to automatically categorize these applications. In this paper, we propose a novel approach in which we use Application Programming Interface (API) calls from third-party libraries for automatic categorization of software applications that use these API calls. Our approach is general since it enables different categorization algorithms to be applied to repositories that contain both source code and bytecode of applications, since API calls can be extracted from both the source code and byte-code. We compare our approach to a state-of-the-art approach that uses machine learning algorithms for software categorization, and conduct experiments on two large Java repositories: an open-source repository containing 3,286 projects and a closed-source repository with 745 applications, where the source code was not available. Our contribution is twofold: we propose a new approach that makes it possible to categorize software projects without any source code using a small number of API calls as attributes, and furthermore we carried out a comprehensive empirical evaluation of automatic categorization approaches.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {582–618},
numpages = {37},
keywords = {Closed-source, Machine learning, Open-source, Software categorization}
}

@article{10.1007/s10489-009-0193-8,
author = {Hewett, Rattikorn},
title = {Mining software defect data to support software testing management},
year = {2011},
issue_date = {April     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {2},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-009-0193-8},
doi = {10.1007/s10489-009-0193-8},
abstract = {Achieving high quality software would be easier if effective software development practices were known and deployed in appropriate contexts. Because our theoretical knowledge of the underlying principles of software development is far from complete, empirical analysis of past experience in software projects is essential for acquiring useful software practices. As advances in software technology continue to facilitate automated tracking and data collection, more software data become available. Our research aims to develop methods to exploit such data for improving software development practices.This paper proposes an empirical approach, based on the analysis of defect data, that provides support for software testing management in two ways: (1) construction of a predictive model for defect repair times, and (2) a method for assessing testing quality across multiple releases. The approach employs data mining techniques including statistical methods and machine learning. To illustrate the proposed approach, we present a case study using the defect reports created during the development of three releases of a large medical software system, produced by a large well-established software company. We validate our proposed testing quality assessment using a statistical test at a significance level of 0.1. Despite the limitations of the available data, our predictive models give accuracies as high as 93%.},
journal = {Applied Intelligence},
month = apr,
pages = {245–257},
numpages = {13},
keywords = {Data mining, Defect report, Quality assurance, Software testing management}
}

@inproceedings{10.1145/1868328.1868357,
author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
title = {Programmer-based fault prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868357},
doi = {10.1145/1868328.1868357},
abstract = {Background: Previous research has provided evidence that a combination of static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions.Aims: We investigate whether files in a large system that are modified by an individual developer consistently contain either more or fewer faults than the average of all files in the system. The goal of the investigation is to determine whether information about which particular developer modified a file is able to improve defect predictions. We also continue an earlier study to evaluate the use of counts of the number of developers who modified a file as predictors of the file's future faultiness.Method: We analyzed change reports filed by 107 programmers for 16 releases of a system with 1,400,000 LOC and 3100 files. A "bug ratio" was defined for programmers, measuring the proportion of faulty files in release R out of all files modified by the programmer in release R-1. The study compares the bug ratios of individual programmers to the average bug ratio, and also assesses the consistency of the bug ratio across releases for individual programmers.Results: Bug ratios varied widely among all the programmers, as well as for many individual programmers across all the releases that they participated in. We found a statistically significant correlation between the bug ratios for programmers for the first half of changed files versus the ratios for the second half, indicating a measurable degree of persistence in the bug ratio. However, when the computation was repeated with the bug ratio controlled not only by release, but also by file size, the correlation disappeared. In addition to the bug ratios, we confirmed that counts of the cumulative number of different developers changing a file over its lifetime can help to improve predictions, while other developer counts are not helpful.Conclusions: The results from this preliminary study indicate that adding information to a model about which particular developer modified a file is not likely to improve defect predictions. The study is limited to a single large system, and its results may not hold more widely. The bug ratio is only one way of measuring the "fault-proneness" of an individual programmer's coding, and we intend to investigate other ways of evaluating bug introduction by individuals.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {19},
numpages = {10},
keywords = {bug ratio, empirical study, fault-prone, prediction, regression model, software faults},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.1016/j.jss.2016.06.006,
author = {Okutan, Ahmet and Taner Yildiz, Olcay},
title = {A novel kernel to predict software defectiveness},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.006},
doi = {10.1016/j.jss.2016.06.006},
abstract = {We propose new kernels for defect prediction that are based on the source code similarity.We model the relationship between source code similarity and defectiveness.The precomputed kernels are used with SVM and KNN classifiers.The proposed technique performs better than the SVM with linear kernel.It also achieves comparable performance when compared to the KNN classifier. Although the software defect prediction problem has been researched for a long time, the results achieved are not so bright. In this paper, we propose to use novel kernels for defect prediction that are based on the plagiarized source code, software clones and textual similarity. We generate precomputed kernel matrices and compare their performance on different data sets to model the relationship between source code similarity and defectiveness. Each value in a kernel matrix shows how much parallelism exists between the corresponding files of a software system chosen. Our experiments on 10 real world datasets indicate that support vector machines (SVM) with a precomputed kernel matrix performs better than the SVM with the usual linear kernel in terms of F-measure. Similarly, when used with a precomputed kernel, the k-nearest neighbor classifier (KNN) achieves comparable performance with respect to KNN classifier. The results from this preliminary study indicate that source code similarity can be used to predict defect proneness.},
journal = {J. Syst. Softw.},
month = sep,
pages = {109–121},
numpages = {13},
keywords = {Defect prediction, Kernel methods, SVM}
}

@article{10.1145/3453478,
author = {Dilhara, Malinda and Ketkar, Ameya and Dig, Danny},
title = {Understanding Software-2.0: A Study of Machine Learning Library Usage and Evolution},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3453478},
doi = {10.1145/3453478},
abstract = {Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models, i.e., Software-2.0, has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries.We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings.Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries, (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {55},
numpages = {42},
keywords = {empirical studies, Software-2.0, Machine learning libraries}
}

@inproceedings{10.1145/2723742.2723759,
author = {Shobe, Joseph F. and Karim, Md Yasser and Kagdi, Huzefa},
title = {How Often does a Source Code Unit Change within a Release Window?},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723759},
doi = {10.1145/2723742.2723759},
abstract = {To form a training set for a source-code change prediction model, e.g., using the association rule mining or machine learning techniques, commits from the source code history are needed. The traceability between releases and commits would facilitate a systematic choice of history in units of the project evolution scale (i.e., commits that constitute a software release). For example, the major release 25.0 in Chrome is mapped to the earliest revision 157687 and latest revision 165096 in the trunk. Using this traceability, an empirical study is reported on the frequency distribution of file changes for different release windows. In Chrome, the majority (50%) of the committed files change only once between a pair of consecutive releases. This trend is reversed after expanding the window size to at least 10. That is, the majority (50%) of the files change multiple times when commits constituting 10 or greater releases are considered. These results suggest that a training set of at least 10 releases is needed to provide a prediction coverage for majority of the files.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {166–175},
numpages = {10},
keywords = {Software Releases, Mining Software Repositories, Empirical Studies, Commit History},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/1831708.1831743,
author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {Software fault prediction tool},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831743},
doi = {10.1145/1831708.1831743},
abstract = {We have developed an interactive tool that predicts fault likelihood for the individual files of successive releases of large, long-lived, multi-developer software systems. Predictions are the result of a two-stage process: first, the extraction of current and historical properties of the system, and second, application of a negative binomial regression model to the extracted data. The prediction model is presented to the user as a GUI-based tool that requires minimal input from the user, and delivers its output as an ordered list of the system's files together with an expected percent of faults each file will have in the release about to undergo system test. The predictions can be used to prioritize testing efforts, to plan code or design reviews, to allocate human and computer resources, and to decide if files should be rewritten.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {275–278},
numpages = {4},
keywords = {prediction, negative binomial, gui tool, fault, defect},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1016/j.ins.2018.05.035,
author = {Siers, Michael J. and Islam, Md Zahidul},
title = {Novel algorithms for cost-sensitive classification and knowledge discovery in class imbalanced datasets with an application to NASA software defects},
year = {2018},
issue_date = {Aug 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {459},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.05.035},
doi = {10.1016/j.ins.2018.05.035},
journal = {Inf. Sci.},
month = aug,
pages = {53–70},
numpages = {18},
keywords = {Software defect prediction, Class imbalance, Cost-sensitive, Decision forest, Knowledge discovery}
}

@inproceedings{10.1145/1868328.1868351,
author = {Hong, Youngki and Kim, Wondae and Joo, Jeongsoo},
title = {Prediction of defect distribution based on project characteristics for proactive project management},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868351},
doi = {10.1145/1868328.1868351},
abstract = {As software has been pervasive and various software projects have been executed since the 1970's, software project management has played a significant role in software industry. There are three major factors in project management; schedule, effort and quality. Especially, to represent quality of products, there are various possible quality characteristics of software, but in practice, frequently, quality management revolves around defects, and delivered defect density has become the current de facto industry standard. The researches related to software quality have been focused on modeling residual defects in software in order to estimate software reliability.However, only the predicted number of defects cannot be sufficient information to provide basis for planning quality assurance activities and assessing them during execution. That is, in order to let projects managers be able to identify the project related information in early phase, we need to predict other possible information for assuring software quality such as defect density by phases, defect types and so on. In this paper, we propose a new approach for predicting distribution of in-process defects, their types based on project characteristics in early phase. For this approach, the model for prediction is established using the curve fitting method and the regression analysis. The maximum likelihood estimation is used in fitting the Weibull probability density function to the actual defect data, and the regression analysis is used to identify the relationship between the project characteristics and the Weibull parameters. The research model is validated by using cross-validation technique.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {15},
numpages = {7},
keywords = {Weibull distribution function, defect distribution, in-process defect prediction, maximum likelihood estimation, project management, software reliability},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.1007/s10664-010-9151-7,
author = {Kpodjedo, Segla and Ricca, Filippo and Galinier, Philippe and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
title = {Design evolution metrics for defect prediction in object oriented systems},
year = {2011},
issue_date = {February  2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-010-9151-7},
doi = {10.1007/s10664-010-9151-7},
abstract = {Testing is the most widely adopted practice to ensure software quality. However, this activity is often a compromise between the available resources and software quality. In object-oriented development, testing effort should be focused on defective classes. Unfortunately, identifying those classes is a challenging and difficult activity on which many metrics, techniques, and models have been tried. In this paper, we investigate the usefulness of elementary design evolution metrics to identify defective classes. The metrics include the numbers of added, deleted, and modified attributes, methods, and relations. The metrics are used to recommend a ranked list of classes likely to contain defects for a system. They are compared to Chidamber and Kemerer's metrics on several versions of Rhino and of ArgoUML. Further comparison is conducted with the complexity metrics computed by Zimmermann et al. on several releases of Eclipse. The comparisons are made according to three criteria: presence of defects, number of defects, and defect density in the top-ranked classes. They show that the design evolution metrics, when used in conjunction with known metrics, improve the identification of defective classes. In addition, they show that the design evolution metrics make significantly better predictions of defect density than other metrics and, thus, can help in reducing the testing effort by focusing test activity on a reduced volume of code.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {141–175},
numpages = {35},
keywords = {Defect prediction, Design evolution metrics, Error tolerant graph matching}
}

@article{10.1007/s11334-017-0295-0,
author = {Shatnawi, Raed},
title = {The application of ROC analysis in threshold identification, data imbalance and metrics selection for software fault prediction},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {2–3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-017-0295-0},
doi = {10.1007/s11334-017-0295-0},
abstract = {Software engineers have limited resources and need metrics analysis tools to investigate software quality such as fault-proneness of modules. There are a large number of software metrics available to investigate quality. However, not all metrics are strongly correlated with faults. In addition, software fault data are imbalanced and affect quality assessment tools such as fault prediction or threshold values that are used to identify risky modules. Software quality is investigated for three purposes. First, the receiver operating characteristics (ROC) analysis is used to identify threshold values to identify risky modules. Second, the ROC analysis is investigated for imbalanced data. Third, the ROC analysis is considered for feature selection. This work validated the use of ROC to identify thresholds for four metrics (WMC, CBO, RFC and LCOM). The ROC results after sampling the data are not significantly different from before sampling. The ROC analysis selects the same metrics (WMC, CBO and RFC) in most datasets, while other techniques have a large variation in selecting metrics.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {201–217},
numpages = {17},
keywords = {Fault-proneness models, Feature selection, Imbalanced data, ROC analysis, Software metrics}
}

@article{10.1016/j.jss.2007.07.034,
author = {Vandecruys, Olivier and Martens, David and Baesens, Bart and Mues, Christophe and De Backer, Manu and Haesen, Raf},
title = {Mining software repositories for comprehensible software fault prediction models},
year = {2008},
issue_date = {May, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.07.034},
doi = {10.1016/j.jss.2007.07.034},
abstract = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.},
journal = {J. Syst. Softw.},
month = may,
pages = {823–839},
numpages = {17},
keywords = {Ant Colony Optimization, Classification, Comprehensibility, Fault prediction, Software mining}
}

@article{10.1016/j.infsof.2009.10.003,
author = {Hassouna, Alaa and Tahvildari, Ladan},
title = {An effort prediction framework for software defect correction},
year = {2010},
issue_date = {February, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.10.003},
doi = {10.1016/j.infsof.2009.10.003},
abstract = {This article tackles the problem of predicting effort (in person-hours) required to fix a software defect posted on an Issue Tracking System. The proposed method is inspired by the Nearest Neighbour Approach presented by the pioneering work of Weiss et al. (2007) [1]. We propose four enhancements to Weiss et al. (2007) [1]: Data Enrichment, Majority Voting, Adaptive Threshold and Binary Clustering. Data Enrichment infuses additional issue information into the similarity-scoring procedure, aiming to increase the accuracy of similarity scores. Majority Voting exploits the fact that many of the similar historical issues have repeating effort values, which are close to the actual. Adaptive Threshold automatically adjusts the similarity threshold to ensure that we obtain only the most similar matches. We use Binary Clustering if the similarity scores are very low, which might result in misleading predictions. This uses common properties of issues to form clusters (independent of the similarity scores) which are then used to produce the predictions. Numerical results are presented showing a noticeable improvement over the method proposed in Weiss et al. (2007) [1].},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {197–209},
numpages = {13},
keywords = {Software effort prediction, Software defect correction, Issue tracking system, Clustering, Case-based reasoning}
}

@inproceedings{10.1145/3266237.3266271,
author = {Ara\'{u}jo, Cristiano Werner and Zapalowski, Vanius and Nunes, Ingrid},
title = {Using code quality features to predict bugs in procedural software systems},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266271},
doi = {10.1145/3266237.3266271},
abstract = {A wide range of metrics have been used as features to build bug (or fault) predictors. However, most of the existing predictors focus mostly on object-oriented (OO) systems, either because they rely on OO metrics or were evaluated mainly with OO systems. Procedural software systems (PSS), less addressed in bug prediction research, often suffer from maintainability problems because they typically consist of low-level applications, using for example preprocessors to cope with variability. Previous work evaluated sets of features (composed of static code metrics) proposed in existing approaches in the PSS context. However, explored metrics are limited to those that are part of traditional metric suites, being often associated with structural code properties. A type of information explored to a smaller extent in this context is the output of code quality tools that statically analyse source code, providing hints of code problems. In this paper, we investigate the use of information collected from quality tools to build bug predictors dedicated to PSS. We specify four features derived from code quality tools or associated with poor programming practices and evaluate the effectiveness of these features. Our evaluation shows that our proposed features improve bug predictors in our investigated context.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {122–131},
numpages = {10},
keywords = {procedural languanges, code metrics, bug prediction},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/2245276.2231967,
author = {Sarro, F. and Di Martino, S. and Ferrucci, F. and Gravino, C.},
title = {A further analysis on the use of Genetic Algorithm to configure Support Vector Machines for inter-release fault prediction},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231967},
doi = {10.1145/2245276.2231967},
abstract = {Some studies have reported promising results on the use of Support Vector Machines (SVMs) for predicting fault-prone software components. Nevertheless, the performance of the method heavily depends on the setting of some parameters. To address this issue, we investigated the use of a Genetic Algorithm (GA) to search for a suitable configuration of SVMs to be used for inter-release fault prediction. In particular, we report on an assessment of the method on five software systems. As benchmarks we exploited SVMs with random and Grid-search configuration strategies and several other machine learning techniques. The results show that the combined use of GA and SVMs is effective for inter-release fault prediction.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1215–1220},
numpages = {6},
keywords = {support vector machines, genetic algorithm, fault prediction},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/3373477.3373486,
author = {Aggarwal, Simran},
title = {Software code analysis using ensemble learning techniques},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373486},
doi = {10.1145/3373477.3373486},
abstract = {Ensuing the advent of advancements in software systems, the probability of them containing high severity defects is exponentially on the rise. With each technological addition, the complexity of software is increasing. Reproduction and rectification of a defect requires time and effort. Current state of the art analysis tools cater to the investigation of static aspects of a production level code. However, it is imperative to assess the dynamic development process of a system so as to be able to timely detect erroneous components early on in the development life cycle of a software. A novel automated defect prediction feature enhancement is proposed that analyses the static structure of the current code and state of the software in past releases to extract relevant static and dynamic feature sets. Data generated is modelled for defect trends in the future release of the software by four ensemble classifiers. Results demonstrate the superiority of Voting algorithm for the problem of defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Advanced Information Science and System},
articleno = {9},
numpages = {7},
keywords = {software quality, object-oriented metrics, machine learning, ensemble learning, empirical validation, defect prediction},
location = {Singapore, Singapore},
series = {AISS '19}
}

@article{10.1016/j.eswa.2007.02.012,
author = {Bibi, S. and Tsoumakas, G. and Stamelos, I. and Vlahavas, I.},
title = {Regression via Classification applied on software defect estimation},
year = {2008},
issue_date = {April, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.02.012},
doi = {10.1016/j.eswa.2007.02.012},
abstract = {In this paper we apply Regression via Classification (RvC) to the problem of estimating the number of software defects. This approach apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies with a certain confidence. RvC also allows the production of comprehensible models of software defects exploiting symbolic learning algorithms. To evaluate this approach we perform an extensive comparative experimental study of the effectiveness of several machine learning algorithms in two software data sets. RvC manages to get better regression error than the standard regression approaches on both datasets.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2091–2101},
numpages = {11},
keywords = {Software quality, Software metrics, Software fault estimation, Regression via Classification, Machine learning, ISBSG data set}
}

@article{10.1504/IJDATS.2010.034058,
author = {Taylor, Quinn and Giraud-Carrier, Christophe},
title = {Applications of data mining in software engineering},
year = {2010},
issue_date = {July 2010},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {3},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2010.034058},
doi = {10.1504/IJDATS.2010.034058},
abstract = {Software engineering processes are complex, and the related activities often produce a large number and variety of artefacts, making them well-suited to data mining. Recent years have seen an increase in the use of data mining techniques on such artefacts with the goal of analysing and improving software processes for a given organisation or project. After a brief survey of current uses, we offer insight into how data mining can make a significant contribution to the success of current software engineering efforts.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = jul,
pages = {243–257},
numpages = {15},
keywords = {software engineering, data mining, applications}
}

@article{10.1007/s10664-008-9079-3,
author = {Jiang, Yue and Cukic, Bojan and Ma, Yan},
title = {Techniques for evaluating fault prediction models},
year = {2008},
issue_date = {October   2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9079-3},
doi = {10.1007/s10664-008-9079-3},
abstract = {Many statistical techniques have been proposed to predict fault-proneness of program modules in software engineering. Choosing the "best" candidate among many available models involves performance assessment and detailed comparison, but these comparisons are not simple due to the applicability of varying performance measures. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Methodologies for precise evaluation of fault prediction models should be at the core of empirical software engineering research, but have attracted sporadic attention. In this paper, we overview model evaluation techniques. In addition to many techniques that have been used in software engineering studies before, we introduce and discuss the merits of cost curves. Using the data from a public repository, our study demonstrates the strengths and weaknesses of performance evaluation techniques and points to a conclusion that the selection of the "best" model cannot be made without considering project cost characteristics, which are specific in each development environment.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {561–595},
numpages = {35},
keywords = {Predictive models in software engineering, Model evaluation, Fault-prediction models, Empirical studies}
}

@inproceedings{10.1145/3416505.3423563,
author = {Palma, Stefano Dalla and Mohammadi, Majid and Di Nucci, Dario and Tamburri, Damian A.},
title = {Singling the odd ones out: a novelty detection approach to find defects in infrastructure-as-code},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423563},
doi = {10.1145/3416505.3423563},
abstract = {Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Novelty Detection, Infrastructure-as-Code, Defect Prediction},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/2856636.2856637,
author = {Lal, Sangeeta and Sureka, Ashish},
title = {LogOpt: Static Feature Extraction from Source Code for Automated Catch Block Logging Prediction},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2856637},
doi = {10.1145/2856636.2856637},
abstract = {Software logging is an important software development practice which is used to trace important software execution points. This execution information can provide important insight to developer while software debugging. Inspite of many benefits logging is often done in an ad-hoc manner based only on knowledge and experience of software developer because of lack of formal guidelines and training required for making strategic logging decision. It is known that appropriate logging is beneficial for developers but inappropriate logging can have adverse effect on the system. Excessive logging can not only cause performance and cost overhead, it can also lessen the benefit of logging by producing tons of useless logs. Sparse logging can make logging ineffective by leaving out important information. In order to lessen the load of software developers and to improve the quality of software logging, in this work we propose 'LogOpt' tool to help developers in making informed logging decision. LogOpt uses static features from source code to make catch block logging decision. LogOpt is a machine learning based framework which learns the characteristics of logged and unlogged training instance to make informed logging decision. We manually analyze snippets of logged and unlogged source code and extracted 46 distinguishing features important in making logging decision. We evaluated LogOpt on two large open source projects Apache Tomcat and CloudStack (nearly 1.41M LOC). Results show that LogOpt is effective for automated logging task.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {151–155},
numpages = {5},
keywords = {Tracing, Source Code Analysis, Machine Learning, Logging, Debugging},
location = {Goa, India},
series = {ISEC '16}
}

@article{10.1016/j.eswa.2010.08.022,
author = {Catal, Cagatay and Sevim, Ugur and Diri, Banu},
title = {Practical development of an Eclipse-based software fault prediction tool using Naive Bayes algorithm},
year = {2011},
issue_date = {March, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.08.022},
doi = {10.1016/j.eswa.2010.08.022},
abstract = {Despite the amount of effort software engineers have been putting into developing fault prediction models, software fault prediction still poses great challenges. This research using machine learning and statistical techniques has been ongoing for 15years, and yet we still have not had a breakthrough. Unfortunately, none of these prediction models have achieved widespread applicability in the software industry due to a lack of software tools to automate this prediction process. Historical project data, including software faults and a robust software fault prediction tool, can enable quality managers to focus on fault-prone modules. Thus, they can improve the testing process. We developed an Eclipse-based software fault prediction tool for Java programs to simplify the fault prediction process. We also integrated a machine learning algorithm called Naive Bayes into the plug-in because of its proven high-performance for this problem. This article presents a practical view to software fault prediction problem, and it shows how we managed to combine software metrics with software fault data to apply Naive Bayes technique inside an open source platform.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {2347–2353},
numpages = {7},
keywords = {Software fault prediction, Naive Bayes, Machine learning, Eclipse technology}
}

@inproceedings{10.1109/FOSE.2007.27,
author = {Binkley, David},
title = {Source Code Analysis: A Road Map},
year = {2007},
isbn = {0769528295},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOSE.2007.27},
doi = {10.1109/FOSE.2007.27},
abstract = {The automated and semi-automated analysis of source code has remained a topic of intense research for more than thirty years. During this period, algorithms and techniques for source-code analysis have changed, sometimes dramatically. The abilities of the tools that implement them have also expanded to meet new and diverse challenges. This paper surveys current work on source-code analysis. It also provides a road map for future work over the next five-year period and speculates on the development of source-code analysis applications, techniques, and challenges over the next 10, 20, and 50 years.},
booktitle = {2007 Future of Software Engineering},
pages = {104–119},
numpages = {16},
series = {FOSE '07}
}

@inproceedings{10.1145/1083165.1083173,
author = {Boetticher, Gary D.},
title = {Nearest neighbor sampling for better defect prediction},
year = {2005},
isbn = {1595931252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083165.1083173},
doi = {10.1145/1083165.1083173},
abstract = {An important step in building effective predictive models applies one or more sampling techniques. Traditional sampling techniques include random, stratified, systemic, and clustered. The problem with these techniques is that they focus on the class attribute, rather than the non-class attributes. For example, if a test instance's nearest neighbor is from the opposite class of the training set, then it seems doomed to misclassification. To illustrate this problem, this paper conducts 20 experiments on five different NASA defect datasets (CM1, JM1, KC1, KC2, PC1) using two different learners (J48 and Na\"{\i}ve Bayes). Each data set is divided into 3 groups, a training set, and "nice/nasty" neighbor test sets. Using a nearest neighbor approach, "Nice neighbors" consist of those test instances closest to class training instances. "Nasty neighbors" are closest to opposite class training instances. The "Nice" experiments average 94 percent accuracy and the "Nasty" experiments average 20 percent accuracy. Based on these results a new nearest neighbor sampling technique is proposed.},
booktitle = {Proceedings of the 2005 Workshop on Predictor Models in Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {NASA data repository, decision trees, defect prediction, empirical software engineering, nearest neighbor analysis},
location = {St. Louis, Missouri},
series = {PROMISE '05}
}

@inproceedings{10.5555/2337223.2337246,
author = {Peters, Fayola and Menzies, Tim},
title = {Privacy and utility for defect prediction: experiments with MORPH},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Ideally, we can learn lessons from software projects across multiple organizations. However, a major impediment to such knowledge sharing are the privacy concerns of software development organizations. This paper aims to provide defect data-set owners with an effective means of privatizing their data prior to release. We explore MORPH which understands how to maintain class boundaries in a data-set. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. The value of training on this MORPHed data is tested via a 10-way within learning study and a cross learning study using Random Forests, Naive Bayes, and Logistic Regression for ten object-oriented defect data-sets from the PROMISE data repository. Measured in terms of exposure of sensitive attributes, the MORPHed data was four times more private than the unMORPHed data. Also, in terms of the f-measures, there was little difference between the MORPHed and unMORPHed data (original data and data privatized by data-swapping) for both the cross and within study. We conclude that at least for the kinds of OO defect data studied in this project, data can be privatized without concerns for inference efficacy.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {189–199},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1007/978-3-030-77967-2_4,
author = {Klikowski, Jakub and Burduk, Robert},
title = {Clustering and Weighted Scoring Algorithm Based on Estimating the Number of Clusters},
year = {2021},
isbn = {978-3-030-77966-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77967-2_4},
doi = {10.1007/978-3-030-77967-2_4},
abstract = {Imbalanced datasets are still a big method challenge in data mining and machine learning. Various machine learning methods and their combinations are considered to improve the quality of the classification of imbalanced datasets. This paper presents the approach with the clustering and weighted scoring function based on geometric space are used. In particular, we proposed a significant modification to our earlier algorithm. The proposed change concerns the use of automatic estimating the number of clusters and determining the minimum number of objects in a particular cluster. The proposed algorithm was compared with our earlier proposal and state-of-the-art algorithms using highly imbalanced datasets. The performed experiments show that the proposed modification is statistically better for a larger number of reference classifiers than the original algorithm.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part III},
pages = {40–49},
numpages = {10},
keywords = {Scoring function, Decision boundary, Class imbalance, Ensemble of classifiers, Imbalanced data},
location = {Krakow, Poland}
}

@article{10.1145/2020976.2020991,
author = {Malhotra, Ruchika and Jain, Ankita},
title = {Software fault prediction for object oriented systems: a literature review},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2020976.2020991},
doi = {10.1145/2020976.2020991},
abstract = {There always has been a demand to produce efficient and high quality software. There are various object oriented metrics that measure various properties of the software like coupling, cohesion, inheritance etc. which affect the software to a large extent. These metrics can be used in predicting important quality attributes such as fault proneness, maintainability, effort, productivity and reliability. Early prediction of fault proneness will help us to focus on testing resources and use them only on the classes which are predicted to be fault-prone. Thus, this will help in early phases of software development to give a measurement of quality assessment.This paper provides the review of the previous studies which are related to software metrics and the fault proneness. In other words, it reviews several journals and conference papers on software fault prediction. There is large number of software metrics proposed in the literature. Each study uses a different subset of these metrics and performs the analysis using different datasets. Also, the researchers have used different approaches such as Support vector machines, naive bayes network, random forest, artificial neural network, decision tree, logistic regression etc. Thus, this study focuses on the metrics used, dataset used and the evaluation or analysis method used by various authors. This review will be beneficial for the future studies as various researchers and practitioners can use it for comparative analysis.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–6},
numpages = {6},
keywords = {software quality, object oriented, metrics, fault proneness, empirical validation}
}

@inproceedings{10.1145/3460945.3464954,
author = {Hasabnis, Niranjan and Gottschlich, Justin},
title = {ControlFlag: a self-supervised idiosyncratic pattern detection system for software control structures},
year = {2021},
isbn = {9781450384674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460945.3464954},
doi = {10.1145/3460945.3464954},
abstract = {Software debugging has been shown to utilize upwards of half of developers’ time. Yet, machine programming (MP), the field concerned with the automation of software (and hardware) development, has recently made strides in both research and production-quality automated debugging systems. In this paper we present ControlFlag, a self-supervised MP system that aims to improve debugging by attempting to detect idiosyncratic pattern violations in software control structures. ControlFlag also suggests possible corrections in the event an anomalous pattern is detected. We present ControlFlag’s design and provide an experimental evaluation and analysis of its efficacy in identifying potential programming errors in production-quality software. As a first concrete evidence towards improving software quality, ControlFlag has already found an anomaly in CURL that has been acknowledged and fixed by its developers. We also discuss future extensions of ControlFlag.},
booktitle = {Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming},
pages = {32–42},
numpages = {11},
keywords = {self-supervised learning, Source-code mining},
location = {Virtual, Canada},
series = {MAPS 2021}
}

@inproceedings{10.1007/11497455_10,
author = {Leszak, Marek},
title = {Software defect analysis of a multi-release telecommunications system},
year = {2005},
isbn = {3540262008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11497455_10},
doi = {10.1007/11497455_10},
abstract = {This paper provides a study of several process metrics of an industrial large-scale embedded software system, the Lucent product Lambda-UniteTM MSS. This product is an evolutionary hardware/software system for the metropolitan and wide-area transmission and switching market. An analysis of defect data is performed, including and comparing all major (i.e. feature) releases till end of 2004. Several defect metrics on file-level are defined and analyzed, as basis for a defect prediction model. Main analysis results include the following. Faults and code size per file show only a weak correlation. Portion of faulty files per release tend to decrease across releases. Size and error-proneness in previous release alone is not a good predictor of a file's faults per release. Customer-found defects are strongly correlated with pre-delivery defects found per subsystem. These results are being compared to a recent similar study of fault distributions; the differences are significant.},
booktitle = {Proceedings of the 6th International Conference on Product Focused Software Process Improvement},
pages = {98–114},
numpages = {17},
keywords = {software process metrics, errorproneness, defect prediction, defect density, case study},
location = {Oulu, Finland},
series = {PROFES'05}
}

@article{10.1016/j.jss.2009.06.055,
author = {Arisholm, Erik and Briand, Lionel C. and Johannessen, Eivind B.},
title = {A systematic and comprehensive investigation of methods to build and evaluate fault prediction models},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.055},
doi = {10.1016/j.jss.2009.06.055},
abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE). The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases - both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.},
journal = {J. Syst. Softw.},
month = jan,
pages = {2–17},
numpages = {16},
keywords = {Verification, Fault prediction models, Cost-effectiveness}
}

@inproceedings{10.1145/1294948.1294953,
author = {Bernstein, Abraham and Ekanayake, Jayalath and Pinzger, Martin},
title = {Improving defect prediction using temporal features and non linear models},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294953},
doi = {10.1145/1294948.1294953},
abstract = {Predicting the defects in the next release of a large software system is a very valuable asset for the project manger to plan her resources. In this paper we argue that temporal features (or aspects) of the data are central to prediction performance. We also argue that the use of non-linear models, as opposed to traditional regression, is necessary to uncover some of the hidden interrelationships between the features and the defects and maintain the accuracy of the prediction in some cases.Using data obtained from the CVS and Bugzilla repositories of the Eclipse project, we extract a number of temporal features, such as the number of revisions and number of reported issues within the last three months. We then use these data to predict both the location of defects (i.e., the classes in which defects will occur) as well as the number of reported bugs in the next month of the project. To that end we use standard tree-based induction algorithms in comparison with the traditional regression.Our non-linear models uncover the hidden relationships between features and defects, and present them in easy to understand form. Results also show that using the temporal features our prediction model can predict whether a source file will have a defect with an accuracy of 99% (area under ROC curve 0.9251) and the number of defects with a mean absolute error of 0.019 (Spearman's correlation of 0.96).},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {11–18},
numpages = {8},
keywords = {mining software repository, defect prediction, decision tree learner},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@article{10.1016/j.jss.2010.11.920,
author = {Xie, Xiaoyuan and Ho, Joshua W. K. and Murphy, Christian and Kaiser, Gail and Xu, Baowen and Chen, Tsong Yueh},
title = {Testing and validating machine learning classifiers by metamorphic testing},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.11.920},
doi = {10.1016/j.jss.2010.11.920},
abstract = {Abstract: Machine learning algorithms have provided core functionality to many application domains - such as bioinformatics, computational linguistics, etc. However, it is difficult to detect faults in such applications because often there is no ''test oracle'' to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classification algorithms which support such applications. Our approach is based on the technique ''metamorphic testing'', which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufficiently effective to detect faults in a supervised classification program. The effectiveness of metamorphic testing is further confirmed by the detection of real faults in a popular open-source classification program.},
journal = {J. Syst. Softw.},
month = apr,
pages = {544–558},
numpages = {15},
keywords = {Verification, Validation, Test oracle, Oracle problem, Metamorphic testing, Machine learning}
}

@article{10.1016/j.ins.2008.12.001,
author = {Catal, Cagatay and Diri, Banu},
title = {Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {179},
number = {8},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2008.12.001},
doi = {10.1016/j.ins.2008.12.001},
abstract = {Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models' performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public NASA datasets from the PROMISE repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public NASA datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve (AUC) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems (AIRS2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used.},
journal = {Inf. Sci.},
month = mar,
pages = {1040–1058},
numpages = {19},
keywords = {Software fault prediction, Random Forests, Naive Bayes, Machine learning, J48, Artificial Immune Systems}
}

@inproceedings{10.1145/1368088.1368114,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368114},
doi = {10.1145/1368088.1368114},
abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Na\"{\i}ve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {181–190},
numpages = {10},
keywords = {software metrics, defect prediction, cost-sensitive classification},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.5555/2486788.2486991,
author = {Haiduc, Sonia and De Rosa, Giuseppe and Bavota, Gabriele and Oliveto, Rocco and De Lucia, Andrea and Marcus, Andrian},
title = {Query quality prediction and reformulation for source code search: the refoqus tool},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Developers search source code frequently during their daily tasks, to find pieces of code to reuse, to find where to implement changes, etc. Code search based on text retrieval (TR) techniques has been widely used in the software engineering community during the past decade. The accuracy of the TR-based search results depends largely on the quality of the query used. We introduce Refoqus, an Eclipse plugin which is able to automatically detect the quality of a text retrieval query and to propose reformulations for it, when needed, in order to improve the results of TR-based code search. A video of Refoqus is found online at http://www.youtube.com/watch?v=UQlWGiauyk4.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1307–1310},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/1869459.1869475,
author = {Gabel, Mark and Yang, Junfeng and Yu, Yuan and Goldszmidt, Moises and Su, Zhendong},
title = {Scalable and systematic detection of buggy inconsistencies in source code},
year = {2010},
isbn = {9781450302036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869459.1869475},
doi = {10.1145/1869459.1869475},
abstract = {Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs.DejaVu operates in two phases. Given a target code base, a parallel /inconsistent clone analysis/ first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible /buggy change analysis/ framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy.On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions.},
booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {175–190},
numpages = {16},
keywords = {static analysis, clone detection, bug detection},
location = {Reno/Tahoe, Nevada, USA},
series = {OOPSLA '10}
}

@article{10.1016/j.neucom.2009.10.020,
author = {Nyb\o{}, Roar},
title = {Fault detection and other time series opportunities in the petroleum industry},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {73},
number = {10–12},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2009.10.020},
doi = {10.1016/j.neucom.2009.10.020},
abstract = {Data-centric methods like soft computing and machine learning have gained greater interest and acceptance in the oil and gas industry in recent years. We give an overview of the opportunities and challenges facing applied time series prediction in this domain, with a focus on fault prediction. In particular, we argue that the physical processes and hierarchies of information flow in the industry strongly determine the choice of soft computing or machine learning methods.},
journal = {Neurocomput.},
month = jun,
pages = {1987–1992},
numpages = {6},
keywords = {Time series, Oil, Intelligent systems, Integrated operations, Fault detection}
}

@inproceedings{10.1145/2597073.2597081,
author = {Gupta, Monika and Sureka, Ashish and Padmanabhuni, Srinivas},
title = {Process mining multiple repositories for software defect resolution from control and organizational perspective},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597081},
doi = {10.1145/2597073.2597081},
abstract = {Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {122–131},
numpages = {10},
keywords = {Software Maintenance, Social Network Analysis, Process Mining, Peer Code Review System, Issue Tracking System, Empirical Software Engineering and Measurements},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.1016/j.eswa.2010.10.024,
author = {Catal, Cagatay},
title = {Review: Software fault prediction: A literature review and current trends},
year = {2011},
issue_date = {April, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.10.024},
doi = {10.1016/j.eswa.2010.10.024},
abstract = {Software engineering discipline contains several prediction approaches such as test effort prediction, correction cost prediction, fault prediction, reusability prediction, security prediction, effort prediction, and quality prediction. However, most of these prediction approaches are still in preliminary phase and more research should be conducted to reach robust models. Software fault prediction is the most popular research area in these prediction approaches and recently several research centers started new projects on this area. In this study, we investigated 90 software fault prediction papers published between year 1990 and year 2009 and then we categorized these papers according to the publication year. This paper surveys the software engineering literature on software fault prediction and both machine learning based and statistical based approaches are included in this survey. Papers explained in this article reflect the outline of what was published so far, but naturally this is not a complete review of all the papers published so far. This paper will help researchers to investigate the previous studies from metrics, methods, datasets, performance evaluation metrics, and experimental results perspectives in an easy and effective manner. Furthermore, current trends are introduced and discussed.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {4626–4636},
numpages = {11},
keywords = {Statistical methods, Software quality engineering, Software engineering, Machine learning, Expert systems, Automated fault prediction models}
}

@article{10.1007/s10664-011-9165-9,
author = {Shin, Yonghee and Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {On the use of calling structure information to improve fault prediction},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9165-9},
doi = {10.1007/s10664-011-9165-9},
abstract = {Previous studies have shown that software code attributes, such as lines of source code, and history information, such as the number of code changes and the number of faults in prior releases of software, are useful for predicting where faults will occur. In this study of two large industrial software systems, we investigate the effectiveness of adding information about calling structure to fault prediction models. Adding calling structure information to a model based solely on non-calling structure code attributes modestly improved prediction accuracy. However, the addition of calling structure information to a model that included both history and non-calling structure code attributes produced no improvement.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {390–423},
numpages = {34},
keywords = {Software faults, Negative binomial model, Empirical study, Calling structure attributes}
}

@inproceedings{10.1145/2915970.2915979,
author = {Petri\'{c}, Jean},
title = {Using different characteristics of machine learners to identify different defect families},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915979},
doi = {10.1145/2915970.2915979},
abstract = {Background: Software defect prediction has been an active area of research for the last few decades. Many models have been developed with aim to find locations in code likely to contain defects. As of yet, these prediction models are of limited use and rarely used in the software industry.Problem: Current modelling techniques are too coarse grained and fail in finding some defects. Most of the prediction models do not look for targeted defect characteristics, but rather treat them as a black box and homogeneous. No study has investigated in greater detail how well certain defect characteristics work with different prediction modelling techniques.Methodology: This PhD will address three major tasks. First, the relation among software defects, prediction models and static code metrics will be analysed. Second, the possibility of a mapping function between prediction models and defect characteristics shall be investigated. Third, an optimised ensemble model that searches for targeted defects will be developed.Contribution: A few contributions will yield from this work. Characteristics of defects will be identified, allowing other researchers to build on this work to produce more efficient prediction models in future. New modelling techniques that better suit state-of-the-art knowledge in defect prediction shall be designed. Such prediction models should be transformed in a tool that can be used by our industrial collaborator in the real industry environment.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {5},
numpages = {4},
keywords = {software defect prediction, prediction modeling, machine learning},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.5555/2820518.2820596,
author = {Altinger, Harald and Siegl, Sebastian and Dajsuren, Yanja and Wotawa, Franz},
title = {A novel industry grade dataset for fault prediction based on model-driven developed automotive embedded software},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {In this paper, we present a novel industry dataset on static software and change metrics for Matlab/Simulink models and their corresponding auto-generated C source code. The data set comprises data of three automotive projects developed and tested accordingly to industry standards and restrictive software development guidelines. We present some background information of the projects, the development process and the issue tracking as well as the creation steps of the dataset and the used tools during development. A specific highlight of the dataset is a low measurement error on change metrics because of the used issue tracking and commit policies.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {494–497},
numpages = {4},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1177/1094342010391989,
author = {Dongarra, Jack and Beckman, Pete and Moore, Terry and Aerts, Patrick and Aloisio, Giovanni and Andre, Jean-Claude and Barkai, David and Berthou, Jean-Yves and Boku, Taisuke and Braunschweig, Bertrand and Cappello, Franck and Chapman, Barbara and Xuebin Chi and Choudhary, Alok and Dosanjh, Sudip and Dunning, Thom and Fiore, Sandro and Geist, Al and Gropp, Bill and Harrison, Robert and Hereld, Mark and Heroux, Michael and Hoisie, Adolfy and Hotta, Koh and Zhong Jin and Ishikawa, Yutaka and Johnson, Fred and Kale, Sanjay and Kenway, Richard and Keyes, David and Kramer, Bill and Labarta, Jesus and Lichnewsky, Alain and Lippert, Thomas and Lucas, Bob and Maccabe, Barney and Matsuoka, Satoshi and Messina, Paul and Michielse, Peter and Mohr, Bernd and Mueller, Matthias S. and Nagel, Wolfgang E. and Nakashima, Hiroshi and Papka, Michael E and Reed, Dan and Sato, Mitsuhisa and Seidel, Ed and Shalf, John and Skinner, David and Snir, Marc and Sterling, Thomas and Stevens, Rick and Streitz, Fred and Sugar, Bob and Sumimoto, Shinji and Tang, William and Taylor, John and Thakur, Rajeev and Trefethen, Anne and Valero, Mateo and Van Der Steen, Aad and Vetter, Jeffrey and Williams, Peg and Wisniewski, Robert and Yelick, Kathy},
title = {The International Exascale Software Project roadmap},
year = {2011},
issue_date = {February  2011},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {25},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342010391989},
doi = {10.1177/1094342010391989},
abstract = {Over the last 20 years, the open-source community has provided more and more software on which the world\^{a} s high-performance computing systems depend for performance and productivity. The community has invested millions of dollars and years of effort to build key components. However, although the investments in these separate software elements have been tremendously valuable, a great deal of productivity has also been lost because of the lack of planning, coordination, and key integration of technologies necessary to make them work together smoothly and efficiently, both within individual petascale systems and between different systems. It seems clear that this completely uncoordinated development model will not provide the software needed to support the unprecedented parallelism required for peta/ exascale computation on millions of cores, or the flexibility required to exploit new hardware models and features, such as transactional memory, speculative execution, and graphics processing units. This report describes the work of the community to prepare for the challenges of exascale computing, ultimately combing their efforts in a coordinated International Exascale Software Project.},
journal = {Int. J. High Perform. Comput. Appl.},
month = feb,
pages = {3–60},
numpages = {58},
keywords = {software stack, high-performance computing, exascale computing}
}

@inproceedings{10.1109/ASE.2011.6100072,
author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
title = {Local vs. global models for effort estimation and defect prediction},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100072},
doi = {10.1109/ASE.2011.6100072},
abstract = {Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {343–351},
numpages = {9},
series = {ASE '11}
}

@inproceedings{10.1109/MSR.2007.17,
author = {Joshi, Hemant and Zhang, Chuanlei and Ramaswamy, S. and Bayrak, Coskun},
title = {Local and Global Recency Weighting Approach to Bug Prediction},
year = {2007},
isbn = {076952950X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MSR.2007.17},
doi = {10.1109/MSR.2007.17},
abstract = {Finding and fixing software bugs is a challenging maintenance task, and a significant amount of effort is invested by software development companies on this issue. In this paper, we use the Eclipse project's recorded software bug history to predict occurrence of future bugs. The history contains information on when bugs have been reported and subsequently fixed.},
booktitle = {Proceedings of the Fourth International Workshop on Mining Software Repositories},
pages = {33},
series = {MSR '07}
}

@article{10.1016/j.ins.2021.05.008,
author = {Zhang, Nana and Ying, Shi and Ding, Weiping and Zhu, Kun and Zhu, Dandan},
title = {WGNCS: A robust hybrid cross-version defect model via multi-objective optimization and deep enhanced feature representation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {570},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.008},
doi = {10.1016/j.ins.2021.05.008},
journal = {Inf. Sci.},
month = sep,
pages = {545–576},
numpages = {32},
keywords = {Convolutional neural network, Wasserstein GAN with Gradient Penalty, Deep learning techniques, Multi-objective feature selection, Cross-version defect prediction}
}

@article{10.1016/j.infsof.2006.07.005,
author = {Kanmani, S. and Uthariaraj, V. Rhymend and Sankaranarayanan, V. and Thambidurai, P.},
title = {Object-oriented software fault prediction using neural networks},
year = {2007},
issue_date = {May, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {5},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.07.005},
doi = {10.1016/j.infsof.2006.07.005},
abstract = {This paper introduces two neural network based software fault prediction models using Object-Oriented metrics. They are empirically validated using a data set collected from the software modules developed by the graduate students of our academic institution. The results are compared with two statistical models using five quality attributes and found that neural networks do better. Among the two neural networks, Probabilistic Neural Networks outperform in predicting the fault proneness of the Object-Oriented modules developed.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {483–492},
numpages = {10},
keywords = {Probabilistic neural network, Object-Oriented metrics, Logistic regression, Fault proneness, Discriminant analysis, Back propagation neural network}
}

@inproceedings{10.5555/1627368.1627435,
author = {Podgorelec, Vili},
title = {On software fault prediction by mining software complexity data with dynamically filtered training sets},
year = {2009},
isbn = {9789604741137},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Software fault prediction methods are very appropriate for improving the software reliability. With the creation of large empirical databases of software projects, as a result of stimulated research on estimation models, metrics and methods for measuring and improving processes and products, intelligent mining of these datasets can largely add to the improvement of software reliability. In the paper we present a study on using decision tree classifiers for predicting software faults. A new training set filtering method is presented that should improve the classification performance when mining the software complexity measures data. The classification improvement should be achieved by removing the identified outliers from a training set. We argue that a classifier trained by a filtered dataset captures a more general knowledge model and should therefore perform better also on unseen cases. The proposed method is applied on a real-world software reliability analysis dataset and the obtained results are discussed.},
booktitle = {Proceedings of the 9th WSEAS International Conference on Simulation, Modelling and Optimization},
pages = {332–337},
numpages = {6},
keywords = {software fault prediction, search-based software engineering, filtering training set, complexity metrics, classification},
location = {Budapest, Hungary},
series = {SMO'09}
}

@article{10.5555/2871240.2871253,
author = {Ul Amin, Rooh and Aijun, Li and Ali, Malik Mazhar},
title = {Fuzzy, neural network and expert systems methodologies and applications-a review},
year = {2015},
issue_date = {April 2015},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {11},
number = {1–2},
issn = {1550-4646},
abstract = {The rapid growth in the field of artificial intelligence from past one decade has a significant impact on various application areas i.e. health, security, home appliances among many. In this paper we aim to review artificial intelligence methodologies and their potential applications intended for variable purposes i.e. Agriculture, applied sciences, business, engineering, finance, management etc. For this purpose articles from past one decade (from 2004 to 2013) are reviewed in order to explore the most recent research advancements in this domain. The review includes 172 articles gathered from related sources including conference proceedings and academic journals. We have categorized the selected articles into four main categories i.e. fuzzy systems, neural network based systems, neuro fuzzy systems and expert systems. Furthermore, expert systems are further classified into three categories: (i) rule based expert systems, (ii) knowledge based expert systems and (iii) intelligent agents. This review presents research implications for practitioners regarding integration of artificial intelligence techniques with classical approaches and suggestions for exploration of AI techniques in variable applications.},
journal = {J. Mob. Multimed.},
month = apr,
pages = {157–176},
numpages = {20},
keywords = {neural network, literature survey, fuzzy systems, expert systems, artificial intelligence methodologies}
}

@article{10.1016/j.jss.2016.01.003,
author = {Kumar, Lov and Rath, Santanu Ku.},
title = {Hybrid functional link artificial neural network approach for predicting maintainability of object-oriented software},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.01.003},
doi = {10.1016/j.jss.2016.01.003},
abstract = {Among all quality parameters, Maintainability is more important to achieve success.This paper focus on maintainability of software using object oriented metrics.Hybrid approach of neural network is used to design a model for prediction.Two feature selection techniques are used to select best set of metrics.We achieved better prediction rate for maintainability as compared to others. In present day, software development methodology is mostly based on object-oriented paradigm. With the increase in the number of these software system, their effective maintenance aspects becomes a crucial factor. Most of the maintainability prediction models in literature are based on techniques such as regression analysis and simple neural network. In this paper, three artificial intelligence techniques (AI) such as hybrid approach of functional link artificial neural network (FLANN) with genetic algorithm (GA), particle swarm optimization (PSO) and clonal selection algorithm (CSA), i.e., FLANN-Genetic (FGA and AFGA), FLANN-PSO (FPSO and MFPSO), FLANN-CSA (FCSA) are applied to design a model for predicting maintainability. These three AI techniques are applied to predict maintainability on two case studies such as Quality Evaluation System (QUES) and User Interface System (UIMS). This paper also focuses on the effectiveness of feature reduction techniques such as rough set analysis (RSA) and principal component analysis (PCA) when they are applied for predicting maintainability. The results show that feature reduction techniques are very effective in obtaining better results while using FLANN-Genetic.},
journal = {J. Syst. Softw.},
month = nov,
pages = {170–190},
numpages = {21},
keywords = {Maintainability, CK metrics suite, Artificial neural network}
}

@inproceedings{10.1145/2594368.2594377,
author = {Ravindranath, Lenin and Nath, Suman and Padhye, Jitendra and Balakrishnan, Hari},
title = {Automatic and scalable fault detection for mobile applications},
year = {2014},
isbn = {9781450327930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594368.2594377},
doi = {10.1145/2594368.2594377},
abstract = {This paper describes the design, implementation, and evaluation of VanarSena, an automated fault finder for mobile applications (``apps''). The techniques in VanarSena are driven by a study of 25 million real-world crash reports of Windows Phone apps reported in 2012. Our analysis indicates that a modest number of root causes are responsible for many observed failures, but that they occur in a wide range of places in an app, requiring a wide coverage of possible execution paths. VanarSena adopts a ``greybox'' testing method, instrumenting the app binary to achieve both coverage and speed. VanarSena runs on cloud servers: the developer uploads the app binary; VanarSena then runs several app ``monkeys'' in parallel to emulate user, network, and sensor data behavior, returning a detailed report of crashes and failures. We have tested VanarSena with 3000 apps from the Windows Phone store, finding that 1108 of them had failures; VanarSena uncovered 2969 distinct bugs in existing apps, including 1227 that were not previously reported. Because we anticipate VanarSena being used in regular regression tests, testing speed is important. VanarSena uses two techniques to improve speed. First, it uses a ``hit testing'' method to quickly emulate an app by identifying which user interface controls map to the same execution handlers in the code. Second, it generates a ProcessingCompleted event to accurately determine when to start the next interaction. These features are key benefits of VanarSena's greybox philosophy.},
booktitle = {Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {190–203},
numpages = {14},
keywords = {testing, software engineering, reliability, mobile applications},
location = {Bretton Woods, New Hampshire, USA},
series = {MobiSys '14}
}

@inproceedings{10.1145/3387904.3389295,
author = {Lenarduzzi, Valentina and Palomba, Fabio and Taibi, Davide and Tamburri, Damian Andrew},
title = {OpenSZZ: A Free, Open-Source, Web-Accessible Implementation of the SZZ Algorithm},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389295},
doi = {10.1145/3387904.3389295},
abstract = {The accurate identification of defect-inducing commits represents a key problem for researchers interested in studying the naturalness of defects and defining defect prediction models. To tackle this problem, software engineering researchers have relied on and proposed several implementations of the well-known Sliwerski-Zimmermann-Zeller (SZZ) algorithm. Despite its popularity and wide usage, no open-source, publicly available, and web-accessible implementation of the algorithm has been proposed so far. In this paper, we prototype and make available one such implementation for further use by practitioners and researchers alike. The evaluation of the proposed prototype showed competitive results and lays the foundation for future work. This paper outlines our prototype, illustrating its usage and reporting on its evaluation in action.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {446–450},
numpages = {5},
keywords = {Web APIs, Software Defect Proneness, Software Defect Prediction, Open-Source Tools},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/1806799.1806855,
author = {Bacchelli, Alberto and Lanza, Michele and Robbes, Romain},
title = {Linking e-mails and source code artifacts},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806855},
doi = {10.1145/1806799.1806855},
abstract = {E-mails concerning the development issues of a system constitute an important source of information about high-level design decisions, low-level implementation concerns, and the social structure of developers.Establishing links between e-mails and the software artifacts they discuss is a non-trivial problem, due to the inherently informal nature of human communication. Different approaches can be brought into play to tackle this trace-ability issue, but the question of how they can be evaluated remains unaddressed, as there is no recognized benchmark against which they can be compared.In this article we present such a benchmark, which we created through the manual inspection of a statistically significant number of e-mails pertaining to six unrelated software systems. We then use our benchmark to measure the effectiveness of a number of approaches, ranging from lightweight approaches based on regular expressions to full-fledged information retrieval approaches.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {375–384},
numpages = {10},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.5555/2666527.2666533,
author = {Morgado, In\^{e}s Coimbra and Paiva, Ana C. R. and Faria, Jo\~{a}o Pascoal and Camacho, Rui},
title = {GUI reverse engineering with machine learning},
year = {2012},
isbn = {9781467317535},
publisher = {IEEE Press},
abstract = {This paper proposes a new approach to reduce the effort of building formal models representative of the structure and behaviour of Graphical User Interfaces (GUI). The main goal is to automatically extract the GUI model with a dynamic reverse engineering process, consisting in an exploration phase, that extracts information by interacting with the GUI, and in a model generation phase that, making use of machine learning techniques, uses the extracted information of the first step to generate a state-machine model of the GUI, including guard conditions to remove ambiguity in transitions.},
booktitle = {Proceedings of the First International Workshop on Realizing AI Synergies in Software Engineering},
pages = {27–31},
numpages = {5},
keywords = {reverse engineering, model-based testing, machine learning, inductive logic programming},
location = {Zurich, Switzerland},
series = {RAISE '12}
}

@inproceedings{10.1145/2568225.2568307,
author = {Lee, Sangho and Jung, Changhee and Pande, Santosh},
title = {Detecting memory leaks through introspective dynamic behavior modelling using machine learning},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568307},
doi = {10.1145/2568225.2568307},
abstract = {This paper expands staleness-based memory leak detection by presenting a machine learning-based framework. The proposed framework is based on an idea that object staleness can be better leveraged in regard to similarity of objects; i.e., an object is more likely to have leaked if it shows significantly high staleness not observed from other similar objects with the same allocation context.  A central part of the proposed framework is the modeling of heap objects. To this end, the framework observes the staleness of objects during a representative run of an application. From the observed data, the framework generates training examples, which also contain instances of hypothetical leaks. Via machine learning, the proposed framework replaces the error-prone user-definable staleness predicates used in previous research with a model-based prediction.  The framework was tested using both synthetic and real-world examples. Evaluation with synthetic leakage workloads of SPEC2006 benchmarks shows that the proposed method achieves the optimal accuracy permitted by staleness-based leak detection. Moreover, by incorporating allocation context into the model, the proposed method achieves higher accuracy than is possible with object staleness alone. Evaluation with real-world memory leaks demonstrates that the proposed method is effective for detecting previously reported bugs with high accuracy.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {814–824},
numpages = {11},
keywords = {Runtime analysis, Memory leak detection, Machine learning},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/1414004.1414063,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {Analysis of the reliability of a subset of change metrics for defect prediction},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414063},
doi = {10.1145/1414004.1414063},
abstract = {In this paper, we describe an experiment, which analyzes the relative importance and stability of change metrics for predicting defects for 3 releases of the Eclipse project. The results indicate that out of 18 change metrics 3 metrics contain most information about software defects. Moreover, those 3 metrics remain stable across 3 releases of the Eclipse project. A comparative analysis with the full model shows that the prediction accuracy is not too much affected by using a subset of 3 metrics and the recall even improves.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {309–311},
numpages = {3},
keywords = {software metrics, feature selection, defect prediction},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@article{10.1145/2556777,
author = {Zhou, Yuming and Xu, Baowen and Leung, Hareton and Chen, Lin},
title = {An in-depth study of the potentially confounding effect of class size in fault prediction},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2556777},
doi = {10.1145/2556777},
abstract = {Background. The extent of the potentially confounding effect of class size in the fault prediction context is not clear, nor is the method to remove the potentially confounding effect, or the influence of this removal on the performance of fault-proneness prediction models. Objective. We aim to provide an in-depth understanding of the effect of class size on the true associations between object-oriented metrics and fault-proneness. Method. We first employ statistical methods to examine the extent of the potentially confounding effect of class size in the fault prediction context. After that, we propose a linear regression-based method to remove the potentially confounding effect. Finally, we empirically investigate whether this removal could improve the prediction performance of fault-proneness prediction models. Results. Based on open-source software systems, we found: (a) the confounding effect of class size on the associations between object-oriented metrics and fault-proneness in general exists; (b) the proposed linear regression-based method can effectively remove the confounding effect; and (c) after removing the confounding effect, the prediction performance of fault prediction models with respect to both ranking and classification can in general be significantly improved. Conclusion. We should remove the confounding effect of class size when building fault prediction models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {10},
numpages = {51},
keywords = {prediction, fault, confounding effect, class size, Metrics}
}

@article{10.1016/j.ins.2021.05.041,
author = {Kluska, Jacek and Madera, Michal},
title = {Extremely simple classifier based on fuzzy logic and gene expression programming},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {571},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.041},
doi = {10.1016/j.ins.2021.05.041},
journal = {Inf. Sci.},
month = sep,
pages = {560–579},
numpages = {20},
keywords = {Interpretability, Gene expression programming, Fuzzy rule-based classifier, Data mining, Machine learning}
}

@inproceedings{10.5555/2818754.2818851,
author = {Peters, Fayola and Menzies, Tim and Layman, Lucas},
title = {LACE2: better privacy-preserving data sharing for cross project defect prediction},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data.In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute "interesting" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {801–811},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2723742.2723752,
author = {Kumar, Lov and Rath, Santanu Ku.},
title = {Predicting Object-Oriented Software Maintainability using Hybrid Neural Network with Parallel Computing Concept},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723752},
doi = {10.1145/2723742.2723752},
abstract = {Software maintenance is an important aspect of software life cycle development, hence prior estimation of effort for maintainability plays a vital role. Existing approaches for maintainability estimation are mostly based on regression analysis and neural network approaches. It is observed that numerous software metrics are even used as input for estimation. In this study, Object-Oriented software metrics are considered to provide requisite input data for designing a model. It helps in estimating the maintainability of Object-Oriented software. Models for estimating maintainability are designed using the parallel computing concept of Neuro-Genetic algorithm (hybrid approach of neural network and genetic algorithm). This technique is employed to estimate the software maintainability of two case studies such as the User Interface System (UIMS), and Quality Evaluation System (QUES). This paper also focuses on the effectiveness of feature reduction techniques such as rough set analysis (RSA) and principal component analysis (PCA). The results show that, RSA and PCA obtained better results for UIMS and QUES respectively. Further, it observed the parallel computing concept is helpful in accelerating the training procedure of the neural network model.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {100–109},
numpages = {10},
keywords = {Parallel Computing, Object-Oriented Metrics, Maintainability, Genetics algorithm, Artificial neural network},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/3273934.3273936,
author = {Ferenc, Rudolf and T\'{o}th, Zolt\'{a}n and Lad\'{a}nyi, Gergely and Siket, Istv\'{a}n and Gyim\'{o}thy, Tibor},
title = {A Public Unified Bug Dataset for Java},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273936},
doi = {10.1145/3273934.3273936},
abstract = {Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (e.g. to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names, (ii) in some cases different tools give different values even if the metric definitions coincide because (iii) one tool works on source code while the other calculates metrics on bytecode, or (iv) in several cases the downloaded source code contained more files which influenced the afferent metric values significantly.Conclusions: Apart from all these imprecisions, we think that having a common metric set can help in building better bug prediction models and deducing more general conclusions. We made the unified dataset publicly available for everyone. By using a public dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {Bug dataset, code metrics, static code analysis},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@article{10.1007/s10664-009-9111-2,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
title = {Comparing the effectiveness of several modeling methods for fault prediction},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9111-2},
doi = {10.1007/s10664-009-9111-2},
abstract = {We compare the effectiveness of four modeling methods--negative binomial regression, recursive partitioning, random forests and Bayesian additive regression trees--for predicting the files likely to contain the most faults for 28 to 35 releases of three large industrial software systems. Predictor variables included lines of code, file age, faults in the previous release, changes in the previous two releases, and programming language. To compare the effectiveness of the different models, we use two metrics--the percent of faults contained in the top 20% of files identified by the model, and a new, more general metric, the fault-percentile-average. The negative binomial regression and random forests models performed significantly better than recursive partitioning and Bayesian additive regression trees, as assessed by either of the metrics. For each of the three systems, the negative binomial and random forests models identified 20% of the files in each release that contained an average of 76% to 94% of the faults.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {277–295},
numpages = {19},
keywords = {Recursive partitioning, Random forests, Negative binomial, Fault-percentile-average, Fault prediction, Empirical study, Bayesian trees}
}

@inproceedings{10.1145/1083165.1083172,
author = {Koru, A. G\"{u}nes and Liu, Hongfang},
title = {An investigation of the effect of module size on defect prediction using static measures},
year = {2005},
isbn = {1595931252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083165.1083172},
doi = {10.1145/1083165.1083172},
abstract = {We used several machine learning algorithms to predict the defective modules in five NASA products, namely, CM1, JM1, KC1, KC2, and PC1. A set of static measures were employed as predictor variables. While doing so, we observed that a large portion of the modules were small, as measured by lines of code (LOC). When we experimented on the data subsets created by partitioning according to module size, we obtained higher prediction performance for the subsets that include larger modules. We also performed defect prediction using class-level data for KC1 rather than the method-level data. In this case, the use of class-level data resulted in improved prediction performance compared to using method-level data. These findings suggest that quality assurance activities can be guided even better if defect prediction is performed by using data that belong to larger modules.},
booktitle = {Proceedings of the 2005 Workshop on Predictor Models in Software Engineering},
pages = {1–5},
numpages = {5},
keywords = {defect prediction, prediction models, software metrics, software quality management, static measures},
location = {St. Louis, Missouri},
series = {PROMISE '05}
}

@article{10.1145/2557833.2557849,
author = {Malhotra, Ruchika and Agrawal, Anushree},
title = {CMS tool: calculating defect and change data from software project repositories},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2557849},
doi = {10.1145/2557833.2557849},
abstract = {Defect and change prediction is a very important activity in software development. Predicting erroneous classes of the system early in the software development life cycle will enable early identification of risky classes in the initial phases. This will assist software practitioners in designing and developing software systems of better quality with focused resources and hence take necessary corrective design actions. In this work we describe a framework to develop and calculate the defect fixes and changes made during various versions of a software system. We develop a tool, Configuration Management System (CMS), which uses log files obtained from a Concurrent Versioning System (CVS) repository in order to collect the number of defects from each class. The tool also calculates the number of changes made during each version of the software. This tool will also assist software practitioners and researchers in collecting defect and change data for software systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–5},
numpages = {5},
keywords = {software project repositories, defect prediction, change prediction, CVS}
}

@article{10.1007/s10664-020-09843-6,
author = {Krishna, Rahul and Menzies, Tim},
title = {Learning actionable analytics from multiple software projects},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09843-6},
doi = {10.1007/s10664-020-09843-6},
abstract = {The current generation of software analytics tools are mostly prediction algorithms (e.g. support vector machines, naive bayes, logistic regression, etc). While prediction is useful, after prediction comes planning about what actions to take in order to improve quality. This research seeks methods that generate demonstrably useful guidance on “what to do” within the context of a specific software project. Specifically, we propose XTREE (for within-project planning) and BELLTREE (for cross-project planning) to generating plans that can improve software quality. Each such plan has the property that, if followed, it reduces the expected number of future defect reports. To find this expected number, planning was first applied to data from release x. Next, we looked for change in release x + 1 that conformed to our plans. This procedure was applied using a range of planners from the literature, as well as XTREE. In 10 open-source JAVA systems, several hundreds of defects were reduced in sections of the code that conformed to XTREE’s plans. Further, when compared to other planners, XTREE’s plans were found to be easier to implement (since they were shorter) and more effective at reducing the expected number of defects.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3468–3500},
numpages = {33},
keywords = {Defect prediction, Bellwethers, Planning, Actionable analytics, Data mining}
}

@inproceedings{10.1109/MSR.2007.14,
author = {Canfora, Gerardo and Cerulo, Luigi and Penta, Massimiliano Di},
title = {Identifying Changed Source Code Lines from Version Repositories},
year = {2007},
isbn = {076952950X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MSR.2007.14},
doi = {10.1109/MSR.2007.14},
abstract = {Observing the evolution of software systems at different levels of granularity has been a key issue for a number of studies, aiming at predicting defects or at studying certain phenomena, such as the presence of clones or of crosscutting concerns. Versioning systems such as CVS and SVN, however, only provide information about lines added or deleted by a contributor: any change is shown as a sequence of additions and deletions. This provides an erroneous estimate of the amount of code changed. This paper shows how the evolution of changes at source code line level can be inferred from CVS repositories, by combining information retrieval techniques and the Levenshtein edit distance. The application of the proposed approach to the ArgoUML case study indicates a high precision and recall.},
booktitle = {Proceedings of the Fourth International Workshop on Mining Software Repositories},
pages = {14},
series = {MSR '07}
}

@article{10.1007/s00500-021-06048-x,
author = {Rathore, Santosh S.},
title = {An exploratory analysis of regression methods for predicting faults in software systems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {23},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06048-x},
doi = {10.1007/s00500-021-06048-x},
abstract = {The use of regression methods, for instance, linear regression, decision tree regression, etc., has been used earlier to build software fault prediction (SFP) models. However, these methods showed limited SFP performance with higher misclassification errors. In previous works, issues such as multicollinearity, feature scaling, and imbalance distribution of faulty and non-faulty modules in the dataset have not been considered reasonably, which might be a potential cause behind the poor prediction performance of these regression methods. Motivated from it, in this paper, we investigate the impact of 15 different regression methods for the faults count prediction in the software system and report their interpretation for fault models. We consider different fault data quality issues, and a comprehensive assessment of the regression methods is presented to handle these issues. We believe that many used regression methods have not been explored before for the SFP by considering different data quality issues. In the presented study, 44 fault datasets and their versions are used that are collected from the PROMISE software data repository are used to validate the performance of the regression methods, and absolute relative error (ARE), root mean square error (RSME), and fault-percentile-average (FPA) are used as the performance measures. For the model building, five different scenarios are considered, (1) original dataset without preprocessing; (2) standardized processed dataset; (3) balanced dataset; (4) non-multicollinearity processed dataset; (5) balanced+non-multicollinearity processed dataset. Experimental results showed that overall kernel-based regression methods, KernelRidge and SVR (Support vector regression, both linear and nonlinear kernels), yielded the best performance for predicting the fault counts compared to other methods. Other regression methods, in particular NNR (Nearest neighbor regression), RFR (Random forest regression), and GBR (Gradient boosting regression), are performed significantly accurately. Further, results showed that applying standardization and handling multicollinearity in the fault dataset helped improve regression methods’ performance. It is concluded that regression methods are promising for building software fault prediction models.},
journal = {Soft Comput.},
month = dec,
pages = {14841–14872},
numpages = {32},
keywords = {Empirical study, PROMISE data repository, Regression methods, Software fault prediction}
}

@inproceedings{10.5555/3524938.3525939,
author = {Yasunaga, Michihiro and Liang, Percy},
title = {Graph-based, self-supervised program repair from diagnostic feedback},
year = {2020},
publisher = {JMLR.org},
abstract = {We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 68.2% full repair rate on DeepFix (+22.9% over the prior best), and 48.4% synthesis success rate on SPoC (+3.7% over the prior best).},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1001},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.1145/3292500.3330780,
author = {Schulze, Jan-Philipp and Mrowca, Artur and Ren, Elizabeth and Loeliger, Hans-Andrea and B\"{o}ttinger, Konstantin},
title = {Context by Proxy: Identifying Contextual Anomalies Using an Output Proxy},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330780},
doi = {10.1145/3292500.3330780},
abstract = {Contextual anomalies arise only under special internal or external stimuli in a system, often making it infeasible to detect them by a rule-based approach. Labelling the underlying problem sources is hard because complex, time-dependent relationships between the inputs arise. We propose a novel unsupervised approach that combines tools from deep learning and signal processing, working in a purely data-driven way. Many systems show a desirable target behaviour which can be used as a proxy quantity removing the need to manually label data. The methodology was evaluated on real-life test car traces in the form of multivariate state message sequences. We successfully identified contextual anomalies during the cars' timeout process along with possible explanations. Novel input encodings allow us to summarise the entire system context including the timing such that more information is available during the decision process.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2059–2068},
numpages = {10},
keywords = {unsupervised learning, signal processing, recurrent neural networks, automotive, anomaly detection},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1109/FLOSS.2009.5071357,
author = {Caglayan, Bora and Bener, Ayse and Koch, Stefan},
title = {Merits of using repository metrics in defect prediction for open source projects},
year = {2009},
isbn = {9781424437207},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FLOSS.2009.5071357},
doi = {10.1109/FLOSS.2009.5071357},
abstract = {Many corporate code developers are the beta testers of open source software.They continue testing until they are sure that they have a stable version to build their code on. In this respect defect predictors play a critical role to identify defective parts of the software. Performance of a defect predictor is determined by correctly finding defective parts of the software without giving any false alarms. Having high false alarms means testers/ developers would inspect bug free code unnecessarily. Therefore in this research we focused on decreasing the false alarm rates by using repository metrics. We conducted experiments on the data sets of Eclipse project. Our results showed that repository metrics decreased the false alarm rates on the average to 23% from 32% corresponding up to 907 less files to inspect.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development},
pages = {31–36},
numpages = {6},
series = {FLOSS '09}
}

@inproceedings{10.5555/3489212.3489359,
author = {Lee, Suyoung and Han, HyungSeok and Cha, Sang Kil and Son, Sooel},
title = {Montage: a neural network language model-guided JavaScript engine fuzzer},
year = {2020},
isbn = {978-1-939133-17-5},
publisher = {USENIX Association},
address = {USA},
abstract = {JavaScript (JS) engine vulnerabilities pose significant security threats affecting billions of web browsers. While fuzzing is a prevalent technique for finding such vulnerabilities, there have been few studies that leverage the recent advances in neural network language models (NNLMs). In this paper, we present Montage, the first NNLM-guided fuzzer for finding JS engine vulnerabilities. The key aspect of our technique is to transform a JS abstract syntax tree (AST) into a sequence of AST subtrees that can directly train prevailing NNLMs. We demonstrate that Montage is capable of generating valid JS tests, and show that it outperforms previous studies in terms of finding vulnerabilities. Montage found 37 real-world bugs, including three CVEs, in the latest JS engines, demonstrating its efficacy in finding JS engine bugs.},
booktitle = {Proceedings of the 29th USENIX Conference on Security Symposium},
articleno = {147},
numpages = {18},
series = {SEC'20}
}

@inproceedings{10.1109/ICMV.2009.54,
author = {Kaur, Arashdeep and Sandhu, Parvinder S. and Bra, Amanpreet Singh},
title = {Early Software Fault Prediction Using Real Time Defect Data},
year = {2010},
isbn = {9780769539447},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMV.2009.54},
doi = {10.1109/ICMV.2009.54},
abstract = {Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using clustering techniques. This approach has been tested with three real time defect datasets of NASA software projects, JM1, PC1 and CM1. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The results show that when all the prediction techniques are evaluated, the best prediction model is found to be the fusion of requirement and code metric model.},
booktitle = {Proceedings of the 2009 Second International Conference on Machine Vision},
pages = {242–245},
numpages = {4},
series = {ICMV '09}
}

@inproceedings{10.5555/2486788.2487006,
author = {Jonsson, Leif},
title = {Increasing anomaly handling efficiency in large organizations using applied machine learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1361–1364},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/2905055.2905123,
author = {Singh, Satwinder and Singla, Rozy},
title = {Comparative Performance of Fault-Prone Prediction Classes with K-means Clustering and MLP},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905123},
doi = {10.1145/2905055.2905123},
abstract = {Software defect in today's era is most important in the field of software engineering. Most of the organizations used various techniques to predict defects in their products before they are delivered. Defect prediction techniques help the organizations to use their resources effectively which results in lower cost and time requirements. There are various techniques that are used for predicting defects in software before it has to be delivered. For example clustering, neural networks, support vector machine (SVM) etc. In this paper two defect prediction techniques: - K-means Clustering and Multilayer Perceptron model (MLP), are compared. Both the techniques are implemented on different platforms. K-means clustering is implemented using WEKA tool and MLP is implemented using SPSS. The results are compared to find which algorithm produces better results. In this paper Object-Oriented metrics are used for predicting defects in the software.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {65},
numpages = {7},
keywords = {Weka, Object-Oriented Metrics, Neural Network, K-means Clustering, Defect prediction},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00061,
author = {Liu, Changlin and Xiao, Xusheng},
title = {ProMal: precise window transition graphs for Android via synergy of program analysis and machine learning},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00061},
doi = {10.1109/ICSE-Companion52605.2021.00061},
abstract = {Mobile apps have been an integral part in our daily life. As these apps become more complex, it is critical to provide automated analysis techniques to ensure the correctness, security, and performance of these apps. A key component for these automated analysis techniques is to create a graphical user interface (GUI) model of an app, i.e., a window transition graph (WTG), that models windows and transitions among the windows. While existing work has provided both static and dynamic analysis to build the WTG for an app, the constructed WTG misses many transitions or contains many infeasible transitions due to the coverage issues of dynamic analysis and over-approximation of the static analysis. We propose ProMal, a "tribrid" analysis that synergistically combines static analysis, dynamic analysis, and machine learning to construct a precise WTG. Specifically, ProMal first applies static analysis to build a static WTG, and then applies dynamic analysis to verify the transitions in the static WTG. For the unverified transitions, ProMal further provides machine learning techniques that leverage runtime information (i.e., screenshots, UI layouts, and text information) to predict whether they are feasible transitions. Our evaluations on 40 real-world apps demonstrate the superiority of ProMal in building WTGs over static analysis, dynamic analysis, and machine learning techniques when they are applied separately.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {144–146},
numpages = {3},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1007/s11219-019-09467-0,
author = {Du, Xiaoting and Zhou, Zenghui and Yin, Beibei and Xiao, Guanping},
title = {Cross-project bug type prediction based on transfer learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09467-0},
doi = {10.1007/s11219-019-09467-0},
abstract = {The prediction of bug types provides useful insights into the software maintenance process. It can improve the efficiency of software testing and help developers adopt corresponding strategies to fix bugs before releasing software projects. Typically, the prediction tasks are performed through machine learning classifiers, which rely heavily on labeled data. However, for a software project that has insufficient labeled data, it is difficult to train the classification model for predicting bug types. Although labeled data of other projects can be used as training data, the results of the cross-project prediction are often poor. To solve this problem, this paper proposes a cross-project bug type prediction framework based on transfer learning. Transfer learning breaks the assumption of traditional machine learning methods that the training set and the test set should follow the same distribution. Our experiments show that the results of cross-project bug type prediction have significant improvement by adopting transfer learning. In addition, we have studied the factors that influence the prediction results, including different pairs of source and target projects, and the number of bug reports in the source project.},
journal = {Software Quality Journal},
month = mar,
pages = {39–57},
numpages = {19},
keywords = {Transfer learning, Bug report, Cross-project, Bug prediction}
}

@article{10.1016/j.future.2019.12.016,
author = {Ortin, Francisco and Rodriguez-Prieto, Oscar and Pascual, Nicolas and Garcia, Miguel},
title = {Heterogeneous tree structure classification to label Java programmers according to their expertise level},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.12.016},
doi = {10.1016/j.future.2019.12.016},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {380–394},
numpages = {15},
keywords = {Big data, Decision trees, Programmer expertise, Abstract syntax trees, Syntax patterns, Machine learning, Big code}
}

@article{10.1007/s10664-021-10024-2,
author = {Zamani, Shayan and Hemmati, Hadi},
title = {A pragmatic approach for hyper-parameter tuning in search-based test case generation},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10024-2},
doi = {10.1007/s10664-021-10024-2},
abstract = {Search-based test case generation, which is the application of meta-heuristic search for generating test cases, has been studied a lot in the literature, lately. Since, in theory, the performance of meta-heuristic search methods is highly dependent on their hyper-parameters, there is a need to study hyper-parameter tuning in this domain. In this paper, we propose a new metric (“Tuning Gain”), which estimates how cost-effective tuning a particular class is. We then predict “Tuning Gain” using static features of source code classes. Finally, we prioritize classes for tuning, based on the estimated “Tuning Gains” and spend the tuning budget only on the highly-ranked classes. To evaluate our approach, we exhaustively analyze 1,200 hyper-parameter configurations of a well-known search-based test generation tool (EvoSuite) for 250 classes of 19 projects from benchmarks such as SF110 and SBST2018 tool competition. We used a tuning approach called Meta-GA and compared the tuning results with and without the proposed class prioritization. The results show that for a low tuning budget, prioritizing classes outperforms the alternatives in terms of extra covered branches (10 times more than a traditional global tuning). In addition, we report the impact of different features of our approach such as search space size, tuning budgets, tuning algorithms, and the number of classes to tune, on the final results.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {35},
keywords = {Source code metrics, Test case generation, Hyper-parameter tuning, Search-based testing}
}

@inproceedings{10.1145/1810295.1810313,
author = {Kl\"{a}s, Michael and Elberzhager, Frank and M\"{u}nch, J\"{u}rgen and Hartjes, Klaus and von Graevemeyer, Olaf},
title = {Transparent combination of expert and measurement data for defect prediction: an industrial case study},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810313},
doi = {10.1145/1810295.1810313},
abstract = {Defining strategies on how to perform quality assurance (QA) and how to control such activities is a challenging task for organizations developing or maintaining software and software-intensive systems. Planning and adjusting QA activities could benefit from accurate estimations of the expected defect content of relevant artifacts and the effectiveness of important quality assurance activities. Combining expert opinion with commonly available measurement data in a hybrid way promises to overcome the weaknesses of purely data-driven or purely expert-based estimation methods. This article presents a case study of the hybrid estimation method HyDEEP for estimating defect content and QA effectiveness in the telecommunication domain. The specific focus of this case study is the use of the method for gaining quantitative predictions. This aspect has not been empirically analyzed in previous work. Among other things, the results show that for defect content estimation, the method performs significantly better statistically than purely data-based methods, with a relative error of 0.3 on average (MMRE).},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {119–128},
numpages = {10},
keywords = {hybrid estimation, effectiveness, defect content, HyDEEP},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/3196398.3196438,
author = {Nayrolles, Mathieu and Hamou-Lhadj, Abdelwahab},
title = {CLEVER: combining code metrics with clone detection for just-in-time fault prevention and resolution in large industrial projects},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196438},
doi = {10.1145/3196398.3196438},
abstract = {Automatic prevention and resolution of faults is an important research topic in the field of software maintenance and evolution. Existing approaches leverage code and process metrics to build metric-based models that can effectively prevent defect insertion in a software project. Metrics, however, may vary from one project to another, hindering the reuse of these models. Moreover, they tend to generate high false positive rates by classifying healthy commits as risky. Finally, they do not provide sufficient insights to developers on how to fix the detected risky commits. In this paper, we propose an approach, called CLEVER (Combining Levels of Bug Prevention and Resolution techniques), which relies on a two-phase process for intercepting risky commits before they reach the central repository. When applied to 12 Ubisoft systems, the results show that CLEVER can detect risky commits with 79% precision and 65% recall, which outperforms the performance of Commit-guru, a recent approach that was proposed in the literature. In addition, CLEVER is able to recommend qualitative fixes to developers on how to fix risky commits in 66.7% of the cases.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {153–164},
numpages = {12},
keywords = {defect predictions, fault fixing, software evolution, software maintenance},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1007/s10586-018-1923-7,
author = {Viji, C. and Rajkumar, N. and Duraisamy, S.},
title = {Prediction of software fault-prone classes using an unsupervised hybrid SOM algorithm},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-1923-7},
doi = {10.1007/s10586-018-1923-7},
abstract = {In software engineering fault proneness prediction is one of the important fields for quality measurement using multiple code metrics. The metrics thresholds are very practical in measuring the code quality for fault proneness prediction. It helps to improvise the software quality in short time with very low cost. Many researchers are in the race to develop a measuring attribute for the software quality using various methodologies. Currently so many fault proneness prediction models are available. Among that most of the methods are used to identify the faults either by data history or by special supervising algorithms. In most of the real time cases the fault data bases may not be available so that the process becomes tedious. This article proposes a hybrid model for identifying the faults in the software models and also we proposed coupling model along with the algorithm so that the metrics are used to identify the faults and the coupling model couples the metrics and the faults for the developed system software.},
journal = {Cluster Computing},
month = jan,
pages = {133–143},
numpages = {11},
keywords = {ANN, Fault proneness, Coupling, Fault prediction, Software metrics}
}

@article{10.1145/2347696.2347709,
author = {Rashid, Ekbal and Patnayak, Srikanta and Bhattacherjee, Vandana},
title = {A survey in the area of machine learning and its application for software quality prediction},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2347696.2347709},
doi = {10.1145/2347696.2347709},
abstract = {This paper explores software quality improvement through early prediction of error patterns. It summarizes a variety of techniques for software quality prediction in the domain of software engineering. The objective of this research is to apply the various machine learning approaches, such as Case-Based Reasoning and Fuzzy logic, to predict software quality. The system predicts the error after accepting the values of certain parameters of the software. This paper advocates the use of case-based reasoning (i.e., CBR) to build a software quality prediction system with the help of human experts. The prediction is based on analogy. We have used different similarity measures to find the best method that increases reliability. This software is compiled using Turbo C++ 3.0 and hence it is very compact and standalone. It can be readily deployed on any configuration without affecting its performance.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–7},
numpages = {7},
keywords = {software quality, similarity, machine learning, function, erffort estimation, analogy, CBR}
}

@article{10.1023/A:1024424811345,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Fault Prediction Modeling for Software Quality Estimation: Comparing Commonly Used Techniques},
year = {2003},
issue_date = {September 2003},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1023/A:1024424811345},
doi = {10.1023/A:1024424811345},
abstract = {High-assurance and complex mission-critical software systems are heavily dependent on reliability of their underlying software applications. An early software fault prediction is a proven technique in achieving high software reliability. Prediction models based on software metrics can predict number of faults in software modules. Timely predictions of such models can be used to direct cost-effective quality enhancement efforts to modules that are likely to have a high number of faults. We evaluate the predictive performance of six commonly used fault prediction techniques: CART-LS (least squares), CART-LAD (least absolute deviation), S-PLUS, multiple linear regression, artificial neural networks, and case-based reasoning. The case study consists of software metrics collected over four releases of a very large telecommunications system. Performance metrics, average absolute and average relative errors, are utilized to gauge the accuracy of different prediction models. Models were built using both, original software metrics (RAW) and their principle components (PCA). Two-way ANOVA randomized-complete block design models with two blocking variables are designed with average absolute and average relative errors as response variables. System release and the model type (RAW or PCA) form the blocking variables and the prediction technique is treated as a factor. Using multiple-pairwise comparisons, the performance order of prediction models is determined. We observe that for both average absolute and average relative errors, the CART-LAD model performs the best while the S-PLUS model is ranked sixth.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {255–283},
numpages = {29},
keywords = {software metrics, neural networks, multiple linear regression, fault prediction, case-based reasoning, Software quality prediction, S-PLUS, CART}
}

@inproceedings{10.1145/3416506.3423578,
author = {Fan, Ming and Jia, Ang and Liu, Jingwen and Liu, Ting and Chen, Wei},
title = {When representation learning meets software analysis},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423578},
doi = {10.1145/3416506.3423578},
abstract = {In recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). Especially, representation learning, which can learn vectors from the syntactic and semantics of the code, offers much convenience and promotion for the downstream tasks such as code search and vulnerability detection. In this work, we introduce our two applications of leveraging representation learning for software analysis, including defect prediction and vulnerability detection.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {17–18},
numpages = {2},
keywords = {vulnerability detection, representation learning, defect prediction},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@inproceedings{10.1145/3205651.3208267,
author = {Xuan, Jifeng and Gu, Yongfeng and Ren, Zhilei and Jia, Xiangyang and Fan, Qingna},
title = {Genetic configuration sampling: learning a sampling strategy for fault detection of configurable systems},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208267},
doi = {10.1145/3205651.3208267},
abstract = {A highly-configurable system provides many configuration options to diversify application scenarios. The combination of these configuration options results in a large search space of configurations. This makes the detection of configuration-related faults extremely hard. Since it is infeasible to exhaust every configuration, several methods are proposed to sample a subset of all configurations to detect hidden faults. Configuration sampling can be viewed as a process of repeating a pre-defined sampling action to the whole search space, such as the one-enabled or pair-wise strategy.In this paper, we propose genetic configuration sampling, a new method of learning a sampling strategy for configuration-related faults. Genetic configuration sampling encodes a sequence of sampling actions as a chromosome in the genetic algorithm. Given a set of known configuration-related faults, genetic configuration sampling evolves the sequence of sampling actions and applies the learnt sequence to new configuration data. A pilot study on three highly-configurable systems shows that genetic configuration sampling performs well among nine sampling strategies in comparison.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1624–1631},
numpages = {8},
keywords = {software configurations, highly-configurable systems, genetic improvement, fault detection, configuration sampling},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1016/j.eswa.2010.08.005,
author = {El-Sebakhy, Emad A.},
title = {Functional networks as a novel data mining paradigm in forecasting software development efforts},
year = {2011},
issue_date = {March, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.08.005},
doi = {10.1016/j.eswa.2010.08.005},
abstract = {This paper proposes a new intelligence paradigm scheme to forecast that emphasizes on numerous software development elements based on functional networks forecasting framework. The most common methods for estimating software development efforts that have been proposed in literature are: line of code (LOC)-based constructive cost model (COCOMO), function point (FP) based on neural networks, regression, and case-based reasoning (CBR). Unfortunately, such forecasting models have numerous of drawbacks, namely, their inability to deal with uncertainties and imprecision present in software projects early in the development life-cycle. The main benefit of this study is to utilize both function points and development environments of recent software development cases prominent, which have high impact on the success of software development projects. Both implementation and learning process are briefly proposed. We investigate the efficiency of the new framework for predicting the software development efforts using both simulation and COCOMO real-life databases. Prediction accuracy of the functional networks framework is evaluated and compared with the commonly used regression and neural networks-based models. The results show that the new intelligence paradigm predicts the required efforts of the initial stage of software development with reliable performance and outperforms both regression and neural networks-based models.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {2187–2194},
numpages = {8},
keywords = {Type I fuzzy logic inference system, Support vector machines, Software development effort, Regression, Neural networks, Functional networks, Data mining}
}

@article{10.1007/s10664-021-09944-w,
author = {Riom, Timoth\'{e} and Sawadogo, Arthur and Allix, Kevin and Bissyand\'{e}, Tegawend\'{e} F. and Moha, Naouel and Klein, Jacques},
title = {Revisiting the VCCFinder approach for the identification of vulnerability-contributing commits},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09944-w},
doi = {10.1007/s10664-021-09944-w},
abstract = {Detecting vulnerabilities in software is a constant race between development teams and potential attackers. While many static and dynamic approaches have focused on regularly analyzing the software in its entirety, a recent research direction has focused on the analysis of changes that are applied to the code. VCCFinder is a seminal approach in the literature that builds on machine learning to automatically detect whether an incoming commit will introduce some vulnerabilities. Given the influence of VCCFinder in the literature, we undertake an investigation into its performance as a state-of-the-art system. To that end, we propose to attempt a replication study on the VCCFinder supervised learning approach. The insights of our failure to replicate the results reported in the original publication informed the design of a new approach to identify vulnerability-contributing commits based on a semi-supervised learning technique with an alternate feature set. We provide all artefacts and a clear description of this approach as a new reproducible baseline for advancing research on machine learning-based identification of vulnerability-introducing commits.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {30},
keywords = {Software engineering, Replication, Machine learning, Vulnerability detection}
}

@inproceedings{10.1145/1370788.1370794,
author = {Watanabe, Shinya and Kaiya, Haruhiko and Kaijiri, Kenji},
title = {Adapting a fault prediction model to allow inter languagereuse},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370794},
doi = {10.1145/1370788.1370794},
abstract = {An important step in predicting error prone modules in a project is to construct the prediction model by using training data of that project, but the resulting prediction model depends on the training data. Therefore it is difficult to apply the model to other projects. The training data consists of metrics data and bug data, and these data should be prepared for each project. Metrics data can be computed by using metric tools, but it is not so easy to collect bug data. In this paper, we try to reuse the generated prediction model. By using the metrics and bug data which are computed from C++ and Java projects, we have evaluated the possibility of applying the prediction model, which is generated based on one project, to other projects, and have proposed compensation techniques for applying to other projects. We showed the evaluation result based on open source projects.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {error prone, inter language prediction, metrics, open source},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@inproceedings{10.1007/978-3-662-54494-5_5,
author = {Ara\'{u}jo, Cristiano Werner and Nunes, Ingrid and Nunes, Daltro},
title = {On the Effectiveness of Bug Predictors with Procedural Systems: A Quantitative Study},
year = {2017},
isbn = {9783662544938},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-54494-5_5},
doi = {10.1007/978-3-662-54494-5_5},
abstract = {Many bug predictors have been proposed, and their main target is object-oriented systems. Although object-orientation is currently the choice for most of the software applications, the procedural paradigm is still being used in many--sometimes crucial--applications, such as operating systems and embedded systems. Consequently, they also deserve attention. We present a study in which we investigated the effectiveness of existing bug prediction approaches with procedural systems. Such approaches use as input static code metrics. We evaluated to what extent they are applicable to our context, and compared their effectiveness using standard metrics, with adaptations when needed. We assessed five approaches, using eight procedural software systems, including open-source and industrial projects. We concluded that lines of code is the metric that plays the key role in our context, and approaches that use of a large set of metrics can introduce noise in the prediction model. In addition, the best results were obtained with open-source systems.},
booktitle = {Proceedings of the 20th International Conference on Fundamental Approaches to Software Engineering - Volume 10202},
pages = {78–95},
numpages = {18},
keywords = {Static code metrics, Procedural programming, Bug prediction}
}

@inproceedings{10.5555/2394450.2394484,
author = {Catal, Cagatay and Diri, Banu},
title = {Software fault prediction with object-oriented metrics based artificial immune recognition system},
year = {2007},
isbn = {3540734597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software testing is a time-consuming and expensive process. Software fault prediction models are used to identify fault-prone classes automatically before system testing. These models can reduce the testing duration, project risks, resource and infrastructure costs. In this study, we propose a novel fault prediction model to improve the testing process. Chidamber-Kemerer Object-Oriented metrics and method-level metrics such as Halstead and McCabe are used as independent metrics in our Artificial Immune Recognition System based model. According to this study, class-level metrics based model which applies AIRS algorithm can be used successfully for fault prediction and its performance is higher than J48 based approach. A fault prediction tool which uses this model can be easily integrated into the testing process.},
booktitle = {Proceedings of the 8th International Conference on Product-Focused Software Process Improvement},
pages = {300–314},
numpages = {15},
location = {Riga, Latvia},
series = {PROFES'07}
}

@article{10.1016/j.eswa.2017.04.014,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {Towards an ensemble based system for predicting the number of software faults},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {82},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.04.014},
doi = {10.1016/j.eswa.2017.04.014},
abstract = {Paper presents ensemble based system for the prediction of number of software faults.System is based on the heterogeneous ensemble method.System uses three fault prediction techniques as base learners for the ensemble.Results are verified on Eclipse datasets. Software fault prediction using different techniques has been done by various researchers previously. It is observed that the performance of these techniques varied from dataset to dataset, which make them inconsistent for fault prediction in the unknown software project. On the other hand, use of ensemble method for software fault prediction can be very effective, as it takes the advantage of different techniques for the given dataset to come up with better prediction results compared to individual technique. Many works are available on binary class software fault prediction (faulty or non-faulty prediction) using ensemble methods, but the use of ensemble methods for the prediction of number of faults has not been explored so far. The objective of this work is to present a system using the ensemble of various learning techniques for predicting the number of faults in given software modules. We present a heterogeneous ensemble method for the prediction of number of faults and use a linear combination rule and a non-linear combination rule based approaches for the ensemble. The study is designed and conducted for different software fault datasets accumulated from the publicly available data repositories. The results indicate that the presented system predicted number of faults with higher accuracy. The results are consistent across all the datasets. We also use prediction at level l (Pred(l)), and measure of completeness to evaluate the results. Pred(l) shows the number of modules in a dataset for which average relative error value is less than or equal to a threshold value l. The results of prediction at level l analysis and measure of completeness analysis have also confirmed the effectiveness of the presented system for the prediction of number of faults. Compared to the single fault prediction technique, ensemble methods produced improved performance for the prediction of number of software faults. Main impact of this work is to allow better utilization of testing resources helping in early and quick identification of most of the faults in the software system.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {357–382},
numpages = {26},
keywords = {Software fault prediction techniques, Promise repository, Linear regression, Gradient boosting, Genetic programming, Empirical study}
}

@inproceedings{10.5555/2035637.2035644,
author = {Cayci, Aysegul and Eibe, Santiago and Menasalvas, Ernestina and Saygin, Yucel},
title = {Bayesian networks to predict data mining algorithm behavior in ubiquitous computing environments},
year = {2010},
isbn = {9783642235986},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The growing demand of data mining services for ubiquitous computing environments necessitates deployment of appropriate mechanisms that make use of circumstantial factors to adapt the data mining behavior. Despite the efforts and results so far for efficient parameter tuning, incorporating dynamically changing context information on the parameter setting decision is lacking in the present work. Thus, Bayesian networks are used to learn, in possible situations the effects of data mining algorithm parameters on the final model obtained. Based on this knowledge, we propose to infer future algorithm configurations appropriate for situations. Instantiation of the approach for association rules is also shown in the paper and the feasibility of the approach is validated by the experimentation.},
booktitle = {Proceedings of the 2010 International Conference on Analysis of Social Media and Ubiquitous Data},
pages = {119–141},
numpages = {23},
keywords = {ubiquitous data mining, data mining configuration, automatic data mining},
location = {Toronto, ON, Canada},
series = {MSM'10/MUSE'10}
}

@article{10.1016/j.csi.2009.11.009,
author = {Arpaia, Pasquale and Bernardi, Mario Luca and Di Lucca, Giuseppe and Inglese, Vitaliano and Spiezia, Giovanni},
title = {An Aspect-Oriented Programming-based approach to software development for fault detection in measurement systems},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {32},
number = {3},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2009.11.009},
doi = {10.1016/j.csi.2009.11.009},
abstract = {An Aspect-Oriented Programming-based approach to the development of software components for fault detection in automatic measurement systems is proposed. Faults are handled by means of specific software units, the ''aspects'', in order to better modularize issues transversal to several components. As a case study, this approach was applied to the design of the fault detection software inside a flexible framework for magnetic measurements, developed at the European Organization for Nuclear Research (CERN). Experimental results of software modularity and performance measurements for comparing aspect- and object-oriented solutions in rotating coils tests on superconducting magnets are reported.},
journal = {Comput. Stand. Interfaces},
month = mar,
pages = {141–152},
numpages = {12},
keywords = {Software development, Fault detection, Automatic measurement systems, Aspect-Oriented Programming}
}

@article{10.1504/IJAACS.2008.019811,
author = {Simao, Adenilso S. and Mello, Rodrigo F. De and Senger, Luciano J. and Yang, Laurence T.},
title = {Improving regression testing performance using the Adaptive Resonance Theory-2A self-organising neural network architecture},
year = {2008},
issue_date = {August 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {1},
number = {3},
issn = {1754-8632},
url = {https://doi.org/10.1504/IJAACS.2008.019811},
doi = {10.1504/IJAACS.2008.019811},
abstract = {Regression testing applies a previously developed test case suite to new software versions. A traditional approach is the execution of all test cases, although, this may be time consuming and, sometimes, not necessary as the source code modification may affect only a test case subset. Some initiatives have addressed this issue. For instance, one of the most promising ones is the modified-based technique that selects test cases based on whether they execute the modified parts of the program. This technique is conservative, but it often selects test cases that are not relevant. This article presents an approach to select test case subsets by using an Adaptive Resonance Theory-2A self-organising neural network architecture. In this approach, test cases are summarised in feature vectors with code coverage information, which are classified by the neural network in clusters. Clusters are labelled representing each software functionality evaluated by the coverage criterion. A new software version is then analysed to determine modified points and, then, clusters, which represent the related functionalities, are chosen. The test case subset is obtained from these clusters. Experiments were conducted to evaluate the approach using feature vectors based on all-uses and -nodes code coverage information against a modification-based technique.},
journal = {Int. J. Auton. Adapt. Commun. Syst.},
month = aug,
pages = {370–385},
numpages = {16},
keywords = {test classification, regression testing, new software versions, neural networks, clustering, adaptive resonance theory, ART}
}

@inproceedings{10.1145/1029894.1029911,
author = {Zitser, Misha and Lippmann, Richard and Leek, Tim},
title = {Testing static analysis tools using exploitable buffer overflows from open source code},
year = {2004},
isbn = {1581138555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029894.1029911},
doi = {10.1145/1029894.1029911},
abstract = {Five modern static analysis tools (ARCHER, BOON, Poly-Space C Verifier, Splint, and UNO) were evaluated using source code examples containing 14 exploitable buffer overflow vulnerabilities found in various versions of Sendmail, BIND, and WU-FTPD. Each code example included a "BAD" case with and a "OK" case without buffer overflows. Buffer overflows varied and included stack, heap, bss and data buffers; access above and below buffer bounds; access using pointers, indices, and functions; and scope differences between buffer creation and use. Detection rates for the "BAD" examples were low except for Poly-Space and Splint which had average detection rates of 87% and 57%, respectively. However, average false alarm rates were high and roughly 50% for these two tools. On patched programs these two tools produce one warning for every 12 to 46 lines of source code and neither tool appears able to accurately distinguished between vulnerable and patched code.},
booktitle = {Proceedings of the 12th ACM SIGSOFT Twelfth International Symposium on Foundations of Software Engineering},
pages = {97–106},
numpages = {10},
keywords = {test detection, static analysis, source code, security, false alarm, exploit, evaluation, buffer overflow},
location = {Newport Beach, CA, USA},
series = {SIGSOFT '04/FSE-12}
}

@article{10.1145/2803171,
author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
title = {Boa: Ultra-Large-Scale Software Repository and Source-Code Mining},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2803171},
doi = {10.1145/2803171},
abstract = {In today's software-centric world, ultra-large-scale software repositories, such as SourceForge, GitHub, and Google Code, are the new library of Alexandria. They contain an enormous corpus of software and related information. Scientists and engineers alike are interested in analyzing this wealth of information. However, systematic extraction and analysis of relevant data from these repositories for testing hypotheses is hard, and best left for mining software repository (MSR) experts! Specifically, mining source code yields significant insights into software development artifacts and processes. Unfortunately, mining source code at a large scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse grained, or sacrifice studying the history of the code. In this article we address mining source code: (a) at a very large scale; (b) at a fine-grained level of detail; and (c) with full history information. To address these challenges, we present domain-specific language features for source-code mining in our language and infrastructure called Boa. The goal of Boa is to ease testing MSR-related hypotheses. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also show drastic improvements in scalability.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {7},
numpages = {34},
keywords = {scalable, mining software repositories, lower barrier to entry, ease of use, domain-specific language, Boa}
}

@inproceedings{10.1145/956750.956795,
author = {Last, Mark and Friedman, Menahem and Kandel, Abraham},
title = {The data mining approach to automated software testing},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956795},
doi = {10.1145/956750.956795},
abstract = {In today's industry, the design of software tests is mostly based on the testers' expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. The costs of the so-called "catastrophic" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs when testing new, potentially flawed releases of the system. To study the feasibility of the proposed approach, we have applied a novel data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a general-purpose code for solving partial differential equations. After being trained on a relatively small number of randomly generated input-output examples, the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {388–396},
numpages = {9},
keywords = {regression testing, input-output analysis, info-fuzzy networks, finite element solver, automated software testing},
location = {Washington, D.C.},
series = {KDD '03}
}

@article{10.1007/s10664-008-9082-8,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
title = {Do too many cooks spoil the broth? Using the number of developers to enhance defect prediction models},
year = {2008},
issue_date = {October   2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9082-8},
doi = {10.1007/s10664-008-9082-8},
abstract = {Fault prediction by negative binomial regression models is shown to be effective for four large production software systems from industry. A model developed originally with data from systems with regularly scheduled releases was successfully adapted to a system without releases to identify 20% of that system's files that contained 75% of the faults. A model with a pre-specified set of variables derived from earlier research was applied to three additional systems, and proved capable of identifying averages of 81, 94 and 76% of the faults in those systems. A primary focus of this paper is to investigate the impact on predictive accuracy of using data about the number of developers who access individual code units. For each system, including the cumulative number of developers who had previously modified a file yielded no more than a modest improvement in predictive accuracy. We conclude that while many factors can "spoil the broth" (lead to the release of software with too many defects), the number of developers is not a major influence.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {539–559},
numpages = {21},
keywords = {Software faults, Negative binomial model, Empirical study, Developer counts}
}

@inproceedings{10.1145/3416505.3423564,
author = {Borovits, Nemania and Kumara, Indika and Krishnan, Parvathy and Palma, Stefano Dalla and Di Nucci, Dario and Palomba, Fabio and Tamburri, Damian A. and van den Heuvel, Willem-Jan},
title = {DeepIaC: deep learning-based linguistic anti-pattern detection in IaC},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423564},
doi = {10.1145/3416505.3423564},
abstract = {Linguistic anti-patterns are recurring poor practices concerning inconsistencies among the naming, documentation, and implementation of an entity. They impede readability, understandability, and maintainability of source code. This paper attempts to detect linguistic anti-patterns in infrastructure as code (IaC) scripts used to provision and manage computing environments. In particular, we consider inconsistencies between the logic/body of IaC code units and their names. To this end, we propose a novel automated approach that employs word embeddings and deep learning techniques. We build and use the abstract syntax tree of IaC code units to create their code embedments. Our experiments with a dataset systematically extracted from open source repositories show that our approach yields an accuracy between 0.785 and 0.915 in detecting inconsistencies.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {Word2Vec, Linguistic Anti-patterns, Infrastructure Code, IaC, Defects, Deep Learning, Code Embedding},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/3106237.3106258,
author = {Wang, Song and Nam, Jaechang and Tan, Lin},
title = {QTEP: quality-aware test case prioritization},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106258},
doi = {10.1145/3106237.3106258},
abstract = {Test case prioritization (TCP) is a practical activity in software testing for exposing faults earlier. Researchers have proposed many TCP techniques to reorder test cases. Among them, coverage-based TCPs have been widely investigated. Specifically, coverage-based TCP approaches leverage coverage information between source code and test cases, i.e., static code coverage and dynamic code coverage, to schedule test cases. Existing coverage-based TCP techniques mainly focus on maximizing coverage while often do not consider the likely distribution of faults in source code. However, software faults are not often equally distributed in source code, e.g., around 80% faults are located in about 20% source code. Intuitively, test cases that cover the faulty source code should have higher priorities, since they are more likely to find faults.  In this paper, we present a quality-aware test case prioritization technique, QTEP, to address the limitation of existing coverage-based TCP algorithms. In QTEP, we leverage code inspection techniques, i.e., a typical statistic defect prediction model and a typical static bug finder, to detect fault-prone source code and then adapt existing coverage-based TCP algorithms by considering the weighted source code in terms of fault-proneness. Our evaluation with 16 variant QTEP techniques on 33 different versions of 7 open source Java projects shows that QTEP could improve existing coverage-based TCP techniques for both regression and new test cases. Specifically, the improvement of the best variant of QTEP for regression test cases could be up to 15.0% and on average 7.6%, and for all test cases (both regression and new test cases), the improvement could be up to 10.0% and on average 5.0%.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {523–534},
numpages = {12},
keywords = {static bug finder, defect prediction, Test case prioritization},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3387904.3389255,
author = {Roy, Devjeet and Fakhoury, Sarah and Lee, John and Arnaoudova, Venera},
title = {A Model to Detect Readability Improvements in Incremental Changes},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389255},
doi = {10.1145/3387904.3389255},
abstract = {Identifying source code that has poor readability allows developers to focus maintenance efforts on problematic code. Therefore, the effort to develop models that can quantify the readability of a piece of source code has been an area of interest for software engineering researchers for several years. However, recent research questions the usefulness of these readability models in practice. When applying these models to readability improvements that are made in practice, i.e., commits, they are unable to capture these incremental improvements, despite a clear perceived improvement by the developers. This results in a discrepancy between the models we have built to measure readability, and the actual perception of readability in practice.In this work, we propose a model that is able to detect incremental readability improvements made by developers in practice with an average precision of 79.2% and an average recall of 67% on an unseen test set. We then investigate the metrics that our model associates with developer perceived readability improvements as well as non-readability changes. Finally, we compare our model to existing state-of-the-art readability models, which our model outperforms by at least 23% in terms of precision and 42% in terms of recall.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {25–36},
numpages = {12},
keywords = {Source code readability, Machine learning, Code quality},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1109/ASE.2019.00090,
author = {Mu, Dongliang and Guo, Wenbo and Cuevas, Alejandro and Chen, Yueqi and Gai, Jinxuan and Xing, Xinyu and Mao, Bing and Song, Chengyu},
title = {RENN: efficient reverse execution with neural-network-assisted alias analysis},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00090},
doi = {10.1109/ASE.2019.00090},
abstract = {Reverse execution and coredump analysis have long been used to diagnose the root cause of software crashes. Each of these techniques, however, face inherent challenges, such as insufficient capability when handling memory aliases. Recent works have used hypothesis testing to address this drawback, albeit with high computational complexity, making them impractical for real world applications. To address this issue, we propose a new deep neural architecture, which could significantly improve memory alias resolution. At the high level, our approach employs a recurrent neural network (RNN) to learn the binary code pattern pertaining to memory accesses. It then infers the memory region accessed by memory references. Since memory references to different regions naturally indicate a non-alias relationship, our neural architecture can greatly reduce the burden of doing hypothesis testing to track down non-alias relation in binary code.Different from previous researches that have utilized deep learning for other binary analysis tasks, the neural network proposed in this work is fundamentally novel. Instead of simply using off-the-shelf neural networks, we designed a new recurrent neural architecture that could capture the data dependency between machine code segments.To demonstrate the utility of our deep neural architecture, we implement it as RENN, a neural network-assisted reverse execution system. We utilize this tool to analyze software crashes corresponding to 40 memory corruption vulnerabilities from the real world. Our experiments show that RENN can significantly improve the efficiency of locating the root cause for the crashes. Compared to a state-of-the-art technique, RENN has 36.25% faster execution time on average, detects an average of 21.35% more non-alias pairs, and successfully identified the root cause of 12.5% more cases.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {924–935},
numpages = {12},
keywords = {reverse execution, memory alias, deep learning},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1016/j.knosys.2016.12.017,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {Linear and non-linear heterogeneous ensemble methods to predict the number of faults in software systems},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {119},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.12.017},
doi = {10.1016/j.knosys.2016.12.017},
abstract = {This paper expands the use of ensemble methods for the prediction of number of faults unlikely the earlier works on ensemble methods that focused on predicting software modules as faulty or non-faulty.This paper investigates the usage of both heterogeneous ensemble methods as well as homogeneous ensemble methods for the prediction of number of faults.We present two linear combination rules and two non-linear combination rules for combining the outputs of the base learners in the ensemble.In addition, we assess the performance of ensemble methods under two different scenarios, intra-release prediction and inter-releases prediction.The experiments are performed over five open-source software systems with their fifteen releases, collected from the PROMISE data repository. Several classification techniques have been investigated and evaluated earlier for the software fault prediction. These techniques have produced different prediction accuracy for the different software systems and none of the technique has always performed consistently better across different domains. On the other hand, software fault prediction using ensemble methods can be very effective, as they take the advantage of each participating technique for the given dataset and try to come up with better prediction results compared to the individual techniques. Many works are available for classifying software modules being faulty or non-faulty using the ensemble methods. These works are only specifying that whether a given software module is faulty or not, but number of faults in that module are not predicted by them. The use of ensemble methods for the prediction of number of faults has not been explored so far. To fulfill this gap, this paper presents ensemble methods for the prediction of number of faults in the given software modules. The experimental study is designed and conducted for five open-source software projects with their fifteen releases, collected from the PROMISE data repository. The results are evaluated under two different scenarios, intra-release prediction and inter-releases prediction. The prediction accuracy of ensemble methods is evaluated using absolute error, relative error, prediction at level l, and measure of completeness performance measures. Results show that the presented ensemble methods yield improved prediction accuracy over the individual fault prediction techniques under consideration. Further, the results are consistent for all the used datasets. The evidences obtained from the prediction at level l and measure of completeness analysis have also confirmed the effectiveness of the proposed ensemble methods for predicting the number of faults.},
journal = {Know.-Based Syst.},
month = mar,
pages = {232–256},
numpages = {25},
keywords = {Software fault prediction, Prediction of number of faults, Heterogeneous ensemble, Ensemble methods}
}

@inproceedings{10.5555/851042.857045,
author = {Tjortjis, Christos and Sinos, Loukas and Layzell, Paul},
title = {Facilitating Program Comprehension by Mining Association Rules from Source Code},
year = {2003},
isbn = {0769518834},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Program comprehension is an important part of software maintenance, especially when program structure is complex and documentation is unavailable or outdated. Data mining can produce structural views of source code thus facilitating legacy systems understanding.This paper presents a method for mining association rules from code aiming at capturing program structure and achieving better system understanding. A tool was implemented to assess this method. It inputs data extracted from code and derives association rules. Rules are then processed to abstract programs into groups containing interrelated entities. Entities are grouped together if their attributes participate in common rules. The abstraction is performed at the function level, in contrast to other approaches, that work at the program level.The method was evaluated using real, working programs. Programs are fed into a code analyser which produces the input needed for the mining tool. Results show that the method facilitates program comprehension by only using source code where domain knowledge andreliable documentation are not available or reliable.},
booktitle = {Proceedings of the 11th IEEE International Workshop on Program Comprehension},
pages = {125},
series = {IWPC '03}
}

@article{10.5555/1534462.1534465,
author = {Lazic, Ljubomir and Kola\v{s};inac, Amel and Avdic, D\v{z}enan},
title = {The software quality economics model for software project optimization},
year = {2009},
issue_date = {January 2009},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {8},
number = {1},
issn = {1109-2750},
abstract = {There are many definitions of quality being given by experts that explains quality for manufacturing industry but still unable to define it with absolute clarity for software engineering. To enable software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices is given. In this paper we propose a model which traces design decisions and the possible alternatives. With this model it is possible to minimize the cost of switching between design alternatives, when the current choice cannot fulfill the quality constraints. With this model we do not aim to automate the software design process or the identification of design alternatives. Much rather we aim to define a method with which it is possible to assist the software engineer in evaluating design alternatives and adjusting design decisions in a systematic manner. As of today there is very little knowledge is available about the economics of software quality. The costs incurred and benefits of implementing different quality practices over the software development life cycle are not well understood. There are some prepositions, which are not being tested comprehensively, but some useful Economic Model of Software Quality Costs (CoSQ) and data from industry are described in this article. Significant research is needed to understand the economics of implementing quality practices and its behaviour. Such research must evaluate the cost benefit trade-offs in investing in quality practices where the returns are maximized over the software development life cycle. From a developer's perspective, there are two types of benefits that can accrue from the implementation of good software quality practices and tools: money and time. A financial ROI calculation of cost savings and the schedule ROI calculation of schedule savings are given.},
journal = {W. Trans. on Comp.},
month = jan,
pages = {21–47},
numpages = {27},
keywords = {software quality, quality cost model, cost optimization, TQM, ROI calculation}
}

@inproceedings{10.1007/978-3-030-47426-3_61,
author = {Perera, Kushani and Chan, Jeffrey and Karunasekera, Shanika},
title = {A Framework for Feature Selection to Exploit Feature Group Structures},
year = {2020},
isbn = {978-3-030-47425-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47426-3_61},
doi = {10.1007/978-3-030-47426-3_61},
abstract = {Filter feature selection methods play an important role in machine learning tasks when low computational costs, classifier independence or simplicity is important. Existing filter methods predominantly focus only on the input data and do not take advantage of the external sources of correlations within feature groups to improve the classification accuracy. We propose a framework which facilitates supervised filter feature selection methods to exploit feature group information from external sources of knowledge and use this framework to incorporate feature group information into minimum Redundancy Maximum Relevance (mRMR) algorithm, resulting in GroupMRMR algorithm. We show that GroupMRMR achieves high accuracy gains over mRMR (up&nbsp;to 35%) and other popular filter methods (up&nbsp;to 50%). GroupMRMR has same computational complexity as that of mRMR, therefore, does not incur additional computational costs. Proposed method has many real world applications, particularly the ones that use genomic, text and image data whose features demonstrate strong group structures.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
pages = {792–804},
numpages = {13},
keywords = {Squared [inline-graphic not available: see fulltext] norm minimisation, Feature groups, Filter feature selection},
location = {Singapore, Singapore}
}

@article{10.1016/j.infsof.2009.06.006,
author = {Briand, Lionel C. and Labiche, Yvan and Bawar, Zaheer and Spido, Nadia Traldi},
title = {Using machine learning to refine Category-Partition test specifications and test suites},
year = {2009},
issue_date = {November, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.06.006},
doi = {10.1016/j.infsof.2009.06.006},
abstract = {In the context of open source development or software evolution, developers often face test suites which have been developed with no apparent rationale and which may need to be augmented or refined to ensure sufficient dependability, or even reduced to meet tight deadlines. We refer to this process as the re-engineering of test suites. It is important to provide both methodological and tool support to help people understand the limitations of test suites and their possible redundancies, so as to be able to refine them in a cost effective manner. To address this problem in the case of black-box, Category-Partition testing, we propose a methodology and a tool based on machine learning that has shown promising results on a case study involving students as testers.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1551–1564},
numpages = {14},
keywords = {Test improvement, Machine learning, Category-Partition, Black box testing}
}

@article{10.1007/s10115-021-01560-w,
author = {Brzezinski, Dariusz and Minku, Leandro L. and Pewinski, Tomasz and Stefanowski, Jerzy and Szumaczuk, Artur},
title = {The impact of data difficulty factors on classification of imbalanced and concept drifting data streams},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {6},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-021-01560-w},
doi = {10.1007/s10115-021-01560-w},
abstract = {Class imbalance introduces additional challenges when learning classifiers from concept drifting data streams. Most existing work focuses on designing new algorithms for dealing with the global imbalance ratio and does not consider other data complexities. Independent research on static imbalanced data has highlighted the influential role of local data difficulty factors such as minority class decomposition and presence of unsafe types of examples. Despite often being present in real-world data, the interactions between concept drifts and local data difficulty factors have not been investigated in concept drifting data streams yet. We thoroughly study the impact of such interactions on drifting imbalanced streams. For this purpose, we put forward a new categorization of concept drifts for class imbalanced problems. Through comprehensive experiments with synthetic and real data streams, we study the influence of concept drifts, global class imbalance, local data difficulty factors, and their combinations, on predictions of representative online classifiers. Experimental results reveal the high influence of new considered factors and their local drifts, as well as differences in existing classifiers’ reactions to such factors. Combinations of multiple factors are the most challenging for classifiers. Although existing classifiers are partially capable of coping with global class imbalance, new approaches are needed to address challenges posed by imbalanced data streams.},
journal = {Knowl. Inf. Syst.},
month = jun,
pages = {1429–1469},
numpages = {41},
keywords = {Stream classification, Drift categorization, Data difficulty factors, Concept drift, Class imbalance}
}

@article{10.1016/j.jss.2009.06.036,
author = {Binkley, David and Feild, Henry and Lawrie, Dawn and Pighin, Maurizio},
title = {Increasing diversity: Natural language measures for software fault prediction},
year = {2009},
issue_date = {November, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.036},
doi = {10.1016/j.jss.2009.06.036},
abstract = {While challenging, the ability to predict faulty modules of a program is valuable to a software project because it can reduce the cost of software development, as well as software maintenance and evolution. Three language-processing based measures are introduced and applied to the problem of fault prediction. The first measure is based on the usage of natural language in a program's identifiers. The second measure concerns the conciseness and consistency of identifiers. The third measure, referred to as the QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgments of software quality. Two case studies consider the language processing measures applicability to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and the measures. Results, while complex, show that language processing measures improve fault prediction, especially when used in combination. Overall, the models explain one-third and two-thirds of the faults in the two case studies. Consistent with other uses of language processing, the value of the three measures increases with the size of the program module considered.},
journal = {J. Syst. Softw.},
month = nov,
pages = {1793–1803},
numpages = {11},
keywords = {Linear regression models, Information retrieval, Fault prediction, Empirical software engineering, Code comprehension}
}

@inproceedings{10.5555/3524938.3525774,
author = {Sipple, John},
title = {Interpretable, multidimensional, multimodal anomaly detection with negative sampling for detection of device failure},
year = {2020},
publisher = {JMLR.org},
abstract = {In this paper we propose a scalable, unsupervised approach for detecting anomalies in the Internet of Things (IoT). Complex devices are connected daily and eagerly generate vast streams of multi-dimensional telemetry. These devices often operate in distinct modes based on external conditions (day/night, occupied/vacant, etc.), and to prevent complete or partial system outage, we would like to recognize as early as possible when these devices begin to operate outside the normal modes. We propose an unsupervised anomaly detection method that creates a negative sample from the positive, observed sample, and trains a classifier to distinguish between positive and negative samples. Using the Concentration Phenomenon, we explain why such a classifier ought to establish suitable decision boundaries between normal and anomalous regions, and show how Integrated Gradients can attribute the anomaly to specific dimensions within the anomalous state vector. We have demonstrated that negative sampling with random forest or neural network classifiers yield significantly higher AUC scores compared to state-of-the-art approaches against benchmark anomaly detection datasets, and a multidimensional, multimodal dataset from real climate control devices. Finally, we describe how negative sampling with neural network classifiers have been successfully deployed at large scale to predict failures in real time in over 15,000 climate-control and power meter devices in 145 office buildings within the California Bay Area.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {836},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.jss.2008.04.007,
author = {Binkley, David and Gold, Nicolas and Harman, Mark and Li, Zheng and Mahdavi, Kiarash},
title = {An empirical study of the relationship between the concepts expressed in source code and dependence},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.04.007},
doi = {10.1016/j.jss.2008.04.007},
abstract = {Programs express domain-level concepts in their source code. It might be expected that such concepts would have a degree of semantic cohesion. This cohesion ought to manifest itself in the dependence between statements all of which contribute to the computation of the same concept. This paper addresses a set of research questions that capture this informal observation. It presents the results of experiments on 10 programs that explore the relationship between domain-level concepts and dependence in source code. The results show that code associated with concepts has a greater degree of coherence, with tighter dependence. This finding has positive implications for the analysis of concepts as it provides an approach to decompose a program into smaller executable units, each of which captures the behaviour of the program with respect to a domain-level concept.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2287–2298},
numpages = {12},
keywords = {Software engineering, Program slicing, Program comprehension, Concept assignment}
}

@article{10.1016/j.infsof.2017.03.007,
author = {Yang, Xinli and Lo, David and Xia, Xin and Sun, Jianling},
title = {TLEL},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.007},
doi = {10.1016/j.infsof.2017.03.007},
abstract = {We propose a novel approach TLEL, which can be seen as a two-layer ensemble learning technique, to achieve a better performance for just-in-time defect prediction problem.We compare TLEL with three baselines, i.e., Deeper, DNC and MKEL, on six large software projects.The experiment results show that our approach can achieve a substantial improvement over all of them. Moreover, TLEL could discover over 70% reviewing only 20% of the lines of code. Display Omitted ContextDefect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1]. ObjectiveEnsemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [25]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work. MethodIn this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more. ResultsTo evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang etal. [2], and MKEL, the approach proposed by Wang etal. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code, as compared with about 50% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets. ConclusionTLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {206–220},
numpages = {15},
keywords = {Just-in-time defect prediction, Ensemble learning, Cost effectiveness}
}

@article{10.1007/s11334-015-0256-4,
author = {Valles-Barajas, Fernando},
title = {A comparative analysis between two techniques for the prediction of software defects: fuzzy and statistical linear regression},
year = {2015},
issue_date = {December  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-015-0256-4},
doi = {10.1007/s11334-015-0256-4},
abstract = {Software engineers should estimate the necessary resources (time, people, software tools among others) to satisfy software project requirements; this activity is carried out in the planning phase. The estimated time for developing software projects is a necessary element to establish the cost of software projects and to assign human resources to every phase of software projects. Most companies fail to finish software projects on time because of a poor estimation technique or the lack of the same. The estimated time must consider the time spent eliminating software defects injected during each of the software phases. A comparative analysis between two techniques (fuzzy linear regression and statistical linear regression) to perform software defect estimation is presented. These two techniques model uncertainty in a different way; statistical linear regression models uncertainty as randomness, whereas fuzzy linear regression models uncertainty as fuzziness. The main objective of this paper was to establish the kind of uncertainty associated with software defect prediction and to contrast these two prediction techniques. The KC1 NASA data set was used to do this analysis. Only six of the metrics included in KC1 data set and lines of code metric were used in this comparative analysis. Descriptive statistics was first used to have an overview of the main characteristics of the data set used in this research. Linearity property between predictor variables and the variable of interest number of defects was checked using scatter plots and Pearson's correlation coefficient. Then the problem of multicollinearity was verified using inter-correlations among metrics and the variance inflation factor. Best subset regression was applied to detect the most influencing subset of predictor variables; this subset was later used to build fuzzy and statistical regression models. Linearity property between metrics and number of defects was confirmed. The problem of multicollinearity was not detected in the predictor variables. Best subset regression found that the subset composed of 5 variables was the most influencing subset. The analysis showed that the statistical regression model in general outperformed the fuzzy regression model. Techniques for making software defect prediction should be carefully employed in order to have quality plans. Software engineers should consider and understand a set of prediction techniques and know their weaknesses and strengths. At least, in the KC1 data set, the uncertainty in the software defect prediction model is due to randomness so it is reasonable to use statistical linear regression instead of fuzzy linear regression to build a prediction model.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {277–287},
numpages = {11},
keywords = {Statistical linear regression, Software defect prediction, Fuzzy linear regression}
}

@inproceedings{10.5555/3507788.3507804,
author = {Ria and Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Process-metrics trends analysis for evaluating file-level error proneness},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Assessing the likelihood of a source code file being buggy or healthy in upcoming commits given its past behavior and its interaction with other files, has been an area where the software engineering community has paid significant attention over the years. Early efforts aimed on associating software metrics with maintainability indexes, while more recent efforts focused on the use of machine learning for classifying a software module as error prone or not. In most approaches to date, this analysis is primarily based on source code metrics or on information extracted from the system's source code, and to a lesser extend on information that relates to process metrics. In this paper, we propose a process-metrics based method for predicting the behavior of a file, based both on its GitHub commits and its interdependences with other co-committed files. More specifically, for each file and for each commit a file participates in, we compute a dependency score this file has with its other co-committed files. This score is appropriately amplified if the file is participating in a bug-fixing commit, or decayed over time if it does not. By examining, over several open source systems, the trend of that dependency score for every file as a product of time, for files whose outcome is known and that are used as gold standard, we report statistics which shed light on estimating the likelihood of whether these trends can predict the future behavior of a file or not.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/1188895.1188910,
author = {Di Fatta, Giuseppe and Leue, Stefan and Stegantova, Evghenia},
title = {Discriminative pattern mining in software fault detection},
year = {2006},
isbn = {1595935843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1188895.1188910},
doi = {10.1145/1188895.1188910},
abstract = {We present a method to enhance fault localization for software systems based on a frequent pattern mining algorithm. Our method is based on a large set of test cases for a given set of programs in which faults can be detected. The test executions are recorded as function call trees. Based on test oracles the tests can be classified into successful and failing tests. A frequent pattern mining algorithm is used to identify frequent subtrees in successful and failing test executions. This information is used to rank functions according to their likelihood of containing a fault. The ranking suggests an order in which to examine the functions during fault analysis. We validate our approach experimentally using a subset of Siemens benchmark programs.},
booktitle = {Proceedings of the 3rd International Workshop on Software Quality Assurance},
pages = {62–69},
numpages = {8},
keywords = {fault isolation, automated debugging},
location = {Portland, Oregon},
series = {SOQUA '06}
}

@inproceedings{10.1145/2070821.2070824,
author = {Menzies, Tim and Bird, Christian and Zimmermann, Thomas and Schulte, Wolfram and Kocaganeli, Ekrem},
title = {The inductive software engineering manifesto: principles for industrial data mining},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070824},
doi = {10.1145/2070821.2070824},
abstract = {The practices of industrial and academic data mining are very different. These differences have significant implications for (a) how we manage industrial data mining projects; (b) the direction of academic studies in data mining; and (c) training programs for engineers who seek to use data miners in an industrial setting.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {19–26},
numpages = {8},
keywords = {industry, inductive engineering},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@inproceedings{10.1109/ICSSP.2019.00011,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {Towards a knowledge warehouse and expert system for the automation of SDLC tasks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00011},
doi = {10.1109/ICSSP.2019.00011},
abstract = {Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks.Recent advances in Artificial Intelligence (AI) and the availability of a large volume of knowledge bearing data at various software development related venues present a ripe opportunity for building tools that can automate software development tasks. For instance, there is significant latent knowledge present in raw or unstructured data associated with items such as source files, code commit logs, defect reports, comments, and so on, available in the Open Source Software (OSS) repositories.We aim to leverage such knowledge-bearing data, the latest advances in AI and hardware to create knowledge warehouses and expert systems for the software development domain. Such tools can help in building applications for performing various software development tasks such as defect prediction, effort estimation, code review, etc.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {5–8},
numpages = {4},
keywords = {supervised learning, software maintenance, data mining, automated software engineering},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@article{10.1007/s10664-021-10004-6,
author = {Quach, Sophia and Lamothe, Maxime and Adams, Bram and Kamei, Yasutaka and Shang, Weiyi},
title = {Evaluating the impact of falsely detected performance bug-inducing changes in JIT models},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10004-6},
doi = {10.1007/s10664-021-10004-6},
abstract = {Performance bugs bear a heavy cost on both software developers and end-users. Tools to reduce the occurrence, impact, and repair time of performance bugs, can therefore provide key assistance for software developers racing to fix these bugs. Classification models that focus on identifying defect-prone commits, referred to as Just-In-Time (JIT) Quality Assurance are known to be useful in allowing developers to review risky commits. These commits can be reviewed while they are still fresh in developers’ minds, reducing the costs of developing high-quality software. JIT models, however, leverage the SZZ approach to identify whether or not a change is bug-inducing. The fixes to performance bugs may be scattered across the source code, separated from their bug-inducing locations. The nature of performance bugs may make SZZ a sub-optimal approach for identifying their bug-inducing commits. Yet, prior studies that leverage or evaluate the SZZ approach do not distinguish performance bugs from other bugs, leading to potential bias in the results. In this paper, we conduct an empirical study on the JIT defect prediction for performance bugs. We concentrate on SZZ’s ability to identify the bug-inducing commits of performance bugs in two open-source projects, Cassandra, and Hadoop. We verify whether the bug-inducing commits found by SZZ are truly bug-inducing commits by manually examining these identified commits. Our manual examination includes cross referencing fix commits and JIRA bug reports. We evaluate model performance for JIT models by using them to identify bug-inducing code commits for performance related bugs. Our findings show that JIT defect prediction classifies non-performance bug-inducing commits better than performance bug-inducing commits, i.e., the SZZ approach does introduce errors when identifying bug-inducing commits. However, we find that manually correcting these errors in the training data only slightly improves the models. In the absence of a large number of correctly labelled performance bug-inducing commits, our findings show that combining all available training data (i.e., truly performance bug-inducing commits, non-performance bug-inducing commits, and non-bug-inducing commits) yields the best classification results.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {32},
keywords = {Performance, Just-in-time, Defect prediction bugs, Software engineering}
}

@article{10.1016/j.jss.2021.111036,
author = {Li, Zheng and Wu, Yonghao and Peng, Bin and Chen, Xiang and Sun, Zeyu and Liu, Yong and Yu, Deli},
title = {SeCNN: A semantic CNN parser for code comment generation},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111036},
doi = {10.1016/j.jss.2021.111036},
journal = {J. Syst. Softw.},
month = nov,
numpages = {17},
keywords = {Long short-term memory network, Convolutional Neural Network, Code comment generation, Program comprehension}
}

@article{10.1504/IJDATS.2017.10003991,
title = {Software fault proneness prediction: a comparative study between bagging, boosting, and stacking ensemble and base learner methods},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {1},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2017.10003991},
doi = {10.1504/IJDATS.2017.10003991},
abstract = {Modules with defects might be the prime reason for decreasing the software quality and increasing the cost of maintenance. Therefore, the prediction of faulty modules of systems under test at early stages contributes to the overall quality of software products. In this research three symmetric ensemble methods: bagging, boosting and stacking are used to predict faulty modules based on evaluating the performance of 11 base learners. The results reveal that the defect prediction performance of the base learner classifier and ensemble learner classifiers is the same for na\"{\i}ve Bayes, Bayes net, PART, random forest, IB1, VFI, decision table, and NB tree base learners, the case was different for boosted SMO, bagged J48 and boosted and bagged random tree. In addition the results showed that the random forest classifier is one of the most significant classifiers that should be stacked with other classifiers to gain the better fault prediction.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = jan,
pages = {1–16},
numpages = {16}
}

@article{10.1155/2016/7658207,
author = {Tomar, Divya and Agarwal, Sonali},
title = {Prediction of defective software modules using class imbalance learning},
year = {2016},
issue_date = {January 2016},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2016},
issn = {1687-9724},
url = {https://doi.org/10.1155/2016/7658207},
doi = {10.1155/2016/7658207},
abstract = {Software defect predictors are useful to maintain the high quality of software products effectively. The early prediction of defective software modules can help the software developers to allocate the available resources to deliver high quality software products. The objective of software defect prediction system is to find as many defective software modules as possible without affecting the overall performance. The learning process of a software defect predictor is difficult due to the imbalanced distribution of software modules between defective and nondefective classes. Misclassification cost of defective software modules generally incurs much higher cost than the misclassification of nondefective one. Therefore, on considering the misclassification cost issue, we have developed a software defect prediction system using Weighted Least Squares Twin Support Vector Machine (WLSTSVM). This system assigns higher misclassification cost to the data samples of defective classes and lower cost to the data samples of nondefective classes. The experiments on eight software defect prediction datasets have proved the validity of the proposed defect prediction system. The significance of the results has been tested via statistical analysis performed by using nonparametric Wilcoxon signed rank test.},
journal = {Appl. Comp. Intell. Soft Comput.},
month = jan,
articleno = {6},
numpages = {1}
}

@article{10.1007/s10922-015-9348-6,
author = {Deljac, \v{Z}eljko and Randi\'{c}, Mirko and Kr\v{c}eli\'{c}, Gordan},
title = {A Multivariate Approach to Predicting Quantity of Failures in Broadband Networks Based on a Recurrent Neural Network},
year = {2016},
issue_date = {January   2016},
publisher = {Plenum Press},
address = {USA},
volume = {24},
number = {1},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-015-9348-6},
doi = {10.1007/s10922-015-9348-6},
abstract = {In this paper, we present a multivariate recurrent neural network model for short-time prediction of the number of failures that are expected to be reported by users of a broadband telecommunication network. An accurate prediction of the expected number of reported failures is becoming increasingly important to service providers. It enables proactive actions and improves the decision-making process, operational network maintenance, and workforce allocation. Our previous studies have shown that the recursive neural network is flexible enough to approximate the dynamics of the failure reporting process. Development of the model is based on long-term monitoring of failure-reporting processes and experience gained through fault management related to the network of one of the leading Croatian telecom providers (T-HT). Many factors, both in the network and outside the network, influence the time series representing failure reporting. The model encompasses the most important predictor variables and their logical and temporal dependencies. Predictor variables represent internal factors such as profiles of past and current quantities of failures as well as external factors like weather forecasts or announced activities (scheduled maintenance) in the network. External factors have a strong effect on fault occurrence, which finally results in failures reported by users. These factors are quantified and included as input variables to our model. The model is fitted to the data from different sources like an error-logging database, a trouble-ticket archive, announced settings logs and a meteo-data archive. The accuracy of the model is examined on simulation tests varying the prediction horizons. Assessment of the model's accuracy is made by comparing results obtained by prediction and the actual data. This research represents a real-world case study from telecom operations. The developed prediction model is scalable and adaptable so that other relevant input factors can be added as needed. Hence, the proposed prediction approach based on the model can be efficiently implemented as a functionality in real fault-management processes where a variety of available input data of different volumes exist.},
journal = {J. Netw. Syst. Manage.},
month = jan,
pages = {189–221},
numpages = {33},
keywords = {Telecommunication network, Proactive fault management, Predictor variables, NARX, Multivariate model, Failure reporting, Failure prediction}
}

@article{10.4018/IJITPM.2020100105,
author = {Rawat, Saurabh and Sah, Anushree and Dumka, Ankur},
title = {Direct-Indirect Link Matrix: A Black Box Testing Technique for Component-Based Software},
year = {2020},
issue_date = {Oct 2020},
publisher = {IGI Global},
address = {USA},
volume = {11},
number = {4},
issn = {1938-0232},
url = {https://doi.org/10.4018/IJITPM.2020100105},
doi = {10.4018/IJITPM.2020100105},
abstract = {Testing of software remains a fundamentally significant way to check that software behaves as required. Component-based software testing (CBST) is a crucial activity of component-based software development (CBSD) and is based on two crucial proportions: components testing by developers with the source code (e.g., system testing, integration testing, unit testing, etc.) and components testing by end users without source code (black box testing). This work proposes a black box testing technique that calculates the total number of interactions made by component-based software. This technique is helpful to identify the number of test cases for those components where availability of source code is questionable. On the basis of interaction among components, the authors draw a component-link graph and a direct-indirect-link matrix, which helps to calculate the number of interactions in component-based software.},
journal = {Int. J. Inf. Technol. Proj. Manag.},
month = oct,
pages = {56–69},
numpages = {14},
keywords = {Software Testing, Software Reliability, Predictive Models, Direct-Indirect-Link Matrix, Component-Link Graph, Component-Based Software Development}
}

@inproceedings{10.1145/3468264.3468545,
author = {Suneja, Sahil and Zheng, Yunhui and Zhuang, Yufan and Laredo, Jim A. and Morari, Alessandro},
title = {Probing model signal-awareness via prediction-preserving input minimization},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468545},
doi = {10.1145/3468264.3468545},
abstract = {This work explores the signal awareness of AI models for source code understanding. Using a software vulnerability detection use case, we evaluate the models' ability to capture the correct vulnerability signals to produce their predictions. Our prediction-preserving input minimization (P2IM) approach systematically reduces the original source code to a minimal snippet which a model needs to maintain its prediction. The model's reliance on incorrect signals is then uncovered when the vulnerability in the original code is missing in the minimal snippet, both of which the model however predicts as being vulnerable. We measure the signal awareness of models using a new metric we propose -- Signal-aware Recall (SAR). We apply P2IM on three different neural network architectures across multiple datasets. The results show a sharp drop in the model's Recall from the high 90s to sub-60s with the new metric, highlighting that the models are presumably picking up a lot of noise or dataset nuances while learning their vulnerability detection logic. Although the drop in model performance may be perceived as an adversarial attack, but this isn't P2IM's objective. The idea is rather to uncover the signal-awareness of a black-box model in a data-driven manner via controlled queries. SAR's purpose is to measure the impact of task-agnostic model training, and not to suggest a shortcoming in the Recall metric. The expectation, in fact, is for SAR to match Recall in the ideal scenario where the model truly captures task-specific signals.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {945–955},
numpages = {11},
keywords = {signal-aware recall, model signal-awareness, machine learning},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3475716.3475781,
author = {Croft, Roland and Newlands, Dominic and Chen, Ziyu and Babar, M. Ali},
title = {An Empirical Study of Rule-Based and Learning-Based Approaches for Static Application Security Testing},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475781},
doi = {10.1145/3475716.3475781},
abstract = {Background: Static Application Security Testing (SAST) tools purport to assist developers in detecting security issues in source code. These tools typically use rule-based approaches to scan source code for security vulnerabilities. However, due to the significant shortcomings of these tools (i.e., high false positive rates), learning-based approaches for Software Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite the similar objectives of these two approaches, their comparative value is unexplored. We provide an empirical analysis of SAST tools and SVP models, to identify their relative capabilities for source code security analysis. Method: We evaluate the detection and assessment performance of several common SAST tools and SVP models on a variety of vulnerability datasets. We further assess the viability and potential benefits of combining the two approaches. Results: SAST tools and SVP models provide similar detection capabilities, but SVP models exhibit better overall performance for both detection and assessment. Unification of the two approaches is difficult due to lacking synergies. Conclusions: Our study generates 12 main findings which provide insights into the capabilities and synergy of these two approaches. Through these observations we provide recommendations for use and improvement.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {8},
numpages = {12},
keywords = {Static Application Security Testing, Security, Machine Learning},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.5555/3155562.3155695,
author = {Krishna, Rahul},
title = {Learning effective changes for software projects},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {The primary motivation of much of software analytics is decision making. How to make these decisions? Should one make decisions based on lessons that arise from within a particular project? Or should one generate these decisions from across multiple projects? This work is an attempt to answer these questions. Our work was motivated by a realization that much of the current generation software analytics tools focus primarily on prediction. Indeed prediction is a useful task, but it is usually followed by ``planning'' about what actions need to be taken. This research seeks to address the planning task by seeking methods that support actionable analytics by offering clear guidance on what to do. Specifically, we propose XTREE and BELLTREE algorithms for generating a set of actionable plans within and across projects. Each of these plans, if followed will improve the quality of the software project.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {1002–1005},
numpages = {4},
keywords = {defect prediction, bellwethers, actionable analytics, Data mining},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@inproceedings{10.1109/ICSE43902.2021.00067,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {Fault Localization with Code Coverage Representation Learning},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00067},
doi = {10.1109/ICSE43902.2021.00067},
abstract = {In this paper, we propose DEEPRL4FL, a deep learning fault localization (FL) approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem. DEEPRL4FL does so via novel code coverage representation learning (RL) and data dependencies RL for program statements. Those two types of RL on the dynamic information in a code coverage matrix are also combined with the code representation learning on the static information of the usual suspicious source code. This combination is inspired by crime scene investigation in which investigators analyze the crime scene (failed test cases and statements) and related persons (statements with dependencies), and at the same time, examine the usual suspects who have committed a similar crime in the past (similar buggy code in the training data).For the code coverage information, DEEPRL4FL first orders the test cases and marks error-exhibiting code statements, expecting that a model can recognize the patterns discriminating between faulty and non-faulty statements/methods. For dependencies among statements, the suspiciousness of a statement is seen taking into account the data dependencies to other statements in execution and data flows, in addition to the statement by itself. Finally, the vector representations for code coverage matrix, data dependencies among statements, and source code are combined and used as the input of a classifier built from a Convolution Neural Network to detect buggy statements/methods. Our empirical evaluation shows that DEEPRL4FL improves the top-1 results over the state-of-the-art statement-level FL baselines from 173.1% to 491.7%. It also improves the top-1 results over the existing method-level FL baselines from 15.0% to 206.3%.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {661–673},
numpages = {13},
keywords = {representation learning, machine learning, fault localization, deep learning, code coverage},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.4018/jismd.2011100102,
author = {Raja, Uzma and Tretter, Marietta J.},
title = {Predicting OSS Development Success: A Data Mining Approach},
year = {2011},
issue_date = {October 2011},
publisher = {IGI Global},
address = {USA},
volume = {2},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2011100102},
doi = {10.4018/jismd.2011100102},
abstract = {Open Source Software OSS has reached new levels of sophistication and acceptance by users and commercial software vendors. This research creates tests and validates a model for predicting successful development of OSS projects. Widely available archival data was used for OSS projects from Sourceforge.net. The data is analyzed with multiple Data Mining techniques. Initially three competing models are created using Logistic Regression, Decision Trees and Neural Networks. These models are compared for precision and are refined in several phases. Text Mining is used to create new variables that improve the predictive power of the models. The final model is chosen based on best fit to separate training and validation data sets and the ability to explain the relationship among variables. Model robustness is determined by testing it on a new dataset extracted from the SF repository. The results indicate that end-user involvement, project age, functionality, usage, project management techniques, project type and team communication methods have a significant impact on the development of OSS projects.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {27–48},
numpages = {22},
keywords = {Software Development, Open Source Software, Neural Networks, Logistic Regression, Decision Trees, Data Models, Data Mining}
}

@inproceedings{10.1007/978-3-030-22744-9_20,
author = {Chora\'{s}, Micha\l{} and Pawlicki, Marek and Kozik, Rafa\l{}},
title = {Recognizing Faults in Software Related Difficult Data},
year = {2019},
isbn = {978-3-030-22743-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22744-9_20},
doi = {10.1007/978-3-030-22744-9_20},
abstract = {In this paper we have investigated the use of numerous machine learning algorithms, with emphasis on multilayer artificial neural networks in the domain of software source code fault prediction. The main contribution lies in enhancing the data pre-processing step as the partial solution for handling software related difficult data. Before we put the data into an Artificial Neural Network, we are implementing PCA (Principal Component Analysis) and k-means clustering. The data-clustering step improves the quality of the whole dataset. Using the presented approach we were able to obtain 10% increase of accuracy of the fault detection. In order to ensure the most reliable results, we implement 10-fold cross-validation methodology during experiments. We have also evaluated a wide range of hyperparameter setups for the network, and compared the results to the state of the art, cost-sensitive approaches - Random Forest, AdaBoost, RepTrees and GBT.},
booktitle = {Computational Science – ICCS 2019: 19th International Conference, Faro, Portugal, June 12–14, 2019, Proceedings, Part III},
pages = {263–272},
numpages = {10},
keywords = {Data clustering, ANN, Faults detection, Pattern recognition},
location = {Faro, Portugal}
}

@article{10.1016/j.jnca.2010.03.012,
author = {Wang, Yao-Tien},
title = {A dynamic resource management in mobile agent by artificial neural network},
year = {2010},
issue_date = {November, 2010},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {33},
number = {6},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2010.03.012},
doi = {10.1016/j.jnca.2010.03.012},
abstract = {In this paper, a resource management for dynamic load balancing in mobile agent by artificial neural network scheme (ANN-DLB) is presented to maximize the number of the served tasks in developing high performance cluster. This dynamic load balance with the growth of the service type and user number in the mobile networks of the higher performance is required in service provision and throughput. Most of the conventional policies are used in load indices with the threshold value to decide the load status of the agent hosts by CPU or memory. The main factor influencing the workload is the competitions among the computing resources such as CPU, memory, I/O and network. There are certain I/O data of the intensive applications where load balancing becomes the important issue. This relationship between the computing resources is very complex to define the rules for deciding the workload. This paper proposed a new dynamic load balancing for evaluating the agent hosts' workload with the artificial neural network (ANN). By applying the automatic learning of the back-propagation network (BPN) model can establish the ANN model and also can measure the agent host loading with five inputs: CPU, memory, I/O, network and run-queue length. The structure of the load balancing system is composed of three design agents: the load index agent (LIA), the resource management agent (RMA) and the load transfer agent (LTA). These experimental results reveal that the proposed ANN-DLB yields better performance than the other methods. These results demonstrate that the proposed method has high throughput, short response time and turnaround time, and less agent host negotiation complexity and migrating tasks than the previous methods.},
journal = {J. Netw. Comput. Appl.},
month = nov,
pages = {672–681},
numpages = {10},
keywords = {Mobile agents, Load balancing, Artificial neural network, Agent negotiation strategies, Agent host}
}

@inproceedings{10.1007/978-3-030-58811-3_67,
author = {ElGhondakly, Roaa and Moussa, Sherin and Badr, Nagwa},
title = {Handling Faults in Service Oriented Computing: A Comprehensive Study},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_67},
doi = {10.1007/978-3-030-58811-3_67},
abstract = {Recently, service-oriented computing paradigms have become a trending development direction, in which software systems are built using a set of loosely coupled services distributed over multiple locations through a service-oriented architecture. Such systems encounter different challenges, as integration, performance, reliability, availability, etc., which made all associated testing activities to be another major challenge to avoid their faults and system failures. Services are considered the substantial element in service-oriented computing. Thus, the quality of services and the service dependability in a web service composition have become essential to manage faults within these software systems. Many studies addressed web service faults from diverse perspectives. In this paper, a comprehensive study is conducted to investigate the different perspectives to manipulate web service faults, including fault tolerance, fault injection, fault prediction and fault localization. An extensive comparison is provided, highlighting the main research gaps, challenges and limitations of each perspective for web services. An analytical discussion is then followed to suggest future research directions that can be adopted to face such obstacles by improving fault handling capabilities for an efficient testing in service-oriented computing systems.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {947–959},
numpages = {13},
keywords = {Service oriented computing, Service testing, Quality of Service, Fault injection, Fault prediction, Fault tolerance},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3330204.3330230,
author = {de Macedo, Charles Mendes and Ruela, Andr\'{e} Siqueira and Delgado, Karina Valdivia},
title = {Application of Clustering Algorithms for Discovering Bug Patterns in JavaScript Software},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330230},
doi = {10.1145/3330204.3330230},
abstract = {Applications developed with JavaScript language are increasing every day, not only for client-side, but also for server-side and for mobile devices. In this context, the existence of tools to identify faults is fundamental in order to assist developers during the evolution of their applications. Different tools and approaches have been proposed over the years, however they have limitations to evolve over time, becoming obsolete quickly. The reason for this is the use of a fixed list of pre-defined faults that are searched in the code. The BugAID tool implements a semiautomatic strategy for discovering bug patterns by grouping the changes made during the project development. The objective of this work is to contribute to the BugAID tool, extending this tool with improvements in the extraction of characteristics to be used by the clustering algorithm. The extended module of the BugAID extraction module (BE) that extracts the characteristics is called BE+. Additionally, an evaluation of the clustering algorithms used for discovering fault patterns in JavaScript software is performed. The results show that the DBScan and Optics algorithms with BE+ presented the best results for the Rand, Jaccard and Adjusted Rand indexes, while HDBScan with BE and BE+ presented the worst result.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {21},
numpages = {8},
keywords = {Software Quality, Pattern Recognition, Machine Learning, Data Mining, Bug Discovery},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/3472674.3473981,
author = {Pontillo, Valeria and Palomba, Fabio and Ferrucci, Filomena},
title = {Toward static test flakiness prediction: a feasibility study},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473981},
doi = {10.1145/3472674.3473981},
abstract = {Flaky tests are tests that exhibit both a passing and failing behavior when run against the same code. While the research community has attempted to define automated approaches for detecting and addressing test flakiness, most of them suffer from scalability issues and uncertainty as they require test cases to be run multiple times. This limitation has been recently targeted by means of machine learning solutions that could predict the flakiness of tests using a set of both static and dynamic metrics that would avoid the re-execution of tests. Recognizing the effort spent so far, this paper poses the first steps toward an orthogonal view of the problem, namely the classification of flaky tests using only statically computable software metrics. We propose a feasibility study on 72 projects of the iDFlakies dataset, and investigate the differences between flaky and non-flaky tests in terms of 25 test and production code metrics and smells. First, we statistically assess those differences. Second, we build a logistic regression model to verify the extent to which the differences observed are still significant when the metrics are considered together. The results show a relation between test flakiness and a number of test and production code factors, indicating the possibility to build classification approaches that exploit those factors to predict test flakiness.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {19–24},
numpages = {6},
keywords = {Software Quality Evaluation, Flaky Tests, Empirical Studies},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@inproceedings{10.1145/2508859.2516665,
author = {Yamaguchi, Fabian and Wressnegger, Christian and Gascon, Hugo and Rieck, Konrad},
title = {Chucky: exposing missing checks in source code for vulnerability discovery},
year = {2013},
isbn = {9781450324779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508859.2516665},
doi = {10.1145/2508859.2516665},
abstract = {Uncovering security vulnerabilities in software is a key for operating secure systems. Unfortunately, only some security flaws can be detected automatically and the vast majority of vulnerabilities is still identified by tedious auditing of source code. In this paper, we strive to improve this situation by accelerating the process of manual auditing. We introduce Chucky, a method to expose missing checks in source code. Many vulnerabilities result from insufficient input validation and thus omitted or false checks provide valuable clues for finding security flaws. Our method proceeds by statically tainting source code and identifying anomalous or missing conditions linked to security-critical objects.In an empirical evaluation with five popular open-source projects, Chucky is able to accurately identify artificial and real missing checks, which ultimately enables us to uncover 12 previously unknown vulnerabilities in two of the projects (Pidgin and LibTIFF).},
booktitle = {Proceedings of the 2013 ACM SIGSAC Conference on Computer &amp; Communications Security},
pages = {499–510},
numpages = {12},
keywords = {vulnerabilities, static analysis, anomaly detection},
location = {Berlin, Germany},
series = {CCS '13}
}

@inproceedings{10.1145/3453483.3454045,
author = {He, Jingxuan and Lee, Cheng-Chun and Raychev, Veselin and Vechev, Martin},
title = {Learning to find naming issues with big code and small supervision},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454045},
doi = {10.1145/3453483.3454045},
abstract = {We introduce a new approach for finding and fixing naming issues in source code. The method is based on a careful combination of unsupervised and supervised procedures: (i) unsupervised mining of patterns from Big Code that express common naming idioms. Program fragments violating such idioms indicates likely naming issues, and (ii) supervised learning of a classifier on a small labeled dataset which filters potential false positives from the violations.  We implemented our method in a system called Namer and evaluated it on a large number of Python and Java programs. We demonstrate that Namer is effective in finding naming mistakes in real world repositories with high precision (~70%). Perhaps surprisingly, we also show that existing deep learning methods are not practically effective and achieve low precision in finding naming issues (up to ~16%).},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {296–311},
numpages = {16},
keywords = {Static analysis, Name-based program analysis, Machine learning, Bug detection, Anomaly detection},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.5555/3433701.3433782,
author = {Boixaderas, Isaac and Zivanovic, Darko and Mor\'{e}, Sergi and Bartolome, Javier and Vicente, David and Casas, Marc and Carpenter, Paul M. and Radojkovi\'{c}, Petar and Ayguad\'{e}, Eduard},
title = {Cost-aware prediction of uncorrected DRAM errors in the field},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {This paper presents and evaluates a method to predict DRAM uncorrected errors, a leading cause of hardware failures in large-scale HPC clusters. The method uses a random forest classifier, which was trained and evaluated using error logs from two years of production of the MareNostrum 3 supercomputer. By enabling the system to take measures to mitigate node failures, our method reduces lost compute time by up to 57%, a net saving of 21,000 node-hours per year. We release all source code as open source.We also discuss and clarify aspects of methodology that are essential for a DRAM prediction method to be useful in practice. We explain why standard evaluation metrics, such as precision and recall, are insufficient, and base the evaluation on a cost-benefit analysis. This methodology can help ensure that any DRAM error predictor is clear from training bias and has a clear cost-benefit calculation.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {61},
numpages = {15},
keywords = {random forest, memory system, machine learning, error prediction, cost-benefit analysis},
location = {Atlanta, Georgia},
series = {SC '20}
}

@mastersthesis{10.5555/AAI28027666,
author = {Brumfield, Marcus and Williams, Byron and Bhowmik, Tanmay},
advisor = {Stefano, Iannucci,},
title = {A Deep Learning Approach to Predict Software Bugs Using Micro Patterns and Software Metrics},
year = {2020},
isbn = {9798664731248},
publisher = {Mississippi State University},
address = {USA},
abstract = {Software bugs prediction is one of the most active research areas in the software engineering community. The process of testing and debugging code proves to be costly during the software development life cycle. Software metrics measure the quality of source code to identify software bugs and vulnerabilities. Traceable code patterns are able to de- scribe code at a finer granularity level to measure quality. Micro patterns will be used in this research to mechanically describe java code at the class level. Machine learning has also been introduced for bug prediction to localize source code for testing and debugging. Deep Learning is a branch of Machine Learning that is relatively new. This research looks to improve the prediction of software bugs by utilizing micro patterns with deep learning techniques. Software bug prediction at a finer granularity level will enable developers to localize code to test and debug during the development process.},
note = {AAI28027666}
}

@article{10.1504/IJCAT.2009.026595,
author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
title = {Comparative analysis of regression and machine learning methods for predicting fault proneness models},
year = {2009},
issue_date = {June 2009},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {35},
number = {2/3/4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2009.026595},
doi = {10.1504/IJCAT.2009.026595},
abstract = {Demand for quality software has undergone rapid growth during the last few years. This is leading to increase in development of machine learning techniques for exploring datasets which can be used in constructing models for predicting quality attributes such as Decision Tree (DT), Support Vector Machine (SVM) and Artificial Neural Network (ANN). This paper examines and compares Logistic Regression (LR), ANN (model predicted in an analogous study using the same dataset), SVM and DT methods. These two methods are explored empirically to find the effect of object-oriented metrics given by Chidamber and Kemerer on the fault proneness of object-oriented system classes. Data collected from Java applications is used in the study. The performance of the methods was compared by Receiver Operating Characteristic (ROC) analysis. DT modelling showed 84.7% of correct classifications of faulty classes and is a better model than the model predicted using LR, SVM and ANN method. The area under the ROC curve of LR, ANN, SVM and DT model is 0.826, 0.85, 0.85 and 0.87, respectively. The paper shows that machine learning methods are useful in constructing software quality models.},
journal = {Int. J. Comput. Appl. Technol.},
month = jun,
pages = {183–193},
numpages = {11},
keywords = {support vector machine, software quality models, receiver operating characteristics curve, object-oriented systems, metrics, machine learning, logistic regression, fault-prone software, decision trees, artificial neural networks, SVM, ANNs}
}

@article{10.1016/j.asoc.2021.107259,
author = {Kang, Yanzhe and Jia, Ning and Cui, Runbang and Deng, Jiang},
title = {A graph-based semi-supervised reject inference framework considering imbalanced data distribution for consumer credit scoring},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107259},
doi = {10.1016/j.asoc.2021.107259},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {19},
keywords = {Label spreading, Semi-supervised learning, Imbalanced learning, Reject inference, Credit scoring, Financial technology}
}

@article{10.1016/j.jksuci.2017.07.006,
author = {Lal, Sangeeta and Sardana, Neetu and Sureka, Ashish},
title = {Three-level learning for improving cross-project logging prediction for if-blocks},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {31},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2017.07.006},
doi = {10.1016/j.jksuci.2017.07.006},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = oct,
pages = {481–496},
numpages = {16}
}

@article{10.1016/j.infsof.2016.04.017,
author = {Fu, Wei and Menzies, Tim and Shen, Xipeng},
title = {Tuning for software analytics},
year = {2016},
issue_date = {August 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {76},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.04.017},
doi = {10.1016/j.infsof.2016.04.017},
abstract = {Context: Data miners have been widely used in software engineering to, say, generate defect predictors from static code measures. Such static code defect predictors perform well compared to manual methods, and they are easy to use and useful to use. But one of the "black arts" of data mining is setting the tunings that control the miner.Objective: We seek simple, automatic, and very effective method for finding those tunings.Method: For each experiment with different data sets (from open source JAVA systems), we ran differential evolution as an optimizer to explore the tuning space (as a first step) then tested the tunings using hold-out data.Results: Contrary to our prior expectations, we found these tunings were remarkably simple: it only required tens, not thousands, of attempts to obtain very good results. For example, when learning software defect predictors, this method can quickly find tunings that alter detection precision from 0% to 60%.Conclusion: Since (1) the improvements are so large, and (2) the tuning is so simple, we need to change standard methods in software analytics. At least for defect prediction, it is no longer enough to just run a data miner and present the result without conducting a tuning optimization study. The implication for other kinds of analytics is now an open and pressing issue.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {135–146},
numpages = {12},
keywords = {Search-based software engineering, Random forest, Differential evolution, Defect prediction, CART}
}

@inproceedings{10.1145/3460319.3464840,
author = {Pan, Cong and Pradel, Michael},
title = {Continuous test suite failure prediction},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464840},
doi = {10.1145/3460319.3464840},
abstract = {Continuous integration advocates to run the test suite of a project frequently, e.g., for every code change committed to a shared repository. This process imposes a high computational cost and sometimes also a high human cost, e.g., when developers must wait for the test suite to pass before a change appears in the main branch of the shared repository. However, only 4% of all test suite invocations turn a previously passing test suite into a failing test suite. The question arises whether running the test suite for each code change is really necessary. This paper presents continuous test suite failure prediction, which reduces the cost of continuous integration by predicting whether a particular code change should trigger the test suite at all. The core of the approach is a machine learning model based on features of the code change, the test suite, and the development history. We also present a theoretical cost model that describes when continuous test suite failure prediction is worthwhile. Evaluating the idea with 15k test suite runs from 242 open-source projects shows that the approach is effective at predicting whether running the test suite is likely to reveal a test failure. Moreover, we find that our approach improves the AUC over baselines that use features proposed for just-in-time defect prediction and test case failure prediction by 13.9% and 2.9%, respectively. Overall, continuous test suite failure prediction can significantly reduce the cost of continuous integration.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {553–565},
numpages = {13},
keywords = {machine learning, cost model, continuous test suite failure prediction, continuous integration},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1109/MSR.2017.4,
author = {Rajbahadur, Gopi Krishnan and Wang, Shaowei and Kamei, Yasutaka and Hassan, Ahmed E.},
title = {The impact of using regression models to build defect classifiers},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.4},
doi = {10.1109/MSR.2017.4},
abstract = {It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers).In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches; ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance.Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {135–145},
numpages = {11},
keywords = {random forest, non-discretization, model interpretation, discretization, classification via regression, bug prediction},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.1145/3408896,
author = {Holmes, Josie and Ahmed, Iftekhar and Brindescu, Caius and Gopinath, Rahul and Zhang, He and Groce, Alex},
title = {Using Relative Lines of Code to Guide Automated Test Generation for Python},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3408896},
doi = {10.1145/3408896},
abstract = {Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation. It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another. However, relative LOC, between components of the same project, turns out to be a highly useful metric for automated testing. In this article, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation. This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead. We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool. In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20%, up to 40% or more) and improve fault detection by an even larger margin (usually more than 75% and up to 400% or more). The LOC heuristic is also easy to combine with other approaches and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {28},
numpages = {38},
keywords = {testing heuristics, static code metrics, Automated test generation}
}

@article{10.5555/3291125.3309645,
author = {Burns, David M. and Whyne, Cari M.},
title = {Seglearn: a python package for learning sequences and time series},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {seglearn is an open-source Python package for performing machine learning on time series or sequences. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. Sequences and series may be learned directly with deep learning models or via feature representation with classical machine learning estimators. This package is compatible with scikit-learn and is listed under scikit-learn "Related Projects". The package depends on numpy, scipy, and scikit-learn. seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage. Source code and documentation can be downloaded from https://github.com/dmbee/seglearn.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3238–3244},
numpages = {7},
keywords = {time-series, sequences, python, machine-learning}
}

@article{10.1145/3127360.3127368,
author = {Kumar, Lov and Behera, Ranjan Kumar and Rath, Santanu and Sureka, Ashish},
title = {Transfer Learning for Cross-Project Change-Proneness Prediction in Object-Oriented Software Systems: A Feasibility Analysis},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3127360.3127368},
doi = {10.1145/3127360.3127368},
abstract = {Change-prone classes or modules are defined as regions of the source code which are more likely to change as a result of a software development of maintenance activity. Automatic identification of change-prone classes are useful for the software development team as they can focus their testing efforts on areas within the source code which are more likely to change. Several machine learning techniques have been proposed for predicting change-prone classes based on the application of source code metrics as indicators. However, most of the work has focused on within-project training and model building. There are several real word scenario in which sufficient training dataset is not available for model building such as in the case of a new project. Cross-project prediction is an approach which consists of training a model from dataset belonging to one project and testing it on dataset belonging to a different project. Cross-project change-proneness prediction is relatively unexplored.We propose a machine learning based approach for cross-project change-proneness prediction. We conduct experiments on 10 open-source Eclipse plug-ins and demonstrate the effectiveness of our approach. We frame several research questions comparing the performance of within project and cross project prediction and also propose a Genetic Algorithm (GA) based approach for identifying the best set of source code metrics. We conclude that for within project experimental setting, Random Forest (RF) technique results in the best precision. In case of cross-project change-proneness prediction, our analysis reveals that the NDTF ensemble method performs higher than other individual classifiers (such as decision tree and logistic regression) and ensemble methods in the experimental dataset. We conduct a comparison of within-project, cross-project without GA and cross-project with GA and our analysis reveals that cross-project with GA performs best followed by within-project and then cross-project without GA.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–11},
numpages = {11}
}

@inproceedings{10.1145/3177457.3191709,
author = {Ren, Yidan and Zhu, Zhengzhou and Chen, Xiangzhou and Ding, Huixia and Zhang, Geng},
title = {Research on Defect Detection Technology of Trusted Behavior Decision Tree Based on Intelligent Data Semantic Analysis of Massive Data},
year = {2018},
isbn = {9781450363396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177457.3191709},
doi = {10.1145/3177457.3191709},
abstract = {With the rapid development of information technology, software systems' scales and complexity are showing a trend of expansion. The users' needs for the software security, software security reliability and software stability are growing increasingly. At present, the industry has applied machine learning methods to the fields of defect detection to repair and improve software defects through the massive data intelligent semantic analysis or code scanning. The model in machine learning is faced with big difficulty of model building, understanding, and the poor visualization in the field of traditional software defect detection. In view of the above problems, we present a point of view that intelligent semantic analysis technology based on massive data, and using the trusted behavior decision tree model to analyze the soft behavior by layered detection technology. At the same time, it is equipped related test environment to compare the tested software. The result shows that the defect detection technology based on intelligent semantic analysis of massive data is superior to other techniques at the cost of building time and error reported ratio.},
booktitle = {Proceedings of the 10th International Conference on Computer Modeling and Simulation},
pages = {168–175},
numpages = {8},
keywords = {software defect detection, intelligent semantic analysis, decision tree, Massive data},
location = {Sydney, Australia},
series = {ICCMS '18}
}

@article{10.1007/s00521-021-05954-3,
author = {Lin, Guanjun and Xiao, Wei and Zhang, Leo Yu and Gao, Shang and Tai, Yonghang and Zhang, Jun},
title = {Deep neural-based vulnerability discovery demystified: data, model and performance},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05954-3},
doi = {10.1007/s00521-021-05954-3},
abstract = {Detecting source-code level vulnerabilities at the development phase is a cost-effective solution to prevent potential attacks from happening at the software deployment stage. Many machine learning, including deep learning-based solutions, have been proposed to aid the process of vulnerability discovery. However, these approaches were mainly evaluated on self-constructed/-collected datasets. It is difficult to evaluate the effectiveness of proposed approaches due to lacking a unified baseline dataset. To bridge this gap, we construct a function-level vulnerability dataset from scratch, providing in source-code-label pairs. To evaluate the constructed dataset, a function-level vulnerability detection framework is built to incorporate six mainstream neural network models as vulnerability detectors. We perform experiments to investigate the performance behaviors of the neural model-based detectors using source code as raw input with continuous Bag-of-Words neural embeddings. Empirical results reveal that the variants of recurrent neural networks and convolutional neural network perform well on our dataset, as the former is capable of handling contextual information and the latter learns features from small context windows. In terms of generalization ability, the fully connected network outperforms the other network architectures. The performance evaluation can serve as a reference benchmark for neural model-based vulnerability detection at function-level granularity. Our dataset can serve as ground truth for ML-based function-level vulnerability detection and a baseline for evaluating relevant approaches.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {13287–13300},
numpages = {14},
keywords = {Performance evaluation, Baseline dataset, Function-level, Deep learning, Vulnerability discovery}
}

@article{10.1504/ijiei.2021.120322,
author = {Lakra, Kirti and Chug, Anuradha},
title = {Application of metaheuristic techniques in software quality prediction: a systematic mapping study},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {4},
issn = {1758-8715},
url = {https://doi.org/10.1504/ijiei.2021.120322},
doi = {10.1504/ijiei.2021.120322},
abstract = {This paper focuses on the systematic review of various metaheuristic techniques employed for analysing different software quality aspects, including fault proneness, defect anticipation, change proneness, maintainability prediction, and software reliability prediction. It is observed that machine learning algorithms are still popular models, but metaheuristic algorithms are also gaining popularity in the field of software quality measurement. This is due to the fact that metaheuristic algorithms are more efficient in solving real-world, search-based, and optimisation problems. Initially, 90 papers were considered and analysed for conducting this study from 2010 to 2020, and 55 studies were shortlisted based on predesigned quality evaluation standards. Resultantly, particle swarm optimisation (PSO), and genetic algorithms came out as the most prominently used metaheuristic techniques for developing software quality models in 36.3% and 27.2% of the shortlisted studies, respectively. The current review will benefit other researchers by providing an insight into the current trends in software quality domain.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {355–399},
numpages = {44},
keywords = {software quality improvement, software maintainability prediction, software reliability prediction, software change prediction, software defect prediction, software fault proneness, software quality, object-oriented metrics, metaheuristic techniques}
}

@article{10.1504/IJISTA.2017.081311,
author = {Singh, Satwinder and Singla, Rozy},
title = {Classification of defective modules using object-oriented metrics},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {1},
issn = {1740-8865},
url = {https://doi.org/10.1504/IJISTA.2017.081311},
doi = {10.1504/IJISTA.2017.081311},
abstract = {Software defect in today's era is crucial in the field of software engineering. Most of the organisations use various techniques to predict defects in their products before they are delivered. Defect prediction techniques help the organisations to use their resources effectively which results in lower cost and time requirements. There are various techniques that are used for predicting defects in software before it has to be delivered, e.g., clustering, neural networks, support vector machine SVM. In this paper two defect prediction techniques: K-means clustering and multi-layer perceptron model MLP are compared. Both the techniques are implemented on different platforms. K-means clustering is implemented using WEKA tool and MLP is implemented using SPSS. The results are compared to find which algorithm produces better results. In this paper object-oriented metrics are used for predicting defects in the software.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {1–13},
numpages = {13},
keywords = {software engineering, software development, software defects, object-oriented metrics, multi-layer perceptron, defective modules, defect prediction, classification, artificial neural networks, WEKA, SPSS, MLP, K-means clustering, ANNs}
}

@article{10.1007/s11063-021-10607-6,
author = {Kassaymeh, Sofian and Abdullah, Salwani and Al-Laham, Mohamad and Alweshah, Mohammed and Al-Betar, Mohammed Azmi and Othman, Zalinda},
title = {Salp Swarm Optimizer for Modeling Software Reliability Prediction Problems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {6},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-021-10607-6},
doi = {10.1007/s11063-021-10607-6},
abstract = {In this paper, software effort prediction (SEP) and software test prediction (STP) (i.e., software reliability problems) are tackled by integrating the salp swarm algorithm (SSA) with a backpropagation neural network (BPNN). Software effort and test prediction problems are common in software engineering and arise when seeking to determine the actual software resources needed to develop a project. BPNN is the most popular prediction algorithm used in the literature. The performance of BPNN depends totally on the initial parameter values such as weight and biases. The main objective of this paper is to integrate SSA with the BPNN to find the optimal weight for every training cycle and thereby improve prediction accuracy. The proposed method, abbreviated as SSA-BPNN, is tested on twelve SEP datasets and two STP datasets. All datasets vary in terms of complexity and size. The results obtained by SSA-BPNN are evaluated according to twelve performance measures: MSE, RMSE, RAE, RRSE, MAE, MRE, MMRE, MdMRE, VAF(%), R2(%), ED, and MD. First, the results obtained by BPNN with SSA (i.e., SSA-BPNN) and without SSA are compared. The evaluation of the results indicates that SSA-BPNN performs better than BPNN for all datasets. In the comparative evaluation, the results of SSA-BPNN are compared against thirteen state-of-the-art methods using the same SEP and STP problem datasets. The evaluation of the results reveals that the proposed method outperforms the comparative methods for almost all datasets, both SEP and STP, in the case of most performance measures. In conclusion, integrating SSA with BPNN is a very powerful approach for solving software reliability problems that can be used widely to yield accurate prediction results.},
journal = {Neural Process. Lett.},
month = dec,
pages = {4451–4487},
numpages = {37},
keywords = {Software test estimation, Software effort estimation, Software reliability problems, Backpropagation neural network, Salp swarm optimizer, Machine learning}
}

@inproceedings{10.5555/2041619.2041634,
author = {Bovenzi, Antonio and Brancati, Francesco and Russo, Stefano and Bondavalli, Andrea},
title = {A statistical anomaly-based algorithm for on-line fault detection in complex software critical systems},
year = {2011},
isbn = {9783642242694},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The next generation of software systems in Large-scale Complex Critical Infrastructures (LCCIs) requires efficient runtime management and reconfiguration strategies, and the ability to take decisions on the basis of current and past behavior of the system. In this paper we propose an anomalybased approach for the detection of online faults, which is able to (i) cope with highly variable and non-stationary environment and to (ii) work without any initial training phase. The novel algorithm is based on Statistical Predictor and Safety Margin (SPS), which was initially developed to estimate the uncertainty in time synchronization mechanisms.The SPS anomaly detection algorithm has been experimented on a case study from the Air Traffic Management (ATM) domain. Results have been compared with an algorithm, which adopts static thresholds, in the same scenarios [5]. Experimental results show limitations of static thresholds in highly variable scenarios, and the ability of SPS to fulfill the expectations.},
booktitle = {Proceedings of the 30th International Conference on Computer Safety, Reliability, and Security},
pages = {128–142},
numpages = {15},
keywords = {on-line software fault diagnosis, anomaly detection, SPS},
location = {Naples, Italy},
series = {SAFECOMP'11}
}

@inproceedings{10.1145/3394486.3403171,
author = {Yoon, Susik and Lee, Jae-Gil and Lee, Byung Suk},
title = {Ultrafast Local Outlier Detection from a Data Stream with Stationary Region Skipping},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403171},
doi = {10.1145/3394486.3403171},
abstract = {Real-time outlier detection from a data stream is an increasingly important problem, especially as sensor-generated data streams abound in many applications owing to the prevalence of IoT and emergence of digital twins. Several density-based approaches have been proposed to address this problem, but arguably none of them is fast enough to meet the performance demand of real applications. This paper is founded upon a novel observation that, in many regions of the data space, data distributions hardly change across window slides. We propose a new algorithm, abbr. STARE, which identifies local regions in which data distributions hardly change and then skips updating the densities in those regions-a notion called stationary region skipping. Two techniques, data distribution approximation and cumulative net-change-based skip, are employed to efficiently and effectively implement the notion. Extensive experiments using synthetic and real data streams as well as a case study show that STARE is several orders of magnitude faster than the existing algorithms while achieving comparable or higher accuracy.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1181–1191},
numpages = {11},
keywords = {outlier detection, local outlier, kernel density estimation, data stream, anomaly detection},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3474624.3477070,
author = {Martins, Luana and Bezerra, Carla and Costa, Heitor and Machado, Ivan},
title = {Smart prediction for refactorings in the software test code},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3477070},
doi = {10.1145/3474624.3477070},
abstract = {Test smells are bad practices to either design or implement a test code. Their presence may reduce the test code quality, harming the software testing activities, primarily from a maintenance perspective. Therefore, defining strategies and tools to handle test smells and improve the test code quality is necessary. State-of-the-art strategies encompass automated support mainly based on hard thresholds of rules, static and dynamic metrics to identify the test smells. Such thresholds are subjective to interpretation and may not consider the complexity of the software projects. Moreover, they are limited as they do not automate test refactoring but only count on developers’ expertise and intuition. In this context, a technique that uses historical implicit or tacit data to generate knowledge could assist the identification and refactoring of test smells. This study aims to establish a novel approach based on machine learning techniques to suggest developers refactoring strategies for test smells. As an expected result, we could understand the applicability of the machine learning techniques to handle test smells and a framework proposal that helps developers in decision-making regarding the refactoring of test smells.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {115–120},
numpages = {6},
keywords = {Test Smells, Software Quality, Machine Learning},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/1985374.1985386,
author = {M\i{}s\i{}rl\i{}, Ayse Tosun and \c{C}a\u{g}layan, Bora and Miranskyy, Andriy V. and Bener, Ay\c{s}e and Ruffolo, Nuzio},
title = {Different strokes for different folks: a case study on software metrics for different defect categories},
year = {2011},
isbn = {9781450305938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985374.1985386},
doi = {10.1145/1985374.1985386},
abstract = {Defect prediction has been evolved with variety of metric sets, and defect types. Researchers found code, churn, and network metrics as significant indicators of defects. However, all metric sets may not be informative for all defect categories such that only one metric type may represent majority of a defect category. Our previous study showed that defect category sensitive prediction models are more successful than general models, since each category has different characteristics in terms of metrics. We extend our previous work, and propose specialized prediction models using churn, code, and network metrics with respect to three defect categories. Results show that churn metrics are the best for predicting all defects. The strength of correlation for code and network metrics varies with defect category: Network metrics have higher correlations than code metrics for defects reported during functional testing and in the field, and vice versa for defects reported during system testing.},
booktitle = {Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics},
pages = {45–51},
numpages = {7},
keywords = {static code metrics, software defect prediction, network metrics, churn metrics},
location = {Waikiki, Honolulu, HI, USA},
series = {WETSoM '11}
}

@inproceedings{10.1007/978-3-030-41579-2_13,
author = {Lin, Guanjun and Xiao, Wei and Zhang, Jun and Xiang, Yang},
title = {Deep Learning-Based Vulnerable Function Detection: A Benchmark},
year = {2019},
isbn = {978-3-030-41578-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41579-2_13},
doi = {10.1007/978-3-030-41579-2_13},
abstract = {The application of Deep Learning (DL) technique for code analysis enables the rich and latent patterns within software code to be revealed, facilitating various downstream tasks such as the software defect and vulnerability detection. Many DL architectures have been applied for identifying vulnerable code segments in recent literature. However, the proposed studies were evaluated on self-constructed/-collected datasets. There is a lack of unified performance criteria, acting as a baseline for measuring the effectiveness of the proposed DL-based approaches. This paper proposes a benchmarking framework for building and testing DL-based vulnerability detectors, providing six built-in mainstream neural network models with three embedding solutions available for selection. The framework also offers easy-to-use APIs for integration of new network models and embedding methods. In addition, we constructed a real-world vulnerability ground truth dataset containing manually labelled 1,471 vulnerable functions and 1,320 vulnerable files from nine open-source software projects. With the proposed framework and the ground truth dataset, researchers can conveniently establish a vulnerability detection baseline system for comparison and evaluation. This paper also includes usage examples of the proposed framework, aiming to investigate the performance behaviours of mainstream neural network models and providing a reference for DL-based vulnerability detection at function-level.},
booktitle = {Information and Communications Security: 21st International Conference, ICICS 2019, Beijing, China, December 15–17, 2019, Revised Selected Papers},
pages = {219–232},
numpages = {14},
keywords = {Function-level detection, Neural network, Vulnerability detection},
location = {Beijing, China}
}

@article{10.1016/j.infsof.2021.106653,
author = {Xiao, Xi and Pan, Yuqing and Zhang, Bin and Hu, Guangwu and Li, Qing and Lu, Runiu},
title = {ALBFL: A novel neural ranking model for software fault localization via combining static and dynamic features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106653},
doi = {10.1016/j.infsof.2021.106653},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {11},
keywords = {Software quality, Learning to rank, Fault localization, Attention mechanism}
}

@article{10.1007/s10515-020-00270-x,
author = {Richter, Cedric and H\"{u}llermeier, Eyke and Jakobs, Marie-Christine and Wehrheim, Heike},
title = {Algorithm selection for software validation based on graph kernels},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00270-x},
doi = {10.1007/s10515-020-00270-x},
abstract = {Algorithm selection is the task of choosing an algorithm from a given set of candidate algorithms when faced with a particular problem instance. Algorithm selection via machine learning (ML) has recently been successfully applied for various problem classes, including computationally hard problems such as SAT. In this paper, we study algorithm selection for software validation, i.e., the task of choosing a software validation tool for a given validation instance. A validation instance consists of a program plus properties to be checked on it. The application of machine learning techniques to this task first of all requires an appropriate representation of software. To this end,
we propose a dedicated kernel function, which compares two programs in terms of their similarity, thus making the algorithm selection task amenable to kernel-based machine learning methods. Our kernel operates on a graph representation of source code mixing elements of control-flow and program-dependence graphs with abstract syntax trees.
Thus, given two such representations as input, the kernel function yields a real-valued score that can be interpreted as a degree of similarity. We experimentally evaluate our kernel in two learning scenarios, namely a classification and a ranking problem: (1) selecting between a verification and a testing tool for bug finding (i.e., property violation), and (2) ranking several verification tools,
from presumably best to worst, for property proving. The evaluation, which is based on data sets from the annual software verification competition SV-COMP, demonstrates our kernel to generalize well and to achieve rather high prediction accuracy, both for the classification and the ranking task.},
journal = {Automated Software Engg.},
month = jun,
pages = {153–186},
numpages = {34},
keywords = {Testing, Verification, Graph kernels, Machine learning, Software validation, Algorithm selection}
}

@article{10.1007/s10515-017-0229-y,
author = {Nizamani, Zeeshan Ahmed and Liu, Hui and Chen, David Matthew and Niu, Zhendong},
title = {Automatic approval prediction for software enhancement requests},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0229-y},
doi = {10.1007/s10515-017-0229-y},
abstract = {Software applications often receive a large number of enhancement requests that suggest developers to fulfill additional functions. Such requests are usually checked manually by the developers, which is time consuming and tedious. Consequently, an approach that can automatically predict whether a new enhancement report will be approved is beneficial for both the developers and enhancement suggesters. With the approach, according to their available time, the developers can rank the reports and thus limit the number of reports to evaluate from large collection of low quality enhancement requests that are unlikely to be approved. The approach can help developers respond to the useful requests more quickly. To this end, we propose a multinomial naive Bayes based approach to automatically predict whether a new enhancement report is likely to be approved or rejected. We acquire the enhancement reports of open-source software applications from Bugzilla for evaluation. Each report is preprocessed and modeled as a vector. Using these vectors with their corresponding approval status, we train a Bayes based classifier. The trained classifier predicts approval or rejection of the new enhancement reports. We apply different machine learning and neural network algorithms, and it turns out that the multinomial naive Bayes classifier yields the highest accuracy with the given dataset. The proposed approach is evaluated with 40,000 enhancement reports from 35 open source applications. The results of tenfold cross validation suggest that the average accuracy is up to 89.25%.},
journal = {Automated Software Engg.},
month = jun,
pages = {347–381},
numpages = {35},
keywords = {Software enhancements, Multinomial naive Bayes, Machine learning, Document classification}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.eswa.2019.01.082,
author = {Czibula, Gabriela and Czibula, Istvan Gergely and Miholca, Diana-Lucia and Crivei, Liana Maria},
title = {A novel concurrent relational association rule mining approach},
year = {2019},
issue_date = {Jul 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {125},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.01.082},
doi = {10.1016/j.eswa.2019.01.082},
journal = {Expert Syst. Appl.},
month = jul,
pages = {142–156},
numpages = {15},
keywords = {68T05, 68N30, 68M15, Concurrency, Relational association rules, Data mining}
}

@inproceedings{10.1145/1540438.1540458,
author = {Boetticher, Gary D.},
title = {From software engineer to day trader in 3 easy steps: a comparison of software engineering (SE) data mining with financial data mining},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540458},
doi = {10.1145/1540438.1540458},
abstract = {One of the research objectives in Software Engineering Data Mining is to produce useful, usable, verifiable, and repeatable models for better managing software processes and projects. This objective implies that an organization will be more profitable as consequence of better management.Financial events, such as bailouts, high unemployment rates, foreclosures, etc. have received extensive news coverage since Fall, 2008. Many professionals are concerned about their job status and their retirement accounts. From the context of the Software Engineering community, one question arises: To what extent can the skills and knowledge attained in Software Engineering Data Mining apply towards Financial Data Mining?A reader may be motivated to explore both types of data mining due to the potential rewards. Furthermore, by studying both domains makes it possible to determine the financial impact of delivering software on time or with fewer defects.This paper examines 3 aspects related to both types of data mining. The underlying data used for constructing models, the models themselves, and validation techniques.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {14},
numpages = {5},
keywords = {data mining, day trading, financial data mining, software engineering, stock market},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@inproceedings{10.1145/1134285.1134343,
author = {Li, Paul Luo and Herbsleb, James and Shaw, Mary and Robinson, Brian},
title = {Experiences and results from initiating field defect prediction and product test prioritization efforts at ABB Inc.},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134343},
doi = {10.1145/1134285.1134343},
abstract = {Quantitatively-based risk management can reduce the risks associated with field defects for both software producers and software consumers. In this paper, we report experiences and results from initiating risk-management activities at a large systems development organization. The initiated activities aim to improve product testing (system/integration testing), to improve maintenance resource allocation, and to plan for future process improvements. The experiences we report address practical issues not commonly addressed in research studies: how to select an appropriate modeling method for product testing prioritization and process improvement planning, how to evaluate accuracy of predictions across multiple releases in time, and how to conduct analysis with incomplete information. In addition, we report initial empirical results for two systems with 13 and 15 releases. We present prioritization of configurations to guide product testing, field defect predictions within the first year of deployment to aid maintenance resource allocation, and important predictors across both systems to guide process improvement planning. Our results and experiences are steps towards quantitatively-based risk management.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {413–422},
numpages = {10},
keywords = {system test prioritization, software reliability modeling, software and hardware configuration metrics, deployment and usage metrics},
location = {Shanghai, China},
series = {ICSE '06}
}

@article{10.1016/j.neucom.2011.08.040,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Software measurement data reduction using ensemble techniques},
year = {2012},
issue_date = {September, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2011.08.040},
doi = {10.1016/j.neucom.2011.08.040},
abstract = {Software defect prediction models are used to identify program modules that are high-risk, or likely to have a high number of faults. These models are built using software metrics which are collected during the software development process. Various techniques and approaches have been created for improving fault predictions. One of these is feature (metric) selection. Choosing the most important features is important to improve the effectiveness of defect predictors. However, using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. In this paper, we present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 54,400 classification models using four well known classifiers. The main conclusion is that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.},
journal = {Neurocomput.},
month = sep,
pages = {124–132},
numpages = {9},
keywords = {Feature selection, Ensembles of feature ranking techniques, Defect prediction}
}

@inproceedings{10.1007/978-3-319-95786-9_2,
author = {Tamminen, Satu and Tiensuu, Henna and Ferreira, Eija and Helaakoski, Heli and Kyll\"{o}nen, Vesa and Jokisaari, Juha and Puukko, Esa},
title = {From Measurements to Knowledge - Online Quality Monitoring and Smart Manufacturing},
year = {2018},
isbn = {978-3-319-95785-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-95786-9_2},
doi = {10.1007/978-3-319-95786-9_2},
abstract = {The purpose of this study was to develop an innovative supervisor system to assist the operators in an industrial manufacturing process to help discover new alternative solutions for improving both the products and the manufacturing process.This paper presents a solution for integrating different types of statistical modelling methods for a usable industrial application in quality monitoring. The two case studies demonstrating the usability of the tool were selected from a steel industry with different needs for knowledge presentation. The usability of the quality monitoring tool was tested in both case studies, both offline and online.},
booktitle = {Advances in Data Mining. Applications and Theoretical Aspects: 18th Industrial Conference, ICDM 2018, New York, NY, USA, July 11-12, 2018, Proceedings},
pages = {17–28},
numpages = {12},
keywords = {Data mining, Smart manufacturing, Online monitoring, Quality prediction, Knowledge representation, Machine learning},
location = {New York, NY, USA}
}

@article{10.1109/TSE.2005.112,
author = {Gyimothy, Tibor and Ferenc, Rudolf and Siket, Istvan},
title = {Empirical Validation of Object-Oriented Metrics on Open Source Software for Fault Prediction},
year = {2005},
issue_date = {October 2005},
publisher = {IEEE Press},
volume = {31},
number = {10},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2005.112},
doi = {10.1109/TSE.2005.112},
abstract = {Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database called Bugzilla using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.},
journal = {IEEE Trans. Softw. Eng.},
month = oct,
pages = {897–910},
numpages = {14},
keywords = {reverse engineering, open source software, metrics validation, fault-proneness detection, compiler wrapping, Mozilla, Index Terms- Fact extraction, Columbus., C++, Bugzilla}
}

@article{10.1016/j.engappai.2009.10.004,
author = {dos Santos Filho, Antonio Luiz and Javier Ramirez-Fernandez, Francisco},
title = {A neural network-based preset generation tool for a steel tandem cold mill},
year = {2010},
issue_date = {March, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {23},
number = {2},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.10.004},
doi = {10.1016/j.engappai.2009.10.004},
abstract = {This paper traces the development of a software tool, based on a combination of artificial neural networks (ANN) and a few process equations, aiming to serve as a backup operation instrument in the reference generation for real-time controllers of a steel tandem cold mill. By emulating the mathematical model responsible for generating presets under normal operational conditions, the system works as an option to maintain plant operation in the event of a failure in the processing unit that executes the mathematical model. The system, built from the production data collected over six years of plant operation, steered to the replacement of the former backup operation mode (based on a lookup table), which degraded both product quality and plant productivity. The study showed that ANN are appropriated tools for the intended purpose and that by this instrument it is possible to achieve nearly the totality of the presets needed by this kind of process. The text characterizes the problem, relates the investigated options to solve it, justifies the choice of the ANN approach, describes the methodology and system implementation and, finally, shows and discusses the attained results.},
journal = {Eng. Appl. Artif. Intell.},
month = mar,
pages = {169–176},
numpages = {8},
keywords = {Mill setup, Intelligent automation, Cold rolling, Artificial neural networks}
}

@article{10.1007/s11219-014-9230-x,
author = {Caglayan, Bora and Tosun Misirli, Ayse and Bener, Ayse Basar and Miranskyy, Andriy},
title = {Predicting defective modules in different test phases},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9230-x},
doi = {10.1007/s11219-014-9230-x},
abstract = {Defect prediction is a well-established research area in software engineering . Prediction models in the literature do not predict defect-prone modules in different test phases. We investigate the relationships between defects and test phases in order to build defect prediction models for different test phases. We mined the version history of a large-scale enterprise software product to extract churn and static code metrics. We used three testing phases that have been employed by our industry partner, namely function, system and field, to build a learning-based model for each testing phase. We examined the relation of different defect symptoms with the testing phases. We compared the performance of our proposed model with a benchmark model that has been constructed for the entire test phase (benchmark model). Our results show that building a model to predict defect-prone modules for each test phase significantly improves defect prediction performance and shortens defect detection time. The benefit analysis shows that using the proposed model, the defects are detected on the average 7 months earlier than the actual. The outcome of prediction models should lead to an action in a software development organization. Our proposed model gives a more granular outcome in terms of predicting defect-prone modules in each testing phase so that managers may better organize the testing teams and effort.},
journal = {Software Quality Journal},
month = jun,
pages = {205–227},
numpages = {23},
keywords = {Testing phase, Software testing, Defect prediction}
}

@article{10.1007/s10489-017-1078-x,
author = {Chatterjee, Subhashis and Maji, Bappa},
title = {A bayesian belief network based model for predicting software faults in early phase of software development process},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-017-1078-x},
doi = {10.1007/s10489-017-1078-x},
abstract = {It is always better to have an idea about the future situation of a present work. Prediction of software faults in the early phase of software development life cycle can facilitate to the software personnel to achieve their desired software product. Early prediction is of great importance for optimizing the development cost of a software project. The present study proposes a methodology based on Bayesian belief network, developed to predict total number of faults and to reach a target value of total number of faults during early development phase of software lifecycle. The model has been carried out using the information from similar or earlier version software projects, domain expert's opinion and the software metrics. Interval type-2 fuzzy logic has been applied for obtaining the conditional probability values in the node probability tables of the belief network. The output pattern corresponding to the total number of faults has been identified by artificial neural network using the input pattern from similar or earlier project data. The proposed Bayesian framework facilitates software personnel to gain the required information about software metrics at early phase for achieving targeted number of software faults. The proposed model has been applied on twenty six software project data. Results have been validated by different statistical comparison criterion. The performance of the proposed approach has been compared with some existing early fault prediction models.},
journal = {Applied Intelligence},
month = aug,
pages = {2214–2228},
numpages = {15},
keywords = {Software metrics, Interval type-2 fuzzy control system, Early fault prediction, Bayesian belief network, Artificial neural network}
}

@article{10.1007/s11219-019-09490-1,
author = {Kudjo, Patrick Kwaku and Chen, Jinfu and Mensah, Solomon and Amankwah, Richard and Kudjo, Christopher},
title = {The effect of Bellwether analysis on software vulnerability severity prediction models},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09490-1},
doi = {10.1007/s11219-019-09490-1},
abstract = {Vulnerability severity prediction (VSP) models provide useful insight for vulnerability prioritization and software maintenance. Previous studies have proposed a variety of machine learning algorithms as an important paradigm for VSP. However, to the best of our knowledge, there are no other existing research studies focusing on investigating how a subset of features can be used to improve VSP. To address this deficiency, this paper presents a general framework for VSP using the Bellwether analysis (i.e., exemplary data). First, we apply the natural language processing techniques to the textual descriptions of software vulnerability. Next, we developed an algorithm termed Bellvul to identify and select an exemplary subset of data (referred to as Bellwether) to be considered as the training set to yield improved prediction accuracy against the growing portfolio, within-project cases, and the k-fold cross-validation subset. Finally, we assessed the performance of four machine learning algorithms, namely, deep neural network, logistic regression, k-nearest neighbor, and random forest using the sampled instances. The prediction results of the suggested models and the benchmark techniques were assessed based on the standard classification evaluation metrics such as precision, recall, and F-measure. The experimental result shows that the Bellwether approach achieves F-measure ranging from 14.3% to 97.8%, which is an improvement over the benchmark techniques. In conclusion, the proposed approach is a promising research direction for assisting software engineers when seeking to predict instances of vulnerability records that demand much attention prior to software release.},
journal = {Software Quality Journal},
month = dec,
pages = {1413–1446},
numpages = {34},
keywords = {Severity, Machine learning algorithms, Feature selection, Software vulnerability, Bellwether}
}

@article{10.1016/j.peva.2012.09.004,
author = {Cotroneo, Domenico and Natella, Roberto and Pietrantuono, Roberto},
title = {Predicting aging-related bugs using software complexity metrics},
year = {2013},
issue_date = {March, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {70},
number = {3},
issn = {0166-5316},
url = {https://doi.org/10.1016/j.peva.2012.09.004},
doi = {10.1016/j.peva.2012.09.004},
abstract = {Long-running software systems tend to show degraded performance and an increased failure occurrence rate. This problem, known as Software Aging, which is typically related to the runtime accumulation of error conditions, is caused by the activation of the so-called Aging-Related Bugs (ARBs). This paper aims to predict the location of Aging-Related Bugs in complex software systems, so as to aid their identification during testing. First, we carried out a bug data analysis on three large software projects in order to collect data about ARBs. Then, a set of software complexity metrics were selected and extracted from the three projects. Finally, by using such metrics as predictor variables and machine learning algorithms, we built fault prediction models that can be used to predict which source code files are more prone to Aging-Related Bugs.},
journal = {Perform. Eval.},
month = mar,
pages = {163–178},
numpages = {16},
keywords = {Software complexity metrics, Software aging, Fault prediction, Aging-related bugs}
}

@inproceedings{10.1145/3293882.3338985,
author = {Kudjo, Patrick Kwaku and Chen, Jinfu},
title = {A cost-effective strategy for software vulnerability prediction based on bellwether analysis},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338985},
doi = {10.1145/3293882.3338985},
abstract = {Vulnerability Prediction Models (VPMs) aims to identify vulnerable and non-vulnerable components in large software systems. Consequently, VPMs presents three major drawbacks (i) finding an effective method to identify a representative set of features from which to construct an effective model. (ii) the way the features are utilized in the machine learning setup (iii) making an implicit assumption that parameter optimization would not change the outcome of VPMs. To address these limitations, we investigate the significant effect of the Bellwether analysis on VPMs. Specifically, we first develop a Bellwether algorithm to identify and select an exemplary subset of data to be considered as the Bellwether to yield improved prediction accuracy against the growing portfolio benchmark. Next, we build a machine learning approach with different parameter settings to show the improvement of performance of VPMs. The prediction results of the suggested models were assessed in terms of precision, recall, F-measure, and other statistical measures. The preliminary result shows the Bellwether approach outperforms the benchmark technique across the applications studied with F-measure values ranging from 51.1%-98.5%.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {424–427},
numpages = {4},
keywords = {Tuning, Software vulnerability, Machine learning, Bellwether},
location = {Beijing, China},
series = {ISSTA 2019}
}

@article{10.1016/j.datak.2008.10.005,
author = {Turhan, Burak and Bener, Ayse},
title = {Analysis of Naive Bayes' assumptions on software fault data: An empirical study},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {2},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2008.10.005},
doi = {10.1016/j.datak.2008.10.005},
abstract = {Software defect prediction is important for reducing test times by allocating testing resources effectively. In terms of predicting the defects in software, Naive Bayes outperforms a wide range of other methods. However, Naive Bayes assumes the 'independence' and 'equal importance' of attributes. In this work, we analyze these assumptions of Naive Bayes using public software defect data from NASA. Our analysis shows that independence assumption is not harmful for software defect data with PCA pre-processing. Our results also indicate that assigning weights to static code attributes may increase the prediction performance significantly, while removing the need for feature subset selection.},
journal = {Data Knowl. Eng.},
month = feb,
pages = {278–290},
numpages = {13},
keywords = {Software defect prediction, Naive Bayes, Empirical study}
}

@article{10.1016/j.infsof.2021.106652,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Zhang, Tao and Yang, Dan and Li, Wei},
title = {A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106652},
doi = {10.1016/j.infsof.2021.106652},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {16},
keywords = {Empirical study, Feature selection, Stack trace, Crash localization}
}

@inproceedings{10.1109/COMPSAC.2006.103,
author = {Simao, Adenilso da Silva and de Mello, Rodrigo Fernandes and Senger, Luciano Jose},
title = {A Technique to Reduce the Test Case Suites for Regression Testing Based on a Self-Organizing Neural Network Architecture},
year = {2006},
isbn = {0769526551},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2006.103},
doi = {10.1109/COMPSAC.2006.103},
abstract = {This paper presents a technique to select subsets of the test cases, reducing the time consumed during the evaluation of a new software version and maintaining the ability to detect defects introduced. Our technique is based on a model to classify test case suites by using an ART-2A selforganizing neural network architecture. Each test case is summarized in a feature vector, which contains all the relevant information about the software behavior. The neural network classifies feature vectors into clusters, which are labeled according to software behavior. The source code of a new software version is analyzed to determine the most adequate clusters from which the test case subset will be selected. Experiments compared feature vectors obtained from all-uses code coverage information to a random selection approach. Results confirm the new technique has improved the precision and recall metrics adopted.},
booktitle = {Proceedings of the 30th Annual International Computer Software and Applications Conference - Volume 02},
pages = {93–96},
numpages = {4},
series = {COMPSAC '06}
}

@inproceedings{10.1145/3387940.3392250,
author = {Rahman, Karishma and Kahanda, Indika and Kanewala, Upulee},
title = {MRpredT: Using Text Mining for Metamorphic Relation Prediction},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392250},
doi = {10.1145/3387940.3392250},
abstract = {Metamorphic relations (MRs) are an essential component of metamorphic testing (MT) that highly affects its fault detection effectiveness. MRs are usually identified with the help of a domain expert, which is a labor-intensive task. In this work, we explore the feasibility of a text classification-based machine learning approach to predict MRs using their program documentation as the sole input. We compare our method to our previously developed graph kernelbased machine learning approach and demonstrate that textual features extracted from program documentation are highly effective for predicting metamorphic relations for matrix calculation programs.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {420–424},
numpages = {5},
keywords = {Text classification, Metamorphic testing, Metamorphic relations},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3278142.3278145,
author = {Tu, Huy and Nair, Vivek},
title = {Is one hyperparameter optimizer enough?},
year = {2018},
isbn = {9781450360562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278142.3278145},
doi = {10.1145/3278142.3278145},
abstract = {Hyperparameter tuning is the black art of automatically finding a good combination of control parameters for a data miner. While widely applied in empirical Software Engineering, there has not been much discussion on which hyperparameter tuner is best for software analytics.To address this gap in the literature, this paper applied a range of hyperparameter optimizers (grid search, random search, differential evolution, and Bayesian optimization) to a defect prediction problem. Surprisingly, no hyperparameter optimizer was observed to be “best” and, for one of the two evaluation measures studied here (F-measure), hyperparameter optimization, in 50% of cases, was no better than using default configurations. We conclude that hyperparameter optimization is more nuanced than previously believed. While such optimization can certainly lead to large improvements in the performance of classifiers used in software analytics, it remains to be seen which specific optimizers should be applied to a new dataset.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics},
pages = {19–25},
numpages = {7},
keywords = {SBSE, Hyperparameter Tuning, Defect Prediction},
location = {Lake Buena Vista, FL, USA},
series = {SWAN 2018}
}

@article{10.1007/s10489-020-02141-0,
author = {Gan, Min and Zhang, Li},
title = {Iteratively local fisher score for feature selection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02141-0},
doi = {10.1007/s10489-020-02141-0},
abstract = {In machine learning, feature selection is a kind of important dimension reduction techniques, which aims to choose features with the best discriminant ability to avoid the issue of curse of dimensionality for subsequent processing. As a supervised feature selection method, Fisher score (FS) provides a feature evaluation criterion and has been widely used. However, FS ignores the association between features by assessing all features independently and loses the local information for fully connecting within-class samples. In order to solve these issues, this paper proposes a novel feature evaluation criterion based on FS, named iteratively local Fisher score (ILFS). Compared with FS, the new criterion pays more attention to the local structure of data by using K nearest neighbours instead of all samples when calculating the scatters of within-class and between-class. In order to consider the relationship between features, we calculate local Fisher scores of feature subsets instead of scores of single features, and iteratively select the current optimal feature to achieve this idea like sequential forward selection (SFS). Experimental results on UCI and TEP data sets show that the improved algorithm performs well in classification activities compared with some other state-of-the-art methods.},
journal = {Applied Intelligence},
month = aug,
pages = {6167–6181},
numpages = {15},
keywords = {Iterative, Neighbourhood, Fisher score, Feature selection}
}

@inproceedings{10.5555/3433701.3433786,
author = {Yazdi, Amirhessam and Lin, Xing and Yang, Lei and Yan, Feng},
title = {SEFEE: lightweight storage error forecasting in large-scale enterprise storage systems},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {With the rapid growth in scale and complexity, today's enterprise storage systems need to deal with significant amounts of errors. Existing proactive methods mainly focus on machine learning techniques trained using SMART measurements. However, such methods are usually expensive to use in practice and can only be applied to a limited types of errors with a limited scale. We collected more than 23-million storage events from 87 deployed NetApp-ONTAP systems managing 14,371 disks for two years and propose a lightweight training-free storage error forecasting method SEFEE. SEFEE employs Tensor Decomposition to directly analyze storage error-event logs and perform online error prediction for all error types in all storage nodes. SEFEE explores hidden spatio-temporal information that is deeply embedded in the global scale of storage systems to achieve record breaking error forecasting accuracy with minimal prediction overhead.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {64},
numpages = {14},
keywords = {training-free prediction, tensor decomposition, storage failures, lightweight forecasting, error prediction},
location = {Atlanta, Georgia},
series = {SC '20}
}

@article{10.1016/j.eswa.2016.05.018,
author = {Arar, \"{O}mer Faruk and Ayan, K\"{u}r\c{s}at},
title = {Deriving thresholds of software metrics to predict faults on open source software},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.05.018},
doi = {10.1016/j.eswa.2016.05.018},
abstract = {We empirically examined if there are effective thresholds for software metrics.Open-source software systems were used as benchmarking datasets.The learner model was created using logistic regression and the Bender method.Experimental results revealed that some metrics have effective threshold values. Object-oriented metrics aim to exhibit the quality of source code and give insight to it quantitatively. Each metric assesses the code from a different aspect. There is a relationship between the quality level and the risk level of source code. The objective of this paper is to empirically examine whether or not there are effective threshold values for source code metrics. It is targeted to derive generalized thresholds that can be used in different software systems. The relationship between metric thresholds and fault-proneness was investigated empirically in this study by using ten open-source software systems. Three types of fault-proneness were defined for the software modules: non-fault-prone, more-than-one-fault-prone, and more-than-three-fault-prone. Two independent case studies were carried out to derive two different threshold values. A single set was created by merging ten datasets and was used as training data by the model. The learner model was created using logistic regression and the Bender method. Results revealed that some metrics have threshold effects. Seven metrics gave satisfactory results in the first case study. In the second case study, eleven metrics gave satisfactory results. This study makes contributions primarily for use by software developers and testers. Software developers can see classes or modules that require revising; this, consequently, contributes to an increment in quality for these modules and a decrement in their risk level. Testers can identify modules that need more testing effort and can prioritize modules according to their risk levels.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {106–121},
numpages = {16},
keywords = {Threshold, Software quality metrics, Software fault prediction, Machine learning, Logistic regression, Bender method}
}

@inproceedings{10.1145/3475960.3475985,
author = {Bhandari, Guru and Naseer, Amara and Moonen, Leon},
title = {CVEfixes: automated collection of vulnerabilities and their fixes from open-source software},
year = {2021},
isbn = {9781450386807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475960.3475985},
doi = {10.1145/3475960.3475985},
abstract = {Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.},
booktitle = {Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {30–39},
numpages = {10},
keywords = {Security vulnerabilities, dataset, software repository mining, source code repair, vulnerability classification, vulnerability prediction},
location = {Athens, Greece},
series = {PROMISE 2021}
}

@article{10.1007/s10664-019-09778-7,
author = {Titcheu Chekam, Thierry and Papadakis, Mike and Bissyand\'{e}, Tegawend\'{e} F. and Le Traon, Yves and Sen, Koushik},
title = {Selecting fault revealing mutants},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09778-7},
doi = {10.1007/s10664-019-09778-7},
abstract = {Mutant selection refers to the problem of choosing, among a large number of mutants, the (few) ones that should be used by the testers. In view of this, we investigate the problem of selecting the fault revealing mutants, i.e., the mutants that are killable and lead to test cases that uncover unknown program faults. We formulate two variants of this problem: the fault revealing mutant selection and the fault revealing mutant prioritization. We argue and show that these problems can be tackled through a set of ‘static’ program features and propose a machine learning approach, named FaRM, that learns to select and rank killable and fault revealing mutants. Experimental results involving 1,692 real faults show the practical benefits of our approach in both examined problems. Our results show that FaRM achieves a good trade-off between application cost and effectiveness (measured in terms of faults revealed). We also show that FaRM outperforms all the existing mutant selection methods, i.e., the random mutant sampling, the selective mutation and defect prediction (mutating the code areas pointed by defect prediction). In particular, our results show that with respect to mutant selection, our approach reveals 23% to 34% more faults than any of the baseline methods, while, with respect to mutant prioritization, it achieves higher average percentage of revealed faults with a median difference between 4% and 9% (from the random mutant orderings).},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {434–487},
numpages = {54},
keywords = {Mutant prioritization, Mutant selection, Machine learning, Mutation testing}
}

@inproceedings{10.1145/3442167.3442177,
author = {Spring, Jonathan M. and Galyardt, April and Householder, Allen D. and VanHoudnos, Nathan},
title = {On managing vulnerabilities in AI/ML systems},
year = {2021},
isbn = {9781450389952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442167.3442177},
doi = {10.1145/3442167.3442177},
abstract = {This paper explores how the current paradigm of vulnerability management might adapt to include machine learning systems through a thought experiment: what if flaws in machine learning (ML) were assigned Common Vulnerabilities and Exposures (CVE) identifiers (CVE-IDs)? We consider both ML algorithms and model objects. The hypothetical scenario is structured around exploring the changes to the six areas of vulnerability management: discovery, report intake, analysis, coordination, disclosure, and response. While algorithm flaws are well-known in academic research community, there is no apparent clear line of communication between this research community and the operational communities that deploy and manage systems that use ML. The thought experiments identify some ways in which CVE-IDs may establish some useful lines of communication between these two communities. In particular, it would start to introduce the research community to operational security concepts, which appears to be a gap left by existing efforts.},
booktitle = {Proceedings of the New Security Paradigms Workshop 2020},
pages = {111–126},
numpages = {16},
keywords = {vulnerability management, prioritization, machine learning, CVE-ID},
location = {Online, USA},
series = {NSPW '20}
}

@article{10.1016/j.infsof.2019.106203,
author = {Raki\'{c}, Gordana and T\'{o}th, Melinda and Budimac, Zoran},
title = {Toward recursion aware complexity metrics},
year = {2020},
issue_date = {Feb 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {118},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106203},
doi = {10.1016/j.infsof.2019.106203},
journal = {Inf. Softw. Technol.},
month = feb,
numpages = {20},
keywords = {Complexity metrics, Source code complexity, Debugging, Source code comprehension, Source code readability, Software maintainability}
}

@article{10.1007/s00521-021-05905-y,
author = {Mishra, Pratik K. and Gautam, Chandan and Tiwari, Aruna},
title = {Minimum variance embedded auto-associative kernel extreme learning machine for one-class classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {19},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05905-y},
doi = {10.1007/s00521-021-05905-y},
abstract = {One-class classification (OCC) needs samples from only a single class to train the classifier. Recently, an auto-associative kernel extreme learning machine was developed for the OCC task. This paper introduces a novel extension of this classifier by embedding minimum variance information within its architecture and is referred to as VAAKELM. The minimum variance embedding forces the network output weights to focus in regions of low variance and reduces the intra-class variance. This leads to a better separation of target samples and outliers, resulting in an improvement in the generalization performance of the classifier. The proposed classifier follows a reconstruction-based approach to OCC and minimizes the reconstruction error by using the kernel extreme learning machine as the base classifier. It uses the deviation in reconstruction error to identify the outliers. We perform experiments on 15 small-size and 10 medium-size one-class benchmark datasets to demonstrate the efficiency of the proposed classifier. We compare the results with 13 existing one-class classifiers by considering the mean F1 score as the comparison metric. The experimental results show that VAAKELM consistently performs better than the existing classifiers, making it a viable alternative for the OCC task. The source code is available on the GitHub homepage:},
journal = {Neural Comput. Appl.},
month = oct,
pages = {12973–12987},
numpages = {15},
keywords = {One-class classification, Reconstruction-based, Kernel extreme learning machine, Minimum variance embedding}
}

@article{10.1016/j.infsof.2012.10.003,
author = {Turhan, Burak and Tosun M\i{}s\i{}rl\i{}, Ay\c{s}e and Bener, Ay\c{s}e},
title = {Empirical evaluation of the effects of mixed project data on learning defect predictors},
year = {2013},
issue_date = {June, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.10.003},
doi = {10.1016/j.infsof.2012.10.003},
abstract = {Context: Defect prediction research mostly focus on optimizing the performance of models that are constructed for isolated projects (i.e. within project (WP)) through retrospective analyses. On the other hand, recent studies try to utilize data across projects (i.e. cross project (CP)) for building defect prediction models for new projects. There are no cases where the combination of within and cross (i.e. mixed) project data are used together. Objective: Our goal is to investigate the merits of using mixed project data for binary defect prediction. Specifically, we want to check whether it is feasible, in terms of defect detection performance, to use data from other projects for the cases (i) when there is an existing within project history and (ii) when there are limited within project data. Method: We use data from 73 versions of 41 projects that are publicly available. We simulate the two above-mentioned cases, and compare the performances of naive Bayes classifiers by using within project data vs. mixed project data. Results: For the first case, we find that the performance of mixed project predictors significantly improves over full within project predictors (p-value&lt;0.001), however the effect size is small (Hedges' g=0.25). For the second case, we found that mixed project predictors are comparable to full within project predictors, using only 10% of available within project data (p-value=0.002, g=0.17). Conclusion: We conclude that the extra effort associated with collecting data from other projects is not feasible in terms of practical performance improvement when there is already an established within project defect predictor using full project history. However, when there is limited project history, e.g. early phases of development, mixed project predictions are justifiable as they perform as good as full within project models.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {1101–1118},
numpages = {18},
keywords = {Within project, Product metrics, Mixed project, Fault prediction, Defect prediction, Cross project}
}

@article{10.1016/j.jpdc.2017.06.022,
author = {Hussain, Shahid and Keung, Jacky and Khan, Arif Ali and Ahmad, Awais and Cuomo, Salvatore and Piccialli, Francesco and Jeon, Gwanggil and Akhunzada, Adnan},
title = {Implications of deep learning for the automation of design patterns organization},
year = {2018},
issue_date = {Jul 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2017.06.022},
doi = {10.1016/j.jpdc.2017.06.022},
journal = {J. Parallel Distrib. Comput.},
month = jul,
pages = {256–266},
numpages = {11},
keywords = {Classifiers, Performance, Feature set, Deep learning, Design patterns}
}

@article{10.1007/s11219-020-09525-y,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {An empirical study on predictability of software maintainability using imbalanced data},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09525-y},
doi = {10.1007/s11219-020-09525-y},
abstract = {In software engineering predictive modeling, early prediction of software modules or classes that possess high maintainability effort is a challenging task. Many prediction models are constructed to predict the maintainability of software classes or modules by applying various machine learning (ML) techniques. If the software modules or classes need&nbsp;high maintainability, effort would be reduced&nbsp;in a dataset, and&nbsp;there would be imbalanced data to train the model. The imbalanced datasets make&nbsp;ML techniques bias their predictions towards low maintainability effort or majority classes, and minority class instances get discarded as noise by the machine learning (ML) techniques. In this direction, this paper presents empirical work to improve the performance of software maintainability prediction (SMP) models developed with ML techniques using imbalanced data. For developing the models, the imbalanced data is pre-processed by applying data resampling methods. Fourteen data resampling methods, including oversampling, undersampling, and hybrid resampling, are used in the study. The study results recommend that the safe-level synthetic minority oversampling technique (Safe-Level-SMOTE) is a useful method to deal with the imbalanced datasets and to develop competent prediction models to forecast software maintainability.},
journal = {Software Quality Journal},
month = dec,
pages = {1581–1614},
numpages = {34},
keywords = {Imbalanced learning, Data resampling, Machine learning, Software maintainability prediction}
}

@inproceedings{10.1145/3377811.3380369,
author = {Bertolino, Antonia and Guerriero, Antonio and Miranda, Breno and Pietrantuono, Roberto and Russo, Stefano},
title = {Learning-to-rank vs ranking-to-learn: strategies for regression testing in continuous integration},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380369},
doi = {10.1145/3377811.3380369},
abstract = {In Continuous Integration (CI), regression testing is constrained by the time between commits. This demands for careful selection and/or prioritization of test cases within test suites too large to be run entirely. To this aim, some Machine Learning (ML) techniques have been proposed, as an alternative to deterministic approaches. Two broad strategies for ML-based prioritization are learning-to-rank and what we call ranking-to-learn (i.e., reinforcement learning). Various ML algorithms can be applied in each strategy. In this paper we introduce ten of such algorithms for adoption in CI practices, and perform a comprehensive study comparing them against each other using subjects from the Apache Commons project. We analyze the influence of several features of the code under test and of the test process. The results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1–12},
numpages = {12},
keywords = {continuous integration, machine learning, regression testing, test prioritization, test selection},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/ICSE43902.2021.00086,
author = {Ma, Wei and Chekam, Thierry Titcheu and Papadakis, Mike and Harman, Mark},
title = {MuDelta: Delta-Oriented Mutation Testing at Commit Time},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00086},
doi = {10.1109/ICSE43902.2021.00086},
abstract = {To effectively test program changes using mutation testing, one needs to use mutants that are relevant to the altered program behaviours. We introduce MuDelta, an approach that identifies commit-relevant mutants; mutants that affect and are affected by the changed program behaviours. Our approach uses machine learning applied on a combined scheme of graph and vector-based representations of static code features. Our results, from 50 commits in 21 Coreutils programs, demonstrate a strong prediction ability of our approach; yielding 0.80 (ROC) and 0.50 (PR-Curve) AUC values with 0.63 and 0.32 precision and recall values. These predictions are significantly higher than random guesses, 0.20 (PR-Curve) AUC, 0.21 and 0.21 precision and recall, and subsequently lead to strong relevant tests that kill 45% more relevant mutants than randomly sampled mutants (either sampled from those residing on the changed component(s) or from the changed lines). Our results also show that MuDelta selects mutants with 27% higher fault revealing ability in fault introducing commits. Taken together, our results corroborate the conclusion that commit-based mutation testing is suitable and promising for evolving software.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {897–909},
numpages = {13},
keywords = {regression testing, mutation testing, machine learning, continuous integration, commit-relevant mutants},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ESEM.2017.49,
author = {Bin, Yi and Zhou, Kai and Lu, Hongmin and Zhou, Yuming and Xu, Baowen},
title = {Training data selection for cross-project defection prediction: which approach is better?},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.49},
doi = {10.1109/ESEM.2017.49},
abstract = {Background: Many relevancy filters have been proposed to select training data for building cross-project defect prediction (CPDP) models. However, up to now, there is no consensus about which relevancy filter is better for CPDP. Goal: In this paper, we conduct a thorough experiment to compare nine relevancy filters proposed in the recent literature. Method: Based on 33 publicly available data sets, we compare not only the retaining ratio of the original training data and the overlapping degree among the retained data but also the prediction performance of the resulting CPDP models under the ranking and classification scenarios. Results: In terms of retaining ratio and overlapping degree, there are important differences among these filters. According to the defect prediction performance, global filter always stays in the first level. Conclusions: For practitioners, it appears that there is no need to filter source project data, as this may lead to better defect prediction results.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {354–363},
numpages = {10},
keywords = {model, filter, defect prediction, cross-project},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@article{10.1007/s11219-020-09546-7,
author = {Oyetoyan, Tosin Daniel and Morrison, Patrick},
title = {An improved text classification modelling approach to identify security messages in heterogeneous projects},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09546-7},
doi = {10.1007/s11219-020-09546-7},
abstract = {Security remains under-addressed in many organisations, illustrated by the number of large-scale software security breaches. Preventing breaches can begin during software development if attention is paid to security during the software’s design and implementation. One approach to security assurance during software development is to examine communications between developers as a means of studying the security concerns of the project. Prior research has investigated models for classifying project communication messages (e.g., issues or commits) as security related or not. A known problem is that these models are project-specific, limiting their use by other projects or organisations. We investigate whether we can build a generic classification model that can generalise across projects. We define a set of security keywords by extracting them from relevant security sources, dividing them into four categories: asset, attack/threat, control/mitigation, and implicit. Using different combinations of these categories and including them in the training dataset, we built a classification model and evaluated it on industrial, open-source, and research-based datasets containing over 45 different products. Our model based on harvested security keywords as a feature set shows average recall from 55 to 86%, minimum recall from 43 to 71% and maximum recall from 60 to 100%. An average f-score between 3.4 and 88%, an average g-measure of at least 66% across all the dataset, and an average AUC of ROC from 69 to 89%. In addition, models that use externally sourced features outperformed models that use project-specific features on average by a margin of 26–44% in recall, 22–50% in g-measure, 0.4–28% in f-score, and 15–19% in AUC of ROC. Further, our results outperform a state-of-the-art prediction model for security bug reports in all cases. We find using sound statistical and effect size tests that (1) using harvested security keywords as features to train a text classification model improve classification models and generalise to other projects significantly. (2) Including features in the training dataset before model construction improve classification models significantly. (3) Different security categories represent predictors for different projects. Finally, we introduce new and promising approaches to construct models that can generalise across different independent projects.},
journal = {Software Quality Journal},
month = jun,
pages = {509–553},
numpages = {45},
keywords = {Machine learning, Software repository, Text classification, Classification model, Security}
}

@inproceedings{10.1145/2970276.2970339,
author = {Krishna, Rahul and Menzies, Tim and Fu, Wei},
title = {Too much automation? the bellwether effect and its implications for transfer learning},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970339},
doi = {10.1145/2970276.2970339},
abstract = {Transfer learning: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple bellwether transfer learner. Given N data sets, we find which one produce the best predictions on all the others. This bellwether data set is then used for all subsequent predictions (or, until such time as its predictions start failing-- at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) bellwethers are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {122–131},
numpages = {10},
keywords = {Transfer learning, Defect Prediction, Data Mining},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.1145/3442694,
author = {Bluemke, Ilona and Malanowska, Agnieszka},
title = {Software Testing Effort Estimation and Related Problems: A Systematic Literature Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442694},
doi = {10.1145/3442694},
abstract = {Although testing effort estimation is a very important task in software project management, it is rarely described in the literature. There are many difficulties in finding any useful methods or tools for this purpose. Solutions to many other problems related to testing effort calculation are published much more often. There is also no research focusing on both testing effort estimation and all related areas of software engineering. To fill this gap, we performed a systematic literature review on both questions. Although our primary objective was to find some tools or implementable metods for test effort estimation, we have quickly discovered many other interesting topics related to the main one. The main contribution of this work is the presentation of the testing effort estimation task in a very wide context, indicating the relations with other research fields. This systematic literature review presents a detailed overview of testing effort estimation task, including challenges and approaches to automating it and the solutions proposed in the literature. It also exhaustively investigates related research topics, classifying publications that can be found in connection to the testing effort according to seven criteria formulated on the basis of our research questions. We present here both synthesis of our finding and the deep analysis of the stated research problems.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {53},
numpages = {38},
keywords = {testing effort estimation-related problems, testing effort estimation, systematic literature review, Testing effort}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@article{10.1007/s10115-018-1241-7,
author = {Ndenga, Malanga Kennedy and Ganchev, Ivaylo and Mehat, Jean and Wabwoba, Franklin and Akdag, Herman},
title = {Performance and cost-effectiveness of change burst metrics in predicting software faults},
year = {2019},
issue_date = {July      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {60},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-018-1241-7},
doi = {10.1007/s10115-018-1241-7},
abstract = {The purpose of this study is to determine a type of software metric at file level exhibiting the best prediction performance. Studies have shown that software process metrics are better predictors of software faults than software product metrics. However, there is need for a specific software process metric which can guarantee the best fault prediction performances consistently across different experimental contexts. We collected software metrics data from Open Source Software projects. We used logistic regression and linear regression algorithms to predict bug status and number of bugs corresponding to a file, respectively. The prediction performance of these models was evaluated against numerical and graphical prediction model performance measures. We found that change burst metrics exhibit the best numerical performance measures and have the highest fault detection probability and least cost of misclassification of software components.},
journal = {Knowl. Inf. Syst.},
month = jul,
pages = {275–302},
numpages = {28},
keywords = {Software process metrics, Software faults, Performance measures, Cost of misclassification, Change burst}
}

@inproceedings{10.1145/3302541.3313101,
author = {Sch\"{o}rgenhumer, Andreas and Kahlhofer, Mario and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Can we Predict Performance Events with Time Series Data from Monitoring Multiple Systems?},
year = {2019},
isbn = {9781450362863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302541.3313101},
doi = {10.1145/3302541.3313101},
abstract = {Predicting performance-related events is an important part of proactive fault management. As a result, many approaches exist for the context of single systems. Surprisingly, despite its potential benefits, multi-system event prediction, i.e., using data from multiple, independent systems, has received less attention. We present ongoing work towards an approach for multi-system event prediction that works with limited data and can predict events for new systems. We present initial results showing the feasibility of our approach. Our preliminary evaluation is based on 20 days of continuous, preprocessed monitoring time series data of 90 independent systems. We created five multi-system machine learning models and compared them to the performance of single-system machine learning models. The results show promising prediction capabilities with accuracies and F1-scores over 90% and false-positive-rates below 10%.},
booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {9–12},
numpages = {4},
keywords = {supervised machine learning, multivariate timeseries, infrastructure monitoring data, event prediction},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1007/978-3-642-13792-1_11,
author = {Turhan, Burak and Bener, Ayse and Menzies, Tim},
title = {Regularities in learning defect predictors},
year = {2010},
isbn = {3642137911},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-13792-1_11},
doi = {10.1007/978-3-642-13792-1_11},
abstract = {Collecting large consistent data sets of real world software projects from a single source is problematic. In this study, we show that bug reports need not necessarily come from the local projects in order to learn defect prediction models. We demonstrate that using imported data from different sites can make it suitable for predicting defects at the local site. In addition to our previous work in commercial software, we now explore open source domain with two versions of an open source anti-virus software (Clam AV) and a subset of bugs in two versions of GNU gcc compiler, to mark the regularities in learning predictors for a different domain. Our conclusion is that there are surprisingly uniform assets of software that can be discovered with simple and repeated patterns in local or imported data using just a handful of examples.},
booktitle = {Proceedings of the 11th International Conference on Product-Focused Software Process Improvement},
pages = {116–130},
numpages = {15},
keywords = {software quality, defect prediction, cross-company, code metrics},
location = {Limerick, Ireland},
series = {PROFES'10}
}

@article{10.1016/j.jss.2019.110451,
author = {Agnelo, Jo\~{a}o and Laranjeiro, Nuno and Bernardino, Jorge},
title = {Using Orthogonal Defect Classification to characterize NoSQL database defects},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110451},
doi = {10.1016/j.jss.2019.110451},
journal = {J. Syst. Softw.},
month = jan,
numpages = {23},
keywords = {Software fault, Software defect, Orthogonal Defect Classification, NoSQL, Defect analysis}
}

@article{10.4018/IJITPM.2021100105,
author = {Muthukumaran V. and Joseph, Rose Bindu and Munirathanam, Meram and Jeyakumar, Balajee and Kumar, V. Vinoth},
title = {Improving Network Security Based on Trust-Aware Routing Protocols Using Long Short-Term Memory-Queuing Segment-Routing Algorithms},
year = {2021},
issue_date = {Oct 2021},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {4},
issn = {1938-0232},
url = {https://doi.org/10.4018/IJITPM.2021100105},
doi = {10.4018/IJITPM.2021100105},
abstract = {Defending all single connection failures for a particular system, segment routing issue, the switch will focus on the problems of selecting a small subset of trust-aware routing to improve the deep learning (DL). In the end, even if there were multiple path failures, these paths may introduce long-term, unnecessary overload in the proposed long short-term memory networks-based queuing routing segmentation (LSTM-QRS) experience of reducing traffic delays and adjusting traffic length by reducing network bandwidth. The critical factor is a novel traffic repair technique used to create a traffic repair path that switches to software-defined network (SDN) using multiple routing and providing additional flexibility in re-routing using long short-term memory networks (LSTM)-based queuing routing segment (LSTM-QRS) algorithms. It reduces the repair path length and recommends replacing the target-based traffic with the connection-based traffic fault detection router to avoid targeted traffic network congestion.},
journal = {Int. J. Inf. Technol. Proj. Manag.},
month = oct,
pages = {47–60},
numpages = {14},
keywords = {Trust-Aware Routing, Traffic Network, Support Vector Machine, Software-Defined Network, Queuing Routing Segment, Long Short-Term Memory Networks, K Nearest Neighbor}
}

@article{10.1504/ijiei.2021.118275,
author = {Mittal, Shruti and Nagpal, Chander Kumar},
title = {Reinforcement learning based predictive analytics framework for survival in stock market},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {3},
issn = {1758-8715},
url = {https://doi.org/10.1504/ijiei.2021.118275},
doi = {10.1504/ijiei.2021.118275},
abstract = {Contemporary research in stock market domain is limited to forecasting of the stock price from one day to one week. Such small period predictions cannot be of much help for continuous gainful survival in the stock market. In fact, there has to be predictive analytics framework which analyses the current situation in the holistic manner and provides the appropriate advice for selling/buying/no action along with the quantity resulting in significant gain for the user/investor. The proposed framework generates various reinforcement signals by applying statistical and machine learning techniques on historical data and studies their impact on the stock prices by analysing future data. The outcome of the process has been used to generate rewards, through the use of fuzzy logic, for various actions in a given state of the environment. Fully automated implementation of the proposed framework can help both institutional and common investor in taking the rational decision.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {294–327},
numpages = {33},
keywords = {single value decomposition, stock technical analysis, stock fundamental, fuzzy rule base, finite state machine, fuzzy sets and logic, reinforcement learning, stock market predictions, machine learning, statistical learning, predictive analytics}
}

@inproceedings{10.1145/3377812.3382146,
author = {Tufano, Michele and Kimko, Jason and Wang, Shiya and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and Poshyvanyk, Denys},
title = {DeepMutation: a neural mutation tool},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382146},
doi = {10.1145/3377812.3382146},
abstract = {Mutation testing can be used to assess the fault-detection capabilities of a given test suite. To this aim, two characteristics of mutation testing frameworks are of paramount importance: (i) they should generate mutants that are representative of real faults; and (ii) they should provide a complete tool chain able to automatically generate, inject, and test the mutants. To address the first point, we recently proposed an approach using a Recurrent Neural Network Encoder-Decoder architecture to learn mutants from ~787k faults mined from real programs. The empirical evaluation of this approach confirmed its ability to generate mutants representative of real faults. In this paper, we address the second point, presenting DeepMutation, a tool wrapping our deep learning model into a fully automated tool chain able to generate, inject, and test mutants learned from real faults.Video: https://sites.google.com/view/learning-mutation/deepmutation},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {29–32},
numpages = {4},
keywords = {software testing, neural networks, mutation testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1007/s10799-009-0062-5,
author = {Raja, Uzma and Tretter, Marietta J.},
title = {Antecedents of open source software defects: A data mining approach to model formulation, validation and testing},
year = {2009},
issue_date = {December  2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {10},
number = {4},
issn = {1385-951X},
url = {https://doi.org/10.1007/s10799-009-0062-5},
doi = {10.1007/s10799-009-0062-5},
abstract = {This paper develops tests and validates a model for the antecedents of open source software (OSS) defects, using Data and Text Mining. The public archives of OSS projects are used to access historical data on over 5,000 active and mature OSS projects. Using domain knowledge and exploratory analysis, a wide range of variables is identified from the process, product, resource, and end-user characteristics of a project to ensure that the model is robust and considers all aspects of the system. Multiple Data Mining techniques are used to refine the model and data is enriched by the use of Text Mining for knowledge discovery from qualitative information. The study demonstrates the suitability of Data Mining and Text Mining for model building. Results indicate that project type, end-user activity, process quality, team size and project popularity have a significant impact on the defect density of operational OSS projects. Since many organizations, both for profit and not for profit, are beginning to use Open Source Software as an economic alternative to commercial software, these results can be used in the process of deciding what software can be reasonably maintained by an organization.},
journal = {Inf. Technol. and Management},
month = dec,
pages = {235–251},
numpages = {17},
keywords = {Text mining, Project performance, Open source software, Model building, Data mining}
}

@inproceedings{10.1007/978-3-030-82136-4_43,
author = {Zhu, Ziye and Wang, Yu and Li, Yun},
title = {TroBo: A Novel Deep Transfer Model for&nbsp;Enhancing Cross-Project Bug&nbsp;Localization},
year = {2021},
isbn = {978-3-030-82135-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82136-4_43},
doi = {10.1007/978-3-030-82136-4_43},
abstract = {Bug localization, which aims to locate buggy files in the software project by leveraging bug reports, plays an important role in software quality control. Recently, many automatic bug localization methods based on historical bug-fix data (i.e., bug reports labeled with corresponding buggy code files) have been proposed. However, the lack of bug-fix data for software projects in the early stages of development limits the performance of most existing supervised learning methods. To address this issue, we propose a deep transfer bug localization model called TroBo, which can transfer shared knowledge from label-rich source project to the target project. Specifically, we accomplish the knowledge transfer on both the bug report and code file. For processing bug reports, which belong to informal text data, we design a soft attention-based module to alleviate the noise problem. For processing code files, we apply an adversarial strategy to learn the project-shared features, and additionally extract project-exclusive features for each project. Furthermore, a project-aware classifier is introduced in TroBo to avoid redundancy between shared and exclusive features. Extensive experiments on four large-scale real-world projects demonstrate that our model significantly outperforms the state-of-the-art techniques.},
booktitle = {Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14–16, 2021, Proceedings, Part I},
pages = {529–541},
numpages = {13},
keywords = {Bug-fix data, Attention mechanism, Adversarial training, Transfer learning, Bug localization},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3464968.3468409,
author = {Jafarinejad, Foad and Narasimhan, Krishna and Mezini, Mira},
title = {NerdBug: automated bug detection in neural networks},
year = {2021},
isbn = {9781450385411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464968.3468409},
doi = {10.1145/3464968.3468409},
abstract = {Despite the exponential growth of deep learning software during the last decade, there is a lack of tools to test and debug issues in deep learning programs. Current static analysis tools do not address challenges specific to deep learning as observed by past research on bugs specific to this area. Existing deep learning bug detection tools focus on specific issues like shape mismatches. In this paper, we present a vision for an abstraction-based approach to detect deep learning bugs and the plan to evaluate our approach. The motivation behind the abstraction-based approach is to be able to build an intermediate version of the neural network that can be analyzed in development time to provide live feedback programmers are used to with other kind of bugs.},
booktitle = {Proceedings of the 1st ACM International Workshop on AI and Software Testing/Analysis},
pages = {13–16},
numpages = {4},
keywords = {Machine Learning, Deep Learning, Debugging, Bug Detection},
location = {Virtual, Denmark},
series = {AISTA 2021}
}

@article{10.5555/3337636.3337642,
title = {Threshold-based empirical validation of object-oriented metrics on different severity levels},
year = {2019},
issue_date = {January 2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {2–3},
issn = {1758-8715},
abstract = {Software metrics has become desideratum for the fault-proneness, reusability and effort prediction. To enhance and intensify the sufficiency of object-oriented OO metrics, it is crucial to perceive the relationship between OO metrics and fault-proneness at distinct severity levels. This paper characterise on the investigation of the software parts with higher probability of occurrence of faults. We examined the effect of thresholds on the OO metrics and build the predictive model based on those threshold values. This paper also instanced on the empirical validation of threshold values calculated for the OO metrics for predicting faults at different severity levels and builds the statistical model using logistic regression. This paper depicts the detection of fault-proneness by extracting the relevant OO metrics and focus on those projects that falls outside the specified risk level for allocating the more resources to them. We presented the effects of threshold values at different risk levels and also validated results on the KC1 dataset using machine learning and different classifiers.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {231–262},
numpages = {32}
}

@inproceedings{10.1145/3121257.3121262,
author = {Czech, Mike and H\"{u}llermeier, Eyke and Jakobs, Marie-Christine and Wehrheim, Heike},
title = {Predicting rankings of software verification tools},
year = {2017},
isbn = {9781450351577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121257.3121262},
doi = {10.1145/3121257.3121262},
abstract = {Today, software verification tools have reached the maturity to be used for large scale programs. Different tools perform differently well on varying code. A software developer is hence faced with the problem of choosing a tool appropriate for her program at hand. A ranking of tools on programs could facilitate the choice. Such rankings can, however, so far only be obtained by running all considered tools on the program.  In this paper, we present a machine learning approach to predicting rankings of tools on programs. The method builds upon so-called label ranking algorithms, which we complement with appropriate kernels providing a similarity measure for programs. Our kernels employ a graph representation for software source code that mixes elements of control flow and program dependence graphs with abstract syntax trees. Using data sets from the software verification competition SV-COMP, we demonstrate our rank prediction technique to generalize well and achieve a rather high predictive accuracy (rank correlation &gt; 0.6).},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Analytics},
pages = {23–26},
numpages = {4},
keywords = {ranking, machine learning, Software verification},
location = {Paderborn, Germany},
series = {SWAN 2017}
}

@inproceedings{10.1145/2499393.2499398,
author = {Calikli, Gul and Bener, Ayse},
title = {An algorithmic approach to missing data problem in modeling human aspects in software development},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499398},
doi = {10.1145/2499393.2499398},
abstract = {Background: In our previous research, we built defect prediction models by using confirmation bias metrics. Due to confirmation bias developers tend to perform unit tests to make their programs run rather than breaking their code. This, in turn, leads to an increase in defect density. The performance of prediction model that is built using confirmation bias was as good as the models that were built with static code or churn metrics.Aims: Collection of confirmation bias metrics may result in partially "missing data" due to developers' tight schedules, evaluation apprehension and lack of motivation as well as staff turnover. In this paper, we employ Expectation-Maximization (EM) algorithm to impute missing confirmation bias data.Method: We used four datasets from two large-scale companies. For each dataset, we generated all possible missing data configurations and then employed Roweis' EM algorithm to impute missing data. We built defect prediction models using the imputed data. We compared the performances of our proposed models with the ones that used complete data.Results: In all datasets, when missing data percentage is less than or equal to 50% on average, our proposed model that used imputed data yielded performance results that are comparable with the performance results of the models that used complete data.Conclusions: We may encounter the "missing data" problem in building defect prediction models. Our results in this study showed that instead of discarding missing or noisy data, in our case confirmation bias metrics, we can use effective techniques such as EM based imputation to overcome this problem.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {10},
numpages = {10},
keywords = {confirmation bias, expectation maximisation (EM) algorithm, handling missing data, software defect prediction},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@article{10.1016/j.ins.2019.01.047,
author = {Azmi, Mohamed and Runger, George C. and Berrado, Abdelaziz},
title = {Interpretable regularized class association rules algorithm for classification in a categorical data space},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {483},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.01.047},
doi = {10.1016/j.ins.2019.01.047},
journal = {Inf. Sci.},
month = may,
pages = {313–331},
numpages = {19},
keywords = {Class association rules, Regularization, Pruning, Association rules, Ensemble learning, Classification}
}

@article{10.1007/s10515-014-0155-1,
author = {Huang, Liguo and Ng, Vincent and Persing, Isaac and Chen, Mingrui and Li, Zeheng and Geng, Ruili and Tian, Jeff},
title = {AutoODC: Automated generation of orthogonal defect classifications},
year = {2015},
issue_date = {March     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0155-1},
doi = {10.1007/s10515-014-0155-1},
abstract = {Orthogonal defect classification (ODC), the most influential framework for software defect classification and analysis, provides valuable in-process feedback to system development and maintenance. Conducting ODC classification on existing organizational defect reports is human-intensive and requires experts' knowledge of both ODC and system domains. This paper presents AutoODC, an approach for automating ODC classification by casting it as a supervised text classification problem. Rather than merely applying the standard machine learning framework to this task, we seek to acquire a better ODC classification system by integrating experts' ODC experience and domain knowledge into the learning process via proposing a novel relevance annotation framework. We have trained AutoODC using two state-of-the-art machine learning algorithms for text classification, Naive Bayes (NB) and support vector machine (SVM), and evaluated it on both an industrial defect report from the social network domain and a larger defect list extracted from a publicly accessible defect tracker of the open source system FileZilla. AutoODC is a promising approach: not only does it leverage minimal human effort beyond the human annotations typically required by standard machine learning approaches, but it achieves overall accuracies of 82.9 % (NB) and 80.7 % (SVM) on the industrial defect report, and accuracies of 77.5 % (NB) and 75.2 % (SVM) on the larger, more diversified open source defect list.},
journal = {Automated Software Engg.},
month = mar,
pages = {3–46},
numpages = {44},
keywords = {Text classification, Orthogonal defect classification (ODC), Natural language processing, Machine learning}
}

@inproceedings{10.1145/2897695.2897704,
author = {Akbarinasaji, Shirin and Soltanifar, Behjat and \c{C}a\u{g}layan, Bora and Bener, Ayse Basar and Miranskyy, Andriy and Filiz, Asli and Kramer, Bryan M. and Tosun, Ayse},
title = {A metric suite proposal for logical dependency},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897704},
doi = {10.1145/2897695.2897704},
abstract = {Logical dependencies refer to hidden interconnections among source files that are changed together in order to address an issue or change in the system. In this study we propose six metrics for logical dependency using heuristics on change time. We evaluated our proposed metrics using data from three different software companies. We also built defect prediction models with our proposed logical dependency metrics, commonly used baseline metrics, as well as combination of both. Result of our empirical studies shows that our proposed metrics are capable of capturing the characteristics of dependency among software components. It also shows the logical dependency metrics improve the performance of defect prediction models.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {57–63},
numpages = {7},
keywords = {software metrics, logical dependency, defect prediction model},
location = {Austin, Texas},
series = {WETSoM '16}
}

@article{10.1016/j.jss.2014.11.040,
author = {Hemmati, Hadi and Nagappan, Meiyappan and Hassan, Ahmed E.},
title = {Investigating the effect of "defect co-fix" on quality assurance resource allocation},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.11.040},
doi = {10.1016/j.jss.2014.11.040},
abstract = {Introducing the concept of "remaining defects" in QA resource prioritization.Introducing "co-fix-awareness" to dynamically rank source code files.Proposing two co-fix-aware ranking algorithms.Empirically comparing several QA resource prioritization algorithms.Investigating the applicability of search algorithms on QA resource prioritization. Allocation of resources to pre-release quality assurance (QA) tasks, such as source code analysis, peer review, and testing, is one of the challenges faced by a software project manager. The goal is to find as many defects as possible with the available QA resources prior to the release. This can be achieved by assigning more resources to the more defect-prone artifacts, e.g., components, classes, and methods. The state-of-the-art QA resource allocation approaches predict the defect-proneness of an artifact using the historical data of different software metrics, e.g., the number of previous defects and the changes in the artifact. Given a QA budget, an allocation technique selects the most defect-prone artifacts, for further investigation by the QA team. While there has been many research efforts on discovering more predictive software metrics and more effective defect prediction algorithms, the cost-effectiveness of the QA resource allocation approaches has always been evaluated by counting the number of defects per selected artifact. The problem with such an evaluation approach is that it ignores the fact that, in practice, fixing a software issue is not bounded to an artifact under investigation. In other words, one may start reviewing a file that is identified as defect-prone and detect a defect, but to fix the defect one may modify not only the defective part of the file under review, but also several other artifacts that are somehow related to the defective code (e.g., a method that calls the defective code). Such co-fixes (fixing several defects together) during analyzing/reviewing/testing of an artifact under investigation will change the number of remaining defects in the other artifacts. Therefore, a QA resource allocation approach is more effective if it prioritizes the artifacts that would lead to the smallest number of remaining defects. Investigating six medium-to-large releases of open source systems (Mylyn, Eclipse, and NetBeans, two releases each), we found that co-fixes happen quite often in software projects (30-42% of the fixes modify more than one artifact). Therefore, in this paper, we first introduce a new cost-effectiveness measure to evaluate QA resource allocation, based on the concept of "remaining defects" per file. We then propose several co-fix-aware prioritization approaches to dynamically optimize the new measure, based on the historical defect co-fixes. The evaluation of these approaches on the six releases shows that (a) co-fix-aware QA prioritization approaches improve the traditional defect prediction-based ones, in terms of density of remaining defects per file and (b) co-fix-aware QA prioritization can potentially benefit from search-based software engineering techniques.},
journal = {J. Syst. Softw.},
month = may,
pages = {412–422},
numpages = {11},
keywords = {Search-based prioritization, Defect prediction, Co-fix}
}

@inproceedings{10.1109/ICSE.2017.9,
author = {Guo, Jin and Cheng, Jinghui and Cleland-Huang, Jane},
title = {Semantically enhanced software traceability using deep learning techniques},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.9},
doi = {10.1109/ICSE.2017.9},
abstract = {In most safety-critical domains the need for trace-ability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts; however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links; however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {3–14},
numpages = {12},
keywords = {traceability, semantic representation, recurrent neural network, deep learning},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/3474624.3474627,
author = {Barros, Daniel and Horita, Flavio and Wiese, Igor and Silva, Kanan},
title = {A Mining Software Repository Extended Cookbook: Lessons learned from a literature review},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3474627},
doi = {10.1145/3474624.3474627},
abstract = {The main purpose of Mining Software Repositories (MSR) is to discover the latest enhancements and provide an insight into how to make improvements in a software project. In light of it, this paper updates the MSR findings of the original MSR Cookbook, by first conducting a systematic mapping study to elicit and analyze the state-of-the-art, and then proposing an extended version of the Cookbook. This extended Cookbook was built on four high-level themes, which were derived from the analysis of a list of 112 selected studies. Hence, it was used to consolidate the extended Cookbook as a contribution to practice and research in the following areas by: 1) including studies published in all available and relevant publication venues; 2) including and updating recommendations in all four high-level themes, with an increase of 84% in comments in this study when compared with the original MSR Cookbook; 3) summarizing the tools employed for each high-level theme; and 4) providing lessons learned for future studies. Thus, the extended Cookbook examined in this work can support new research projects, as upgraded recommendations and the lessons learned are available with the aid of samples and tools.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Software Repositories, Mining Software Repositories, Data Mining, Artificial Intelligence},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1016/j.infsof.2018.02.005,
author = {Agrawal, Amritanshu and Fu, Wei and Menzies, Tim},
title = {What is wrong with topic modeling? And how to fix it using search-based software engineering},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.02.005},
doi = {10.1016/j.infsof.2018.02.005},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {74–88},
numpages = {15},
keywords = {Differential evolution, Tuning, LDA, Stability, Topic modeling}
}

@article{10.1002/smr.2158,
author = {Grano, Giovanni and Titov, Timofey V. and Panichella, Sebastiano and Gall, Harald C.},
title = {Branch coverage prediction in automated testing},
year = {2019},
issue_date = {September 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {9},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2158},
doi = {10.1002/smr.2158},
abstract = {Software testing is crucial in continuous integration (CI). Ideally, at every commit, all the test cases should be executed, and moreover, new test cases should be generated for the new source code. This is especially true in a Continuous Test Generation (CTG) environment, where the automatic generation of test cases is integrated into the continuous integration pipeline. In this context, developers want to achieve a certain minimum level of coverage for every software build. However, executing all the test cases and, moreover, generating new ones for all the classes at every commit is not feasible. As a consequence, developers have to select which subset of classes has to be tested and/or targeted by test‐case generation. We argue that knowing a priori the branch coverage that can be achieved with test‐data generation tools can help developers into taking informed decision about those issues. In this paper, we investigate the possibility to use source‐code metrics to predict the coverage achieved by test‐data generation tools. We use four different categories of source‐code features and assess the prediction on a large data set involving more than 3'000 Java classes. We compare different machine learning algorithms and conduct a fine‐grained feature analysis aimed at investigating the factors that most impact the prediction accuracy. Moreover, we extend our investigation to four different search budgets. Our evaluation shows that the best model achieves an average 0.15 and 0.21 MAE on nested cross‐validation over the different budgets, respectively, on evosuite and randoop. Finally, the discussion of the results demonstrate the relevance of coupling‐related features for the prediction accuracy.In this paper, we predict the coverage achieved by test‐data generator tools using source‐code metrics. We build a Random Forest Regressor model with an average MAE of 0.2. This results substantially improve the performance of the state‐of‐art predictor.


image
image},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {18},
keywords = {software testing, machine learning, coverage prediction, automated software testing}
}

@inproceedings{10.1145/3210459.3210473,
author = {Mi, Qing and Keung, Jacky and Xiao, Yan and Mensah, Solomon and Mei, Xiupei},
title = {An Inception Architecture-Based Model for Improving Code Readability Classification},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210473},
doi = {10.1145/3210459.3210473},
abstract = {The process of classifying a piece of source code into a Readable or Unreadable class is referred to as Code Readability Classification. To build accurate classification models, existing studies focus on handcrafting features from different aspects that intuitively seem to correlate with code readability, and then exploring various machine learning algorithms based on the newly proposed features. On the contrary, our work opens up a new way to tackle the problem by using the technique of deep learning. Specifically, we propose IncepCRM, a novel model based on the Inception architecture that can learn multi-scale features automatically from source code with little manual intervention. We apply the information of human annotators as the auxiliary input for training IncepCRM and empirically verify the performance of IncepCRM on three publicly available datasets. The results show that: 1) Annotator information is beneficial for model performance as confirmed by robust statistical tests (i.e., the Brunner-Munzel test and Cliff's delta); 2) IncepCRM can achieve an improved accuracy against previously reported models across all datasets. The findings of our study confirm the feasibility and effectiveness of deep learning for code readability classification.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {139–144},
numpages = {6},
keywords = {Inception Architecture, Empirical Software Engineering, Deep Learning, Code Readability Classification},
location = {Christchurch, New Zealand},
series = {EASE '18}
}

@article{10.1007/s10664-017-9582-5,
author = {Kintis, Marinos and Papadakis, Mike and Papadopoulos, Andreas and Valvis, Evangelos and Malevris, Nicos and Le Traon, Yves},
title = {How effective are mutation testing tools? An empirical analysis of Java mutation testing tools with manual analysis and real faults},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9582-5},
doi = {10.1007/s10664-017-9582-5},
abstract = {Mutation analysis is a well-studied, fault-based testing technique. It requires testers to design tests based on a set of artificial defects. The defects help in performing testing activities by measuring the ratio that is revealed by the candidate tests. Unfortunately, applying mutation to real-world programs requires automated tools due to the vast number of defects involved. In such a case, the effectiveness of the method strongly depends on the peculiarities of the employed tools. Thus, when using automated tools, their implementation inadequacies can lead to inaccurate results. To deal with this issue, we cross-evaluate four mutation testing tools for Java, namely PIT, muJava, Major and the research version of PIT, PITRV, with respect to their fault-detection capabilities. We investigate the strengths of the tools based on: a) a set of real faults and b) manual analysis of the mutants they introduce. We find that there are large differences between the tools' effectiveness and demonstrate that no tool is able to subsume the others. We also provide results indicating the application cost of the method. Overall, we find that PITRV achieves the best results. In particular, PITRV outperforms the other tools by finding 6% more faults than the other tools combined.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2426–2463},
numpages = {38},
keywords = {Tool comparison, Real faults, Mutation testing, Human study, Fault detection}
}

@inproceedings{10.1145/3416505.3423561,
author = {Vasiliev, Roman and Koznov, Dmitrij and Chernishev, George and Khvorov, Aleksandr and Luciv, Dmitry and Povarov, Nikita},
title = {TraceSim: a method for calculating stack trace similarity},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423561},
doi = {10.1145/3416505.3423561},
abstract = {Many contemporary software products have subsystems for automatic crash reporting. However, it is well-known that the same bug can produce slightly different reports. To manage this problem, reports are usually grouped, often manually by developers. Manual triaging, however, becomes infeasible for products that have large userbases, which is the reason for many different approaches to automating this task. Moreover, it is important to improve quality of triaging due to a large volume of reports that needs to be processed properly. Therefore, even a relatively small improvement could play a significant role in the overall accuracy of report bucketing. The majority of existing studies use some kind of a stack trace similarity metric, either based on information retrieval techniques or string matching methods. However, it should be stressed that the quality of triaging is still insufficient.  In this paper, we describe TraceSim — a novel approach to this problem which combines TF-IDF, Levenshtein distance, and machine learning to construct a similarity metric. Our metric has been implemented inside an industrial-grade report triaging system. The evaluation on a manually labeled dataset shows significantly better results compared to baseline approaches.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {25–30},
numpages = {6},
keywords = {Stack Trace, Software Repositories, Software Engineering, Information Retrieval, Duplicate Crash Report, Duplicate Bug Report, Deduplication, Crash Stack, Crash Reports, Crash Report Deduplication, Automatic Problem Reporting Tools, Automatic Crash Reporting},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.1016/j.jss.2014.10.032,
author = {Moeyersoms, Julie and Junqu\'{e} de Fortuny, Enric and Dejaeger, Karel and Baesens, Bart and Martens, David},
title = {Comprehensible software fault and effort prediction},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {100},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.10.032},
doi = {10.1016/j.jss.2014.10.032},
abstract = {HighlightsWe argue that comprehensibility is crucial in software effort and fault prediction.We extracted new datasets based on the Android repository.ALPA extracts a tree that mimics the performance of the complex model.The extracted trees are not only comprehensible but also more accurate. Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data.},
journal = {J. Syst. Softw.},
month = feb,
pages = {80–90},
numpages = {11},
keywords = {Software fault and effort prediction, Rule extraction, Comprehensibility}
}

@article{10.1007/s10618-021-00755-7,
author = {Zhu, Ziye and Li, Yun and Wang, Yu and Wang, Yaojing and Tong, Hanghang},
title = {A deep multimodal model for bug localization},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-021-00755-7},
doi = {10.1007/s10618-021-00755-7},
abstract = {Bug localization utilizes the collected bug reports to locate the buggy source files. The state of the art falls short in handling the following three aspects, including (L1) the subtle difference between natural language and programming language, (L2) the noise in the bug reports and (L3) the multi-grained nature of programming language. To overcome these limitations, we propose a novel deep multimodal model named DeMoB for bug localization. It embraces three key features, each of which is tailored to address each of the three limitations. To be specific, the proposed DeMoB generates the multimodal coordinated representations for both bug reports and source files for addressing L1. It further incorporates the AttL encoder to process bug reports for addressing L2, and the MDCL encoder to process source files for addressing L3. Extensive experiments on four large-scale real-world data sets demonstrate that the proposed DeMoB significantly outperforms existing techniques.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {1369–1392},
numpages = {24},
keywords = {Multi-grained features, Attention mechanism, Multimodal learning, Bug report, Bug localization}
}

@inproceedings{10.1007/978-3-030-67667-4_8,
author = {Nedelkoski, Sasho and Bogatinovski, Jasmin and Acker, Alexander and Cardoso, Jorge and Kao, Odej},
title = {Self-supervised Log Parsing},
year = {2020},
isbn = {978-3-030-67666-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67667-4_8},
doi = {10.1007/978-3-030-67667-4_8},
abstract = {Logs are extensively used during the development and maintenance of software systems. They collect runtime events and allow tracking of code execution, which enables a variety of critical tasks such as troubleshooting and fault detection. However, large-scale software systems generate massive volumes of semi-structured log records, posing a major challenge for automated analysis. Parsing semi-structured records with free-form text log messages into structured templates is the first and crucial step that enables further analysis. Existing approaches rely on log-specific heuristics or manual rule extraction. These are often specialized in parsing certain log types, and thus, limit performance scores and generalization. We propose a novel parsing technique called NuLog that utilizes a self-supervised learning model and formulates the parsing task as masked language modeling (MLM). In the process of parsing, the model extracts summarizations from the logs in the form of a vector embedding. This allows the coupling of the MLM as pre-training with a downstream anomaly detection task. We evaluate the parsing performance of NuLog on 10 real-world log datasets and compare the results with 12 parsing techniques. The results show that NuLog outperforms existing methods in parsing accuracy with an average of 99% and achieves the lowest edit distance to the ground truth templates. Additionally, two case studies are conducted to demonstrate the ability of the approach for log-based anomaly detection in both supervised and unsupervised scenario. The results show that NuLog can be successfully used to support troubleshooting tasks. The implementation is available at .},
booktitle = {Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part IV},
pages = {122–138},
numpages = {17},
keywords = {Representation learning, Log parsing, Transformers, Anomaly detection, IT systems},
location = {Ghent, Belgium}
}

@article{10.1145/3447332.3447334,
author = {Islam, Md Rakibul and Zibran, Minhaz F.},
title = {What changes in where? an empirical study of bug-fixing change patterns},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1559-6915},
url = {https://doi.org/10.1145/3447332.3447334},
doi = {10.1145/3447332.3447334},
abstract = {A deep understanding of the common patterns of bug-fixing changes is useful in several ways: (a) such knowledge can help developers in proactively avoiding coding patterns that lead to bugs and (b) bug-fixing patterns are exploited in devising techniques for automatic bug localization and program repair.This work includes an in-depth quantitative and qualitative analysis over 4,653 buggy revisions of five software systems. Our study identifies 38 bug-fixing edit patterns and discovers 37 new patterns of nested code structures, which frequently host the bug-fixing edits. While some of the edit patterns were reported in earlier studies, these nesting patterns are new and were never targeted before.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jan,
pages = {18–34},
numpages = {17},
keywords = {vulnerability, source code, software, pattern, nesting, fault, error, empirical study, edits, defect, bug, analysis}
}

@article{10.1007/s10664-020-09906-8,
author = {Shu, Rui and Xia, Tianpei and Chen, Jianfeng and Williams, Laurie and Menzies, Tim},
title = {How to Better Distinguish Security Bug Reports (Using Dual Hyperparameter Optimization)},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09906-8},
doi = {10.1007/s10664-020-09906-8},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {37},
keywords = {Security bug report, Data pre-processing, Hyperparameter Optimization}
}

@inproceedings{10.1145/3340482.3342747,
author = {Lenarduzzi, Valentina and Martini, Antonio and Taibi, Davide and Tamburri, Damian Andrew},
title = {Towards surgically-precise technical debt estimation: early results and research roadmap},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342747},
doi = {10.1145/3340482.3342747},
abstract = {The concept of technical debt has been explored from many perspectives but its precise estimation is still under heavy empirical and experimental inquiry. We aim to understand whether, by harnessing approximate, data-driven, machine-learning approaches it is possible to improve the current techniques for technical debt estimation, as represented by a top industry quality analysis tool such as SonarQube. For the sake of simplicity, we focus on relatively simple regression modelling techniques and apply them to modelling the additional project cost connected to the sub-optimal conditions existing in the projects under study. Our results shows that current techniques can be improved towards a more precise estimation of technical debt and the case study shows promising results towards the identification of more accurate estimation of technical debt.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {37–42},
numpages = {6},
keywords = {Technical Debt, Machine Learning, Empirical Study},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1016/j.eswa.2010.09.136,
author = {Chiu, Nan-Hsing},
title = {Combining techniques for software quality classification: An integrated decision network approach},
year = {2011},
issue_date = {April, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.09.136},
doi = {10.1016/j.eswa.2010.09.136},
abstract = {Accurately predicting fault-prone modules is a major problem in quality control of a software system during software development. Selecting an appropriate suggestion from various software quality classification models is a difficult decision for software project managers. In this paper, an integrated decision network is proposed to combine the well-known software quality classification models in providing the summarized suggestion. A particle swarm optimization algorithm is used to search for suitable combinations among the software quality classification models in the integrated decision network. The experimental results show that the proposed integrated decision network outperforms the independent software quality classification models. It also provides an appropriate summary for decision makers.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {4618–4625},
numpages = {8},
keywords = {Software quality classification, Software project management, Particle swarm optimization, Integrated decision network, Decision support systems}
}

@article{10.1007/s10515-010-0073-9,
author = {Hall, Robert J.},
title = {Editorial: data mining in software engineering},
year = {2010},
issue_date = {December  2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0073-9},
doi = {10.1007/s10515-010-0073-9},
journal = {Automated Software Engg.},
month = dec,
pages = {373–374},
numpages = {2}
}

@inproceedings{10.1145/3377811.3380411,
author = {Zhao, Dehai and Xing, Zhenchang and Chen, Chunyang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui},
title = {Seenomaly: vision-based linting of GUI animation effects against design-don't guidelines},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380411},
doi = {10.1145/3377811.3380411},
abstract = {GUI animations, such as card movement, menu slide in/out, snackbar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform's UI design guidelines (referred to as design-don't guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can "see" the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don't guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial autoencoder to solve this linting problem. Our autoencoder learns to group similar GUI animations by "seeing" lots of unlabeled real-application GUI animations and learning to generate them. As the first work of its kind, we build the datasets of synthetic and real-world GUI animations. Through experiments on these datasets, we systematically investigate the learning capability of our model and its effectiveness and practicality for linting GUI animations, and identify the challenges in this linting problem for future work.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1286–1297},
numpages = {12},
keywords = {GUI animation, design guidelines, lint, unsupervised learning},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/1988997.1989017,
author = {Jeet, Kawal and Bhatia, Nitin and Minhas, Rajinder Singh},
title = {A bayesian network based approach for software defects prediction},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1988997.1989017},
doi = {10.1145/1988997.1989017},
abstract = {Since the inception of software project management, the quality of a software project is always a priority. Defects in a software development project lead to degradation of the quality which might be the cause of its failure. In order to predict the defects in a software development project we propose a Bayesian Network based approach. It is developed by using project repositories which store software metrics and defect information. In this approach we combine subjective judgements from experienced project managers and available defect rate data to produce a network and use this to forecast and hence control defect rates.},
journal = {SIGSOFT Softw. Eng. Notes},
month = aug,
pages = {1–5},
numpages = {5},
keywords = {software project management, software defects, bayesian networks, K2 algorithm}
}

@article{10.1016/j.jss.2019.110455,
author = {Sotiropolos, Panagiotis and Vassilakis, Costas},
title = {Detection of intermittent faults in software programs through identification of suspicious shared variable access patterns},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110455},
doi = {10.1016/j.jss.2019.110455},
journal = {J. Syst. Softw.},
month = jan,
numpages = {16},
keywords = {Model-based checking, Shared variables, Fault detection, Intermittent faults}
}

@article{10.1016/j.neucom.2018.04.069,
author = {Cejnek, Matous and Bukovsky, Ivo},
title = {Concept drift robust adaptive novelty detection for data streams},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.069},
doi = {10.1016/j.neucom.2018.04.069},
journal = {Neurocomput.},
month = oct,
pages = {46–53},
numpages = {8},
keywords = {Streaming data, Adaptive filter, Learning algorithms, Concept drift, Outlier detection, System change, Novelty detection}
}

@inproceedings{10.1145/3377816.3381738,
author = {Arokiam, Jude and Bradbury, Jeremy S.},
title = {Automatically predicting bug severity early in the development process},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381738},
doi = {10.1145/3377816.3381738},
abstract = {Bug severity is an important factor in prioritizing which bugs to fix first. The process of triaging bug reports and assigning a severity requires developer expertise and knowledge of the underlying software. Methods to automate the assignment of bug severity have been developed to reduce the developer cost, however, many of these methods require 70-90% of the project's bug reports as training data and delay their use until later in the development process. Not being able to automatically predict a bug report's severity early in a project can greatly reduce the benefits of automation. We have developed a new bug report severity prediction method that leverages how bug reports are written rather than what the bug reports contain. Our method allows for the prediction of bug severity at the beginning of the project by using an organization's historical data, in the form of bug reports from past projects, to train the prediction classifier. In validating our approach, we conducted over 1000 experiments on a dataset of five NASA robotic mission software projects. Our results demonstrate that our method was not only able to predict the severity of bugs earlier in development, but it was also able to outperform an existing keyword-based classifier for a majority of the NASA projects.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {17–20},
numpages = {4},
keywords = {natural language processing, machine learning, bug severity},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@article{10.1145/2557833.2560586,
author = {Peiris, Manjula and Hill, James H.},
title = {Towards detecting software performance anti-patterns using classification techniques},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2560586},
doi = {10.1145/2557833.2560586},
abstract = {This paper presents a non-intrusive machine learning approach called Non-intrusive Performance Anti-pattern Detecter (NiPAD) for identifying and classifying software performance anti-patterns. NiPAD uses only system performance metrics-as opposed to analyzing application level performance metrics or source code and the design of a software application to identify and classify software performance anti-patterns within an application. The results of applying NiPAD to an example application show that NiPAD is able to predict the One Lane Bridge software performance anti-pattern within a software application with 0.94 accuracy.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–4},
numpages = {4},
keywords = {software performance anti-patterns, machine learning, dynamic software analysis, classification}
}

@inproceedings{10.1007/11427469_146,
author = {Wang, Qi and Zhu, Jie and Yu, Bo},
title = {Combining classifiers in software quality prediction: a neural network approach},
year = {2005},
isbn = {3540259147},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11427469_146},
doi = {10.1007/11427469_146},
abstract = {Software quality prediction models seek to predict quality factors such as whether a component is fault prone or not. This can be treated as a kind of pattern recognition problem. In pattern recognition, there is a growing use of multiple classifier combinations with the goal to increase recognition performance. In this paper, we propose a neural network approach to combine multiple classifiers. The combination network consists of two neural networks: a Kohonen self-organization network and a multilayer perceptron network. The multilayer perceptron network is used as Dynamic Selection Network (DSN) and Kohonen self-organization network is served as the final combiner. A case study illustrates our approach and provides the evidence that the combination network with DSN performs better than some other popular combining schemes and the DSN can efficiently improve the performance of the combination network.},
booktitle = {Proceedings of the Second International Conference on Advances in Neural Networks - Volume Part III},
pages = {921–926},
numpages = {6},
location = {Chongqing, China},
series = {ISNN'05}
}

@inproceedings{10.5555/645654.665647,
author = {Kato, Yoshikiyo and Yairi, Takehisa and Hori, Koichi},
title = {Integrating Data Mining Techniques and Design Information Management for Failure Prevention},
year = {2001},
isbn = {3540430709},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Stories of the recent failures in complex systems tell us that they could have been avoided if the right information was presented to the right person at the right time. We propose a method for fault detection of spacecrafts by mining association rules from house keeping data. We also argue that merely detecting anomalies is not enough for failure prevention. We present a framework of design information management in order to capture and use design rationale for failure prevention. We believe that the framework provides the basis for improved development process and effective anomaly handling.},
booktitle = {Proceedings of the Joint JSAI 2001 Workshop on New Frontiers in Artificial Intelligence},
pages = {475–480},
numpages = {6}
}

@inproceedings{10.1145/3428757.3429149,
author = {Binlashram, Arwa and Bouricha, Hajer and Hsairi, Lobna and Al Ahmadi, Haneen},
title = {A new Multi-Agents System based on Blockchain for Prediction Anomaly from System Logs},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429149},
doi = {10.1145/3428757.3429149},
abstract = {The execution traces generated by an application contain information that the developers believed would be useful in debugging or monitoring the application, it contains application states and significant events at various critical points that help them gain insight into failures and identify and predict potential problems before they occur. Despite the ubiquity of these traces universally in almost all computer systems, they are rarely exploited because they are not readily machine-parsable. In this paper, we propose a Multi-Agents approach for prediction process using Blockchain technology, which allows automatically analysis of execution traces and detects early warning signals for system failure prediction during executing. The proposed prediction approach is constructed using a four-layer Multi-Agents system architecture. The proposed prediction approach performance is based on data prepossessing and supervised learning algorithms for prediction. Blockchain was used to coordinate collaboration between agents, and to synchronize prediction between agents and the administrators. We validated our approach by applying it to real-world distributed systems, where we predicted problems before they occurred with high accuracy. In this paper we will focus on the Architecture of our prediction approach.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {467–471},
numpages = {5},
keywords = {Prediction, Multi-Agents system, Machine learning, Execution traces, Blockchain technology, Agents},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@article{10.1016/j.infsof.2017.11.005,
author = {Boucher, Alexandre and Badri, Mourad},
title = {Software metrics thresholds calculation techniques to predict fault-proneness: An empirical comparison},
year = {2018},
issue_date = {Apr 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {96},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.11.005},
doi = {10.1016/j.infsof.2017.11.005},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {38–67},
numpages = {30},
keywords = {Object-oriented programming, Code quality, Cross-validation, Clustering, Machine learning, Fault-proneness prediction, Faults, Object-oriented metrics, Class-level metrics, Metrics thresholds}
}

@article{10.5555/2639037.2639042,
author = {Zazworka, Nico and Vetro', Antonio and Izurieta, Clemente and Wong, Sunny and Cai, Yuanfang and Seaman, Carolyn and Shull, Forrest},
title = {Comparing four approaches for technical debt identification},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
abstract = {Software systems accumulate technical debt (TD) when short-term goals in software development are traded for long-term goals (e.g., quick-and-dirty implementation to reach a release date versus a well-refactored implementation that supports the long-term health of the project). Some forms of TD accumulate over time in the form of source code that is difficult to work with and exhibits a variety of anomalies. A number of source code analysis techniques and tools have been proposed to potentially identify the code-level debt accumulated in a system. What has not yet been studied is if using multiple tools to detect TD can lead to benefits, that is, if different tools will flag the same or different source code components. Further, these techniques also lack investigation into the symptoms of TD "interest" that they lead to. To address this latter question, we also investigated whether TD, as identified by the source code analysis techniques, correlates with interest payments in the form of increased defect- and change-proneness. Comparing the results of different TD identification approaches to understand their commonalities and differences and to evaluate their relationship to indicators of future TD "interest." We selected four different TD identification techniques (code smells, automatic static analysis issues, grime buildup, and Modularity violations) and applied them to 13 versions of the Apache Hadoop open source software project. We collected and aggregated statistical measures to investigate whether the different techniques identified TD indicators in the same or different classes and whether those classes in turn exhibited high interest (in the form of a large number of defects and higher change-proneness). The outputs of the four approaches have very little overlap and are therefore pointing to different problems in the source code. Dispersed Coupling and Modularity violations were co-located in classes with higher defect-proneness. We also observed a strong relationship between Modularity violations and change-proneness. Our main contribution is an initial overview of the TD landscape, showing that different TD techniques are loosely coupled and therefore indicate problems in different locations of the source code. Moreover, our proxy interest indicators (change- and defect-proneness) correlate with only a small subset of TD indicators.},
journal = {Software Quality Journal},
month = sep,
pages = {403–426},
numpages = {24},
keywords = {Technical debt, Source code analysis, Software quality, Software maintenance, Modularity violations, Grime, Code smells, ASA}
}

@inproceedings{10.1145/2568225.2568269,
author = {Rahman, Foyzur and Khatri, Sameer and Barr, Earl T. and Devanbu, Premkumar},
title = {Comparing static bug finders and statistical prediction},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568269},
doi = {10.1145/2568225.2568269},
abstract = {The all-important goal of delivering better software at lower cost has led to a vital, enduring quest for ways to find and remove defects efficiently and accurately. To this end, two parallel lines of research have emerged over the last years. Static analysis seeks to find defects using algorithms that process well-defined semantic abstractions of code. Statistical defect prediction uses historical data to estimate parameters of statistical formulae modeling the phenomena thought to govern defect occurrence and predict where defects are likely to occur. These two approaches have emerged from distinct intellectual traditions and have largely evolved independently, in “splendid isolation”. In this paper, we evaluate these two (largely) disparate approaches on a similar footing. We use historical defect data to apprise the two approaches, compare them, and seek synergies. We find that under some accounting principles, they provide comparable benefits; we also find that in some settings, the performance of certain static bug-finders can be enhanced using information provided by statistical defect prediction.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {424–434},
numpages = {11},
keywords = {Software Quality, Inspection, Fault Prediction, Empirical Software Engineering, Empirical Research},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@article{10.1016/j.ins.2019.08.077,
author = {Peng, Zhendong and Xiao, Xi and Hu, Guangwu and Kumar Sangaiah, Arun and Atiquzzaman, Mohammed and Xia, Shutao},
title = {ABFL: An autoencoder based practical approach for software fault localization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {510},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.08.077},
doi = {10.1016/j.ins.2019.08.077},
journal = {Inf. Sci.},
month = feb,
pages = {108–121},
numpages = {14},
keywords = {Autoencoder, SBFL, Debugging, Fault localization}
}

@inproceedings{10.1145/3387940.3391464,
author = {Briem, J\'{o}n Arnar and Smit, Jordi and Sellik, Hendrig and Rapoport, Pavel and Gousios, Georgios and Aniche, Maur\'{\i}cio},
title = {OffSide: Learning to Identify Mistakes in Boundary Conditions},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391464},
doi = {10.1145/3387940.3391464},
abstract = {Mistakes in boundary conditions are the cause of many bugs in software. These mistakes happen when, e.g., developers make use of '&lt;' or '&gt;' in cases where they should have used '&lt;=' or '&gt;='. Mistakes in boundary conditions are often hard to find and manually detecting them might be very time-consuming for developers. While researchers have been proposing techniques to cope with mistakes in the boundaries for a long time, the automated detection of such bugs still remains a challenge. We conjecture that, for a tool to be able to precisely identify mistakes in boundary conditions, it should be able to capture the overall context of the source code under analysis. In this work, we propose a deep learning model that learn mistakes in boundary conditions and, later, is able to identify them in unseen code snippets. We train and test a model on over 1.5 million code snippets, with and without mistakes in different boundary conditions. Our model shows an accuracy from 55% up to 87%. The model is also able to detect 24 out of 41 real-world bugs; however, with a high false positive rate. The existing state-of-the-practice linter tools are not able to detect any of the bugs. We hope this paper can pave the road towards deep learning models that will be able to support developers in detecting mistakes in boundary conditions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {203–208},
numpages = {6},
keywords = {software testing, software engineering, machine learning for software testing, machine learning for software engineering, deep learning for software testing, boundary testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1007/s00165-021-00543-6,
author = {Yang, Zhibin and Bao, Yang and Yang, Yongqiang and Huang, Zhiqiu and Bodeveix, Jean-Paul and Filali, Mamoun and Gu, Zonghua},
title = {Exploiting augmented intelligence in the modeling of safety-critical autonomous systems},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {3},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00543-6},
doi = {10.1007/s00165-021-00543-6},
abstract = {Machine learning (ML) is used increasingly in safety-critical
systems to provide more complex autonomy to make the system to do
decisions by itself in uncertain environments. Using ML to learn
system features is fundamentally different from manually
implementing them in conventional components written in source code.
In this paper, we make a first step towards exploring
the architecture modeling of safety-critical autonomous systems
which are composed of conventional components and ML components,
based on natural language requirements. Firstly, augmented
intelligence for restricted natural language requirement
modeling is proposed. In that, several AI technologies such as
natural language processing and clustering are used to recommend
candidate terms to the glossary, as well as machine learning is used
to predict the category of requirements. The glossary including data
dictionary and domain glossary and the category of requirements will
be used in the restricted natural language requirement specification
method RNLReq, which is equipped with a set of restriction rules and
templates to structure and restrict the way how users document
requirements. Secondly, automatic generation of SysML architecture
models from the RNLReq requirement specifications is presented.
Thirdly, the prototype tool is implemented based on Papyrus.
Finally,  it presents the evaluation of the proposed
approach using an industrial autonomous guidance, navigation and
control case study.},
journal = {Form. Asp. Comput.},
month = jun,
pages = {343–384},
numpages = {42},
keywords = {SysML, Machine learning, Natural language processing, Restricted natural language requirements, Augmented intelligence, Safety-critical autonomous system}
}

@inproceedings{10.1145/3460319.3464834,
author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
title = {Empirically evaluating readily available information for regression test optimization in continuous integration},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464834},
doi = {10.1145/3460319.3464834},
abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90% of the failures, we show that practitioners can expect to save on average 84% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {491–504},
numpages = {14},
keywords = {software testing, regression test optimization, machine learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.5555/998675.999452,
author = {Brun, Yuriy and Ernst, Michael D.},
title = {Finding Latent Code Errors via Machine Learning over Program Executions},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper proposes a technique for identifying programproperties that indicate errors. The technique generates machinelearning models of program properties known to resultfrom errors, and applies these models to program propertiesof user-written code to classify and rank propertiesthat may lead the user to errors. Given a set of propertiesproduced by the program analysis, the technique selectssubset of properties that are most likely to reveal an error.An implementation, the Fault Invariant Classifier,demonstrates the efficacy of the technique. The implementationuses dynamic invariant detection to generate programproperties. It uses support vector machine and decision treelearning tools to classify those properties. In our experimentalevaluation, the technique increases the relevance(the concentration of fault-revealing properties) by a factorof 50 on average for the C programs, and 4.8 for the Javaprograms. Preliminary experience suggests that most of thefault-revealing properties do lead a programmer to an error.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {480–490},
numpages = {11},
series = {ICSE '04}
}

@inproceedings{10.1145/3482909.3482916,
author = {Camara, Bruno and Silva, Marco and Endo, Andre and Vergilio, Silvia},
title = {On the use of test smells for prediction of flaky tests},
year = {2021},
isbn = {9781450385039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482909.3482916},
doi = {10.1145/3482909.3482916},
abstract = {Regression testing is an important phase to deliver software with quality. However, flaky tests hamper the evaluation of test results and can increase costs. This is because a flaky test may pass or fail non-deterministically and to identify properly the flakiness of a test requires rerunning the test suite multiple times. To cope with this challenge, approaches have been proposed based on prediction models and machine learning. Existing approaches based on the use of the test case vocabulary may be context-sensitive and prone to overfitting, presenting low performance when executed in a cross-project scenario. To overcome these limitations, we investigate the use of test smells as predictors of flaky tests. We conducted an empirical study to understand if test smells have good performance as a classifier to predict the flakiness in the cross-project context, and analysed the information gain of each test smell. We also compared the test smell-based approach with the vocabulary-based one. As a result, we obtained a classifier that had a reasonable performance (Random Forest, 0.83%) to predict the flakiness in the testing phase. This classifier presented better performance than vocabulary-based model for cross-project prediction. The Assertion Roulette and Sleepy Test test smell types are the ones associated with the best information gain values.},
booktitle = {Proceedings of the 6th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {46–54},
numpages = {9},
keywords = {test smells, test flakiness, regression testing, machine learning},
location = {Joinville, Brazil},
series = {SAST '21}
}

@inproceedings{10.5555/2486788.2486839,
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
title = {Transfer defect learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {382–391},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1016/j.jksuci.2018.05.008,
author = {Bhatt, Arpita Jadhav and Gupta, Chetna and Mittal, Sangeeta},
title = {iABC-AL: Active learning-based privacy leaks threat detection for iOS applications},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {7},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.05.008},
doi = {10.1016/j.jksuci.2018.05.008},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = sep,
pages = {769–786},
numpages = {18},
keywords = {Active learning, Permission extraction, Static analysis, Information security, iOS applications}
}

@inproceedings{10.1145/3106237.3117766,
author = {Volf, Zahy and Shmueli, Edi},
title = {Screening heuristics for project gating systems},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117766},
doi = {10.1145/3106237.3117766},
abstract = {Continuous Integration (CI) is a hot topic. Yet, little attention is payed to how CI systems work and what impacts their behavior. In parallel, bug prediction in software is gaining high attention. But this is done mostly in the context of software engineering, and the relation to the realm of CI and CI systems engineering has not been established yet. In this paper we describe how Project Gating systems operate, which are a specific type of CI systems used to keep the mainline of development always clean. We propose and evaluate three heuristics for improving Gating performance and demonstrate their trade-offs. The third heuristic, which leverages state-of-the-art bug prediction achieves the best performance across the entire spectrum of workload conditions.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {872–877},
numpages = {6},
keywords = {Project Gating, Machine Learning, Continuous Integration},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/1294948.1294954,
author = {Aversano, Lerina and Cerulo, Luigi and Del Grosso, Concettina},
title = {Learning from bug-introducing changes to prevent fault prone code},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294954},
doi = {10.1145/1294948.1294954},
abstract = {A version control system, such as CVS/SVN, can provide the history of software changes performed during the evolution of a software project. Among all the changes performed there are some which cause the introduction of bugs, often resolved later with other changes.In this paper we use a technique to identify bug-introducing changes to train a model that can be used to predict if a new change may introduces or not a bug. We represent software changes as elements of a n-dimensional vector space of terms coordinates extracted from source code snapshots.The evaluation of various learning algorithms on a set of open source projects looks very promising, in particular for KNN (K-Nearest Neighbor algorithm) where a significant tradeoff between precision and recall has been obtained.},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {19–26},
numpages = {8},
keywords = {software evolution, mining software repositories, bug prediction},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.1007/978-3-642-39742-4_2,
author = {Yao, Xin},
title = {Some Recent Work on Multi-objective Approaches to Search-Based Software Engineering},
year = {2013},
isbn = {9783642397417},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39742-4_2},
doi = {10.1007/978-3-642-39742-4_2},
abstract = {Multi-objective algorithms have been used to solve difficult software engineering problems for a long time. This article summarises some selected recent work of applying latest meta-heuristic optimisation algorithms and machine learning algorithms to software engineering problems, including software module clustering, testing resource allocation in modular software system, protocol tuning, Java container testing, software project scheduling, software project effort estimation, and software defect prediction. References will be given, from which the details of such application of computational intelligence techniques to software engineering problems can be found.},
booktitle = {Proceedings of the 5th International Symposium on Search Based Software Engineering - Volume 8084},
pages = {4–15},
numpages = {12},
location = {St. Petersburg, Russia},
series = {SSBSE 2013}
}

@inproceedings{10.5555/2022348.2022375,
author = {Di Martino, Sergio and Ferrucci, Filomena and Gravino, Carmine and Sarro, Federica},
title = {A genetic algorithm to configure support vector machines for predicting fault-prone components},
year = {2011},
isbn = {9783642218422},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In some studies, Support Vector Machines (SVMs) have been turned out to be promising for predicting fault-prone software components. Nevertheless, the performance of the method depends on the setting of some parameters. To address this issue, we propose the use of a Genetic Algorithm (GA) to search for a suitable configuration of SVMs parameters that allows us to obtain optimal prediction performance. The approach has been assessed carrying out an empirical analysis based on jEdit data from the PROMISE repository. We analyzed both the inter- and the intra-release performance of the proposed method. As benchmarks we exploited SVMs with Grid-search and several other machine learning techniques. The results show that the proposed approach let us to obtain an improvement of the performance with an increasing of the Recall measure without worsening the Precision one. This behavior was especially remarkable for the inter-release use with respect to the other prediction techniques.},
booktitle = {Proceedings of the 12th International Conference on Product-Focused Software Process Improvement},
pages = {247–261},
numpages = {15},
keywords = {support vector machines, genetic algorithm, fault prediction},
location = {Torre Canne, Italy},
series = {PROFES'11}
}

@inproceedings{10.1145/3383219.3383268,
author = {Lenz, Luca and Felderer, Michael and Schwedes, Sascha and M\"{u}ller, Kai},
title = {Explainable Priority Assessment of Software-Defects using Categorical Features at SAP HANA},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383268},
doi = {10.1145/3383219.3383268},
abstract = {We want to automate priority assessment of software defects. To do so we provide a tool which uses an explainability-driven framework and classical machine learning algorithms to keep the decisions transparent. Differing from other approaches we only use objective and categorical fields from the bug tracking system as features. This makes our approach lightweight and extremely fast. We perform binary classification with priority labels corresponding to deadlines. Additionally, we evaluate the tool on real data to ensure good performance in the practical use case.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {366–367},
numpages = {2},
keywords = {software quality, machine learning, defect assessment, bug priority},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/225014.225018,
author = {Wong, W. Eric and Horgan, Joseph R. and London, Saul and Mathur, Aditya P.},
title = {Effect of test set minimization on fault detection effectiveness},
year = {1995},
isbn = {0897917081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/225014.225018},
doi = {10.1145/225014.225018},
booktitle = {Proceedings of the 17th International Conference on Software Engineering},
pages = {41–50},
numpages = {10},
location = {Seattle, Washington, USA},
series = {ICSE '95}
}

@inproceedings{10.1145/3434581.3434669,
author = {Yang, Borui and Fu, Guicui and Wan, Bo and Wang, Ye},
title = {Diagnosis and Prediction of Bearing Fault Using EEMD and CNN},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434669},
doi = {10.1145/3434581.3434669},
abstract = {Rolling bearing is a very essential component of the industrial machinery. The bearing fault could cause a significant loss. Therefore, it is necessary to perform fault diagnosis and prediction on the bearing. This paper combines Ensemble Empirical Mode Decomposition (EEMD), Singular Value Decomposition (SVD) difference spectrum de-noising, and the convolutional neural network (CNN) to realize the diagnosis and prediction of bearing faults. EEMD is used to extract features, and SVD difference spectrum de-noising is used to denoise the decomposed signals. The reconstructed vibration signals are then fed into CNN to realize fault diagnosis. Further, by analyzing the output of the softmax layer after the input of testing sets, the prediction of bearing fault can be realized. The bearing vibration signals are used to perform diagnosis. And we use partial samples of bearing fault full-period data to retrain CNN for prediction. In this paper, these methods successfully lead to bearing fault diagnosis with high accuracy and early bearing fault prediction.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {282–289},
numpages = {8},
keywords = {SVD, EEMD, CNN, Bearing fault Diagnosis and Prediction},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/1540438.1540446,
author = {Tosun, Ay\c{s}e and Turhan, Burak and Bener, Ay\c{s}e},
title = {Validation of network measures as indicators of defective modules in software systems},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540446},
doi = {10.1145/1540438.1540446},
abstract = {In ICSE'08, Zimmermann and Nagappan show that network measures derived from dependency graphs are able to identify critical binaries of a complex system that are missed by complexity metrics. The system used in their analysis is a Windows product. In this study, we conduct additional experiments on public data to reproduce and validate their results. We use complexity and network metrics from five additional systems. We examine three small scale embedded software and two versions of Eclipse to compare defect prediction performance of these metrics. We select two different granularity levels to perform our experiments: function-level and source file-level. In our experiments, we observe that network measures are important indicators of defective modules for large and complex systems, whereas they do not have significant effects on small scale projects.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {5},
numpages = {9},
keywords = {code metrics, defect prediction, network metrics, public datasets},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@article{10.1145/3345628,
author = {Kim, Yunho and Mun, Seokhyeon and Yoo, Shin and Kim, Moonzoo},
title = {Precise Learn-to-Rank Fault Localization Using Dynamic and Static Features of Target Programs},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3345628},
doi = {10.1145/3345628},
abstract = {Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information and structural complexity of program entities. All such information is used by GP to train a ranking model for fault localization. The empirical evaluation on 65 real-world faults from CoREBench, 84 artificial faults from SIR, and 310 real-world faults from Defects4J shows that PRINCE outperforms the state-of-the-art SBFL, MBFL, and learn-to-rank techniques significantly. PRINCE localizes a fault after reviewing 2.4% of the executed statements on average (4.2 and 3.0 times more precise than the best of the compared SBFL and MBFL techniques, respectively). Also, PRINCE ranks 52.9% of the target faults within the top ten suspicious statements.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {23},
numpages = {34},
keywords = {source file characteristics, mutation analysis, machine learning, Fault localization}
}

@inproceedings{10.1109/ICSE43902.2021.00085,
author = {Wang, Yaohui and Li, Guozheng and Wang, Zijian and Kang, Yu and Zhou, Yangfan and Zhang, Hongyu and Gao, Feng and Sun, Jeffrey and Yang, Li and Lee, Pochian and Xu, Zhangwei and Zhao, Pu and Qiao, Bo and Li, Liqun and Zhang, Xu and Lin, Qingwei},
title = {Fast Outage Analysis of Large-scale Production Clouds with Service Correlation Mining},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00085},
doi = {10.1109/ICSE43902.2021.00085},
abstract = {Cloud-based services are surging into popularity in recent years. However, outages, i.e., severe incidents that always impact multiple services, can dramatically affect user experience and incur severe economic losses. Locating the root-cause service, i.e., the service that contains the root cause of the outage, is a crucial step to mitigate the impact of the outage. In current industrial practice, this is generally performed in a bootstrap manner and largely depends on human efforts: the service that directly causes the outage is identified first, and the suspected root cause is traced back manually from service to service during diagnosis until the actual root cause is found. Unfortunately, production cloud systems typically contain a large number of interdependent services. Such a manual root cause analysis is often time-consuming and labor-intensive. In this work, we propose COT, the first outage triage approach that considers the global view of service correlations. COT mines the correlations among services from outage diagnosis data. After learning from historical outages, COT can infer the root cause of emerging ones accurately. We implement COT and evaluate it on a real-world dataset containing one year of data collected from Microsoft Azure, one of the representative cloud computing platforms in the world. Our experimental results show that COT can reach a triage accuracy of 82.1%~83.5%, which outperforms the state-of-the-art triage approach by 28.0%~29.7%.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {885–896},
numpages = {12},
keywords = {root cause analysis, outage triage, machine learning, cloud computing},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2961111.2962601,
author = {Soltanifar, Behjat and Erdem, Atakan and Bener, Ayse},
title = {Predicting Defectiveness of Software Patches},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962601},
doi = {10.1145/2961111.2962601},
abstract = {Context: Software code review, as an engineering best practice, refers to the inspection of the code change in order to find possible defects and ensure change quality. Code reviews, however, may not guarantee finding the defects. Thus, there is a risk for a defective code change in a given patch, to pass the review process and be submitted.Goal: In this research, we aim to apply different machine learning algorithms in order to predict the defectiveness of a patch after being reviewed, at the time of its submission.Method: We built three models using three different machine learning algorithms: Logistic Regression, Na\~{A}undefinedve Bayes, and Bayesian Network model. To build the models, we consider different factors involved in review process in terms of Product, Process and People (3P).Results: Our empirical results show that, Bayesian Networks is able to better predict the defectiveness of the changed code with 76% accuracy.Conclusions: Predicting defectiveness of change code is beneficial in making patch release decisions. The Bayesian Network model outperforms the others since it capturs the relationship among the factors in the review process.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {22},
numpages = {10},
keywords = {Software Patch Defectiveness, Defect Prediction, Code review, Code Review Quality},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Quality assurance, Machine learning, Software testing, Software variability, Configurable system, Software product line}
}

@article{10.1007/s11219-021-09568-9,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Copula-based software metrics aggregation},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09568-9},
doi = {10.1007/s11219-021-09568-9},
abstract = {A quality model is a conceptual decomposition of an abstract notion of quality into relevant, possibly conflicting characteristics and further into measurable metrics. For quality assessment and decision making, metrics values are aggregated to characteristics and ultimately to quality scores. Aggregation has often been problematic as quality models do not provide the semantics of aggregation. This makes it hard to formally reason about metrics, characteristics, and quality. We argue that aggregation needs to be interpretable and mathematically well defined in order to assess, to compare, and to improve quality. To address this challenge, we propose a probabilistic approach to aggregation and define quality scores based on joint distributions of absolute metrics values. To evaluate the proposed approach and its implementation under realistic conditions, we conduct empirical studies on bug prediction of ca. 5000 software classes, maintainability of ca. 15000 open-source software systems, and on the information quality of ca. 100000 real-world technical documents. We found that our approach is feasible, accurate, and scalable in performance.},
journal = {Software Quality Journal},
month = dec,
pages = {863–899},
numpages = {37},
keywords = {Copula, Probabilistic models, Multivariate statistical methods, Aggregation, Software metrics, Quantitative methods, Quality assessment}
}

@article{10.1145/2853073.2853083,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {A Decision Tree Regression based Approach for the Number of Software Faults Prediction},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853083},
doi = {10.1145/2853073.2853083},
abstract = {Software fault prediction is an important activity to make software quality assurance (SQA) process more efficient, economic and targeted. Most of earlier works related to software fault prediction have focused on classifying software modules as faulty or non-faulty. However, prediction of the number of faults in a given software module is not adequately investigated. In this paper, we explore the capability of decision tree regression (DTR) for the number of faults prediction in two different scenarios, intra-release prediction and inter-releases prediction for the given software system. The experimental study is performed over five open-source software projects with their nineteen releases collected from the PROMISE data repository. The predictive accuracy of DTR is evaluated using absolute error and relative error, prediction at level l and goodness-of-t measure. The results show that decision tree regression produced significant prediction accuracy for the number of faults prediction in both the considered scenarios. The relative comparison of intra-release and inter-releases fault prediction shows that intra-project prediction produced better accuracy compared to inter-releases prediction across all the datasets},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–6},
numpages = {6},
keywords = {Software Fault Prediction, Number of Faults Prediction, Decision Tree Regression}
}

@inproceedings{10.1109/COMPSAC.2015.143,
author = {Mori, Keita and Mizuno, Osamu},
title = {An Implementation of Just-in-Time Fault-Prone Prediction Technique Using Text Classifier},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.143},
doi = {10.1109/COMPSAC.2015.143},
abstract = {Since the fault prediction is an important technique to help allocating software maintenance effort, much research on fault prediction has been proposed so far. The goal of these studies is applying their prediction technique to actual software development. In this paper, we implemented a prototype fault-prone module prediction tool using a text-filtering based technique named "Fault-Prone Filtering". Our tool aims to show the result of fault prediction for each change (i.e., Commits) as a probability that a source code file to be faulty. The result is shown on a Web page and easy to track the histories of prediction. A case study performed on three open source projects shows that our tool could detect 90 percent of the actual fault modules (i.e., The recall of 0.9) with the accuracy of 0.67 and the precision of 0.63 on average.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 03},
pages = {609–612},
numpages = {4},
keywords = {spam filter, software maintenance, software development support tool, mining software repository, machine learning, fault prediction},
series = {COMPSAC '15}
}

@article{10.1016/j.neucom.2020.01.120,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {An empirical study to investigate the impact of data resampling techniques on the performance of class maintainability prediction models},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {459},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2020.01.120},
doi = {10.1016/j.neucom.2020.01.120},
journal = {Neurocomput.},
month = oct,
pages = {432–453},
numpages = {22},
keywords = {Object-oriented metrics, Search-based techniques, Machine learning techniques, Data resampling techniques, Imbalanced data, Maintainability prediction}
}

@article{10.1016/j.neucom.2019.11.002,
author = {Fu, Xiao and Li, Kenli and Liu, Jing and Li, Keqin and Zeng, Zeng and Chen, Cen},
title = {A two-stage attention aware method for train bearing shed oil inspection based on convolutional neural networks},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {380},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.002},
doi = {10.1016/j.neucom.2019.11.002},
journal = {Neurocomput.},
month = mar,
pages = {212–224},
numpages = {13},
keywords = {99-00, 00-01, Transfer learning, Railway inspection, Object detection, Image segmentation, Attention mechanism}
}

@inproceedings{10.1007/978-3-030-88106-1_7,
author = {Olsthoorn, Mitchell and Panichella, Annibale},
title = {Multi-objective Test Case Selection Through Linkage Learning-Based Crossover},
year = {2021},
isbn = {978-3-030-88105-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88106-1_7},
doi = {10.1007/978-3-030-88106-1_7},
abstract = {Test Case Selection (TCS) aims to select a subset of the test suite to run for regression testing. The selection is typically based on past coverage and execution cost data. Researchers have successfully used multi-objective evolutionary algorithms (MOEAs), such as NSGA-II and its variants, to solve this problem. These MOEAs use traditional crossover operators to create new candidate solutions through genetic recombination. Recent studies in numerical optimization have shown that better recombinations can be made using machine learning, in particular linkage learning. Inspired by these recent advances in this field, we propose a new variant of NSGA-II, called L2-NSGA, that uses linkage learning to optimize test case selection. In particular, we use an unsupervised clustering algorithm to infer promising patterns among the solutions (subset of test suites). Then, these patterns are used in the next iterations of L2-NSGA to create solutions that preserve these inferred patterns. Our results show that our customizations make NSGA-II more effective for test case selection. The test suite sub-sets generated by L2-NSGA are less expensive and detect more faults than those generated by MOEAs used in the literature for regression testing.},
booktitle = {Search-Based Software Engineering: 13th International Symposium, SSBSE 2021, Bari, Italy, October 11–12, 2021, Proceedings},
pages = {87–102},
numpages = {16},
keywords = {Search-based software engineering, Multi-objective optimization, Test case selection, Regression testing},
location = {Bari, Italy}
}

@article{10.1007/s10664-021-10026-0,
author = {Silva, Camila Costa and Galster, Matthias and Gilson, Fabian},
title = {Topic modeling in software engineering research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10026-0},
doi = {10.1007/s10664-021-10026-0},
abstract = {Topic modeling using models such as Latent Dirichlet Allocation (LDA) is a text mining technique to extract human-readable semantic “topics” (i.e., word clusters) from a corpus of textual documents. In software engineering, topic modeling has been used to analyze textual data in empirical studies (e.g., to find out what developers talk about online), but also to build new techniques to support software engineering tasks (e.g., to support source code comprehension). Topic modeling needs to be applied carefully (e.g., depending on the type of textual data analyzed and modeling parameters). Our study aims at describing how topic modeling has been applied in software engineering research with a focus on four aspects: (1) which topic models and modeling techniques have been applied, (2) which textual inputs have been used for topic modeling, (3) how textual data was “prepared” (i.e., pre-processed) for topic modeling, and (4) how generated topics (i.e., word clusters) were named to give them a human-understandable meaning. We analyzed topic modeling as applied in 111 papers from ten highly-ranked software engineering venues (five journals and five conferences) published between 2009 and 2020. We found that (1) LDA and LDA-based techniques are the most frequent topic modeling techniques, (2) developer communication and bug reports have been modelled most, (3) data pre-processing and modeling parameters vary quite a bit and are often vaguely reported, and (4) manual topic naming (such as deducting names based on frequent words in a topic) is common.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {62},
keywords = {Literature analysis, Natural language processing, Text mining, Topic modeling}
}

@article{10.1016/j.eswa.2015.12.027,
author = {Kang, Pilsung and Kim, Dongil and Cho, Sungzoon},
title = {Semi-supervised support vector regression based on self-training with label uncertainty},
year = {2016},
issue_date = {June 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {51},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.12.027},
doi = {10.1016/j.eswa.2015.12.027},
abstract = {A new semi-supervised support vector regression method is proposed.Label distribution is estimated by probabilistic local reconstruction algorithm.Different oversampling rate is used based on uncertainty information.Expected margin based pattern selection is used to reduce the training complexity.The proposed method improves the prediction performance with lower time complexity. Dataset size continues to increase and data are being collected from numerous applications. Because collecting labeled data is expensive and time consuming, the amount of unlabeled data is increasing. Semi-supervised learning (SSL) has been proposed to improve conventional supervised learning methods by training from both unlabeled and labeled data. In contrast to classification problems, the estimation of labels for unlabeled data presents added uncertainty for regression problems. In this paper, a semi-supervised support vector regression (SS-SVR) method based on self-training is proposed. The proposed method addresses the uncertainty of the estimated labels for unlabeled data. To measure labeling uncertainty, the label distribution of the unlabeled data is estimated with two probabilistic local reconstruction (PLR) models. Then, the training data are generated by oversampling from the unlabeled data and their estimated label distribution. The sampling rate is different based on uncertainty. Finally, expected margin-based pattern selection (EMPS) is employed to reduce training complexity. We verify the proposed method with 30 regression datasets and a real-world problem: virtual metrology (VM) in semiconductor manufacturing. The experiment results show that the proposed method improves the accuracy by 8% compared with conventional supervised SVR, and the training time for the proposed method is 20% shorter than that of the benchmark methods.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {85–106},
numpages = {22},
keywords = {Virtual metrology, Support vector regression, Semiconductor manufacturing, Semi-supervised learning, Probabilistic local reconstruction, Data generation}
}

@inproceedings{10.1145/3269206.3271811,
author = {Loyola, Pablo and Gajananan, Kugamoorthy and Satoh, Fumiko},
title = {Bug Localization by Learning to Rank and Represent Bug Inducing Changes},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271811},
doi = {10.1145/3269206.3271811},
abstract = {In software development, bug localization is the process finding portions of source code associated to a submitted bug report. This task has been modeled as an information retrieval task at source code file, where the report is the query. In this work, we propose a model that, instead of working at file level, learns feature representations from source changes extracted from the project history at both syntactic and code change dependency perspectives to support bug localization. To that end, we structured an end-to-end architecture able to integrate feature learning and ranking between sets of bug reports and source code changes. We evaluated our model against the state of the art of bug localization on several real world software projects obtaining competitive results in both intra-project and cross-project settings. Besides the positive results in terms of model accuracy, as we are giving the developer not only the location of the bug associated to the report, but also the change that introduced, we believe this could give a broader context for supporting fixing tasks.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {657–665},
numpages = {9},
keywords = {source code changes, information retrieval, bug localization},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1007/s00521-020-05039-7,
author = {Zakeri Nasrabadi, Morteza and Parsa, Saeed and Kalaee, Akram},
title = {Format-aware learn&amp;fuzz: deep test data generation for efficient fuzzing},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {5},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05039-7},
doi = {10.1007/s00521-020-05039-7},
abstract = {Appropriate test data are a crucial factor to succeed in fuzz testing. Most of the real-world applications, however, accept complex structure inputs containing data surrounded by meta-data which is processed in several stages comprising of the parsing and rendering (execution). The complex structure of some input files makes it difficult to generate efficient test data automatically. The success of deep learning to cope with complex tasks, specifically generative tasks, has motivated us to exploit it in the context of test data generation for complicated structures such as PDF files. In this respect, a neural language model (NLM) based on deep recurrent neural networks (RNNs) is used to learn the structure of complex inputs. To target both the parsing and rendering steps of the software under test (SUT), our approach generates new test data while distinguishing between data and meta-data that significantly improve the input fuzzing. To assess the proposed approach, we have developed a modular file format fuzzer, IUST-DeepFuzz. Our experimental results demonstrate the relatively high coverage of MuPDF code by our proposed fuzzer, IUST-DeepFuzz, in comparison with the state-of-the-art tools such as learn&amp;fuzz, AFL, Augmented-AFL, and random fuzzing. In summary, our experiments with many deep learning models revealed the fact that the simpler the deep learning models applied to generate test data, the higher the code coverage of the software under test will be.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1497–1513},
numpages = {17},
keywords = {Deep learning, Recurrent neural network, Neural language model, Code coverage, File format fuzzing, Test data generation}
}

@inproceedings{10.1007/978-3-030-75762-5_31,
author = {Li, Ding and Li, Wenzhong and Chen, Yizhou and Lin, Mingkai and Lu, Sanglu},
title = {Learning-Based Dynamic Graph Stream&nbsp;Sketch},
year = {2021},
isbn = {978-3-030-75761-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-75762-5_31},
doi = {10.1007/978-3-030-75762-5_31},
abstract = {A graph stream is a kind of dynamic graph representation that consists of a consecutive sequence of edges where each edge is represented by two endpoints and a weight. Graph stream is widely applied in many application scenarios to describe the relationships in social networks, communication networks, academic collaboration networks, etc. Graph sketch mechanisms were proposed to summarize large-scale graphs by compact data structures with hash functions to support fast queries in a graph stream. However, the existing graph sketches use fixed-size memory and inevitably suffer from dramatic performance drops after a massive number of edge updates. In this paper, we propose a novel Dynamic Graph Sketch (DGS) mechanism, which is able to adaptively extend graph sketch size to mitigate the performance degradation caused by memory overload. The proposed DGS mechanism incorporates deep neural network structures with graph sketch to actively detect the query errors, and dynamically expand the memory size and hash space of a graph sketch to keep the error below a pre-defined threshold. We conducted extensive experiments on three real-world graph stream datasets, which show that DGS outperforms the state-of-the-arts with regard to the accuracy of different kinds of graph queries.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part I},
pages = {383–394},
numpages = {12},
keywords = {Sketch, Data stream, Graph stream}
}

@article{10.1007/s11219-017-9398-y,
author = {Ouriques, Jo\~{a}o Felipe and Cartaxo, Emanuela G. and Machado, Patr\'{\i}cia D.},
title = {Test case prioritization techniques for model-based testing: a replicated study},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9398-y},
doi = {10.1007/s11219-017-9398-y},
abstract = {Recently, several test case prioritization (TCP) techniques have been proposed to order test cases for achieving a goal during test execution, particularly, revealing faults sooner. In the model-based testing (MBT) context, such techniques are usually based on heuristics related to structural elements of the model and derived test cases. In this sense, techniques' performance may vary due to a number of factors. While empirical studies comparing the performance of TCP techniques have already been presented in literature, there is still little knowledge, particularly in the MBT context, about which factors may influence the outcomes suggested by a TCP technique. In a previous family of empirical studies focusing on labeled transition systems, we identified that the model layout, i.e., amount of branches, joins, and loops in the model, alone may have little influence on the effectiveness of TCP techniques investigated, whereas characteristics of test cases that actually fail definitely influences this aspect. However, we considered only synthetic artifacts in the study, which reduced the ability of representing properly the reality. In this paper, we present a replication of one of these studies, now with a larger and more representative selection of techniques and considering test suites from industrial systems as experimental objects. Our objective is to find out whether the results remain while increasing the validity in comparison to the original study. Results reinforce that there is no best performer among the investigated techniques and characteristics of test cases that fail represent an important factor, although adaptive random-based techniques are less affected by it.},
journal = {Software Quality Journal},
month = dec,
pages = {1451–1482},
numpages = {32},
keywords = {Test case prioritization, Model-based testing, Fault detection, Empirical evaluation}
}

@article{10.1155/2018/6791683,
author = {Ji, Haijin and Huang, Song and Gutierrez, Pedro Antonio},
title = {Kernel Entropy Component Analysis with Nongreedy L1-Norm Maximization},
year = {2018},
issue_date = {2018},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2018},
issn = {1687-5265},
url = {https://doi.org/10.1155/2018/6791683},
doi = {10.1155/2018/6791683},
abstract = {Kernel entropy component analysis (KECA) is a newly proposed dimensionality reduction (DR) method, which has showed superiority in many pattern analysis issues previously solved by principal component analysis (PCA). The optimized KECA (OKECA) is a state-of-the-art variant of KECA and can return projections retaining more expressive power than KECA. However, OKECA is sensitive to outliers and accused of its high computational complexities due to its inherent properties of L2-norm. To handle these two problems, we develop a new extension to KECA, namely, KECA-L1, for DR or feature extraction. KECA-L1 aims to find a more robust kernel decomposition matrix such that the extracted features retain information potential as much as possible, which is measured by L1-norm. Accordingly, we design a nongreedy iterative algorithm which has much faster convergence than OKECA’s. Moreover, a general semisupervised classifier is developed for KECA-based methods and employed into the data classification. Extensive experiments on data classification and software defect prediction demonstrate that our new method is superior to most existing KECA- and PCA-based approaches. Code has been also made publicly available.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {9}
}

@inproceedings{10.1145/3379597.3387448,
author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
title = {AIMMX: Artificial Intelligence Model Metadata Extractor},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387448},
doi = {10.1145/3379597.3387448},
abstract = {Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87% precision and 83% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42% of models in our sample citing their datasets, method reproducibility is more common at 72% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {81–92},
numpages = {12},
keywords = {Model Mining, Model Metadata, Model Catalog, Metadata Extraction, Machine Learning, Artificial Intelligence},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1007/s11219-012-9180-0,
author = {\c{C}al\i{}kl\i{}, G\"{u}l and Bener, Ay\c{s}e Ba\c{s}ar},
title = {Influence of confirmation biases of developers on software quality: an empirical study},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9180-0},
doi = {10.1007/s11219-012-9180-0},
abstract = {The thought processes of people have a significant impact on software quality, as software is designed, developed and tested by people. Cognitive biases, which are defined as patterned deviations of human thought from the laws of logic and mathematics, are a likely cause of software defects. However, there is little empirical evidence to date to substantiate this assertion. In this research, we focus on a specific cognitive bias, confirmation bias, which is defined as the tendency of people to seek evidence that verifies a hypothesis rather than seeking evidence to falsify a hypothesis. Due to this confirmation bias, developers tend to perform unit tests to make their program work rather than to break their code. Therefore, confirmation bias is believed to be one of the factors that lead to an increased software defect density. In this research, we present a metric scheme that explores the impact of developers' confirmation bias on software defect density. In order to estimate the effectiveness of our metric scheme in the quantification of confirmation bias within the context of software development, we performed an empirical study that addressed the prediction of the defective parts of software. In our empirical study, we used confirmation bias metrics on five datasets obtained from two companies. Our results provide empirical evidence that human thought processes and cognitive aspects deserve further investigation to improve decision making in software development for effective process management and resource allocation.},
journal = {Software Quality Journal},
month = jun,
pages = {377–416},
numpages = {40},
keywords = {Software psychology, Human factors, Defect prediction, Confirmation bias}
}

@article{10.1145/3057269,
author = {Kazmi, Rafaqut and Jawawi, Dayang N. A. and Mohamad, Radziah and Ghani, Imran},
title = {Effective Regression Test Case Selection: A Systematic Literature Review},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3057269},
doi = {10.1145/3057269},
abstract = {Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {29},
numpages = {32},
keywords = {fault detection ability, coverage, cost effectiveness, Software testing, SLR}
}

@inproceedings{10.1145/3243127.3243130,
author = {Ognawala, Saahil and Amato, Ricardo Nales and Pretschner, Alexander and Kulkarni, Pooja},
title = {Automatically assessing vulnerabilities discovered by compositional analysis},
year = {2018},
isbn = {9781450359726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243127.3243130},
doi = {10.1145/3243127.3243130},
abstract = {Testing is the most widely employed method to find vulnerabilities in real-world software programs. Compositional analysis, based on symbolic execution, is an automated testing method to find vulnerabilities in medium- to large-scale programs consisting of many interacting components. However, existing compositional analysis frameworks do not assess the severity of reported vulnerabilities. In this paper, we present a framework to analyze vulnerabilities discovered by an existing compositional analysis tool and assign CVSS3 (Common Vulnerability Scoring System v3.0) scores to them, based on various heuristics such as interaction with related components, ease of reachability, complexity of design and likelihood of accepting unsanitized input. By analyzing vulnerabilities reported with CVSS3 scores in the past, we train simple machine learning models. By presenting our interactive framework to developers of popular open-source software and other security experts, we gather feedback on our trained models and further improve the features to increase the accuracy of our predictions. By providing qualitative (based on community feedback) and quantitative (based on prediction accuracy) evidence from 21 open-source programs, we show that our severity prediction framework can effectively assist developers with assessing vulnerabilities.},
booktitle = {Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis},
pages = {16–25},
numpages = {10},
keywords = {vulnerability assessment, symbolic execution, software testing, compositional analysis},
location = {Montpellier, France},
series = {MASES 2018}
}

@article{10.1016/j.neunet.2020.01.015,
author = {Mygdalis, Vasileios and Tefas, Anastasios and Pitas, Ioannis},
title = {K-Anonymity inspired adversarial attack and multiple one-class classification defense},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {124},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2020.01.015},
doi = {10.1016/j.neunet.2020.01.015},
journal = {Neural Netw.},
month = apr,
pages = {296–307},
numpages = {12},
keywords = {Kernel learning, Deep SVDD, Adversarial attack, Adversarial defense, K-Anonymity}
}

@article{10.1145/3483424,
author = {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
title = {A Survey of AIOps Methods for Failure Management},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3483424},
doi = {10.1145/3483424},
abstract = {Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {81},
numpages = {45},
keywords = {artificial intelligence, failure management, IT operations and maintenance, AIOps}
}

@inproceedings{10.1145/3324884.3416532,
author = {Tian, Haoye and Liu, Kui and Kabor\'{e}, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Evaluating representation learning of code changes for predicting patch correctness in program repair},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416532},
doi = {10.1145/3324884.3416532},
abstract = {A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {981–992},
numpages = {12},
keywords = {distributed representation learning, embeddings, machine learning, patch correctness, program repair},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1109/ASE.2015.55,
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
title = {Predicting delays in software projects using networked classification},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.55},
doi = {10.1109/ASE.2015.55},
abstract = {Software projects have a high risk of cost and schedule overruns, which has been a source of concern for the software engineering community for a long time. One of the challenges in software project management is to make reliable prediction of delays in the context of constant and rapid changes inherent in software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether a subset of software tasks (among the hundreds to thousands of ongoing tasks) in a software project have a risk of being delayed. Our approach makes use of not only features specific to individual software tasks (i.e. local data) - as done in previous work - but also their relationships (i.e. networked data). In addition, using collective classification, our approach can simultaneously predict the degree of delay for a group of related tasks. Our evaluation results show a significant improvement over traditional approaches which perform classification on each task independently: achieving 46%--97% precision (49% improved), 46%--97% recall (28% improved), 56%--75% F-measure (39% improved), and 78%--95% Area Under the ROC Curve (16% improved).},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {353–364},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.4018/ijossp.2014040101,
author = {Syeed, M.M. Mahbubul and Hammouda, Imed and Syst\"{a}, Tarja},
title = {Prediction Models and Techniques for Open Source Software Projects: A Systematic Literature Review},
year = {2014},
issue_date = {April 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {2},
issn = {1942-3926},
url = {https://doi.org/10.4018/ijossp.2014040101},
doi = {10.4018/ijossp.2014040101},
abstract = {Open Source Software OSS is currently a widely adopted approach to developing and distributing software. For effective adoption of OSS, fundamental knowledge of project development is needed. This often calls for reliable prediction models to simulate project evolution and to envision project future. These models provide help in supporting preventive maintenance and building quality software. This paper reports on a systematic literature survey aimed at the identification and structuring of research that offer prediction models and techniques in analyzing OSS projects. In this review, we systematically selected and reviewed 52 peer reviewed articles that were published between January, 2000 and March, 2013. The study outcome provides insight in what constitutes the main contributions of the field, identifies gaps and opportunities, and distills several important future research directions.},
journal = {Int. J. Open Source Softw. Process.},
month = apr,
pages = {1–39},
numpages = {39},
keywords = {Systematic Literature Review, Prediction, Open Source Software, OSS Community, Fault Prediction}
}

@inproceedings{10.1007/978-3-030-91265-9_8,
author = {Lu, Yuteng and Sun, Weidi and Sun, Meng},
title = {Mutation Testing of Reinforcement Learning Systems},
year = {2021},
isbn = {978-3-030-91264-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91265-9_8},
doi = {10.1007/978-3-030-91265-9_8},
abstract = {Reinforcement Learning (RL), one of the most active research areas in artificial intelligence, focuses on goal-directed learning from interaction with an uncertain environment. RL systems play an increasingly important role in many aspects of society. Therefore, its safety issues have received more and more attention. Testing has achieved great success in ensuring safety of the traditional software systems. However, current testing approaches hardly consider RL systems. To fill this gap, we propose the first Mutation Testing technique specialized for RL systems. We define a series of mutation operators simulating possible problems RL systems may encounter. Next, we design test environments that could reveal possible problems within the RL systems. The mutation score specialized for RL systems is proposed to analyze the extent of potential faults and evaluate the quality of test environments. Our evaluation in three popular environments, namely FrozenLake, CartPole, and MountainCar demonstrates the practicability of the proposed techniques.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 7th International Symposium, SETTA 2021, Beijing, China, November 25–27, 2021, Proceedings},
pages = {143–160},
numpages = {18},
keywords = {AI Safety, Reinforcement Learning, Mutation Testing},
location = {Beijing, China}
}

@article{10.1145/3428264,
author = {Livinskii, Vsevolod and Babokin, Dmitry and Regehr, John},
title = {Random testing for C and C++ compilers with YARPGen},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428264},
doi = {10.1145/3428264},
abstract = {Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel® C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20% for LLVM and 40% for GCC. We also created tools for automating most of the common tasks related to compiler fuzzing; these tools are also useful for fuzzers other than ours.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {196},
numpages = {25},
keywords = {random testing, random program generation, compiler testing, compiler defect, automated testing}
}

@inproceedings{10.5555/3540261.3542395,
author = {Allamanis, Miltiadis and Jackson-Flux, Henry and Brockschmidt, Marc},
title = {Self-supervised bug detection and repair},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BUGLAB, an approach for self-supervised learning of bug detection and repair. BUGLAB co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BUGLAB improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2134},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1145/3361242.3361261,
author = {Zhu, Jing and Rong, Guoping and Huang, Guocheng and Gu, Shenghui and Zhang, He and Shao, Dong},
title = {JLLAR: A Logging Recommendation Plug-in Tool for Java},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361261},
doi = {10.1145/3361242.3361261},
abstract = {Logs are the execution results of logging statements in software systems after being triggered by various events, which is able to capture the dynamic behavior of software systems during runtime and provide important information for software analysis, e.g., issue tracking, performance monitoring, etc. Obviously, to meet this purpose, the quality of the logs is critical, which requires appropriately placement of logging statements. Existing research on this topic reveals that where to log? and what to log? are two most concerns when conducting logging practice in software development, which mainly relies on developers' personal skills, expertise and preference, rendering several problems impacting the quality of the logs inevitably. One of the reasons leading to this phenomenon might be that several recognized best practices(strategies as well) are easily neglected by software developers. Especially in those software projects with relatively large number of participants. To address this issue, we designed and implemented a plug-in tool (i.e., JLLAR) based on the Intellij IDEA, which applied machine learning technology to identify and create a set of rules reflecting commonly recognized logging practices. Based on this rule set, JLLAR can be used to scan existing source code to identify issues regarding the placement of logging statements. Moreover, JLLAR also provides automatic code completion and semi code completion (i.e., to provide recommendations) regarding logging practice to support software developers during coding.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {16},
numpages = {6},
keywords = {tool, machine learning, logging practice},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@article{10.1007/s11219-021-09549-y,
author = {Magalh\~{a}es, Claudio and Mota, Alexandre and Momente, Luis},
title = {UI Test case prioritization on an industrial setting: A search for the best criteria},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09549-y},
doi = {10.1007/s11219-021-09549-y},
abstract = {This work was developed in an industrial setting towards UI regression testing, where we do not have access to source code and the majority of test cases are manually executed (and only part of the regression-based test cases can be executed due to limited resources). Test case prioritization (TCP) is indicated for such a scenario. But characteristic of many TCP techniques is that they rely on source code coverage information, whereas we just have access to test cases, change requests, and their features. Thus, our goal is to investigate which criteria is the most relevant for prioritization. Thus, according to the&nbsp;literature we create an optimization model based on historical data. This model is embedded in a constraint solver designed for optimization. Our optimization function is based on the APFD (Average of the Percentage of Faults Detected) metric, but other metrics can be used as well. We have found that our partner already uses an appropriate criterion to identify failures which is statistically equivalent to other criteria used in experiments using our optimization model.},
journal = {Software Quality Journal},
month = jun,
pages = {381–403},
numpages = {23},
keywords = {Constraint satisfaction, Historical data, UI Test case prioritization}
}

@inproceedings{10.1145/3412841.3442029,
author = {Ferreira, Fabio and Silva, Luciana Lourdes and Valente, Marco Tulio},
title = {Software engineering meets deep learning: a mapping study},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442029},
doi = {10.1145/3412841.3442029},
abstract = {Deep Learning (DL) is being used nowadays in many traditional Software Engineering (SE) problems and tasks. However, since the renaissance of DL techniques is still very recent, we lack works that summarize and condense the most recent and relevant research conducted at the intersection of DL and SE. Therefore, in this paper, we describe the first results of a mapping study covering 81 papers about DL &amp; SE. Our results confirm that DL is gaining momentum among SE researchers over the years and that the top-3 research problems tackled by the analyzed papers are documentation, defect prediction, and testing.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1542–1549},
numpages = {8},
keywords = {deep learning, software engineering},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1145/3444944,
author = {Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
title = {A Survey on Causal Inference},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3444944},
doi = {10.1145/3444944},
abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {74},
numpages = {46},
keywords = {Treatment effect estimation; Representation learning}
}

@article{10.4018/IJOSSP.2015010104,
author = {Lal, Sangeeta and Sardana, Neetu and Sureka, Ashish},
title = {Two Level Empirical Study of Logging Statements in Open Source Java Projects},
year = {2015},
issue_date = {January 2015},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2015010104},
doi = {10.4018/IJOSSP.2015010104},
abstract = {Log statements present in source code provide important information to the software developers because they are useful in various software development activities. Most of the previous studies on logging analysis and prediction provide insights and results after analyzing only a few code constructs. In this paper, the authors perform an in-depth and large-scale analysis of logging code constructs at two levels. They answer nine research questions related to statistical and content analysis. Statistical analysis at file level reveals that fewer files consist of log statements but logged files have a greater complexity than that of non-logged files. Results show that a positive correlation exists between size and logging count of the logged files. Statistical analysis on catch-blocks show that try-blocks associated with logged catch-blocks have greater complexity than non-logged catch-blocks and the logging ratio of an exception type is project specific. Content-based analysis of catch-blocks reveals the presence of different topics in try-blocks associated with logged and non-logged catch-blocks.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {49–73},
numpages = {25},
keywords = {Tracing, Source Code Metrics, Source Code Analysis, Logging, Latent, Empirical Software Engineering and Measurement, Dirichlet Allocation LDA, Debugging}
}

@inproceedings{10.1145/2070821.2070829,
author = {Zhang, Dongmei and Dang, Yingnong and Lou, Jian-Guang and Han, Shi and Zhang, Haidong and Xie, Tao},
title = {Software analytics as a learning case in practice: approaches and experiences},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070829},
doi = {10.1145/2070821.2070829},
abstract = {Software analytics is to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services. In this position paper, we advocate that when applying analytic technologies in practice of software analytics, one should (1) incorporate a broad spectrum of domain knowledge and expertise, e.g., management, machine learning, large-scale data processing and computing, and information visualization; and (2) investigate how practitioners take actions on the produced information, and provide effective support for such information-based action taking. Our position is based on our experiences of successful technology transfer on software analytics at Microsoft Research Asia.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {55–58},
numpages = {4},
keywords = {technology transfer, software analytics, machine learning},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00091,
author = {Dola, Swaroopa and Dwyer, Matthew B. and Soffa, Mary Lou},
title = {Artifact: distribution-aware testing of neural networks using generative models},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00091},
doi = {10.1109/ICSE-Companion52605.2021.00091},
abstract = {The artifact used for the experimental evaluation of Distribution-Aware Testing of Neural Networks Using Generative Models is publicly available on GitHub and it is reusable. The artifact consists of python scripts, trained deep neural network model files and data required for running the experiments. It is also provided as a VirtualBox VM image for reproducing the paper results. Users should be familiar with using VirtualBox software and Linux platform to reproduce or reuse the artifact.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {205–206},
numpages = {2},
keywords = {test generation, test coverage, input validation, deep neural networks},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inbook{10.5555/3454287.3455330,
author = {Alam, Mejbah and Gottschlich, Justin and Tatbul, Nesime and Turek, Javier and Mattson, Timothy and Muzahid, Abdullah},
title = {A zero-positive learning approach for diagnosing software performance regressions},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The field of machine programming (MP), the automation of the development of software, is making notable research advances. This is, in part, due to the emergence of a wide range of novel techniques in machine learning. In this paper, we apply MP to the automation of software performance regression testing. A performance regression is a software performance degradation caused by a code change. We present AutoPerf – a novel approach to automate regression testing that utilizes three core techniques: (i) zero-positive learning, (ii) autoencoders, and (iii) hardware telemetry. We demonstrate AutoPerf's generality and efficacy against 3 types of performance regressions across 10 real performance bugs in 7 benchmark and open-source programs. On average, AutoPerf exhibits 4% profiling overhead and accurately diagnoses more performance bugs than prior state-of-the-art approaches. Thus far, AutoPerf has produced no false negatives.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1043},
numpages = {13}
}

@inproceedings{10.1109/ICSE.2019.00069,
author = {Cui, Di and Liu, Ting and Cai, Yuanfang and Zheng, Qinghua and Feng, Qiong and Jin, Wuxia and Guo, Jiaqi and Qu, Yu},
title = {Investigating the impact of multiple dependency structures on software defects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00069},
doi = {10.1109/ICSE.2019.00069},
abstract = {Over the past decades, numerous approaches were proposed to help practitioner to predict or locate defective files. These techniques often use syntactic dependency, history co-change relation, or semantic similarity. The problem is that, it remains unclear whether these different dependency relations will present similar accuracy in terms of defect prediction and localization. In this paper, we present our systematic investigation of this question from the perspective of software architecture. Considering files involved in each dependency type as an individual design space, we model such a design space using one DRSpace. We derived 3 DRSpaces for each of the 117 Apache open source projects, with 643,079 revision commits and 101,364 bug reports in total, and calculated their interactions with defective files. The experiment results are surprising: the three dependency types present significantly different architectural views, and their interactions with defective files are also drastically different. Intuitively, they play completely different roles when used for defect prediction/localization. The good news is that the combination of these structures has the potential to improve the accuracy of defect prediction/localization. In summary, our work provides a new perspective regarding to which type(s) of relations should be used for the task of defect prediction/localization. These quantitative and qualitative results also advance our knowledge of the relationship between software quality and architectural views formed using different dependency types.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {584–595},
numpages = {12},
keywords = {software structure, software quality, software maintenance},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3468264.3468623,
author = {Patra, Jibesh and Pradel, Michael},
title = {Semantic bug seeding: a learning-based approach for creating realistic bugs},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468623},
doi = {10.1145/3468264.3468623},
abstract = {When working on techniques to address the wide-spread problem of software bugs, one often faces the need for a large number of realistic bugs in real-world programs. Such bugs can either help evaluate an approach, e.g., in form of a bug benchmark or a suite of program mutations, or even help build the technique, e.g., in learning-based bug detection. Because gathering a large number of real bugs is difficult, a common approach is to rely on automatically seeded bugs. Prior work seeds bugs based on syntactic transformation patterns, which often results in unrealistic bugs and typically cannot introduce new, application-specific code tokens.  This paper presents SemSeed, a technique for automatically seeding bugs in a semantics-aware way. The key idea is to imitate how a given real-world bug would look like in other programs by semantically adapting the bug pattern to the local context. To reason about the semantics of pieces of code, our approach builds on learned token embeddings that encode the semantic similarities of identifiers and literals. Our evaluation with real-world JavaScript software shows that the approach effectively reproduces real bugs and clearly outperforms a semantics-unaware approach. The seeded bugs are useful as training data for learning-based bug detection, where they significantly improve the bug detection ability. Moreover, we show that SemSeed-created bugs complement existing mutation testing operators, and that our approach is efficient enough to seed hundreds of thousands of bugs within an hour.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {906–918},
numpages = {13},
keywords = {token embeddings, machine learning, dataset, bugs, bug injection},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1109/APSEC.2013.39,
author = {Liu, Kaiping and Tan, Hee Beng Kuan},
title = {Mining Attribute Lifecycle to Predict Faults and Incompleteness in Database Applications},
year = {2013},
isbn = {9781479921447},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2013.39},
doi = {10.1109/APSEC.2013.39},
abstract = {In a database application, for each attribute, a value is created initially via insertion. Then, the value can be referenced or updated via selection and updating respectively. Eventually, when the record is deleted, the values of the attributes are also deleted. These occurrences of events are associated with the states to constitute the attribute lifecycle. Our empirical studies discover that faults and incompleteness in database applications are highly associated with the attribute lifecycle. Consequently, we propose a novel approach to automatically extract the attribute lifecycle out of a database application from its source code through inter-procedural static program analysis. Data mining methods are applied to predict faults and incompleteness in database applications. Experiments on PHP systems give evidence to support applicability and accuracy of the proposed method.},
booktitle = {Proceedings of the 2013 20th Asia-Pacific Software Engineering Conference (APSEC) - Volume 01},
pages = {223–230},
numpages = {8},
keywords = {incompleteness prediction, data mining, attribute lifecycle, Fault prediction},
series = {APSEC '13}
}

@article{10.1007/s10462-018-9667-6,
author = {Fazel Zarandi, Mohammad Hossein and Sadat Asl, Ali Akbar and Sotudian, Shahabeddin and Castillo, Oscar},
title = {A state of the art review of intelligent scheduling},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-018-9667-6},
doi = {10.1007/s10462-018-9667-6},
abstract = {Intelligent scheduling covers various tools and techniques for successfully and efficiently solving the scheduling problems. In this paper, we provide a survey of intelligent scheduling systems by categorizing them into five major techniques containing fuzzy logic, expert systems, machine learning, stochastic local search optimization algorithms and constraint programming. We also review the application case studies of these techniques.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {501–593},
numpages = {93},
keywords = {Constraint programming, Stochastic local search optimization algorithms, Machine learning, Expert system, Fuzzy logic, Intelligent scheduling}
}

@inproceedings{10.1145/3377929.3398128,
author = {Rosenbauer, Lukas and Stein, Anthony and Maier, Roland and P\"{a}tzel, David and H\"{a}hner, J\"{o}rg},
title = {XCS as a reinforcement learning approach to automatic test case prioritization},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398128},
doi = {10.1145/3377929.3398128},
abstract = {Testing is a crucial part in the development of new products. With the rise of test automation methods, companies start relying on an even higher number of tests. Sometimes it is not feasible to run all tests and the goal is to determine which tests are crucial and which are less important. This prioritization problem has just recently gotten into the focus of reinforcement learning. A neural network combined with prioritized experience replay (ER) was used to identify critical tests. We are the first to apply XCS classifier systems (XCS) for this use case and reveal that XCS is not only suitable for this problem, but can also be superior to the aforementioned neural network and leads to more stable results. In this work, we adapt XCS's learning mechanism to the task by introducing a batch update which is based on Monte Carlo control. Further, we investigate if prioritized ER has the same positive effects on XCS as on the neural network for this test prioritization problem. Our experiments show that in general this is not the case for XCS.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1798–1806},
numpages = {9},
keywords = {test automation, reinforcement learning, experience replay, artificial intelligence, XCS classifier system},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1016/j.advengsoft.2011.03.010,
author = {Alsmadi, Izzat and Najadat, Hassan},
title = {Evaluating the change of software fault behavior with dataset attributes based on categorical correlation},
year = {2011},
issue_date = {August, 2011},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {42},
number = {8},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2011.03.010},
doi = {10.1016/j.advengsoft.2011.03.010},
abstract = {Utilization of data mining in software engineering has been the subject of several research papers. Majority of subjects of those paper were in making use of historical data for decision making activities such as cost estimation and product or project attributes prediction and estimation. The ability to predict software fault modules and the ability to correlate relations between faulty modules and product attributes using statistics is the subject of this paper. Correlations and relations between the attributes and the categorical variable or the class are studied through generating a pool of records from each dataset and then select two samples every time from the dataset and compare them. The correlation between the two selected records is studied in terms of changing from faulty to non-faulty or the opposite for the module defect attribute and the value change between the two records in each evaluated attribute (e.g. equal, larger or smaller). The goal was to study if there are certain attributes that are consistently affecting changing the state of the module from faulty to none, or the opposite. Results indicated that such technique can be very useful in studying the correlations between each attribute and the defect status attribute. Another prediction algorithm is developed based on statistics of the module and the overall dataset. The algorithm gave each attribute true class and faulty class predictions. We found that dividing prediction capability for each attribute into those two (i.e. correct and faulty module prediction) facilitate understanding the impact of attribute values on the class and hence improve the overall prediction relative to previous studies and data mining algorithms. Results were evaluated and compared with other algorithms and previous studies. ROC metrics were used to evaluate the performance of the developed metrics. Results from those metrics showed that accuracy or prediction performance calculated traditionally using accurately predicted records divided by the total number of records in the dataset does not necessarily give the best indicator of a good metric or algorithm predictability. Those predictions may give wrong implication if other metrics are not considered with them. The ROC metrics were able to show some other important aspects of performance or accuracy.},
journal = {Adv. Eng. Softw.},
month = aug,
pages = {535–546},
numpages = {12},
keywords = {Software quality, Software mining, Prediction algorithms, Fault prone modules, Data mining, Correlation, Clustering}
}

@inproceedings{10.1145/1882362.1882410,
author = {Marcus, Andrian and Menzies, Timothy},
title = {Software is data too},
year = {2010},
isbn = {9781450304276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882362.1882410},
doi = {10.1145/1882362.1882410},
abstract = {Software systems are designed and engineered to process data. However, software is data too. The size and variety of today's software artifacts and the multitude of stakeholder activities result in so much data that individuals can no longer reason about all of it. We argue in this position paper that data mining, statistical analysis, machine learning, information retrieval, data integration, etc., are necessary solutions to deal with software data. New research is needed to adapt existing algorithms and tools for software engineering data and processes, and new ones will have to be created.In order for this type of research to succeed, it should be supported with new approaches to empirical work, where data and results are shared globally among researchers and practitioners. Software engineering researchers can get inspired by other fields, such as, bioinformatics, where results of mining and analyzing biological data are often stored in databases shared across the world.},
booktitle = {Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research},
pages = {229–232},
numpages = {4},
keywords = {statistical analysis, software engineering, machine learning, information retrieval, empirical research, data mining},
location = {Santa Fe, New Mexico, USA},
series = {FoSER '10}
}

@article{10.1145/3276517,
author = {Pradel, Michael and Sen, Koushik},
title = {DeepBugs: a learning approach to name-based bug detection},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276517},
doi = {10.1145/3276517},
abstract = {Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89% and 95%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68% true positive rate) in real-world code.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {147},
numpages = {25},
keywords = {Natural language, Name-based program analysis, Machine learning, JavaScript, Bug detection}
}

@article{10.1016/j.procs.2020.03.274,
author = {Taneja, Divya and Singh, Rajvir and Singh, Ajmer and Malik, Himanshu},
title = {A Novel technique for test case minimization in object oriented testing},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.274},
doi = {10.1016/j.procs.2020.03.274},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {2221–2228},
numpages = {8},
keywords = {Object oriented metrics, Test Case Minimization, machine learning, object oriented testing}
}

@article{10.1007/s00500-020-05005-4,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {A systematic literature review on empirical studies towards prediction of software maintainability},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05005-4},
doi = {10.1007/s00500-020-05005-4},
abstract = {Software maintainability prediction in the earlier stages of software development involves the construction of models for the accurate estimation of maintenance effort. This guides the software practitioners to manage the resources optimally. This study aims at systematically reviewing the prediction models from January 1990 to October 2019 for predicting software maintainability. We analyze the effectiveness of these models according to various aspects. To meet the goal of the research, we have identified 36 research papers. On investigating these papers, we found that various machine learning (ML), statistical (ST), and hybridized (HB) techniques have been applied to develop prediction models to predict software maintainability. The significant finding of this review is that the overall performance of ML-based models is better than that of ST models. The use of HB techniques for prediction of software maintainability is limited. The results of this review revealed that software maintainability prediction (SMP) models developed using ML techniques outperformed models developed using ST techniques. Also, the prediction performance of few models developed using HB techniques is encouraging, yet no conclusive results about the performance of HB techniques could be reported because different HB techniques are applied in a few studies.},
journal = {Soft Comput.},
month = nov,
pages = {16655–16677},
numpages = {23},
keywords = {Hybridized techniques, Statistical techniques, Machine learning techniques, Software maintainability, Software maintenance}
}

@article{10.1504/ijbidm.2020.104743,
author = {Jalila, A. and Mala, D. Jeya},
title = {Automated optimal test data generation for OCL specification using harmony search algorithm},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {2},
issn = {1743-8195},
url = {https://doi.org/10.1504/ijbidm.2020.104743},
doi = {10.1504/ijbidm.2020.104743},
abstract = {Exploring software testing possibilities at an early software life cycle is increasingly necessary to avoid the propagation of defects to the subsequent phases. This requirement demands technique that can generate automated test cases at the initial phases of software development. Thus, we propose a novel framework for automated test data generation using formal specifications written in object constraint language (OCL). We also defined a novel fitness function named exit-predicate-wise branch coverage (EPWBC) to evaluate the generated test data. Another focus of the proposed approach is to optimise the test case generation process by applying, harmony search (HS) algorithm. The experimental results indicate that the proposed framework outperforms the other OCL-based test case generation techniques. Furthermore, it has been inferred that OCL based testing adopting HS algorithm forms an excellent combination to produce more test coverage and an optimal test suite thereby improving the quality of a system.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {231–259},
numpages = {28},
keywords = {HS, harmony search algorithm, optimal test case generation, EPWBC, exit-predicate-wise branch coverage, OCL, object constraint language, SBT, specification-based testing}
}

@article{10.1016/j.cageo.2021.104910,
author = {Li, Yue and Wang, Yuying and Wu, Ning},
title = {Noise suppression method based on multi-scale Dilated Convolution Network in desert seismic data},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2021.104910},
doi = {10.1016/j.cageo.2021.104910},
journal = {Comput. Geosci.},
month = nov,
numpages = {11},
keywords = {Desert seismic data denoising, Convolutional neural network, Dilated convolution, Multi-scale}
}

@inproceedings{10.1145/2810146.2810149,
author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
title = {Different Classifiers Find Different Defects Although With Different Level of Consistency},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810149},
doi = {10.1145/2810146.2810149},
abstract = {BACKGROUND -- During the last 10 years hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall.OBJECTIVE -- We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers.METHOD -- We perform a sensitivity analysis to compare the performance of Random Forest, Na\"{\i}ve Bayes, RPart and SVM classifiers when predicting defects in 12 NASA data sets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty is compared against different classifiers.RESULTS -- Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others.CONCLUSIONS -- Our results confirm that a unique sub-set of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Classifier ensembles with decision making strategies not based on majority voting are likely to perform best.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {3},
numpages = {10},
location = {Beijing, China},
series = {PROMISE '15}
}

@article{10.1016/j.neucom.2021.05.039,
author = {Nie, Lun Yiu and Gao, Cuiyun and Zhong, Zhicong and Lam, Wai and Liu, Yang and Xu, Zenglin},
title = {CoreGen: Contextualized Code Representation Learning for Commit Message Generation},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {459},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.05.039},
doi = {10.1016/j.neucom.2021.05.039},
journal = {Neurocomput.},
month = oct,
pages = {97–107},
numpages = {11},
keywords = {Contextualized code representation, Self-supervised learning, Code-to-text generation, Code representation learning, Commit message generation}
}

@inproceedings{10.1109/ICSE43902.2021.00107,
author = {Jiang, Nan and Lutellier, Thibaud and Tan, Lin},
title = {CURE: Code-Aware Neural Machine Translation for Automatic Program Repair},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00107},
doi = {10.1109/ICSE43902.2021.00107},
abstract = {Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1161–1173},
numpages = {13},
keywords = {software reliability, automatic program repair},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3092703.3092717,
author = {Sohn, Jeongju and Yoo, Shin},
title = {FLUCCS: using code and change metrics to improve fault localization},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092717},
doi = {10.1145/3092703.3092717},
abstract = {Fault localization aims to support the debugging activities of human developers by highlighting the program elements that are suspected to be responsible for the observed failure. Spectrum Based Fault Localization (SBFL), an existing localization technique that only relies on the coverage and pass/fail results of executed test cases, has been widely studied but also criticized for the lack of precision and limited effort reduction. To overcome restrictions of techniques based purely on coverage, we extend SBFL with code and change metrics that have been studied in the context of defect prediction, such as size, age and code churn. Using suspiciousness values from existing SBFL formulas and these source code metrics as features, we apply two learn-to-rank techniques, Genetic Programming (GP) and linear rank Support Vector Machines (SVMs). We evaluate our approach with a ten-fold cross validation of method level fault localization, using 210 real world faults from the Defects4J repository. GP with additional source code metrics ranks the faulty method at the top for 106 faults, and within the top five for 173 faults. This is a significant improvement over the state-of-the-art SBFL formulas, the best of which can rank 49 and 127 faults at the top and within the top five, respectively.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {273–283},
numpages = {11},
keywords = {SBSE, Genetic Programming, Fault Localization},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@article{10.4018/jdsst.2013010104,
author = {Rahimi, Iman and Behmanesh, Reza and Yusuff, Rosnah Mohd.},
title = {A Hybrid Method for Prediction and Assessment Efficiency of Decision Making Units: Real Case Study: Iranian Poultry Farms},
year = {2013},
issue_date = {January 2013},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {1},
issn = {1941-6296},
url = {https://doi.org/10.4018/jdsst.2013010104},
doi = {10.4018/jdsst.2013010104},
abstract = {The objective of this article is an evaluation and assessment efficiency of the poultry meat farm as a case study with the new method. As it is clear poultry farm industry is one of the most important sub-sectors in comparison to other ones. The purpose of this study is the prediction and assessment efficiency of poultry farms as decision making units DMUs. Although, several methods have been proposed for solving this problem, the authors strongly need a methodology to discriminate performance powerfully. Their methodology is comprised of data envelopment analysis and some data mining techniques same as artificial neural network ANN, decision tree DT, and cluster analysis CA. As a case study, data for the analysis were collected from 22 poultry companies in Iran. Moreover, due to a small data set and because of the fact that the authors must use large data set for applying data mining techniques, they employed k-fold cross validation method to validate the authors' model. After assessing efficiency for each DMU and clustering them, followed by applied model and after presenting decision rules, results in precise and accurate optimizing technique.},
journal = {Int. J. Decis Support Syst. Technol.},
month = jan,
pages = {66–83},
numpages = {18},
keywords = {Poultry Meat Farming, Efficiency, Decision Tree, Data Envelopment Analysis, Artificial Neural Network}
}

@article{10.1016/j.infsof.2021.106665,
author = {Panichella, Sebastiano and Canfora, Gerardo and Di Sorbo, Andrea},
title = {“Won’t We Fix this Issue?” Qualitative characterization and automated identification of wontfix issues on GitHub},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106665},
doi = {10.1016/j.infsof.2021.106665},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Machine learning, Empirical study, Issue management, Issue tracking}
}

@inproceedings{10.1145/3387904.3389260,
author = {Lin, Jinfeng and Liu, Yalin and Cleland-Huang, Jane},
title = {Supporting Program Comprehension through Fast Query response in Large-Scale Systems},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389260},
doi = {10.1145/3387904.3389260},
abstract = {Software traceability provides support for various engineering activities including Program Comprehension; however, it can be challenging and arduous to complete in large industrial projects. Researchers have proposed automated traceability techniques to create, maintain and leverage trace links. Computationally intensive techniques, such as repository mining and deep learning, have showed the capability to deliver accurate trace links. The objective of achieving trusted, automated tracing techniques at industrial scale has not yet been successfully accomplished due to practical performance challenges. This paper evaluates high-performance solutions for deploying effective, computationally expensive trace-ability algorithms in large scale industrial projects and leverages generated trace links to answer Program Comprehension Queries. We comparatively evaluate four different platforms for supporting industrial-scale tracing solutions, capable of tackling software projects with millions of artifacts. We demonstrate that tracing solutions built using big data frameworks scale well for large projects and that our Spark implementation outperforms relational database, graph database (GraphDB), and plain Java implementations. These findings contradict earlier results which suggested that GraphDB solutions should be adopted for large-scale tracing problems.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {285–295},
numpages = {11},
keywords = {traceability, scalability, performance, Software project queries},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3416507.3423190,
author = {Sonnekalb, Tim and Heinze, Thomas S. and Kurnatowski, Lynn von and Schreiber, Andreas and Gonzalez-Barahona, Jesus M. and Packer, Heather},
title = {Towards automated, provenance-driven security audit for git-based repositories: applied to germany's corona-warn-app: vision paper},
year = {2020},
isbn = {9781450381260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416507.3423190},
doi = {10.1145/3416507.3423190},
abstract = {Software repositories contain information about source code, software development processes, and team interactions. We combine provenance of the development process with code security analysis to automatically discover insights. This provides fast feedback on the software's design and security issues, which we evaluate on projects that are developed under time pressure, such as Germany's COVID-19 contact tracing app 'Corona-Warn-App'.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Security from Design to Deployment},
pages = {15–18},
numpages = {4},
keywords = {software security, repository mining, provenance, program analysis, open source software, covid-19},
location = {Virtual, USA},
series = {SEAD 2020}
}

@article{10.1016/j.ins.2011.09.034,
author = {Yu, Lean},
title = {An evolutionary programming based asymmetric weighted least squares support vector machine ensemble learning methodology for software repository mining},
year = {2012},
issue_date = {May, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {191},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2011.09.034},
doi = {10.1016/j.ins.2011.09.034},
abstract = {In this paper, a novel evolutionary programming (EP) based asymmetric weighted least squares support vector machine (LSSVM) ensemble learning methodology is proposed for software repository mining. In this methodology, an asymmetric weighted LSSVM model is first proposed. Then the process of building the EP-based asymmetric weighted LSSVM ensemble learning methodology is described in detail. Two publicly available software defect datasets are finally used for illustration and verification of the effectiveness of the proposed EP-based asymmetric weighted LSSVM ensemble learning methodology. Experimental results reveal that the proposed EP-based asymmetric weighted LSSVM ensemble learning methodology can produce promising classification accuracy in software repository mining, relative to other classification methods listed in this study.},
journal = {Inf. Sci.},
month = may,
pages = {31–46},
numpages = {16},
keywords = {Software repository mining, Evolutionary programming, Ensemble learning algorithm, Asymmetric weighted least squares support vector machine}
}

@inproceedings{10.1145/3358502.3361270,
author = {Mattis, Toni and Rein, Patrick and Hirschfeld, Robert},
title = {Ambiguous, informal, and unsound: metaprogramming for naturalness},
year = {2019},
isbn = {9781450369855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358502.3361270},
doi = {10.1145/3358502.3361270},
abstract = {Program code needs to be understood by both machines and programmers. While the goal of executing programs requires the unambiguity of a formal language, programmers use natural language within these formal constraints to explain implemented concepts to each other. This so called naturalness – the property of programs to resemble human communication – motivated many statistical and machine learning (ML) approaches with the goal to improve software engineering activities. The metaprogramming facilities of most programming environments model the formal elements of a program (meta-objects). If ML is used to support engineering or analysis tasks, complex infrastructure needs to bridge the gap between meta-objects and ML models, changes are not reflected in the ML model, and the mapping from an ML output back into the program’s meta-object domain is laborious. In the scope of this work, we propose to extend metaprogramming facilities to give tool developers access to the representations of program elements within an exchangeable ML model. We demonstrate the usefulness of this abstraction in two case studies on test prioritization and refactoring. We conclude that aligning ML representations with the program’s formal structure lowers the entry barrier to exploit statistical properties in tool development.},
booktitle = {Proceedings of the 4th ACM SIGPLAN International Workshop on Meta-Programming Techniques and Reflection},
pages = {1–10},
numpages = {10},
keywords = {naturalness, metaprogramming, meta-objects, machine learning},
location = {Athens, Greece},
series = {META 2019}
}

@inproceedings{10.1145/3377812.3381396,
author = {Halepmollasi, Ru\c{s}en},
title = {A composed technical debt identification methodology to predict software vulnerabilities},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381396},
doi = {10.1145/3377812.3381396},
abstract = {Technical debt (TD), its impact on development and its consequences such as defects and vulnerabilities, are of common interest and great importance to software researchers and practitioners. Although there exist many studies investigating TD, the majority of them focuses on identifying and detecting TD from a single stage of development. There are also studies that analyze vulnerabilities focusing on some phases of the life cycle. Moreover, several approaches have investigated the relationship between TD and vulnerabilities, however, the generalizability and validity of findings are limited due to small dataset. In this study, we aim to identify TD through multiple phases of development, and to automatically measure it through data and text mining techniques to form a comprehensive feature model. We plan to utilize neural network based classifiers that will incorporate evolutionary changes on TD measures into predicting vulnerabilities. Our approach will be empirically assessed on open source and industrial projects.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {186–189},
numpages = {4},
keywords = {technical debt, software security, machine learning, feature engineering},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1504/ijwmc.2021.117570,
author = {Jing, Wang and Fangfang, Liu and Hongyan, Liu and Qingqing, Wang},
title = {Application of deep learning in network security fault diagnosis and prediction},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {20},
number = {4},
issn = {1741-1084},
url = {https://doi.org/10.1504/ijwmc.2021.117570},
doi = {10.1504/ijwmc.2021.117570},
abstract = {At present, deep learning method has been successfully applied in many application directions, but few researchers try to apply deep learning to network security fault diagnosis. This paper summarises the deep learning methods applied to network security fault diagnosis and prediction, and focuses on the attack detection using stacked automatic encoder. The network data sets are used to compare various attacks. The fault diagnosis process based on the deep learning method and the analysis and verification of the experimental results are introduced in detail. At the same time, the automatic operation time is implemented in order to monitor and predict the network application characteristics and deep learning mechanism, intrusion detection system can be used to monitor network applications and send out an alarm when an attack is detected.},
journal = {Int. J. Wire. Mob. Comput.},
month = jan,
pages = {381–389},
numpages = {8},
keywords = {automatic encoder, fault diagnosis, network security, deep learning}
}

@article{10.1186/s13677-020-00206-6,
author = {Gassais, Robin and Ezzati-Jivan, Naser and Fernandez, Jose M. and Aloise, Daniel and Dagenais, Michel R.},
title = {Multi-level host-based intrusion detection system for Internet of things},
year = {2020},
issue_date = {Dec 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {9},
number = {1},
issn = {2192-113X},
url = {https://doi.org/10.1186/s13677-020-00206-6},
doi = {10.1186/s13677-020-00206-6},
abstract = {The growth of the Internet of things (IoT) has ushered in a new area of inter-connectivity and innovation in the home. Many devices, once separate, can now be interacted with remotely, improving efficiency and organization. This, however, comes at the cost of rising security vulnerabilities. Vendors are competing to create and release quickly innovative connected objects, without focusing on the security issues. As a consequence, attacks involving smart devices, or targeting them, are proliferating, creating threats to user’s privacy and even their physical security. Additionally, the heterogeneous technologies involved in IoT make attempts to develop protection on smart devices much harder. Most of the intrusion detection systems developed for those platforms are based on network activity. However, on many systems, intrusions cannot easily or reliably be detected from network traces. We propose a novel host-based automated framework for intrusion detection. Our work combines user space and kernel space information and machine learning techniques to detect various kinds of intrusions in smart devices. Our solution use tracing techniques to automatically get devices behavior, process this data into numeric arrays to train several machine learning algorithms, and raise alerts whenever an intrusion is found. We implemented several machine learning algorithms, including deep learning ones, to achieve high detection capabilities, while adding little overhead on the monitored devices. We tested our solution within a realistic home automation system with actual threats.},
journal = {J. Cloud Comput.},
month = nov,
numpages = {16},
keywords = {Execution tracing, Machine learning, Anomaly detection, Internet of things, Host-based intrusion detection system}
}

@inproceedings{10.1145/3338906.3340455,
author = {Mesbah, Ali and Rice, Andrew and Johnston, Emily and Glorioso, Nick and Aftandilian, Edward},
title = {DeepDelta: learning to repair compilation errors},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340455},
doi = {10.1145/3338906.3340455},
abstract = {Programmers spend a substantial amount of time manually repairing code that does not compile. We observe that the repairs for any particular error class typically follow a pattern and are highly mechanical. We propose a novel approach that automatically learns these patterns with a deep neural network and suggests program repairs for the most costly classes of build-time compilation failures. We describe how we collect all build errors and the human-authored, in-progress code changes that cause those failing builds to transition to successful builds at Google. We generate an AST diff from the textual code changes and transform it into a domain-specific language called Delta that encodes the change that must be made to make the code compile. We then feed the compiler diagnostic information (as source) and the Delta changes that resolved the diagnostic (as target) into a Neural Machine Translation network for training. For the two most prevalent and costly classes of Java compilation errors, namely missing symbols and mismatched method signatures, our system called DeepDelta, generates the correct repair changes for 19,314 out of 38,788 (50%) of unseen compilation errors. The correct changes are in the top three suggested fixes 86% of the time on average.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {925–936},
numpages = {12},
keywords = {program repair, neural machine translation, compilation errors},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1007/s00521-018-3560-8,
author = {Anwar, Zeeshan and Afzal, Hammad and Bibi, Nazia and Abbas, Haider and Mohsin, Athar and Arif, Omar},
title = {A hybrid-adaptive neuro-fuzzy inference system for multi-objective regression test suites optimization},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3560-8},
doi = {10.1007/s00521-018-3560-8},
abstract = {Regression testing is a mandatory activity of software development life cycle, which is performed to ensure that modifications have not caused any adverse effects on the system’s functionality. With every change in software in the maintenance phase, the size of regression test suite grows as new test cases are written to validate changes. The bigger size of regression test suite makes the testing expensive and time-consuming. Optimization of regression test suite is a possible solution to cope with this problem. Various techniques of optimization have been proposed; however, there is no perfect solution for the problem and therefore, requires better solutions to improve the optimization process. This paper presents a novel technique named as hybrid-adaptive neuro-fuzzy inference system tuned with genetic algorithm and particle swarm optimization algorithm that is used to optimize the regression test suites. Evaluation of the proposed approach is performed on benchmark test suites including “previous date problem” and “Siemens print token.” Experimental results are compared with existing state-of-the-art techniques, and results show that the proposed approach is more effective for the reduction in a regression test suites with higher requirement coverage. The size of regression test suites can be reduced up to 48% using the proposed approach without reducing the fault detection rate.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {7287–7301},
numpages = {15},
keywords = {Adaptive neuro-fuzzy inference system, Particle swarm algorithm, Genetic algorithm, Regression test suite optimization}
}

@inproceedings{10.1007/978-3-030-58811-3_69,
author = {Hegedundefineds, P\'{e}ter},
title = {Inspecting JavaScript Vulnerability Mitigation Patches with Automated Fix Generation in Mind},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_69},
doi = {10.1007/978-3-030-58811-3_69},
abstract = {Software security has become a primary concern for both the industry and academia in recent years. As dependency on critical services provided by software systems grows globally, a potential security threat in such systems poses higher and higher risks (e.g. economical damage, a threat to human life, criminal activity).Finding potential security vulnerabilities at the code level automatically is a very popular approach to aid security testing. However, most of the methods based on machine learning and statistical models stop at listing potentially vulnerable code parts and leave their validation and mitigation to the developers. Automatic program repair could fill this gap by automatically generating vulnerability mitigation code patches. Nonetheless, it is still immature, especially in targeting security-relevant fixes.In this work, we try to establish a path towards automatic vulnerability fix generation techniques in the context of JavaScript programs. We inspect 361 actual vulnerability mitigation patches collected from vulnerability databases and GitHub. We found that vulnerability mitigation patches are not short on average and in many cases affect not just program code but test code as well. These results point towards that a general automatic repair approach targeting all the different types of vulnerabilities is not feasible. The analysis of the code properties and fix patterns for different vulnerability types might help in setting up a more realistic goal in the area of automatic JavaScript vulnerability repair.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {975–988},
numpages = {14},
keywords = {Automatic repair, Prediction models, JavaScript, Vulnerability, Security},
location = {Cagliari, Italy}
}

@inproceedings{10.5555/3504035.3504676,
author = {Liang, Yuding and Zhu, Kenny Q.},
title = {Automatic generation of text descriptive comments for code blocks},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {We propose a framework to automatically generate descriptive comments for source code blocks. While this problem has been studied by many researchers previously, their methods are mostly based on fixed template and achieves poor results. Our framework does not rely on any template, but makes use of a new recursive neural network called Code-RNN to extract features from the source code and embed them into one vector. When this vector representation is input to a new recurrent neural network (Code-GRU), the overall framework generates text descriptions of the code with accuracy (Rouge-2 value) significantly higher than other learning-based approaches such as sequence-to-sequence model. The Code-RNN model can also be used in other scenario where the representation of code is required.1},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {641},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.5555/2820282.2820292,
author = {Thung, Ferdian and Le, Xuan-Bach D. and Lo, David},
title = {Active semi-supervised defect categorization},
year = {2015},
publisher = {IEEE Press},
abstract = {Defects are inseparable part of software development and evolution. To better comprehend problems affecting a software system, developers often store historical defects and these defects can be categorized into families. IBM proposes Orthogonal Defect Categorization (ODC) which include various classifications of defects based on a number of orthogonal dimensions (e.g., symptoms and semantics of defects, root causes of defects, etc.). To help developers categorize defects, several approaches that employ machine learning have been proposed in the literature. Unfortunately, these approaches often require developers to manually label a large number of defect examples. In practice, manually labelling a large number of examples is both time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labelling while still being able to achieve good performance is crucial towards the adoption of such approaches. To deal with this challenge, in this work, we propose an active semi-supervised defect prediction approach. It is performed by actively selecting a small subset of diverse and informative defect examples to label (i.e., active learning), and by making use of both labeled and unlabeled defect examples in the prediction model learning process (i.e., semi-supervised learning). Using this principle, our approach is able to learn a good model while minimizing the manual labeling effort.To evaluate the effectiveness of our approach, we make use of a benchmark dataset that contains 500 defects from three software systems that have been manually labelled into several families based on ODC. We investigate our approach's ability in achieving good classification performance, measured in terms of weighted precision, recall, F-measure, and AUC, when only a small number of manually labelled defect examples are available. Our experiment results show that our active semi-supervised defect categorization approach is able to achieve a weighted precision, recall, F-measure, and AUC of 0.651, 0.669, 0.623, and 0.710, respectively, when only 50 defects are manually labelled. Furthermore, it outperforms an existing active multi-class classification algorithm, proposed in the machine learning community, by a substantial margin.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {60–70},
numpages = {11},
location = {Florence, Italy},
series = {ICPC '15}
}

@inproceedings{10.5555/3016387.3016470,
author = {Elmishali, Amir and Stern, Roni and Kalech, Meir},
title = {Data-augmented software diagnosis},
year = {2016},
publisher = {AAAI Press},
abstract = {Software fault prediction algorithms predict which software components is likely to contain faults using machine learning techniques. Software diagnosis algorithm identify the faulty software components that caused a failure using model-based or spectrum based approaches. We show how software fault prediction algorithms can be used to improve software diagnosis. The resulting data-augmented diagnosis algorithm overcomes key problems in software diagnosis algorithms: ranking diagnoses and distinguishing between diagnoses with high probability and low probability. We demonstrate the efficiency of the proposed approach empirically on three open sources domains, showing significant increase in accuracy of diagnosis and efficiency of troubleshooting. These encouraging results suggests broader use of data-driven methods to complement and improve existing model-based methods.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4003–4009},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.5555/3546258.3546523,
author = {Wang, Feicheng and Janson, Lucas},
title = {Exact asymptotics for linear quadratic adaptive control},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Recent progress in reinforcement learning has led to remarkable performance in a range of applications, but its deployment in high-stakes settings remains quite rare. One reason is a limited understanding of the behavior of reinforcement algorithms, both in terms of their regret and their ability to learn the underlying system dynamics--existing work is focused almost exclusively on characterizing rates, with little attention paid to the constants multiplying those rates that can be critically important in practice. To start to address this challenge, we study perhaps the simplest non-bandit reinforcement learning problem: linear quadratic adaptive control (LQAC) . By carefully combining recent finite-sample performance bounds for the LQAC problem with a particular (less-recent) martingale central limit theorem, we are able to derive asymptotically-exact expressions for the regret, estimation error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm. In simulations on both stable and unstable systems, we find that our asymptotic theory also describes the algorithm's finite-sample behavior remarkably well.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {265},
numpages = {112},
keywords = {exact asymptotics, uncertainty quantification, safety, system identification, linear dynamical system, adaptive control, reinforcement learning}
}

@inproceedings{10.1109/ICSE.2019.00054,
author = {Philip, Adithya Abraham and Bhagwan, Ranjita and Kumar, Rahul and Maddila, Chandra Sekhar and Nagappan, Nachiappan},
title = {FastLane: test minimization for rapidly deployed large-scale online services},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00054},
doi = {10.1109/ICSE.2019.00054},
abstract = {Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases.This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that our techniques can save 18.04%, i.e., almost a fifth of test-time while obtaining a test outcome accuracy of 99.99%.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {408–418},
numpages = {11},
keywords = {test prioritization, machine learning, commit risk},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3377811.3380361,
author = {Hoang, Thong and Kang, Hong Jin and Lo, David and Lawall, Julia},
title = {CC2Vec: distributed representations of code changes},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380361},
doi = {10.1145/3377811.3380361},
abstract = {Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {518–529},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2832987.2833051,
author = {Abunadi, Ibrahim and Alenezi, Mamdouh},
title = {Towards Cross Project Vulnerability Prediction in Open Source Web Applications},
year = {2015},
isbn = {9781450334181},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832987.2833051},
doi = {10.1145/2832987.2833051},
abstract = {Building secure software is challenging, time-consuming, and expensive. Software vulnerability prediction models that identify vulnerable software components are usually used to focus security efforts, with the aim of helping to reduce the time and effort needed to secure software. Existing vulnerability prediction models use process or product metrics and machine learning techniques to identify vulnerable software components. Cross project vulnerability prediction plays a significant role in appraising the most likely vulnerable software components, specifically for new or inactive projects. Little effort has been spent to deliver clear guidelines on how to choose the training data for project vulnerability prediction. In this work, we present an empirical study aiming at clarifying how useful cross project prediction techniques in predicting software vulnerabilities. Our study employs the classification provided by different machine learning techniques to improve the detection of vulnerable components. We have elaborately compared the prediction performance of five well-known classifiers. The study is conducted on a publicly available dataset of several PHP open source web applications and in the context of cross project vulnerability prediction, which represents one of the main challenges in the vulnerability prediction field.},
booktitle = {Proceedings of the The International Conference on Engineering &amp; MIS 2015},
articleno = {42},
numpages = {5},
keywords = {Software security, Software quality, Data mining, Cross-project vulnerability prediction},
location = {Istanbul, Turkey},
series = {ICEMIS '15}
}

@inproceedings{10.1145/3278142.3278147,
author = {Falessi, Davide and Moede, Max Jason},
title = {Facilitating feasibility analysis: the pilot defects prediction dataset maker},
year = {2018},
isbn = {9781450360562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278142.3278147},
doi = {10.1145/3278142.3278147},
abstract = {Our industrial experience in institutionalizing defect prediction models in the software industry shows that the first step is to measure prediction metrics and defects to assess the feasibility of the tool, i.e., if the accuracy of the defect prediction tool is higher than of a random predictor. However, computing prediction metrics is time consuming and error prone. Thus, the feasibility analysis has a cost which needs some initial investment by the potential clients. This initial investment acts as a barrier for convincing potential clients of the benefits of institutionalizing a software prediction model. To reduce this barrier, in this paper we present the Pilot Defects Prediction Dataset Maker (PDPDM), a desktop application for measuring metrics to use for defect prediction. PDPDM receives as input the repository’s information of a software project, and it provides as output, in an easy and replicable way, a dataset containing a set of 17 well-defined product and process metrics, that have been shown to be useful for defect prediction, such as size and smells. PDPDM avoids the use of outdated datasets and it allows researchers and practitioners to create defect datasets without the need to write any lines of code.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics},
pages = {15–18},
numpages = {4},
keywords = {Defects prediction},
location = {Lake Buena Vista, FL, USA},
series = {SWAN 2018}
}

@inproceedings{10.1109/IWoR.2019.00015,
author = {Sae-Lim, Natthawute and Hayashi, Shinpei and Saeki, Motoshi},
title = {Toward proactive refactoring: an exploratory study on decaying modules},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IWoR.2019.00015},
doi = {10.1109/IWoR.2019.00015},
abstract = {Source code quality is often measured using code smell, which is an indicator of design flaw or problem in the source code. Code smells can be detected using tools such as static analyzer that detects code smells based on source code metrics. Further, developers perform refactoring activities based on the result of such detection tools to improve source code quality. However, such approach can be considered as reactive refactoring, i.e., developers react to code smells after they occur. This means that developers first suffer the effects of low quality source code (e.g., low readability and understandability) before they start solving code smells. In this study, we focus on proactive refactoring, i.e., refactoring source code before it becomes smelly. This approach would allow developers to maintain source code quality without having to suffer the impact of code smells.To support the proactive refactoring process, we propose a technique to detect decaying modules, which are non-smelly modules that are about to become smelly. We present empirical studies on open source projects with the aim of studying the characteristics of decaying modules. Additionally, to facilitate developers in the refactoring planning process, we perform a study on using a machine learning technique to predict decaying modules and report a factor that contributes most to the performance of the model under consideration.},
booktitle = {Proceedings of the 3rd International Workshop on Refactoring},
pages = {39–46},
numpages = {8},
keywords = {refactoring, code smell, code quality},
location = {Montreal, Quebec, Canada},
series = {IWOR '19}
}

@inproceedings{10.1145/3460120.3484813,
author = {He, Jingxuan and Sivanrupan, Gishor and Tsankov, Petar and Vechev, Martin},
title = {Learning to Explore Paths for Symbolic Execution},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484813},
doi = {10.1145/3460120.3484813},
abstract = {Symbolic execution is a powerful technique that can generate tests steering program execution into desired paths. However, the scalability of symbolic execution is often limited by path explosion, i.e., the number of symbolic states representing the paths under exploration quickly explodes as execution goes on. Therefore, the effectiveness of symbolic execution engines hinges on the ability to select and explore the right symbolic states.In this work, we propose a novel learning-based strategy, called Learch, able to effectively select promising states for symbolic execution to tackle the path explosion problem. Learch directly estimates the contribution of each state towards the goal of maximizing coverage within a time budget, as opposed to relying on manually crafted heuristics based on simple statistics as a crude proxy for the objective. Moreover, Learch leverages existing heuristics in training data generation and feature extraction, and can thus benefit from any new expert-designed heuristics. We instantiated Learch in KLEE, a widely adopted symbolic execution engine. We evaluated Learch on a diverse set of programs, showing that Learch is practically effective: it covers more code and detects more security violations than existing manual heuristics, as well as combinations of those heuristics. We also show that using tests generated by Learch as initial fuzzing seeds enables the popular fuzzer AFL to find more paths and security violations.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2526–2540},
numpages = {15},
keywords = {symbolic execution, program testing, machine learning, fuzzing},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3379597.3387474,
author = {Akbar, Shayan A. and Kak, Avinash C.},
title = {A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387474},
doi = {10.1145/3379597.3387474},
abstract = {This paper reports on a large-scale comparative evaluation of IR-based tools for automatic bug localization. We have divided the tools in our evaluation into the following three generations: (1) The first-generation tools, now over a decade old, that are based purely on the Bag-of-Words (BoW) modeling of software libraries. (2) The somewhat more recent second-generation tools that augment BoW-based modeling with two additional pieces of information: historical data, such as change history, and structured information such as class names, method names, etc. And, finally, (3) The third-generation tools that are currently the focus of much research and that also exploit proximity, order, and semantic relationships between the terms. It is important to realize that the original authors of all these three generations of tools have mostly tested them on relatively small-sized datasets that typically consisted no more than a few thousand bug reports. Additionally, those evaluations only involved Java code libraries. The goal of the present paper is to present a comprehensive large-scale evaluation of all three generations of bug-localization tools with code libraries in multiple languages. Our study involves over 20,000 bug reports drawn from a diverse collection of Java, C/C++, and Python projects. Our results show that the third-generation tools are significantly superior to the older tools. We also show that the word embeddings generated using code files written in one language are effective for retrieval from code libraries in other languages.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {21–31},
numpages = {11},
keywords = {word embeddings, source code search, information retrieval, bug localization},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Software Engineering, Search-Based Software Engineering, Product Line Testing, Highly-Configurable Systems, Fault Detection, Cyber-Physical Systems},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2019.106224,
author = {Coviello, Carmen and Romano, Simone and Scanniello, Giuseppe and Marchetto, Alessandro and Corazza, Anna and Antoniol, Giuliano},
title = {Adequate vs. inadequate test suite reduction approaches},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106224},
doi = {10.1016/j.infsof.2019.106224},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {19},
keywords = {Test suite reduction, Regression testing, Inadequate test suite reduction, Clustering, Adequate test suite reduction}
}

@inproceedings{10.1145/3238147.3238165,
author = {Udeshi, Sakshi and Arora, Pryanshu and Chattopadhyay, Sudipta},
title = {Automated directed fairness testing},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238165},
doi = {10.1145/3238147.3238165},
abstract = {Fairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aeqitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aeqitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aeqitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aeqitas and we have evaluated it on six stateof- the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aeqitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aeqitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {98–108},
numpages = {11},
keywords = {Software Fairness, Machine Learning, Directed Testing},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1145/2491509.2491513,
author = {Yoo, Shin and Harman, Mark and Clark, David},
title = {Fault localization prioritization: Comparing information-theoretic and coverage-based approaches},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2491509.2491513},
doi = {10.1145/2491509.2491513},
abstract = {Test case prioritization techniques seek to maximize early fault detection. Fault localization seeks to use test cases already executed to help find the fault location. There is a natural interplay between the two techniques; once a fault is detected, we often switch focus to fault fixing, for which localization may be a first step. In this article we introduce the Fault Localization Prioritization (FLP) problem, which combines prioritization and localization. We evaluate three techniques: a novel FLP technique based on information theory, FLINT (Fault Localization using INformation Theory), that we introduce in this article, a standard Test Case Prioritization (TCP) technique, and a “test similarity technique” used in previous work. Our evaluation uses five different releases of four software systems. The results indicate that FLP and TCP can statistically significantly reduce fault localization costs for 73% and 76% of cases, respectively, and that FLINT significantly outperforms similarity-based localization techniques in 52% of the cases considered in the study.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {19},
numpages = {29},
keywords = {information theory, fault localization, Test case prioritization}
}

@article{10.1007/s10115-013-0721-z,
author = {Czibula, Gabriela and Marian, Zsuzsanna and Czibula, Istvan Gergely},
title = {Detecting software design defects using relational association rule mining},
year = {2015},
issue_date = {March     2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {42},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-013-0721-z},
doi = {10.1007/s10115-013-0721-z},
abstract = {In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal.},
journal = {Knowl. Inf. Syst.},
month = mar,
pages = {545–577},
numpages = {33},
keywords = {Software design, Machine learning, Defect detection, Data mining, Association rule mining}
}

@article{10.1007/s11219-021-09564-z,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Corrective commit probability: a measure of the effort invested in bug fixing},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09564-z},
doi = {10.1007/s11219-021-09564-z},
abstract = {The effort invested in software development should ideally be devoted to the implementation of new features. But some of the effort is invariably also invested in corrective maintenance, that is in fixing bugs. Not much is known about what fraction of software development work is devoted to bug fixing, and what factors affect this fraction. We suggest the Corrective Commit Probability (CCP), which measures the probability that a commit reflects corrective maintenance, as an estimate of the relative effort invested in fixing bugs. We identify corrective commits by applying a linguistic model to the commit messages, achieving an accuracy of 93%, higher than any previously reported model. We compute the CCP of all large active GitHub projects (7,557 projects with 200+ commits in 2019). This leads to the creation of an investment scale, suggesting that the bottom 10% of projects spend less than 6% of their total effort on bug fixing, while the top 10% of projects spend at least 39% of their effort on bug fixing — more than 6 times more. Being a process metric, CCP is conditionally independent of source code metrics, enabling their evaluation and investigation. Analysis of project attributes shows that lower CCP (that is, lower relative investment in bug fixing) is associated with smaller files, lower coupling, use of languages like JavaScript and C# as opposed to PHP and C++, fewer code smells, lower project age, better perceived quality, fewer developers, lower developer churn, better onboarding, and better productivity.},
journal = {Software Quality Journal},
month = dec,
pages = {817–861},
numpages = {45},
keywords = {Process metric, Effort estimate, Corrective commits, Corrective maintenance}
}

@article{10.1007/s11334-020-00383-2,
author = {Althar, Raghavendra Rao and Samanta, Debabrata},
title = {The realist approach for evaluation of computational intelligence in software engineering},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {1},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-020-00383-2},
doi = {10.1007/s11334-020-00383-2},
abstract = {Secured software development must employ a security mindset across software engineering practices. Software security must be considered during the requirements phase so that it is included throughout the development phase. Do the requirements gathering team get the proper input from the technical team? This paper unearths some of the data sources buried within software development phases and describes the potential approaches to understand them. Concepts such as machine learning and deep learning are explored to understand the data sources and explore how these learnings can be provided to the requirements gathering team. This knowledge system will help bring objectivity in the conversations between the requirements gathering team and the customer's business team. A literature review is also done to secure requirements management and identify the possible gaps in providing future research direction to enhance our understanding. Feature engineering in the landscape of software development is explored to understand the data sources. Experts offer their insight on the root cause of the lack of security focus in requirements gathering practices. The core theme is statistical modeling of all the software artifacts that hold information related to the software development life cycle. Strengthening of some traditional methods like threat modeling is also a key area explored. Subjectivity involved in these approaches can be made more objective.},
journal = {Innov. Syst. Softw. Eng.},
month = mar,
pages = {17–27},
numpages = {11},
keywords = {Threat modeling, Software requirements management, Computational intelligence, Data science, Software engineering}
}

@article{10.1007/s10664-018-9611-z,
author = {Liu, Bing and Nejati, Shiva and Lucia and Briand, Lionel C.},
title = {Effective fault localization of automotive Simulink models: achieving the trade-off between test oracle effort and fault localization accuracy},
year = {2019},
issue_date = {February  2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9611-z},
doi = {10.1007/s10664-018-9611-z},
abstract = {One promising way to improve the accuracy of fault localization based on statistical debugging is to increase diversity among test cases in the underlying test suite. In many practical situations, adding test cases is not a cost-free option because test oracles are developed manually or running test cases is expensive. Hence, we require to have test suites that are both diverse and small to improve debugging. In this paper, we focus on improving fault localization of Simulink models by generating test cases. We identify four test objectives that aim to increase test suite diversity. We use four objectives in a search-based algorithm to generate diversified but small test suites. To further minimize test suite sizes, we develop a prediction model to stop test generation when adding test cases is unlikely to improve fault localization. We evaluate our approach using three industrial subjects. Our results show (1) expanding test suites used for fault localization using any of our four test objectives, even when the expansion is small, can significantly improve the accuracy of fault localization, (2) varying test objectives used to generate the initial test suites for fault localization does not have a significant impact on the fault localization results obtained based on those test suites, and (3) we identify an optimal configuration for prediction models to help stop test generation when it is unlikely to be beneficial. We further show that our optimal prediction model is able to maintain almost the same fault localization accuracy while reducing the average number of newly generated test cases by more than half.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {444–490},
numpages = {47},
keywords = {Test suite diversity, Supervised learning, Simulink models, Search-based testing, Fault localization}
}

@inproceedings{10.1145/3422392.3422420,
author = {Lima, Rodrigo and Souza, Jairo and Fonseca, Baldoino and Teixeira, Leopoldo and Gheyi, Rohit and Ribeiro, M\'{a}rcio and Garcia, Alessandro and de Mello, Rafael},
title = {Understanding and Detecting Harmful Code},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422420},
doi = {10.1145/3422392.3422420},
abstract = {Code smells typically indicate poor design implementation and choices that may degrade software quality. Hence, they need to be carefully detected to avoid such poor design. In this context, some studies try to understand the impact of code smells on the software quality, while others propose rules or machine learning-based techniques to detect code smells. However, none of those studies or techniques focus on analyzing code snippets that are really harmful to software quality. This paper presents a study to understand and classify code harmfulness. We analyze harmfulness in terms of CLEAN, SMELLY, BUGGY, and HARMFUL code. By HARMFUL CODE, we define a SMELLY code element having one or more bugs reported. These bugs may have been fixed or not. Thus, the incidence of HARMFUL CODE may represent a increased risk of introducing new defects and/or design problems during its fixing. We perform our study with 22 smell types, 803 versions of 13 open-source projects, 40,340 bugs and 132,219 code smells. The results show that even though we have a high number of code smells, only 0.07% of those smells are harmful. The Abstract Function Call From Constructor is the smell type more related to HARMFUL CODE. To cross-validate our results, we also perform a survey with 60 developers. Most of them (98%) consider code smells harmful to the software, and 85% of those developers believe that code smells detection tools are important. But, those developers are not concerned about selecting tools that are able to detect HARMFUL CODE. We also evaluate machine learning techniques to classify code harmfulness: they reach the effectiveness of at least 97% to classify HARMFUL CODE. While the Random Forest is effective in classifying both SMELLY and HARMFUL CODE, the Gaussian Naive Bayes is the less effective technique. Our results also suggest that both software and developers' metrics are important to classify HARMFUL CODE.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {223–232},
numpages = {10},
keywords = {Code Smells, Machine Learning, Software Quality},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.1016/j.procs.2020.02.099,
author = {Li, Zhen and Jiang, Ying and Zhang, Xiao Jiang and Xu, Hai Yan},
title = {The Metric for Automatic Code Generation},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.02.099},
doi = {10.1016/j.procs.2020.02.099},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {279–286},
numpages = {8},
keywords = {Efficiency, Quality, Metric, Automatic code generation}
}

@article{10.1007/s10619-020-07291-1,
author = {Xiao, Lei and Miao, Huaikou and Shi, Tingting and Hong, Yu},
title = {LSTM-based deep learning for spatial–temporal software testing},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {38},
number = {3},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-020-07291-1},
doi = {10.1007/s10619-020-07291-1},
abstract = {Continuous integration (CI) software development practice has become more and more popular. Regression testing occurs very frequently in CI. Test case suites constantly change since new test cases are inserted and obsolete test case are removed in each cycle. The software developer hunts for quick-feedback of faults because of time constraint. An embedded software usually includes the spatial–temporal data in CI. The efficiency of regression testing for the embedded software is related to the space–time. To achieve ideal regression testing goals for the embedded software in CI, this paper proposes a novel test case prioritization approach using LSTM-Based (Long short-term memory) deep learning. LSTM is a time series prediction model. It can predict the probability of each test case detection fault in the next cycle according to the testing history information of all the previous CI cycles. The priority of test case can be obtained dynamically under the guidance of the probability. The experiments are conducted on two industrial data sets. The results verify that compared with some exiting test case prioritization approaches, our approach has better performance for embedded software as follows: (1) improve the prioritization effectiveness, (2) increase the fault detection rate in CI environment, and (3) decrease the testing execution time through automatic reduction the obsolete test cases.},
journal = {Distrib. Parallel Databases},
month = sep,
pages = {687–712},
numpages = {26},
keywords = {Long short-term memory, Test case prioritization, Regression testing, Continuous integration}
}

@inproceedings{10.1145/3293882.3330574,
author = {Li, Xia and Li, Wei and Zhang, Yuqun and Zhang, Lingming},
title = {DeepFL: integrating multiple fault diagnosis dimensions for deep fault localization},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330574},
doi = {10.1145/3293882.3330574},
abstract = {Learning-based fault localization has been intensively studied recently. Prior studies have shown that traditional Learning-to-Rank techniques can help precisely diagnose fault locations using various dimensions of fault-diagnosis features, such as suspiciousness values computed by various off-the-shelf fault localization techniques. However, with the increasing dimensions of features considered by advanced fault localization techniques, it can be quite challenging for the traditional Learning-to-Rank algorithms to automatically identify effective existing/latent features. In this work, we propose DeepFL, a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization. Although the approach is general, in this work, we collect various suspiciousness-value-based, fault-proneness-based and textual-similarity-based features from the fault localization, defect prediction and information retrieval areas, respectively. DeepFL has been studied on 395 real bugs from the widely used Defects4J benchmark. The experimental results show DeepFL can significantly outperform state-of-the-art TraPT/FLUCCS (e.g., localizing 50+ more faults within Top-1). We also investigate the impacts of deep model configurations (e.g., loss functions and epoch settings) and features. Furthermore, DeepFL is also surprisingly effective for cross-project prediction.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {169–180},
numpages = {12},
keywords = {Mutation testing, Fault localization, Deep learning},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3475716.3475789,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Wang, Dandan and Wang, Qing},
title = {Characterizing and Predicting Good First Issues},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475789},
doi = {10.1145/3475716.3475789},
abstract = {Background. Where to start contributing to a project is a critical challenge for newcomers of open source projects. To support newcomers, GitHub utilizes the Good First Issue (GFI) label, with which project members can manually tag issues in an open source project that are suitable for the newcomers. However, manually labeling GFIs is time- and effort-consuming given the large number of candidate issues. In addition, project members need to have a close understanding of the project to label GFIs accurately.Aims. This paper aims at providing a thorough understanding of the characteristics of GFIs and an automatic approach in GFIs prediction, to reduce the burden of project members and help newcomers easily onboard.Method. We first define 79 features to characterize the GFIs and further analyze the correlation between each feature and GFIs. We then build machine learning models to predict GFIs with the proposed features.Results. Experiments are conducted with 74,780 issues from 10 open source projects from GitHub. Results show that features related to the semantics, readability, and text richness of issues can be used to effectively characterize GFIs. Our prediction model achieves a median AUC of 0.88. Results from our user study further prove its potential practical value.Conclusions. This paper provides new insights and practical guidelines to facilitate the understanding of GFIs and the automation of GFIs labeling.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {13},
numpages = {12},
keywords = {Open Source Software, Newcomers, Machine Learning, Issue Report},
location = {Bari, Italy},
series = {ESEM '21}
}

@article{10.1007/s10664-019-09730-9,
author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
title = {Deep code comment generation with hybrid lexical and syntactical information},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09730-9},
doi = {10.1007/s10664-019-09730-9},
abstract = {During software maintenance, developers spend a lot of time understanding the source code. Existing studies show that code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named Hybrid-DeepCom to automatically generate code comments for the functional units of Java language, namely, Java methods. The generated comments aim to help developers understand the functionality of Java methods. Hybrid-DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. It formulates the comment generation task as the machine translation problem. Hybrid-DeepCom exploits a deep neural network that combines the lexical and structure information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects on GitHub. We evaluate the experimental results on both machine translation metrics and information retrieval metrics. Experimental results demonstrate that our method Hybrid-DeepCom outperforms the state-of-the-art by a substantial margin. In addition, we evaluate the influence of out-of-vocabulary tokens on comment generation. The results show that reducing the out-of-vocabulary tokens improves the accuracy effectively.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {2179–2217},
numpages = {39},
keywords = {Deep learning, Comment generation, Program comprehension}
}

@inproceedings{10.1145/3460319.3464813,
author = {Luo, Sicheng and Xu, Hui and Bi, Yanxiang and Wang, Xin and Zhou, Yangfan},
title = {Boosting symbolic execution via constraint solving time prediction (experience paper)},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464813},
doi = {10.1145/3460319.3464813},
abstract = {Symbolic execution is an essential approach for automated test case generation. However, the approach is generally not scalable to large programs. One critical reason is that the constraint solving problems in symbolic execution are generally hard. Consequently, the symbolic execution process may get stuck in solving such hard problems. To mitigate this issue, symbolic execution tools generally rely on a timeout threshold to terminate the solving. Such a timeout is generally set to a fixed, predefined value, e.g., five minutes in angr. Nevertheless, how to set a proper timeout is critical to the tool’s efficiency. This paper proposes an approach to tackle the problem by predicting the time required for solving a constraint model so that the symbolic execution engine could base on the information to determine whether to continue the current solving process. Due to the cost of the prediction itself, our approach triggers the predictor only when the solving time has exceeded a relatively small value. We have shown that such a predictor can achieve promising performance with several different machine learning models and datasets. By further employing an adaptive design, the predictor can achieve an F1-score ranging from 0.743 to 0.800 on these datasets. We then apply the predictor to eight programs and conduct simulation experiments. Results show that the efficiency of constraint solving for symbolic execution can be improved by 1.25x to 3x, depending on the distribution of the hardness of their constraint models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {336–347},
numpages = {12},
keywords = {Symbolic execution, SMT solving, Adaptive machine learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3368089.3409687,
author = {Kampmann, Alexander and Havrikov, Nikolas and Soremekun, Ezekiel O. and Zeller, Andreas},
title = {When does my program do this? learning circumstances of software behavior},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409687},
doi = {10.1145/3368089.3409687},
abstract = {A program fails. Under which circumstances does the failure occur? Our Alhazenapproach starts with a run that exhibits a particular behavior and automatically determines input features associated with the behavior in question: (1) We use a grammar to parse the input into individual elements. (2) We use a decision tree learner to observe and learn which input elements are associated with the behavior in question. (3) We use the grammar to generate additional inputs to further strengthen or refute hypotheses as learned associations. (4) By repeating steps 2&nbsp;and&nbsp;3, we obtain a theory that explains and predicts the given behavior. In our evaluation using inputs for find, grep, NetHack, and a JavaScript transpiler, the theories produced by Alhazen predict and produce failures with high accuracy and allow developers to focus on a small set of input features: “grep fails whenever the --fixed-strings option is used in conjunction with an empty search string.”},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1228–1239},
numpages = {12},
keywords = {software behavior, machine learning, error diagnosis, debugging},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3394486.3403220,
author = {Mansouri, Mehrdad and Arab, Ali and Zohrevand, Zahra and Ester, Martin},
title = {Heidegger: Interpretable Temporal Causal Discovery},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403220},
doi = {10.1145/3394486.3403220},
abstract = {Temporal causal discovery aims to find cause-effect relationships between time-series. However, none of the existing techniques is able to identify the causal profile, the temporal pattern that the causal variable needs to follow in order to trigger the most significant change in the outcome. Toward a new horizon, this study introduces the novel problem of Causal Profile Discovery, which is crucial for many applications such as adverse drug reaction and cyber-attack detection. This work correspondingly proposes Heidegger to discover causal profiles, comprised of a flexible randomized block design for hypothesis evaluation and an efficient profile search via on-the-fly graph construction and entropy-based pruning. Heidegger's performance is demonstrated/evaluated extensively on both synthetic and real-world data. The experimental results show the proposed method is robust to noise and flexible at detecting complex patterns.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1688–1696},
numpages = {9},
keywords = {temporal causal discovery, randomized-block design, pattern recognition, graph search},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1007/s11219-010-9128-1,
author = {M\i{}s\i{}rl\i{}, Ay\c{s}e Tosun and Bener, Ay\c{s}e Ba\c{s}ar and Turhan, Burak},
title = {An industrial case study of classifier ensembles for locating software defects},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9128-1},
doi = {10.1007/s11219-010-9128-1},
abstract = {As the application layer in embedded systems dominates over the hardware, ensuring software quality becomes a real challenge. Software testing is the most time-consuming and costly project phase, specifically in the embedded software domain. Misclassifying a safe code as defective increases the cost of projects, and hence leads to low margins. In this research, we present a defect prediction model based on an ensemble of classifiers. We have collaborated with an industrial partner from the embedded systems domain. We use our generic defect prediction models with data coming from embedded projects. The embedded systems domain is similar to mission critical software so that the goal is to catch as many defects as possible. Therefore, the expectation from a predictor is to get very high probability of detection (pd). On the other hand, most embedded systems in practice are commercial products, and companies would like to lower their costs to remain competitive in their market by keeping their false alarm (pf) rates as low as possible and improving their precision rates. In our experiments, we used data collected from our industry partners as well as publicly available data. Our results reveal that ensemble of classifiers significantly decreases pf down to 15% while increasing precision by 43% and hence, keeping balance rates at 74%. The cost-benefit analysis of the proposed model shows that it is enough to inspect 23% of the code on local datasets to detect around 70% of defects.},
journal = {Software Quality Journal},
month = sep,
pages = {515–536},
numpages = {22},
keywords = {Static code attributes, Ensemble of classifiers, Embedded software, Defect prediction}
}

@article{10.1016/j.infsof.2020.106380,
author = {Mo, Ran and Yin, Zhen},
title = {Exploring software bug-proneness based on evolutionary clique modeling and analysis},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106380},
doi = {10.1016/j.infsof.2020.106380},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {10},
keywords = {Co-change analysis, Mining repository, Software bug-proneness, Software design}
}

@inbook{10.5555/3454287.3455351,
author = {Gupta, Rahul and Kanade, Aditya and Shevade, Shirish},
title = {Neural attribution for semantic bug-localization in student programs},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Providing feedback is an integral part of teaching. Most open online courses on programming make use of automated grading systems to support programming assignments and give real-time feedback. These systems usually rely on test results to quantify the programs' functional correctness. They return failing tests to the students as feedback. However, students may find it difficult to debug their programs if they receive no hints about where the bug is and how to fix it. In this work, we present NeuralBugLocator, a deep learning based technique, that can localize the bugs in a faulty program with respect to a failing test, without even running the program. At the heart of our technique is a novel tree convolutional neural network which is trained to predict whether a program passes or fails a given test. To localize the bugs, we analyze the trained network using a state-of-the-art neural prediction attribution technique and see which lines of the programs make it predict the test outcomes. Our experiments show that NeuralBugLocator is generally more accurate than two state-of-the-art program-spectrum based and one syntactic difference based bug-localization baselines.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1064},
numpages = {11}
}

@article{10.1007/s10515-021-00287-w,
author = {Gadelha, Guilherme and Ramalho, Franklin and Massoni, Tiago},
title = {Traceability recovery between bug reports and test cases-a Mozilla Firefox case study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00287-w},
doi = {10.1007/s10515-021-00287-w},
abstract = {Automatic recovery of traceability between software artifacts may promote early detection of issues and better calculate change impact. Information Retrieval (IR) techniques have been proposed for the task, but they differ considerably in input parameters and results. It is difficult to assess results when those techniques are applied in isolation, usually in small or medium-sized software projects. Recently, multilayered approaches to machine learning, in special Deep Learning (DL), have achieved success in text classification through their capacity to model complex relationships among data. In this article, we apply several IR and DL techniques for investing automatic traceability between bug reports and manual test cases, using historical data from the Mozilla Firefox’s Quality Assurance (QA) team. In this case study, we assess the following IR techniques: LSI, LDA, and BM25, in addition to a DL architecture called Convolutional Neural Networks (CNNs), through the use of Word Embeddings. In this context of traceability, we observe poor performances from three out of the four studied techniques. Only the LSI technique presented acceptable results, standing out even over the state-of-the-art BM25 technique. The obtained results suggest that the semi-automatic application of the LSI technique – with an appropriate combination of thresholds – may be feasible for real-world software projects.},
journal = {Automated Software Engg.},
month = nov,
numpages = {46},
keywords = {Deep learning, Information retrieval, Traceability, Test cases, System features, Bug reports}
}

@article{10.4018/IJOSSP.2016040103,
author = {Sureka, Ashish and Lal, Sangeeta and Sardana, Neetu},
title = {Improving Logging Prediction on Imbalanced Datasets: A Case Study on Open Source Java Projects},
year = {2016},
issue_date = {April 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {2},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016040103},
doi = {10.4018/IJOSSP.2016040103},
abstract = {Logging is an important yet tough decision for OSS developers. Machine-learning models are useful in improving several steps of OSS development, including logging. Several recent studies propose machine-learning models to predict logged code construct. The prediction performances of these models are limited due to the class-imbalance problem since the number of logged code constructs is small as compared to non-logged code constructs. No previous study analyzes the class-imbalance problem for logged code construct prediction. The authors first analyze the performances of J48, RF, and SVM classifiers for catch-blocks and if-blocks logged code constructs prediction on imbalanced datasets. Second, the authors propose LogIm, an ensemble and threshold-based machine-learning model. Third, the authors evaluate the performance of LogIm on three open-source projects. On average, LogIm model improves the performance of baseline classifiers, J48, RF, and SVM, by 7.38%, 9.24%, and 4.6% for catch-blocks, and 12.11%, 14.95%, and 19.13% for if-blocks logging prediction.},
journal = {Int. J. Open Source Softw. Process.},
month = apr,
pages = {43–71},
numpages = {29},
keywords = {Tracing, Open Source, Machine Learning, Logging, Imbalanced Data, Ensemble Methods, Debugging, Data Sampling}
}

@article{10.1007/s10664-021-10010-8,
author = {Zhou, Cheng and Li, Bin and Sun, Xiaobing and Bo, Lili},
title = {Why and what happened? Aiding bug comprehension with automated category and causal link identification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10010-8},
doi = {10.1007/s10664-021-10010-8},
abstract = {When a new bug report is assigned to developers, they first need to understand what the bug report expresses (what) and why this bug occurs (why). To do so, developers usually explore different bug related data sources to investigate whether there are historical bugs with similar symptoms and causes related to the bug at hand. Automatic bug classification with respect to what and why information of bugs would enable developers to narrow down their search of bug resources and improve the bug fixing productivity. To achieve this goal, we propose an approach, BugClass, which applies a deep neural network classification approach based on Hierarchical Attention Networks (HAN) to automatically classify the bugs into different what and why categories by exploiting the bug repository and commit repository. Then, we explore the causal link relationship between what and why categories to further improve the accuracy of the bug classification. Experimental results demonstrate that BugClass is effective to classify the given bug reports into what and why categories, and can be also effectively used for identifying the why category for new bugs based on the causal link relations.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {36},
keywords = {Hierarchical attention network, Causal link, Bug classification, Bug comprehension}
}

@inproceedings{10.1145/2889160.2889164,
author = {Luo, Qi and Poshyvanyk, Denys and Nair, Aswathy and Grechanik, Mark},
title = {FOREPOST: a tool for detecting performance problems with feedback-driven learning software testing},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889164},
doi = {10.1145/2889160.2889164},
abstract = {A goal of performance testing is to find situations when applications unexpectedly exhibit worsened characteristics for certain combinations of input values. A fundamental question of performance testing is how to select a manageable subset of the input data faster to find performance problems in applications automatically.We present a novel tool, FOREPOST, for finding performance problems in applications automatically using black-box software testing. In this paper, we demonstrate how FOREPOST extracts rules from execution traces of applications by using machine learning algorithms, and then uses these rules to select test input data automatically to steer applications towards computationally intensive paths and to find performance problems. FOREPOST is available in our online appendix (http://www.cs.wm.edu/semeru/data/ICSE16-FOREPOST), which contains the tool, source code and demo video.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {593–596},
numpages = {4},
keywords = {performance testing, machine learning, black-box testing},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.eswa.2020.114134,
author = {Yang, Xueqi and Yu, Zhe and Wang, Junjie and Menzies, Tim},
title = {Understanding static code warnings: An incremental AI approach},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114134},
doi = {10.1016/j.eswa.2020.114134},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {12},
keywords = {Selection process, Static analysis, Active learning, Actionable warning identification}
}

@article{10.1155/2021/8522839,
author = {Althar, Raghavendra Rao and Samanta, Debabrata and Kaur, Manjit and Alnuaim, Abeer Ali and Aljaffan, Nouf and Aman Ullah, Mohammad and Koundal, Deepika},
title = {Software Systems Security Vulnerabilities Management by Exploring the Capabilities of Language Models Using NLP},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/8522839},
doi = {10.1155/2021/8522839},
abstract = {Security of the software system is a prime focus area for software development teams. This paper explores some data science methods to build a knowledge management system that can assist the software development team to ensure a secure software system is being developed. Various approaches in this context are explored using data of insurance domain-based software development. These approaches will facilitate an easy understanding of the practical challenges associated with actual-world implementation. This paper also discusses the capabilities of language modeling and its role in the knowledge system. The source code is modeled to build a deep software security analysis model. The proposed model can help software engineers build secure software by assessing the software security during software development time. Extensive experiments show that the proposed models can efficiently explore the software language modeling capabilities to classify software systems’ security vulnerabilities.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@article{10.1016/j.asoc.2017.03.016,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Herrera, Francisco},
title = {Cost-Sensitive back-propagation neural networks with binarization techniques in addressing multi-class problems and non-competent classifiers},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {56},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.03.016},
doi = {10.1016/j.asoc.2017.03.016},
abstract = {A novel method based on cost-sensitive neural networks with binarization techniques for multi-class problems is developed.The effect of aggregation methods for the proposed method is studied.The positive synergy between the management of non-competent classifiers and the proposed method is found.The effectiveness of our method tested on three different kinds of cost matrices is investigated.In this study, 25 real-world applications, from KEEL dataset repository, are selected for the experimental study. Multi-class classification problems can be addressed by using decomposition strategy. One of the most popular decomposition techniques is the One-vs-One (OVO) strategy, which consists of dividing multi-class classification problems into as many as possible pairs of easier-to-solve binary sub-problems. To discuss the presence of classes with different cost, in this paper, we examine the behavior of an ensemble of Cost-Sensitive Back-Propagation Neural Networks (CSBPNN) with OVO binarization techniques for multi-class problems. To implement this, the original multi-class cost-sensitive problem is decomposed into as many sub-problems as possible pairs of classes and each sub-problem is learnt in an independent manner using CSBPNN. Then a combination method is used to aggregate the binary cost-sensitive classifiers. To verify the synergy of the binarization technique and CSBPNN for multi-class cost-sensitive problems, we carry out a thorough experimental study. Specifically, we first develop the study to check the effectiveness of the OVO strategy for multi-class cost-sensitive learning problems. Then, we develop a comparison of several well-known aggregation strategies in our scenario. Finally, we explore whether further improvement can be achieved by using the management of non-competent classifiers. The experimental study is performed with three types of cost matrices and proper statistical analysis is employed to extract the meaningful findings.},
journal = {Appl. Soft Comput.},
month = jul,
pages = {357–367},
numpages = {11},
keywords = {One-vs-one, Neural networks, Dynamic classifier selection, Cost-sensitive learning, Aggregation strategies}
}

@inproceedings{10.1145/3468264.3468580,
author = {Lou, Yiling and Zhu, Qihao and Dong, Jinhao and Li, Xia and Sun, Zeyu and Hao, Dan and Zhang, Lu and Zhang, Lingming},
title = {Boosting coverage-based fault localization via graph-based representation learning},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468580},
doi = {10.1145/3468264.3468580},
abstract = {Coverage-based fault localization has been extensively studied in the literature due to its effectiveness and lightweightness for real-world systems. However, existing techniques often utilize coverage in an oversimplified way by abstracting detailed coverage into numbers of tests or boolean vectors, thus limiting their effectiveness in practice. In this work, we present a novel coverage-based fault localization technique, GRACE, which fully utilizes detailed coverage information with graph-based representation learning. Our intuition is that coverage can be regarded as connective relationships between tests and program entities, which can be inherently and integrally represented by a graph structure: with tests and program entities as nodes, while with coverage and code structures as edges. Therefore, we first propose a novel graph-based representation to reserve all detailed coverage information and fine-grained code structures into one graph. Then we leverage Gated Graph Neural Network to learn valuable features from the graph-based coverage representation and rank program entities in a listwise way. Our evaluation on the widely used benchmark Defects4J (V1.2.0) shows that GRACE significantly outperforms state-of-the-art coverage-based fault localization: GRACE localizes 195 bugs within Top-1 whereas the best compared technique can at most localize 166 bugs within Top-1. We further investigate the impact of each GRACE component and find that they all positively contribute to GRACE. In addition, our results also demonstrate that GRACE has learnt essential features from coverage, which are complementary to various information used in existing learning-based fault localization. Finally, we evaluate GRACE in the cross-project prediction scenario on extra 226 bugs from Defects4J (V2.0.0), and find that GRACE consistently outperforms state-of-the-art coverage-based techniques.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {664–676},
numpages = {13},
keywords = {Representation Learning, Graph Neural Network, Fault Localization},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1007/s11219-018-9430-x,
author = {Gergely, Tam\'{a}s and Balogh, Gergo? and Horv\'{a}th, Ferenc and Vancsics, B\'{e}la and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor},
title = {Differences between a static and a dynamic test-to-code traceability recovery method},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9430-x},
doi = {10.1007/s11219-018-9430-x},
abstract = {Recovering test-to-code traceability links may be required in virtually every phase of development. This task might seem simple for unit tests thanks to two fundamental unit testing guidelines: isolation (unit tests should exercise only a single unit) and separation (they should be placed next to this unit). However, practice shows that recovery may be challenging because the guidelines typically cannot be fully followed. Furthermore, previous works have already demonstrated that fully automatic test-to-code traceability recovery for unit tests is virtually impossible in a general case. In this work, we propose a semi-automatic method for this task, which is based on computing traceability links using static and dynamic approaches, comparing their results and presenting the discrepancies to the user, who will determine the final traceability links based on the differences and contextual information. We define a set of discrepancy patterns, which can help the user in this task. Additional outcomes of analyzing the discrepancies are structural unit testing issues and related refactoring suggestions. For the static test-to-code traceability, we rely on the physical code structure, while for the dynamic, we use code coverage information. In both cases, we compute combined test and code clusters which represent sets of mutually traceable elements. We also present an empirical study of the method involving 8 non-trivial open source Java systems.},
journal = {Software Quality Journal},
month = jun,
pages = {797–822},
numpages = {26},
keywords = {Unit testing, Traceability link recovery, Test-to-code traceability, Structural test smells, Refactoring, Code coverage}
}

@inproceedings{10.1145/2950290.2950308,
author = {Hanam, Quinn and Brito, Fernando S. de M. and Mesbah, Ali},
title = {Discovering bug patterns in JavaScript},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950308},
doi = {10.1145/2950290.2950308},
abstract = {JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug finding tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using language-construct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {144–156},
numpages = {13},
keywords = {static analysis, data mining, Node.js, JavaScript, Bug patterns},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@article{10.1016/j.infsof.2019.106204,
author = {Theisen, Christopher and Williams, Laurie},
title = {Better together: Comparing vulnerability prediction models},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106204},
doi = {10.1016/j.infsof.2019.106204},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {12},
keywords = {Software engineering, Prediction model, Vulnerabilities, Security}
}

@inproceedings{10.1145/3395363.3397369,
author = {Lutellier, Thibaud and Pham, Hung Viet and Pang, Lawrence and Li, Yitong and Wei, Moshi and Tan, Lin},
title = {CoCoNuT: combining context-aware neural translation models using ensemble for program repair},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397369},
doi = {10.1145/3395363.3397369},
abstract = {Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages. To address these challenges, we propose a new G&amp;V technique—CoCoNuT, which uses ensemble learning on the combination of convolutional neural networks (CNNs) and a new context-aware neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. To better represent the context of a bug, we introduce a new context-aware NMT architecture that represents the buggy source code and its surrounding context separately. CoCoNuT uses CNNs instead of recurrent neural networks (RNNs), since CNN layers can be stacked to extract hierarchical features and better model source code at different granularity levels (e.g., statements and functions). In addition, CoCoNuT takes advantage of the randomness in hyperparameter tuning to build multiple models that fix different bugs and combines these models using ensemble learning to fix more bugs. Our evaluation on six popular benchmarks for four programming languages (Java, C, Python, and JavaScript) shows that CoCoNuT correctly fixes (i.e., the first generated patch is semantically equivalent to the developer’s patch) 509 bugs, including 309 bugs that are fixed by none of the 27 techniques with which we compare.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {101–114},
numpages = {14},
keywords = {Neural Machine Translation, Deep Learning, Automated program repair, AI and Software Engineering},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3338906.3338941,
author = {Jimenez, Matthieu and Rwemalika, Renaud and Papadakis, Mike and Sarro, Federica and Le Traon, Yves and Harman, Mark},
title = {The importance of accounting for real-world labelling when predicting software vulnerabilities},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338941},
doi = {10.1145/3338906.3338941},
abstract = {Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis- lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {695–705},
numpages = {11},
keywords = {Software Vulnerabilities, Prediction Modelling, Machine Learning},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1016/j.infsof.2017.08.004,
title = {MULTI},
year = {2018},
issue_date = {January 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {93},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.08.004},
doi = {10.1016/j.infsof.2017.08.004},
abstract = {Context: Just-in-time software defect prediction (JIT-SDP) aims to conduct defect prediction on code changes, which have finer granularity. A recent study by Yang etal. has shown that there exist some unsupervised methods, which are comparative to supervised methods in effort-aware JIT-SDP.Objective: However, we still believe that supervised methods should have better prediction performance since they effectively utilize the gathered defect prediction datasets. Therefore we want to design a new supervised method for JIT-SDP with better performance.Method: In this article, we propose a multi-objective optimization based supervised method MULTI to build JIT-SDP models. In particular, we formalize JIT-SDP as a multi-objective optimization problem. One objective is designed to maximize the number of identified buggy changes and another object is designed to minimize the efforts in software quality assurance activities. There exists an obvious conflict between these two objectives. MULTI uses logistic regression to build the models and uses NSGA-II to generate a set of non-dominated solutions, which each solution denotes the coefficient vector for the logistic regression.Results: We design and conduct a large-scale empirical studies to compare MULTI with 43 state-of-the-art supervised and unsupervised methods under the three commonly used performance evaluation scenarios: cross-validation, cross-project-validation, and timewise-cross-validation. Based on six open-source projects with 227,417 changes in total, our experimental results show that MULTI can perform significantly better than all of the state-of-the-art methods when considering ACC and POPT performance metrics.Conclusion: By using multi-objective optimization, MULTI can perform significantly better than the state-of-the-art supervised and unsupervised methods in the three performance evaluation scenarios. The results confirm that supervised methods are still promising in effort-aware JIT-SDP.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {1–13},
numpages = {13}
}

@article{10.1155/2021/8325417,
author = {Amara, Dalila and Fatnassi, Ezzeddine and Ben Arfa Rabai, Latifa and Brada, P\v{r}emek},
title = {An Empirical Assessment and Validation of Redundancy Metrics Using Defect Density as Reliability Indicator},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/8325417},
doi = {10.1155/2021/8325417},
abstract = {Software metrics which are language-dependent are proposed as quantitative measures to assess internal quality factors for both method and class levels like cohesion and complexity. The external quality factors like reliability and maintainability are in general predicted using different metrics of internal attributes. Literature review shows a lack of software metrics which are proposed for reliability measurement and prediction. In this context, a suite of four semantic language-independent metrics was proposed by Mili et al. (2014) to assess program redundancy using Shannon entropy measure. The main objective of these metrics is to monitor program reliability. Despite their important purpose, they are manually computed and only theoretically validated. Therefore, this paper aims to assess the redundancy metrics and empirically validate them as significant reliability indicators. As software reliability is an external attribute that cannot be directly evaluated, we employ other measurable quality factors that represent direct reflections of this attribute. Among these factors, defect density is widely used to measure and predict software reliability based on software metrics. Therefore, a linear regression technique is used to show the usefulness of these metrics as significant indicators of software defect density. A quantitative model is then proposed to predict software defect density based on redundancy metrics in order to monitor software reliability.},
journal = {Sci. Program.},
month = jan,
numpages = {20}
}

@inproceedings{10.1109/ASE.2015.12,
author = {Di Sorbo, Andrea and Panichella, Sebastiano and Visaggio, Corrado A. and Di Penta, Massimiliano and Canfora, Gerardo and Gall, Harald C.},
title = {Development emails content analyzer: intention mining in developer discussions},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.12},
doi = {10.1109/ASE.2015.12},
abstract = {Written development communication (e.g. mailing lists, issue trackers) constitutes a precious source of information to build recommenders for software engineers, for example aimed at suggesting experts, or at redocumenting existing source code. In this paper we propose a novel, semi-supervised approach named DECA (Development Emails Content Analyzer) that uses Natural Language Parsing to classify the content of development emails according to their purpose (e.g. feature request, opinion asking, problem discovery, solution proposal, information giving etc), identifying email elements that can be used for specific tasks. A study based on data from Qt and Ubuntu, highlights a high precision (90%) and recall (70%) of DECA in classifying email content, outperforming traditional machine learning strategies. Moreover, we successfully used DECA for re-documenting source code of Eclipse and Lucene, improving the recall, while keeping high precision, of a previous approach based on ad-hoc heuristics.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {12–23},
numpages = {12},
keywords = {unstructured data mining, natural language processing, empirical study},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1145/3477535,
author = {Liu, Chao and Gao, Cuiyun and Xia, Xin and Lo, David and Grundy, John and Yang, Xiaohu},
title = {On the Reproducibility and Replicability of Deep Learning in Software Engineering},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3477535},
doi = {10.1145/3477535},
abstract = {Context: Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge.Objective: Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) reproducibility—whether the reported experimental results can be obtained by other researchers using authors’ artifacts (i.e., source code and datasets) with the same experimental setup; and (2) replicability—whether the reported experimental result can be obtained by other researchers using their re-implemented artifacts with a different experimental setup. We observed that DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process, unlike classical supervised machine learning (ML) methods (e.g., random forest). This study aims to investigate the urgency and importance of reproducibility and replicability for DL studies on SE tasks.Method: In this study, we conducted a literature review on 147 DL studies recently published in 20 SE venues and 20 AI (Artificial Intelligence) venues to investigate these issues. We also re-ran four representative DL models in SE to investigate important factors that may strongly affect the reproducibility and replicability of a study.Results: Our statistics show the urgency of investigating these two factors in SE, where only 10.2% of the studies investigate any research question to show that their models can address at least one issue of replicability and/or reproducibility. More than 62.6% of the studies do not even share high-quality source code or complete data to support the reproducibility of their complex models. Meanwhile, our experimental results show the importance of reproducibility and replicability, where the reported performance of a DL model could not be reproduced for an unstable optimization process. Replicability could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data.Conclusion: It is urgent for the SE community to provide a long-lasting link to a high-quality reproduction package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {15},
numpages = {46},
keywords = {software engineering, reproducibility, replicability, Deep learning}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {empirical study, expectation maximization, metric threshold}
}

@inproceedings{10.1145/3196321.3196326,
author = {Li, Xiaochen and Jiang, He and Liu, Dong and Ren, Zhilei and Li, Ge},
title = {Unsupervised deep bug report summarization},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196326},
doi = {10.1145/3196321.3196326},
abstract = {Bug report summarization is an effective way to reduce the considerable time in wading through numerous bug reports. Although some supervised and unsupervised algorithms have been proposed for this task, their performance is still limited, due to the particular characteristics of bug reports, including the evaluation behaviours in bug reports, the diverse sentences in software language and natural language, and the domain-specific predefined fields. In this study, we conduct the first exploration of the deep learning network on bug report summarization. Our approach, called DeepSum, is a novel stepped auto-encoder network with evaluation enhancement and predefined fields enhancement modules, which successfully integrates the bug report characteristics into a deep neural network. DeepSum is unsupervised. It significantly reduces the efforts on labeling huge training sets. Extensive experiments show that DeepSum outperforms the comparative algorithms by up to 13.2% and 9.2% in terms of F-score and Rouge-n metrics respectively over the public datasets, and achieves the state-of-the-art performance. Our work shows promising prospects for deep learning to summarize millions of bug reports.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {144–155},
numpages = {12},
keywords = {bug report summarization, deep learning, mining software repositories, unsupervised learning},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@article{10.1504/ijwgs.2020.110945,
author = {Sun, Chang-ai and Fu, An and Liu, Yiqiang and Wen, Qing and Wang, Zuoyi and Wu, Peng and Chen, Tsong Yueh},
title = {An iterative metamorphic testing technique for web services and case studies},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {4},
issn = {1741-1106},
url = {https://doi.org/10.1504/ijwgs.2020.110945},
doi = {10.1504/ijwgs.2020.110945},
abstract = {Metamorphic testing (MT) is an innovative approach to alleviating the oracle problem in software testing, which uses metamorphic relations of the program under test, instead of the test oracles, to verify its outputs. To alleviate the oracle problem of testing web services, we had previously proposed an MT framework for web services. In this paper, we further improve the efficiency and automation of this framework by leveraging metamorphic relations to iteratively generate test cases. We present a fixed-size iterative MT algorithm and implement it in the MT framework. We conduct three case studies to evaluate the fault detection effectiveness and efficiency of the proposed approach. Experimental results suggest that, compared with the conventional MT, iterative MT can achieve a comparable fault detection effectiveness, but with significantly fewer resources. Observations and limitations are summarised to provide new insights into the application of iterative MT.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {364–392},
numpages = {28},
keywords = {web services, test case generation, metamorphic relations, metamorphic testing}
}

@article{10.1016/j.infsof.2018.03.003,
author = {Xiao, Yan and Keung, Jacky and Bennin, Kwabena E. and Mi, Qing},
title = {Machine translation-based bug localization technique for bridging lexical gap},
year = {2018},
issue_date = {Jul 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {99},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.03.003},
doi = {10.1016/j.infsof.2018.03.003},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {58–61},
numpages = {4},
keywords = {Lexical mismatch, Machine translation, Deep learning, Bug localization}
}

@inproceedings{10.1145/2901739.2901740,
author = {Guo, Jin and Rahimi, Mona and Cleland-Huang, Jane and Rasin, Alexander and Hayes, Jane Huffman and Vierhauser, Michael},
title = {Cold-start software analytics},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901740},
doi = {10.1145/2901739.2901740},
abstract = {Software project artifacts such as source code, requirements, and change logs represent a gold-mine of actionable information. As a result, software analytic solutions have been developed to mine repositories and answer questions such as "who is the expert?," "which classes are fault prone?," or even "who are the domain experts for these fault-prone classes?" Analytics often require training and configuring in order to maximize performance within the context of each project. A cold-start problem exists when a function is applied within a project context without first configuring the analytic functions on project-specific data. This scenario exists because of the non-trivial effort necessary to instrument a project environment with candidate tools and algorithms and to empirically evaluate alternate configurations. We address the cold-start problem by comparatively evaluating 'best-of-breed' and 'profile-driven' solutions, both of which reuse known configurations in new project contexts. We describe and evaluate our approach against 20 project datasets for the three analytic areas of artifact connectivity, fault-prediction, and finding the expert, and show that the best-of-breed approach outperformed the profile-driven approach in all three areas; however, while it delivered acceptable results for artifact connectivity and find the expert, both techniques underperformed for cold-start fault prediction.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {142–153},
numpages = {12},
keywords = {software analytics, configuration, cold-start},
location = {Austin, Texas},
series = {MSR '16}
}

@article{10.1007/s11219-008-9058-3,
author = {Khoshgoftaar, Taghi M. and Rebours, Pierre and Seliya, Naeem},
title = {Software quality analysis by combining multiple projects and learners},
year = {2009},
issue_date = {March     2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-008-9058-3},
doi = {10.1007/s11219-008-9058-3},
abstract = {When building software quality models, the approach often consists of training data mining learners on a single fit dataset. Typically, this fit dataset contains software metrics collected during a past release of the software project that we want to predict the quality of. In order to improve the predictive accuracy of such quality models, it is common practice to combine the predictive results of multiple learners to take advantage of their respective biases. Although multi-learner classifiers have been proven to be successful in some cases, the improvement is not always significant because the information in the fit dataset sometimes can be insufficient. We present an innovative method to build software quality models using majority voting to combine the predictions of multiple learners induced on multiple training datasets. To our knowledge, no previous study in software quality has attempted to take advantage of multiple software project data repositories which are generally spread across the organization. In a large scale empirical study involving seven real-world datasets and seventeen learners, we show that, on average, combining the predictions of one learner trained on multiple datasets significantly improves the predictive performance compared to one learner induced on a single fit dataset. We also demonstrate empirically that combining multiple learners trained on a single training dataset does not significantly improve the average predictive accuracy compared to the use of a single learner induced on a single fit dataset.},
journal = {Software Quality Journal},
month = mar,
pages = {25–49},
numpages = {25},
keywords = {Software quality classification model, Multiple software metrics repositories, Multiple learners, Majority voting, Data mining, Cost of misclassification}
}

@inproceedings{10.1007/978-3-030-59618-7_11,
author = {Kimball, Joshua and Lima, Rodrigo Alves and Pu, Calton},
title = {Finding Performance Patterns from Logs with High Confidence},
year = {2020},
isbn = {978-3-030-59617-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59618-7_11},
doi = {10.1007/978-3-030-59618-7_11},
abstract = {Performance logs contain rich information about a system’s state. Large-scale web service infrastructures deployed in the cloud are notoriously difficult to troubleshoot, especially performance bugs. Detecting, isolating and diagnosing fine-grained performance anomalies requires integrating system performance measures across space and time. To achieve scale, we present our megatables approach, which automatically interprets performance log data and outputs millibottleneck predictions along with supporting visualizations. We evaluate our method with three illustrative scenarios, and we assess its predictive ability. We also evaluate its ability to extract meaningful information from many log samples drawn from the wild.},
booktitle = {Web Services – ICWS 2020: 27th International Conference, Held as Part of the Services Conference Federation, SCF 2020, Honolulu, HI, USA, September 18–20, 2020, Proceedings},
pages = {164–178},
numpages = {15},
keywords = {Cloud computing, Performance debugging, Anomaly detection, Data mining, Log data analysis, Information integration},
location = {Honolulu, HI, USA}
}

@inproceedings{10.5555/3155562.3155633,
author = {Ocariza, Jr., Frolin S. and Pattabiraman, Karthik and Mesbah, Ali},
title = {Detecting unknown inconsistencies in web applications},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {Although there has been increasing demand for more reliable web applications, JavaScript bugs abound in web applications. In response to this issue, researchers have proposed automated fault detection tools, which statically analyze the web application code to find bugs. While useful, these tools either only target a limited set of bugs based on predefined rules, or they do not detect bugs caused by cross-language interactions, which occur frequently in web application code. To address this problem, we present an anomaly-based inconsistency detection approach, implemented in a tool called Holocron. The main novelty of our approach is that it does not look for hard-coded inconsistency classes. Instead, it applies subtree pattern matching to infer inconsistency classes and association rule mining to detect inconsistencies that occur both within a single language, and between two languages. We evaluated Holocron, and it successfully detected 51 previously unreported inconsistencies - including 18 bugs and 33 code smells - in 12 web applications.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {566–577},
numpages = {12},
keywords = {fault detection, cross-language interactions, JavaScript},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@article{10.1016/j.future.2018.09.051,
author = {Viegas, Eduardo and Santin, Altair and Bessani, Alysson and Neves, Nuno},
title = {BigFlow: Real-time and reliable anomaly-based intrusion detection for high-speed networks},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.051},
doi = {10.1016/j.future.2018.09.051},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {473–485},
numpages = {13},
keywords = {Anomaly-based intrusion detection, Classification reliability, Stream learning, Data stream}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00036,
author = {Beller, Moritz and Wong, Chu-Pan and Bader, Johannes and Scott, Andrew and Machalica, Mateusz and Chandra, Satish and Meijer, Erik},
title = {What it would take to use mutation testing in industry: a study at Facebook},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00036},
doi = {10.1109/ICSE-SEIP52600.2021.00036},
abstract = {Traditionally, mutation testing generates an abundance of small deviations of a program, called mutants. At industrial systems the scale and size of Facebook's, doing this is infeasible. We should not create mutants that the test suite would likely fail on or that give no actionable signal to developers. To tackle this problem, in this paper, we semi-automatically learn error-inducing patterns from a corpus of common Java coding errors and from changes that caused operational anomalies at Facebook specifically. We combine the mutations with instrumentation that measures which tests exactly visited the mutated piece of code. Results on more than 15,000 generated mutants show that more than half of the generated mutants survive Facebook's rigorous test suite of unit, integration, and system tests. Moreover, in a case study with 26 developers, all but two expressed that the mutation exposed a lack of testing in principle. As such, almost half of the 26 would actually act on the mutant presented to them by adapting an existing or creating a new test. The others did not for a variety of reasons often outside the scope of mutation testing. It remains a practical challenge how we can include such external information to increase the actionability rate on mutants.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {268–277},
numpages = {10},
keywords = {mutation testing, mutation monkey, machine learning, getafix},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3383219.3383236,
author = {Petri\'{c}, Jean and Hall, Tracy and Bowes, David},
title = {Which Software Faults Are Tests Not Detecting?},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383236},
doi = {10.1145/3383219.3383236},
abstract = {Context: Software testing plays an important role in assuring the reliability of systems. Assessing the efficacy of testing remains challenging with few established test effectiveness metrics. Those metrics that have been used (e.g. coverage and mutation analysis) have been criticised for insufficiently differentiating between the faults detected by tests. Objective: We investigate how effective tests are at detecting different types of faults and whether some types of fault evade tests more than others. Our aim is to suggest to developers specific ways in which their tests need to be improved to increase fault detection. Method: We investigate seven fault types and analyse how often each goes undetected in 10 open source systems. We statistically look for any relationship between the test set and faults. Results: Our results suggest that the fault detection rates of unit tests are relatively low, typically finding only about a half of all faults. In addition, conditional boundary and method call removals are less well detected by tests than other fault types. Conclusions: We conclude that the testing of these open source systems needs to be improved across the board. In addition, despite boundary cases being long known to attract faults, tests covering boundaries need particular improvement. Overall, we recommend that developers do not rely only on code coverage and mutation score to measure the effectiveness of their tests.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {160–169},
numpages = {10},
keywords = {unit tests, test effectiveness, software testing},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3379597.3387482,
author = {Pinto, Gustavo and Miranda, Breno and Dissanayake, Supun and d'Amorim, Marcelo and Treude, Christoph and Bertolino, Antonia},
title = {What is the Vocabulary of Flaky Tests?},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387482},
doi = {10.1145/3379597.3387482},
abstract = {Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research activity on this topic, no effort has been made on understanding the vocabulary of flaky tests. This work proposes to automatically classify tests as flaky or not based on their vocabulary. Static classification of flaky tests is important, for example, to detect the introduction of flaky tests and to search for flaky tests after they are introduced in regression test suites.We evaluated performance of various machine learning algorithms to solve this problem. We constructed a data set of flaky and non-flaky tests by running every test case, in a set of 64k tests, 100 times (6.4 million test executions). We then used machine learning techniques on the resulting data set to predict which tests are flaky from their source code. Based on features, such as counting stemmed tokens extracted from source code identifiers, we achieved an F-measure of 0.95 for the identification of flaky tests. The best prediction performance was obtained when using Random Forest and Support Vector Machines. In terms of the code identifiers that are most strongly associated with test flakiness, we noted that job, action, and services are commonly associated with flaky tests. Overall, our results provides initial yet strong evidence that static detection of flaky tests is effective.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {492–502},
numpages = {11},
keywords = {Text classification, Test flakiness, Regression testing},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3324884.3415292,
author = {Yu, Runze and Zhang, Youzhe and Xuan, Jifeng},
title = {MetPurity: a learning-based tool of pure method identification for automatic test generation},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415292},
doi = {10.1145/3324884.3415292},
abstract = {In object-oriented programming, a method is pure if calling the method does not change object states that exist in the pre-states of the method call. Pure methods are widely-used in automatic techniques, including test generation, compiler optimization, and program repair. Due to the source code dependency, it is infeasible to completely and accurately identify all pure methods. Instead, existing techniques such as ReImInfer are designed to identify a subset of accurate results of pure method and mark the other methods as unknown ones. In this paper, we designed and implemented MetPurity, a learning-based tool of pure method identification. Given all methods in a project, MetPurity labels a training set via automatic program analysis and builds a binary classifier (implemented with the random forest classifier) based on the training set. This classifier is used to predict the purity of all the other methods (i.e., unknown ones) in the same project. Preliminary evaluation on four open-source Java projects shows that MetPurity can provide a list of identified pure methods with a low error rate. Applying MetPurity to EvoSuite can increase the number of generated assertions for regression testing in test generation by EvoSuite.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1326–1330},
numpages = {5},
keywords = {debugging, machine learning, method purity, regression testing, static analysis, test generation},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3428283,
author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
title = {A structural model for contextual code changes},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428283},
doi = {10.1145/3428283},
abstract = {We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program’s Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task. We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs, Transformers, and neural CRFs. Our experiments show that our model achieves a 28% relative gain over state-of-the-art sequential models and 2\texttimes{} higher accuracy than syntactic models that learn to generate the edited code, as opposed to modeling the edits directly. Our code, dataset, and trained models are publicly available at &lt;a&gt;https://github.com/tech-srl/c3po/&lt;/a&gt; .},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {215},
numpages = {28},
keywords = {Neural Models of Code, Machine Learning, Edit Completions}
}

@inproceedings{10.1007/978-3-030-78292-4_1,
author = {Abhinav, Kumar and Sharvani, Vijaya and Dubey, Alpana and D’Souza, Meenakshi and Bhardwaj, Nitish and Jain, Sakshi and Arora, Veenu},
title = {RepairNet: Contextual Sequence-to-Sequence Network for Automated Program Repair},
year = {2021},
isbn = {978-3-030-78291-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78292-4_1},
doi = {10.1007/978-3-030-78292-4_1},
abstract = {Compile-time errors can wreak havoc for programmers – seasoned and novice. Often developers spend a lot of time debugging them. An automated system to repair such errors can be a useful aid to the developers for their productivity. In this work, we propose a deep generative model, RepairNet, that automatically repairs programs that fail at compile time. RepairNet is based on sequence-to-sequence modeling and uses both code and error messages to repair the program. We evaluated the effectiveness of our system on 6,971 erroneous submissions for 93 programming tasks. RepairNet outperforms the existing state-of-the-art technique, MACER, with 17% relative improvement of repair accuracy. Our approach can fix 66.4% of the erroneous submissions completely and 14.2% partially.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part I},
pages = {3–15},
numpages = {13},
keywords = {Bug fixing, Sequence modeling, Program repair},
location = {Utrecht, The Netherlands}
}

@inproceedings{10.1109/ICSE43902.2021.00041,
author = {Mastropaolo, Antonio and Scalabrino, Simone and Cooper, Nathan and Palacio, David Nader and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
title = {Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00041},
doi = {10.1109/ICSE43902.2021.00041},
abstract = {Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {336–347},
numpages = {12},
keywords = {Empirical software engineering, Deep Learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/99412.99487,
author = {Sheppard, John and Simpson, William R.},
title = {Using a competitive learning neural network to evaluate software complexity},
year = {1990},
isbn = {0897913477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/99412.99487},
doi = {10.1145/99412.99487},
abstract = {With recent advances in neural networks, an increasing number of application areas are being explored for this technology. Also, as software takes a more prominent role in systems engineering, ensuring the quality of software is becoming a critical issue. This paper explores the application of one neural network paradigm—the competitive learning network—to the problem of evaluating software complexity. The network was developed by ARINC Research Corporation for its SofTest software analysis system, developed on a Sun workstation. In this paper, we discuss the network used in SofTest and the approach taken to train the network. We conclude with a discussion of the implications of the approach and areas for further research.},
booktitle = {Proceedings of the 1990 ACM SIGSMALL/PC Symposium on Small Systems},
pages = {262–267},
numpages = {6},
location = {Crystal City, Virginia, USA},
series = {SIGSMALL '90}
}

@inproceedings{10.1145/3485832.3488018,
author = {Koo, Hyungjoon and Park, Soyeon and Kim, Taesoo},
title = {A Look Back on a Function Identification Problem},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3488018},
doi = {10.1145/3485832.3488018},
abstract = {A function recognition problem serves as a basis for further binary analysis and many applications. Although common challenges for function detection are well known, prior works have repeatedly claimed a noticeable result with a high precision and recall. In this paper, we aim to fill the void of what has been overlooked or misinterpreted by closely looking into the previous datasets, metrics, and evaluations with varying case studies. Our major findings are that i)&nbsp;a common corpus like GNU utilities is insufficient to represent the effectiveness of function identification, ii)&nbsp;it is difficult to claim, at least in the current form, that an ML-oriented approach is scientifically superior to deterministic ones like IDA or Ghidra, iii)&nbsp;the current metrics may not be reasonable enough to measure varying function detection cases, and iv)&nbsp;the capability of recognizing functions depends on each tool’s strategic or peculiar choice. We perform re-evaluation of existing approaches on our own dataset, demonstrating that not a single state-of-the-art tool dominates all the others. In conclusion, a function detection problem has not yet been fully addressed, and we need a better methodology and metric to make advances in the field of function identification.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {158–168},
numpages = {11},
keywords = {Binary, Function Identification, Function Recognition, Lookback, ML-oriented},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@inproceedings{10.1145/2889160.2889260,
author = {Le, Duc and Medvidovic, Nenad},
title = {Architectural-based speculative analysis to predict bugs in a software system},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889260},
doi = {10.1145/2889160.2889260},
abstract = {Over time, a software system's code and its underlying design tend to decay steadily and, in turn, to complicate the system's maintenance. In order to address that phenomenon, many researchers tried to help engineers predict parts of a system that are most likely to create problems while or even before they are modifying the system. Problems that creep into a system may manifest themselves as bugs, in which case engineers have no choice but to fix them or develop workarounds. However, these problems may also be more subtle, such as code clones, circular dependencies among system elements, very large APIs, individual elements that implement multiple diffuse concerns, etc. Even though such architectural and code "smells" may not crash a system outright, they impose real costs in terms of engineers' time and effort, as well as system correctness and performance. Along the time, implicit problems may be revealed as explicit problems. However, most current techniques predict explicit problems of a system only based on explicit problems themselves. Our research takes a further step by using implicit problems, e.g., architectural- and code-smells, in combination with explicit problems to provide an accurate, systematic and in depth approach to predict potential system problems, particularly bugs.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {807–810},
numpages = {4},
keywords = {speculative analysis, bug prediction, architectural-based analysis, architectural decay},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.jss.2016.08.071,
author = {Szke, Gbor and Antal, Gbor and Nagy, Csaba and Ferenc, Rudolf and Gyimthy, Tibor},
title = {Empirical study on refactoring large-scale industrial systems and its effects on maintainability},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.08.071},
doi = {10.1016/j.jss.2016.08.071},
abstract = {We examine hundreds of manual refactoring commits from large-scale industrial systems.We study the effects of these commits on source code using a maintainability model.Developers preferred to fix concrete coding issues rather than fix code smells.A single refactoring had only a small impact (sometimes even negative effect).Whole refactoring process has significant beneficial effect on the maintainability. Software evolves continuously, it gets modified, enhanced, and new requirements always arise. If we do not spend time occasionally on improving our source code, its maintainability will inevitably decrease. The literature tells us that we can improve the maintainability of a software system by regularly refactoring it. But does refactoring really increase software maintainability? Can it happen that refactoring decreases the maintainability? Empirical studies show contradicting answers to these questions and there have been only a few studies which were performed in a large-scale, industrial context. In our paper, we assess these questions in an in vivo context, where we analyzed the source code and measured the maintainability of 6 large-scale, proprietary software systems in their manual refactoring phase. We analyzed 2.5 million lines of code and studied the effects on maintainability of 315 refactoring commits which fixed 1273 coding issues. We found that single refactorings only make a very little difference (sometimes even decrease maintainability), but a whole refactoring period, in general, can significantly increase maintainability, which can result not only in the local, but also in the global improvement of the code.},
journal = {J. Syst. Softw.},
month = jul,
pages = {107–126},
numpages = {20},
keywords = {Software quality, Refactoring, Maintainability, ISO/IEC 25010, Coding issues, Antipatterns}
}

@inproceedings{10.1145/2896921.2896923,
author = {Felbinger, Hermann and Wotawa, Franz and Nica, Mihai},
title = {Empirical study of correlation between mutation score and model inference based test suite adequacy assessment},
year = {2016},
isbn = {9781450341516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896921.2896923},
doi = {10.1145/2896921.2896923},
abstract = {In this paper we investigate a method for test suite evaluation that is based on an inferred model from the test suite. The idea is to use the similarity between the inferred model and the system under test as a measure of test suite adequacy, which is the ability of a test suite to expose errors in the system under test. We define similarity using the root mean squared error computed from the differences of the system under test output and the model output for certain inputs not used for model inference. In the paper we introduce the approach and provide results of an experimental evaluation where we compare the similarity with the mutation score. We used the Pearson Correlation coefficient to calculate whether a linear correlation between mutation score and root mean squared error exists. As a result we obtain that in certain cases the computed similarity strongly correlates with the mutation score.},
booktitle = {Proceedings of the 11th International Workshop on Automation of Software Test},
pages = {43–49},
numpages = {7},
keywords = {software test, mutation score, machine learning},
location = {Austin, Texas},
series = {AST '16}
}

@article{10.1016/j.jss.2019.01.056,
author = {Liu, Yong and Li, Meiying and Wu, Yonghao and Li, Zheng},
title = {A weighted fuzzy classification approach to identify and manipulate coincidental correct test cases for fault localization},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.056},
doi = {10.1016/j.jss.2019.01.056},
journal = {J. Syst. Softw.},
month = may,
pages = {20–37},
numpages = {18},
keywords = {K-Nearest Neighbor, Fuzzy classification, Coincidental correct test cases, Coverage-Based fault localization, Software debugging}
}

@inproceedings{10.1145/3387904.3389252,
author = {Wu, Liwei and Li, Fei and Wu, Youhua and Zheng, Tao},
title = {GGF: A Graph-based Method for Programming Language Syntax Error Correction},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389252},
doi = {10.1145/3387904.3389252},
abstract = {Syntax errors combined with obscure error messages generated by compilers usually annoy programmers and cause them to waste a lot of time on locating errors. The existing models do not utilize the structure in the code and just treat the code as token sequences. It causes low accuracy and poor performance on this task. In this paper, we propose a novel deep supervised learning model, called Graph-based Grammar Fix(GGF), to help programmers locate and fix the syntax errors. GGF treats the code as a mixture of the token sequences and graphs. The graphs build upon the Abstract Syntax Tree (AST) structure information. GGF encodes an erroneous code with its sub-AST structure, predicts the error position using pointer network and generates the right token. We utilized the DeepFix dataset which contains 46500 correct C programs and 6975 programs with errors written by students taking an introductory programming course. GGF is trained with the correct programs from the DeepFix dataset with intentionally injected syntax errors. After training, GGF could fix 4054 (58.12%) of the erroneous code, while the existing state of the art tool DeepFix fixes 1365 (19.57%) of the erroneous code.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {139–148},
numpages = {10},
keywords = {Syntax Error Correction, GGNN, Deep Learning},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.1016/j.patcog.2016.08.023,
author = {Zhang, Xiuzhen and Li, Yuxuan and Kotagiri, Ramamohanarao and Wu, Lifang and Tari, Zahir and Cheriet, Mohamed},
title = {KRNN},
year = {2017},
issue_date = {February 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {62},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.08.023},
doi = {10.1016/j.patcog.2016.08.023},
abstract = {Imbalanced classification is a challenging problem. Re-sampling and cost-sensitive learning are global strategies for generality-oriented algorithms such as the decision tree, targeting inter-class imbalance. We research local strategies for the specificity-oriented learning algorithms like the k Nearest Neighbour (KNN) to address the within-class imbalance issue of positive data sparsity. We propose an algorithm k Rare-class Nearest Neighbour, or KRNN, by directly adjusting the induction bias of KNN. We propose to form dynamic query neighbourhoods, and to further adjust the positive posterior probability estimation to bias classification towards the rare class. We conducted extensive experiments on thirty real-world and artificial datasets to evaluate the performance of KRNN. Our experiments showed that KRNN significantly improved KNN for classification of the rare class, and often outperformed re-sampling and cost-sensitive learning strategies with generality-oriented base learners. HighlightsNearest neighbour classification algorithm for accurate rare-class classification.Dynamic nearest neighbourhood formulation.Adjusted posterior class probability estimation biased for the rare class.},
journal = {Pattern Recogn.},
month = feb,
pages = {33–44},
numpages = {12},
keywords = {Re-sampling, Nearest neighbour classification, KNN, Imbalanced classification, Cost-sensitive learning}
}

@inproceedings{10.1109/MSR.2019.00018,
author = {Kiehn, Max and Pan, Xiangyi and Camci, Fatih},
title = {Empirical study in using version histories for change risk classification},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00018},
doi = {10.1109/MSR.2019.00018},
abstract = {Many techniques have been proposed for mining software repositories, predicting code quality and evaluating code changes. Prior work has established links between code ownership and churn metrics, and software quality at file and directory level based on changes that fix bugs. Other metrics have been used to evaluate individual code changes based on preceding changes that induce fixes. This paper combines the two approaches in an empirical study of assessing risk of code changes using established code ownership and churn metrics with fix inducing changes on a large proprietary code repository. We establish a machine learning model for change risk classification which achieves average precision of 0.76 using metrics from prior works and 0.90 using a wider array of metrics. Our results suggest that code ownership metrics can be applied in change risk classification models based on fix inducing changes.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {58–62},
numpages = {5},
keywords = {machine learning, file metrics, code ownership, change risk},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/1328279.1328287,
author = {Madhavan, Janaki T. and Whitehead, E. James},
title = {Predicting buggy changes inside an integrated development environment},
year = {2007},
isbn = {9781605580159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1328279.1328287},
doi = {10.1145/1328279.1328287},
abstract = {We present a tool that predicts whether the software under development inside an IDE has a bug. An IDE plugin performs this prediction, using the Change Classification technique to classify source code changes as buggy or clean during the editing session. Change Classification uses Support Vector Machines (SVM), a machine learning classifier algorithm, to classify changes to projects mined from their configuration management repository. This technique, besides being language independent and relatively accurate, can (a) classify a change immediately upon its completion and (b) use features extracted solely from the change delta (added, deleted) and the source code to predict buggy changes. Thus, integrating change classification within an IDE can predict potential bugs in the software as the developer edits the source code, ideally reducing the amount of time spent on fixing bugs later. To this end, we have developed a Change Classification plugin for Eclipse based on client-server architecture, described in this paper.},
booktitle = {Proceedings of the 2007 OOPSLA Workshop on Eclipse Technology EXchange},
pages = {36–40},
numpages = {5},
keywords = {integrated development environments, bug prediction},
location = {Montreal, Quebec, Canada},
series = {eclipse '07}
}

@article{10.1155/2018/6798042,
author = {Guo, Huaping and Diao, Xiaoyu and Liu, Hongbing and Gutierrez, Pedro Antonio},
title = {Embedding Undersampling Rotation Forest for Imbalanced Problem},
year = {2018},
issue_date = {2018},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2018},
issn = {1687-5265},
url = {https://doi.org/10.1155/2018/6798042},
doi = {10.1155/2018/6798042},
abstract = {Rotation Forest is an ensemble learning approach achieving better performance comparing to Bagging and Boosting through building accurate and diverse classifiers using rotated feature space. However, like other conventional classifiers, Rotation Forest does not work well on the imbalanced data which are characterized as having much less examples of one class (minority class) than the other (majority class), and the cost of misclassifying minority class examples is often much more expensive than the contrary cases. This paper proposes a novel method called Embedding Undersampling Rotation Forest (EURF) to handle this problem (1) sampling subsets from the majority class and learning a projection matrix from each subset and (2) obtaining training sets by projecting re-undersampling subsets of the original data set to new spaces defined by the matrices and constructing an individual classifier from each training set. For the first method, undersampling is to force the rotation matrix to better capture the features of the minority class without harming the diversity between individual classifiers. With respect to the second method, the undersampling technique aims to improve the performance of individual classifiers on the minority class. The experimental results show that EURF achieves significantly better performance comparing to other state-of-the-art methods.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {15}
}

@article{10.1109/TSE.2011.5,
author = {Le Goues, Claire and Weimer, Westley},
title = {Measuring Code Quality to Improve Specification Mining},
year = {2012},
issue_date = {January 2012},
publisher = {IEEE Press},
volume = {38},
number = {1},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2011.5},
doi = {10.1109/TSE.2011.5},
abstract = {Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code.},
journal = {IEEE Trans. Softw. Eng.},
month = jan,
pages = {175–190},
numpages = {16},
keywords = {software engineering, program understanding., machine learning, code metrics, Specification mining}
}

@article{10.1504/ijics.2021.116302,
author = {Jabeen, Gul and Yang, Xi and Luo, Ping},
title = {Vulnerability severity prediction model for software based on Markov chain},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {2–3},
issn = {1744-1765},
url = {https://doi.org/10.1504/ijics.2021.116302},
doi = {10.1504/ijics.2021.116302},
abstract = {Software vulnerabilities primarily constitute security risks. Commonalities between faults and vulnerabilities prompt developers to utilise traditional fault prediction models and metrics for vulnerability prediction. Although traditional models can predict the number of vulnerabilities and their occurrence time, they fail to accurately determine the seriousness of vulnerabilities, impacts, and severity level. To address these deficits, we propose a method for predicting software vulnerabilities based on a Markov chain model, which offers a more comprehensive descriptive model with the potential to accurately predict vulnerability type, i.e., the seriousness of the vulnerabilities. The experiments are performed using real vulnerability data of three types of popular software: Windows 10, Adobe Flash Player and Firefox. Our model is shown to produce accurate predictive results.},
journal = {Int. J. Inf. Comput. Secur.},
month = jan,
pages = {109–140},
numpages = {31},
keywords = {Markov chain, software security, prediction model, severity/seriousness, VL, software vulnerability}
}

@inproceedings{10.1145/3340422.3343639,
author = {Kumar, Lov and Hota, Chinmay and Mahindru, Arvind and Neti, Lalita Bhanu Murthy},
title = {Android Malware Prediction Using Extreme Learning Machine with Different Kernel Functions},
year = {2019},
isbn = {9781450368490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340422.3343639},
doi = {10.1145/3340422.3343639},
abstract = {Android is currently the most popular smartphone platform which occupied 88% of global sale by the end of 2nd quarter 2018. With the popularity of these applications, it is also inviting cybercriminals to develop malware application for accessing important information from smartphones. The major objective of cybercriminals to develop Malware apps or Malicious apps to threaten the organization privacy data, user privacy data, and device integrity. Early identification of such malware apps can help the android user to save private data and device integrity. In this study, features extracted from intermediate code representations obtained using decompilation of APK file are used for providing requisite input data to develop the models for predicting android malware applications. These models are trained using extreme learning with multiple kernel functions ans also compared with the model trained using most frequently used classifiers like linear regression, decision tree, polynomial regression, and logistic regression. This paper also focuses on the effectiveness of data sampling techniques for balancing data and feature selection methods for selecting right sets of significant uncorrelated metrics. The high-value of accuracy and AUC confirm the predicting capability of data sampling, sets of metrics, and training algorithms to malware and normal applications.},
booktitle = {Proceedings of the 15th Asian Internet Engineering Conference},
pages = {33–40},
numpages = {8},
keywords = {Parallel Computing, Object-Oriented Metrics, Maintainability, Genetics algorithm, Artificial neural network},
location = {Phuket, Thailand},
series = {AINTEC '19}
}

@inproceedings{10.1145/3383219.3383264,
author = {Madeyski, Lech and Lewowski, Tomasz},
title = {MLCQ: Industry-Relevant Code Smell Data Set},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383264},
doi = {10.1145/3383219.3383264},
abstract = {Context Research on code smells accelerates and there are many studies that discuss them in the machine learning context. However, while data sets used by researchers vary in quality, all which we encountered share visible shortcomings---data sets are gathered from a rather small number of often outdated projects by single individuals whose professional experience is unknown.Aim This study aims to provide a new data set that addresses the aforementioned issues and, additionally, opens new research opportunities.Method We collaborate with professional software developers (including the code quest company behind the codebeat automated code review platform integrated with GitHub) to review code samples with respect to bad smells. We do not provide additional hints as to what do we mean by a given smell, because our goal is to extract professional developers' contemporary understanding of code smells instead of imposing thresholds from the legacy literature. We gather samples from active open source projects manually verified for industry-relevance and provide repository links and revisions. Records in our MLCQ data set contain the type of smell, its severity and the exact location in source code, but do not contain any source code metrics which can be calculated using various tools. To open new research opportunities, we provide results of an extensive survey of developers involved in the study including a wide range of details concerning their professional experience in software development and many other characteristics. This allows us to track each code review to the developer's background. To the best of our knowledge, this is a unique trait of the presented data set.Conclusions The MLCQ data set with nearly 15000 code samples was created by software developers with professional experience who reviewed industry-relevant, contemporary Java open source projects. We expect that this data set should stay relevant for a longer time than data sets that base on code released years ago and, additionally, will enable researchers to investigate the relationship between developers' background and code smells' perception.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {342–347},
numpages = {6},
keywords = {software quality, software development, data set, code smells, bad code smells},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.1016/j.infsof.2019.01.007,
author = {Bafandeh Mayvan, B. and Rasoolzadegan, A. and Ebrahimi, A.M.},
title = {A new benchmark for evaluating pattern mining methods based on the automatic generation of testbeds},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.007},
doi = {10.1016/j.infsof.2019.01.007},
journal = {Inf. Softw. Technol.},
month = may,
pages = {60–79},
numpages = {20},
keywords = {Automatic code generation, Design pattern detection, Pattern mining, Benchmarking}
}

@article{10.1016/j.neucom.2008.08.024,
author = {Glezakos, Thomas J. and Tsiligiridis, Theodore A. and Iliadis, Lazaros S. and Yialouris, Constantine P. and Maris, Fotis P. and Ferentinos, Konstantinos P.},
title = {Feature extraction for time-series data: An artificial neural network evolutionary training model for the management of mountainous watersheds},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {73},
number = {1–3},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2008.08.024},
doi = {10.1016/j.neucom.2008.08.024},
abstract = {The present manuscript is the result of research conducted towards a wider use of artificial neural networks in the management of mountainous water supplies. The novelty lies on the evolutionary clustering of time-series data which are then used for the training and testing of a neural object, applying meta-heuristics in the neural training phase, for the management of water resources and for torrential risk estimation and modelling. It is essentially an attempt towards the development of a more credible forecasting system, exploiting an evolutionary approach used to interpret and model the significance which time-series data pose on the behavior of the aforementioned environmental reserves. The proposed model, designed such as to effectively estimate the average annual water supply for the various mountainous watersheds, accepts as inputs a wide range of meta-data produced via an evolutionary genetic process. The data used for the training and testing of the system refer to certain watersheds spread over the island of Cyprus and span a wide temporal period. The method proposed incorporates an evolutionary process to manipulate the time-series data of the average monthly rainfall recorded by the measuring stations, while the algorithm includes special encoding, initialization, performance evaluation, genetic operations and pattern matching tools for the evolution of the time-series into significantly sampled data.},
journal = {Neurocomput.},
month = dec,
pages = {49–59},
numpages = {11},
keywords = {Maximum volume of water flow, Genetic algorithms, Genetic ANN training, Evolutionary time-series processing, Average annual water supply, Artificial neural networks}
}

@inproceedings{10.1145/3395363.3397355,
author = {Liu, Hui and Shen, Mingzhu and Jin, Jiahao and Jiang, Yanjie},
title = {Automated classification of actions in bug reports of mobile apps},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397355},
doi = {10.1145/3395363.3397355},
abstract = {When users encounter problems with mobile apps, they may commit such problems to developers as bug reports. To facilitate the processing of bug reports, researchers proposed approaches to validate the reported issues automatically according to the steps to reproduce specified in bug reports. Although such approaches have achieved high success rate in reproducing the reported issues, they often rely on a predefined vocabulary to identify and classify actions in bug reports. However, such manually constructed vocabulary and classification have significant limitations. It is challenging for the vocabulary to cover all potential action words because users may describe the same action with different words. Besides that, classification of actions solely based on the action words could be inaccurate because the same action word, appearing in different contexts, may have different meaning and thus belongs to different action categories. To this end, in this paper we propose an automated approach, called MaCa, to identify and classify action words in Mobile apps’ bug reports. For a given bug report, it first identifies action words based on natural language processing. For each of the resulting action words, MaCa extracts its contexts, i.e., its enclosing segment, the associated UI target, and the type of its target element by both natural language processing and static analysis of the associated app. The action word and its contexts are then fed into a machine learning based classifier that predicts the category of the given action word in the given context. To train the classifier, we manually labelled 1,202 actions words from 525 bug reports that are associated with 207 apps. Our evaluation results on manually labelled data suggested that MaCa was accurate with high accuracy varying from 95% to 96.7%. We also investigated to what extent MaCa could further improve existing approaches (i.e., Yakusu and ReCDroid) in reproducing bug reports. Our evaluation results suggested that integrating MaCa into existing approaches significantly improved the success rates of ReCDroid and Yakusu by 22.7% = (69.2%-56.4%)/56.4% and 22.9%= (62.7%-51%)/51%, respectively.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {128–140},
numpages = {13},
keywords = {Test Case Generation, Mobile Testing, Classification, Bug report},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.5555/3291656.3291668,
author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
title = {Doomsday: predicting which node will fail when on supercomputers},
year = {2018},
publisher = {IEEE Press},
abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {9},
numpages = {14},
keywords = {machine learning, failure analysis, HPC},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1145/3377811.3380378,
author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
title = {Repairing deep neural networks: fix patterns and challenges},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380378},
doi = {10.1145/3377811.3380378},
abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1135–1146},
numpages = {12},
keywords = {bug fix, bug fix patterns, bugs, deep neural networks},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.eswa.2020.114114,
author = {Chikushi, Rohgi Toshio Meneses and Barros, Roberto Souto Maior de and Silva, Marilu Gomes N. Monte da and Maciel, Bruno Iran Ferreira},
title = {Using spectral entropy and bernoulli map to handle concept drift},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114114},
doi = {10.1016/j.eswa.2020.114114},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {19},
keywords = {Online learning, Data stream, Bernoulli map, Spectral entropy, Drift detection, Concept drift}
}

@inproceedings{10.1145/3440749.3442661,
author = {Ngoc, Hai Nguyen and Viet, Hoang Nguyen and Uehara, Tetsutaro},
title = {An Extended Benchmark System of Word Embedding Methods for Vulnerability Detection},
year = {2021},
isbn = {9781450388863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440749.3442661},
doi = {10.1145/3440749.3442661},
abstract = {Security researchers have used Natural Language Processing (NLP) and Deep Learning techniques for programming code analysis tasks such as automated bug detection and vulnerability prediction or classification. These studies mainly generate the input vectors for the deep learning models based on the NLP embedding methods. Nevertheless, while there are many existing embedding methods, the structures of neural networks are diverse and usually heuristic. This makes it difficult to select effective combinations of neural models and the embedding techniques for training the code vulnerability detectors. To address this challenge, we extended a benchmark system to analyze the compatibility of four popular word embedding techniques with four different neural networks, including the standard Bidirectional Long Short-Term Memory (Bi-LSTM), the Bi-LSTM applied attention mechanism, the Convolutional Neural Network (CNN), and the classic Deep Neural Network (DNN). We trained and tested the models by using two types of vulnerable function datasets written in C code. Our results revealed that the Bi-LSTM model combined with the FastText embedding technique showed the most efficient detection rate on a real-world but not on an artificially constructed dataset. Further comparisons with the other combinations are also discussed in detail in our result.},
booktitle = {Proceedings of the 4th International Conference on Future Networks and Distributed Systems},
articleno = {54},
numpages = {8},
keywords = {Word Embedding, Vulnerability Detection, LSTM, Deep Learning, CNN},
location = {St.Petersburg, Russian Federation},
series = {ICFNDS '20}
}

@inproceedings{10.1145/1370788.1370801,
author = {Menzies, Tim and Turhan, Burak and Bener, Ayse and Gay, Gregory and Cukic, Bojan and Jiang, Yue},
title = {Implications of ceiling effects in defect predictors},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370801},
doi = {10.1145/1370788.1370801},
abstract = {Context: There are many methods that input static code features and output a predictor for faulty code modules. These data mining methods have hit a "performance ceiling"; i.e., some inherent upper bound on the amount of information offered by, say, static code features when identifying modules which contain faults. Objective: We seek an explanation for this ceiling effect. Perhaps static code features have "limited information content"; i.e. their information can be quickly and completely discovered by even simple learners. Method:An initial literature review documents the ceiling effect in other work. Next, using three sub-sampling techniques (under-, over-, and micro-sampling), we look for the lower useful bound on the number of training instances. Results: Using micro-sampling, we find that as few as 50 instances yield as much information as larger training sets. Conclusions: We have found much evidence for the limited information hypothesis. Further progress in learning defect predictors may not come from better algorithms. Rather, we need to be improving the information content of the training data, perhaps with case-based reasoning methods.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {47–54},
numpages = {8},
keywords = {defect prediction, naive bayes, over-sampling, under-sampling},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@inproceedings{10.1109/SC.2018.00012,
author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
title = {Doomsday: predicting which node will fail when on supercomputers},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00012},
doi = {10.1109/SC.2018.00012},
abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {9},
numpages = {14},
keywords = {machine learning, failure analysis, HPC},
location = {Dallas, Texas},
series = {SC '18}
}

@article{10.1016/j.aei.2016.07.001,
author = {Bilal, Muhammad and Oyedele, Lukumon O. and Qadir, Junaid and Munir, Kamran and Ajayi, Saheed O. and Akinade, Olugbenga O. and Owolabi, Hakeem A. and Alaka, Hafiz A. and Pasha, Maruf},
title = {Big Data in the construction industry},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {30},
number = {3},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2016.07.001},
doi = {10.1016/j.aei.2016.07.001},
abstract = {Existing works for Big Data Analytics/Engineering in the construction industry are discussed.It is highlighted that the adoption of Big Data is still at nascent stageOpportunities to employ Big Data technologies in construction sub-domains are highlighted.Future works for Big Data technologies are presented.Pitfalls of Big Data technologies in the construction industry are also pointed out. The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon-dubbed as Big Data-has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.},
journal = {Adv. Eng. Inform.},
month = aug,
pages = {500–521},
numpages = {22},
keywords = {Machine learning, Construction industry, Big Data Engineering, Big Data Analytics}
}

@inproceedings{10.5555/2486788.2486808,
author = {Cotroneo, Domenico and Pietrantuono, Roberto and Russo, Stefano},
title = {A learning-based method for combining testing techniques},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {This work presents a method to combine testing techniques adaptively during the testing process. It intends to mitigate the sources of uncertainty of software testing processes, by learning from past experience and, at the same time, adapting the technique selection to the current testing session. The method is based on machine learning strategies. It uses offline strategies to take historical information into account about the techniques performance collected in past testing sessions; then, online strategies are used to adapt the selection of test cases to the data observed as the testing proceeds. Experimental results show that techniques performance can be accurately characterized from features of the past testing sessions, by means of machine learning algorithms, and that integrating this result into the online algorithm allows improving the fault detection effectiveness with respect to single testing techniques, as well as to their random combination.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {142–151},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3379597.3387458,
author = {Mattis, Toni and Rein, Patrick and D\"{u}rsch, Falco and Hirschfeld, Robert},
title = {RTPTorrent: An Open-source Dataset for Evaluating Regression Test Prioritization},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387458},
doi = {10.1145/3379597.3387458},
abstract = {The software engineering practice of automated testing helps programmers find defects earlier during development. With growing software projects and longer-running test suites, frequency and immediacy of feedback decline, thereby making defects harder to repair. Regression test prioritization (RTP) is concerned with running relevant tests earlier to lower the costs of defect localization and to improve feedback.Finding representative data to evaluate RTP techniques is non-trivial, as most software is published without failing tests. In this work, we systematically survey a wide range of RTP literature regarding whether their dataset uses real or synthetic defects or tests, whether they are publicly available, and whether datasets are reused. We observed that some datasets are reused, however, many projects study only few projects and these rarely resemble real-world development activity.In light of these threats to ecological validity, we describe the construction and characteristics of a new dataset, named RTPTorrent, based on 20 open-source Java programs.Our dataset allows researchers to evaluate prioritization heuristics based on version control meta-data, source code, and test results from fine-grained, automated builds over 9 years of development history. We provide reproducible baselines for initial comparisons and make all data publicly available.We see this as a step towards better reproducibility, ecological validity, and long-term availability of studied software in the field of test prioritization.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {385–396},
numpages = {12},
keywords = {TravisCI, Regression Test Prioritization, Java, GitHub, Dataset},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1007/s11219-010-9112-9,
author = {Bak\i{}r, Ay\c{s}e and Turhan, Burak and Bener, Ay\c{s}e},
title = {A comparative study for estimating software development effort intervals},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9112-9},
doi = {10.1007/s11219-010-9112-9},
abstract = {Software cost/effort estimation is still an open challenge. Many researchers have proposed various methods that usually focus on point estimates. Until today, software cost estimation has been treated as a regression problem. However, in order to prevent overestimates and underestimates, it is more practical to predict the interval of estimations instead of the exact values. In this paper, we propose an approach that converts cost estimation into a classification problem and that classifies new software projects in one of the effort classes, each of which corresponds to an effort interval. Our approach integrates cluster analysis with classification methods. Cluster analysis is used to determine effort intervals while different classification algorithms are used to find corresponding effort classes. The proposed approach is applied to seven public datasets. Our experimental results show that the hit rate obtained for effort estimation are around 90---100%, which is much higher than that obtained by related studies. Furthermore, in terms of point estimation, our results are comparable to those in the literature although a simple mean/median is used for estimation. Finally, the dynamic generation of effort intervals is the most distinctive part of our study, and it results in time and effort gain for project managers through the removal of human intervention.},
journal = {Software Quality Journal},
month = sep,
pages = {537–552},
numpages = {16},
keywords = {Software effort estimation, Machine learning, Interval prediction, Cluster analysis, Classification}
}

@article{10.1007/s10515-021-00297-8,
author = {Chouchane, Mabrouka and Soui, Makram and Ghedira, Khaled},
title = {The impact of the code smells of the presentation layer on the diffuseness of aesthetic defects of Android apps},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00297-8},
doi = {10.1007/s10515-021-00297-8},
abstract = {Recently, the number of Android apps has witnessed an ever-increase that is becoming a ubiquitous presence in our daily lives. These apps are evolving fast by offering new characteristics and functionalities. These ongoing improvements often affect app quality due to bad design practices and poor coding, known as Android code smells. In this context, the recent works highlighted the importance of the design quality of mobile application. To this end, many methods and tools are proposed to assess the quality of graphical user interface (GUI) and source code of Android apps, such as heuristic evaluation and field-testing, etc. In addition, the features and design of these Android apps may introduce bad design practices, that can highly decrease the quality and the performance of these Android applications. In this paper, we empirically study the diffuseness of GUI aesthetic defects and the code smells of the presentation layer of Android apps. Then, we investigate the impact of the appearance of code smells on the aesthetic of Android apps. To this end, we use two evaluation tools. The first one is called PLAIN which consists of detecting aesthetic defects by measuring a set of structural metrics of GUI. The second one is Android UI Detector which aims to identify the presentation layer code smells of Android apps. This analysis study is based on 8480 GUIs of 120 Android apps. The obtained results confirm that code smells of the presentation layer of Android apps have an impact on GUI aesthetic defects.},
journal = {Automated Software Engg.},
month = nov,
numpages = {29},
keywords = {Android apps, Presentation layer, Aesthetic metrics, Graphical android user interface}
}

@article{10.1007/s11219-011-9132-0,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Predicting high-risk program modules by selecting the right software measurements},
year = {2012},
issue_date = {March     2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9132-0},
doi = {10.1007/s11219-011-9132-0},
abstract = {A timely detection of high-risk program modules in high-assurance software is critical for avoiding the high consequences of operational failures. While software risk can initiate from external sources, such as management or outsourcing, software quality is adversely affected when internal software risks are realized, such as improper practice of standard software processes or lack of a defined software quality infrastructure. Practitioners employ various techniques to identify and rectify high-risk or low-quality program modules. Effectiveness of detecting such modules is affected by the software measurements used, making feature selection an important step during software quality prediction. We use a wrapper-based feature ranking technique to select the optimal set of software metrics to build defect prediction models. We also address the adverse effects of class imbalance (very few low-quality modules compared to high-quality modules), a practical problem observed in high-assurance systems. Applying a data sampling technique followed by feature selection is a relatively unique contribution of our work. We present a comprehensive investigation on the impact of data sampling followed by attribute selection on the defect predictors built with imbalanced data. The case study data are obtained from several real-world high-assurance software projects. The key results are that attribute selection is more efficient when applied after data sampling, and defect prediction performance generally improves after applying data sampling and feature selection.},
journal = {Software Quality Journal},
month = mar,
pages = {3–42},
numpages = {40},
keywords = {Wrapper-based feature ranking, Software quality classification, Performance metrics, Imbalanced data, Feature selection, Data sampling}
}

@article{10.1016/j.infsof.2021.106686,
author = {Liu, Shiran and Guo, Zhaoqiang and Li, Yanhui and Lu, Hongmin and Chen, Lin and Xu, Lei and Zhou, Yuming and Xu, Baowen},
title = {Prioritizing code documentation effort: Can we do it simpler but better?},
year = {2021},
issue_date = {Dec 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {140},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106686},
doi = {10.1016/j.infsof.2021.106686},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {16},
keywords = {Metrics, PageRank, Program comprehension, Code documentation}
}

@inproceedings{10.5555/2664446.2664480,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
title = {Can we predict types of code changes? an empirical analysis},
year = {2012},
isbn = {9781467317610},
publisher = {IEEE Press},
abstract = {There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
pages = {217–226},
numpages = {10},
keywords = {software quality, software maintenance, machine learning},
location = {Zurich, Switzerland},
series = {MSR '12}
}

@inproceedings{10.1007/978-3-030-81682-7_9,
author = {Rosenbauer, Lukas and P\"{a}tzel, David and Stein, Anthony and H\"{a}hner, J\"{o}rg},
title = {An Organic Computing System for&nbsp;Automated Testing},
year = {2021},
isbn = {978-3-030-81681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-81682-7_9},
doi = {10.1007/978-3-030-81682-7_9},
abstract = {Testing is a vital part of the development of a new software product. With the rise of test automation, companies more and more rely on large sets of test cases. This leads to situations in which it is unfeasible to run all tests due to a limited time budget which eventually results in the need for selecting an optimal subset of tests to execute. Recently, this test selection problem has been approached using machine learning methods. In this work, we design an Organic Computing (OC) system which makes use of these methods. While OC design techniques have originally been targeted at creating embedded systems, we show that these methodologies can be employed to software verification as well. We are able to demonstrate that the implemented system is a robust and highly autonomous solution which fits modern development practices such as continuous integration well.},
booktitle = {Architecture of Computing Systems: 34th International Conference, ARCS 2021, Virtual Event, June 7–8, 2021, Proceedings},
pages = {135–149},
numpages = {15},
keywords = {Organic computing, Continuous integration, Testing, System architecture}
}

@article{10.1016/j.eswa.2020.113808,
author = {Mohsin, Hufsa and Shi, Chongyang},
title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113808},
doi = {10.1016/j.eswa.2020.113808},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {15},
keywords = {Bug classification, Bug report analysis, Self-paced learning, Defect localization, Bug triaging}
}

@inproceedings{10.1145/3427228.3427269,
author = {Das, Sanjeev and James, Kedrian and Werner, Jan and Antonakakis, Manos and Polychronakis, Michalis and Monrose, Fabian},
title = {A Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427269},
doi = {10.1145/3427228.3427269},
abstract = {Among various fuzzing approaches, coverage-guided grey-box fuzzing is perhaps the most prominent, due to its ease of use and effectiveness. Using this approach, the selection of inputs focuses on maximizing program coverage, e.g., in terms of the different branches that have been traversed. In this work, we begin with the observation that selecting any input that explores a new path, and giving equal weight to all paths, can lead to severe inefficiencies. For instance, although seemingly “new” crashes involving previously unexplored paths may be discovered, these often have the same root cause and actually correspond to the same bug. To address these inefficiencies, we introduce a framework that incorporates a tighter feedback loop to guide the fuzzing process in exploring truly diverse code paths. Our framework employs (i) a vulnerability-aware selection of coverage metrics for enhancing the effectiveness of code exploration, (ii) crash deduplication information for early feedback, and (iii) a configurable input culling strategy that interleaves multiple strategies to achieve comprehensiveness. A novel aspect of our work is the use of hardware performance counters to derive coverage metrics. We present an approach for assessing and selecting the hardware events that can be used as a meaningful coverage metric for a target program. The results of our empirical evaluation using real-world programs demonstrate the effectiveness of our approach: in some cases, we explore fewer than 50% of the paths compared to a base fuzzer (AFL, MOpt, and Fairfuzz), yet on average, we improve new bug discovery by 31%, and find the same bugs (as the base) 3.3 times faster. Moreover, although we specifically chose applications that have been subject to recent fuzzing campaigns, we still discovered 9 new vulnerabilities.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {345–359},
numpages = {15},
keywords = {Machine Learning, Hardware Performance Counters, Fuzzing},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/3464298.3493400,
author = {Tak, Byungchul and Han, Wook-Shin},
title = {Lognroll: discovering accurate log templates by iterative filtering},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3493400},
doi = {10.1145/3464298.3493400},
abstract = {Modern IT systems rely heavily on log analytics for critical operational tasks. Since the volume of logs produced from numerous distributed components is overwhelming, it requires us to employ automated processing. The first step of automated log processing is to convert streams of log lines into the sequence of log format IDs, called log templates. A log template serves as a base string with unfilled parts from which logs are generated during runtime by substitution of contextual information. The problem of log template discovery from the volume of collected logs poses a great challenge due to the semi-structured nature of the logs and the computational overheads. Our investigation reveals that existing techniques show various limitations. We approach the log template discovery problem as search-based learning by applying the ILP (Inductive Logic Programming) framework. The algorithm core consists of narrowing down the logs into smaller sets by analyzing value compositions on selected log column positions. Our evaluation shows that it produces accurate log templates from diverse application logs with small computational costs compared to existing methods. With the quality metric we defined, we obtained about 21%-51% improvements of log template quality.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {273–285},
numpages = {13},
keywords = {sequential covering, log template, log analysis},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@article{10.1007/s10922-013-9289-x,
author = {Bashar, Abul and Parr, Gerard and Mcclean, Sally and Scotney, Bryan and Nauck, Detlef},
title = {Application of Bayesian Networks for Autonomic Network Management},
year = {2014},
issue_date = {April     2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9289-x},
doi = {10.1007/s10922-013-9289-x},
abstract = {The ever evolving telecommunication networks in terms of their technology, infrastructure, and supported services have always posed challenges to the network managers to come up with an efficient Network Management System (NMS) for effective network management. The need for automated and efficient management of the current networks, more specifically the Next Generation Network (NGN), is the subject addressed in this research. A detailed description of the management challenges in the context of current networks is presented and then this work enlists the desired features and characteristics of an efficient NMS. It then proposes that there is a need to apply Artificial Intelligence (AI) and Machine Learning (ML) approaches for enhancing and automating the functions of NMS. The first contribution of this work is a comprehensive survey of the AI and ML approaches applied to the domain of NM. The second contribution of this work is that it presents the reasoning and evidence to support the choice of Bayesian Networks (BN) as a viable solution for ML-based NMS. The final contribution of this work is that it proposes and implements three novel NM solutions based on the BN approach, namely BN-based Admission Control (BNAC), BN-based Distributed Admission Control (BNDAC) and BN-based Intelligent Traffic Engineering (BNITE), along with the description of algorithms underpinning the proposed framework.},
journal = {J. Netw. Syst. Manage.},
month = apr,
pages = {174–207},
numpages = {34},
keywords = {Next Generation Networks, Network Management, Machine Learning, Intelligent Traffic Engineering, Data Mining, Call Admission Control, Bayesian Networks, Artificial Intelligence}
}

@article{10.1016/j.neunet.2019.12.001,
author = {Gautam, Chandan and Mishra, Pratik K. and Tiwari, Aruna and Richhariya, Bharat and Pandey, Hari Mohan and Wang, Shuihua and Tanveer, M.},
title = {Minimum variance-embedded deep kernel regularized least squares method for one-class classification and its applications to biomedical data},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {123},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.12.001},
doi = {10.1016/j.neunet.2019.12.001},
journal = {Neural Netw.},
month = mar,
pages = {191–216},
numpages = {26},
keywords = {Breast cancer, Magnetic resonance imaging, Alzheimer’s disease, Outlier detection, Kernel learning, One-class classification}
}

@inproceedings{10.1145/2501221.2501228,
author = {Zulkernine, Farhana and Martin, Patrick and Powley, Wendy and Soltani, Sima and Mankovskii, Serge and Addleman, Mark},
title = {CAPRI: a tool for mining complex line patterns in large log data},
year = {2013},
isbn = {9781450323246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2501221.2501228},
doi = {10.1145/2501221.2501228},
abstract = {Log files provide important information for troubleshooting complex systems. However, the structure and contents of the log data and messages vary widely. For automated processing, it is necessary to first understand the layout and the structure of the data, which becomes very challenging when a massive amount of data and messages are reported by different system components in the same log file. Existing approaches apply supervised mining techniques and return frequent patterns only for single line messages. We present CAPRI (type-CAsted Pattern and Rule mIner), which uses a novel pattern mining algorithm to efficiently mine structural line patterns from semi-structured multi-line log messages. It discovers line patterns in a type-casted format; categorizes all data lines; identifies frequent, rare and interesting line patterns, and uses unsupervised learning and incremental mining techniques. It also mines association rules to identify the contextual relationship between two successive line patterns. In addition, CAPRI lists the frequent term and value patterns given the minimum support thresholds. The line and term pattern information can be applied in the next stage to categorize and reformat multi-line data, extract variables from the messages, and discover further correlation among messages for troubleshooting complex systems. To evaluate our approach, we present a comparative study of our tool against some of the existing popular open-source research tools using three different layouts of log data including a complex multi-line log file from the z/OS mainframe system.},
booktitle = {Proceedings of the 2nd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {47–54},
numpages = {8},
keywords = {value pattern mining, type-casting, term, line, association rule},
location = {Chicago, Illinois},
series = {BigMine '13}
}

@article{10.1109/TSE.2018.2816639,
author = {Bian, Pan and Liang, Bin and Zhang, Yan and Yang, Chaoqun and Shi, Wenchang and Cai, Yan},
title = {Detecting Bugs by Discovering Expectations and Their Violations},
year = {2019},
issue_date = {Oct. 2019},
publisher = {IEEE Press},
volume = {45},
number = {10},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2018.2816639},
doi = {10.1109/TSE.2018.2816639},
abstract = {Code mining has been proven to be a promising approach to inferring implicit programming rules for finding software bugs. However, existing methods may report large numbers of false positives and false negatives. In this paper, we propose a novel approach called EAntMiner to improve the effectiveness of code mining. EAntMiner elaborately reduces noises from statements irrelevant to interesting rules and different implementation forms of the same logic. During preprocessing, we employ program slicing to decompose the original source repository into independent sub-repositories. In each sub-repository, statements irrelevant to critical operations (automatically extracted from source code) are excluded and various semantics-equivalent implementations are normalized into a canonical form as far as possible. Moreover, to tackle the challenge that some bugs are difficult to be detected by mining frequent patterns as rules, we further developed a kNN-based method to identify them. We have implemented EAntMiner and evaluated it on four large-scale C systems. EAntMiner successfully detected 105 previously unknown bugs that have been confirmed by corresponding development communities. A set of comparative evaluations also demonstrate that EAntMiner can effectively improve the precision of code mining.},
journal = {IEEE Trans. Softw. Eng.},
month = oct,
pages = {984–1001},
numpages = {18}
}

@inproceedings{10.1145/3345629.3345635,
author = {Wang, Song and Bansal, Chetan and Nagappan, Nachiappan and Philip, Adithya Abraham},
title = {Leveraging Change Intents for Characterizing and Identifying Large-Review-Effort Changes},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345635},
doi = {10.1145/3345629.3345635},
abstract = {Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. In most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis.In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes regarding review effort---changes with large review effort. Specifically, we first propose a feedback-driven and heuristics-based approach to obtain change intents. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on a large-scale project from Microsoft and three large-scale open source projects, i.e., Qt, Android, and OpenStack. Our results show that, (i) code changes with some intents are more likely to be LRE changes, (ii) machine learning based prediction models can efficiently help identify LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. The tool developed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {46–55},
numpages = {10},
keywords = {Code review, change intent, machine learning, review effort},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@article{10.1007/s11761-020-00312-y,
author = {Nooraei Abadeh, Maryam},
title = {Genetic-based web regression testing: an ontology-based multi-objective evolutionary framework to auto-regression testing of web applications},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {1},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-020-00312-y},
doi = {10.1007/s11761-020-00312-y},
abstract = {Regression testing is one of the most critical activities in the software maintenance process and its importance is twofold for evolutionary applications, e.g., modern flexible web-based applications. By increasing the complexity of application due to the rapid change, automatic evolutionary testing approaches are essential to find solutions providing different trade-offs between testing objectives by applying evolutionary computation. This paper proposes a model-based regression test case generation framework, as an optimization solution, by impressively taking the advantages of the genetic algorithms (GAs), called genetic-based web regression testing (GbWRT). The aim of the paper is twofold. Firstly, a meta-ontology has been designed based on an in-deep assessment to capture testing challenges caused by the inherent dynamic properties of web applications. Secondly, the multi-objective fitness functions of GbWRT are defined built on top of the meta-ontology in terms of the most important meta-model features. GbWRT minimizes exploration and exploitation in regression testing using an incremental change adaption technique implemented in the proposed GA. This approach allows a new incremental regression testing strategy to solve fault detection effectiveness and the coverability problems which are extendable to different domain-specific modeling environments. GbWRT is evaluated using the proposed fitness functions on two experimental case studies. Also, the results of comparison with three non-evolutionary and evolutionary regression testing methods indicate that the GbWRT is competitive with the state of the art regarding solution quality to perform in web-based non-stationary environments.},
journal = {Serv. Oriented Comput. Appl.},
month = mar,
pages = {55–74},
numpages = {20},
keywords = {Coverage criteria, Automatic regression testing, Ontology, Web-based application, Genetic algorithm}
}

@article{10.1007/s40595-017-0100-x,
author = {\"{O}zt\"{u}rk, Muhammed Maruf},
title = {A bat-inspired algorithm for prioritizing test cases},
year = {2018},
issue_date = {February  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {5},
number = {1},
issn = {2196-8888},
url = {https://doi.org/10.1007/s40595-017-0100-x},
doi = {10.1007/s40595-017-0100-x},
abstract = {By ordering test cases, early fault detection is focused on test case prioritization. In this field, it is widely known that algorithm and coverage criteria focused works are common. Previous works, which are related to test case prioritization, showed that practitioners need a novel method that optimizes test cases according to the cost of each test case instead of regarding the total cost of a test suite. In this work, by utilizing local and global search properties of a bat algorithm, a new bat-inspired test cases prioritization algorithm (BITCP) is proposed. In order to develop BITCP, test case execution time and the number of faults were adapted to the distance from the prey and loudness, respectively. The proposed method is then compared with four methods which are commonly used in this field. According to the results of the experiment, BITCP is superior to the conventional methods. In addition, as the complexity of the code of test cases increases, the decline in average percentage of fault detection is less in BITCP than the other four comparison algorithms produced.},
journal = {Vietnam J. of Computer Science},
month = feb,
pages = {45–57},
numpages = {13},
keywords = {Test case prioritization, Regression testing, Bat-inspired algorithm}
}

@inproceedings{10.1145/2642937.2653469,
author = {Borg, Markus},
title = {Embrace your issues: compassing the software engineering landscape using bug reports},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2653469},
doi = {10.1145/2642937.2653469},
abstract = {Software developers in large projects work in complex information landscapes, and staying on top of all relevant software artifacts is challenging. As software systems often evolve for years, a high number of issue reports is typically managed during the lifetime of a system. Efficient management of incoming issue requires successful navigation of the information landscape. In our work, we address two important work tasks involved in issue management: Issue Assignment (IA) and Change Impact Analysis (CIA). IA is the early task of allocating an issue report to a development team. CIA deals with identifying how source code changes affect the software system, a fundamental activity in safety-critical development. Our solution approach is to support navigation, both among development teams and software artifacts, based on information available in historical issue reports. We present how we apply techniques from machine learning and information retrieval to develop recommendation systems. Finally, we report intermediate results from two controlled experiments and an industrial case study.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {891–894},
numpages = {4},
keywords = {recommendation systems, machine learning, issue management, information retrieval},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1016/j.jss.2019.07.016,
author = {Grano, Giovanni and Palomba, Fabio and Di Nucci, Dario and De Lucia, Andrea and Gall, Harald C.},
title = {Scented since the beginning: On the diffuseness of test smells in automatically generated test code},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.07.016},
doi = {10.1016/j.jss.2019.07.016},
journal = {J. Syst. Softw.},
month = oct,
pages = {312–327},
numpages = {16},
keywords = {Empirical studies, Software quality, Test case generation, Test smells}
}

@inproceedings{10.1007/978-3-030-64881-7_17,
author = {Altiero, Francesco and Corazza, Anna and Di Martino, Sergio and Peron, Adriano and Starace, Luigi Libero Lucio},
title = {Inspecting Code Churns to Prioritize Test&nbsp;Cases},
year = {2020},
isbn = {978-3-030-64880-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64881-7_17},
doi = {10.1007/978-3-030-64881-7_17},
abstract = {Within the context of software evolution, due to time-to-market pressure, it is not uncommon that a company has not enough time and/or resources to re-execute the whole test suite on the new software version, to check for non-regression. To face this issue, many Regression Test Prioritization techniques have been proposed, aimed at ranking test cases in a way that tests more likely to expose faults have higher priority. Some of these techniques exploit code churn metrics, i.e. some quantification of code changes between two subsequent versions of a software artifact, which have been proven to be effective indicators of defect-prone components. In this paper, we first present three new Regression Test Prioritization strategies, based on a novel code churn metric, that we empirically assessed on an open source software system. Results highlighted that the proposal is promising, but that it might be further improved by a more detailed analysis on the nature of the changes introduced between two subsequent code versions. To this aim, in this paper we also sketch a more refined approach we are currently investigating, that quantifies changes in a code base at a finer grained level. Intuitively, we seek to prioritize tests that stress more fault-prone changes (e.g., structural changes in the control flow), w.r.t. those that are less likely to introduce errors (e.g., the renaming of a variable). To do so, we propose the exploitation of the Abstract Syntax Tree (AST) representation of source code, and to quantify differences between ASTs by means of specifically designed Tree Kernel functions, a type of similarity measure for tree-based data structures, which have shown to be very effective in other domains, thanks to their customizability.},
booktitle = {Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9–11, 2020, Proceedings},
pages = {272–285},
numpages = {14},
keywords = {Code churn., Test prioritization, Regression testing},
location = {Naples, Italy}
}

@inproceedings{10.1109/ICSE43902.2021.00032,
author = {Dola, Swaroopa and Dwyer, Matthew B. and Soffa, Mary Lou},
title = {Distribution-Aware Testing of Neural Networks Using Generative Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00032},
doi = {10.1109/ICSE43902.2021.00032},
abstract = {The reliability of software that has a Deep Neural Network (DNN) as a component is urgently important today given the increasing number of critical applications being deployed with DNNs. The need for reliability raises a need for rigorous testing of the safety and trustworthiness of these systems. In the last few years, there have been a number of research efforts focused on testing DNNs. However the test generation techniques proposed so far lack a check to determine whether the test inputs they are generating are valid, and thus invalid inputs are produced. To illustrate this situation, we explored three recent DNN testing techniques. Using deep generative model based input validation, we show that all the three techniques generate significant number of invalid test inputs. We further analyzed the test coverage achieved by the test inputs generated by the DNN testing techniques and showed how invalid test inputs can falsely inflate test coverage metrics.To overcome the inclusion of invalid inputs in testing, we propose a technique to incorporate the valid input space of the DNN model under test in the test generation process. Our technique uses a deep generative model-based algorithm to generate only valid inputs. Results of our empirical studies show that our technique is effective in eliminating invalid tests and boosting the number of valid test inputs generated.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {226–237},
numpages = {12},
keywords = {test generation, test coverage, input validation, deep neural networks, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/COMPSAC.2014.10,
author = {Liu, Huqiu and Wang, Yuping and Jiang, Lingbo and Hu, Shimin},
title = {PF-Miner: A New Paired Functions Mining Method for Android Kernel in Error Paths},
year = {2014},
isbn = {9781479935758},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2014.10},
doi = {10.1109/COMPSAC.2014.10},
abstract = {Drivers are significant components of the operating systems(OSs), and they run in kernel mode. Generally, drivers have many errors to handle, and the functions called in the normal execution paths and error handling paths are in pairs, which are named as paired functions. However, some developers do not handle the errors completely as they forget about or are unaware of releasing the acquired resources, thus memory leaks and other potential problems can be easily introduced. Therefore, it is highly valuable to automatically extract paired functions for these problems and detect violations for the programmers. This paper proposes an efficient tool named PF-Miner, which can automatically extract paired functions and detect violations between normal execution paths and error handling paths from the source code of C program with the data mining and statistical methods. We have evaluated PF-Miner on different versions of Android kernel 2.6.39 and 3.10.0, and 81 bugs reported by PF-Miner in 2.6.39 have been fixed before the latest version 3.10.0. PF-Miner only needs about 150 seconds to analyze the source code of 3.10.0, and 983 violations have been detected from 546 paired functions that have been extracted. We have reported the top 51 violations as potential bugs to the developers, and 15 bugs have been confirmed.},
booktitle = {Proceedings of the 2014 IEEE 38th Annual Computer Software and Applications Conference},
pages = {33–42},
numpages = {10},
keywords = {Paired function mining, data mining, violations detection, error path checking},
series = {COMPSAC '14}
}

@inproceedings{10.1145/2897695.2897696,
author = {Ortu, Marco and Destefanis, Giuseppe and Swift, Stephen and Marchesi, Michele},
title = {Measuring high and low priority defects on traditional and mobile open source software},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897696},
doi = {10.1145/2897695.2897696},
abstract = {Software defects are the major cause for system failures. To effectively design tools and provide support for detecting and recovering from software failures, requires a deep understanding of defect features. In this paper we present an analysis of defect characteristics in two different open source software development domains: Mobile and Traditional. Our attention is focused on measuring the differences between High-Priority and Low-Priority defects. High or Low priority of a given defect is decided by a developer when creating a bug report for an issue tracking system. We sampled hundreds of real world bugs in hundreds of large and representative open-source projects. We used natural language text classification techniques to automatically analyse roughly 700,000 bug reports from the Bugzilla, Jira and Google Issues issue tracking systems. Results show that there are differences between High-Priority and Low-Priority defects classification in Mobile and Traditional development domains.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {1–7},
numpages = {7},
keywords = {data mining, bug reports, bug categorisation},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1109/ICSE43902.2021.00034,
author = {Wardat, Mohammad and Le, Wei and Rajan, Hridesh},
title = {DeepLocalize: Fault Localization for Deep Neural Networks},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00034},
doi = {10.1109/ICSE43902.2021.00034},
abstract = {Deep neural networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques don't support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach and a tool that automatically determines whether the model is buggy or not, and identifies the root causes for DNN errors. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and also localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer/parameter on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state-of-the-practice Keras library. For 34/40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32/40 faults. Our approach was able to localize 21/40 bugs whereas Keras did not localize any faults.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {251–262},
numpages = {12},
keywords = {Program Analysis, Fault Location, Deep learning bugs, Deep Neural Networks, Debugging},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s11219-018-9439-1,
author = {K\i{}ra\c{c}, M. Furkan and Aktemur, Bar\i{}\c{s} and S\"{o}zer, Hasan and Gebizli, Ceren \c{S}ahin},
title = {Automatically learning usage behavior and generating event sequences for black-box testing of reactive systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9439-1},
doi = {10.1007/s11219-018-9439-1},
abstract = {We propose a novel technique based on recurrent artificial neural networks to generate test cases for black-box testing of reactive systems. We combine functional testing inputs that are automatically generated from a model together with manually-applied test cases for robustness testing. We use this combination to train a long short-term memory (LSTM) network. As a result, the network learns an implicit representation of the usage behavior that is liable to failures. We use this network to generate new event sequences as test cases. We applied our approach in the context of an industrial case study for the black-box testing of a digital TV system. LSTM-generated test cases were able to reveal several faults, including critical ones, that were not detected with existing automated or manual testing activities. Our approach is complementary to model-based and exploratory testing, and the combined approach outperforms random testing in terms of both fault coverage and execution time.},
journal = {Software Quality Journal},
month = jun,
pages = {861–883},
numpages = {23},
keywords = {Test case generation, Recurrent neural networks, Long short-term memory networks, Learning usage behavior, Black-box testing}
}

@article{10.1007/s10664-017-9567-4,
author = {Wu, Rongxin and Wen, Ming and Cheung, Shing-Chi and Zhang, Hongyu},
title = {ChangeLocator: locate crash-inducing changes based on crash reports},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9567-4},
doi = {10.1007/s10664-017-9567-4},
abstract = {Software crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crash-inducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2866–2900},
numpages = {35},
keywords = {Software crash, Crash-inducing change, Crash stack, Bug localization}
}

@article{10.1016/j.micpro.2015.09.008,
author = {Beg, Azam and Awwad, Falah and Ibrahim, Walid and Ahmed, Faheem},
title = {On the reliability estimation of nano-circuits using neural networks},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {39},
number = {8},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2015.09.008},
doi = {10.1016/j.micpro.2015.09.008},
abstract = {As the integrated circuit geometries shrink, it becomes important for the designers to take into consideration the reliability of the circuits. Different techniques can be used for reliability calculation or estimation. Some of these techniques are accurate but time-consuming while others are quick but not accurate. For example, using a set of mathematical equations for reliability estimation is very fast but not precise enough for large systems. Alternatively, Monte Carlo simulations are highly accurate, but very time-intensive. This work presents three different neural network models for estimating circuit reliability. The models provide better prediction accuracies than the mathematical technique. A reasonably large number of combinational circuits were simulated over a wide range of device reliabilities to collect the training data for the models. Multiple slices of an ISCAS-85 benchmark circuit were used to validate the models' prediction results.},
journal = {Microprocess. Microsyst.},
month = nov,
pages = {674–685},
numpages = {12},
keywords = {Reliability, Neural network, Nano-electronics, Modeling, Digital circuit}
}

@inproceedings{10.1145/3106237.3106284,
author = {Murali, Vijayaraghavan and Chaudhuri, Swarat and Jermaine, Chris},
title = {Bayesian specification learning for finding API usage errors},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106284},
doi = {10.1145/3106237.3106284},
abstract = {We present a Bayesian framework for learning probabilistic specifications from large, unstructured code corpora, and then using these specifications to statically detect anomalous, hence likely buggy, program behavior. Our key insight is to build a statistical model that correlates all specifications hidden inside a corpus with the syntax and observed behavior of programs that implement these specifications. During the analysis of a particular program, this model is conditioned into a posterior distribution that prioritizes specifications that are relevant to the program. The problem of finding anomalies is now framed quantitatively, as a problem of computing a distance between a "reference distribution" over program behaviors that our model expects from the program, and the distribution over behaviors that the program actually produces.  We implement our ideas in a system, called Salento, for finding anomalous API usage in Android programs. Salento learns specifications using a combination of a topic model and a neural network model. Our encouraging experimental results show that the system can automatically discover subtle errors in Android applications in the wild, and has high precision and recall compared to competing probabilistic approaches.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {151–162},
numpages = {12},
keywords = {Specification Learning, Bug Finding, Anomaly Detection, APIs},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1109/ASE.2019.00127,
author = {Xie, Xiaofei and Chen, Hongxu and Li, Yi and Ma, Lei and Liu, Yang and Zhao, Jianjun},
title = {Coverage-guided fuzzing for feedforward neural networks},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00127},
doi = {10.1109/ASE.2019.00127},
abstract = {Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1162–1165},
numpages = {4},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1007/s10664-019-09686-w,
author = {Minku, Leandro L.},
title = {A novel online supervised hyperparameter tuning procedure applied to cross-company software effort estimation},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09686-w},
doi = {10.1007/s10664-019-09686-w},
abstract = {Software effort estimation is an online supervised learning problem, where new training projects may become available over time. In this scenario, the Cross-Company (CC) approach Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving their collection cost. However, Dycom requires CC projects to be split into subsets. Both the number and composition of such subsets can affect Dycom’s predictive performance. Even though clustering methods could be used to automatically create CC subsets, there are no procedures for automatically tuning the number of clusters over time in online supervised scenarios. This paper proposes the first procedure for that. An investigation of Dycom using six clustering methods and three automated tuning procedures is performed, to check whether clustering with automated tuning can create well performing CC splits. A case study with the ISBSG Repository shows that the proposed tuning procedure in combination with a simple threshold-based clustering method is the most successful in enabling Dycom to drastically reduce (by a factor of 10) the number of required WC training projects, while maintaining (or even improving) predictive performance in comparison with a corresponding WC model. A detailed analysis is provided to understand the conditions under which this approach does or does not work well. Overall, the proposed online supervised tuning procedure was generally successful in enabling a very simple threshold-based clustering approach to obtain the most competitive Dycom results. This demonstrates the value of automatically tuning hyperparameters over time in a supervised way.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {3153–3204},
numpages = {52},
keywords = {Hyperparameter tuning, Online learning, Concept drift, Transfer learning, Cross-company learning, Software effort estimation}
}

@inproceedings{10.1145/3242887.3242889,
author = {Zaman, Tarannum Shaila and Yu, Tingting},
title = {Extracting implicit programming rules: comparing static and dynamic approaches},
year = {2018},
isbn = {9781450359757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242887.3242889},
doi = {10.1145/3242887.3242889},
abstract = {Programs often follow implicit programming rules, such as, function call A must be followed by function call B. Rules of such kinds are rarely documented by developers. Nevertheless, programming rules play an important role in software testing and maintenance. For example, the rules can be used as test oracles to detect violations. If a programmer can be notified of these rules before updating the source code, the chances of generating defects due to rule violations might be minimized. Prior works have used static and dynamic analysis techniques to extract implicit programming rules, but none compares the effectiveness of the two techniques. In this paper, we have undertaken an empirical study to compare the two techniques when they are being used for extracting programming rules. Our results indicate that the performance of the dynamic analysis technique depends on the number and the diversity of the traces. Moreover, the dynamic analysis technique generates more precise rules than the static analysis technique if a diverse and sufficient number of test cases are provided.},
booktitle = {Proceedings of the 7th International Workshop on Software Mining},
pages = {1–7},
numpages = {7},
keywords = {Static Analysis, Implicit Programming Rules, Empirical study, Dynamic analysis},
location = {Montpellier, France},
series = {SoftwareMining 2018}
}

@article{10.1145/3476105,
author = {Parry, Owain and Kapfhammer, Gregory M. and Hilton, Michael and McMinn, Phil},
title = {A Survey of Flaky Tests},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3476105},
doi = {10.1145/3476105},
abstract = {Tests that fail inconsistently, without changes to the code under test, are described as flaky. Flaky tests do not give a clear indication of the presence of software bugs and thus limit the reliability of the test suites that contain them. A recent survey of software developers found that 59% claimed to deal with flaky tests on a monthly, weekly, or daily basis. As well as being detrimental to developers, flaky tests have also been shown to limit the applicability of useful techniques in software testing research. In general, one can think of flaky tests as being a threat to the validity of any methodology that assumes the outcome of a test only depends on the source code it covers. In this article, we systematically survey the body of literature relevant to flaky test research, amounting to 76 papers. We split our analysis into four parts: addressing the causes of flaky tests, their costs and consequences, detection strategies, and approaches for their mitigation and repair. Our findings and their implications have consequences for how the software-testing community deals with test flakiness, pertinent to practitioners and of interest to those wanting to familiarize themselves with the research area.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {17},
numpages = {74},
keywords = {software testing, Flaky tests}
}

@inproceedings{10.1145/3434581.3434732,
author = {Yang, Qin and Cao, Xinwei and Ye, Qing},
title = {Rotating machinery vibration fault diagnosis model based on autoencoder},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434732},
doi = {10.1145/3434581.3434732},
abstract = {Rotating machinery is a large-scale equipment widely used in industrial production, and its safety issues should be paid attention to. Fault diagnosis of equipment is of great significance for ensuring safe production. With the rapid development of artificial intelligence, it is a trend to apply deep learning technology to vibration fault diagnosis of rotating machinery. This paper proposes a vibration fault diagnosis model for rotating machinery based on auto-encoder. Deeply under-complete auto-encoders, deep-noise-reduction auto-encoders, and deep-sparse autoencoders are constructed respectively. Four types of fault data of the test bench are used for verification. The results prove the effectiveness of this model.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {393–398},
numpages = {6},
keywords = {Rotating machinery, Fault diagnosis, Deep learning, Autoencoder},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1109/ICSE.2019.00075,
author = {Yatish, Suraj and Jiarpakdee, Jirayus and Thongtanunam, Patanamon and Tantithamthavorn, Chakkrit},
title = {Mining software defects: should we consider affected releases?},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00075},
doi = {10.1109/ICSE.2019.00075},
abstract = {With the rise of the Mining Software Repositories (MSR) field, defect datasets extracted from software repositories play a foundational role in many empirical studies related to software quality. At the core of defect data preparation is the identification of post-release defects. Prior studies leverage many heuristics (e.g., keywords and issue IDs) to identify post-release defects. However, such the heuristic approach is based on several assumptions, which pose common threats to the validity of many studies. In this paper, we set out to investigate the nature of the difference of defect datasets generated by the heuristic approach and the realistic approach that leverages the earliest affected release that is realistically estimated by a software development team for a given defect. In addition, we investigate the impact of defect identification approaches on the predictive accuracy and the ranking of defective modules that are produced by defect models. Through a case study of defect datasets of 32 releases, we find that that the heuristic approach has a large impact on both defect count datasets and binary defect datasets. Surprisingly, we find that the heuristic approach has a minimal impact on defect count models, suggesting that future work should not be too concerned about defect count models that are constructed using heuristic defect datasets. On the other hand, using defect datasets generated by the realistic approach lead to an improvement in the predictive accuracy of defect classification models.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {654–665},
numpages = {12},
keywords = {software quality, mining software repositories, empirical software engineering, defect prediction models},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3127005.3127007,
author = {Minku, Leandro L. and Hou, Siqing},
title = {Clustering Dycom: An Online Cross-Company Software Effort Estimation Study},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127007},
doi = {10.1145/3127005.3127007},
abstract = {Background: Software Effort Estimation (SEE) can be formulated as an online learning problem, where new projects are completed over time and may become available for training. In this scenario, a Cross-Company (CC) SEE approach called Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving the high cost of collecting such training projects. However, Dycom relies on splitting CC projects into different subsets in order to create its CC models. Such splitting can have a significant impact on Dycom's predictive performance. Aims: This paper investigates whether clustering methods can be used to help finding good CC splits for Dycom. Method: Dycom is extended to use clustering methods for creating the CC subsets. Three different clustering methods are investigated, namely Hierarchical Clustering, K-Means, and Expectation-Maximisation. Clustering Dycom is compared against the original Dycom with CC subsets of different sizes, based on four SEE databases. A baseline WC model is also included in the analysis. Results: Clustering Dycom with K-Means can potentially help to split the CC projects, managing to achieve similar or better predictive performance than Dycom. However, K-Means still requires the number of CC subsets to be pre-defined, and a poor choice can negatively affect predictive performance. EM enables Dycom to automatically set the number of CC subsets while still maintaining or improving predictive performance with respect to the baseline WC model. Clustering Dycom with Hierarchical Clustering did not offer significant advantage in terms of predictive performance. Conclusion: Clustering methods can be an effective way to automatically generate Dycom's CC subsets.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {Software effort estimation, concept drift, cross-company learning, ensembles, online learning},
location = {Toronto, Canada},
series = {PROMISE}
}

@article{10.1145/3417986,
author = {C., Marimuthu and Chandrasekaran, K. and Chimalakonda, Sridhar},
title = {Energy Diagnosis of Android Applications: A Thematic Taxonomy and Survey},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3417986},
doi = {10.1145/3417986},
abstract = {The abnormal energy consumption of Android applications is a significant problem faced by developers and users. In recent years, researchers have invested their efforts to develop energy diagnosis tools that pinpoint and fix the energy bugs from source code automatically. These tools use traditional software engineering methods such as program analysis, refactoring, software repair, and bug localization to diagnose energy inefficiencies. Existing surveys focus only on energy measurement techniques and profiling tools and do not consider automated energy diagnosis tools. Therefore, this article organizes state of the art by surveying 25 relevant studies on Android applications’ automatic energy diagnosis. Further, this survey presents a systematic thematic taxonomy of existing approaches from a software engineering perspective. The taxonomy presented in this article would serve as a body of knowledge and help researchers and developers to understand the state of the field better. The future research directions discussed in this article might help prospective researchers to identify suitable topics to improve the current research work in this field.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {117},
numpages = {36},
keywords = {thematic taxonomy, state of the art, android application, Automated energy diagnosis}
}

@article{10.1145/3310013.3310016,
author = {Adriano, Christian},
title = {Microtasking Software Failure Resolution: Early Results},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3310013.3310016},
doi = {10.1145/3310013.3310016},
abstract = {Open source software development enabled distributed teams of programmers to contribute to large software systems that became standards in the operation of government and business. Crowdsourcing went further by enabling contributions in the form of small and independent tasks. This allowed teams to scale from dozens to hundreds of people. While crowdsourcing established as industry practice in the areas of software testing, it is challenging for source code related tasks, e.g., software debugging. One of the reasons is that the complex dependencies in the source code can make many tasks difficult to partition and sequence, and later aggregate their outcomes. I am investigating these problems in the context of failure resolution tasks. A failure resolution task consists of inspecting the source code with the objective to identify and explain the root-cause of a software failure. My approach partitions code inspection into questions that are automatically instantiated from templates. I present here my research plan and the early results of experiments on the efficacy, efficiency, and scalability of my approach.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {36–39},
numpages = {4},
keywords = {statistical fault localization, software debugging, microtask, mechanical turk, crowdsourcing}
}

@inproceedings{10.1145/3287921.3287958,
author = {Van Thuy, Hoang and Anh, Phan Viet and Hoai, Nguyen Xuan},
title = {Automated Large Program Repair based on Big Code},
year = {2018},
isbn = {9781450365390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287921.3287958},
doi = {10.1145/3287921.3287958},
abstract = {The task of automatic program repair is to automatically localize and generate the correct patches for the bugs. A prominent approach is to produce a space of candidate patches, then find and validate candidates on test case sets. However, searching for the correct candidates is really challenging, since the search space is dominated by incorrect patches and its size is huge.This paper presents several methods to improve the automated program repair system Prophet, called Prophet+. Our approach contributes three improvements over Prophet: 1) extract twelve relations of statements and blocks for Bi-gram model using Big code, 2) prune the search space, 3) develop an algorithm to re-rank candidate patches in the search space. The experimental results show that our proposed system enhances the performance of Prophet, recognized as the state-of-the-art system, significantly. Specifically, for the top 1, our system generates the correct patches for 17 over 69 bugs while the number achieved by Prophet is 15.},
booktitle = {Proceedings of the 9th International Symposium on Information and Communication Technology},
pages = {375–381},
numpages = {7},
keywords = {N-gram, Machine Learning, Bigcode, Automated Program Repair},
location = {Danang City, Viet Nam},
series = {SoICT '18}
}

@article{10.1007/s11390-020-0526-y,
author = {Xu, Yi-Sen and Jia, Xiang-Yang and Wu, Fan and Li, Lingbo and Xuan, Ji-Feng},
title = {Automatically Identifying Calling-Prone Higher-Order Functions of Scala Programs to Assist Testers},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {6},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-0526-y},
doi = {10.1007/s11390-020-0526-y},
abstract = {For the rapid development of internetware, functional programming languages, such as Haskell and Scala, can be used to implement complex domain-specific applications. In functional programming languages, a higher-order function is a function that takes functions as parameters or returns a function. Using higher-order functions in programs can increase the generality and reduce the redundancy of source code. To test a higher-order function, a tester needs to check the requirements and write another function as the test input. However, due to the complex structure of higher-order functions, testing higher-order functions is a time-consuming and labor-intensive task. Testers have to spend an amount of manual effort in testing all higher-order functions. Such testing is infeasible if the time budget is limited, such as a period before a project release. In practice, not every higher-order function is actually called. We refer to higher-order functions that are about to be called as calling-prone ones. Calling-prone higher-order functions should be tested first. In this paper, we propose an automatic approach, namely Phof, which predicts whether a higher-order function of Scala programs will be called in the future, i.e., identifying calling-prone higher-order functions. Our approach can assist testers to reduce the number of higher-order functions of Scala programs under test. In Phof, we extracted 24 features from source code and logs to train a predictive model based on known higher-order function calls. We empirically evaluated our approach on 4 832 higher-order functions from 27 real-world Scala projects. Experimental results show that Phof based on the random forest algorithm and the Synthetic Minority Oversampling Technique Processing strategy (SMOTE) performs well in the prediction of calls of higher-order functions. Our work can be used to support the scheduling of limited test resources.},
journal = {J. Comput. Sci. Technol.},
month = nov,
pages = {1278–1294},
numpages = {17},
keywords = {internetware, Scala program, test management, testing tool, higher-order function}
}

@article{10.1016/j.knosys.2015.11.013,
author = {Yijing, Li and Haixiang, Guo and Xiao, Liu and Yanan, Li and Jinling, Li},
title = {Adapted ensemble classification algorithm based on multiple classifier system and feature selection for classifying multi-class imbalanced data},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {94},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.11.013},
doi = {10.1016/j.knosys.2015.11.013},
abstract = {Learning from imbalanced data, where the number of observations in one class is significantly rarer than in other classes, has gained considerable attention in the data mining community. Most existing literature focuses on binary imbalanced case while multi-class imbalanced learning is barely mentioned. What's more, most proposed algorithms treated all imbalanced data consistently and aimed to handle all imbalanced data with a versatile algorithm. In fact, the imbalanced data varies in their imbalanced ratio, dimension and the number of classes, the performances of classifiers for learning from different types of datasets are different. In this paper we propose an adaptive multiple classifier system named of AMCS to cope with multi-class imbalanced learning, which makes a distinction among different kinds of imbalanced data. The AMCS includes three components, which are, feature selection, resampling and ensemble learning. Each component of AMCS is selected discriminatively for different types of imbalanced data. We consider two feature selection methods, three resampling mechanisms, five base classifiers and five ensemble rules to construct a selection pool, the adapting criterion of choosing each component from the selection pool to frame AMCS is analyzed through empirical study. In order to verify the effectiveness of AMCS, we compare AMCS with several state-of-the-art algorithms, the results show that AMCS can outperform or be comparable with the others. At last, AMCS is applied in oil-bearing reservoir recognition. The results indicate that AMCS makes no mistake in recognizing characters of layers for oilsk81-oilsk85 well logging data which is collected in Jianghan oilfield of China.},
journal = {Know.-Based Syst.},
month = feb,
pages = {88–104},
numpages = {17},
keywords = {Oil reservoir, Multiple classifier system, Imbalanced data, Adaptive learning}
}

@article{10.1016/j.ins.2010.12.016,
author = {Seiffert, Chris and Khoshgoftaar, Taghi M. and Van Hulse, Jason and Folleco, Andres},
title = {An empirical study of the classification performance of learners on imbalanced and noisy software quality data},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {259},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2010.12.016},
doi = {10.1016/j.ins.2010.12.016},
abstract = {Data mining techniques are commonly used to construct models for identifying software modules that are most likely to contain faults. In doing so, an organization's limited resources can be intelligently allocated with the goal of detecting and correcting the greatest number of faults. However, there are two characteristics of software quality datasets that can negatively impact the effectiveness of these models: class imbalance and class noise. Software quality datasets are, by their nature, imbalanced. That is, most of a software system's faults can be found in a small percentage of software modules. Therefore, the number of fault-prone, fp, examples (program modules) in a software project dataset is much smaller than the number of not fault-prone, nfp, examples. Data sampling techniques attempt to alleviate the problem of class imbalance by altering a training dataset's distribution. A program module contains class noise if it is incorrectly labeled. While several studies have been performed to evaluate data sampling methods, the impact of class noise on these techniques has not been adequately addressed. This work presents a systematic set of experiments designed to investigate the impact of both class noise and class imbalance on classification models constructed to identify fault-prone program modules. We analyze the impact of class noise and class imbalance on 11 different learning algorithms (learners) as well as 7 different data sampling techniques. We identify which learners and which data sampling techniques are most robust when confronted with noisy and imbalanced data.},
journal = {Inf. Sci.},
month = feb,
pages = {571–595},
numpages = {25},
keywords = {Sampling, Imbalance, Class noise, Binary classification}
}

@inproceedings{10.1145/3395363.3397354,
author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
title = {Reinforcement learning based curiosity-driven testing of Android applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397354},
doi = {10.1145/3395363.3397354},
abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {153–164},
numpages = {12},
keywords = {reinforcement learning, functional scenario division, Android app testing},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1109/ICSE43902.2021.00131,
author = {Li, Zhenhao and Li, Heng and Chen, Tse-Hsun Peter and Shang, Weiyi},
title = {DeepLV: Suggesting Log Levels Using Ordinal Based Neural Networks},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00131},
doi = {10.1109/ICSE43902.2021.00131},
abstract = {Developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems. Log level is an important component of a logging statement, which enables developers to control the information to be generated at system runtime. However, due to the complexity of software systems and their runtime behaviors, deciding a proper log level for a logging statement is a challenging task. For example, choosing a higher level (e.g., error) for a trivial event may confuse end users and increase system maintenance overhead, while choosing a lower level (e.g., trace) for a critical event may prevent the important execution information to be conveyed opportunely. In this paper, we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels. We find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels, and log levels that are further apart in order (e.g., trace and error) tend to have more differences in their characteristics. Based on this, we then propose a deep-learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels, by using the syntactic context and message features of the logging statements extracted from the source code. Through an evaluation on nine large-scale open source projects, we find that: 1) our approach outperforms the state-of-the-art baseline approaches; 2) we can further improve the performance of our approach by enlarging the training data obtained from other systems; 3) our approach also achieves promising results on cross-system suggestions that are even better than the baseline approaches on within-system suggestions. Our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1461–1472},
numpages = {12},
keywords = {logs, log level, empirical study, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.5555/2747015.2747194,
author = {Qusef, Abdallah and Bavota, Gabriele and Oliveto, Rocco and De Lucia, Andrea and Binkley, Dave},
title = {Recovering test-to-code traceability using slicing and textual analysis},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {Test suites are a valuable source of up-to-date documentation as developers continuously modify them to reflect changes in the production code and preserve an effective regression suite. While maintaining traceability links between unit test and the classes under test can be useful to selectively retest code after a change, the value of having traceability links goes far beyond this potential savings. One key use is to help developers better comprehend the dependencies between tests and classes and help maintain consistency during refactoring. Despite its importance, test-to-code traceability is not common in software development and, when needed, traceability information has to be recovered during software development and evolution. We propose an advanced approach, named SCOTCH+ (Source code and COncept based Test to Code traceability Hunter), to support the developer during the identification of links between unit tests and tested classes. Given a test class, represented by a JUnit class, the approach first exploits dynamic slicing to identify a set of candidate tested classes. Then, external and internal textual information associated with the classes retrieved by slicing is analyzed to refine this set of classes and identify the final set of candidate tested classes. The external information is derived from the analysis of the class name, while internal information is derived from identifiers and comments. The approach is evaluated on five software systems. The results indicate that the accuracy of the proposed approach far exceeds the leading techniques found in the literature.},
journal = {J. Syst. Softw.},
month = feb,
pages = {147–168},
numpages = {22},
keywords = {Test-to-code traceability, Information retrieval, Dynamic slicing}
}

@article{10.1016/j.jss.2016.02.007,
author = {Liu, Hu-Qiu and Wang, Yu-Ping and Bai, Jia-Ju and Hu, Shi-Min},
title = {PF-Miner},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.007},
doi = {10.1016/j.jss.2016.02.007},
abstract = {Proposed a method for mining paired function on the source code of drivers.Sequence dependency rules based on paired functions are also proposed.A theory model base on paired function has been proposed for violation detection.PF-Miner mined 546 paired functions, 28"49"(not 28) sequence dependency rules.15 bugs have been confirmed for the top 51 violations based on mining results. Generally, drivers have many errors to handle, and the functions called in the normal execution paths and error handling paths are in pairs, which are named as paired functions. However, some developers do not handle the errors completely as they forget or unaware of releasing the acquired resources, thus memory leaks and other potential problems can be easily introduced into the system. Therefore, it is highly valuable to automatically extract paired functions and detect violations for programmers.This paper proposes an efficient tool named PF-Miner, which can automatically extract paired functions and detect violations between normal execution paths and error handling paths from the source code of drivers with the data mining and statistical methods. We have evaluated PF-Miner on two different versions of Android kernel 2.6.39 and 3.10.0, and 81 bugs reported by PF-Miner in 2.6.39 have been fixed before the latest version 3.10.0. PF-Miner only needs about 150 s to analyze the source code of 3.10.0, and 983 violations have been detected from 546 paired functions which have been extracted. We have reported the top 51 violations as potential bugs to the developers, and 15 bugs have been confirmed so far.},
journal = {J. Syst. Softw.},
month = nov,
pages = {234–246},
numpages = {13},
keywords = {Violations detection, Paired function mining, Error path checking}
}

@inproceedings{10.1145/3434581.3434730,
author = {Feng, Jian and Xiang, Rongxin and Xie, Yuanbo},
title = {Fault diagnosis of rotating machinery based on deep learning},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434730},
doi = {10.1145/3434581.3434730},
abstract = {In the current machinery manufacturing industry, rotating machinery occupies a very important position. Rotating machinery mainly refers to the machinery that can complete specific functions with the help of rotary action. The main vibration faults of rotating machinery include rotor unbalance, rotor misalignment, friction between moving and static parts and looseness of support parts. Generally, large rotating machinery is generally equipped with vibration monitoring protection and fault diagnosis system to ensure the safe operation of rotating machinery, but there are some limitations. In order to find the hidden danger in the machinery in time and avoid serious mechanical accidents, this paper combines the deep learning technology in machine learning to increase the accuracy and reliability of fault identification, reduce the risk of failure, and provide more guarantee for the production safety of machinery industry. Aiming at the main faults of rotating machinery, this paper proposes a fault recognition method of rotating machinery based on LSTM. Firstly, the sample data is screened and judged effectively, and then the abnormal data are classified by LSTM and softmax. Finally, the evaluation index and result of the fault category are obtained, which is compared with other classification algorithms to show the good applicability of the algorithm. At present, there are still some problems in the model, which need to be further discussed.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {388–392},
numpages = {5},
keywords = {fault classification, deep learning, Rotating machinery, LSTM},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/3239235.3239244,
author = {Walkinshaw, Neil and Minku, Leandro},
title = {Are 20% of files responsible for 80% of defects?},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239244},
doi = {10.1145/3239235.3239244},
abstract = {Background: Over the past two decades a mixture of anecdote from the industry and empirical studies from academia have suggested that the 80:20 rule (otherwise known as the Pareto Principle) applies to the relationship between source code files and the number of defects in the system: a small minority of files (roughly 20%) are responsible for a majority of defects (roughly 80%).Aims: This paper aims to establish how widespread the phenomenon is by analysing 100 systems (previous studies have focussed on between one and three systems), with the goal of whether and under what circumstances this relationship does hold, and whether the key files can be readily identified from basic metrics.Method: We devised a search criterion to identify defect fixes from commit messages and used this to analyse 100 active Github repositories, spanning a variety of languages and domains. We then studied the relationship between files, basic metrics (churn and LOC), and defect fixes.Results: We found that the Pareto principle does hold, but only if defects that incur fixes to multiple files count as multiple defects. When we investigated multi-file fixes, we found that key files (belonging to the top 20%) are commonly fixed alongside other much less frequently-fixed files. We found LOC to be poorly correlated with defect proneness, Code Churn was a more reliable indicator, but only for extremely high values of Churn.Conclusions: It is difficult to reliably identify the "most fixed" 20% of files from basic metrics. However, even if they could be reliably predicted, focussing on them would probably be misguided. Although fixes will naturally involve files that are often involved in other fixes too, they also tend to include other less frequently-fixed files.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {2},
numpages = {10},
keywords = {survey, pareto principle, defect distribution},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.1016/j.eswa.2017.08.004,
author = {Wangchamhan, Tanachapong and Chiewchanwattana, Sirapat and Sunat, Khamron},
title = {Efficient algorithms based on the k-means and Chaotic League Championship Algorithm for numeric, categorical, and mixed-type data clustering},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.08.004},
doi = {10.1016/j.eswa.2017.08.004},
abstract = {The search algorithm C-LCA is done by adding two chaotic parameters into the LCA.The search clustering using CS-LCA and the KSC-LCA algorithms are proposed.The Gower distance and a mechanism are adopted for handling the mixed-type data.The search clustering CS-LCA ranks first for the pure categorical data.The KSC-LCA ranks first for the pure numeric and mixed-type data. The success rates of the expert or intelligent systems depend on the selection of the correct data clusters. The k-means algorithm is a well-known method in solving data clustering problems. It suffers not only from a high dependency on the algorithm's initial solution but also from the used distance function. A number of algorithms have been proposed to address the centroid initialization problem, but the produced solution does not produce optimum clusters. This paper proposes three algorithms (i) the search algorithm C-LCA that is an improved League Championship Algorithm (LCA), (ii) a search clustering using C-LCA (SC-LCA), and (iii) a hybrid-clustering algorithm called the hybrid of k-means and Chaotic League Championship Algorithm (KSC-LCA) and this algorithm has of two computation stages. The C-LCA employs chaotic adaptation for the retreat and approach parameters, rather than constants, which can enhance the search capability. Furthermore, to overcome the limitation of the original k-means algorithm using the Euclidean distance that cannot handle the categorical attribute type properly, we adopt the Gower distance and the mechanism for handling a discrete value requirement of the categorical value attribute. The proposed algorithms can handle not only the pure numeric data but also the mixed-type data and can find the best centroids containing categorical values. Experiments were conducted on 14 datasets from the UCI repository. The SC-LCA and KSC-LCA competed with 16 established algorithms including the k-means, k-means++, global k-means algorithms, four search clustering algorithms and nine hybrids of k-means algorithm with several state-of-the-art evolutionary algorithms. The experimental results show that the SC-LCA produces the cluster with the highest F-Measure on the pure categorical dataset and the KSC-LCA produces the cluster with the highest F-Measure for the pure numeric and mixed-type tested datasets. Out of 14 datasets, there were 13 centroids produced by the SC-LCA that had better F-Measures than that of the k-means algorithm. On the Tic-Tac-Toe dataset containing only categorical attributes, the SC-LCA can achieve an F-Measure of 66.61 that is 21.74 points over that of the k-means algorithm (44.87). The KSC-LCA produced better centroids than k-means algorithm in all 14 datasets; the maximum F-Measure improvement was 11.59 points. However, in terms of the computational time, the SC-LCA and KSC-LCA took more NFEs than the k-means and its variants but the KSC-LCA ranks first and SC-LCA ranks fourth among the hybrid clustering and the search clustering algorithms that we tested. Therefore, the SC-LCA and KSC-LCA are general and effective clustering algorithms that could be used when an expert or intelligent system requires an accurate high-speed cluster selection.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {146–167},
numpages = {22},
keywords = {Search clustering algorithm, Mixed-type data, League Championship Algorithm (LCA), Hybrid clustering algorithm, Data clustering, Chaos optimization algorithms (COA)}
}

@article{10.1016/j.eswa.2014.09.014,
author = {Bala, Anju and Chana, Inderveer},
title = {Intelligent failure prediction models for scientific workflows},
year = {2015},
issue_date = {February 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.09.014},
doi = {10.1016/j.eswa.2014.09.014},
abstract = {Intelligent task failure models using machine learning approaches are proposed.The accuracy of proposed models is validated in Pegasus and Amazon EC2.The prediction accuracy of (94%) is achieved using Na\"{\i}ve Bayes approach. The ever-growing demand and heterogeneity of Cloud Computing is garnering popularity with scientific communities to utilize the services of Cloud for executing large scale scientific applications in the form of set of tasks known as Workflows. As scientific workflows stipulate a process or computation to be executed in the form of data flow and task dependencies that allow users to simply articulate multi-step computational and complex tasks. Hence, proactive fault tolerance is required for the execution of scientific workflows. To reduce the failure effect of workflow tasks on the Cloud resources during execution, task failures can be intelligently predicted by proactively analyzing the data of multiple scientific workflows using the state of the art of machine learning approaches for failure prediction. Therefore, this paper makes an effort to focus on the research problem of designing an intelligent task failure prediction models for facilitating proactive fault tolerance by predicting task failures for Scientific Workflow applications. Firstly, failure prediction models have been implemented through machine learning approaches using evaluated performance metrics and also demonstrates the maximum prediction accuracy for Naive Bayes. Then, the proposed failure models have also been validated using Pegasus and Amazon EC2 by comparing actual task failures with predicted task failures.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {980–989},
numpages = {10},
keywords = {Workflows, Scientific workflows, Machine learning, Failure prediction, Cloud Computing}
}

@article{10.1145/3433928,
author = {Vandehei, Bailey and Costa, Daniel Alencar Da and Falessi, Davide},
title = {Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3433928},
doi = {10.1145/3433928},
abstract = {Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach. However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation &lt;2)—across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {24},
numpages = {35},
keywords = {developing defects repository, defect origin, SZZ, Affected version}
}

@inproceedings{10.1145/3452383.3452393,
author = {Kapur, Ritu and Sodhi, Balwinder and Rao, Poojith U and Sharma, Shipra},
title = {Using Paragraph Vectors to improve our existing code review assisting tool-CRUSO},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452393},
doi = {10.1145/3452383.3452393},
abstract = {Code reviews are one of the effective methods to estimate defectiveness in source code. However, the existing methods are dependent on experts or inefficient. In this paper, we improve the performance (in terms of speed and memory usage) of our existing code review assisting tool–CRUSO. The central idea of the approach is to estimate the defectiveness for an input source code by using the defectiveness score of similar code fragments present in various StackOverflow (SO) posts. The significant contributions of our paper are i) SOpostsDB: a dataset containing the PVA vectors and the SO posts information, ii) CRUSO-P: a code review assisting system based on PVA models trained on SOpostsDB. For a given input source code, CRUSO-P labels it as {Likely to be defective, Unlikely to be defective, Unpredictable}. To develop CRUSO-P, we processed &gt;3 million SO posts and 188200+ GitHub source files. CRUSO-P is designed to work with source code written in the popular programming languages {C, C#, Java, JavaScript, and Python}. CRUSO-P outperforms CRUSO with an improvement of 97.82% in response time and a storage reduction of 99.15%. CRUSO-P achieves the highest mean accuracy score of 99.6% when tested with the C programming language, thus achieving an improvement of 5.6% over the existing method.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {10},
numpages = {11},
keywords = {Automated code review, Code quality, Paragraph Vector, Software maintenance, StackOverflow},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@article{10.1007/s10664-014-9300-5,
author = {Kocaguneli, Ekrem and Menzies, Tim and Mendes, Emilia},
title = {Transfer learning in effort estimation},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9300-5},
doi = {10.1007/s10664-014-9300-5},
abstract = {When projects lack sufficient local data to make predictions, they try to transfer information from other projects. How can we best support this process? In the field of software engineering, transfer learning has been shown to be effective for defect prediction. This paper checks whether it is possible to build transfer learners for software effort estimation. We use data on 154 projects from 2 sources to investigate transfer learning between different time intervals and 195 projects from 51 sources to provide evidence on the value of transfer learning for traditional cross-company learning problems. We find that the same transfer learning method can be useful for transfer effort estimation results for the cross-company learning problem and the cross-time learning problem. It is misguided to think that: (1) Old data of an organization is irrelevant to current context or (2) data of another organization cannot be used for local solutions. Transfer learning is a promising research direction that transfers relevant cross data between time intervals and domains.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {813–843},
numpages = {31},
keywords = {k-NN, Transfer learning, Effort estimation, Data mining}
}

@inproceedings{10.1145/3377811.3380354,
author = {Bai, Yude and Xing, Zhenchang and Li, Xiaohong and Feng, Zhiyong and Ma, Duoyuan},
title = {Unsuccessful story about few shot malware family classification and siamese network to the rescue},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380354},
doi = {10.1145/3377811.3380354},
abstract = {To battle the ever-increasing Android malware, malware family classification, which classifies malware with common features into a malware family, has been proposed as an effective malware analysis method. Several machine-learning based approaches have been proposed for the task of malware family classification. Our study shows that malware families suffer from several data imbalance, with many families with only a small number of malware applications (referred to as few shot malware families in this work). Unfortunately, this issue has been overlooked in existing approaches. Although existing approaches achieve high classification performance at the overall level and for large malware families, our experiments show that they suffer from poor performance and generalizability for few shot malware families, and traditionally downsampling method cannot solve the problem. To address the challenge in few shot malware family classification, we propose a novel siamese-network based learning method, which allows us to train an effective MultiLayer Perceptron (MLP) network for embedding malware applications into a real-valued, continuous vector space by contrasting the malware applications from the same or different families. In the embedding space, the performance of malware family classification can be significantly improved for all scales of malware families, especially for few shot malware families, which also leads to the significant performance improvement at the overall level.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1560–1571},
numpages = {12},
keywords = {few shot learning, malware family classification, siamese network},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.inffus.2017.09.010,
author = {Cruz, Rafael M.O. and Sabourin, Robert and Cavalcanti, George D.C.},
title = {Dynamic classifier selection},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {41},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2017.09.010},
doi = {10.1016/j.inffus.2017.09.010},
abstract = {An updated taxonomy of Dynamic Selection techniques is proposed.A review of the state-of-the-art dynamic selection techniques is presented.Empirical comparison between 18 dynamic selection techniques is conducted.We discuss about the recent findings and open research question in this field. Multiple Classifier Systems (MCS) have been widely studied as an alternative for increasing accuracy in pattern recognition. One of the most promising MCS approaches is Dynamic Selection (DS), in which the base classifiers are selected on the fly, according to each new sample to be classified. This paper provides a review of the DS techniques proposed in the literature from a theoretical and empirical point of view. We propose an updated taxonomy based on the main characteristics found in a dynamic selection system: (1) The methodology used to define a local region for the estimation of the local competence of the base classifiers; (2) The source of information used to estimate the level of competence of the base classifiers, such as local accuracy, oracle, ranking and probabilistic models, and (3) The selection approach, which determines whether a single or an ensemble of classifiers is selected. We categorize the main dynamic selection techniques in the DS literature based on the proposed taxonomy. We also conduct an extensive experimental analysis, considering a total of 18 state-of-the-art dynamic selection techniques, as well as static ensemble combination and single classification models. To date, this is the first analysis comparing all the key DS techniques under the same experimental protocol. Furthermore, we also present several perspectives and open research questions that can be used as a guide for future works in this domain.},
journal = {Inf. Fusion},
month = may,
pages = {195–216},
numpages = {22},
keywords = {Survey, Multiple classifier systems, Ensemble of classifiers, Dynamic ensemble selection, Dynamic classifier selection, Classifier competence}
}

@article{10.1109/TSE.2009.70,
author = {Buse, Raymond P. L. and Weimer, Westley R.},
title = {Learning a Metric for Code Readability},
year = {2010},
issue_date = {July 2010},
publisher = {IEEE Press},
volume = {36},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2009.70},
doi = {10.1109/TSE.2009.70},
abstract = {In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from 120 human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80 percent effective and better than a human, on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with three measures of software quality: code changes, automated defect reports, and defect log messages. We measure these correlations on over 2.2 million lines of code, as well as longitudinally, over many releases of selected projects. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggest that comments, in and of themselves, are less important than simple blank lines to local judgments of readability.},
journal = {IEEE Trans. Softw. Eng.},
month = jul,
pages = {546–558},
numpages = {13},
keywords = {software maintenance, program understanding, machine learning, code metrics, Software readability, program understanding, machine learning, software maintenance, code metrics, FindBugs., Software readability, FindBugs.}
}

@inproceedings{10.1145/2070821.2070823,
author = {Roychowdhury, Shounak and Khurshid, Sarfraz},
title = {Software fault localization using feature selection},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070823},
doi = {10.1145/2070821.2070823},
abstract = {Manually locating and fixing faults can be tedious and hard. Recent years have seen much progress in automated techniques for fault localization. A particularly promising approach is to analyze passing and failing runs to compute how likely each statement is to be faulty. Techniques based on this approach have so far largely focused on either using statistical analysis or similarity based algorithms, which have a natural application in evaluating such runs. We present a novel approach to fault localization using feature selection techniques from machine learning. Our insight is that each additional failing or passing run can provide significantly diverse amount of information, which can help localize faults in code -- the statements with maximum feature diversity information can point to most suspicious lines of code. Experimental results show that our approach outperforms state-of-the-art approaches for localizing faults in most subject programs of the Siemens suite, which have previously been used to evaluate several fault localization techniques.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {statistical debugging, machine learning, feature selection, fault localization, automated debugging, RELIEF},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@article{10.1155/2021/7295627,
author = {Xiaolong, He and Huiqi, Zhao and Lunchao, Zhong and Nazir, Shah and Jun, Deng and Khan, Adnan Shahid and Yang, Zhongguo},
title = {Soft Computing and Decision Support System for Software Process Improvement: A Systematic Literature Review},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/7295627},
doi = {10.1155/2021/7295627},
abstract = {Software project development is very crucial, and measuring the exact cost and effort of development is becoming tedious and challenging. Organizations are trying to wind up their project of software development within the agreed budget and schedule successfully. Traditional practices are inadequate to achieve the current needs of the software industry. Underestimation and overestimation of software development effort lead to financial implications in the form of resources, cost of staffing, and budget of developing the software project. Soft computing (SC) approaches and tools deliver an addition of techniques for anticipating resistance to the deception, defect, incomplete truth for traceability and ambiguity, low arrangement cost, and strength. A large amount of SC approaches is prevailing in the literature to accomplish way-out to difficulties precisely, practically, and speedily. The approaches of SC can give better prediction, high performance, and dynamic behavior. SC deals with computational intelligence which integrates the concept of agent paradigm and SC. The proposed study presents a systematic literature review (SLR) of the approaches, tools, and techniques of SC used in the literature. The study presented a comprehensive review by searching the defined keywords in the popular libraries, filtered the paper, and obtained most relevant papers. After the selection of the papers, the quality assessment process of the included papers has been done in order to determine the relevancy of the papers. The study will help researchers in the area of research to devise novel ideas and solutions to overcome the existing issue on the basis of this study as evidence of the literature.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1145/3319008.3319716,
author = {Alsolai, Hadeel and Roper, Marc},
title = {Application of Ensemble Techniques in Predicting Object-Oriented Software Maintainability},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319716},
doi = {10.1145/3319008.3319716},
abstract = {While prior object-oriented software maintainability literature acknowledges the role of machine learning techniques as valuable predictors of potential change, the most suitable technique that achieves consistently high accuracy remains undetermined. With the objective of obtaining more consistent results, an ensemble technique is investigated to advance the performance of the individual models and increase their accuracy in predicting software maintainability of the object-oriented system. This paper describes the research plan for predicting object-oriented software maintainability using ensemble techniques. First, we present a brief overview of the main research background and its different components. Second, we explain the research methodology. Third, we provide expected results. Finally, we conclude summary of the current status.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {370–373},
numpages = {4},
keywords = {software maintainability, prediction, individual model, ensemble model, Object-oriented system},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@article{10.1162/evco_a_00213,
author = {Ferrucci, Filomena and Salza, Pasquale and Sarro, Federica},
title = {Using Hadoop MapReduce for Parallel Genetic Algorithms: A Comparison of the Global, Grid and Island Models},
year = {2018},
issue_date = {Winter 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {26},
number = {4},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00213},
doi = {10.1162/evco_a_00213},
abstract = {The need to improve the scalability of Genetic Algorithms (GAs) has motivated the research on Parallel Genetic Algorithms (PGAs), and different technologies and approaches have been used. Hadoop MapReduce represents one of the most mature technologies to develop parallel algorithms. Based on the fact that parallel algorithms introduce communication overhead, the aim of the present work is to understand if, and possibly when, the parallel GAs solutions using Hadoop MapReduce show better performance than sequential versions in terms of execution time. Moreover, we are interested in understanding which PGA model can be most effective among the global, grid, and island models. We empirically assessed the performance of these three parallel models with respect to a sequential GA on a software engineering problem, evaluating the execution time and the achieved speedup. We also analysed the behaviour of the parallel models in relation to the overhead produced by the use of Hadoop MapReduce and the GAs' computational effort, which gives a more machine-independent measure of these algorithms. We exploited three problem instances to differentiate the computation load and three cluster configurations based on 2, 4, and 8 parallel nodes. Moreover, we estimated the costs of the execution of the experimentation on a potential cloud infrastructure, based on the pricing of the major commercial cloud providers. The empirical study revealed that the use of PGA based on the island model outperforms the other parallel models and the sequential GA for all the considered instances and clusters. Using 2, 4, and 8 nodes, the island model achieves an average speedup over the three datasets of 1.8, 3.4, and 7.0 times, respectively. Hadoop MapReduce has a set of different constraints that need to be considered during the design and the implementation of parallel algorithms. The overhead of data store (i.e., HDFS) accesses, communication, and latency requires solutions that reduce data store operations. For this reason, the island model is more suitable for PGAs than the global and grid model, also in terms of costs when executed on a commercial cloud provider.},
journal = {Evol. Comput.},
month = dec,
pages = {535–567},
numpages = {33},
keywords = {fault prediction., island model, grid model, global model, Hadoop MapReduce, parallel genetic algorithms, Genetic algorithms}
}

@article{10.1016/j.jss.2019.110406,
author = {Ruan, Hang and Chen, Bihuan and Peng, Xin and Zhao, Wenyun},
title = {         DeepLink: Recovering issue-commit links based on deep learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110406},
doi = {10.1016/j.jss.2019.110406},
journal = {J. Syst. Softw.},
month = dec,
numpages = {13},
keywords = {Semantic understanding, Deep learning, Issue-commit links}
}

@inproceedings{10.1007/978-3-030-63618-0_5,
author = {Scott, Joseph and Mora, Federico and Ganesh, Vijay},
title = {BanditFuzz: A Reinforcement-Learning Based Performance Fuzzer for SMT Solvers},
year = {2020},
isbn = {978-3-030-63617-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63618-0_5},
doi = {10.1007/978-3-030-63618-0_5},
abstract = {Satisfiability Modulo Theories (SMT) solvers are fundamental tools that are used widely in software engineering, verification, and security research. Precisely because of their widespread use, it is imperative we develop efficient and systematic methods to test them. To this end, we present a reinforcement-learning based fuzzing system, BanditFuzz, that learns grammatical constructs of well-formed inputs that may cause performance slowdown in SMT solvers. To the best of our knowledge, BanditFuzz is the first machine-learning based performance fuzzer for SMT solvers.BanditFuzz takes the following as input: a grammar G describing well-formed inputs to a set of distinct solvers (say, a target solver T and a reference solver R) that implement the same specification, and a fuzzing objective (e.g., aim to maximize the relative performance difference between T and R). BanditFuzz outputs a list of grammatical constructs that are ranked in descending order by how likely they are to increase the performance difference between solvers T and R. Using BanditFuzz, we constructed two benchmark suites (with 400 floating-point and 300 string instances) that expose performance issues in all considered solvers, namely, Z3, CVC4, Colibri, MathSAT, Z3seq, and Z3str3. We also performed a comparison of BanditFuzz against random, mutation, and evolutionary fuzzing methods and observed up&nbsp;to a 81% improvement based on PAR-2 scores used in SAT competitions. That is, relative to other fuzzing methods considered, BanditFuzz was found to be more efficient at constructing inputs with wider performance margin between a target and a set of reference solvers.},
booktitle = {Software Verification: 12th International Conference, VSTTE 2020, and 13th International Workshop, NSV 2020, Los Angeles, CA, USA, July 20–21, 2020, Revised Selected Papers},
pages = {68–86},
numpages = {19},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/259526.259548,
author = {Cheatham, Thomas J. and Yoo, Jungsoon P. and Wahl, Nancy J.},
title = {Software testing: a machine learning experiment},
year = {1995},
isbn = {0897917375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/259526.259548},
doi = {10.1145/259526.259548},
booktitle = {Proceedings of the 1995 ACM 23rd Annual Conference on Computer Science},
pages = {135–141},
numpages = {7},
location = {Nashville, Tennessee, USA},
series = {CSC '95}
}

@article{10.1016/j.jss.2017.09.026,
author = {Mensah, Solomon and Keung, Jacky and Svajlenko, Jeffery and Bennin, Kwabena Ebo and Mi, Qing},
title = {On the value of a prioritization scheme for resolving Self-admitted technical debt},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.026},
doi = {10.1016/j.jss.2017.09.026},
abstract = {A prioritization scheme for resolving self-admitted technical debt is proposed.The scheme uses regular expressions to search for prioritized tasks.Majority of buggy prone (vital few) tasks are design debts.A rework effort of 1025 commented LOC is needed for the vital few tasks. Programmers tend to leave incomplete, temporary workarounds and buggy codes that require rework in software development and such pitfall is referred to as Self-admitted Technical Debt (SATD). Previous studies have shown that SATD negatively affects software project and incurs high maintenance overheads. In this study, we introduce a prioritization scheme comprising mainly of identification, examination and rework effort estimation of prioritized tasks in order to make a final decision prior to software release. Using the proposed prioritization scheme, we perform an exploratory analysis on four open source projects to investigate how SATD can be minimized. Four prominent causes of SATD are identified, namely code smells (23.2%), complicated and complex tasks (22.0%), inadequate code testing (21.2%) and unexpected code performance (17.4%). Results show that, among all the types of SATD, design debts on average are highly prone to software bugs across the four projects analysed. Our findings show that a rework effort of approximately 10 to 25 commented LOC per SATD source file is needed to address the highly prioritized SATD (vital few) tasks. The proposed prioritization scheme is a novel technique that will aid in decision making prior to software release in an attempt to minimize high maintenance overheads.},
journal = {J. Syst. Softw.},
month = jan,
pages = {37–54},
numpages = {18},
keywords = {Textual indicators, Source code comment, Self-admitted technical debt, Prioritization scheme, Open source projects}
}

@inproceedings{10.5555/978-3-319-49586-6_fm,
title = {Front Matter},
year = {2016},
isbn = {978-3-319-49585-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Advanced Data Mining and Applications: 12th International Conference, ADMA 2016, Gold Coast, QLD, Australia, December 12-15, 2016, Proceedings},
pages = {I–XVI},
location = {Gold Coast, Australia}
}

@article{10.1016/j.infsof.2019.03.001,
author = {Zhang, Wen and Li, Ziqiang and Wang, Qing and Li, Juan},
title = {FineLocator: A novel approach to method-level fine-grained bug localization by query expansion},
year = {2019},
issue_date = {Jun 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {110},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.001},
doi = {10.1016/j.infsof.2019.03.001},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {121–135},
numpages = {15},
keywords = {Call dependency, Temporal proximity, Semantic similarity, Query expansion, Method-level bug localization}
}

@article{10.1007/s10586-017-1362-x,
author = {Lazarova-Molnar, Sanja and Mohamed, Nader},
title = {Collaborative data analytics for smart buildings: opportunities and models},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1362-x},
doi = {10.1007/s10586-017-1362-x},
abstract = {Smart buildings equipped with state-of-the-art sensors and meters are becoming more common. Large quantities of data are being collected by these devices. For a single building to benefit from its own collected data, it will need to wait for a long time to collect sufficient data to build accurate models to help improve the smart buildings systems. Therefore, multiple buildings need to cooperate to amplify the benefits from the collected data and speed up the model building processes. Apparently, this is not so trivial and there are associated challenges. In this paper, we study the importance of collaborative data analytics for smart buildings, its benefits, as well as presently possible models of carrying it out. Furthermore, we present a framework for collaborative fault detection and diagnosis as a case of collaborative data analytics for smart buildings. We also provide a preliminary analysis of the energy efficiency benefit of such collaborative framework for smart buildings. The result shows that significant energy savings can be achieved for smart buildings using collaborative data analytics.},
journal = {Cluster Computing},
month = jan,
pages = {1065–1077},
numpages = {13},
keywords = {Fault detection and diagnosis, Energy efficiency, Models, Collaborative data analytics, Smart buildings}
}

@article{10.1016/j.jss.2015.09.001,
title = {Qualitative optimization in software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.001},
doi = {10.1016/j.jss.2015.09.001},
abstract = {Some problems in software optimization involve qualitative (subjective) human input.Quantitative optimization uses heuristics/surrogates to quantify qualitative input.Qualitative optimization allows incomparability among solutions to find optimal one.Qualitative preference reasoning languages developed in AI are potentially useful.SBSE &amp; recommender systems can readily leverage such qualitative formalisms. Many software engineering problems involve finding optimal solutions from a set of feasible solutions. Such methods often require stakeholders such as developers and testers to specify preferences over multiple attributes/objectives that are to be optimized. However, in many cases it is more natural for stakeholders to express such preferences in simple, qualitative terms. We survey relevant literature within software engineering for problems in which qualitative optimization techniques can be useful. We also present a model of optimization that relies on the stakeholders qualitative preferences leveraging recent advances in decision theoretic artificial intelligence, which could prove useful and spawn connections between qualitative decision theory and software engineering.},
journal = {J. Syst. Softw.},
month = jan,
pages = {149–156},
numpages = {8}
}

@article{10.1145/3375572.3375582,
author = {Fontana, Francesca Arcelli and Perrouin, Gilles and Ampatzoglou, Apostolos and Archer, Mathieu and Walter, Bartosz and Cordy, Maxime and Palomba, Fabio and Devroey, Xavier},
title = {MALTESQUE 2019 Workshop Summary},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3375572.3375582},
doi = {10.1145/3375572.3375582},
abstract = {Welcome to the third edition of the workshop on Machine Learn- ing Techniques for Software Quality Evaluation (MaLTeSQuE 2019), held in Tallinn, Estonia, August 27th, 2019, co-located with ESEC / FSE 2019. This year MALTESQUE merged with the MASES (Machine Learning and Software Engineering in Symbiosis) work- shop, co-located with the ASE 2018 conference. Ten papers from all over the world were submitted, seven of them were accepted. The program also featured a keynote by Lionel Briand on the use of machine learning to improve software testing.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {34–35},
numpages = {2}
}

@article{10.1007/s10515-017-0226-1,
author = {Dilshener, Tezcan and Wermelinger, Michel and Yu, Yijun},
title = {Locating bugs without looking back},
year = {2018},
issue_date = {September 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0226-1},
doi = {10.1007/s10515-017-0226-1},
abstract = {Bug localisation is a core program comprehension task in software maintenance: given the observation of a bug, e.g. via a bug report, where is it located in the source code? Information retrieval (IR) approaches see the bug report as the query, and the source code files as the documents to be retrieved, ranked by relevance. Such approaches have the advantage of not requiring expensive static or dynamic analysis of the code. However, current state-of-the-art IR approaches rely on project history, in particular previously fixed bugs or previous versions of the source code. We present a novel approach that directly scores each current file against the given report, thus not requiring past code and reports. The scoring method is based on heuristics identified through manual inspection of a small sample of bug reports. We compare our approach to eight others, using their own five metrics on their own six open source projects. Out of 30 performance indicators, we improve 27 and equal 2. Over the projects analysed, on average we find one or more affected files in the top 10 ranked files for 76% of the bug reports. These results show the applicability of our approach to software projects without history.},
journal = {Automated Software Engg.},
month = sep,
pages = {383–434},
numpages = {52},
keywords = {Information retrieval, Empirical study, Bug localisation}
}

@article{10.1016/j.knosys.2009.05.001,
author = {Kultur, Yigit and Turhan, Burak and Bener, Ayse},
title = {Ensemble of neural networks with associative memory (ENNA) for estimating software development costs},
year = {2009},
issue_date = {August, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {22},
number = {6},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2009.05.001},
doi = {10.1016/j.knosys.2009.05.001},
abstract = {Companies usually have limited amount of data for effort estimation. Machine learning methods have been preferred over parametric models due to their flexibility to calibrate the model for the available data. On the other hand, as machine learning methods become more complex, they need more data to learn from. Therefore the challenge is to increase the performance of the algorithm when there is limited data. In this paper, we use a relatively complex machine learning algorithm, neural networks, and show that stable and accurate estimations are achievable with an ensemble using associative memory. Our experimental results show that our proposed algorithm (ENNA) produces significantly better results than neural network (NN) in terms of accuracy and robustness. We also analyze the effect of feature subset selection on ENNA's estimation performance in a wrapper framework. We show that the proposed ENNA algorithm that use the features selected by the wrapper does not perform worse than those that use all available features. Therefore, measuring only company specific key factors is sufficient to obtain accurate and robust estimates about software cost estimation using ENNA.},
journal = {Know.-Based Syst.},
month = aug,
pages = {395–402},
numpages = {8},
keywords = {Wrapper, Software cost estimation, Neural network, Ensemble, Associative memory, Adaptive resonance theory}
}

@inproceedings{10.1145/3183440.3183487,
author = {Saha, Ripon K. and Yoshida, Hiroaki and Prasad, Mukul R. and Tokumoto, Susumu and Takayama, Kuniharu and Nanba, Isao},
title = {Elixir: an automated repair tool for Java programs},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183487},
doi = {10.1145/3183440.3183487},
abstract = {Object-oriented (OO) languages, by design, make heavy use of method invocations (MI). Unsurprisingly, a large fraction of OO-program bug patches also involve method invocations. However, current program repair techniques incorporate MIs in very limited ways, ostensibly to avoid searching the huge repair space that method invocations afford. To address this challenge, in previous work, we proposed a generate-and-validate repair technique which can effectively synthesize patches from a repair space rich in method invocation expressions, by using a machine-learned model to rank the space of concrete repairs. In this paper we describe the tool Elixir that instantiates this technique for the repair of Java programs. We describe the architecture, user-interface, and salient features of Elixir, and specific use-cases it can be applied in. We also report on our efforts towards practical deployment of Elixir within our organization, including the initial results of a trial of Elixir on a project of interest to potential customers. A video demonstrating Elixir is available at: https://elixir-tool.github.io/demo-video.html},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {77–80},
numpages = {4},
keywords = {OOP, automatic program repair, machine learning},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2962695.2962710,
author = {Nassar, Bashar and Shahrokni, Ali and Scandariato, Riccardo},
title = {Traceability Data in Early Development Phases as an Enabler for Decision Support},
year = {2016},
isbn = {9781450341349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2962695.2962710},
doi = {10.1145/2962695.2962710},
abstract = {Traceability information between requirements, architectural elements and the results of test cases can be used to unearth interesting relationships between the early phases of the software development process and the software faults in the end product. For instance, complex dependencies between features and software components could lead to an increased level of flaws in the code. Such patterns can be detected and visualized as early warnings to the relevant stakeholders (e.g., the architect or the project manager). Ultimately, a fully-fledged prediction model can be developed if enough historical information is available from previous software projects. In this paper we introduce a method for building a decision support system based on historic product data.},
booktitle = {Proceedings of the Scientific Workshop Proceedings of XP2016},
articleno = {15},
numpages = {4},
keywords = {Traceability link, Test, Requirement, Fault prediction, Early development phases, Architecture},
location = {Edinburgh, Scotland, UK},
series = {XP '16 Workshops}
}

@article{10.1007/s11265-016-1219-1,
author = {Eddine, Benrachou Djamel and Dos Santos, Filipe Neves and Boulebtateche, Brahim and Bensaoula, Salah},
title = {EyeLSD a Robust Approach for Eye Localization and State Detection},
year = {2018},
issue_date = {January   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {90},
number = {1},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-016-1219-1},
doi = {10.1007/s11265-016-1219-1},
abstract = {Improving the safety of public roads and industrial factories requires more reliable and robust computer vision-based approaches for monitoring the eye state (open or closed) of human operators. Getting this information in real time when humans are driving cars or using hazardous machinery will help to prevent accidents and deaths. This paper proposes a new framework called EyeLSD to localize the eyes and detect their states without face detection step. For EyeLSD aims, two novel descriptors are proposed: enhanced Pyramidal Local Binary Pattern Histogram (ePLBPH) and Multi-Three-Patch LBP histogram (Multi-TPLBP). The performance of EyeLSD with ePLBPH and Multi-TPLBP is evaluated and compared against other approaches. For this evaluation three independent and public datasets were used: BioID, CAS-PEAL-R1 and ZJU datasets. The set EyeLSD, ePLBPH and Multi-TPLBP have a greater performance when compared against the state-of-the-art algorithms. The proposed approach is very stable under large range of eye appearances caused by expression, rotation, lighting, head pose, and occlusion.},
journal = {J. Signal Process. Syst.},
month = jan,
pages = {99–125},
numpages = {27},
keywords = {Machine learning, Image processing, Eye state measurement, Eye localization}
}

@inproceedings{10.1145/3468264.3468539,
author = {Rabin, Md Rafiqul Islam and Hellendoorn, Vincent J. and Alipour, Mohammad Amin},
title = {Understanding neural code intelligence through program simplification},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468539},
doi = {10.1145/3468264.3468539},
abstract = {A wide range of code intelligence (CI) tools, powered by deep neural networks, have been developed recently to improve programming productivity and perform program analysis. To reliably use such tools, developers often need to reason about the behavior of the underlying models and the factors that affect them. This is especially challenging for tools backed by deep neural networks. Various methods have tried to reduce this opacity in the vein of "transparent/interpretable-AI". However, these approaches are often specific to a particular set of network architectures, even requiring access to the network's parameters. This makes them difficult to use for the average programmer, which hinders the reliable adoption of neural CI systems. In this paper, we propose a simple, model-agnostic approach to identify critical input features for models in CI systems, by drawing on software debugging research, specifically delta debugging. Our approach, SIVAND, uses simplification techniques that reduce the size of input programs of a CI model while preserving the predictions of the model. We show that this approach yields remarkably small outputs and is broadly applicable across many model architectures and problem domains. We find that the models in our experiments often rely heavily on just a few syntactic features in input programs. We believe that SIVAND's extracted features may help understand neural CI systems' predictions and learned behavior.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {441–452},
numpages = {12},
keywords = {Program Simplification, Models of Code, Interpretable AI},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3196321.3196334,
author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
title = {Deep code comment generation},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196334},
doi = {10.1145/3196321.3196334},
abstract = {During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {200–210},
numpages = {11},
keywords = {comment generation, deep learning, program comprehension},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@article{10.1016/j.infsof.2019.07.009,
author = {Gomes, Luiz Alberto Ferreira and Torres, Ricardo da Silva and C\^{o}rtes, Mario L\'{u}cio},
title = {Bug report severity level prediction in open source software: A survey and research opportunities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.009},
doi = {10.1016/j.infsof.2019.07.009},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {58–78},
numpages = {21},
keywords = {Machine learning, Systematic mapping, Software repositories, Severity level prediction, Bug reports, Bug tracking systems, Software maintenance}
}

@article{10.1504/IJCAT.2015.071417,
author = {Chandra, Pravin and Singh, R. K. and Singh, Yogesh},
title = {Review and extension of fault class hierarchy for testing Boolean specification},
year = {2015},
issue_date = {August 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {52},
number = {1},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2015.071417},
doi = {10.1504/IJCAT.2015.071417},
abstract = {Fault hierarchy specifies the inter-relationships amongst various fault classes in terms of their fault detection capability. Kuhn has developed a fault hierarchy for Boolean expression in disjunctive normal form which was complemented by Tsuchiya and Kikuno. Lau and Yu extended the fault hierarchy by adding more fault classes in the hierarchy. In this paper, we give the fault detection criteria for clause disjunction fault CDF and associative shift fault ASF and further extend the fault hierarchy by adding these fault classes in the fault hierarchy.},
journal = {Int. J. Comput. Appl. Technol.},
month = aug,
pages = {29–38},
numpages = {10}
}

@inproceedings{10.1109/IWSM.Mensura.2014.12,
author = {Wetzlmaier, Thomas and Klammer, Claus and Ramler, Rudolf},
title = {Extracting Dependencies from Software Changes: An Industry Experience Report},
year = {2014},
isbn = {9781479941742},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IWSM.Mensura.2014.12},
doi = {10.1109/IWSM.Mensura.2014.12},
abstract = {Retrieving and analyzing information from software repositories and detecting dependencies are important tasks supporting software evolution. Dependency information is used for change impact analysis, defect prediction as well as cohesion and coupling measurement. In this paper we report our experience from extracting dependency information from the change history of a commercial software system. We analyzed the software system's evolution of about six years, from the start of development to the transition to product releases and maintenance. Analyzing the co-evolution of software artifacts allows detecting logical dependencies between system parts implemented with heterogeneous technologies as well as between different types of development artifacts such as source code, data models or documentation. However, the quality of the extracted dependencies relies on established development practices and conformance to a defined change process. In this paper we indicate resulting limitations and recommend further processing and filtering steps to prepare the dependency data for subsequent analysis and measurement activities.},
booktitle = {Proceedings of the 2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement},
pages = {163–168},
numpages = {6},
keywords = {mining software repositories, logical coupling, dependency analysis, change history},
series = {IWSM-MENSURA '14}
}

@article{10.1016/j.eswa.2013.05.044,
author = {L\'{o}Pez-Chau, Asdr\'{u}Bal and Cervantes, Jair and L\'{o}Pez-Garc\'{\i}A, Lourdes and Lamont, Farid Garc\'{\i}A},
title = {Fisher's decision tree},
year = {2013},
issue_date = {November, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {16},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.05.044},
doi = {10.1016/j.eswa.2013.05.044},
abstract = {Univariate decision trees are classifiers currently used in many data mining applications. This classifier discovers partitions in the input space via hyperplanes that are orthogonal to the axes of attributes, producing a model that can be understood by human experts. One disadvantage of univariate decision trees is that they produce complex and inaccurate models when decision boundaries are not orthogonal to axes. In this paper we introduce the Fisher's Tree, it is a classifier that takes advantage of dimensionality reduction of Fisher's linear discriminant and uses the decomposition strategy of decision trees, to come up with an oblique decision tree. Our proposal generates an artificial attribute that is used to split the data in a recursive way. The Fisher's decision tree induces oblique trees whose accuracy, size, number of leaves and training time are competitive with respect to other decision trees reported in the literature. We use more than ten public available data sets to demonstrate the effectiveness of our method.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6283–6291},
numpages = {9},
keywords = {Oblique decision tree, Fisher's linear discriminant, C4.5}
}

@article{10.1007/s10664-013-9291-7,
author = {Jabangwe, Ronald and B\"{o}rstler, J\"{u}rgen and Smite, Darja and Wohlin, Claes},
title = {Empirical evidence on the link between object-oriented measures and external quality attributes: a systematic literature review},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9291-7},
doi = {10.1007/s10664-013-9291-7},
abstract = {There is a plethora of studies investigating object-oriented measures and their link with external quality attributes, but usefulness of the measures may differ across empirical studies. This study aims to aggregate and identify useful object-oriented measures, specifically those obtainable from the source code of object-oriented systems that have gone through such empirical evaluation. By conducting a systematic literature review, 99 primary studies were identified and traced to four external quality attributes: reliability, maintainability, effectiveness and functionality. A vote-counting approach was used to investigate the link between object-oriented measures and the attributes, and to also assess the consistency of the relation reported across empirical studies. Most of the studies investigate links between object-oriented measures and proxies for reliability attributes, followed by proxies for maintainability. The least investigated attributes were: effectiveness and functionality. Measures from the C&amp;K measurement suite were the most popular across studies. Vote-counting results suggest that complexity, cohesion, size and coupling measures have a better link with reliability and maintainability than inheritance measures. However, inheritance measures should not be overlooked during quality assessment initiatives; their link with reliability and maintainability could be context dependent. There were too few studies traced to effectiveness and functionality attributes; thus a meaningful vote-counting analysis could not be conducted for these attributes. Thus, there is a need for diversification of quality attributes investigated in empirical studies. This would help with identifying useful measures during quality assessment initiatives, and not just for reliability and maintainability aspects.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {640–693},
numpages = {54},
keywords = {Systematic literature review, Static analysis, Source code measures, Source code analysis, Software quality, Software metrics, Object-oriented system}
}

@article{10.1155/2020/8814394,
author = {Zhang, Hao and Zhang, Jie and Shi, Ke and Wang, Hui and M\'{e}ndez-P\'{e}rez, Juan-Albino},
title = {Applying Software Metrics to RNN for Early Reliability Evaluation},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1687-5249},
url = {https://doi.org/10.1155/2020/8814394},
doi = {10.1155/2020/8814394},
abstract = {Structural modeling is an important branch of software reliability modeling. It works in the early reliability engineering to optimize the architecture design and guide the later testing. Compared with traditional models using test data, structural models are often difficult to be applied due to lack of actual data. A software metrics-based method is presented here for empirical studies. The recurrent neural network (RNN) is used to process the metric data to identify defeat-prone code blocks, and a specified aggregation scheme is used to calculate the module reliability. Based on this, a framework is proposed to evaluate overall reliability for actual projects, in which algebraic tools are introduced to build the structural reliability model automatically and accurately. Studies in two open-source projects show that early evaluation results based on this framework are effective and the related methods have good applicability.},
journal = {J. Control Sci. Eng.},
month = jan,
numpages = {10}
}

@article{10.1016/j.jss.2021.111062,
author = {Luu, Quang-Hung and Lau, Man F. and Ng, Sebastian P.H. and Chen, Tsong Yueh},
title = {Testing multiple linear regression systems with metamorphic testing},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111062},
doi = {10.1016/j.jss.2021.111062},
journal = {J. Syst. Softw.},
month = dec,
numpages = {21},
keywords = {Metamorphic relation, Metamorphic testing, Multiple linear regression}
}

@inproceedings{10.1007/978-3-642-39038-8_25,
author = {Hao, Dan and Lan, Tian and Zhang, Hongyu and Guo, Chao and Zhang, Lu},
title = {Is this a bug or an obsolete test?},
year = {2013},
isbn = {9783642390371},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39038-8_25},
doi = {10.1007/978-3-642-39038-8_25},
abstract = {In software evolution, developers typically need to identify whether the failure of a test is due to a bug in the source code under test or the obsoleteness of the test code when they execute a test suite. Only after finding the cause of a failure can developers determine whether to fix the bug or repair the obsolete test. Researchers have proposed several techniques to automate test repair. However, test-repair techniques typically assume that test failures are always due to obsolete tests. Thus, such techniques may not be applicable in real world software evolution when developers do not know whether the failure is due to a bug or an obsolete test. To know whether the cause of a test failure lies in the source code under test or in the test code, we view this problem as a classification problem and propose an automatic approach based on machine learning. Specifically, we target Java software using the JUnit testing framework and collect a set of features that may be related to failures of tests. Using this set of features, we adopt the Best-first Decision Tree Learning algorithm to train a classifier with some existing regression test failures as training instances. Then, we use the classifier to classify future failed tests. Furthermore, we evaluated our approach using two Java programs in three scenarios (within the same version, within different versions of a program, and between different programs), and found that our approach can effectively classify the causes of failed tests.},
booktitle = {Proceedings of the 27th European Conference on Object-Oriented Programming},
pages = {602–628},
numpages = {27},
location = {Montpellier, France},
series = {ECOOP'13}
}

@article{10.4018/IJITSA.290003,
author = {Gagnon, St\'{e}phane and Andriamaharosoa, Sylvia and Valverde, Raul},
title = {Detecting the Causal Structure of Risk in Industrial Systems by Using Dynamic Bayesian Networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {1},
issn = {1935-570X},
url = {https://doi.org/10.4018/IJITSA.290003},
doi = {10.4018/IJITSA.290003},
abstract = {Our study deals with detecting the causal structure of risk in industrial systems. We focus on the prioritization of risks in the form of correlated events sequences. To improve existing prioritization methods, we propose a new methodology using Dynamic Bayesian networks (DBN). We explore a new user interface for industrial control systems and data acquisition, known as Supervisory Control and Data Acquisition (SCADA), to demonstrate the analysis method of risk causal structure. Our results show that: (1) the network of variables before and after the failure is represented by a limited and distinct number of factors;(2) the network of variables before and after the failure can be graphically represented dynamically in a user interface to assist in fault prevention and diagnosis;(3) variables related to the sequence of events at the time of failure can be used as a model to predict its occurrence, and find the main cause of it, thus making it possible to prioritize the requirements of the production system on the right variables to be monitored and manage in the event of a breakdown},
journal = {Int. J. Inf. Technol. Syst. Appoach},
month = nov,
pages = {1–22},
numpages = {22},
keywords = {Machine Learning, Industrial System, Dynamic Bayesian Network}
}

@inproceedings{10.5555/3370272.3370296,
author = {Sabor, Korosh K. and Hamou-Lhadj, Abdelwahab and Trabelsi, Abdelaziz and Hassine, Jameleddine},
title = {Predicting bug report fields using stack traces and categorical attributes},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {Studies have shown that the lack of information about a bug often delays the bug report (BR) resolution process. Existing approaches rely mainly on BR descriptions as the main features for predicting BR fields. BR descriptions, however, tend to be informal and not always reliable. In this study, we show that the use of stack traces, a more formal source, and categorical features of BRs provides better accuracy than BR descriptions. We focus on the prediction of faulty components and products, two important BR fields, often used by developers to investigate a bug. Our method relies on mining historical BRs in order to predict faulty components and products of new incoming bugs. We map stack traces of historical BRs to feature vectors, weighted using TF-IDF. The vectors, together with a selected set of BR categorical information, are then fed to a classification algorithm. The method also tackles the problem of unbalanced data. Our approach achieves an average accuracy of 58% (when predicting faulty components) and 60% (when predicting faulty products) on Eclipse dataset and 70% (when predicting faulty components) and 70% (when predicting faulty products) on Gnome dataset. For both datasets, our approach improves over the method that uses BR descriptions by a large margin, up to an average of 46%.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {224–233},
numpages = {10},
keywords = {software maintenance and evolution, software bugs reports, mining software repositories, machine learning},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/3468264.3473922,
author = {Haas, Roman and Elsner, Daniel and Juergens, Elmar and Pretschner, Alexander and Apel, Sven},
title = {How can manual testing processes be optimized? developer survey, optimization guidelines, and case studies},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473922},
doi = {10.1145/3468264.3473922},
abstract = {Manual software testing is tedious and costly as it involves significant human effort. Yet, it is still widely applied in industry and will be in the foreseeable future. Although there is arguably a great need for optimization of manual testing processes, research focuses mostly on optimization techniques for automated tests. Accordingly, there is no precise understanding of the practices and processes of manual testing in industry nor about pitfalls and optimization potential that is untapped. To shed light on this issue, we conducted a survey among 38 testing professionals from 16 companies, to investigate their manual testing processes and to identify potential for optimization. We synthesize guidelines when optimization techniques from automated testing can be implemented for manual testing. By means of case studies on two industrial software projects, we show that fault detection likelihood, test feedback time and test creation efforts can be improved when following our guidelines.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1281–1291},
numpages = {11},
keywords = {test optimization, manual testing, Software testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1504/ijitst.2020.109535,
author = {Viswanathan, Ganesh and Prabhu, J.},
title = {Survey of methodologies for quantifying software reliability},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {5},
issn = {1748-569X},
url = {https://doi.org/10.1504/ijitst.2020.109535},
doi = {10.1504/ijitst.2020.109535},
abstract = {An important problem that arises in the testing of software programs is that given piece of source code is reliable or not. Software reliability is an important segment of software quality. So software reliability must be quantified. Quantification refers to measurement. A number of software metrics and statistical reliability models have emerged during the past four decades but no model can solve the issue. In this paper, we conduct a survey on various software reliability metrics and models.},
journal = {Int. J. Internet Technol. Secur. Syst.},
month = jan,
pages = {565–575},
numpages = {10},
keywords = {survey of software reliability quantification, software reliability assessment, software reliability metrics, software reliability}
}

@inproceedings{10.1145/3387940.3392236,
author = {Anvik, John and Galappaththi, Akalanka},
title = {Are Automatic Bug Report Summarizers Missing the Target?},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392236},
doi = {10.1145/3387940.3392236},
abstract = {Bug reports can be lengthy due to long descriptions and long conversation threads. Automatic summarization of the text in a bug report can reduce the time spent by software project members on understanding the content of a bug report. Quality of the bug report summaries have been historically evaluated using human-created gold-standard summaries. However, we believe this is not a good practice for two reasons. First, we observed high disagreement levels in the annotated summaries and the number of annotators to create gold-standard summaries was lower than the established value for stable annotation. We believe that creating a fixed summary length of 25% of the word count of the corresponding bug report is not suitable for every time when a person refers to a bug report. Therefore, we propose an automatic sentence annotation method and an interface to customize the presented summary.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {149–152},
numpages = {4},
keywords = {software bug reports, human annotation, gold-standard summary, bug report summarization},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1007/s10664-018-9656-z,
author = {Blincoe, Kelly and Dehghan, Ali and Salaou, Abdoul-Djawadou and Neal, Adam and Linaker, Johan and Damian, Daniela},
title = {High-level software requirements and iteration changes: a predictive model},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9656-z},
doi = {10.1007/s10664-018-9656-z},
abstract = {Knowing whether a software feature will be completed in its planned iteration can help with release planning decisions. However, existing research has focused on predictions of only low-level software tasks, like bug fixes. In this paper, we describe a mixed-method empirical study on three large IBM projects. We investigated the types of iteration changes that occur. We show that up to 54% of high-level requirements do not make their planned iteration. Requirements are most often pushed out to the next iteration, but high-level requirements are also commonly moved to the next minor or major release or returned to the product or release backlog. We developed and evaluated a model that uses machine learning to predict if a high-level requirement will be completed within its planned iteration. The model includes 29 features that were engineered based on prior work, interviews with IBM developers, and domain knowledge. Predictions were made at four different stages of the requirement lifetime. Our model is able to achieve up to 100% precision. We ranked the importance of our model features and found that some features are highly dependent on project and prediction stage. However, some features (e.g., the time remaining in the iteration and creator of the requirement) emerge as important across all projects and stages. We conclude with a discussion on future research directions.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1610–1648},
numpages = {39},
keywords = {Software requirements, Release planning, Mining software repositories, Machine learning, Completion prediction}
}

@article{10.1007/s10664-020-09866-z,
author = {Liao, Lizhi and Chen, Jinfu and Li, Heng and Zeng, Yi and Shang, Weiyi and Guo, Jianmei and Sporea, Catalin and Toma, Andrei and Sajedi, Sarah},
title = {Using black-box performance models to detect performance regressions under varying workloads: an empirical study},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09866-z},
doi = {10.1007/s10664-020-09866-z},
abstract = {Performance regressions of large-scale software systems often lead to both financial and reputational losses. In order to detect performance regressions, performance tests are typically conducted in an in-house (non-production) environment using test suites with predefined workloads. Then, performance analysis is performed to check whether a software version has a performance regression against an earlier version. However, the real workloads in the field are constantly changing, making it unrealistic to resemble the field workloads in predefined test suites. More importantly, performance testing is usually very expensive as it requires extensive resources and lasts for an extended period. In this work, we leverage black-box machine learning models to automatically detect performance regressions in the field operations of large-scale software systems. Practitioners can leverage our approaches to complement or replace resource-demanding performance tests that may not even be realistic in a fast-paced environment. Our approaches use black-box models to capture the relationship between the performance of a software system (e.g., CPU usage) under varying workloads and the runtime activities that are recorded in the readily-available logs. Then, our approaches compare the black-box models derived from the current software version with an earlier version to detect performance regressions between these two versions. We performed empirical experiments on two open-source systems and applied our approaches on a large-scale industrial system. Our results show that such black-box models can effectively and timely detect real performance regressions and injected ones under varying workloads that are unseen when training these models. Our approaches have been adopted in practice to detect performance regressions of a large-scale industry system on a daily basis.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {4130–4160},
numpages = {31},
keywords = {Performance engineering, Field workloads, Black-box performance models, Performance regression}
}

@inproceedings{10.1145/2901739.2901760,
author = {Layman, Lucas and Nikora, Allen P. and Meek, Joshua and Menzies, Tim},
title = {Topic modeling of NASA space system problem reports: research in practice},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901760},
doi = {10.1145/2901739.2901760},
abstract = {Problem reports at NASA are similar to bug reports: they capture defects found during test, post-launch operational anomalies, and document the investigation and corrective action of the issue. These artifacts are a rich source of lessons learned for NASA, but are expensive to analyze since problem reports are comprised primarily of natural language text. We apply topic modeling to a corpus of NASA problem reports to extract trends in testing and operational failures. We collected 16,669 problem reports from six NASA space flight missions and applied Latent Dirichlet Allocation topic modeling to the document corpus. We analyze the most popular topics within and across missions, and how popular topics changed over the lifetime of a mission. We find that hardware material and flight software issues are common during the integration and testing phase, while ground station software and equipment issues are more common during the operations phase. We identify a number of challenges in topic modeling for trend analysis: 1) that the process of selecting the topic modeling parameters lacks definitive guidance, 2) defining semantically-meaningful topic labels requires non-trivial effort and domain expertise, 3) topic models derived from the combined corpus of the six missions were biased toward the larger missions, and 4) topics must be semantically distinct as well as cohesive to be useful. Nonetheless, topic modeling can identify problem themes within missions and across mission lifetimes, providing useful feedback to engineers and project managers.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {303–314},
numpages = {12},
keywords = {topic modeling, natural language processing, defects, data mining, LDA},
location = {Austin, Texas},
series = {MSR '16}
}

@article{10.5555/3292734.3292735,
title = {A deep learning approach to software evolution},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {58},
number = {3},
issn = {0952-8091},
abstract = {Software evolution techniques should be made as important as software development techniques. One possible way to help with the situation is to learn from software development, along with learning from software evolution techniques. The breakout of Machine Learning and Deep Learning ML&amp;DL is becoming popular in technology and should be studied for being made available for servicing software evolution. Open source projects provide an open defect repository to which users and developers can report bugs. It is a challenge to document bug reports to the appropriate developers. In this paper, we apply deep learning approaches and a topic model to learn the features of defect reports and then make recommendations. Compared to the traditional machine learning approaches, the proposed approach based on deep learning can perform better in accuracy and assign defect reports to developers more effectively and correctly along with the dataset increasing.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {175–183},
numpages = {9}
}

@inproceedings{10.5555/2486788.2486846,
author = {Rahman, Foyzur and Devanbu, Premkumar},
title = {How, and why, process metrics are better},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they dont change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {432–441},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3230467.3230477,
author = {Shatnawi, Raed},
title = {Identifying Threshold Values of Change-Prone Modules},
year = {2018},
isbn = {9781450364300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230467.3230477},
doi = {10.1145/3230467.3230477},
abstract = {Software changes frequently during the lifetime of a project. These changes increase the total cost of software development. Knowing where changes are more likely help in allocating appropriate resources in different activities of software lifecycle. The change-proneness of modules is defined as likelihood of change in a module. Change-proneness can be predicted using software metrics such as Chidamber and Kemerer metrics. However, the use of prediction models requires knowledge in advanced techniques such as data mining and regression analysis. Hence, there is a need to a more direct and simple technique to have more information about change-prone modules. Threshold values are proposed as such technique to identify the modules that are more change-prone. We propose the ROC analysis to identify thresholds. The ROC analysis suggests that large values of software metrics are indicators of change proneness. The research could identify practical thresholds for four metrics, CBO, RFC, WMC and the LCOM. The inheritance metrics do not have practical thresholds.In this paper, we describe the formatting guidelines for ACM SIG Proceedings.},
booktitle = {Proceedings of the 2018 International Conference on E-Business and Mobile Commerce},
pages = {39–43},
numpages = {5},
keywords = {product metrics, change-prone metrics, ROC curves, CK metrics},
location = {Chengdu, China},
series = {ICEMC '18}
}

@inproceedings{10.1145/3345629.3345631,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Which Refactoring Reduces Bug Rate?},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345631},
doi = {10.1145/3345629.3345631},
abstract = {We present a methodology to identify refactoring operations that reduce the bug rate in the code. The methodology is based on comparing the bug fixing rate in certain time windows before and after the refactoring. We analyzed 61,331 refactor commits from 1,531 large active GitHub projects. When comparing three-month windows, the bug rate is substantially reduced in 17% of the files of analyzed refactors, compared to 12% of the files in random commits. Within this group, implementing 'todo's provides the most benefits. Certain operations like reuse, upgrade, and using enum and namespaces are also especially beneficial.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–15},
numpages = {4},
keywords = {Code quality, machine learning, refactoring},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@article{10.1016/j.eswa.2017.05.069,
author = {Mrquez-Chamorro, Alfonso E. and Resinas, Manuel and Ruiz-Corts, Antonio and Toro, Miguel},
title = {Run-time prediction of business process indicators using evolutionary decision rules},
year = {2017},
issue_date = {November 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {87},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.05.069},
doi = {10.1016/j.eswa.2017.05.069},
abstract = {An evolutionary rule-based system for the prediction of BP indicators is proposed.Generated decision rules can be easily interpreted by users.A software stack to support the stages of a predictive monitoring system is presented. Predictive monitoring of business processes is a challenging topic of process mining which is concerned with the prediction of process indicators of running process instances. The main value of predictive monitoring is to provide information in order to take proactive and corrective actions to improve process performance and mitigate risks in real time. In this paper, we present an approach for predictive monitoring based on the use of evolutionary algorithms. Our method provides a novel event window-based encoding and generates a set of decision rules for the run-time prediction of process indicators according to event log properties. These rules can be interpreted by users to extract further insight of the business processes while keeping a high level of accuracy. Furthermore, a full software stack consisting of a tool to support the training phase and a framework that enables the integration of run-time predictions with business process management systems, has been developed. Obtained results show the validity of our proposal for two large real-life datasets: BPI Challenge 2013 and IT Department of Andalusian Health Service (SAS).},
journal = {Expert Syst. Appl.},
month = nov,
pages = {1–14},
numpages = {14},
keywords = {Process mining, Predictive monitoring, Evolutionary algorithm, Business process management, Business process indicator}
}

@inproceedings{10.1007/978-3-642-05415-0_18,
author = {Ferzund, Javed and Ahsan, Syed Nadeem and Wotawa, Franz},
title = {Empirical Evaluation of Hunk Metrics as Bug Predictors},
year = {2009},
isbn = {9783642054143},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-05415-0_18},
doi = {10.1007/978-3-642-05415-0_18},
abstract = {Reducing the number of bugs is a crucial issue during software development and maintenance. Software process and product metrics are good indicators of software complexity. These metrics have been used to build bug predictor models to help developers maintain the quality of software. In this paper we empirically evaluate the use of hunk metrics as predictor of bugs. We present a technique for bug prediction that works at smallest units of code change called hunks. We build bug prediction models using random forests, which is an efficient machine learning classifier. Hunk metrics are used to train the classifier and each hunk metric is evaluated for its bug prediction capabilities. Our classifier can classify individual hunks as buggy or bug-free with 86 % accuracy, 83 % buggy hunk precision and 77% buggy hunk recall. We find that history based and change level hunk metrics are better predictors of bugs than code level hunk metrics.},
booktitle = {Proceedings of the International Conferences on Software Process and Product Measurement},
pages = {242–254},
numpages = {13},
keywords = {software faults, hunk metrics, empirical software engineering, code metrics, Bug predictor},
location = {Amsterdam, The Netherlands},
series = {IWSM '09 /Mensura '09}
}

@inproceedings{10.5555/3540261.3540956,
author = {Franzen, Daniel and Wand, Michael},
title = {General nonlinearities in SO(2)-equivariant CNNs},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Invariance under symmetry is an important problem in machine learning. Our paper looks specifically at equivariant neural networks where transformations of inputs yield homomorphic transformations of outputs. Here, steerable CNNs have emerged as the standard solution. An inherent problem of steerable representations is that general nonlinear layers break equivariance, thus restricting architectural choices. Our paper applies harmonic distortion analysis to illuminate the effect of nonlinearities on Fourier representations of SO(2). We develop a novel FFT-based algorithm for computing representations of non-linearly transformed activations while maintaining band-limitation. It yields exact equivariance for polynomial (approximations of) nonlinearities, as well as approximate solutions with tunable accuracy for general functions. We apply the approach to build a fully E(3)-equivariant network for sampled 3D surface data. In experiments with 2D and 3D data, we obtain results that compare favorably to the state-of-the-art in terms of accuracy while permitting continuous symmetry and exact equivariance.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {695},
numpages = {13},
series = {NIPS '21}
}

@article{10.1145/3360588,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N. and Van Nguyen, Son},
title = {Improving bug detection via context-based code representation learning and attention-based neural networks},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360588},
doi = {10.1145/3360588},
abstract = {Bug detection has been shown to be an effective way to help developers in detecting bugs early, thus, saving much effort and time in software development process. Recently, deep learning-based bug detection approaches have gained successes over the traditional machine learning-based approaches, the rule-based program analysis approaches, and mining-based approaches. However, they are still limited in detecting bugs that involve multiple methods and suffer high rate of false positives. In this paper, we propose a combination approach with the use of contexts and attention neural network to overcome those limitations. We propose to use as the global context the Program Dependence Graph (PDG) and Data Flow Graph (DFG) to connect the method under investigation with the other relevant methods that might contribute to the buggy code. The global context is complemented by the local context extracted from the path on the AST built from the method’s body. The use of PDG and DFG enables our model to reduce the false positive rate, while to complement for the potential reduction in recall, we make use of the attention neural network mechanism to put more weights on the buggy paths in the source code. That is, the paths that are similar to the buggy paths will be ranked higher, thus, improving the recall of our model. We have conducted several experiments to evaluate our approach on a very large dataset with +4.973M methods in 92 different project versions. The results show that our tool can have a relative improvement up to 160% on F-score when comparing with the state-of-the-art bug detection approaches. Our tool can detect 48 true bugs in the list of top 100 reported bugs, which is 24 more true bugs when comparing with the baseline approaches. We also reported that our representation is better suitable for bug detection and relatively improves over the other representations up to 206% in accuracy.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {162},
numpages = {30},
keywords = {Program Graphs, Network Embedding, Deep Learning, Code Representation Learning, Bug Detection, Attention Neural Networks}
}

@inproceedings{10.1109/MSR.2017.56,
author = {Tiwari, Nitin M and Upadhyaya, Ganesha and Nguyen, Hoan Anh and Rajan, Hridesh},
title = {Candoia: a platform for building and sharing mining software repositories tools as apps},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.56},
doi = {10.1109/MSR.2017.56},
abstract = {We propose Candoia, a novel platform and ecosystem for building and sharing Mining Software Repositories (MSR) tools. Using Candoia, MSR tools are built as apps, and Candoia ecosystem, acting as an appstore, allows effective sharing. Candoia platform provides, data extraction tools for curating custom datasets for user projects, and data abstractions for enabling uniform access to MSR artifacts from disparate sources, which makes apps portable and adoptable across diverse software project settings of MSR researchers and practitioners. The structured design of a Candoia app and the languages selected for building various components of a Candoia app promotes easy customization. To evaluate Candoia we have built over two dozen MSR apps for analyzing bugs, software evolution, project management aspects, and source code and programming practices showing the applicability of the platform for building a variety of MSR apps. For testing portability of apps across diverse project settings, we tested the apps using ten popular project repositories, such as Apache Tomcat, JUnit, Node.js, etc, and found that apps required no changes to be portable. We performed a user study to test customizability and we found that five of eight Candoia users found it very easy to customize an existing app. Candoia is available for download.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {53–63},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.1016/j.websem.2010.04.009,
author = {Tappolet, Jonas and Kiefer, Christoph and Bernstein, Abraham},
title = {Semantic web enabled software analysis},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {8},
number = {2–3},
issn = {1570-8268},
url = {https://doi.org/10.1016/j.websem.2010.04.009},
doi = {10.1016/j.websem.2010.04.009},
abstract = {One of the most important decisions researchers face when analyzing software systems is the choice of a proper data analysis/exchange format. In this paper, we present EvoOnt, a set of software ontologies and data exchange formats based on OWL. EvoOnt models software design, release history information, and bug-tracking meta-data. Since OWL describes the semantics of the data, EvoOnt (1) is easily extendible, (2) can be processed with many existing tools, and (3) allows to derive assertions through its inherent Description Logic reasoning capabilities. The contribution of this paper is that it introduces a novel software evolution ontology that vastly simplifies typical software evolution analysis tasks. In detail, we show the usefulness of EvoOnt by repeating selected software evolution and analysis experiments from the 2004-2007 Mining Software Repositories Workshops (MSR). We demonstrate that if the data used for analysis were available in EvoOnt then the analyses in 75% of the papers at MSR could be reduced to one or at most two simple queries within off-the-shelf SPARQL tools. In addition, we present how the inherent capabilities of the Semantic Web have the potential of enabling new tasks that have not yet been addressed by software evolution researchers, e.g., due to the complexities of the data integration.},
journal = {Web Semant.},
month = jul,
pages = {225–240},
numpages = {16},
keywords = {Software release similarity, Software evolution, Software comprehension framework, Bug prediction}
}

@inproceedings{10.1007/978-3-030-63406-3_9,
author = {Khoo, Teck Ping and Sun, Jun and Chattopadhyay, Sudipta},
title = {Learning Fault Models of Cyber Physical Systems},
year = {2020},
isbn = {978-3-030-63405-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63406-3_9},
doi = {10.1007/978-3-030-63406-3_9},
abstract = {Cyber Physical Systems (CPSs) comprise sensors and actuators which interact with the physical environment over a computer network to achieve some control objective. Bugs in CPSs can have severe consequences as CPSs are increasingly deployed in safety-critical applications. Debugging CPSs is therefore an important real world problem. Traces from a CPS can be lengthy and are usually linked to different parts of the system, making debugging CPSs a complex and time-consuming undertaking. It is challenging to isolate a component without running the whole CPS. In this work, we propose a model-based approach to debugging a CPS. For each CPS property, active automata learning is applied to learn a fault model, which is a Deterministic Finite Automata (DFA) of the violation of the property. The L* algorithm (L*) will find a minimum DFA given the queries and counterexamples. Short test cases can then be easily extracted from the DFA and executed on the actual CPS for bug rectification.This is a black-box approach which does not require access to the PLC source code, making it easy to apply in practice. Where source code is available, the bug can be rectified. We demonstrate the ease and effectiveness of this approach by applying it to a commercially supplied miniature lift controlled by a Programmable Logic Controller (PLC). Two bugs were discovered in the supplier code. Both of them were patched with relative ease using the models generated. We then created 20 mutated versions of the patched code and applied our approach to these mutants. Our prototype implementation successfully built at least one model for each mutant corresponding to the property violated, demonstrating its effectiveness.},
booktitle = {Formal Methods and Software Engineering: 22nd International Conference on Formal Engineering Methods, ICFEM 2020, Singapore, Singapore, March 1–3, 2021, Proceedings},
pages = {147–162},
numpages = {16},
keywords = {Programmable logic controllers, L* algorithm, Active automata learning, Debugging},
location = {Singapore, Singapore}
}

@inproceedings{10.1109/ASE.2019.00018,
author = {Yang, Yibiao and Jiang, Yanyan and Zuo, Zhiqiang and Wang, Yang and Sun, Hao and Lu, Hongmin and Zhou, Yuming and Xu, Baowen},
title = {Automatic self-validation for code coverage profilers},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00018},
doi = {10.1109/ASE.2019.00018},
abstract = {Code coverage as the primitive dynamic program behavior information, is widely adopted to facilitate a rich spectrum of software engineering tasks, such as testing, fuzzing, debugging, fault detection, reverse engineering, and program understanding. Thanks to the widespread applications, it is crucial to ensure the reliability of the code coverage profilers.Unfortunately, due to the lack of research attention and the existence of testing oracle problem, coverage profilers are far away from being tested sufficiently. Bugs are still regularly seen in the widely deployed profilers, like gcov and llvm-cov, along with gcc and llvm, respectively.This paper proposes Cod, an automated self-validator for effectively uncovering bugs in the coverage profilers. Starting from a test program (either from a compiler's test suite or generated randomly), Cod detects profiler bugs with zero false positive using a metamorphic relation in which the coverage statistics of that program and a mutated variant are bridged.We evaluated Cod over two of the most well-known code coverage profilers, namely gcov and llvm-cov. Within a four-month testing period, a total of 196 potential bugs (123 for gcov, 73 for llvm-cov) are found, among which 23 are confirmed by the developers.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {79–90},
numpages = {12},
keywords = {metamorphic testing, coverage profilers, code coverage, bug detection},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1007/s11219-017-9392-4,
author = {Patel, Krishna and Hierons, Robert M.},
title = {A mapping study on testing non-testable systems},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9392-4},
doi = {10.1007/s11219-017-9392-4},
abstract = {The terms "Oracle Problem" and "Non-testable system" interchangeably refer to programs in which the application of test oracles is infeasible. Test oracles are an integral part of conventional testing techniques; thus, such techniques are inoperable in these programs. The prevalence of the oracle problem has inspired the research community to develop several automated testing techniques that can detect functional software faults in such programs. These techniques include N-Version testing, Metamorphic Testing, Assertions, Machine Learning Oracles, and Statistical Hypothesis Testing. This paper presents a Mapping Study that covers these techniques. The Mapping Study presents a series of discussions about each technique, from different perspectives, e.g. effectiveness, efficiency, and usability. It also presents a comparative analysis of these techniques in terms of these perspectives. Finally, potential research opportunities within the non-testable systems problem domain are highlighted within the Mapping Study. We believe that the aforementioned discussions and comparative analysis will be invaluable for new researchers that are attempting to familiarise themselves with the field, and be a useful resource for practitioners that are in the process of selecting an appropriate technique for their context, or deciding how to apply their selected technique. We also believe that our own insights, which are embedded throughout these discussions and the comparative analysis, will be useful for researchers that are already accustomed to the field. It is our hope that the potential research opportunities that have been highlighted by the Mapping Study will steer the direction of future research endeavours.},
journal = {Software Quality Journal},
month = dec,
pages = {1373–1413},
numpages = {41},
keywords = {Test oracles, Survey, Software testing, Oracle problem, Non-testable, Mapping study}
}

@inproceedings{10.1007/978-3-030-26250-1_27,
author = {Woehrle, Matthias and Gladisch, Christoph and Heinzemann, Christian},
title = {Open Questions in Testing of Learned Computer Vision Functions for Automated Driving},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_27},
doi = {10.1007/978-3-030-26250-1_27},
abstract = {Vision is an important sensing modality in automated driving. Deep learning-based approaches have gained popularity for different computer vision (CV) tasks such as semantic segmentation and object detection. However, the black-box nature of deep neural nets (DNN) is a challenge for practical software verification. With this paper, we want to initiate a discussion in the academic community about research questions w.r.t. software testing of DNNs for safety-critical CV tasks. To this end, we provide an overview of related work from various domains, including software testing, machine learning and computer vision and derive a set of open research questions to start discussion between the fields.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {333–345},
numpages = {13},
location = {Turku, Finland}
}

@inproceedings{10.1145/3236024.3236060,
author = {Lin, Qingwei and Hsieh, Ken and Dang, Yingnong and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Lou, Jian-Guang and Li, Chenggang and Wu, Youjiang and Yao, Randolph and Chintalapati, Murali and Zhang, Dongmei},
title = {Predicting Node failure in cloud service systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236060},
doi = {10.1145/3236024.3236060},
abstract = {In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {480–490},
numpages = {11},
keywords = {service availability, node failure, maintenance, cloud service systems, Failure prediction},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1504/ijcse.2021.113657,
author = {Ghosh, Debolina and Singh, Jagannath},
title = {A dynamic slicing-based approach for effective SBFL technique},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {24},
number = {1},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2021.113657},
doi = {10.1504/ijcse.2021.113657},
abstract = {Fault finding is an activity to locate the fault or bug present in a software. It is a very time taking job and needs much more effort if done manually. Hence, automated fault localisation is always in high demand which reduces the human effort and also makes the task more accurate. Among different existing debugging techniques, spectrum-based debugging is the most efficient for automated fault localisation. Dynamic program slicing is another technique which can reduce the debugging time by reducing the unaffected source codes depending on slicing criteria. In this paper, we present a spectrum-based fault localisation technique by using dynamic slicing. Context-sensitive slicing is used to diminish fault localisation time and makes the process more effective. SBFL metrics are used in the sliced program to find the suspiciousness score of individual program statements. The efficiency of the proposed approach is evaluated on three open-source programs. From results, we notice that due to dynamic slicing the technique takes less time to find the suspiciousness score of individual statements in the sliced program compared to original program. We have also observed that the programmer needs to inspect less source code to detect the buggy statement. The result says that the proposed approach outperforms the pure spectrum-based fault localisation techniques.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {98–107},
numpages = {9},
keywords = {context-sensitive, Java, statistical formula, spectrum-based fault localisation, program slicing}
}

@inproceedings{10.1007/978-3-030-76352-7_15,
author = {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
title = {A Systematic Mapping Study in AIOps},
year = {2020},
isbn = {978-3-030-76351-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-76352-7_15},
doi = {10.1007/978-3-030-76352-7_15},
abstract = {IT systems of today are becoming larger and more complex, rendering their human supervision more difficult. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to AI and Big Data. However, past AIOps contributions are scattered, unorganized and missing a common terminology convention, which renders their discovery and comparison impractical. In this work, we conduct an in-depth mapping study to collect and organize the numerous scattered contributions to AIOps in a unique reference index. We create an AIOps taxonomy to build a foundation for future contributions and allow an efficient comparison of AIOps papers treating similar problems. We investigate temporal trends and classify AIOps contributions based on the choice of algorithms, data sources and the target components. Our results show a recent and growing interest towards AIOps, specifically to those contributions treating failure-related tasks (62%), such as anomaly detection and root cause analysis.},
booktitle = {Service-Oriented Computing  – ICSOC 2020 Workshops: AIOps, CFTIC, STRAPS, AI-PA, AI-IOTS, and Satellite Events, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {110–123},
numpages = {14},
keywords = {Artificial Intelligence, Operations and Maintenance, AIOps},
location = {Dubai, United Arab Emirates}
}

@inproceedings{10.1145/3196398.3196435,
author = {de P\'{a}dua, Guilherme B. and Shang, Weiyi},
title = {Studying the relationship between exception handling practices and post-release defects},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196435},
doi = {10.1145/3196398.3196435},
abstract = {Modern programming languages, such as Java and C#, typically provide features that handle exceptions. These features separate error-handling code from regular source code and aim to assist in the practice of software comprehension and maintenance. Nevertheless, their misuse can still cause reliability degradation or even catastrophic software failures. Prior studies on exception handling revealed the suboptimal practices of the exception handling flows and the prevalence of their anti-patterns. However, little is known about the relationship between exception handling practices and software quality. In this work, we investigate the relationship between software quality (measured by the probability of having post-release defects) and: (i) exception flow characteristics and (ii) 17 exception handling anti-patterns. We perform a case study on three Java and C# open-source projects. By building statistical models of the probability of post-release defects using traditional software metrics and metrics that are associated with exception handling practice, we study whether exception flow characteristics and exception handling anti-patterns have a statistically significant relationship with post-release defects. We find that exception flow characteristics in Java projects have a significant relationship with post-release defects. In addition, although the majority of the exception handing anti-patterns are not significant in the models, there exist anti-patterns that can provide significant explanatory power to the probability of post-release defects. Therefore, development teams should consider allocating more resources to improving their exception handling practices and avoid the anti-patterns that are found to have a relationship with post-release defects. Our findings also highlight the need for techniques that assist in handling exceptions in the software development practice.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {564–575},
numpages = {12},
keywords = {empirical software engineering, exception handling, software quality},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3387904.3389281,
author = {Zhang, Jinglei and Xie, Rui and Ye, Wei and Zhang, Yuhan and Zhang, Shikun},
title = {Exploiting Code Knowledge Graph for Bug Localization via Bi-directional Attention},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389281},
doi = {10.1145/3387904.3389281},
abstract = {Bug localization automatic localize relevant source files given a natural language description of bug within a software project. For a large project containing hundreds and thousands of source files, developers need cost lots of time to understand bug reports generated by quality assurance and localize these buggy source files. Traditional methods are heavily depending on the information retrieval technologies which rank the similarity between source files and bug reports in lexical level. Recently, deep learning based models are used to extract semantic information of code with significant improvements for bug localization. However, programming language is a highly structural and logical language, which contains various relations within and cross source files. Thus, we propose KGBugLocator to utilize knowledge graph embeddings to extract these interrelations of code, and a keywords supervised bi-directional attention mechanism regularize model with interactive information between source files and bug reports. With extensive experiments on four different projects, we prove our model can reach the new the-state-of-art(SOTA) for bug localization.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {219–229},
numpages = {11},
keywords = {knowledge graph, deep learning, code representation, bug localization},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.4018/IJOSSP.2016010102,
author = {Chahal, Kuljit Kaur and Saini, Munish},
title = {Open Source Software Evolution: A Systematic Literature Review Part 2},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016010102},
doi = {10.4018/IJOSSP.2016010102},
abstract = {This paper presents the results of a systematic literature review conducted to understand the Open Source Software OSS development process on the basis of evidence found in the empirical research studies. The study targets the OSS project evolution research papers to understand the methods and techniques employed for analysing the OSS evolution process. Our results suggest that there is lack of a uniform approach to analyse and interpret the results. The use of prediction techniques that just extrapolate the historic trends into the future should be a conscious task as it is observed that there are no long-term correlations in data of such systems. OSS evolution as a research area is still in nascent stage. Even after a number of empirical studies, the field has failed to establish a theory. There is need to formalize the field as a systematic and formal approach can produce better software.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {28–48},
numpages = {21},
keywords = {Software Reuse, Software Evolution Theory, Programming Languages, OSS Prediction, Co-Evolution, Automation Support, ARIMA Modelling}
}

@article{10.1007/s10664-016-9496-7,
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
title = {Predicting the delay of issues with due dates in software projects},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9496-7},
doi = {10.1007/s10664-016-9496-7},
abstract = {Issue-tracking systems (e.g. JIRA) have increasingly been used in many software projects. An issue could represent a software bug, a new requirement or a user story, or even a project task. A deadline can be imposed on an issue by either explicitly assigning a due date to it, or implicitly assigning it to a release and having it inherit the release's deadline. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether an issue is at risk of being delayed against its deadline. A set of features (hereafter called risk factors) characterizing delayed issues were extracted from eight open source projects: Apache, Duraspace, Java.net, JBoss, JIRA, Moodle, Mulesoft, and WSO2. Risk factors with good discriminative power were selected to build predictive models to predict if the resolution of an issue will be at risk of being delayed. Our predictive models are able to predict both the the extend of the delay and the likelihood of the delay occurrence. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 79 % precision, 61 % recall, 68 % F-measure, and 83 % Area Under the ROC Curve. Our predictive models also have low error rates: on average 0.66 for Macro-averaged Mean Cost-Error and 0.72 Macro-averaged Mean Absolute Error.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1223–1263},
numpages = {41},
keywords = {Project management, Mining software engineering repositories, Empirical software engineering}
}

@article{10.1145/3417330,
author = {Ma, Wei and Papadakis, Mike and Tsakmalis, Anestis and Cordy, Maxime and Traon, Yves Le},
title = {Test Selection for Deep Learning Systems},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3417330},
doi = {10.1145/3417330},
abstract = {Testing of deep learning models is challenging due to the excessive number and complexity of the computations involved. As a result, test data selection is performed manually and in an ad hoc way. This raises the question of how we can automatically select candidate data to test deep learning models. Recent research has focused on defining metrics to measure the thoroughness of a test suite and to rely on such metrics to guide the generation of new tests. However, the problem of selecting/prioritising test inputs (e.g., to be labelled manually by humans) remains open. In this article, we perform an in-depth empirical comparison of a set of test selection metrics based on the notion of model uncertainty (model confidence on specific inputs). Intuitively, the more uncertain we are about a candidate sample, the more likely it is that this sample triggers a misclassification. Similarly, we hypothesise that the samples for which we are the most uncertain are the most informative and should be used in priority to improve the model by retraining. We evaluate these metrics on five models and three widely used image classification problems involving real and artificial (adversarial) data produced by five generation algorithms. We show that uncertainty-based metrics have a strong ability to identify misclassified inputs, being three times stronger than surprise adequacy and outperforming coverage-related metrics. We also show that these metrics lead to faster improvement in classification accuracy during retraining: up to two times faster than random selection and other state-of-the-art metrics on all models we considered.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {13},
numpages = {22},
keywords = {software testing, software engineering, Deep learning testing}
}

@inproceedings{10.1145/3300115.3309516,
author = {Kangas, Vilma and Pirttinen, Nea and Nygren, Henrik and Leinonen, Juho and Hellas, Arto},
title = {Does Creating Programming Assignments with Tests Lead to Improved Performance in Writing Unit Tests?},
year = {2019},
isbn = {9781450362597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300115.3309516},
doi = {10.1145/3300115.3309516},
abstract = {We have constructed a tool, CrowdSorcerer, in which students create programming assignments, their model solutions and associated test cases using a simple input-output format. We have used the tool as a part of an introductory programming course with normal course activities such as programming assignments and a final exam. In our work, we focus on whether creating programming assignments and associated tests correlate with students' performance in a testing-related exam question. We study this through an analysis of the quality of student-written tests within the tool, measured using the number of test cases, line coverage and mutation coverage, and students' performance in testing related exam question, measured using exam points. Finally, we study whether previous programming experience correlates with how students act within the tool and within the testing related exam question.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
pages = {106–112},
numpages = {7},
keywords = {testing, educational data mining, crowdsourcing, assignment creation},
location = {Chengdu,Sichuan, China},
series = {CompEd '19}
}

@article{10.5555/2372179.2372185,
author = {Puuronen, Seppo and Tsymbal, Alexey},
title = {Local Feature Selection with Dynamic Integration of Classifiers},
year = {2001},
issue_date = {January 2001},
publisher = {IOS Press},
address = {NLD},
volume = {47},
number = {1–2},
issn = {0169-2968},
abstract = {Multidimensional data is often feature space heterogeneous so that individual features have unequal importance in different sub areas of the feature space. This motivates to search for a technique that provides a strategic splitting of the instance space being able to identify the best subset of features for each instance to be classified. Our technique applies the wrapper approach where a classification algorithm is used as an evaluation function to differentiate between different feature subsets. In order to make the feature selection local, we apply the recent technique for dynamic integration of classifiers. This allows to determine which classifier and which feature subset should be used for each new instance. Decision trees are used to help to restrict the number of feature combinations analyzed. For each new instance we consider only those feature combinations that include the features present in the path taken by the new instance in the decision tree built on the whole feature set. We evaluate our technique on data sets from the UCI machine learning repository. In our experiments, we use the C4.5 algorithm as the learning algorithm for base classifiers and for the decision trees that guide the local feature selection. The experiments show some advantages of the local feature selection with dynamic integration of classifiers in comparison with the selection of one feature subset for the whole space.},
journal = {Fundam. Inf.},
month = jan,
pages = {91–117},
numpages = {27},
keywords = {machine learning, ensemble of classifiers, dynamic integration, data mining, Feature selection}
}

@inproceedings{10.1145/3387940.3391541,
author = {Zhang, Jie M.},
title = {Automatic Improvement of Machine Translation Using Mutamorphic Relation: Invited Talk Paper},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391541},
doi = {10.1145/3387940.3391541},
abstract = {This paper introduces Mutamorphic Relation for Machine Learning Testing. Mutamorphic Relation combines data mutation and metamorphic relations as test oracles for machine learning systems. These oracles can help achieve fully automatic testing as well as automatic repair of the machine learning models.The paper takes TransRepair as an example to show the effectiveness of Mutamorphic Relation in automatically testing and improving machine translators, TransRepair detects inconsistency bugs without access to human oracles. It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Manual inspection indicates that the translations repaired by TransRepair improve consistency in 87% of cases (degrading it in 2%), and that the repairs of have better translation acceptability in 27% of the cases (worse in 8%).},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {425–426},
numpages = {2},
keywords = {mutation testing, mutamorphic relation, metamorphic testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3236024.3275428,
author = {Zhou, Cheng},
title = {Intelligent bug fixing with software bug knowledge graph},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275428},
doi = {10.1145/3236024.3275428},
abstract = {Software bugs continuously emerge during the process of software evolution. With the increasing size and complexity of software, bug fixing becomes increasingly more difficult. Bug and commit data of open source projects, Q&amp;A documents and other software resources contain a sea of bug knowledge which can be utilized to help developers understand and fix bugs. Existing work focuses on data mining from a certain software resource in isolation to assist in bug fixing, which may reduce the efficiency of bug fixing. How to obtain, organize and understand bug knowledge from multi-source software data is an urgent problem to be solved. In order to solve this problem, we utilize knowledge graph (KG) technology to explore the deep semantic and structural relationships in the multi-source software data, propose effective search and recommendation techniques based on the knowledge graph, and design a bug-fix knowledge question &amp; answering system to assist developers in intelligent software bug fixing. At present, we have designed a bug knowledge graph construction framework, proposed the identification principles and methods for bug knowledge entities and relationships, constructed a preliminary knowledge graph based on the bug repository. In the following work, we will further improve the knowledge graph, complete the knowledge graph fusion of multi-source database, comprehend bug knowledge through knowledge reasoning, utilize the collaborative search and recommendation technology for bug-fixing knowledge question and answering.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {944–947},
numpages = {4},
keywords = {software bug knowledge graph, collaborative search and recommendation, bug-fixing knowledge question and answering, Intelligent bug fixing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3387940.3392171,
author = {Nielebock, Sebastian and Heum\"{u}ller, Robert and Kr\"{u}ger, Jacob and Ortmeier, Frank},
title = {Using API-Embedding for API-Misuse Repair},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392171},
doi = {10.1145/3387940.3392171},
abstract = {Application Programming Interfaces (APIs) are a way to reuse existing functionalities of one application in another one. However, due to lacking knowledge on the correct usage of a particular API, developers sometimes commit misuses, causing unintended or faulty behavior. To detect and eventually repair such misuses automatically, inferring API usage patterns from real-world code is the state-of-the-art. A contradiction to an identified usage pattern denotes a misuse, while applying the pattern fixes the respective misuse. The success of this process heavily depends on the quality of the usage patterns and on the code from which these are inferred. Thus, a lack of code demonstrating the correct usage makes it impossible to detect and fix a misuse. In this paper, we discuss the potential of using machine-learning vector embeddings to improve automatic program repair and to extend it towards cross-API and cross-language repair. We illustrate our ideas using one particular technique for API-embedding (i.e., API2Vec) and describe the arising possibilities and challenges.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {1–2},
numpages = {2},
keywords = {Program Repair, API Misuse, API Embeddings},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1109/SMC.2018.00599,
author = {Teshima, Yunosuke and Watanobe, Yutaka},
title = {Bug Detection Based on LSTM Networks and Solution Codes},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2018.00599},
doi = {10.1109/SMC.2018.00599},
abstract = {Debugging a program is always an obstacle to programmers and learners. In particular, novice programmers waste a lot of time finding bugs, so a feedback system to support debugging is required. Although existing editors and IDEs support finding syntax errors, their functions for detecting logical errors are limited. In the present paper, we present bug detection methods for the feedback system of an online judge system which contains many programming problems and accumulates numerous lines of solution source code. The proposed method uses the solutions and a language model based on long short-term memory (LSTM) networks for bug detection. In addition, since LSTM networks have some hyperparameters, we investigate the best model for bug detection in terms of perplexity and training time. The results of experiments show that models trained by solutions can detect bugs in a compiled code based on the static structure of a program.},
booktitle = {2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {3541–3546},
numpages = {6},
location = {Miyazaki, Japan}
}

@inproceedings{10.1109/ICSE43902.2021.00047,
author = {He, Pinjia and Meister, Clara and Su, Zhendong},
title = {Testing Machine Translation via Referential Transparency},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00047},
doi = {10.1109/ICSE43902.2021.00047},
abstract = {Machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying neural networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {410–422},
numpages = {13},
keywords = {Testing, Referential transparency, Metamorphic testing, Machine translation},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.sysarc.2019.01.007,
author = {Blaiech, Ahmed Ghazi and Ben Khalifa, Khaled and Valderrama, Carlos and Fernandes, Marcelo A.C. and Bedoui, Mohamed Hedi},
title = {A Survey and Taxonomy of FPGA-based Deep Learning Accelerators},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2019.01.007},
doi = {10.1016/j.sysarc.2019.01.007},
journal = {J. Syst. Archit.},
month = sep,
pages = {331–345},
numpages = {15},
keywords = {FPGA, Optimized implementation, Framework, Deep learning}
}

@inbook{10.5555/3454287.3454513,
author = {Dai, Hanjun and Li, Yujia and Wang, Chenglong and Singh, Rishabh and Huang, Po-Sen and Kohli, Pushmeet},
title = {Learning transferable graph exploration},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a 'learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the 'exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {226},
numpages = {12}
}

@inproceedings{10.1145/3092703.3092709,
author = {Spieker, Helge and Gotlieb, Arnaud and Marijan, Dusica and Mossige, Morten},
title = {Reinforcement learning for automatic test case prioritization and selection in continuous integration},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092709},
doi = {10.1145/3092703.3092709},
abstract = {Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {12–22},
numpages = {11},
keywords = {Test case selection, Test case prioritization, Reinforcement Learning, Regression testing, Machine Learning, Continuous Integration},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/2597073.2597099,
author = {Valdivia Garcia, Harold and Shihab, Emad},
title = {Characterizing and predicting blocking bugs in open source projects},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597099},
doi = {10.1145/2597073.2597099},
abstract = {As software becomes increasingly important, its quality becomes an increasingly important issue. Therefore, prior work focused on software quality and proposed many prediction models to identify the location of software bugs, to estimate their fixing-time, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These blocking bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems.  In this paper, we study blocking-bugs in six open source projects and propose a model to predict them. Our goal is to help developers identify these blocking bugs early on. We collect the bug reports from the bug tracking systems of the projects, then we obtain 14 different factors related to, for example, the textual description of the bug, the location the bug is found in and the people involved with the bug. Based on these factors we build decision trees for each project to predict whether a bug will be a blocking bug or not. Then, we analyze these decision trees in order to determine which factors best indicate these blocking bugs. Our results show that our prediction models achieve F-measures of 15-42%, which is a two- to four-fold improvement over the baseline random predictors. We also find that the most important factors in determining blocking bugs are the comment text, comment size, the number of developers in the CC list of the bug report and the reporter's experience. Our analysis shows that our models reduce the median time to identify a blocking bug by 3-18 days.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {72–81},
numpages = {10},
keywords = {Process Metrics, Post-release Defects, Code Metrics},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.1155/2021/6617882,
author = {Yang, Zhe and Ying, Shi and Wang, Bingming and Li, Yiyao and Dong, Bo and Geng, Jiangyi and Zhang, Ting and Wang, Pengwei},
title = {A System Fault Diagnosis Method with a Reclustering Algorithm},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/6617882},
doi = {10.1155/2021/6617882},
abstract = {The log analysis-based system fault diagnosis method can help engineers analyze the fault events generated by the system. The K-means algorithm can perform log analysis well and does not require a lot of prior knowledge, but the K-means-based system fault diagnosis method needs to be improved in both efficiency and accuracy. To solve this problem, we propose a system fault diagnosis method based on a reclustering algorithm. First, we propose a log vectorization method based on the PV-DM language model to obtain low-dimensional log vectors which can provide effective data support for the subsequent fault diagnosis; then, we improve the K-means algorithm and make the effect of K-means algorithm based log clustering; finally, we propose a reclustering method based on keywords’ extraction to improve the accuracy of fault diagnosis. We use system log data generated by two supercomputers to verify our method. The experimental results show that compared with the traditional K-means method, our method can improve the accuracy of fault diagnosis while ensuring the efficiency of fault diagnosis.},
journal = {Sci. Program.},
month = jan,
numpages = {8}
}

@inproceedings{10.1145/3468264.3468586,
author = {Chen, Ke and Li, Yufei and Chen, Yingfeng and Fan, Changjie and Hu, Zhipeng and Yang, Wei},
title = {GLIB: towards automated test oracle for graphically-rich applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468586},
doi = {10.1145/3468264.3468586},
abstract = {Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose GLIB based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of GLIB on 20 real-world game apps (with bug reports available) and the result shows that GLIB can achieve 100% precision and 99.5% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of GLIB on another 14 real-world games (without bug reports) further demonstrates that GLIB can effectively uncover GUI glitches, with 48 of 53 bugs reported by GLIB having been confirmed and fixed so far.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1093–1104},
numpages = {12},
keywords = {Game Testing, GUI Testing, Deep Learning, Automated Test Oracle},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1007/s11219-014-9240-8,
author = {Galinac Grbac, Tihana and Car, \v{Z}eljka and Huljeni\'{c}, Darko},
title = {A quality cost reduction model for large-scale software development},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9240-8},
doi = {10.1007/s11219-014-9240-8},
abstract = {Understanding quality costs is recognized as a prerequisite for decreasing the variability of the success of software development projects. This paper presents an empirical quality cost reduction (QCR) model to support the decision-making process for additional investment in the early phases of software verification. The main idea of the QCR model is to direct additional investment into software units that have some fault-slip potential in their later verification phases, with the aim of reducing costs and increasing product quality. The fault-slip potential of a software unit within a system is determined by analogy with historical projects. After a preliminary study on a sample of software units, which proves that we can lower quality costs with additional investment into particular verification activities, we examine the effectiveness of the proposed QCR model using real project data. The results show that applying the model produces a positive business case, meaning that the model lowers quality costs and increases quality, resulting in economic benefit. The potential to reduce quality costs is growing significantly with the evolution of software systems and the reuse of their software units. The proposed model is the result of a research project performed at Ericsson.},
journal = {Software Quality Journal},
month = jun,
pages = {363–390},
numpages = {28},
keywords = {Verification, Quality cost, Large-scale software, Fault detection, Control model}
}

@inproceedings{10.1145/3183440.3183471,
author = {Wang, Cong and Jiang, Yu and Zhao, Xibin and Song, Xiaoyu and Gu, Ming and Sun, Jiaguang},
title = {Weak-assert: a weakness-oriented assertion recommendation toolkit for program analysis},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183471},
doi = {10.1145/3183440.3183471},
abstract = {Assertions are helpful in program analysis, such as software testing and verification. The most challenging part of automatically recommending assertions is to design the assertion patterns and to insert assertions in proper locations. In this paper, we develop Weak-Assert1, a weakness-oriented assertion recommendation toolkit for program analysis of C code. A weakness-oriented assertion is an assertion which can help to find potential program weaknesses. Weak-Assert uses well-designed patterns to match the abstract syntax trees of source code automatically. It collects significant messages from trees and inserts assertions into proper locations of programs. These assertions can be checked by using program analysis techniques. The experiments are set up on Juliet test suite and several actual projects in Github. Experimental results show that Weak-Assert helps to find 125 program weaknesses in 26 actual projects. These weaknesses are confirmed manually to be triggered by some test cases.The address of the abstract demo video is: https://youtu.be/_RWC4GJvRWc},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {69–72},
numpages = {4},
keywords = {assertion recommendation, formal program verification, program testing, program weakness},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.5555/3489212.3489277,
author = {Dai, Jiarun and Zhang, Yuan and Jiang, Zheyue and Zhou, Yingtian and Chen, Junyan and Xing, Xinyu and Zhang, Xiaohan and Tan, Xin and Yang, Min and Yang, Zhemin},
title = {BScout: direct whole patch presence test for Java executables},
year = {2020},
isbn = {978-1-939133-17-5},
publisher = {USENIX Association},
address = {USA},
abstract = {To protect end-users and software from known vulnerabilities, it is crucial to apply security patches to affected executables timely. To this end, patch presence tests are proposed with the capability of independently investigating patch application status on a target without source code. Existing work on patch presence testing adopts a signature-based approach. To make a trade-off between the uniqueness and the stability of the signature, existing work is limited to use a small and localized patch snippet (instead of the whole patch) for signature generation, so they are inherently unreliable.In light of this, we present BSCOUT, which directly checks the presence of a whole patch in Java executables without generating signatures. BSCOUT features several new techniques to bridge the semantic gap between source code and bytecode instructions during the testing, and accurately checks the fine-grained patch semantics in the whole target executable. We evaluate BScout with 194 CVEs from the Android framework and third-party libraries. The results show that it achieves remarkable accuracy with and without line number information (i.e., debug information) presented in a target executable. We further apply BSCOUT to perform a large-scale patch application practice study with 2,506 Android system images from 7 vendors. Our study reveals many findings that have not yet been reported.},
booktitle = {Proceedings of the 29th USENIX Conference on Security Symposium},
articleno = {65},
numpages = {18},
series = {SEC'20}
}

@inproceedings{10.5555/2946711.2946727,
author = {Mayo, Michael and Spacey, Simon},
title = {Predicting Regression Test Failures Using Genetic Algorithm-Selected Dynamic Performance Analysis Metrics},
year = {2013},
isbn = {9783642397417},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A novel framework for predicting regression test failures is proposed. The basic principle embodied in the framework is to use performance analysis tools to capture the runtime behaviour of a program as it executes each test in a regression suite. The performance information is then used to build a dynamically predictive model of test outcomes. Our framework is evaluated using a genetic algorithm for dynamic metric selection in combination with state-of-the-art machine learning classifiers. We show that if a program is modified and some tests subsequently fail, then it is possible to predict with considerable accuracy which of the remaining tests will also fail which can be used to help prioritise tests in time constrained testing environments.},
booktitle = {Proceedings of the 5th International Symposium on Search Based Software Engineering - Volume 8084},
pages = {158–171},
numpages = {14},
keywords = {test failure prediction, regression testing, program analysis, machine learning, genetic metric selection},
location = {St. Petersburg, Russia},
series = {SSBSE 2013}
}

@inproceedings{10.1145/3328433.3328459,
author = {Ishida, Toyomi and Uwano, Hidetake},
title = {Time series analysis of programmer's EEG for debug state classification},
year = {2019},
isbn = {9781450362573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328433.3328459},
doi = {10.1145/3328433.3328459},
abstract = {Appropriate support for debugging contributes to efficient software development. Several previous studies used bio information such as brain activity to classify the inner-state of programmers without any interruption. In this paper, we measure programmer's brain waves while they comprehend the source code. In an experiment, we analyze difference of time-series brain wave features between success/failure for tasks involving source code comprehension and bug judgement. The result of the experiment showed the participants who successfully understand source code significantly increased power spectrum of α and β wave with the time passage.},
booktitle = {Companion Proceedings of the 3rd International Conference on the Art, Science, and Engineering of Programming},
articleno = {25},
numpages = {7},
keywords = {program comprehension, debugging, EEG},
location = {Genova, Italy},
series = {Programming '19}
}

@inproceedings{10.1145/3338906.3340448,
author = {Yu, Zhe and Fahid, Fahmid and Menzies, Tim and Rothermel, Gregg and Patrick, Kyle and Cherian, Snehit},
title = {TERMINATOR: better automated UI test case prioritization},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340448},
doi = {10.1145/3338906.3340448},
abstract = {Automated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page's microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process.  To mitigate this problem and give developers faster feedback on their code, test case prioritization techniques are used to reorder the automated UI test cases so that more failures can be detected earlier. Given that much of the automated UI testing is "black box" in nature, very little information (only the test case descriptions and testing results) can be utilized to prioritize these automated UI test cases. Hence, this paper evaluates 17 "black box" test case prioritization approaches that do not rely on source code information. Among these, we propose a novel TCP approach, that dynamically re-prioritizes the test cases when new failures are detected, by applying and adapting a state of the art framework from the total recall problem. Experimental results on LexisNexis automated UI testing data show that our new approach (which we call TERMINATOR), outperformed prior state of the art approaches in terms of failure detection rates with negligible CPU overhead.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {883–894},
numpages = {12},
keywords = {total recall, test case prioritization, automated UI testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1007/s11334-014-0231-5,
author = {Barb, Adrian S. and Neill, Colin J. and Sangwan, Raghvinder S. and Piovoso, Michael J.},
title = {A statistical study of the relevance of lines of code measures in software projects},
year = {2014},
issue_date = {December  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-014-0231-5},
doi = {10.1007/s11334-014-0231-5},
abstract = {Lines of code metrics are routinely used as measures of software system complexity, programmer productivity, and defect density, and are used to predict both effort and cost. The guidelines for using a direct metric, such as lines of code, as a proxy for a quality factor such as complexity or defect density, or in derived metrics such as cost and effort are clear. Amongst other criteria, the direct metric must be linearly related to, and accurately predict, the quality factor and these must be validated through statistical analysis following a rigorous validation methodology. In this paper, we conduct such an analysis to determine the validity and utility of lines of code as a measure using the ISBGS-10 data set. We find that it fails to meet the specified validity tests and, therefore, has limited utility in derived measures.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {243–260},
numpages = {18},
keywords = {Software estimation, Lines of code, Linear models, ISBSG-10, Data mining}
}

@inproceedings{10.5555/2820518.2820552,
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
title = {Characterization and prediction of issue-related risks in software projects},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Identifying risks relevant to a software project and planning measures to deal with them are critical to the success of the project. Current practices in risk assessment mostly rely on high-level, generic guidance or the subjective judgements of experts. In this paper, we propose a novel approach to risk assessment using historical data associated with a software project. Specifically, our approach identifies patterns of past events that caused project delays, and uses this knowledge to identify risks in the current state of the project. A set of risk factors characterizing "risky" software tasks (in the form of issues) were extracted from five open source projects: Apache, Duraspace, JBoss, Moodle, and Spring. In addition, we performed feature selection using a sparse logistic regression model to select risk factors with good discriminative power. Based on these risk factors, we built predictive models to predict if an issue will cause a project delay. Our predictive models are able to predict both the risk impact (i.e. the extend of the delay) and the likelihood of a risk occurring. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 48%--81% precision, 23%--90% recall, 29%--71% F-measure, and 70%--92% Area Under the ROC Curve. Our predictive models also have low error rates: 0.39--0.75 for Macro-averaged Mean Cost-Error and 0.7--1.2 for Macro-averaged Mean Absolute Error.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {280–291},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1007/s11219-016-9332-8,
author = {Musco, Vincenzo and Monperrus, Martin and Preux, Philippe},
title = {A large-scale study of call graph-based impact prediction using mutation testing},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9332-8},
doi = {10.1007/s11219-016-9332-8},
abstract = {In software engineering, impact analysis involves predicting the software elements (e.g., modules, classes, methods) potentially impacted by a change in the source code. Impact analysis is required to optimize the testing effort. In this paper, we propose an evaluation technique to predict impact propagation. Based on 10 open-source Java projects and 5 classical mutation operators, we create 17,000 mutants and study how the error they introduce propagates. This evaluation technique enables us to analyze impact prediction based on four types of call graph. Our results show that graph sophistication increases the completeness of impact prediction. However, and surprisingly to us, the most basic call graph gives the best trade-off between precision and recall for impact prediction.},
journal = {Software Quality Journal},
month = sep,
pages = {921–950},
numpages = {30},
keywords = {Mutation testing, Change impact analysis, Call graphs}
}

@inproceedings{10.1007/978-3-030-27455-9_12,
author = {Dantas, Altino and de Souza, Eduardo F. and Souza, Jerffeson and Camilo-Junior, Celso G.},
title = {Code Naturalness to Assist Search Space Exploration in Search-Based Program Repair Methods},
year = {2019},
isbn = {978-3-030-27454-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27455-9_12},
doi = {10.1007/978-3-030-27455-9_12},
abstract = {Automated Program Repair (APR) is a research field that has recently gained attention due to its advances in proposing methods to fix buggy programs without human intervention. Search-Based Program Repair methods have difficulties to traverse the search space, mainly, because it is challenging and costly to evaluate each variant. Therefore, aiming to improve each program’s variant evaluation through providing more information to the fitness function, we propose the combination of two techniques, Doc2vec and LSTM, to capture high-level differences among variants and to capture the dependence between source code statements in the fault localization region. The experiments performed with the IntroClass benchmark show that our approach captures differences between variants according to the level of changes they received, and the resulting information is useful to balance the search between the exploration and exploitation steps. Besides, the proposal might be promising to filter program variants that are adequate to the suspicious portion of the code.},
booktitle = {Search-Based Software Engineering: 11th International Symposium, SSBSE 2019, Tallinn, Estonia, August 31 – September 1, 2019, Proceedings},
pages = {164–170},
numpages = {7},
keywords = {Code naturalness, Search space exploration, Automated Program Repair},
location = {Tallinn, Estonia}
}

@article{10.1007/s11219-013-9224-0,
author = {Fang, Chunrong and Chen, Zhenyu and Wu, Kun and Zhao, Zhihong},
title = {Similarity-based test case prioritization using ordered sequences of program entities},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9224-0},
doi = {10.1007/s11219-013-9224-0},
abstract = {Test suites often grow very large over many releases, such that it is impractical to re-execute all test cases within limited resources. Test case prioritization rearranges test cases to improve the effectiveness of testing. Code coverage has been widely used as criteria in test case prioritization. However, the simple way may not reveal some bugs, such that the fault detection rate decreases. In this paper, we use the ordered sequences of program entities to improve the effectiveness of test case prioritization. The execution frequency profiles of test cases are collected and transformed into the ordered sequences. We propose several novel similarity-based test case prioritization techniques based on the edit distances of ordered sequences. An empirical study of five open source programs was conducted. The experimental results show that our techniques can significantly increase the fault detection rate and be effective in detecting faults in loops. Moreover, our techniques are more cost-effective than the existing techniques.},
journal = {Software Quality Journal},
month = jun,
pages = {335–361},
numpages = {27},
keywords = {Test case prioritization, Similarity, Ordered sequence, Farthest-first algorithm, Edit distance}
}

@inproceedings{10.1007/978-3-030-64148-1_18,
author = {Al-Sabbagh, Khaled Walid and Hebig, Regina and Staron, Miroslaw},
title = {The Effect of Class Noise on Continuous Test Case Selection: A Controlled Experiment on Industrial Data},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_18},
doi = {10.1007/978-3-030-64148-1_18},
abstract = {Continuous integration and testing produce a large amount of data about defects in code revisions, which can be utilized for training a predictive learner to effectively select a subset of test suites. One challenge in using predictive learners lies in the noise that comes in the training data, which often leads to a decrease in classification performances. This study examines the impact of one type of noise, called class noise, on a learner’s ability for selecting test cases. Understanding the impact of class noise on the performance of a learner for test case selection would assist testers decide on the appropriateness of different noise handling strategies. For this purpose, we design and implement a controlled experiment using an industrial data-set to measure the impact of class noise at six different levels on the predictive performance of a learner. We measure the learning performance using the Precision, Recall, F-score, and Mathew Correlation Coefficient (MCC) metrics. The results show a statistically significant relationship between class noise and the learner’s performance for test case selection. Particularly, a significant difference between the three performance measures (Precision, F-score, and MCC) under all the six noise levels and at 0% level was found, whereas a similar relationship between recall and class noise was found at a level above 30%. We conclude that higher class noise ratios lead to missing out more tests in the predicted subset of test suite and increases the rate of false alarms when the class noise ratio exceeds 30%.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {287–303},
numpages = {17},
keywords = {Continuous integration, Test case selection, Class noise, Controlled experiment},
location = {Turin, Italy}
}

@inproceedings{10.1109/ICPC.2017.19,
author = {Catolino, Gemma and Palomba, Fabio and De Lucia, Andrea and Ferrucci, Filomena and Zaidman, Andy},
title = {Developer-related factors in change prediction: an empirical assessment},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.19},
doi = {10.1109/ICPC.2017.19},
abstract = {Predicting the areas of the source code having a higher likelihood to change in the future is a crucial activity to allow developers to plan preventive maintenance operations such as refactoring or peer-code reviews. In the past the research community was active in devising change prediction models based on structural metrics extracted from the source code. More recently, Elish et al. showed how evolution metrics can be more efficient for predicting change-prone classes. In this paper, we aim at making a further step ahead by investigating the role of different developer-related factors, which are able to capture the complexity of the development process under different perspectives, in the context of change prediction. We also compared such models with existing change-prediction models based on evolution and code metrics. Our findings reveal the capabilities of developer-based metrics in identifying classes of a software system more likely to be changed in the future. Moreover, we observed interesting complementarities among the experimented prediction models, that may possibly lead to the definition of new combined models exploiting developer-related factors as well as product and evolution metrics.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {186–195},
numpages = {10},
keywords = {mining software repositories, empirical studies, change prediction},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@article{10.1007/s10664-012-9219-7,
author = {Thomas, Stephen W. and Hemmati, Hadi and Hassan, Ahmed E. and Blostein, Dorothea},
title = {Static test case prioritization using topic models},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9219-7},
doi = {10.1007/s10664-012-9219-7},
abstract = {Software development teams use test suites to test changes to their source code. In many situations, the test suites are so large that executing every test for every source code change is infeasible, due to time and resource constraints. Development teams need to prioritize their test suite so that as many distinct faults as possible are detected early in the execution of the test suite. We consider the problem of static black-box test case prioritization (TCP), where test suites are prioritized without the availability of the source code of the system under test (SUT). We propose a new static black-box TCP technique that represents test cases using a previously unused data source in the test suite: the linguistic data of the test cases, i.e., their identifier names, comments, and string literals. Our technique applies a text analysis algorithm called topic modeling to the linguistic data to approximate the functionality of each test case, allowing our technique to give high priority to test cases that test different functionalities of the SUT. We compare our proposed technique with existing static black-box TCP techniques in a case study of multiple real-world open source systems: several versions of Apache Ant and Apache Derby. We find that our static black-box TCP technique outperforms existing static black-box TCP techniques, and has comparable or better performance than two existing execution-based TCP techniques. Static black-box TCP methods are widely applicable because the only input they require is the source code of the test cases themselves. This contrasts  with other TCP techniques which require access to the SUT runtime behavior, to the SUT specification models, or to the SUT source code.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {182–212},
numpages = {31},
keywords = {Topic models, Testing and debugging, Test case prioritization}
}

@article{10.1007/s11219-016-9334-6,
author = {S\"{o}ylemez, Mehmet and Tarhan, Ayca},
title = {Challenges of software process and product quality improvement: catalyzing defect root-cause investigation by process enactment data analysis},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9334-6},
doi = {10.1007/s11219-016-9334-6},
abstract = {It is claimed by software quality management that the quality of a software product is highly influenced by the quality of the software process followed to develop it. Since measurement of the software process is a challenging task, it is frequently the defects in the software product that are used to measure development quality. By extracting semantic information from defect records, practitioners can investigate and address root causes of software defects to improve development process and product quality. Investigating root causes requires effort for a detailed analysis into the components of the development process that originated the software defects, and is therefore encouraged only at higher maturity levels by most known process improvement models such as Capability Maturity Model Integration (CMMI). This practice, however, postpones the benefits that root-cause analysis would bring in gaining process awareness to improve the software development process and product quality in emergent organizations or organizations residing at lower maturity levels (MLs). In this article, we present a method for and results from applying root-cause analysis for software defects recorded in a software-intensive project of a CMMI ML3 certified institute. The suggested method combines process enactment data collection and analysis with Orthogonal Defect Classification which is a known technique in defect root-cause analysis. Prior to and after implementing the method in the study, defect attributes were analyzed and compared in order to understand any improvements in development performance and product quality. The results of the comparison indicate that the suggested method was efficient in the effort it required and effective in improving development performance and product quality. Defect triggers have become more active in identifying software defects in the earlier phases of software development, and the cost of quality due to software defects has decreased in consequence.},
journal = {Software Quality Journal},
month = jun,
pages = {779–807},
numpages = {29},
keywords = {Software process, Software defect, Product quality, Process improvement, Process enactment, Orthogonal defect classification, Causal analysis}
}

@inproceedings{10.1145/1629575.1629587,
author = {Xu, Wei and Huang, Ling and Fox, Armando and Patterson, David and Jordan, Michael I.},
title = {Detecting large-scale system problems by mining console logs},
year = {2009},
isbn = {9781605587523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629575.1629587},
doi = {10.1145/1629575.1629587},
abstract = {Surprisingly, console logs rarely help operators detect problems in large-scale datacenter services, for they often consist of the voluminous intermixing of messages from many software components written by independent developers. We propose a general methodology to mine this rich source of information to automatically detect system runtime problems. We first parse console logs by combining source code analysis with information retrieval to create composite features. We then analyze these features using machine learning to detect operational problems. We show that our method enables analyses that are impossible with previous methods because of its superior ability to create sophisticated features. We also show how to distill the results of our analysis to an operator-friendly one-page decision tree showing the critical messages associated with the detected problems. We validate our approach using the Darkstar online game server and the Hadoop File System, where we detect numerous real problems with high accuracy and few false positives. In the Hadoop case, we are able to analyze 24 million lines of console logs in 3 minutes. Our methodology works on textual console logs of any size and requires no changes to the service software, no human input, and no knowledge of the software's internals.},
booktitle = {Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles},
pages = {117–132},
numpages = {16},
keywords = {tracing, statistical learning, source code analysis, problem detection, pca, monitoring, console log analysis},
location = {Big Sky, Montana, USA},
series = {SOSP '09}
}

@inproceedings{10.1007/978-3-030-58811-3_62,
author = {Kalouptsoglou, Ilias and Siavvas, Miltiadis and Tsoukalas, Dimitrios and Kehagias, Dionysios},
title = {Cross-Project Vulnerability Prediction Based on Software Metrics and Deep Learning},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_62},
doi = {10.1007/978-3-030-58811-3_62},
abstract = {Vulnerability prediction constitutes a mechanism that enables the identification and mitigation of software vulnerabilities early enough in the development cycle, improving the security of software products, which is an important quality attribute according to ISO/IEC 25010. Although existing vulnerability prediction models have demonstrated sufficient accuracy in predicting the occurrence of vulnerabilities in the software projects with which they have been trained, they have failed to demonstrate sufficient accuracy in cross-project prediction. To this end, in the present paper we investigate whether the adoption of deep learning along with software metrics may lead to more accurate cross-project vulnerability prediction. For this purpose, several machine learning (including deep learning) models are constructed, evaluated, and compared based on a dataset of popular real-world PHP software applications. Feature selection is also applied with the purpose to examine whether it has an impact on cross-project prediction. The results of our analysis indicate that the adoption of software metrics and deep learning may result in vulnerability prediction models with sufficient performance in cross-project vulnerability prediction. Another interesting conclusion is that the performance of the models in cross-project prediction is enhanced when the projects exhibit similar characteristics with respect to their software metrics.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {877–893},
numpages = {17},
keywords = {Vulnerability prediction, Security, Software quality},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3377811.3380415,
author = {Gao, Xiang and Saha, Ripon K. and Prasad, Mukul R. and Roychoudhury, Abhik},
title = {Fuzz testing based data augmentation to improve robustness of deep neural networks},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380415},
doi = {10.1145/3377811.3380415},
abstract = {Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average. Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1147–1158},
numpages = {12},
keywords = {DNN, data augmentation, genetic algorithm, robustness},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1155/2021/5834807,
author = {Rehan, Muhammad and Senan, Norhalina and Aamir, Muhammad and Samad, Ali and Husnain, Mujtaba and Ibrahim, Noraini and Ali, Sikandar and Khatak, Hizbullah and Ullah, Kifayat},
title = {A Systematic Analysis of Regression Test Case Selection: A Multi-Criteria-Based Approach},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5834807},
doi = {10.1155/2021/5834807},
abstract = {In applied software engineering, the algorithms for selecting the appropriate test cases are used to perform regression testing. The key objective of this activity is to make sure that modification in the system under test (SUT) has no impact on the overall functioning of the updated software. It is concluded from the literature that the efficacy of the test case selection solely depends on the following metrics, namely, the execution cost of the test case, the lines of the code covered in unit time also known as the code coverage, the ability to capture the potential faults, and the code modifications. Furthermore, it is also observed that the approaches for the regression testing developed so far generated results by focusing on one or two parameters. In this paper, our key objectives are twofold: one is to explore the importance of the role of each metric in detail. The secondary objective is to study the combined effect of these metrics in test case selection task that is capable of achieving more than one objective. In this paper, a detailed and comprehensive review of the work related to regression testing is provided in a very distinct and principled way. This survey will be useful for the researchers contributing to the field of regression testing. It is noteworthy that our systematic literature review (SLR) included the noteworthy work published from 2007 to 2020. Our study observed that about 52 relevant studies focused on all of the four metrics to perform their respective tasks. The results also revealed that about 30% of the different categories of regression test case reported the results using metaheuristic regression test selection (RTS). Similarly, about 31% of the literature reported results using the generic regression test case selection techniques. Most of the researchers focus on the datasets, namely, Software-Artefact Infrastructure Repository (SIR), JodaTime, TreeDataStructure, and Apache Software Foundation. For validation purpose, following parameters were focused, namely, the inclusiveness, precision, recall, and retest-all.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {11}
}

@inproceedings{10.1145/1390630.1390647,
author = {Buse, Raymond P.L. and Weimer, Westley R.},
title = {A metric for software readability},
year = {2008},
isbn = {9781605580500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390630.1390647},
doi = {10.1145/1390630.1390647},
abstract = {In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80% effective, and better than a human on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with two traditional measures of software quality, code changes and defect reports. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggests that comments, in of themselves, are less important than simple blank lines to local judgments of readability.},
booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
pages = {121–130},
numpages = {10},
keywords = {software readability, software maintenance, program understanding, machine learning, code metrics, FindBugs},
location = {Seattle, WA, USA},
series = {ISSTA '08}
}

@article{10.1007/s10664-021-09948-6,
author = {Yang, Xueqi and Chen, Jianfeng and Yedida, Rahul and Yu, Zhe and Menzies, Tim},
title = {Learning to recognize actionable static code warnings (is intrinsically easy)},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09948-6},
doi = {10.1007/s10664-021-09948-6},
abstract = {Static code warning tools often generate warnings that programmers ignore. Such tools can be made more useful via data mining algorithms that select the “actionable” warnings; i.e. the warnings that are usually not ignored. In this paper, we look for actionable warnings within a sample of 5,675 actionable warnings seen in 31,058 static code warnings from FindBugs. We find that data mining algorithms can find actionable warnings with remarkable ease. Specifically, a range of data mining methods (deep learners, random forests, decision tree learners, and support vector machines) all achieved very good results (recalls and AUC(TRN, TPR) measures usually over 95% and false alarms usually under 5%). Given that all these learners succeeded so easily, it is appropriate to ask if there is something about this task that is inherently easy. We report that while our data sets have up to 58 raw features, those features can be approximated by less than two underlying dimensions. For such intrinsically simple data, many different kinds of learners can generate useful models with similar performance. Based on the above, we conclude that learning to recognize actionable static code warnings is easy, using a wide range of learning algorithms, since the underlying data is intrinsically simple. If we had to pick one particular learner for this task, we would suggest linear SVMs (since, at least in our sample, that learner ran relatively quickly and achieved the best median performance) and we would not recommend deep learning (since this data is intrinsically very simple).},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {24},
keywords = {Intrinsic dimensionality, Linear SVM, Deep learning, Actionable warnings, Static code analysis}
}

@inproceedings{10.1007/978-3-030-72699-7_43,
author = {Rosenbauer, Lukas and P\"{a}tzel, David and Stein, Anthony and H\"{a}hner, J\"{o}rg},
title = {Transfer Learning for Automated Test Case Prioritization Using XCSF},
year = {2021},
isbn = {978-3-030-72698-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72699-7_43},
doi = {10.1007/978-3-030-72699-7_43},
abstract = {With the rise of test automation, companies start to rely on large amounts of test cases. However, there are situations where it is unfeasible to perform every test case as only a limited amount of time is available. Under such circumstances a set of crucial tests has to be compiled. Recent research has shown that reinforcement learning methods such as XCSF classifier systems are well-suited for this task. This work investigates whether reusing knowledge of XCSF-based agents is beneficial for prioritizing test cases and subsequently selecting test suites in terms of performance. We developed a simplistic population transformation and evaluate it in a series of experiments. Our evaluation shows that XCSF may indeed benefit from transfer learning for this use case.},
booktitle = {Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 7–9, 2021, Proceedings},
pages = {681–696},
numpages = {16},
keywords = {XCSF classifier system, Transfer learning, Evolutionary machine learning, Automated testing}
}

@inproceedings{10.1145/3236024.3236053,
author = {Chen, Junjie and Lou, Yiling and Zhang, Lingming and Zhou, Jianyi and Wang, Xiaoleng and Hao, Dan and Zhang, Lu},
title = {Optimizing test prioritization via test distribution analysis},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236053},
doi = {10.1145/3236024.3236053},
abstract = {Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice.  To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming state-of-the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks from the testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {656–667},
numpages = {12},
keywords = {Test Prioritization, Regression Testing, Machine Learning},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1016/j.infsof.2021.106668,
author = {Tong, Yao and Zhang, Xiaofang},
title = {Crowdsourced test report prioritization considering bug severity},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106668},
doi = {10.1016/j.infsof.2021.106668},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {15},
keywords = {Textual description, Bug severity, Prioritization, Test report processing, Crowdsourced testing}
}

@inproceedings{10.1145/3213846.3213866,
author = {Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu},
title = {An empirical study on TensorFlow program bugs},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213866},
doi = {10.1145/3213846.3213866},
abstract = {Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {129–140},
numpages = {12},
keywords = {TensorFlow Program Bug, Empirical Study, Deep Learning},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1007/978-3-030-64437-6_13,
author = {Liu, Xiaotong and Jia, Tong and Li, Ying and Yu, Hao and Yue, Yang and Hou, Chuanjia},
title = {Automatically Generating Descriptive Texts in Logging Statements: How Far Are We?},
year = {2020},
isbn = {978-3-030-64436-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64437-6_13},
doi = {10.1007/978-3-030-64437-6_13},
abstract = {In most cases, logs are the only accurate information available for administrators to understand system behavior and diagnose failure root causes. However, due to the lack of well-defined logging guidance, it is challenging for developers to decide what to log, especially logging statements that contain descriptive texts and variables. In this paper, we explore automatically generation of descriptive texts in logging statements and evaluate the effectiveness of various automatic generation methods. We propose that to generate descriptive texts in logging statements can be transferred as a retrieval-based Q&amp;A task. According to the roles of query and answer, we design two retrieval strategies including Code&amp;Code and Code&amp;Log. To measure the similarity between the query and answer, we utilize two types of retrieval algorithms including Information retrieval-based and neural networks-based algorithms. We conduct a systematic analysis of various retrieval algorithms under different retrieval strategies in terms of their effectiveness, and assess their accuracy using the automatic metrics and human evaluation during which 5 instructive findings are presented. We believe that these findings can provide potential implications for both researchers and practitioners for relevant research. Moreover, we construct and release a log text dataset containing over 138K valid log texts from 85 Java projects in Apache ecosystem for future logging statement analysis and generation.},
booktitle = {Programming Languages and Systems: 18th Asian Symposium, APLAS 2020, Fukuoka, Japan, November 30 – December 2, 2020, Proceedings},
pages = {251–269},
numpages = {19},
keywords = {Logging, Log text, Automatic generation, Experimental analysis},
location = {Fukuoka, Japan}
}

@inproceedings{10.1145/2851613.2851777,
author = {Moshtari, Sara and Sami, Ashkan},
title = {Evaluating and comparing complexity, coupling and a new proposed set of coupling metrics in cross-project vulnerability prediction},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851777},
doi = {10.1145/2851613.2851777},
abstract = {Software security is an important concern in the world moving towards Information Technology. Detecting software vulnerabilities is a difficult and resource consuming task. Therefore, automatic vulnerability prediction would help development teams to predict vulnerability-prone components and prioritize security inspection efforts. Software source code metrics and data mining techniques have been recently used to predict vulnerability-prone components. Some of previous studies used a set of unit complexity and coupling metrics to predict vulnerabilities. In this study, first, we compare the predictability power of these two groups of metrics in cross-project vulnerability prediction. In cross-project vulnerability prediction we create the prediction model based on datasets of completely different projects and try to detect vulnerabilities in another project. The experimental results show that unit complexity metrics are stronger vulnerability predictors than coupling metrics. Then, we propose a new set of coupling metrics which are called Included Vulnerable Header (IVH) metrics. These new coupling metrics, which consider interaction of application modules with outside of the application, predict vulnerabilities highly better than regular coupling metrics. Furthermore, adding IVH metrics to the set of complexity metrics improves Recall of the best predictor from 60.9% to 87.4% and shows the best set of metrics for cross-project vulnerability prediction.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1415–1421},
numpages = {7},
keywords = {complexity metrics, coupling metrics, cross-project, software security, vulnerability prediction},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1007/s10664-016-9484-y,
author = {Le, Tien-Duy B. and Thung, Ferdian and Lo, David},
title = {Will this localization tool be effective for this bug? Mitigating the impact of unreliability of information retrieval based bug localization tools},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9484-y},
doi = {10.1007/s10664-016-9484-y},
abstract = {Information retrieval (IR) based bug localization approaches process a textual bug report and a collection of source code files to find buggy files. They output a ranked list of files sorted by their likelihood to contain the bug. Recently, several IR-based bug localization tools have been proposed. However, there are no perfect tools that can successfully localize faults within a few number of most suspicious program elements for every single input bug report. Therefore, it is difficult for developers to decide which tool would be effective for a given bug report. Furthermore, for some bug reports, no bug localization tools would be useful. Even a state-of-the-art bug localization tool outputs many ranked lists where buggy files appear very low in the lists. This potentially causes developers to distrust bug localization tools. In this work, we build an oracle that can automatically predict whether a ranked list produced by an IR-based bug localization tool is likely to be effective or not. We consider a ranked list to be effective if a buggy file appears in the top-N position of the list. If a ranked list is unlikely to be effective, developers do not need to waste time in checking the recommended files one by one. In such cases, it is better for developers to use traditional debugging methods or request for further information to localize bugs. To build this oracle, our approach extracts features that can be divided into four categories: score features, textual features, topic model features, and metadata features. We build a separate prediction model for each category, and combine them to create a composite prediction model which is used as the oracle. We name this solution APRILE, which stands for Automated PRediction of IR-based Bug Localization's Effectiveness. We further integrate APRILE with two other components that are learned using our bagging-based ensemble classification (BEC) method. We refer to the extension of APRILE as APRILE +. We have evaluated APRILE + to predict the effectiveness of three state-of-the-art IR-based bug localization tools on more than three thousands bug reports from AspectJ, Eclipse, SWT, and Tomcat. APRILE + can achieve an average precision, recall, and F-measure of 77.61 %, 88.94 %, and 82.09 %, respectively. Furthermore, APRILE + outperforms a baseline approach by Le and Lo and APRILE by up to a 17.43 % and 10.51 % increase in F-measure respectively.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2237–2279},
numpages = {43},
keywords = {Information retrieval, Text classification, Bug localization, Bug reports, Effectiveness prediction}
}

@inproceedings{10.1145/3196398.3196444,
author = {M\"{a}ntyl\"{a}, Mika V. and Calefato, Fabio and Claes, Maelick},
title = {Natural language or not (NLON): a package for software engineering text analysis pipeline},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196444},
doi = {10.1145/3196398.3196444},
abstract = {The use of natural language processing (NLP) is gaining popularity in software engineering. In order to correctly perform NLP, we must pre-process the textual information to separate natural language from other information, such as log messages, that are often part of the communication in software engineering. We present a simple approach for classifying whether some textual input is natural language or not. Although our NLoN package relies on only 11 language features and character tri-grams, we are able to achieve an area under the ROC curve performances between 0.976-0.987 on three different data sources, with Lasso regression from Glmnet as our learner and two human raters for providing ground truth. Cross-source prediction performance is lower and has more fluctuation with top ROC performances from 0.913 to 0.980. Compared with prior work, our approach offers similar performance but is considerably more lightweight, making it easier to apply in software engineering text mining pipelines. Our source code and data are provided as an R-package for further improvements.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {387–391},
numpages = {5},
keywords = {character n-grams, filtering, glmnet, lasso, logistic regression, machine learning, natural language processing, preprocessing, regular expressions},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/MSR.2019.00054,
author = {Montandon, Jo\~{a}o Eduardo and Silva, Luciana Lourdes and Valente, Marco Tulio},
title = {Identifying experts in software libraries and frameworks among GitHub users},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00054},
doi = {10.1109/MSR.2019.00054},
abstract = {Software development increasingly depends on libraries and frameworks to increase productivity and reduce time-to-market. Despite this fact, we still lack techniques to assess developers expertise in widely popular libraries and frameworks. In this paper, we evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io. First, we collect 13 features about developers activity on GitHub projects, including commits on source code files that depend on these libraries. We also build a ground truth including the expertise of 575 developers on the studied libraries, as self-reported by them in a survey. Based on our findings, we document the challenges of using machine learning classifiers to predict expertise in software libraries, using features extracted from GitHub. Then, we propose a method to identify library experts based on clustering feature data from GitHub; by triangulating the results of this method with information available on Linkedin profiles, we show that it is able to recommend dozens of GitHub users with evidences of being experts in the studied JavaScript libraries. We also provide a public dataset with the expertise of 575 developers on the studied libraries.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {276–287},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1504/IJCAT.2015.070498,
author = {Gupta, Chetna and Srivastav, Maneesha and Gupta, Varun},
title = {Software change impact analysis: an approach to differentiate type of change to minimise regression test selection},
year = {2015},
issue_date = {July 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {51},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2015.070498},
doi = {10.1504/IJCAT.2015.070498},
abstract = {Software evolution is a continuous process carried out with the aim of extending base applications either for adding new functionalities or for adapting software to the changing environment. This paper proposes a new approach of estimating impact analysis by allocating tokens to the changes encountered in the two versions of the software system. An algorithm is proposed for token allocation and for determining matches which take into account a minimum threshold value to predict the matched results. To establish confidence in effectiveness and efficiency, presented technique is illustrated with the help of an example and the results of analysis are promising towards achieving the aim of the proposed endeavour. Further, a performance-based comparison between existing techniques is also provided in support of this research. The impact set produced will be more precise than other techniques and this data can then be used by a software engineer in determining the changes made to the software system. Thus, the proposed technique can help software engineer to perform selective regression testing by optimising the number of test cases.},
journal = {Int. J. Comput. Appl. Technol.},
month = jul,
pages = {366–375},
numpages = {10}
}

@inproceedings{10.1145/3357223.3362701,
author = {Dogga, Pradeep and Narasimhan, Karthik and Sivaraman, Anirudh and Netravali, Ravi},
title = {A System-Wide Debugging Assistant Powered by Natural Language Processing},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362701},
doi = {10.1145/3357223.3362701},
abstract = {Despite advances in debugging tools, systems debugging today remains largely manual. A developer typically follows an iterative and time-consuming process to move from a reported bug to a bug fix. This is because developers are still responsible for making sense of system-wide semantics, bridging together outputs and features from existing debugging tools, and extracting information from many diverse data sources (e.g., bug reports, source code, comments, documentation, and execution traces). We believe that the latest statistical natural language processing (NLP) techniques can help automatically analyze these data sources and significantly improve the systems debugging experience. We present early results to highlight the promise of NLP-powered debugging, and discuss systems and learning challenges that must be overcome to realize this vision.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {171–177},
numpages = {7},
keywords = {natural language processing, systems debugging},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.1145/3239235.3240503,
author = {Xu, Bowen and Shirani, Amirreza and Lo, David and Alipour, Mohammad Amin},
title = {Prediction of relatedness in stack overflow: deep learning vs. SVM: a reproducibility study},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3240503},
doi = {10.1145/3239235.3240503},
abstract = {Background Xu et al. used a deep neural network (DNN) technique to classify the degree of relatedness between two knowledge units (question-answer threads) on Stack Overflow. More recently, extending Xu et al.'s work, Fu and Menzies proposed a simpler classification technique based on a fine-tuned support vector machine (SVM) that achieves similar performance but in a much shorter time. Thus, they suggested that researchers need to compare their sophisticated methods against simpler alternatives.Aim The aim of this work is to replicate the previous studies and further investigate the validity of Fu and Menzies' claim by evaluating the DNN- and SVM-based approaches on a larger dataset. We also compare the effectiveness of these two approaches against SimBow, a lightweight SVM-based method that was previously used for general community question-answering.Method We (1) collect a large dataset containing knowledge units from Stack Overflow, (2) show the value of the new dataset addressing shortcomings of the original one, (3) re-evaluate both the DNN-and SVM-based approaches on the new dataset, and (4) compare the performance of the two approaches against that of SimBow.Results We find that: (1) there are several limitations in the original dataset used in the previous studies, (2) effectiveness of both Xu et al.'s and Fu and Menzies' approaches (as measured using F1-score) drop sharply on the new dataset, (3) similar to the previous finding, performance of SVM-based approaches (Fu and Menzies' approach and SimBow) are slightly better than the DNN-based approach, (4) contrary to the previous findings, Fu and Menzies' approach runs much slower than DNN-based approach on the larger dataset - its runtime grows sharply with increase in dataset size, and (5) SimBow outperforms both Xu et al. and Fu and Menzies' approaches in terms of runtime.Conclusion We conclude that, for this task, simpler approaches based on SVM performs adequately well. We also illustrate the challenges brought by the increased size of the dataset and show the benefit of a lightweight SVM-based approach for this task.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {21},
numpages = {10},
keywords = {deep learning, relatedness prediction, support vector machine},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/1835804.1835822,
author = {Maxwell, Evan K. and Back, Godmar and Ramakrishnan, Naren},
title = {Diagnosing memory leaks using graph mining on heap dumps},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835822},
doi = {10.1145/1835804.1835822},
abstract = {Memory leaks are caused by software programs that prevent the reclamation of memory that is no longer in use. They can cause significant slowdowns, exhaustion of available storage space and, eventually, application crashes. Detecting memory leaks is challenging because real-world applications are built on multiple layers of software frameworks, making it difficult for a developer to know whether observed references to objects are legitimate or the cause of a leak. We present a graph mining solution to this problem wherein we analyze heap dumps to automatically identify subgraphs which could represent potential memory leak sources. Although heap dumps are commonly analyzed in existing heap profiling tools, our work is the first to apply a graph grammar mining solution to this problem. Unlike classical graph mining work, we show that it suffices to mine the dominator tree of the heap dump, which is significantly smaller than the underlying graph. Our approach identifies not just leaking candidates and their structure, but also provides aggregate information about the access path to the leaks. We demonstrate several synthetic as well as real-world examples of heap dumps for which our approach provides more insight into the problem than state-of-the-art tools such as Eclipse's MAT.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {115–124},
numpages = {10},
keywords = {dominator tree, graph grammars, graph mining, heap profiling, memory leaks},
location = {Washington, DC, USA},
series = {KDD '10}
}

@article{10.4018/ijsse.2014070102,
author = {Hovsepyan, Aram and Scandariato, Riccardo and Steff, Maximilian and Joosen, Wouter},
title = {Design Churn as Predictor of Vulnerabilities?},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {1947-3036},
url = {https://doi.org/10.4018/ijsse.2014070102},
doi = {10.4018/ijsse.2014070102},
abstract = {This paper evaluates a metric suite to predict vulnerable Java classes based on how much the design of an application has changed over time. It refers to this concept as design churn in analogy with code churn. Based on a validation on 10 Android applications, it shows that several design churn metrics are in fact significantly associated with vulnerabilities. When used to build a prediction model, the metrics yield an average precision of 0.71 and an average recall of 0.27.},
journal = {Int. J. Secur. Softw. Eng.},
month = jul,
pages = {16–31},
numpages = {16},
keywords = {Android Applications, Machine Learning, Security Vulnerability Prediction, Software Metrics}
}

@article{10.5555/3159246.3159248,
title = {Hybrid firefly algorithm based regression testcase prioritisation},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {4},
issn = {1743-8195},
abstract = {Regression testing is one among the most serious activities of software development and conservation. The main influence of our study is regression test case generation, factors documentation, clustering for test case prioritisation and optimisation of ordered test case. In this investigation, the K-means clustering algorithm will be utilised to discrete the pertinent test cases from immaterial test cases. Pertinent test cases signify the prioritised test cases. We will reflect only these pertinent test cases subsequent from the clustering algorithm to optimise it along with hybrid fire fly algorithm HFFA. The hybridisation of artificial bee colony ABC algorithm and also the firefly FF algorithm are utilised for the function of HFFA. The FF will be administered within the scout bee constituent of ABC that leads to fast conjunction and restricted search space controlled depended on optimisation of locations in our HFFA optimisation algorithm. Therefore we will acquire effective prioritised test cases.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {340–357},
numpages = {18}
}

@article{10.1007/s10664-021-10003-7,
author = {Yang, Deheng and Liu, Kui and Kim, Dongsun and Koyuncu, Anil and Kim, Kisub and Tian, Haoye and Lei, Yan and Mao, Xiaoguang and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Where were the repair ingredients for Defects4j bugs? Exploring the impact of repair ingredient retrieval on the performance of 24 program repair systems},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10003-7},
doi = {10.1007/s10664-021-10003-7},
abstract = {A significant body of automated program repair research has built approaches under the redundancy assumption. Patches are then heuristically generated by leveraging repair ingredients (change actions and donor code) that are found in code bases (either the buggy program itself or big code). For example, common change actions (i.e., fix patterns) are frequently mined offline and serve as an important ingredient for many patch generation engines. Although the repetitiveness of code changes has been studied in general, the literature provides little insight into the relationship between the performance of the repair system and the source code base where the change actions were mined. Similarly, donor code is another important repair ingredient to concretize patches guided by abstract patterns. Yet, little attention has been paid to where such ingredients can actually be found. Through a large scale empirical study on the execution results of 24 repair systems evaluated on real-world bugs from Defects4J, we provide a comprehensive view on the distribution of repair ingredients that are relevant for these bugs. In particular, we show that (1) a half of bugs cannot be fixed simply because the relevant repair ingredient is not available in the search space of donor code; (2) bugs that are correctly fixed by literature tools are mostly addressed with shallow change actions; (3) programs with little history of changes can benefit from mining change actions in other programs; (4) parts of donor code to repair a given bug can be found separately at different search locations; (5) bug-triggering test cases are a rich source for donor code search.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {33},
keywords = {Automated Program Repair, Fix Ingredient, Code Change Action, Donor Code}
}

@inproceedings{10.1145/2901739.2901753,
author = {Trautsch, Fabian and Herbold, Steffen and Makedonski, Philip and Grabowski, Jens},
title = {Adressing problems with external validity of repository mining studies through a smart data platform},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901753},
doi = {10.1145/2901739.2901753},
abstract = {Research in software repository mining has grown considerably the last decade. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the external validity of results. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created the prototype SmartSHARK that implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present SmartSHARK and discuss our experiences regarding the use of SmartSHARK and the mentioned problems.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {97–108},
numpages = {12},
keywords = {smart data, software analytics, software mining},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1109/ICSE43902.2021.00140,
author = {Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
title = {FlakeFlagger: Predicting Flakiness Without Rerunning Tests},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00140},
doi = {10.1109/ICSE43902.2021.00140},
abstract = {When developers make changes to their code, they typically run regression tests to detect if their recent changes (re)introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1572–1584},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2617848.2617857,
author = {Kim, Youngsoo and Jiang, Lingxiao},
title = {The Learning Curves in Open-Source Software (OSS) Development Network},
year = {2014},
isbn = {9781450326186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2617848.2617857},
doi = {10.1145/2617848.2617857},
abstract = {We examine the learning curves of individual software developers in Open-Source Software (OSS) Development. We collected the dataset of multi-year code change histories from the repositories for five open source software projects involving more than 100 developers. We build and estimate regression models to assess individual developers' learning progress (in reducing the likelihood they may make a bug). Our estimation results show that developer's coding experience does not decrease bug ratios while cumulative bug-fixing experience leads to learning progress. The results may have implications and provoke future research on project management about allocating resources on tasks that add new code versus tasks that debug and fix existing code. We also find that different developers indeed make different kinds of bug patterns, supporting personalized bug prediction in OSS network. We found the moderating effects of bug types on learning progress. Developers exhibit learning effects for some simple bug types (e.g., wrong literals) or bug types with many instances (e.g., wrong if conditionals).},
booktitle = {Proceedings of the Sixteenth International Conference on Electronic Commerce},
pages = {41–48},
numpages = {8},
location = {Philadelphia, PA, USA},
series = {ICEC '14}
}

@article{10.1007/s10664-018-9640-7,
author = {Chowdhury, Shaiful and Borle, Stephanie and Romansky, Stephen and Hindle, Abram},
title = {GreenScaler: training software energy models with automatic test generation},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9640-7},
doi = {10.1007/s10664-018-9640-7},
abstract = {Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications (apps) are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data we can make more accurate energy models. This data, however, was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. In this paper, we show that automatic random test generation with resource-utilization heuristics can be used successfully to build accurate software energy consumption models. Code coverage, although well-known as a heuristic for generating and selecting tests in traditional software testing, performs poorly at selecting energy hungry tests. We propose an accurate software energy model, GreenScaler, that is built on random tests with CPU-utilization as the test selection heuristic. GreenScaler not only accurately estimates energy consumption for randomly generated tests, but also for meaningful developer written tests. Also, the produced models are very accurate in detecting energy regressions between versions of the same app. This is directly helpful for the app developers who want to know if a change in the source code, for example, is harmful for the total energy consumption. We also show that developers can use GreenScaler to select the most energy efficient API when multiple APIs are available for solving the same problem. Researchers can also use our test generation methodology to further study how to build more accurate software energy models.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1649–1692},
numpages = {44},
keywords = {Automatic software testing, Energy modeling, Energy optimization, Machine learning, Mining software repositories, Software energy consumption, Software energy efficiency, Test generation}
}

@article{10.1016/j.neucom.2019.02.040,
author = {Mohandes, Saeed Reza and Zhang, Xueqing and Mahdiyar, Amir},
title = {A comprehensive review on the application of artificial neural networks in building energy analysis},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {340},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.02.040},
doi = {10.1016/j.neucom.2019.02.040},
journal = {Neurocomput.},
month = may,
pages = {55–75},
numpages = {21},
keywords = {Building energy analysis, Water heating and cooling systems, Heating ventilation air conditioning, Indoor air temperature, Building energy consumption, Artificial neural networks, ANN, AI, RBF, Tanh, Log, GF, MLRNN, GRNN, ARXNN, RBFNN, RNN, LSTMNN, CNN, GD, CG, BP, LM, NM, BR, BRBP, BPNN, GDBP, LMBP, NMBP, NPA, RS, HVAC, DT, SVR, SVM, MLR, CBR, PCA, CDA, CRBM, FCRBM, ARX, SHGC, EoS, COP, DAT, PID, RCMAC, AHU, FDD, SH, DHW, TRNSYS, MATLAB, BEC, EC, ELEC, ASHRAE, HCL, SR, AT, IAT, OAT, DBT, WS, O, SI, H, CL, HL, R2, R, RMSE, MSE, ERR, MAE, MRE, MBE, MAPE, APE, MAXERR, CV, CVRMSE, SSE, and NSEC}
}

@inproceedings{10.1145/3213846.3213848,
author = {Cummins, Chris and Petoumenos, Pavlos and Murray, Alastair and Leather, Hugh},
title = {Compiler fuzzing through deep learning},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213848},
doi = {10.1145/3213846.3213848},
abstract = {Random program generation — fuzzing — is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03\texttimes{} less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {95–105},
numpages = {11},
keywords = {Compiler Fuzzing, Deep Learning, Differential Testing},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@article{10.1016/j.infsof.2013.04.002,
author = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
title = {Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.04.002},
doi = {10.1016/j.infsof.2013.04.002},
abstract = {Context: SQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure. Objective: This paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to SQLI or XSS. Method: From the observations of input sanitization code that are commonly implemented in web applications to avoid SQLI and XSS vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. We then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict SQLI and XSS vulnerabilities. Results: We developed a prototype tool called PhpMinerI for data collection and used it to evaluate our models on eight open source web applications. Our best model achieved an averaged result of 93% recall and 11% false alarm rate in predicting SQLI vulnerabilities, and 78% recall and 6% false alarm rate in predicting XSS vulnerabilities. Conclusion: The experiment results show that our proposed vulnerability predictors are useful and effective at predicting SQLI and XSS vulnerabilities.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1767–1780},
numpages = {14},
keywords = {Data mining, Empirical study, Input sanitization, Static code attributes, Vulnerability prediction, Web application vulnerability}
}

@inproceedings{10.1109/ASE.2019.00055,
author = {Zaman, Tarannum Shaila and Han, Xue and Yu, Tingting},
title = {SCMiner: localizing system-level concurrency faults from large system call traces},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00055},
doi = {10.1109/ASE.2019.00055},
abstract = {Localizing concurrency faults that occur in production is hard because, (1) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers to reproduce the failure; (2) it is often impractical to assume the availability of multiple failing executions to localize the faults using existing techniques; (3) it is challenging to search for buggy locations in an application given limited runtime data; and, (4) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which cannot be handled by existing tools for diagnosing intra-process (thread-level) failures. To address these problems, we present SCMiner, a practical online bug diagnosis tool to help developers understand how a system-level concurrency fault happens based on the logs collected by the default system audit tools. SCMiner achieves online bug diagnosis to obviate the need for offline bug reproduction. SCMiner does not require code instrumentation on the production system or rely on the assumption of the availability of multiple failing executions. Specifically, after the system call traces are collected, SCMiner uses data mining and statistical anomaly detection techniques to identify the failure-inducing system call sequences. It then maps each abnormal sequence to specific application functions. We have conducted an empirical study on 19 real-world benchmarks. The results show that SCMiner is both effective and efficient at localizing system-level concurrency faults.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {515–526},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1155/2021/4788357,
author = {Tang, Yahui and Li, Tong and Zhu, Rui and Du, Fei and Wang, Jishu and Ma, Zifei and Ortin, Francisco},
title = {A Discovery Method for Hierarchical Software Execution Behavior Models Based on Components},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4788357},
doi = {10.1155/2021/4788357},
abstract = {Software is rapidly evolving and operates in a changing environment; therefore, in addition to software design and testing, it is essential to observe and understand the software execution behavior by modeling data recorded during the execution of the software to improve its reliability. The nested call relationship between methods during the execution of software is common, but most process-mining methods are unable to discover them, only generating flat models with low fitness. Meanwhile, it is easy to generate “spaghetti-like” models with low comprehensibility when dealing with complex software execution data. This paper proposes a component-based hierarchical software behavior model discovery method that can discover hierarchical nested call structures during software runtime, improving the fitness of the model; meanwhile, the proposed method partitions the discovery model into several parts by component information to improve the comprehensibility of the model, which can also reflect the interaction behavior within and between components. The proposed approach was implemented in a process mining toolkit. Using real-life software event logs and public datasets, we demonstrated that compared with other advanced process mining techniques, our approach can visualize actual software execution behavior in a more accurate and easy-to-understand way while balancing time performance.},
journal = {Sci. Program.},
month = jan,
numpages = {22}
}

@inproceedings{10.5555/2663370.2663378,
author = {Kanewala, Upulee and Bieman, James M.},
title = {Techniques for testing scientific programs without an Oracle},
year = {2013},
isbn = {9781467362610},
publisher = {IEEE Press},
abstract = {The existence of an oracle is often assumed in software testing. But in many situations, especially for scientific programs, oracles do not exist or they are too hard to implement. This paper examines three techniques that are used to test programs without oracles: (1) Metamorphic testing, (2) Run-time Assertions and (3) Developing test oracles using machine learning. We examine these methods in terms of their (1) fault finding ability, (2) automation, and (3) required domain knowledge. Several case studies apply these three techniques to effectively test scientific programs that do not have oracles. Certain techniques have reported a better fault finding ability than the others when testing specific programs. Finally, there is potential to increase the level of automation of these techniques, thereby reducing the required level of domain knowledge. Techniques that can potentially be automated include (1) detection of likely metamorphic relations, (2) static analyses to eliminate spurious invariants and (3) structural analyses to develop machine learning generated oracles.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering for Computational Science and Engineering},
pages = {48–57},
numpages = {10},
keywords = {assertion checking, machine learning, metamorphic relation, metamorphic testing, mutation analysis, scientific software testing, test oracles},
location = {San Francisco, California},
series = {SE-CSE '13}
}

@inproceedings{10.5555/2820518.2820522,
author = {Greiler, Michaela and Herzig, Kim and Czerwonka, Jacek},
title = {Code ownership and software quality: a replication study},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {In a traditional sense, ownership determines rights and duties in regard to an object, for example a property. The owner of source code usually refers to the person that invented the code. However, larger code artifacts, such as files, are usually composed by multiple engineers contributing to the entity over time through a series of changes. Frequently, the person with the highest contribution, e.g. the most number of code changes, is defined as the code owner and takes responsibility for it. Thus, code ownership relates to the knowledge engineers have about code. Lacking responsibility and knowledge about code can reduce code quality. In an earlier study, Bird et al. [1] showed that Windows binaries that lacked clear code ownership were more likely to be defect prone. However recommendations for large artifacts such as binaries are usually not actionable. E.g. changing the concept of binaries and refactoring them to ensure strong ownership would violate system architecture principles. A recent replication study by Foucault et al. [2] on open source software replicate the original results and lead to doubts about the general concept of ownership impacting code quality. In this paper, we replicated and extended the previous two ownership studies [1, 2] and reflect on their findings. Further, we define several new ownership metrics to investigate the dependency between ownership and code quality on file and directory level for 4 major Microsoft products. The results confirm the original findings by Bird et al. [1] that code ownership correlates with code quality. Using new and refined code ownership metrics we were able to classify source files that contained at least one bug with a median precision of 0.74 and a median recall of 0.38. On directory level, we achieve a precision of 0.76 and a recall of 0.60.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {2–12},
numpages = {11},
keywords = {code ownership, empirical software engineering, software quality},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.15388/21-INFOR454,
author = {Sayago-Heredia, Jaime and P\'{e}rez-Castillo, Ricardo and Piattini, Mario},
title = {A Systematic Mapping Study on Analysis of Code Repositories},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {3},
issn = {0868-4952},
url = {https://doi.org/10.15388/21-INFOR454},
doi = {10.15388/21-INFOR454},
abstract = {Code repositories contain valuable information, which can be extracted, processed and synthesized into valuable information. It enabled developers to improve maintenance, increase code quality and understand software evolution, among other insights. Certain research has been made during the last years in this field. This paper presents a systematic mapping study to find, evaluate and investigate the mechanisms, methods and techniques used for the analysis of information from code repositories that allow the understanding of the evolution of software. Through this mapping study, we have identified the main information used as input for the analysis of code repositories (commit data and source code), as well as the most common methods and techniques of analysis (empirical/experimental and automatic). We believe the conducted research is useful for developers working on software development projects and seeking to improve maintenance and understand the evolution of software through the use and analysis of code repositories.},
journal = {Informatica},
month = jan,
pages = {619–660},
numpages = {42},
keywords = {code repository analysis, repository mining, code repository, GitHub, systematic mapping study}
}

@inproceedings{10.1145/3236024.3236052,
author = {Song, Liyan and Minku, Leandro L. and Yao, Xin},
title = {A novel automated approach for software effort estimation based on data augmentation},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236052},
doi = {10.1145/3236024.3236052},
abstract = {Software effort estimation (SEE) usually suffers from data scarcity problem due to the expensive or long process of data collection. As a result, companies usually have limited projects for effort estimation, causing unsatisfactory prediction performance. Few studies have investigated strategies to generate additional SEE data to aid such learning. We aim to propose a synthetic data generator to address the data scarcity problem of SEE. Our synthetic generator enlarges the SEE data set size by slightly displacing some randomly chosen training examples. It can be used with any SEE method as a data preprocessor. Its effectiveness is justified with 6 state-of-the-art SEE models across 14 SEE data sets. We also compare our data generator against the only existing approach in the SEE literature. Experimental results show that our synthetic projects can significantly improve the performance of some SEE methods especially when the training data is insufficient. When they cannot significantly improve the prediction performance, they are not detrimental either. Besides, our synthetic data generator is significantly superior or perform similarly to its competitor in the SEE literature. Therefore, our data generator plays a non-harmful if not significantly beneficial effect on the SEE methods investigated in this paper. Therefore, it is helpful in addressing the data scarcity problem of SEE.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {468–479},
numpages = {12},
keywords = {Software effort estimation, data augmentation, data generation, data scarcity, synthetic data},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ICSE43902.2021.00066,
author = {K\"{u}\c{c}\"{u}k, Yi\u{g}it and Henderson, Tim A. D. and Podgurski, Andy},
title = {Improving Fault Localization by Integrating Value and Predicate Based Causal Inference Techniques},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00066},
doi = {10.1109/ICSE43902.2021.00066},
abstract = {Statistical fault localization (SFL) techniques use execution profiles and success/failure information from software executions, in conjunction with statistical inference, to automatically score program elements based on how likely they are to be faulty. SFL techniques typically employ one type of profile data: either coverage data, predicate outcomes, or variable values. Most SFL techniques actually measure correlation, not causation, between profile values and success/failure, and so they are subject to confounding bias that distorts the scores they produce. This paper presents a new SFL technique, named UniVal, that uses causal inference techniques and machine learning to integrate information about both predicate outcomes and variable values to more accurately estimate the true failure-causing effect of program statements. UniVal was empirically compared to several coverage-based, predicate-based, and value-based SFL techniques on 800 program versions with real faults.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {649–660},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.jss.2018.01.033,
author = {Wang, Ying and Zhu, Zhiliang and Yang, Bo and Guo, Fangda and Yu, Hai},
title = {Using reliability risk analysis to prioritize test cases},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.01.033},
doi = {10.1016/j.jss.2018.01.033},
journal = {J. Syst. Softw.},
month = may,
pages = {14–31},
numpages = {18},
keywords = {Regression testing, Test case prioritization, Probabilistic risk analysis, Information flow, Complex network}
}

@article{10.1145/3328746,
author = {Bosu, Michael F. and Macdonell, Stephen G.},
title = {Experience: Quality Benchmarking of Datasets Used in Software Effort Estimation},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3328746},
doi = {10.1145/3328746},
abstract = {Data is a cornerstone of empirical software engineering (ESE) research and practice. Data underpin numerous process and project management activities, including the estimation of development effort and the prediction of the likely location and severity of defects in code. Serious questions have been raised, however, over the quality of the data used in ESE. Data quality problems caused by noise, outliers, and incompleteness have been noted as being especially prevalent. Other quality issues, although also potentially important, have received less attention. In this study, we assess the quality of 13 datasets that have been used extensively in research on software effort estimation. The quality issues considered in this article draw on a taxonomy that we published previously based on a systematic mapping of data quality issues in ESE. Our contributions are as follows: (1) an evaluation of the “fitness for purpose” of these commonly used datasets and (2) an assessment of the utility of the taxonomy in terms of dataset benchmarking. We also propose a template that could be used to both improve the ESE data collection/submission process and to evaluate other such datasets, contributing to enhanced awareness of data quality issues in the ESE community and, in time, the availability and use of higher-quality datasets.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {19},
numpages = {38},
keywords = {Data quality, benchmarking, empirical software engineering, missing data, noise, software effort estimation}
}

