@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {spl in practice, state of practice, tool support, tooling roadmap},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Clafer, Energy Efficiency, Metrics, Optimisation, Repository, Software Product Line, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Feature interaction, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {cloud computing, microservices, re-engineering, software product line, variability management},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3023956.3023957,
author = {Wille, David and Runge, Tobias and Seidl, Christoph and Schulze, Sandro},
title = {Extractive software product line engineering using model-based delta module generation},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023957},
doi = {10.1145/3023956.3023957},
abstract = {To satisfy demand for customized products, companies commonly apply so-called clone-and-own strategies by copying functionality from existing products and modifying it to create product variants that have to be developed, maintained, and evolved in isolation. In previous work, we introduced a variability mining technique to identify variability information (commonalities and differences) in block-based model variants (e.g., MATLAB/Simulink models), which can be used to guide manual transition from clone-and-own to managed reuse of a software product line (SPL). In this paper, we present a procedure that uses the extracted variability information to generate a transformational delta-oriented SPL fully automatically. We generate a delta language specifically tailored to transforming models in the analyzed modeling language and utilize it to generate delta modules expressing variation of the SPL's implementation artifacts. The procedure seamlessly integrates with our variability mining technique and allows to fully adopt a managed reuse strategy (i.e., generation of products from a single code base) without manual overhead. We show the feasibility of the procedure by applying it to state chart and MATLAB/Simulink model variants from two industrial case studies.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {36–43},
numpages = {8},
keywords = {clone-and-own, delta modeling, extractive product line engineering, model-based, variability mining},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {SBSE, SPL, genetic programming, program synthesis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00122,
author = {Rosiak, Kamil},
title = {Extractive multi product-line engineering},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00122},
doi = {10.1109/ICSE-Companion52605.2021.00122},
abstract = {Cloning is a general approach to create new functionality within variants as well as new system variants. It is a fast, flexible, intuitive, and economical approach to evolve systems in the short run. However, in the long run, the maintenance effort increases. A common solution to this problem is the extraction of a product line from a set of cloned variants. This process requires a detailed analysis of variants to extract variability information. However, clones within a variant are usually not considered in the process, but are also a cause for unsustainable software. This thesis proposes an extractive multi product-line engineering approach to re-establish the sustainable development of software variants. We propose an approach to re-engineer intra-system and inter-system clones into reusable, configurable components stored in an integrated platform and synthesize a matching multilayer feature model.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {263–265},
numpages = {3},
keywords = {clone detection, multi product-line, refactoring, variability mining},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2491627.2499880,
author = {Clarke, Dave and Schaefer, Ina and ter Beek, Maurice H. and Apel, Sven and Atlee, Joanne M.},
title = {Formal methods and analysis in software product line engineering: 4th edition of FMSPLE workshop series},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2499880},
doi = {10.1145/2491627.2499880},
abstract = {FMSPLE 2013 is the fourth edition of the FMSPLE workshop series aimed at connecting researchers and practitioners interested in raising the efficiency and the effectiveness of software product line engineering through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {266–267},
numpages = {2},
keywords = {evolution, formal methods, semantics, software product lines, testing, variability, verification},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1007/978-3-662-45234-9_18,
author = {Schaefer, Ina and Beek, Maurice H.},
title = {Fomal Methods and Analyses in Software Product Line Engineering},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_18},
doi = {10.1007/978-3-662-45234-9_18},
abstract = {Software product line engineering SPLE [5,11] aims to develop a family of software-intensive systems via systematic, large-scale reuse in order to reduce time-to-market and costs and to increase the quality of individual products. In order to achieve these goals, formal methods offer promising analysis techniques, which are best applied throughout the product-line lifecycle so as to maximize their overall efficiency and effectiveness.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {253–256},
numpages = {4}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Service-oriented architecture, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/2362536.2362576,
author = {ter Beek, Maurice H. and Becker, Martin and Classen, Andreas and Roos-Frantz, Fabricia and Schaefer, Ina and Wong, Peter Y. H.},
title = {Formal methods and analysis in software product line engineering: 3rd edition of FMSPLE workshop series},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362576},
doi = {10.1145/2362536.2362576},
abstract = {FMSPLE 2012 is the third edition of the FMSPLE workshop series, traditionally affiliated with SPLC, which aims to connect researchers and practitioners interested in raising the efficiency and the effectiveness of SPLE through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {286–287},
numpages = {2},
keywords = {evolution, formal methods, semantics, software product lines, testing, variability, verification},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {model driven engineering, non-functional requirements, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/2491627.2493906,
author = {Bashari, Mahdi and Bagheri, Ebrahim},
title = {Engineering self-adaptive systems and dynamic software product line},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2493906},
doi = {10.1145/2491627.2493906},
abstract = {Self-adaptive systems are a class of software applications, which are able to dynamically transform their internal structure and hence their behavior in response to internal or external stimuli. The transformation may provide the basis for new functionalities or improve or maintain non-functional properties in order to match the application better to its operational requirements and standards. Software Product Line Engineering has rich methods and techniques in variability modeling and management which is one of the main issues in developing self-adaptive systems. Dynamic software product lines (DSPL) have been proposed to exploit the knowledge acquired in SPLE to develop self-adaptive software systems.In this tutorial, we portray the problem of developing self-adaptive systems. Then we investigate how the idea of dynamic software product line could help to deal with the challenges that we face in developing efficient self-adaptive software. We also offer insight into the different approaches that use dynamic software product line engineering for developing self-adaptive systems focusing on practical approaches by showing how the approaches are applied to real case studies and also methods for evaluating these approaches. This tutorial also discuss how DSPL could be used some relevant areas to self-adaptive systems and challenges which still exist in the area.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {285},
numpages = {1},
keywords = {dynamic software product line, self-adaptive systems},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1109/APSEC.2008.45,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Saake, Gunter},
title = {Measuring Non-Functional Properties in Software Product Line for Product Derivation},
year = {2008},
isbn = {9780769534466},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2008.45},
doi = {10.1109/APSEC.2008.45},
abstract = {A software product line (SPL) enables stakeholders to derive different software products for a domain while providing a high degree of reuse of their code units. Software products are derived in a configuration process by composing different code units. The configuration process becomes complex if SPLs contain hundreds of features. In many cases, a stakeholder is not only interested in functional but also in non-functional properties of a desired product. Because SPLs can be used in different application scenarios alternative implementations of already existing functionality are developed to meet special non-functional requirements, like restricted binary size and performance guarantees. To enable these complex configurations we discuss and present techniques to measure non-functional properties of software modules and use these values to compute SPL configurations optimized to the users needs.},
booktitle = {Proceedings of the 2008 15th Asia-Pacific Software Engineering Conference},
pages = {187–194},
numpages = {8},
keywords = {Non-functional Properties, Product Derivation, Software Product Lines},
series = {APSEC '08}
}

@inproceedings{10.1145/2648511.2648548,
author = {Sierszecki, Krzysztof and Steffens, Michaela and Hojrup, Helene H. and Savolainen, Juha and Beuche, Danilo},
title = {Extending variability management to the next level},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648548},
doi = {10.1145/2648511.2648548},
abstract = {Danfoss Power Electronics is a centre with both extensive power electronics know-how and many competencies within frequency converters and solar inverters. Development of embedded controllers built in Danfoss products raises similar challenges found in many other companies: creation of product series with an increasing number of variants, while at the same time decreasing time-to-market and keeping development costs low. Introduction of a Software Product Line approach into product development is a challenge that Danfoss Power Electronics decided to take in order to reduce software development efforts few years ago. The approach has been successful allowing for development of a number of highly engineered products. However, the software product line is in a constant evolution. It grows over time as new functionality is added in the form of extra software artefacts and further products are configured from it. As a result, the overall complexity and maintenance of assets hinders further efficiency of the approach. This paper presents extension of the variability management that goes beyond the scope of software assets reuse previously introduced into the organization. A prototype of the technique linking multi-level variability management is further elaborated using pure::variants.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {320–329},
numpages = {10},
keywords = {industrial experience, product specifications, variability management},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@inproceedings{10.1145/2701319.2701326,
author = {Soares, Larissa Rocha and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Non-Functional Properties in Software Product Lines: A Reuse Approach},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701326},
doi = {10.1145/2701319.2701326},
abstract = {Software Product Line Engineering (SPLE) emerges for software organizations interested in customized products at reasonable costs. Based on the selection of features, stakeholders can derive programs satisfying a range of functional properties and non-functional ones. The explicit definition of Non-Functional Properties (NFP) during software configuration has been considered a challenging task. Dealing with them is not well established yet, neither in theory nor in practice. In this sense, we present a framework to specify NFP for SPLE and we also propose a reuse approach that promotes the reuse of NFP values during the product configuration. We discuss the results of a case study aimed to evaluate the applicability of the proposed work.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {Empirical Software Engineering, Quality Attributes, Software Product Line},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {ClaferMoo, ClaferMoo visualizer, clafer, exploration, feature modeling, optimal variant, pareto front, product line engineering, visualization},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.knosys.2019.104883,
author = {Ayala, Inmaculada and Amor, Mercedes and Horcas, Jose-Miguel and Fuentes, Lidia},
title = {A goal-driven software product line approach for evolving multi-agent systems in the Internet of Things},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {184},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104883},
doi = {10.1016/j.knosys.2019.104883},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {Software product line, Evolution, Internet of Things, MAS-PL, Goal models, GORE}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {configuration, feature model, recommendation systems, software product lines, visualization},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1109/SPLC.2011.20,
author = {Siegmund, Norbert and Rosenmuller, Marko and Kastner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable Prediction of Non-functional Properties in Software Product Lines},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.20},
doi = {10.1109/SPLC.2011.20},
abstract = {A software product line is a family of related software products, typically, generated from a set of common assets. Users can select features to derive a product that fulfills their needs. Often, users expect a product to have specific non-functional properties, such as a small footprint or a minimum response time. Because a product line can contain millions of products, it is usually not feasible to generate and measure non-functional properties for each possible product of a product line. Hence, we propose an approach to predict a product's non-functional properties, based on the product's feature selection. To this end, we generate and measure a small set of products, and by comparing the measurements, we approximate each feature's non-functional properties. By aggregating the approximations of selected features, we predict the product's properties. Our technique is independent of the implementation approach and language. We show how already little domain knowledge can improve predictions and discuss trade-offs regarding accuracy and the required number of measurements. Although our approach is in general applicable for quantifiable non-functional properties, we evaluate it for the non-functional property footprint. With nine case studies, we demonstrate that our approach usually predicts the footprint with an accuracy of 98% and an accuracy of over 99% if feature interactions are known.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {160–169},
numpages = {10},
keywords = {SPL Conqueror, measurement, non-functional properties, predicition, software product lines},
series = {SPLC '11}
}

@inproceedings{10.5220/0007955905140521,
author = {Derras, M. and Deruelle, L. and Douin, J.-M. and Levy, N. and Losavio, F. and Mahamane, R. and Reiner, V.},
title = {Approach for Variability Management of Legal Rights in Human Resources Software Product Lines},
year = {2019},
isbn = {9789897583797},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0007955905140521},
doi = {10.5220/0007955905140521},
abstract = {This work concerns software product lines (SPL); it comes from the experience gained collaborating with Berger-Levrault, a French society leader in Human Resources systems. This enterprise serves many French and European territorial communities. They had a variability problem associated to the differences of applicable legal rights in different countries or territories, and this activity was performed manually at a high cost. On the other hand, functionalities were common and mandatory and did not vary much. The crucial issue in SPL development and practice is to manage the correct selection of variants. However, no standard methods have been developed yet, and industry builds SPL using on-the-market or in-house techniques and methods, aware of the benefits a product line can provide; nevertheless, this development must return the investment, and this is not always the case. In this work an approach to variability management in case of legal rights applicability to different entities is proposed. This architecture-centric and quality-based approach uses a reference architecture that has been built with a bottom-up strategy. Variability is incorporated to the reference architecture at abstract level considering non-functional properties. A production plan to reduce the gap between abstraction and implementation levels is defined.},
booktitle = {Proceedings of the 14th International Conference on Software Technologies},
pages = {514–521},
numpages = {8},
keywords = {Human Resources, Legal Rights., Software Product Lines (SPL), Variability Management},
location = {Prague, Czech Republic},
series = {ICSOFT 2019}
}

@inproceedings{10.1145/2814204.2814212,
author = {Seidl, Christoph and Schuster, Sven and Schaefer, Ina},
title = {Generative software product line development using variability-aware design patterns},
year = {2015},
isbn = {9781450336871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814204.2814212},
doi = {10.1145/2814204.2814212},
abstract = {Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing an SPL, instances of certain design patterns are employed to handle variability, which makes these "variability-aware design patterns" a best practice for SPL design. However, there currently is no dedicated method for proactively developing SPL using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to elements of realization artifacts to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. With this method, we support proactive development of SPL using design patterns to apply best practices for the realization of variability. We present an implementation of our approach within the Eclipse IDE and demonstrate it within a case study.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {151–160},
numpages = {10},
keywords = {Design Pattern, Generative Development, Role Modeling, Software Product Line (SPL)},
location = {Pittsburgh, PA, USA},
series = {GPCE 2015}
}

@article{10.1016/j.infsof.2012.07.020,
author = {Siegmund, Norbert and Rosenm\"{u}Ller, Marko and K\"{a}Stner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.020},
doi = {10.1016/j.infsof.2012.07.020},
abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {491–507},
numpages = {17},
keywords = {Measurement, Non-functional properties, Prediction, SPL Conqueror, Software product lines}
}

@inproceedings{10.1109/SEAA.2014.48,
author = {Soares, Larissa Rocha and Potena, Pasqualina and Machado, Ivan do Carmo and Crnkovic, Ivica and Almeida, Eduardo Santana de},
title = {Analysis of Non-functional Properties in Software Product Lines: A Systematic Review},
year = {2014},
isbn = {9781479957958},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2014.48},
doi = {10.1109/SEAA.2014.48},
abstract = {Software Product Lines (SPL) approach has been widely developed in academia and successfully applied in industry. Based on the selection of features, stakeholders can efficiently derive tailor-made programs satisfying different requirements. While SPL was very successful at building products based on identified features, achievements and preservation of many nonfunctional properties (NFPs) remain challenging. A knowledge how to deal with NFPs is still not fully obtained. In this paper, we present a systematic literature review of NFPs analysis for SPL products, focusing on runtime NFPs. The goal of the paper is twofold: (i) to present an holistic overview of SPL approaches that have been reported regarding the analysis of runtime NFPs, and (ii) to categorize NFPs treated in the scientific literature regarding development of SPLs. We analyzed 36 research papers, and identified that system performance attributes are typically the most considered. The results also aid future research studies in NFPs analysis by providing an unbiased view of the body of empirical evidence and by guiding future research directions.},
booktitle = {Proceedings of the 2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
pages = {328–335},
numpages = {8},
keywords = {Non-functional Properties, Product Derivation, Software Product Lines, Systematic Literature Review},
series = {SEAA '14}
}

@article{10.1007/s00766-013-0165-8,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Dynamic decision models for staged software product line configuration},
year = {2014},
issue_date = {June      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0165-8},
doi = {10.1007/s00766-013-0165-8},
abstract = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process.},
journal = {Requir. Eng.},
month = jun,
pages = {187–212},
numpages = {26},
keywords = {Feature models, Software product lines, Stakeholder preferences, Utility elicitation}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {core assets, higher-order simple predicate logic, quality attributes, relational algebra, type theory},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1109/SPLC.2011.33,
author = {Ghezzi, Carlo and Sharifloo, Amir Molzam},
title = {Verifying Non-functional Properties of Software Product Lines: Towards an Efficient Approach Using Parametric Model Checking},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.33},
doi = {10.1109/SPLC.2011.33},
abstract = {In this paper, we describe how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions in the early stages of development. Furthermore, we discuss how verification time can surprisingly be reduced by applying parametric model checking instead of classic model checking, and show that the approach can be effective in practice.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {170–174},
numpages = {5},
keywords = {Non-Functional Requirements, Probabilistic Model Checking, Software Product Lines},
series = {SPLC '11}
}

@inproceedings{10.1109/APSEC.2010.26,
author = {Sincero, Julio and Schroder-Preikschat, Wolfgang and Spinczyk, Olaf},
title = {Approaching Non-functional Properties of Software Product Lines: Learning from Products},
year = {2010},
isbn = {9780769542669},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2010.26},
doi = {10.1109/APSEC.2010.26},
abstract = {Approaching the configuration of non-functional properties (NFPs) in traditional software systems is not an easy task, addressing the configuration of these properties in software product lines (SPLs) imposes even further challenges. Therefore, we have devised the Feedback Approach, which extends the traditional SPL development techniques in order to improve the configuration of NFPs. In this work we present the general guidelines of our approach and also we show the feasibility of the idea by presenting a case study using the Linux Kernel.},
booktitle = {Proceedings of the 2010 Asia Pacific Software Engineering Conference},
pages = {147–155},
numpages = {9},
keywords = {Linux, Non-Functional Properties, Software Product Lines},
series = {APSEC '10}
}

@article{10.1016/j.infsof.2015.09.004,
author = {Souza Neto, Pl\'{a}cido A. and Vargas-Solar, Genoveva and da Costa, Umberto Souza and Musicante, Martin A.},
title = {Designing service-based applications in the presence of non-functional properties},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.09.004},
doi = {10.1016/j.infsof.2015.09.004},
abstract = {ContextThe development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users. ObjectiveThis paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view. MethodOur analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. ResultsThis paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. ConclusionThis systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping_service-based-app_nfr.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {84–105},
numpages = {22},
keywords = {Non-functional requirements, Service-based software process, Systematic mapping}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Feature-oriented software development, Measurement and optimization, Non-functional properties, SPL Conqueror, Software product lines}
}

@article{10.1016/j.scico.2012.05.003,
author = {Laguna, Miguel A. and Crespo, Yania},
title = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.05.003},
doi = {10.1016/j.scico.2012.05.003},
abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {1010–1034},
numpages = {25},
keywords = {Evolution, Legacy system, Reengineering, Refactoring, Software product line}
}

@article{10.1016/j.infsof.2012.07.017,
author = {Ghezzi, Carlo and Molzam Sharifloo, Amir},
title = {Model-based verification of quantitative non-functional properties for software product lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.017},
doi = {10.1016/j.infsof.2012.07.017},
abstract = {Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {508–524},
numpages = {17},
keywords = {Non-functional requirements, Parametric verification, Probabilistic model checking, Quality analysis, Software product lines}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@inproceedings{10.5555/2820656.2820666,
author = {Tzeremes, Vasilios and Gomaa, Hassan},
title = {A software product line approach for end user development of smart spaces},
year = {2015},
publisher = {IEEE Press},
abstract = {Several End User Development (EUD) tools have been proposed that enable end users to create software applications for smart spaces. Even though most of the tools focus on architecture and usability they don't take into account the end user background. For instance some end users are domain experts, experienced software developers, and others have very limited computer skills. Furthermore current EUD approaches do not address reuse. In this paper we present XANA, an EUD framework that extends existing EUD tools with Software Product Line (SPL) concepts. The framework targets two types of users: the application designers and the end users. Application designers create the SPL for end users. End users select SPL features and derive applications for their smart spaces. XANA promotes reuse by allowing end users to reuse features and components to create applications. We illustrate its use with examples in a smart home setting.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {23–26},
numpages = {4},
keywords = {end user development, feature modeling, smart spaces, software product lines, software reuse},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473942,
author = {Kahraman, G\"{o}khan and Cleophas, Loek},
title = {Automated derivation of variants in manufacturing systems design},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473942},
doi = {10.1145/3461002.3473942},
abstract = {The Logistics Specification and Analysis Tool (LSAT) is a modelbased engineering tool used for design-space exploration of flexible manufacturing systems. LSAT provides domain specific languages to model a manufacturing system and means to analyze the productivity characteristics of such a system. In LSAT, developers can specify a system and model its deterministic operations as a set of activities. Given a set of activities, it is possible to construct an individual activity sequence that represents one valid system execution, and with minor variations in the specification individual systems can be obtained. To avoid modeling each variant separately, which means cloning and maintaining the common parts, new functionality is needed to deal with the variability of system specifications. In this study, we aim to establish integration between LSAT and product line engineering techniques. Specifically, we provide a realization of a toolchain including variability representation of LSAT realization artifacts and automated variant derivation for the LSAT model variants. Delta modeling, a transformational variability realization mechanism, is employed to model the variability within LSAT realization artifacts. Using the toolchain, we develop an industry-related case for a product line, the so called Extended Twilight System, a Cyber Physical System (CPS) inspired by the CPSs of our industrial partner.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {45–50},
numpages = {6},
keywords = {delta modeling, manufacturing systems, model-based engineering, product lines, variability modeling},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.4018/ijismd.2014070103,
author = {Lotz, Alex and Ingl\'{e}s-Romero, Juan F. and Stampfer, Dennis and Lutz, Matthias and Vicente-Chicote, Cristina and Schlegel, Christian},
title = {Towards a Stepwise Variability Management Process for Complex Systems: A Robotics Perspective},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {1947-8186},
url = {https://doi.org/10.4018/ijismd.2014070103},
doi = {10.4018/ijismd.2014070103},
abstract = {Complex systems are executed in environments with a huge number of potential situations and contingencies, therefore a mechanism is required to express dynamic variability at design-time that can be efficiently resolved in the application at run-time based on the then available information. We present an approach for dynamic variability modeling and its exploitation at run-time. It supports different developer roles and allows the separation of two different kinds of dynamic variability at design-time: (i) variability related to the system operation, and (ii) variability associated with QoS. The former provides robustness to contingencies, maintaining a high success rate in task fulfillment. The latter focuses on the quality of the application execution (defined in terms of non-functional properties like safety or task efficiency) under changing situations and limited resources. The authors also discuss different alternatives for the run-time integration of the two variability management mechanisms, and show real-world robotic examples to illustrate them.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = jul,
pages = {55–74},
numpages = {20},
keywords = {Modeling Run-Time Variability, Service Robotics, SmartTCL, VML, Variability Management}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Dynamic Software Product Lines, Dynamic variability, Feature models, Software architecture}
}

@inproceedings{10.5555/2022115.2022129,
author = {Gamez, Nadia and Fuentes, Lidia},
title = {Software product line evolution with cardinality-based feature models},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature models are widely used for modelling variability present in a Software Product Line family. We propose using cardinality-based feature models and clonable features to model and manage the evolution of the structural variability present in pervasive systems, composed by a large variety of heterogeneous devices. The use of clonable features increases the expressiveness of feature models, but also greatly increases the complexity of the resulting configurations. So, supporting the evolution of product configurations becomes an intractable task to do it manually. In this paper, we propose a model driven development process to propagate changes made in an evolved feature model, into existing configurations. Furthermore, our process allows us to calculate the effort needed to perform the evolution changes in the customized products. To do this, we have defined two operators, one to calculate the differences between two configurations and another to create a new configuration from a previous one. Finally, we validate our approach, showing that by using our tool support we can generate new configurations for a family of products with thousands of cloned features.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {102–118},
numpages = {17},
keywords = {evolution, feature models, software product lines},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@inproceedings{10.1109/ICWS.2015.20,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {SPL-TQSSS: A Software Product Line Approach for Stateful Service Selection},
year = {2015},
isbn = {9781467372725},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICWS.2015.20},
doi = {10.1109/ICWS.2015.20},
abstract = {An important problem in Web services composition process is optimal selection of services meeting the user functional requirements (tasks of a workflow) and ensuring a reliable execution of the composition. Therefore, non-functional properties of services such as their transactional behavior as well as their Quality of Service (QoS) must be considered. In this context, a challenging objective is to assist users in integrating on the fly the operations of services to realize their required tasks by further meeting their transactional and QoS preferences. Towards this purpose, we present SPLTQSSS, a Software Product Line based approach for Stateful (conversation-based) Service Selection problem with Transactional and QoS support. SPL-TQSSS considers the set of functionally-equivalent services as part of a service family by modeling their internal operations using Feature Models. Then, SPL-TQSSS chooses the best services, from the service families matching with every task of the workflow, which fit with the user transactional preference and satisfy QoS constraints.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Web Services},
pages = {73–80},
numpages = {8},
keywords = {Feature Model, QoS, Service Selection, Software Product Line, Transactional, Variability},
series = {ICWS '15}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {benchmark extractor, feature location, feature revision, repository mining, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {architecture description language, dynamic reconfiguration, feature-oriented analysis, self-adaptive systems, software architecture},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {cyber-physical systems, dynamic software product lines, multiobjective optimization algorithms, self-adaptation, transfer learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {configurable system, decision propagation, software product lines, t-wise sampling, uniform random sampling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/1842752.1842809,
author = {Helleboogh, Alexander and Avgeriou, Paris and Bouck\'{e}, Nelis and Heymans, Patrick},
title = {Workshop on Variability in Software Product Line Architectures (VARI-ARCH 2010)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842809},
doi = {10.1145/1842752.1842809},
abstract = {The objective of this workshop is to bring together researchers from the software product line community and software architecture community to identify critical challenges and progress the state-of-the-art on variability in software product line architectures.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {309–311},
numpages = {3},
keywords = {assets, concern, model, product line architecture, product lines, software architecture, variability, view, viewpoint},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/2648511.2648546,
author = {Van Landuyt, Dimitri and Op de beeck, Steven and Hovsepyan, Aram and Michiels, Sam and Joosen, Wouter and Meynckens, Sven and de Jong, Gjalt and Barais, Olivier and Acher, Mathieu},
title = {Towards managing variability in the safety design of an automotive hall effect sensor},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648546},
doi = {10.1145/2648511.2648546},
abstract = {This paper discusses the merits and challenges of adopting software product line engineering (SPLE) as the main development process for an automotive Hall Effect sensor. This versatile component is integrated into a number of automotive applications with varying safety requirements (e.g., windshield wipers and brake pedals).This paper provides a detailed explanation as to why the process of safety assessment and verification of the Hall Effect sensor is currently cumbersome and repetitive: it must be repeated entirely for every automotive application in which the sensor is to be used. In addition, no support is given to the engineer to select and configure the appropriate safety solutions and to explain the safety implications of his decisions.To address these problems, we present a tailored SPLE-based approach that combines model-driven development with advanced model composition techniques for applying and reasoning about specific safety solutions. In addition, we provide insights about how this approach can reduce the overall complexity, improve reusability, and facilitate safety assessment of the Hall Effect sensor.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {304–309},
numpages = {6},
keywords = {ASIL validation, automotive, hardware/software co-design, safety patterns, software product line engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3382025.3414970,
author = {Kr\"{u}ger, Jacob and Mahmood, Wardah and Berger, Thorsten},
title = {Promote-pl: a round-trip engineering process model for adopting and evolving product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414970},
doi = {10.1145/3382025.3414970},
abstract = {Process models for software product-line engineering focus on proactive adoption scenarios---that is, building product-line platforms from scratch. They comprise the two phases domain engineering (building a product-line platform) and application engineering (building individual variants), each of which defines various development activities. Established more than two decades ago, these process models are still the de-facto standard for steering the engineering of platforms and variants. However, observations from industrial and open-source practice indicate that the separation between domain and application engineering, with their respective activities, does not fully reflect reality. For instance, organizations rarely build platforms from scratch, but start with developing individual variants that are re-engineered into a platform when the need arises. Organizations also appear to evolve platforms by evolving individual variants, and they use contemporary development activities aligned with technical advances. Recognizing this discrepancy, we present an updated process model for engineering software product lines. We employ a method for constructing process theories, building on recent literature as well as our experiences with industrial partners to identify development activities and the orders in which these are performed. Based on these activities, we synthesize and discuss the new process model, called promote-pl. Also, we explain its relation to modern software-engineering practices, such as continuous integration, model-driven engineering, or simulation testing. We hope that our work offers contemporary guidance for product-line engineers developing and evolving platforms, and inspires researchers to build novel methods and tools aligned with current practice.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {2},
numpages = {12},
keywords = {process model, round-trip engineering, software reuse},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {DSML, SPL, methodology, software factory, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {software product line, systematic review, tertiary study},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/1383559.1383571,
author = {Tawhid, Rasha and Petriu, Dorina C.},
title = {Towards automatic derivation of a product performance model from a UML software product line model},
year = {2008},
isbn = {9781595938732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1383559.1383571},
doi = {10.1145/1383559.1383571},
abstract = {Software Product Line (SPL) engineering is a software development approach that takes advantage of the commonality and variability between products from a family, and supports the generation of specific products by reusing a set of core family assets. This paper proposes a UML model transformation approach for software product lines to derive a performance model for a specific product. The input to the proposed technique, the "source model", is a UML model of a SPL with performance annotations, which uses two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The source model is generic and therefore its performance annotations must be parameterized. The proposed derivation of a performance model for a concrete product requires two steps: a) the transformation of a SPL model to a UML model with performance annotations for a given product, and b) the transformation of the outcome of the first step into a performance model. This paper focuses on the first step, whereas the second step will use the PUMA transformation approach of annotated UML models to performance models, developed in previous work. The output of the first step, named "target model", is a UML model with MARTE annotations, where the variability expressed in the SPL model has been analyzed and bound to a specific product, and the generic performance annotations have been bound to concrete values for the product. The proposed technique is illustrated with an e-commerce case study.},
booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
pages = {91–102},
numpages = {12},
keywords = {marte, model transformation, software performance engineering, software product line, uml},
location = {Princeton, NJ, USA},
series = {WOSP '08}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {feature models, software product lines, unified language},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {NFQA, attribute, model, numerical, software product line, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342391,
author = {Beuche, Danilo},
title = {Industrial Variant Management with pure::variants},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342391},
doi = {10.1145/3307630.3342391},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built, tested and maintained by using the modeling and integration capabilities provided by pure::variants. With pure::variants being available for a long time, the demonstration (and the paper) combines both basics of pure::variants, known to parts of the audience, and new capabilities, introduced within the last year.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {37–39},
numpages = {3},
keywords = {feature modelling, software product lines, tools},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {configuration optimization, search-based software engineering, software product line configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {artificial intelligence, binary decision diagrams, configurable system, decision models, feature models, knownledge compilation, product configuration, satisfiability solving, software configuration, software product line},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {consumption, energy, measurement, mitigation, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1109/TPDS.2017.2766151,
author = {Fraternali, Francesco and Bartolini, Andrea and Cavazzoni, Carlo and Benini, Luca},
title = {Quantifying the Impact of Variability and Heterogeneity on the Energy Efficiency for a Next-Generation Ultra-Green Supercomputer},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {29},
number = {7},
issn = {1045-9219},
url = {https://doi.org/10.1109/TPDS.2017.2766151},
doi = {10.1109/TPDS.2017.2766151},
abstract = {Supercomputers, nowadays, aggregate a large number of nodes featuring the same nominal HW components (e.g., processors and GPGPUS). In real-life machines, the chips populating each node are subject to a wide range of variability sources, related to performance and temperature operating points (i.e., ACPI p-states) as well as process variations and die binning. Eurora is a fully operational supercomputer prototype that topped July 2013 Green500 and it represents a unique ’living lab’ for next-generation ultra-green supercomputers. In this paper we evaluate and quantify the impact of variability on Eurora's energy-performance tradeoffs under a wide range of workloads intensity. Our experiments demonstrate that variability comes from hardware component mismatches as well as from the interplay between run-time energy management and workload variations. Thus, variability has a significant impact on energy efficiency even at the moderate scale of the Eurora machine, thereby substantiating the critical importance of variability management in future green supercomputers.},
journal = {IEEE Trans. Parallel Distrib. Syst.},
month = jul,
pages = {1575–1588},
numpages = {14}
}

@inproceedings{10.1109/HICSS.2009.472,
title = {Towards Tool Support for the Configuration of Non-Functional Properties in SPLs},
year = {2009},
isbn = {9780769534503},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2009.472},
doi = {10.1109/HICSS.2009.472},
abstract = {The configuration of NFPs (non-functional properties) is a crucial problem in the development of software-intensive systems. Most of the approaches currently available tackle this problem during software design. However, at this stage, NFPs cannot be properly predicted. As a solution for this problem we present the new extensions of the Feedback approach which aims at improving the configuration of NFPs in SPLs. We introduce our set of tools that are used to support the approach and show how to use them by applying it to the well-known SPL (The Graph Product Line) that was suggested as a platform for evaluating SPL technologies.},
booktitle = {Proceedings of the 42nd Hawaii International Conference on System Sciences},
pages = {1–7},
numpages = {7},
series = {HICSS '09}
}

@article{10.1016/j.micpro.2017.05.012,
author = {Gomez, A. and Bartolini, A. and Rossi, D. and Can Kara, B. and Fatemi, H. and Pineda de Gyvez, J. and Benini, L.},
title = {Increasing the energy efficiency of microcontroller platforms with low-design margin co-processors},
year = {2017},
issue_date = {August 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2017.05.012},
doi = {10.1016/j.micpro.2017.05.012},
abstract = {Reducing the energy consumption in low cost, performance-constrained microcontroller units (MCUs) cannot be achieved with complex energy minimization techniques (i.e. fine-grained DVFS, Thermal Management, etc), due to their high overheads. To this end, we propose an energy-efficient, multi-core architecture combining two homogeneous cores with different design margins. One is a performance-guaranteed core, also called Heavy Core (HC), fabricated with a worst-case design margin. The other is a low-power core, called Light Core (LC), which has only a typical-corner design margin. Post-silicon measurements show that the Light core has a 30% lower power density compared to the Heavy core, with only a small loss in reliability. Furthermore, we derive the energy-optimal workload distribution and propose a runtime environment for Heavy/Light MCU platforms. The runtime decreases the overall energy by exploiting available parallelism to minimize the platforms active time. Results show that, depending on the core to peripherals power-ratio and the Light cores operating frequency, the expected energy savings range from 10 to 20%.},
journal = {Microprocess. Microsyst.},
month = aug,
pages = {213–228},
numpages = {16}
}

@inproceedings{10.1145/3109729.3109751,
author = {Krieter, Sebastian and Pinnecke, Marcus and Kr\"{u}ger, Jacob and Sprey, Joshua and Sontag, Christopher and Th\"{u}m, Thomas and Leich, Thomas and Saake, Gunter},
title = {FeatureIDE: Empowering Third-Party Developers},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109751},
doi = {10.1145/3109729.3109751},
abstract = {FeatureIDE is a popular open-source tool for modeling, implementing, configuring, and analyzing software product lines. However, FeatureIDE's initial design was lacking mechanisms that facilitate extension and reuse of core implementations. In current releases, we improve these traits by providing a modular concept for core data structures and functionalities. As a result, we are facilitating the usage of external implementations for feature models and file formats within FeatureIDE. Additionally, we provide a Java library containing FeatureIDE's core functionalities, including feature modeling and configuration. This allows developers to use these functionalities in their own tools without relying on external dependencies, such as the Eclipse framework.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {Software product line, configuration, feature modeling, feature-oriented software development},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2499777.2500725,
author = {Varshosaz, Mahsa and Khosravi, Ramtin},
title = {Discrete time Markov chain families: modeling and verification of probabilistic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500725},
doi = {10.1145/2499777.2500725},
abstract = {Software product line engineering (SPLE) enables systematic reuse in development of a family of related software systems by explicitly defining commonalities and variabilities among the individual products in the family. Nowadays, SPLE is used in a variety of complex domains such as avionics and automotive. As such domains include safety critical systems which exhibit probabilistic behavior, there is a major need for modeling and verification approaches dealing with probabilistic aspects of systems in the presence of variabilities. In this paper, we introduce a mathematical model, Discrete Time Markov Chain Family (DTMCF), which compactly represents the probabilistic behavior of all the products in the product line. We also provide a probabilistic model checking method to verify DTMCFs against Probabilistic Computation Tree Logic (PCTL) properties. This way, instead of verifying each product individually, the whole family is model checked at once, resulting in the set of products satisfying the desired property. This reduces the required cost for model checking by eliminating redundant processing caused by the commonalities among the products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {34–41},
numpages = {8},
keywords = {probabilistic model checking, software product line, variable discrete time Markov chains},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {architecture, case study, software product line, variability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {product-line analysis, regression analysis, software configuration management, software evolution, software product lines, software variation, variability management, variability-aware analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {software product lines, textual specification languages, variability modeling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2648511.2655956,
author = {Chitchyan, Ruzanna and Noppen, Joost and Groher, Iris},
title = {Sustainability in software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2655956},
doi = {10.1145/2648511.2655956},
abstract = {Sustainability encompasses a wide set of aims: ranging from energy efficient software products (environmental sustainability), reduction of software development and maintenance costs (economic sustainability), to employee wellbeing (social sustainability). This panel brings together researchers and practitioners to explore the role that sustainability will play in software product line engineering. The panel aims to explore how sustainability manifests itself in domain engineering, via study of, for instance, sustainability patterns in domain analysis, architectural decisions motivated by specific sustainability concerns, types of variability that results from sustainability considerations, as well as engineering of sustainability as a domain itself. This panel explores challenges in research and practice for Sustainability in Software Product Line Engineering.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {367},
numpages = {1},
keywords = {domain analysis, product line, sustainability, sustainability of product lines, sustainability through product liens},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Energy Consumption, FQA, Quality Attributes, SPL, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {feature-oriented systems, formal methods, roles, verification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342411,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Towards Modeling Variability of Products, Processes and Resources in Cyber-Physical Production Systems Engineering},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342411},
doi = {10.1145/3307630.3342411},
abstract = {Planning and developing Cyber-Physical Production Systems (CPPS) are multi-disciplinary engineering activities that rely on effective and efficient knowledge exchange for better collaboration between engineers of different disciplines. The Product-Process-Resource (PPR) approach allows modeling products produced by industrial processes using specific production resources. In practice, a CPPS manufactures a portfolio of product type variants, i.e., a product line. Therefore, engineers need to create and maintain several PPR models to cover PPR variants and their evolving versions. In this paper, we detail a representative use case, identify challenges for using Variability Modeling (VM) methods to describe and manage PPR variants, and present a first solution approach based on cooperation with domain experts at an industry partner, a system integrator of automation for high-performance CPPS. We conclude that integrating basic variability concepts into PPR models is a promising first step and describe our further research plans to support PPR VM in CPPS.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {49–56},
numpages = {8},
keywords = {cyber-physical production system, product-process-resource, variability modelling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {benchmark, product lines, software evolution, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2934466.2962729,
author = {Beuche, Danilo},
title = {Using pure: variants across the product line lifecycle},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962729},
doi = {10.1145/2934466.2962729},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built, tested and maintained by using the modeling and integration capabilities provided by pure::variants. With pure::variants being available for a long time, the demonstration (and the paper) combines both basics of pure::variants, known to parts of the audience, and new capabilities, introduced within the last year.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {333–336},
numpages = {4},
keywords = {feature modelling, software product lines, tools},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.compind.2006.09.004,
author = {Boucher, Xavier and Bonjour, Eric and Grabot, Bernard},
title = {Formalisation and use of competencies for industrial performance optimisation: A survey},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {2},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2006.09.004},
doi = {10.1016/j.compind.2006.09.004},
abstract = {For many years, industrial performance has been implicitly considered as deriving from the optimisation of technological and material resources (machines, inventories, etc.), made possible by centralized organisations. The topical requirements for reactive and flexible industrial systems have progressively reintroduced the human workforce as the main source of industrial performance. Making this paradigm operational requires the identification and careful formalisation of the link between human resource and industrial performance, through concepts like skills, competencies or know-how. This paper provides a general survey of the formalisation and integration of competence-oriented concepts within enterprise information systems and decision systems, aiming at providing new methods and tools for performance management.},
journal = {Comput. Ind.},
month = feb,
pages = {98–117},
numpages = {20},
keywords = {Competence model, Enterprise modelling, Information and decision systems, Performance}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3106195.3106204,
author = {Luthmann, Lars and Stephan, Andreas and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Modeling and Testing Product Lines with Unbounded Parametric Real-Time Constraints},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106204},
doi = {10.1145/3106195.3106204},
abstract = {Real-time requirements are crucial for embedded software in many modern application domains of software product lines. Hence, techniques for modeling and analyzing time-critical software have to be lifted to software product line engineering, too. Existing approaches extend timed automata (TA) by feature constraints to so-called featured timed automata (FTA) facilitating efficient verification of real-time properties for entire product lines in a single run. In this paper, we propose a novel modeling formalism, called configurable parametric timed automata (CoPTA), extending expressiveness of FTA by supporting freely configurable and therefore a-priori unbounded timing intervals for real-time constraints, which are defined as feature attributes in extended feature models with potentially infinite configuration spaces. We further describe an efficient test-suite generation methodology for CoPTA models, achieving location coverage on every possible model configuration. Finally, we present evaluation results gained from applying our tool implementation to a collection of case studies, demonstrating efficiency improvements compared to a variant-by-variant analysis.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {104–113},
numpages = {10},
keywords = {Model-based Testing, Real-Time Systems, Software Product Lines, Timed Automata},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3382025.3414963,
author = {Creff, Stephen and Noir, J\'{e}r\^{o}me Le and Lenormand, Eric and Madel\'{e}nat, S\'{e}bastien},
title = {Towards facilities for modeling and synthesis of architectures for resource allocation problem in systems engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414963},
doi = {10.1145/3382025.3414963},
abstract = {Exploring architectural design space is often beyond human capacity and makes architectural design a difficult task. Model-based systems engineering must include assistance to the system designer in identifying candidate architectures to subsequently analyze tradeoffs. Unfortunately, existing languages and approaches do not incorporate this concern, generally favoring solution analysis over exploring a set of candidate architectures.In this paper, we explore the advantages of designing and configuring the variability problem to solve one of the problems of exploring (synthesizing) candidate architectures in systems engineering: the resource allocation problem. More specifically, this work reports on the use of the Clafer modeling language and its gateway to the CSP Choco Solver, on an industrial case study of heterogeneous hardware resource allocation (GPP-GPGPU-FPGA).Based on experiments on the modeling in Clafer, and the impact of its translation into the constraint programming paradigm (performance studies), discussions highlight some issues concerning facilities for modeling and synthesis of architectures and recommendations are proposed towards the use of this variability approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {32},
numpages = {11},
keywords = {allocation problem, architecture synthesis, constraint solving, empirical study, variability modeling},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {dynamic software product lines, reconfiguration decisions, timed automata},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2499777.2500719,
author = {Schr\"{o}ter, Reimar and Siegmund, Norbert and Th\"{u}m, Thomas},
title = {Towards modular analysis of multi product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500719},
doi = {10.1145/2499777.2500719},
abstract = {Software product-line engineering enables efficient development of tailor-made software by means of reusable artifacts. As practitioners increasingly develop software systems as product lines, there is a growing potential to reuse product lines in other product lines, which we refer to as multi product line. We identify challenges when developing multi product lines and propose interfaces for different levels of abstraction ranging from variability modeling to functional and non-functional properties. We argue that these interfaces ease the reuse of product lines and identify research questions that need to be solved toward modular analysis of multi product lines.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {96–99},
numpages = {4},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {choice calculus, satisfiability solving, software product lines, variation},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342404,
author = {Th\"{u}m, Thomas and Seidl, Christoph and Schaefer, Ina},
title = {On Language Levels for Feature Modeling Notations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342404},
doi = {10.1145/3307630.3342404},
abstract = {Configuration is a key enabling technology for the engineering of systems and software as wells as physical goods. A selection of configuration options (aka. features) is often enough to automatically generate a product tailored to the needs of a customer. It is common that not all combinations of features are possible in a given domain. Feature modeling is the de-facto standard for specifying features and their valid combinations. However, a pivotal hurdle for practitioners, researchers, and teachers in applying feature modeling is that there are hundreds of tools and languages available. While there have been first attempts to define a standard feature modeling language, they still struggle with finding an appropriate level of expressiveness. If the expressiveness is too high, the language will not be adopted, as it is too much effort to support all language constructs. If the expressiveness is too low, the language will not be adopted, as many interesting domains cannot be modeled in such a language. Towards a standard feature modeling notation, we propose the use of language levels with different expressiveness each and discuss criteria to be used to define such language levels. We aim to raise the awareness on the expressiveness and eventually contribute to a standard feature modeling notation.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {158–161},
numpages = {4},
keywords = {automated analysis, expressiveness, feature model, language design, product lines, variability modeling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/11946441_74,
author = {Bonelli, Andreas and Franchetti, Franz and Lorenz, Juergen and P\"{u}schel, Markus and Ueberhuber, Christoph W.},
title = {Automatic performance optimization of the discrete fourier transform on distributed memory computers},
year = {2006},
isbn = {3540680675},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11946441_74},
doi = {10.1007/11946441_74},
abstract = {This paper introduces a formal framework for automatically generating performance optimized implementations of the discrete Fourier transform (DFT) for distributed memory computers. The framework is implemented as part of the program generation and optimization system Spiral. DFT algorithms are represented as mathematical formulas in Spiral's internal language SPL. Using a tagging mechanism and formula rewriting, we extend Spiral to automatically generate parallelized formulas. Using the same mechanism, we enable the generation of rescaling DFT algorithms, which redistribute the data in intermediate steps to fewer processors to reduce communication overhead. It is a novel feature of these methods that the redistribution steps are merged with the communication steps of the algorithm to avoid additional communication overhead. Among the possible alternative algorithms, Spiral's search mechanism now determines the fastest for a given platform, effectively generating adapted code without human intervention. Experiments with DFT MPI programs generated by Spiral show performance gains of up to 30% due to rescaling. Further, our generated programs compare favorably with Fftw-MPI 2.1.5.},
booktitle = {Proceedings of the 4th International Conference on Parallel and Distributed Processing and Applications},
pages = {818–832},
numpages = {15},
location = {Sorrento, Italy},
series = {ISPA'06}
}

@inproceedings{10.1145/2019136.2019168,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Exploring the use of reference architectures in the development of product line artifacts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019168},
doi = {10.1145/2019136.2019168},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. In another context, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the Reference Architecture research area. In spite of the positive impact of this type of architecture on reuse and productivity, the use of existing domain-specific reference architectures as basis of SPL has not been widely explored. The main contribution of this paper is to present how and when elements contained in existing reference architectures could contribute to the building of SPL artifacts during development of an SPL. We have observed that, in fact, reference architectures could make an important contribution to improving reuse and productivity, which are also important concerns in SPL.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {28},
numpages = {8},
keywords = {SPL design method, reference architecture, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2499777.2500710,
author = {Gabillon, Yoann and Biri, Nicolas and Otjacques, Beno\^{\i}t},
title = {Methodology to integrate multi-context UI variations into a feature model},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500710},
doi = {10.1145/2499777.2500710},
abstract = {Software product line (SPL) paradigm aims to explore commonalities and variabilities in a set of applications for developing an efficient derivation of products. One of the most common ways to model variability in this paradigm is to use a Feature Model. However, variability in SPL is often limited to functional features. The User Interface (UI) variations are modeled as entire UIs and thus these variations are not reusable and inspectable. Research in the Human Computer Interaction (HCI) field has proven the importance of variability for non functional, purely UI centric features. The HCI community has proposed several levels of abstraction for multi-context UI design. Indeed, new variations can be introduced at each abstraction level. UI designers are used to them and they usually introduce variability at each step of the UI definition without using SPL. To build usable softwares that take into account UI, we propose to merge functional concerns and UI concerns, providing a methodology to integrate variability of both aspects into a single Feature Model.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {74–81},
numpages = {8},
keywords = {abstraction levels, feature model, multi-context, software product line, usability, user interface, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3307630.3342417,
author = {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine and Salinesi, Camille and Mazo, Ra\'{u}l},
title = {Evaluation of the State-Constraint Transition Modelling Language: A Goal Question Metric Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342417},
doi = {10.1145/3307630.3342417},
abstract = {Self-adaptive systems (SAS) are exceptional systems, on account of their versatile composition, dynamic behavior and evolutive nature. Existing formal languages for the specification of SAS focus on adapting system elements to achieve a target goal, following specific rules, without much attention on the adaptation of requirements themselves. The State-Constraint Transition (SCT) modeling language enables the specification of dynamic requirements, both at the domain and application level, as a result of space or time variability. This language, evaluated in this paper, enables the specification of a variety of requirement types, for SASs from different domains, while generating a configuration, all configurations, and number of possible configurations, in milliseconds. This paper presents these results, namely; expressiveness, domain independence and scalability, from the viewpoint of designers and domain engineers, following a goal-question-metric approach. However, being primarily based on constraint programming (CP), the language suffers from drawbacks inherited from this paradigm, specifically time related requirements, like (e.g. order, frequency and staged requirements).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {106–113},
numpages = {8},
keywords = {IoT, constraint programming, dynamic software product lines, modeling language, state machine},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3336294.3336316,
author = {Tolvanen, Juha-Pekka and Kelly, Steven},
title = {How Domain-Specific Modeling Languages Address Variability in Product Line Development: Investigation of 23 Cases},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336316},
doi = {10.1145/3336294.3336316},
abstract = {Domain-Specific Modeling raises the level of abstraction beyond programming by specifying the solution directly with domain concepts. Within product lines domain-specific approaches are applied to specify variability and then generate final products together with commonality. Such automated product derivation is possible because both the modeling language and generator are made for a particular product line --- often inside a single company. In this paper we examine which kinds of reuse and product line approaches are applied in industry with domain-specific modeling. Our work is based on empirical analysis of 23 cases and the languages and models created there. The analysis reveals a wide variety and some commonalities in the size of languages and in the ways they apply reuse and product line approaches.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {155–163},
numpages = {9},
keywords = {code generation, domain-specific language, domain-specific modeling, product derivation, product line variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {domain models, feature interaction, sampling algorithms, software product lines, testing},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/1944892.1944909,
author = {Thomas, Jacques and Dziobek, Christian and Hedenetz, Bernd},
title = {Variability management in the AUTOSAR-based development of applications for in-vehicle systems},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944909},
doi = {10.1145/1944892.1944909},
abstract = {In automotive electrical/electronics systems, reuse of software applications over vehicle lines is becoming essential due to the growing complexity of the applications. In addition, a growing number of variants have to be handled because of the increasing number of differences arising in vehicle lines. Software product lines are a common approach to address these issues. This paper presents the challenges and our vision for the introduction of a product line approach in the context of AUTOSAR (AUTomotive Open System Architecture)-based development of applications for in vehicle systems.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {137–140},
numpages = {4},
keywords = {AUTOSAR, feature-oriented development, product line evolution, variability},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791114,
author = {Beuche, Danilo and Hellebrand, Robert},
title = {Using pure::variants across the product line lifecycle},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791114},
doi = {10.1145/2791060.2791114},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built, tested and maintained by using the modeling and integration capabilities provided by pure::variants [2].},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {352–354},
numpages = {3},
keywords = {feature modelling, software product lines, tools},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2499777.2500711,
author = {Ciolfi Felice, Marianela and Filho, Joao Bosco Ferreira and Acher, Mathieu and Blouin, Arnaud and Barais, Olivier},
title = {Interactive visualisation of products in online configurators: a case study for variability modelling technologies},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500711},
doi = {10.1145/2499777.2500711},
abstract = {Numerous companies develop interactive environments to assist users in customising sales products through the selection of configuration options. A visual representation of these products is an important factor in terms of user experience. However, an analysis of 100+ existing configurators highlights that not all provide visual representations of configured products. One of the current challenges is the trade-off developers face between either the memory consuming use of pregenerated images of all the combinations of options, or rendering products on the fly, which is non trivial to implement efficiently. We believe that a new approach to associate product configurations to visual representations is needed to compose and render them dynamically. In this paper we present a formal statement of the problem and a model-driven perspective for addressing it as well as our ongoing work and further challenges.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {82–85},
numpages = {4},
keywords = {configurator, software product line, user interface, variability modelling},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {industrial experience, multiple product lines, software reuse},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2499777.2499782,
author = {Beuche, Danilo},
title = {Modeling and building product lines with pure::variants},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499782},
doi = {10.1145/2499777.2499782},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built by using the modeling capabilities provided by pure::variants [2].},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {147–149},
numpages = {3},
keywords = {feature modelling, software product lines, tools},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3233027.3233037,
author = {Butting, Arvid and Eikermann, Robert and Kautz, Oliver and Rumpe, Bernhard and Wortmann, Andreas},
title = {Modeling language variability with reusable language components},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233037},
doi = {10.1145/3233027.3233037},
abstract = {Proliferation of modeling languages has produced a great variety of similar languages whose individual maintenance is challenging and costly. Reusing the syntax and semantics of modeling languages and their heterogeneous constituents, however, is rarely systematic. Current research on modeling language reuse focuses on reusing abstract syntax in form of metamodel parts. Systematic reuse of static and dynamic semantics is yet to be achieved. We present an approach to compose syntax and semantics of independently developed modeling languages through language product lines and derive new stand-alone language products. Using the MontiCore language workbench, we implemented a mechanism to compose language syntaxes and the realization of their semantics in form of template-based code generators according to language product line configurations. Leveraging variability of product lines greatly facilitates reusing modeling language and alleviates their proliferation.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {65–75},
numpages = {11},
keywords = {language product lines, language variability, software language engineering},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {code metrics, model checking, non-functional properties, sampling, software product lines, static analysis, testing, theorem proving, tool support, type checking},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {architecture, decision-making, design exploration, model-driven engineering, multi-criteria decision analysis, systems engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791100,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Applying the product lines paradigm to the quantitative analysis of collective adaptive systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791100},
doi = {10.1145/2791060.2791100},
abstract = {Engineering a Collective Adaptive System (CAS) requires the support of a framework for quantitative modeling and analysis of the system. In order to jointly address variability and quantitative analysis, we apply the Product Lines paradigm, considered at the level of system engineering, to a case study of the European project QUANTICOL, by first defining a reference feature model and then adding feature attributes and global quantitative constraints, in the form of a Clafer attributed feature model. ClaferMOOVisualizer is subsequently used for quantitative analyses and multi-objective optimization of the resulting attributed feature model.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {321–326},
numpages = {6},
keywords = {ClaferMOO, collective adaptive systems, multi-objective optimization, quantitative analysis, quantitative modeling, variability analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s10270-020-00791-9,
author = {Westfechtel, Bernhard and Greiner, Sandra},
title = {Extending single- to multi-variant model transformations by trace-based propagation of variability annotations},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00791-9},
doi = {10.1007/s10270-020-00791-9},
abstract = {Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {853–888},
numpages = {36},
keywords = {Model transformation, Software product line, Annotative variability}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {domain specific modeling, model composition, model-driven software product line, quality attribute, sensitivity point},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {cyber-physical system product lines, search-based software engineering, test case selection},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2019136.2019187,
author = {Abbas, Nadeem},
title = {Towards autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019187},
doi = {10.1145/2019136.2019187},
abstract = {We envision an Autonomic Software Product Line (ASPL). The ASPL is a dynamic software product line that supports self adaptable products. We plan to use reflective architecture to model and develop ASPL. To evaluate the approach, we have implemented three autonomic product lines which show promising results. The ASPL approach is at initial stages, and require additional work. We plan to exploit online learning to realize more dynamic software product lines to cope with the problem of product line evolution. We propose on-line knowledge sharing among products in a product line to achieve continuous improvement of quality in product line products.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {44},
numpages = {8},
keywords = {knowledge, on-line learning, self-adaptation},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3233027.3233046,
author = {Beek, Maurice H. ter and Fantechi, Alessandro and Gnesi, Stefania},
title = {Product line models of large cyber-physical systems: the case of ERTMS/ETCS},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233046},
doi = {10.1145/3233027.3233046},
abstract = {A product line perspective may help to understand the possible variants in interactions between the subsystems of a large, cyber-physical system. This observation is exemplified in this paper by proposing a feature model of the family of ERTMS/ETCS train control systems and their foreseen extensions. This model not only shows the different components that have to be installed when deploying the system at the different levels established by the ERTMS/ETCS standards, but it also helps to identify and discuss specific issues, such as the borders between onboard and wayside equipment, different manufacturers of the subsystems, interoperability among systems developed at different levels, backward compatibility of trains equipped with higher level equipment running on lines equipped with lower level equipment, and evolution towards future trends of railway signalling. The feature model forms the basis for formal modelling of the behaviour of the critical components of the system and for evaluating the overall cost, effectiveness and sustainability, for example by adding cost and performance attributes to the feature model.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {208–214},
numpages = {7},
keywords = {ERTMS/ETCS train control systems, cyber-physical systems, feature models, product lines, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s10664-014-9353-5,
author = {Asadi, Mohsen and Soltani, Samaneh and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {The effects of visualization and interaction techniques on feature model configuration},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9353-5},
doi = {10.1007/s10664-014-9353-5},
abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1706–1743},
numpages = {38},
keywords = {Controlled experiment, Software product line engineering, Tools}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {CVL, clafer, energy efficiency, metrics, optimisation, repository, software product line, variability},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {features, product variants, traceability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2019136.2019159,
author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
title = {Small inexpensive core asset construction for large gainful product line development: developing a communication system firmware product line},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019159},
doi = {10.1145/2019136.2019159},
abstract = {Product line development of communication system firmware with more than 2,000 features was performed in a large-scale project that involved more than 300 engineers (at a maximum) across four distributed sites. However, since intense demands to reduce development costs and time made it prohibitive to construct core assets for all those identified features, the project screened a limited number of the features, for which core assets were constructed, and then performed partial application of product line engineering. Nevertheless, when compared with previously engineered derivative developments, when the second product of the product line was released, it was clear that the project had achieved significant improvements in quality, as well as reductions in development costs and time requirements. Automatic code generation also contributed to those improvements.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {20},
numpages = {5},
keywords = {case study, core assets, feature modeling, product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1147/JRD.2015.2432652,
author = {Kostenko, W. P. and Demetriou, D. W. and Goth, G. F.},
title = {IBM z13: energy efficiency, increased environmental range, and flexible data center characteristics},
year = {2015},
issue_date = {July/September 2015},
publisher = {IBM Corp.},
address = {USA},
volume = {59},
number = {4–5},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2015.2432652},
doi = {10.1147/JRD.2015.2432652},
abstract = {A maximum configuration IBM z13™ provides 45% more compute capacity per watt than its predecessor mainframe, the IBM zEnterprise® EC12 (zEC12). The z13 adds support of the American Society of Heating Refrigeration and Air Conditioning Engineers (ASHRAE) class A2 environment, with a maximum air-inlet temperature of 35 °C, versus 32 °C for the class A1, zEC12. It accomplishes this while maintaining a wide range of AC and higher voltage DC power capability, overhead and/or underfloor I/O cabling, raised or non-raised floor installation, and external air or direct water cooling, also with increased temperature. The z13 is discussed, in the context of the history of IBM z Systems™ energy efficiency and data center flexibility, with significant attention paid to the system's redesigned airflow and its customizable exhaust air direction capability. Using computational fluid dynamics modeling, this paper discusses the integration of the system into several data center environments. Also reviewed are the potential advantages of intermittently operating the data center at higher temperatures, including discussion of the cooling energy and hardware costs, economizers, and robustness improvements when faced with cooling infrastructure failures. Lastly, we describe the cooling innovations, designs, and protections that enable its wider temperature range capability.},
journal = {IBM J. Res. Dev.},
month = jul,
pages = {10:1–10:9},
numpages = {9}
}

@article{10.1016/j.asoc.2016.07.040,
author = {Xue, Yinxing and Zhong, Jinghui and Tan, Tian Huat and Liu, Yang and Cai, Wentong and Chen, Manman and Sun, Jun},
title = {IBED},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.040},
doi = {10.1016/j.asoc.2016.07.040},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose to combine IBEA and DE for the optimal feature selection in SPLE.We propose a feedback-directed method into EAs to improve the correctness of results.Our IBED with the seeding method has significantly shortened the search time.In most cases, IBED finds more unique and non-dominated solutions than IBEA. Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1215–1231},
numpages = {17},
keywords = {Differential evolutionary algorithm (DE), Indicator-based evolutionary algorithm (IBEA), Optimal feature selection, Software product line engineering}
}

@inproceedings{10.5555/1753235.1753265,
author = {Weston, Nathan and Chitchyan, Ruzanna and Rashid, Awais},
title = {A framework for constructing semantically composable feature models from natural language requirements},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Software Product Line Engineering (SPLE) requires the construction of feature models from large, unstructured and heterogeneous documents, and the reliable derivation of product variants from the resulting model. This can be an arduous task when performed manually, and can be error-prone in the presence of a change in requirements. In this paper we introduce a tool suite which automatically processes natural-language requirements documents into a candidate feature model, which can be refined by the requirements engineer. The framework also guides the process of identifying variant concerns and their composition with other features. We also provide language support for specifying semantic variant feature compositions which are resilient to change. We show that feature models produced by this framework compare favourably with those produced by domain experts by application to a real-life industrial example.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {211–220},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {Software product line, configuration management, critical path analysis, product derivation, weighted approach}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {business calculations, product configuration, variability models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {automation systems, configuration, consistency check, feature model, multi-criteria evaluation, product lines},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {Software product line, multi-objective optimisation, test prioritisation, test selection}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {autonomic computing, mobile systems},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/941350.941369,
author = {Li, Li (Erran) and Sinha, Prasun},
title = {Throughput and energy efficiency in topology-controlled multi-hop wireless sensor networks},
year = {2003},
isbn = {1581137648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/941350.941369},
doi = {10.1145/941350.941369},
abstract = {In the context of multi-hop wireless networks, various topology control algorithms have been proposed to adapt the transmission range of nodes based on local information while maintaining a connected topology. These algorithms are particularly suited for deployment in sensor networks which typically consist of energy constrained sensors. Sensor nodes should support power adaptation in order to use the benefits of topology control for energy conservation. In this paper, we design a framework for evaluating the performance of topology control algorithms using overall network throughput, and total energy consumption per packet delivered, as the metrics. Our goal is to identify the scenarios in which topology control improves the network performance. We supplement our analysis with ns2 simulations using the cone-based topology control algorithm [10, 19].Based on our analysis and simulations, we find that link layer retransmissions are essential with topology control to avoid throughput degradation due to increase in number of hops in lightly loaded networks. In heavily loaded networks, the throughput can be improved by a factor up to k2, where k is the average factor of reduction in transmission range using topology control. Studies of energy consumption reveal that improvements of up to $k^4$ can be obtained using topology control. However, these improvements decrease as the traffic pattern shifts from local (few hop connections) to non-local (hop lengths of the order of the diameter of the network). These results can be used to guide the deployment of topology control algorithms in sensor networks.},
booktitle = {Proceedings of the 2nd ACM International Conference on Wireless Sensor Networks and Applications},
pages = {132–140},
numpages = {9},
keywords = {ad-hoc networks, sensor networks, topology control, wireless networks},
location = {San Diego, CA, USA},
series = {WSNA '03}
}

@article{10.1109/92.863617,
author = {Hegde, Rajamohana and Shanbhag, Naresh R.},
title = {Toward achieving energy efficiency in presence of deep submicron noise},
year = {2000},
issue_date = {Aug. 2000},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {8},
number = {4},
issn = {1063-8210},
url = {https://doi.org/10.1109/92.863617},
doi = {10.1109/92.863617},
abstract = {Presented in this paper are: 1) information-theoretic lower bounds on energy consumption of noisy digital gates and 2) the concept of noise tolerance via coding for achieving energy efficiency in the presence of noise. In particular, lower bounds on a) circuit speed f/sub c/ and supply voltage V/sub dd/; b) transition activity t in presence of noise; c) dynamic energy dissipation; and d) total (dynamic and static) energy dissipation are derived. A surprising result is that in a scenario where dynamic component of power dissipation dominates, the supply voltage for minimum energy operation (V/sub dd, opt/) is greater than the minimum supply voltage (V/sub dd, min/)for reliable operation. We then propose noise tolerance via coding to approach the lower bounds on energy dissipation. We show that the lower bounds on energy for an off-chip I/O signaling example are a factor of 24/spl times/ below present day systems. A very simple Hamming code can reduce the energy consumption by a factor of 3/spl times/, while Reed-Muller (RM) codes give a 4/spl times/ reduction in energy dissipation.},
journal = {IEEE Trans. Very Large Scale Integr. Syst.},
month = aug,
pages = {379–391},
numpages = {13},
keywords = {coding, energy dissipation, gate capacity, lower bound, noise, noise-tolerant computing}
}

@inproceedings{10.1145/2499777.2500716,
author = {Saller, Karsten and Lochau, Malte and Reimund, Ingo},
title = {Context-aware DSPLs: model-based runtime adaptation for resource-constrained systems},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500716},
doi = {10.1145/2499777.2500716},
abstract = {Dynamic Software Product Lines (DSPLs) provide a promising approach for planning and applying runtime reconfiguration scenarios to adaptive software systems. However, applying DSPLs in the vital domain of highly context-aware systems, e.g., mobile devices, is obstructed by the inherently limited resources being insufficient to handle large, constrained (re-)configurations spaces. To tackle these drawbacks, we propose a novel model-based approach for designing DSPLs in a way that allows for a trade-off between precomputation of reconfiguration scenarios at development time and on-demand evolution at runtime. Therefore, we (1) enrich feature models with context information to reason about potential context changes, and (2) specify context-aware reconfiguration processes on the basis of a scalable transition system incorporating state space abstractions and incremental refinement at runtime. We illustrate our concepts by means of a smartphone case study and present an implementation and evaluation considering different trade-off metrics.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {106–113},
numpages = {8},
keywords = {DSPL, adaptive systems, contexts, feature models, state space reduction},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/11554844_7,
author = {Zhang, Weishan and Jarzabek, Stan},
title = {Reuse without compromising performance: industrial experience from RPG software product line for mobile devices},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_7},
doi = {10.1007/11554844_7},
abstract = {It is often believed that reusable solutions, being generic, must necessarily compromise performance. In this paper, we consider a family of Role-Playing Games (RPGs). We analyzed similarities and differences among four RPGs. By applying a reuse technique of XVCL, we built an RPG product line architecture (RPG-PLA) from which we could derive any of the four RPGs. We built into the RPG-PLA a number of performance optimization strategies that could benefit any of the four (and possibly other similar) RPGs. By comparing the original vs. the new RPGs derived from the RPG-PLA, we demonstrated that reuse allowed us to achieve improved performance, both speed and memory utilization, as compared to each game developed individually. At the same time, our solution facilitated rapid development of new games, for new mobile devices, as well as ease of evolving with new features the RPG-PLA and custom games already in use.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {57–69},
numpages = {13},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.1145/2019136.2019152,
author = {Kozuka, Nobuaki and Ishida, Yuzo},
title = {Building a product line architecture for variant-rich enterprise applications using a data-oriented approach},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019152},
doi = {10.1145/2019136.2019152},
abstract = {IT industry in Japan has grown by providing specific made-to-order enterprise applications for various industries. Most of enterprise applications are built upon relational database management system (RDBMS), which takes the responsibility of keeping data integrity and data manipulation. However, data explosion in recent years especially in retail and telecommunication industries makes IT industry difficult to satisfy quality attributes such as scalability, availability and data consistency with traditional development techniques. From the beginning of this century, NRI has built and refined product line architecture as a primary core asset for such data intensive industries, which have very rich variations in functional and nonfunctional requirements of their enterprise applications. This paper summarizes key criteria to build such an architecture based on our ten years experience in developing dozens of mission critical IT systems as product families for those industries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {14},
numpages = {6},
keywords = {core asset development, data intensiveness, data oriented approach, enterprise applications, product line architecture, quality attributes, relational database management system},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@inproceedings{10.1007/978-3-642-25535-9_29,
author = {Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Asadi, Mohsen and Bagheri, Ebrahim and Bo\v{s}kovi\'{c}, Marko},
title = {A quality aggregation model for service-oriented software product lines based on variability and composition patterns},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_29},
doi = {10.1007/978-3-642-25535-9_29},
abstract = {Quality evaluation is a challenging task in monolithic software systems. It is even more complex when it comes to Service-Oriented Software Product Lines (SOSPL), as it needs to analyze the attributes of a family of SOA systems. In SOSPL, variability can be planned and managed at the architectural level to develop a software product with the same set of functionalities but different degrees of non-functional quality attribute satisfaction. Therefore, architectural quality evaluation becomes crucial due to the fact that it allows for the examination of whether or not the final product satisfies and guarantees all the ranges of quality requirements within the envisioned scope. This paper addresses the open research problem of aggregating QoS attribute ranges with respect to architectural variability. Previous solutions for quality aggregation do not consider architectural variability for composite services. Our approach introduces variability patterns that can possibly occur at the architectural level of an SOSPL. We propose an aggregation model for QoS computation which takes both variability and composition patterns into account.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {436–451},
numpages = {16},
keywords = {QoS aggregation, feature modeling, non-functional properties, process family, service variability, service-oriented architecture (SOA), software product line (SPL), variability management},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@inproceedings{10.1145/2647908.2655961,
author = {Seidl, Christoph and Domachowska, Irena},
title = {Teaching variability engineering to cognitive psychologists},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655961},
doi = {10.1145/2647908.2655961},
abstract = {In research of cognitive psychology, experiments to measure cognitive processes may be run in many similar yet slightly different configurations. Variability engineering offers techniques to handle variable configurations both conceptually and technically. However, these techniques are largely unknown to cognitive psychologists so that experiment configurations are specified informally or too coarse grain. This is problematic, because it becomes difficult to get an overview of paradigm configurations used in the so far conducted experiments. Variability engineering techniques provide, i.a., concise notations for capturing variability in software and can also be used to express the configurable nature of a wide range of experiments in cognitive psychology. Furthermore, it enables cognitive psychologists to structure configuration knowledge, to identify suitably similar experiment setups and to more efficiently identify individual configuration options as relevant reasons for a particular effect in the outcome of an experiment. In this paper, we present experiences with teaching variability engineering to cognitive psychologists along with a suitable curriculum.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {16–23},
numpages = {8},
keywords = {cognitive psychology, feature model, teaching, variability engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {Software product lines, software engineering teaching, software product line teaching, variability modeling}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Service composition, Feature model, Software product lines, Self adaptation}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2648511.2648514,
author = {Seidl, Christoph and Schaefer, Ina and A\ss{}mann, Uwe},
title = {Integrated management of variability in space and time in software families},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648514},
doi = {10.1145/2648511.2648514},
abstract = {Software product lines (SPLs) and software ecosystems (SECOs) encompass a family of closely related software systems in terms of common and variable assets that are configured to concrete products (variability in space). Over the course of time, variable assets of SPLs and especially SECOs are subject to change in order to meet new requirements as part of software evolution (variability in time). Even though both dimensions of variability have to be handled simultaneously, e.g., as not all customers upgrade their respective products immediately or completely, there currently is no approach that can create variants with a selection of variable assets in various versions. In this paper, we introduce an integrated approach to manage variability in space and time in software families using Hyper Feature Models (HFMs) with feature versions and combine them with an extension of the transformational variability realization mechanism delta modeling. This allows derivation of concrete software systems from an SPL or SECO configuring both functionality (features) as well as versions.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {22–31},
numpages = {10},
keywords = {delta modeling, evolution, hyper feature models (HFMs), software ecosystems, software product lines, variability},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Method Engineering, Method Oriented Architecture MOA, Semantic Web, Software Development, Software Product Line}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {Domain engineering, conflicting configurations, constraint satisfaction problem, domain-specific language, extended feature model, model transformation chain},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@article{10.1007/s11219-013-9197-z,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality attribute modeling and quality aware product configuration in software product lines},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9197-z},
doi = {10.1007/s11219-013-9197-z},
abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study.},
journal = {Software Quality Journal},
month = sep,
pages = {365–401},
numpages = {37},
keywords = {Analytic hierarchical process (AHP), Feature model, Non-functional requirement (NFR) framework, Product configuration, Quality attributes assessment, Software product line}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Ten\'{o}rio, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, M\'{a}rcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Empirical software engineering, Feature modeling, Ontology, Software product line}
}

@inproceedings{10.1145/2499777.2500721,
author = {Seidl, Christoph and Schaefer, Ina and A\ss{}mann, Uwe},
title = {Variability-aware safety analysis using delta component fault diagrams},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500721},
doi = {10.1145/2499777.2500721},
abstract = {Component Fault Diagrams (CFD) allow the specification of fault propagation paths, which is employed for the design of safety-critical systems as well as their certification. Even though families of safety-critical systems exist with many similar, yet not equal, variants there is no dedicated variability mechanism for CFDs to reuse commonalities of all family members and to alter only variable parts. In this paper, we present a variability representation approach for CFDs based on delta modeling that allows to transform an initial CFD within a closed or open variant space. Furthermore, we provide delta-aware analysis techniques for CFDs in order to analyse multiple variants efficiently. We show the feasibility of our approach by means of an example scenario based on the personal home robot TurtleBot using a prototypical implementation of our concepts.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {2–9},
numpages = {8},
keywords = {component fault diagrams, delta modeling, minimum cut set, safety, software fault trees, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Extended product line, Product configuration, Systematic literature review}
}

@inproceedings{10.1145/2188286.2188347,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Hirundo: a mechanism for automated production of optimized data stream graphs},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188347},
doi = {10.1145/2188286.2188347},
abstract = {Stream programs have to be crafted carefully to maximize the performance gain that can be obtained from stream processing environments. Manual fine tuning of a stream program is a very difficult process which requires considerable amount of programmer time and expertise. In this paper we present Hirundo, which is a mechanism for automatically generating optimized stream programs that are tailored for the environment they run. Hirundo analyzes, identifies the structure of a stream program, and transforms it to many different sample programs with same semantics using the notions of Tri-Operator Transformation, Transformer Blocks, and Operator Blocks Fusion. Then it uses empirical optimization information to identify a small subset of generated sample programs that could deliver high performance. It runs the selected sample programs in the run-time environment for a short period of time to obtain their performance information. Hirundo utilizes these information to output a ranked list of optimized stream programs that are tailored for a particular run-time environment. Hirundo has been developed using Python as a prototype application for optimizing SPADE programs, which run on System S stream processing run-time. Using three example real world stream processing applications we demonstrate effectiveness of our approach, and discuss how well it generalizes for automatic stream program performance optimization.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {335–346},
numpages = {12},
keywords = {data-intensive computing, fault tolerance, performance optimization, scalability, stream processing},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@article{10.1016/j.jss.2019.01.057,
author = {Kr\"{u}ger, Jacob and Mukelabai, Mukelabai and Gu, Wanzi and Shen, Hui and Hebig, Regina and Berger, Thorsten},
title = {Where is my feature and what is it about? A case study on recovering feature facets},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.057},
doi = {10.1016/j.jss.2019.01.057},
journal = {J. Syst. Softw.},
month = jun,
pages = {239–253},
numpages = {15},
keywords = {Feature location, Marlin, Bitcoin-wallet, Case study, Feature facets, Software product line}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {active learning, adaptive instance selection, benchmarking, fractional factorial design, non-functional properties},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.jss.2019.110422,
author = {Edded, Sabrine and Sassi, Sihem Ben and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Collaborative configuration approaches in software product lines engineering: A systematic mapping study},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110422},
doi = {10.1016/j.jss.2019.110422},
journal = {J. Syst. Softw.},
month = dec,
numpages = {17},
keywords = {Product lines, Collaborative configuration, Systematic mapping study, Framework}
}

@inproceedings{10.1145/3168365.3168377,
author = {Ananieva, Sofia and Klare, Heiko and Burger, Erik and Reussner, Ralf},
title = {Variants and Versions Management for Models with Integrated Consistency Preservation},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168377},
doi = {10.1145/3168365.3168377},
abstract = {Modern software systems are often developed and maintained by describing them in several modeling and programming languages. To reduce complexity and improve understandability of such systems, models represent specific views on the system. These views have semantic interrelations (e.g., by sharing common or dependent information) that need to be kept consistent during evolution of the system. Apart from that, modern systems need to run in many different contexts and be highly configurable to satisfy the demand for fully customizable products. Such variable systems often comprise various dependencies from which inconsistencies may arise. Combining solutions for consistency management with variants and versions management, however, comes with many challenges.In this research-in-progress paper, we introduce the VaVe approach which makes variants and versions management aware of automated consistency preservation in the context of multi-view modeling. We explain core features of the approach and reason about its benefits and limitations.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {3–10},
numpages = {8},
keywords = {Delta-Based Consistency Preservation, Software Product Lines, Variability Management},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.5220/0006791403830391,
author = {Aouzal, Khadija and Hafiddi, Hatim and Dahchour, Mohamed},
title = {Handling Tenant-Specific Non-Functional Requirements through a Generic SLA},
year = {2018},
isbn = {9789897583001},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0006791403830391},
doi = {10.5220/0006791403830391},
abstract = {In a multi-tenant architecture of a Software as a Service (SaaS) application, one single instance is shared amongdifferent tenants. However, this architectural style supports only the commonalities among tenants and doesnot cope with the variations and the specific context of each tenant. These variations concern either functionalor non-functional properties. In this paper, we deal with non-functional variability in SaaS services in orderto support the different quality levels that a service may have. For that purpose, we propose an approachthat considers Service Level Agreements (SLAs) as Families in terms of Software Product Line Engineering.We define two metamodels: NFVariability metamodel and VariableSLA metamodel. The first one modelsand captures variability in quality attributes of services. The second one models a dynamic and variableSLA. Model-to-model transformations are performed to transform Feature Model (NFVariability metamodelinstance) to Generic SLA (VariableSLA instance) in order to dynamically deal with the tenant-specific nonfunctionalrequirements.},
booktitle = {Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {383–391},
numpages = {9},
keywords = {MDE, Non-Functional Variability, QoS Characteristics, SLA., SPLE, SaaS},
location = {Funchal, Madeira, Portugal},
series = {ENASE 2018}
}

@article{10.1007/s10619-013-7130-x,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Automatic optimization of stream programs via source program operator graph transformations},
year = {2013},
issue_date = {December  2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {4},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-013-7130-x},
doi = {10.1007/s10619-013-7130-x},
abstract = {Distributed data stream processing is a data analysis paradigm where massive amounts of data produced by various sources are analyzed online within real-time constraints. Execution performance of a stream program/query executed on such middleware is largely dependent on the ability of the programmer to fine tune the program to match the topology of the stream processing system. However, manual fine tuning of a stream program is a very difficult, error prone process that demands huge amounts of programmer time and expertise which are expensive to obtain. We describe an automated process for stream program performance optimization that uses semantic preserving automatic code transformation to improve stream processing job performance. We first identify the structure of the input program and represent the program structure in a Directed Acyclic Graph. We transform the graph using the concepts of Tri-OP Transformation and Bi-Op Transformation. The resulting sample program space is pruned using both empirical as well as profiling information to obtain a ranked list of sample programs which have higher performance compared to their parent program. We successfully implemented this methodology on a prototype stream program performance optimization mechanism called Hirundo. The mechanism has been developed for optimizing SPADE programs which run on System S stream processing run-time. Using five real world applications (called VWAP, CDR, Twitter, Apnoea, and Bargain) we show the effectiveness of our approach. Hirundo was able to identify a 31.1 times higher performance version of the CDR application within seven minutes time on a cluster of 4 nodes.},
journal = {Distrib. Parallel Databases},
month = dec,
pages = {543–599},
numpages = {57},
keywords = {Automatic tuning, Code transformation, Data-intensive computing, Performance optimization, Stream processing}
}

@article{10.1016/j.infsof.2012.02.002,
author = {Holl, Gerald and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
year = {2012},
issue_date = {August, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.002},
doi = {10.1016/j.infsof.2012.02.002},
abstract = {Context: Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective: The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method: Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results: The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions: We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {828–852},
numpages = {25},
keywords = {Large-scale systems, Multi product lines, Product line engineering, Systematic literature review}
}

@inproceedings{10.1145/3131473.3131486,
author = {Kazdaridis, Giannis and Keranidis, Stratos and Symeonidis, Polychronis and Dias, Paulo Sousa and Gon\c{c}alves, Pedro and Loureiro, Bruno and Gjanci, Petrika and Petrioli, Chiara},
title = {EVERUN: Enabling Power Consumption Monitoring in Underwater Networking Platforms},
year = {2017},
isbn = {9781450351478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131473.3131486},
doi = {10.1145/3131473.3131486},
abstract = {The energy restricted nature of underwater sensor networks directly affects the expected lifetime of autonomous deployments and limits the capabilities for long term underwater monitoring. Towards the goal of developing energy-efficient protocols and algorithms, researchers and equipment vendors require in-depth understanding of the power consumption characteristics of underwater hardware when deployed in-field. In this work, we introduce the EVERUN power monitoring framework, consisting of hardware and software components that were integrated with real equipment of the SUNRISE testbed facilities. Through the execution of a wide set of experiments under realistic conditions, we highlighted the limitations of model-based energy evaluation tools and characterized the energy efficiency performance of key protocols and mechanisms. The accuracy of the collected power data, along with the interesting derived findings, verified the applicability of our approach in evaluating the energy efficiency performance of proposed solutions.},
booktitle = {Proceedings of the 11th Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; CHaracterization},
pages = {83–90},
numpages = {8},
keywords = {energy efficiency, power consumption monitorin, testbed experimentation, underwater networking},
location = {Snowbird, Utah, USA},
series = {WiNTECH '17}
}

@inproceedings{10.1145/3417113.3423000,
author = {de Macedo, Jo\~{a}o and Alo\'{\i}sio, Jo\~{a}o and Gon\c{c}alves, Nelson and Pereira, Rui and Saraiva, Jo\~{a}o},
title = {Energy wars - Chrome vs. Firefox: which browser is more energy efficient?},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3423000},
doi = {10.1145/3417113.3423000},
abstract = {This paper presents a preliminary study on the energy consumption of two popular web browsers. In order to properly measure the energy consumption of both environments, we simulate the usage of various applications, which the goal to mimic typical user interactions and usage.Our preliminary results show interesting findings based on observation, such as what type of interactions generate high peaks of energy consumption, and which browser is overall the most efficient. Our goal with this preliminary study is to show to users how very different the efficiency of web browsers can be, and may serve with advances in this area of study.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {159–165},
numpages = {7},
keywords = {energy efficiency, green software, web browsers},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.5220/0005679102800287,
author = {Diedrich, Alexander and B\"{o}ttcher, Bj\"{o}rn and Niggemann, Oliver},
title = {Exposing Design Mistakes During Requirements Engineering by Solving Constraint Satisfaction Problems to Obtain Minimum Correction Subsets},
year = {2016},
isbn = {9789897581724},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005679102800287},
doi = {10.5220/0005679102800287},
abstract = {In recent years, the complexity of production plants and therefore of the underlying automation systems has grown significantly. This makes the manual design of automation systems increasingly difficult. As a result, errors are found only during production, plant modifications are hindered by not maintainable automation solutions and criteria such as energy efficiency or cost are often not optimized. This work shows how utilizing Minimum Correction Subsets (MCS) of a Constraint Satisfaction Problem improves the collaboration of automation system designers and prevents inconsistent requirements and thus subsequent errors in the design. This opens up a new field of application for constraint satisfaction techniques. As a use case, an example from the field of automation system design is presented. To meet the automation industry\^{a} s requirement for standardised solutions that assure reliability, the calculation of MCS is formulated in such a way that most constraint solvers can be used without any extensions. Experimental results with typical problems demonstrate the practicalness concerning runtime and hardware resources.},
booktitle = {Proceedings of the 8th International Conference on Agents and Artificial Intelligence},
pages = {280–287},
numpages = {8},
keywords = {Constraint Satisfaction, Feature Models, Minimum Correction Subsets., Product Line Engineering},
location = {Rome, Italy},
series = {ICAART 2016}
}

@inproceedings{10.1145/3023956.3023962,
author = {Nieke, Michael and Engel, Gil and Seidl, Christoph},
title = {DarwinSPL: an integrated tool suite for modeling evolving context-aware software product lines},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023962},
doi = {10.1145/3023956.3023962},
abstract = {Software Product Lines (SPLs) are an approach for large-scale reuse for software families by means of variabilities and commonalities. We consider three dimensions of variability representing sources of software systems to behave differently: configuration as spatial variability, dependence on surroundings as contextual variability and evolution as temporal variability. The three dimensions of variability strongly correlate: Contextual variability changes the set of possible configurations in spatial variability. Temporal variability captures changes of spatial and contextual variability over the course of time. However, currently, there is no tool support for integrated modeling of these three dimensions of variability. In this paper, we present DarwinSPL, a tool suite supporting integrated definition of spatial, contextual and temporal variability. With DarwinSPL, spatial variability is modeled as feature models with constraints. Additionally, we are able to capture the current context and its impact on functionality of the SPL. Moreover, by providing support for temporal variability, DarwinSPL supports performing arbitrary evolutionary changes to spatial and contextual variability and tracking of previous evolution and planning future evolution of SPLs. We show the feasibility of DarwinSPL by performing a case study adapted from our industrial partner in the automotive domain.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {92–99},
numpages = {8},
keywords = {DarwinSPL, context-aware, evolution, software product line, temporal feature model, tool suite},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1007/978-3-642-25535-9_27,
author = {Nguyen, Tuan and Colman, Alan and Han, Jun},
title = {Modeling and managing variability in process-based service compositions},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_27},
doi = {10.1007/978-3-642-25535-9_27},
abstract = {Variability in process-based service compositions needs to be explicitly modeled and managed in order to facilitate service/process customization and increase reuse in service/process development. While related work has been able to capture variability and variability dependencies within a composition, these approaches fail to capture variability dependenciesbetween the composition and partner services. Consequently, these approaches cannot address the situation when a composite service is orchestrated from partner services some of which are customizable. In this paper, we propose a feature-based approach that is able to effectively model variability within and across compositions. The approach is supported by a process development methodology that enables the systematic reuse and management of variability. We develop a prototype system supporting extended BPMN 2.0 to demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {404–420},
numpages = {17},
keywords = {feature modeling, model driven engineering (MDE), model mapping, process variability, service composition, service variability, software product line (SPL), variability management},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@inproceedings{10.5555/1158337.1158709,
author = {Spinczyk, Olaf and Papajewski, Holger},
title = {Using Feature Models for Product Derivation},
year = {2006},
isbn = {0769525997},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In general the implementation of a software product line leads to a high degree of variability within the software architecture. For an effective development and deployment it is necessary to resolve variation points within the architecture and source code automatically during product/variant derivation. Given the complexity of most software systems tool support is necessary for these tasks.},
booktitle = {Proceedings of the 10th International on Software Product Line Conference},
pages = {225},
series = {SPLC '06}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Evolutionary algorithm, Metaheuristics, Search based software engineering, Software product line, Systematic mapping study}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.5555/2820656.2820662,
author = {Chitchyan, Ruzanna and Noppen, Joost and Groher, Iris},
title = {What can software engineering do for sustainability: case of software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Sustainable living, i.e., living within the bounds of the available environmental, social, and economic resources, is the focus of many present-day social and scientific discussions. But what does sustainability mean within the context of Software Product Line Engineering (SPLE)? And what does SPLE do for sustainable living? In this paper we take the first step towards identification of the sustainability-related characteristics relevant to SPLE. The paper also discusses how the key areas of interest to the current SPL community (as reflected by what is measured and optimised in SPLs today) relate to these sustainability characteristics.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {11–14},
numpages = {4},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Energy Aware, Machine Learning, Software Product Line, Web System},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Software product line testing, Search-Based software engineering, Preference-Based algorithms}
}

@inproceedings{10.1145/3185768.3186405,
author = {Verriet, Jacques and Dankers, Reinier and Somers, Lou},
title = {Performance Prediction for Families of Data-Intensive Software Applications},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186405},
doi = {10.1145/3185768.3186405},
abstract = {Performance is a critical system property of any system, in particular of data-intensive systems, such as image processing systems. We describe a performance engineering method for families of data-intensive systems that is both simple and accurate; the performance of new family members is predicted using models of existing family members. The predictive models are calibrated using static code analysis and regression. Code analysis is used to extract performance profiles, which are used in combination with regression to derive predictive performance models. A case study presents the application for an industrial image processing case, which revealed as benefits the easy application and identification of code performance optimization points.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {189–194},
numpages = {6},
keywords = {data-intensive systems, loop analysis, product families, software performance engineering},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {UML state machine, aspect-oriented modeling, behavioral variability, model-based testing, product line engineering},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/1529282.1529388,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hn\v{e}tynka, Petr and Malohlava, Michal},
title = {Using a product line for creating component systems},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529388},
doi = {10.1145/1529282.1529388},
abstract = {Component systems have become a wide-spread technology and found their place in several application domains. Each component system has its specifics and particularities that reflect its focus and the application domain it is intended for. Although important, the diversity of component systems leads to a number of problems including having different tools for each systems, unnecessary duplication of functionality and problems with integration when several domains are to be targeted. Based on categorization of component application domains, we propose a "meta-component system", which provides a software product line for creating custom component systems. We focus especially on the deployment and execution environment, which is where most diversities are found. We demonstrate the usage of the "meta-component system" and propose how it is to be realized by two core concepts of SOFA 2, namely connector generator and microcomponents.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {501–508},
numpages = {8},
keywords = {component systems, generative programming, product line engineering, runtime environment},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/1944892.1944913,
author = {Nguyen, Tuan and Colman, Alan and Talib, Muhammad Adeel and Han, Jun},
title = {Managing service variability: state of the art and open issues},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944913},
doi = {10.1145/1944892.1944913},
abstract = {In addition to inherited characteristics from software variability, service variability exposes two distinct characteristics that impose certain challenges in variability management. These characteristics are: i) Different types of variability and their inter-relationships; and ii) Dynamic and recursive variability communication among different stakeholders. This paper elaborates these distinct characteristics in detail with a case study. The challenges brought about by these distinct characteristics in managing variability also are highlighted. We present a review of related work in service variability management and briefly propose our ongoing approach to addressing these challenges.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–173},
numpages = {9},
keywords = {service oriented computing, service variability, variability communication, variability management, web services},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1007/s10270-015-0459-z,
author = {S\'{a}nchez, Ana B. and Segura, Sergio and Parejo, Jos\'{e} A. and Ruiz-Cort\'{e}s, Antonio},
title = {Variability testing in the wild: the Drupal case study},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0459-z},
doi = {10.1007/s10270-015-0459-z},
abstract = {Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in single features and 160 faults triggered by the interaction of up to four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {173–194},
numpages = {22},
keywords = {Automated testing, Non-functional properties, Test case prioritization, Test case selection, Variability testing, Variability-intensive systems}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {decision modelling, design-space exploration, multi-criteria decision analysis, multi-objective optimisation},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1007/978-3-662-45234-9_20,
author = {Collet, Philippe},
title = {Domain Specific Languages for Managing Feature Models: Advances and Challenges},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_20},
doi = {10.1007/978-3-662-45234-9_20},
abstract = {Managing multiple and complex feature models is a tedious and error-prone activity in software product line engineering. Despite many advances in formal methods and analysis techniques, the supporting tools and APIs are not easily usable together, nor unified. In this paper, we report on the development and evolution of the Familiar Domain-Specific Language DSL. Its toolset is dedicated to the large scale management of feature models through a good support for separating concerns, composing feature models and scripting manipulations. We overview various applications of Familiar and discuss both advantages and identified drawbacks. We then devise salient challenges to improve such DSL support in the near future.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {273–288},
numpages = {16}
}

@article{10.1016/j.cageo.2014.09.004,
author = {Buccella, Agustina and Cechich, Alejandra and Pol'la, Matias and Arias, Maximiliano and del Socorro Doldan, Maria and Morsan, Enrique},
title = {Marine ecology service reuse through taxonomy-oriented SPL development},
year = {2014},
issue_date = {December 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {73},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2014.09.004},
doi = {10.1016/j.cageo.2014.09.004},
abstract = {Nowadays, reusing software applications encourages researchers and industrials to collaborate in order to increase software quality and to reduce software development costs. However, effective reuse is not easy and only a limited portion of reusable models actually offers effective evidence regarding their appropriateness, usability and/or effectiveness. Focusing reuse on a particular domain, such as marine ecology, allows us to narrow the scope; and along with a systematic approach such as software product line development, helps us to potentially improving reuse. From our experiences developing a subdomain-oriented software product line (SPL for the marine ecology subdomain), in this paper we describe semantic resources created for assisting this development and thus promoting systematic software reuse. The main contributions of our work are focused on the definition of a standard conceptual model for marine ecology applications together with a set of services and guides which assist the process of product derivation. The services are structured in a service taxonomy (as a specialization of the ISO 19119 std) in which we create a new set of categories and services built over a conceptual model for marine ecology applications. We also define and exemplify a set of guides for composing the services of the taxonomy in order to fulfill different functionalities of particular systems in the subdomain. HighlightsSolutions for software reuse for GIS domains by using standard information.Domain-specific taxonomy for supporting the generation of software artifacts.Guides for using geographic services in order to fulfill different GIS functionalities of systems in the domain.Evaluation of the effectiveness of the taxonomy and guides when building an SPL and two derived products.Improvements on time and costs of new GIS products being developed.},
journal = {Comput. Geosci.},
month = dec,
pages = {108–121},
numpages = {14},
keywords = {Domain engineering, Domain-specific taxonomies, Geographic information systems, ISO 19100 standards, Software reuse}
}

@inproceedings{10.1145/3238147.3240466,
author = {Cashman, Mikaela and Cohen, Myra B. and Ranjan, Priya and Cottingham, Robert W.},
title = {Navigating the maze: the impact of configurability in bioinformatics software},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240466},
doi = {10.1145/3238147.3240466},
abstract = {The bioinformatics software domain contains thousands of applications for automating tasks such as the pairwise alignment of DNA sequences, building and reasoning about metabolic models or simulating growth of an organism. Its end users range from sophisticated developers to those with little computational experience. In response to their needs, developers provide many options to customize the way their algorithms are tuned. Yet there is little or no automated help for the user in determining the consequences or impact of the options they choose. In this paper we describe our experience working with configurable bioinformatics tools. We find limited documentation and help for combining and selecting options along with variation in both functionality and performance. We also find previously undetected faults. We summarize our findings with a set of lessons learned, and present a roadmap for creating automated techniques to interact with bioinformatics software. We believe these will generalize to other types of scientific software.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {757–767},
numpages = {11},
keywords = {bioinformatics, configurability, software testing},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/1629716.1629735,
author = {Asadi, Mohsen and Mohabbati, Bardia and Kaviani, Nima and Ga\v{s}evi\'{c}, Dragan and Bo\v{s}kovi\'{c}, Marko and Hatala, Marek},
title = {Model-driven development of families of Service-Oriented Architectures},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629735},
doi = {10.1145/1629716.1629735},
abstract = {The paradigms of Service Oriented Architecture (SOA) and Software Product Line Engineering (SPLE) facilitate the development of families of software-intensive products. Software Product Line practices can be leveraged to support the development of service-oriented applications to promote the reusability of assets throughout the iterative and incremental development of software product families. Such an approach enables various service oriented business processes and software products of the same family to be systematically created and integrated. In this paper, we advocate integration of software product line engineering with model driven engineering to enable a model driven specification of software services, capable of creating software products from a family of software services. Using the proposed method, we aim to provide a consistent view of a composed software system from a higher business administration perspective to lower levels of service implementation and deployment. We demonstrate how Model Driven Engineering (MDE) can help with injecting the set of required commonalities and variabilities of a software product from a high level business process design to the lower levels of service use.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {95–102},
numpages = {8},
keywords = {business process management, semantic web, service-oriented architectures, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1016/j.vlsi.2018.02.013,
author = {Stamelakos, Ioannis and Xydis, Sotirios and Palermo, Gianluca and Silvano, Cristina},
title = {Workload- and process-variation aware voltage/frequency tuning for energy efficient performance sustainability of NTC manycores},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {C},
issn = {0167-9260},
url = {https://doi.org/10.1016/j.vlsi.2018.02.013},
doi = {10.1016/j.vlsi.2018.02.013},
journal = {Integr. VLSI J.},
month = mar,
pages = {252–262},
numpages = {11},
keywords = {Near-threshold computing, Manycore architectures, Low power, Energy efficiency, Variability}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Model checking, Product-line analysis, Reliability analysis, Software product lines, Verification}
}

@inproceedings{10.1007/978-3-662-49224-6_22,
author = {Beek, Maurice H. and Gnesi, Stefania and Latella, Diego and Massink, Mieke},
title = {Towards Automatic Decision Support for Bike-Sharing System Design},
year = {2015},
isbn = {9783662492239},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-49224-6_22},
doi = {10.1007/978-3-662-49224-6_22},
abstract = {Public bike-sharing systems are a popular means of sustainable urban mobility, but their successful introduction in a city stands or falls with their specific designs. What kind of bikes and docking stations are needed, how many and where to install them? How to avoid as much as possible that stations are completely empty or full for some period? Hence, a bike-sharing system can be seen both as a highly reconfigurable system and as a collective adaptive system. In this paper, we present two complementary strategies for the evaluation of bike-sharing system designs by means of automated tool support. We use the Clafer toolset to perform multi-objective optimisation of attributed feature models known from software product line engineering and the recently developed mean field model checker FlyFast to assess performance and user satisfaction aspects of variants of large-scale bike-sharing systems. The combined use of these analysis approaches is a preliminary step in the direction of automatic decision support for the initial design of a bike-sharing system as well as its successive adaptations and reconfigurations that considers both qualitative and performance aspects.},
booktitle = {Revised Selected Papers of the SEFM 2015 Collocated Workshops on Software Engineering and Formal Methods - Volume 9509},
pages = {266–280},
numpages = {15}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {auto-tuning, feature model, genetic algorithm, java virtual machine, optimization},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.5555/3288338.3288341,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Finding correlations of features affecting energy consumption and performance of web servers using the HADAS eco-assistant},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {100},
number = {11},
issn = {0010-485X},
abstract = {The impact of energy consumption on the environment and the economy is raising awareness of "green" software engineering. HADAS is an eco-assistant that makes developers aware of the influence of their designs and implementations on the energy consumption and performance of the final product. In this paper, we extend HADAS to better support the requirements of users: researchers, automatically dumping the energy-consumption of different software solutions; and developers, who want to perform a sustainability analysis of different software solutions. This analysis has been extended by adding Pearson's chi-squared differentials and Bootstrapping statistics, to automatically check the significance of correlations of the energy consumption, or the execution time, with any other variable (e.g., the number of users) that can influence the selection of a particular eco-efficient configuration. We have evaluated our approach by performing a sustainability analysis of the most common web servers (i.e. PHP servers) using the time and energy data measured with the Watts Up? Pro tool previously dumped in HADAS. We show how HADAS helps web server providers to make a trade-off between energy consumption and execution time, allowing them to sell different server configurations with different costs without modifying the hardware.},
journal = {Computing},
month = nov,
pages = {1155–1173},
numpages = {19},
keywords = {68M20, 68N30, 68U35, 97K80, Energy efficiency, Linux, Performance, Web servers}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {Java, feature-oriented software development, memory footprint, monitoring},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {Feature models, modeling principles, software product lines},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1016/j.csi.2019.04.011,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Matalonga, Santiago},
title = {Trends in software reuse research: A tertiary study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.011},
doi = {10.1016/j.csi.2019.04.011},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {18},
keywords = {Software reuse, Trends in software reuse, Systematic literature review, Tertiary study}
}

@inproceedings{10.5555/2093889.2093921,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Ensan, Faezeh and Ga\v{s}evi\'{c}, Dragan and Mohabbati, Bardia},
title = {Bringing semantics to feature models with SAFMDL},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of a target domain and through the development of comprehensive and variability-covering domain models. The domain models developed within the software product line development process need to cover all of the possible features and aspects of the target domain. In other words, the domain models often described using feature models should be elaborate representations of the feature space of that domain. In order to operationalize feature-based representations of a software application, appropriate implementation mechanisms need to be employed. In this paper, we propose a Semantic Web-oriented language, called Semantic Annotations for Feature Modeling Description Language (SAFMDL) that provides the means to semantically describe feature models. We will show that using SAFMDL along with Semantic Web Query techniques, we are able to bridge the gap between software product lines and SOA technology. Our proposed work allows software practitioners to use Semantic Web technology to quickly and rapidly develop new software products based on SOA technology from software product lines.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {287–300},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.1145/3338906.3338928,
author = {Shahin, Ramy and Chechik, Marsha and Salay, Rick},
title = {Lifting Datalog-based analyses to software product lines},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338928},
doi = {10.1145/3338906.3338928},
abstract = {Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to ”lift” particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\'{e} Datalog engine. We evaluate our implementation on a set of benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {39–49},
numpages = {11},
keywords = {Datalog, Doop, Lifting, Pointer Analysis, Program Analysis, Software Product Lines, Souffl'{e}},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Numerical variability model, Feature, Non-functional requirement, Quality attribute, Category theory},
location = {Melbourne, VIC, Australia}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@article{10.1007/s10009-012-0250-1,
author = {Wong, Peter Y. and Albert, Elvira and Muschevici, Radu and Proen\c{c}a, Jos\'{e} and Sch\"{a}fer, Jan and Schlatte, Rudolf},
title = {The ABS tool suite: modelling, executing and analysing distributed adaptable object-oriented systems},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0250-1},
doi = {10.1007/s10009-012-0250-1},
abstract = {Modern software systems must support a high degree of variability to accommodate a wide range of requirements and operating conditions. This paper introduces the Abstract Behavioural Specification (ABS) language and tool suite, a comprehensive platform for developing and analysing highly adaptable distributed concurrent software systems. The ABS language has a hybrid functional and object- oriented core, and comes with extensions that support the development of systems that are adaptable to diversified requirements, yet capable to maintain a high level of trustworthiness. Using ABS, system variability is consistently traceable from the level of requirements engineering down to object behaviour. This facilitates temporal evolution, as changes to the required set of features of a system are automatically reflected by functional adaptation of the system's behaviour. The analysis capabilities of ABS stretch from debugging, observing and simulating to resource analysis of ABS models and help ensure that a system will remain dependable throughout its evolutionary lifetime. We report on the experience of using the ABS language and the ABS tool suite in an industrial case study.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {567–588},
numpages = {22},
keywords = {Concurrency, Feature modelling, Formal modelling and analysis, Software product line, Tool support, Variability}
}

@inproceedings{10.1007/978-3-642-39031-9_10,
author = {Silva, Eduardo and Medeiros, Ana Luisa and Cavalcante, Everton and Batista, Thais},
title = {A lightweight language for software product lines architecture description},
year = {2013},
isbn = {9783642390302},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39031-9_10},
doi = {10.1007/978-3-642-39031-9_10},
abstract = {The architecture description of a software product line (SPL) is essential to make it clear how the architecture realizes the feature model and to represent both the domain and application engineering architectural artefacts. However, most architecture description languages (ADLs) for SPL have limited support regarding variability management and they do not express the relationship between features and the architecture, besides the lack of tools for graphical and textual modelling and a non-clear separation between the domain and application engineering activities. In order to overcome these deficiencies, this paper presents LightPL-ACME, an ADL whose main goal is to be a simple, lightweight language for the SPL architecture description, and enable the association between the architectural specification and the artefacts involved in the SPL development process, including the relationship with the feature model and the representation of both domain and application engineering elements.},
booktitle = {Proceedings of the 7th European Conference on Software Architecture},
pages = {114–121},
numpages = {8},
keywords = {ACME, LightPL-ACME, architecture description languages, software product lines architectures},
location = {Montpellier, France},
series = {ECSA'13}
}

@inproceedings{10.5555/2555523.2555556,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Light-weight software product lines for small and medium-sized enterprises (SMEs)},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Product line engineering practices promote the idea of systematic reuse of core assets and have been reported to decrease time-to-market and development costs for new products. However, our recent efforts to transfer our product line engineering knowledge to several of our small and medium-size enterprise industrial partner showed that there are challenges that need to be addressed before core product line engineering ideas can be deployed in SME context. These challenges include upfront investment costs, business traceability, levels of abstraction of functional features and semantic distinction between functional and non-functional software aspects. In order to address these challenges within the context of SMEs, we adopt and extend the behavior-driven development methodology in a way to not only offer agility in practice but also to equip software developers with the means to capture and manage software variability within the behavior-driven development process. We introduce the details of the extended methodology and discuss its advantages and disadvantages in detail.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {311–324},
numpages = {14},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.5555/1885639.1885643,
author = {Lee, Kwanwoo and Kang, Kyo C.},
title = {Usage context as key driver for feature selection},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product derivation in software product line engineering starts with selection of variable features manifested in a feature model. Selection of variable features for a particular product, however, is not made arbitrarily. There are various factors affecting feature selection. We experienced that the usage context of a product is often the primary driver for feature selection. In this paper, we propose a model showing how product usage contexts are related to product features, and present a method for developing such a model during the domain engineering process and utilizing it to derive an optimal product configuration during the application engineering process. An elevator control software example is used to illustrate and validate the concept and the method.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {32–46},
numpages = {15},
keywords = {commonality and variability, feature modeling, product derivation, product usage contexts},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {multi-objective optimization, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1007/s11219-011-9153-8,
author = {Mussbacher, Gunter and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Amyot, Daniel},
title = {AoURN-based modeling and analysis of software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9153-8},
doi = {10.1007/s11219-011-9153-8},
abstract = {Software Product Line Engineering concerns itself with domain engineering and application engineering. During domain engineering, the whole product family is modeled with a preferred flavor of feature models and additional models as required (e.g., domain models or scenario-based models). During application engineering, the focus shifts toward a single family member and the configuration of the member's features. Recently, aspectual concepts have been employed to better encapsulate individual features of a Software Product Line (SPL), but the existing body of SPL work does not include a unified reasoning framework that integrates aspect-oriented feature description artifacts with the capability to reason about stakeholders' goals while taking feature interactions into consideration. Goal-oriented SPL approaches have been proposed, but do not provide analysis capabilities that help modelers meet the needs of the numerous stakeholders involved in an SPL while at the same time considering feature interactions. We present an aspect-oriented SPL approach for the requirements phase that allows modelers (a) to capture features, goals, and scenarios in a unified framework and (b) to reason about stakeholders' needs and perform trade-off analyses while considering undesirable interactions that are not obvious from the feature model. The approach is based on the Aspect-oriented User Requirements Notation (AoURN) and helps identify, prioritize, and choose products based on analysis results provided by AoURN editor and analysis tools. We apply the AoURN-based SPL framework to the Via Verde SPL to demonstrate the feasibility of this approach through the selection of a Via Verde product configuration that satisfies stakeholders' needs and results in a high-level, scenario-based specification that is free from undesirable feature interactions.},
journal = {Software Quality Journal},
month = sep,
pages = {645–687},
numpages = {43},
keywords = {Aspect-oriented modeling, Feature interactions, Goal-based requirements engineering, Scenario-based requirements engineering, Software product lines, User Requirements Notation}
}

@article{10.1016/j.future.2015.05.017,
title = {Allocating resources for customizable multi-tenant applications in clouds using dynamic feature placement},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.05.017},
doi = {10.1016/j.future.2015.05.017},
abstract = {Multi-tenancy, where multiple end users make use of the same application instance, is often used in clouds to reduce hosting costs. A disadvantage of multi-tenancy is however that it makes it difficult to create customizable applications, as all end users use the same application instance. In this article, we describe an approach for the development and management of highly customizable multi-tenant cloud applications. We apply software product line engineering techniques to cloud applications, and use an approach where applications are composed of multiple interacting components, referred to as application features. Using this approach, multiple features can be shared between different applications. Allocating resources for these feature-based applications is complex, as relations between components must be taken into account, and is referred to as the feature placement problem.In this article, we describe dynamic feature placement algorithms that minimize migrations between subsequent invocations, and evaluate them in dynamic scenarios where applications are added and removed throughout the evaluation scenario. We find that the developed algorithm achieves a low cost, while resulting in few resource migrations. In our evaluations, we observe that adding migration-awareness to the management algorithms reduces the number of instance migrations by more than 77 % and reduces the load moved between instances by more than 96 % when compared to a static management approach. Despite this reduction in number of migrations, a cost that is on average less than 3 % more than the optimal cost is achieved. We model customizable SaaS applications using feature modeling.A dynamic, migration-aware management approach is presented.Two ILP-based algorithms and a heuristic algorithm are compared.The dynamic algorithms reduce migrations and remain within 3% of the optimal cost.},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {63–76},
numpages = {14}
}

@inproceedings{10.1145/2188286.2188304,
author = {Tawhid, Rasha and Petriu, Dorina},
title = {User-friendly approach for handling performance parameters during predictive software performance engineering},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188304},
doi = {10.1145/2188286.2188304},
abstract = {A Software Product Line (SPL) is a set of similar software systems that share a common set of features. Instead of building each product from scratch, SPL development takes advantage of the reusability of the core assets shared among the SPL members. In this work, we integrate performance analysis in the early phases of SPL development process, applying the same reusability concept to the performance annotations. Instead of annotating from scratch the UML model of every derived product, we propose to annotate the SPL model once with generic performance annotations. After deriving the model of a product from the family model by an automatic transformation, the generic performance annotations need to be bound to concrete product-specific values provided by the developer. Dealing manually with a large number of performance annotations, by asking the developer to inspect every diagram in the generated model and to extract these annotations is an error-prone process. In this paper we propose to automate the collection of all generic parameters from the product model and to present them to the developer in a user-friendly format (e.g., a spreadsheet per diagram, indicating each generic parameter together with guiding information that helps the user in providing concrete binding values). There are two kinds of generic parametric annotations handled by our approach: product-specific (corresponding to the set of features selected for the product) and platform-specific (such as device choices, network connections, middleware, and runtime environment). The following model transformations for (a) generating a product model with generic annotations from the SPL model, (b) building the spreadsheet with generic parameters and guiding information, and (c) performing the actual binding are all realized in the Atlas Transformation Language (ATL).},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {109–120},
numpages = {12},
keywords = {atl, marte, model-driven development, performance completion, performance model, spl, uml},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@inproceedings{10.1145/1868433.1868445,
author = {Trujillo, Salvador and Perez, Antonio and Gonzalez, David and Hamid, Brahim},
title = {Towards the integration of advanced engineering paradigms into RCES: raising the issues for the safety-critical model-driven product-line case},
year = {2010},
isbn = {9781450303682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868433.1868445},
doi = {10.1145/1868433.1868445},
abstract = {The conception and design of Resource Constrained Embedded Systems is an inherently complex endeavor. In particular, non-functional requirements from security, dependability and variability are exacerbating this complexity. Recent times have seen a paradigm shift in terms of design through the combination of multiple software engineering paradigms together, namely, Model Driven Engineering and Software Product Line Engineering. Such paradigm shift is changing the way systems are developed nowadays, reducing development time significantly. Embedded systems are a case in point where a range of products for assorted domains such as energy, transportation, automotive, and so on are conceived as a family. However, most of the work so far has been focused on functional parts. The purpose of this talk is to foster some discussion during the workshop on the issues that need to be faced for these techniques to be applicable for Resource Constrained Embedded Systems for which security and dependability are primary requirements.},
booktitle = {Proceedings of the International Workshop on Security and Dependability for Resource Constrained Embedded Systems},
articleno = {9},
numpages = {4},
keywords = {dependability, model-driven development, resource constrained embedded systems, software product lines},
location = {Vienna, Austria},
series = {S&amp;D4RCES '10}
}

@inproceedings{10.1145/2642937.2642990,
author = {Abal, Iago and Brabrand, Claus and Wasowski, Andrzej},
title = {42 variability bugs in the linux kernel: a qualitative analysis},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642990},
doi = {10.1145/2642937.2642990},
abstract = {Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {421–432},
numpages = {12},
keywords = {bugs, feature interactions, linux, software variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1007/s11276-018-1718-z,
author = {Mukhlif, Fadhil and Noordin, Kamarul Ariffin Bin and Mansoor, Ali Mohammed and Kasirun, Zarinah Mohd},
title = {Green transmission for C-RAN based on SWIPT in 5G: a review},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-018-1718-z},
doi = {10.1007/s11276-018-1718-z},
abstract = {C-RAN is a promising new design for the next generation, an important aspect of it in the energy efficiency consideration. Hence, it is considering an innovative candidate to use it as an alternative cellular network instead of the traditional. Investigation green transmission of mobile cloud radio access networks based on SWIPT for 5G cellular networks. Especially, with considering SWIPT as a future solution for increasing the lifetime of end-user battery’s, that’s mean this technique will improving energy efficiency (EE). Addressing SWIPT into C-RAN is a challenging and it is needed to developing a new algorithm to use it on the cellular network with many trying to ensure the success of the system performance. C-RAN as a network and SWIPT as a promising technique with the suggesting green wireless network are discussed besides the importance of energy efficiency for the next generation. Furthermore, there was a study on fifth enabling technologies that can be used for 5G with emphasis on two of them (C-RAN and energy efficiency). Lastly, research challenges and future direction that require substantial research efforts are summarized.},
journal = {Wirel. Netw.},
month = jul,
pages = {2621–2649},
numpages = {29},
keywords = {Green transmission, Power transfer, Cloud radio access network, Energy harvesting (EH), Information decoding (ID), Time switching, Power splitting, MIMO}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Autonomic computing, Constraint programming, Dynamic adaptation, Dynamic software product line, Models at runtime, Variability, Verification, Web service composition}
}

@article{10.1007/s10922-013-9265-5,
author = {Moens, Hendrik and Truyen, Eddy and Walraven, Stefan and Joosen, Wouter and Dhoedt, Bart and De Turck, Filip},
title = {Cost-Effective Feature Placement of Customizable Multi-Tenant Applications in the Cloud},
year = {2014},
issue_date = {October   2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {4},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9265-5},
doi = {10.1007/s10922-013-9265-5},
abstract = {Cloud computing technologies can be used to more flexibly provision application resources. By exploiting multi-tenancy, instances can be shared between users, lowering the cost of providing applications. A weakness of current cloud offerings however, is the difficulty of creating customizable applications that retain these advantages. In this article, we define a feature-based cloud resource management model, making use of Software Product Line Engineering techniques, where applications are composed of feature instances using a service-oriented architecture. We focus on how resources can be allocated in a cost-effective way within this model, a problem which we refer to as the feature placement problem. A formal description of this problem, that can be used to allocate resources in a cost-effective way, is provided. We take both the cost of failure to place features, and the cost of using servers into account, making it possible to take energy costs or the cost of public cloud infrastructure into consideration during the placement calculation. Four algorithms that can be used to solve the feature placement problem are defined. We evaluate the algorithm solutions, comparing them with the optimal solution determined using an integer linear problem solver, and evaluating the execution times of the algorithms, making use of both generated inputs and a use case based on three applications. We show that, using our approach a higher degree of multi-tenancy can be achieved, and that for the considered scenarios, taking the relationships between features into account and using application-oriented placement performs 25---40 % better than a purely feature-oriented placement.},
journal = {J. Netw. Syst. Manage.},
month = oct,
pages = {517–558},
numpages = {42},
keywords = {Application placement, Cloud computing, Distributed computing, SPLE}
}

@inproceedings{10.1145/3030207.3030226,
author = {Stefan, Petr and Horky, Vojtech and Bulej, Lubomir and Tuma, Petr},
title = {Unit Testing Performance in Java Projects: Are We There Yet?},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030226},
doi = {10.1145/3030207.3030226},
abstract = {Although methods and tools for unit testing of performance exist for over a decade, anecdotal evidence suggests unit testing of performance is not nearly as common as unit testing of functionality. We examine this situation in a study of GitHub projects written in Java, looking for occurrences of performance evaluation code in common performance testing frameworks. We quantify the use of such frameworks, identifying the most relevant performance testing approaches, and describe how we adjust the design of our SPL performance testing framework to follow these conclusions.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {401–412},
numpages = {12},
keywords = {jmh, open source, performance unit testing, spl, survey},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/1629716.1629738,
author = {Alf\'{e}rez, Mauricio and Moreira, Ana and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Mateus, Ricardo and Amaral, Vasco},
title = {Detecting feature interactions in SPL requirements analysis models},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629738},
doi = {10.1145/1629716.1629738},
abstract = {The consequences of unwanted feature interactions in a Software Product Line (SPL) can range from minor problems to critical software failures. However, detecting feature interactions in reasonably complex model-based SPLs is a non-trivial task. This is due to the often large number of interdependent models that describe the SPL features and the lack of support for analyzing the relationships inside those models. We believe that the early detection of the points, where two or more features interact --- based on the models that describe the behavior of the features ---, is a starting point for the detection of conflicts and inconsistencies between features, and therefore, take an early corrective action.This vision paper foresees a process to find an initial set of points where it is likely to find potential feature interactions in model-based SPL requirements, by detecting: (i) dependency patterns between features using use case models; and (ii) overlapping between use case scenarios modeled using activity models.We focus on requirements models, which are special, since they do not contain many details about the structural components and the interactions between the higher-level abstraction modules of the system. Therefore, use cases and activity models are the means that help us to analyze the functionality of a complex system looking at it from a high level end-user view to anticipate the places where there are potential feature interactions. We illustrate the approach with a home automation SPL and then discuss about its applicability.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {117–123},
numpages = {7},
keywords = {feature interactions, software product lines requirements},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/3302541.3311525,
author = {Machida, Fumio},
title = {Practices in Model Component Reuse for Efficient Dependability Analysis},
year = {2019},
isbn = {9781450362863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302541.3311525},
doi = {10.1145/3302541.3311525},
abstract = {Model-based dependability analysis provides an effective manner to evaluate and design the dependability of critical IT systems by abstracting the system architecture and operations. As the size and the complexity of systems increase, however, the process to compose the dependability model becomes complicated and time-consuming. Improving the efficiency of modeling process is practically an important challenge of dependability engineering. In this paper, we review the techniques for model component reuse that makes dependability model composition and analysis more efficient. In particular, component-based modeling approaches for reliability, availability, maintainability and safety analysis presented in the literature are summarized. In order to effectively apply model component reuse, we advocate the importance of asset-based dependability analysis approach that associates the reusable model components with underlying system development process. Finally, we discuss the necessary extensions of these techniques toward efficient dependability analysis for IoT systems which are significantly affecting real world.},
booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {65–70},
numpages = {6},
keywords = {availability, dependability, internet of things, reuse, safety},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3141848.3141853,
author = {Schuster, Sven and Seidl, Christoph and Schaefer, Ina},
title = {Towards a development process for maturing Delta-oriented software product lines},
year = {2017},
isbn = {9781450355186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141848.3141853},
doi = {10.1145/3141848.3141853},
abstract = {A Software Product Line (SPL) exploits reuse-in-the-large to enable customization by explicitly modeling commonalities and variabilities of closely related software systems. Delta-Oriented Programming (DOP) is a flexible implementation approach to SPL engineering, which transforms an existing core product to another desired product by applying transformation operations. By capturing product alterations related to configurable functionality within delta modules, DOP closely resembles a natural process of software development, which proves beneficial in early stages of development. However, increasing complexity for a growing SPL in later development stages caused by the invasiveness of DOP drastically impairs maintenance and extensibility. Hence, a process utilizing the invasiveness of DOP in early development stages and restricting it in later stages would allow developers to mature growing delta-oriented SPLs. Moreover, ever-increasing complexity requires means to migrate into less invasive development approaches that are more suited for large-scale configurable applications. To this end, we propose a development process for delta-oriented SPLs including explicit variability points, metrics and refactorings as well as a semi-automatic reengineering of a delta-oriented SPL into a development approach based on blackbox-components. In this paper, we sketch this development process with its constituents and point out required research essential for successfully maturing a delta-oriented SPL.},
booktitle = {Proceedings of the 8th ACM SIGPLAN International Workshop on Feature-Oriented Software Development},
pages = {41–50},
numpages = {10},
keywords = {Delta-Oriented Programming, Software Product Lines},
location = {Vancouver, BC, Canada},
series = {FOSD 2017}
}

@inproceedings{10.1007/978-3-662-45234-9_19,
author = {Iosif-Laz\u{a}r, Alexandru F. and Schaefer, Ina and W\k{a}sowski, Andrzej},
title = {A Core Language for Separate Variability Modeling},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_19},
doi = {10.1007/978-3-662-45234-9_19},
abstract = {Separate variability modeling adds variability to a modeling language without requiring modifications of the language or the supporting tools. We define a core language for separate variability modeling using a single kind of variation point to define transformations of software artifacts in object models. Our language, Featherweight VML, has several distinctive features. Its architecture and operations are inspired by the recently proposed Common Variability Language CVL. Its semantics is considerably simpler than that of CVL, while remaining confluent unlike CVL. We simplify complex hierarchical dependencies between variation points via copying and flattening. Thus, we reduce a model with intricate dependencies to a flat executable model transformation consisting of simple unconditional local variation points. The core semantics is extremely concise: it boils down to two operational rules, which makes it suitable to serve as a specification for implementations of trustworthy variant derivation. Featherweight VML offers insights in the execution of other variability modeling languages such as the Orthogonal Variability Model and Delta Modeling. To the best of our knowledge, this is the first attempt to comprehensively formalize variant derivation, encompassing feature models, variation points, implementation artifacts and transformations.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {257–272},
numpages = {16}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {Quality attributes, SPL, reconfiguration, software architecture, variability},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/2897045.2897047,
author = {da Mota Silveira Neto, Paulo Anselmo and de Santana, Taijara Loiola and de Almeida, Eduardo Santana and Cavalcanti, Yguarata Cerqueira},
title = {RiSE events: a testbed for software product lines experimentation},
year = {2016},
isbn = {9781450341769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897045.2897047},
doi = {10.1145/2897045.2897047},
abstract = {Software Product Lines (SPL) demand mature software engineering, planning and reuse, adequate practices of management and development, and also the ability to deal with organizational issues and architectural complexity. Thus, it is important the development of new techniques, tools and methods to deal with SPL complexity required by the variability management. To address this issue, an SPL has been proposed, where the existing variability was implemented by applying conditional compilation. Moreover, no framework was used to develop it, allowing any researcher to use the SPL without losing time learning some framework. In this work, we implemented an SPL test bed containing 34 functional features has 26.457 lines of code, 1493 methods and 496 classes.},
booktitle = {Proceedings of the 1st International Workshop on Variability and Complexity in Software Design},
pages = {12–13},
numpages = {2},
keywords = {security and availability tacticts, software product lines, test bed, variability},
location = {Austin, Texas},
series = {VACE '16}
}

@inproceedings{10.1145/3106237.3106252,
author = {Kn\"{u}ppel, Alexander and Th\"{u}m, Thomas and Mennicke, Stephan and Meinicke, Jens and Schaefer, Ina},
title = {Is there a mismatch between real-world feature models and product-line research?},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106252},
doi = {10.1145/3106237.3106252},
abstract = {Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {291–302},
numpages = {12},
keywords = {Software product lines, cross-tree constraints, exclude constraints, expressiveness, feature modeling, model transformation, require constraints},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1007/978-3-030-26250-1_3,
author = {Ne\v{s}i\'{c}, Damir and Nyberg, Mattias},
title = {Modular Safety Cases for Product Lines Based on Assume-Guarantee Contracts},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_3},
doi = {10.1007/978-3-030-26250-1_3},
abstract = {Safety cases are recommended, and in some cases required, by a number of standards. In the product line context, unlike for single systems, safety cases are inherently complex because they must argue about the safety of a family of products that share various types of engineering assets. Safety case modularization has been proposed to reduce safety case complexity by separating concerns, modularizing tightly coupled arguments, and localizing effects of changes to particular modules. Existing modular safety-case approaches for product lines propose a feature-based modularization, which is too coarse to modularize the claims of different types, at different levels of abstraction. To overcome these limitation, a novel, modular safety-case architecture is presented. The modularization is based on a contract-based specification product-line model, which jointly captures the component-based architecture of systems and corresponding safety requirements as assume-guarantee contracts. The proposed safety-case architecture is analyzed against possible product-line changes and it is shown that it is robust both with respect to fine and coarse-grained, and also product and implementation-level changes. The proposed modular safety case is exemplified on a simplified, but real automotive system.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {28–40},
numpages = {13},
keywords = {Modular safety case, Assume-guarantee contract, Product line},
location = {Turku, Finland}
}

@inproceedings{10.1145/2701319.2701335,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {Managing the Variability in the Transactional Services Selection},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701335},
doi = {10.1145/2701319.2701335},
abstract = {Web service composition is the capability to recursively construct a value added service by means of picking up existing services. An important step in the composition process is the selection step, which includes choosing services located in repositories. The selection approaches of Web services need to consider their specifics which raises important challenges as the management of the inherent service variability in functionality and implementation and ensuring correct execution termination between others. To realize reliable service compositions, transactional properties of services must be considered during the selection step. We argue that the transactional properties should be considered at the operation level of each service to be composed. However, modelling transactional services composition at the operation level drastically increment the complexity of service selection. In order to overcome this difficulty, in this paper we report on our research in progress on transactional service selection, which follows a Software Product Line approach considering the set of services that provide the same functionality as part of a service family. We model the variable operations of the service families using Feature Models. In this way, the selection process consists of selecting each service from a service family such that the aggregated transactional property satisfies the user preference.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {88–95},
numpages = {8},
keywords = {Discovery and Selection, Feature Modeling, Transactional Services},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/3358960.3379144,
author = {Musaafir, Ahmed and Uta, Alexandru and Dreuning, Henk and Varbanescu, Ana-Lucia},
title = {A Sampling-Based Tool for Scaling Graph Datasets},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379144},
doi = {10.1145/3358960.3379144},
abstract = {Graph processing has become a topic of interest in many domains. However, we still observe a lack of representative datasets for in-depth performance and scalability analysis. Neither data collections, nor graph generators provide enough diversity and control for thorough analysis. To address this problem, we proposea heuristic method for scaling existing graphs. Our approach, based onsampling andinterconnection, can provide a scaled "version" of a given graph. Moreover, we provide analytical models to predict the topological properties of the scaled graphs (such as the diameter, degree distribution, density, or the clustering coefficient), and further enable the user to tweak these properties. Property control is achieved through a portfolio of graph interconnection methods (e.g., star, ring, chain, fully connected) applied for combining the graph samples. We further implement our method as an open-source tool which can be used to quickly provide families of datasets for in-depth benchmarking of graph processing algorithms. Our empirical evaluation demonstrates our tool provides scaled graphs of a wide range of sizes, whose properties match well with model predictions and/or user requirements. Finally, we also illustrate, through a case-study, how scaled graphs can be used for in-depth performance analysis of graph processing algorithms.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {graph datasets scaling, graph sampling, graph scaling tool, heuristic methods},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1109/ICSE.2017.58,
author = {Behringer, Benjamin and Palz, Jochen and Berger, Thorsten},
title = {PEoPL: projectional editing of product lines},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.58},
doi = {10.1109/ICSE.2017.58},
abstract = {The features of a software product line---a portfolio of system variants---can be realized using various implementation techniques (a.k.a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts.We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (&lt;45ms on average for our largest subject Berkeley DB).},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {563–574},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1007/11741060_6,
author = {Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang and Spinczyk, Olaf},
title = {The design of application-tailorable operating system product lines},
year = {2005},
isbn = {3540336893},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11741060_6},
doi = {10.1007/11741060_6},
abstract = {System software for deeply embedded devices has to cope with a broad variety of requirements and platforms, but especially with strict resource constraints. To compete against proprietary systems (and thereby to facilitate reuse), an operating system product line for deeply embedded systems has to be highly configurable and tailorable. It is therefore crucial that all selectable and configurable features can be encapsulated into fine-grained, exchangeable and reusable implementation components. However, the encapsulation of non-functional properties is often limited, due to their cross-cutting character. Fundamental system policies, like synchronization or activation points for the scheduler, have typically to be reflected in many points of the operating system component code. The presented approach is based on feature modeling, C++ class composition and overcomes the above mentioned problems by means of aspect-oriented programming (AOP). It facilitates a fine-grained encapsulation and configuration of even non-functional properties in system software.},
booktitle = {Proceedings of the Second International Conference on Construction and Analysis of Safe, Secure, and Interoperable Smart Devices},
pages = {99–117},
numpages = {19},
location = {Nice, France},
series = {CASSIS'05}
}

@article{10.1016/j.infsof.2012.06.012,
author = {Gamez, Nadia and Fuentes, Lidia},
title = {Architectural evolution of FamiWare using cardinality-based feature models},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.012},
doi = {10.1016/j.infsof.2012.06.012},
abstract = {Context: Ambient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems. Objective: We present a software product line engineering process to manage the evolution of FamiWare, a family of middleware for ambient intelligence environments. This process drives the evolution of FamiWare middleware configurations using cardinality-based feature models, which are especially well suited to express the structural variability of ambient intelligence systems. Method: FamiWare uses cardinality-based feature models and clonable features to model the structural variability present in ambient intelligence systems, composed of a large variety of heterogeneous devices. Since the management evolution of configurations with clonable features is manually untreatable due to the high number of features, our process automates it and propagates changes made at feature level to the architectural components of the FamiWare middleware. This is a model driven development process as the evolution management, the propagation of evolution changes and the code generation are performed using some kind of model mappings and transformations. Concretely we present a variability modelling language to map the selection of features to the corresponding FamiWare middleware architectural components. Results: Our process is able to manage the evolution of cardinality-based feature models with thousands of features, something which is not possible to tackle manually. Thanks to the use of the variability language and the automatic code generation it is possible to propagate and maintain a correspondence between the FamiWare architectural model and the code. The process is then able to calculate the architectural differences between the evolved configuration and the previous one. Checking these differences, our process helps to calculate the effort needed to perform the evolution changes in the customized products. To perform those tasks we have defined two operators, one to calculate the differences between two feature model configurations and another to create a new configuration from a previous one. Conclusion: Our process automatically propagates the evolution changes of the middleware family into the existing configurations where the middleware is already deployed and also helps us to calculate the effort in performing the changes in every configuration. Finally, we validated our approach, demonstrating the functioning of the defined operators and showing that by using our tool we can generate evolved configurations for FamiWare with thousands of cloned features, for several case studies.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {563–580},
numpages = {18},
keywords = {Evolution, Feature Models, Middleware family, Software Product Lines}
}

@article{10.1016/j.cl.2018.05.004,
author = {Combemale, Benoit and Kienzle, J\"{o}rg and Mussbacher, Gunter and Barais, Olivier and Bousse, Erwan and Cazzola, Walter and Collet, Philippe and Degueule, Thomas and Heinrich, Robert and J\'{e}z\'{e}quel, Jean-Marc and Leduc, Manuel and Mayerhofer, Tanja and Mosser, S\'{e}bastien and Sch\"{o}ttle, Matthias and Strittmatter, Misha and Wortmann, Andreas},
title = {Concern-oriented language development (COLD): Fostering reuse in language engineering},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.05.004},
doi = {10.1016/j.cl.2018.05.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {139–155},
numpages = {17},
keywords = {Domain-specific languages, Language concern, Language reuse}
}

@article{10.1007/s10270-016-0569-2,
author = {Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Lochau, Malte and Meinicke, Jens and Saake, Gunter},
title = {Effective product-line testing using similarity-based product prioritization},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0569-2},
doi = {10.1007/s10270-016-0569-2},
abstract = {A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufficient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efficiently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-based prioritization on three product lines with real faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {499–521},
numpages = {23},
keywords = {Combinatorial interaction testing, Model-based testing, Product-line testing, Software product lines, Test-case prioritization}
}

@inproceedings{10.1007/978-3-662-45234-9_22,
author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tapia Tarifa, S. Lizeth},
title = {Deployment Variability in Delta-Oriented Models},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_22},
doi = {10.1007/978-3-662-45234-9_22},
abstract = {Software engineering increasingly emphasizes variability by developing families of products for a range of application contexts or user requirements. ABS is a modeling language which supports variability in the formal modeling of software by using feature selection to transform a delta-oriented base model into a concrete product model. ABS also supports deployment models, with a separation of concerns between execution cost and server capacity. This allows the model-based assessment of deployment choices on a product's quality of service. This paper combines deployment models with the variability concepts of ABS, to model deployment choices as features when designing a family of products.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {304–319},
numpages = {16}
}

@inproceedings{10.1007/978-3-319-07317-0_8,
author = {Lochau, Malte and Peldszus, Sven and Kowal, Matthias and Schaefer, Ina},
title = {Model-Based Testing},
year = {2014},
isbn = {9783319073163},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07317-0_8},
doi = {10.1007/978-3-319-07317-0_8},
abstract = {Software more and more pervades our everyday lives. Hence, we have high requirements towards the trustworthiness of the software. Software testing greatly contributes to the quality assurance of modern software systems. However, as today's software system get more and more complex and exist in many different variants, we need rigorous and systematic approaches towards software testing. In this tutorial, we, first, present model-based testing as an approach for systematic test case generation, test execution and test result evaluation for single system testing. The central idea of model-based testing is to base all testing activities on an executable model-based test specification. Second, we consider model-based testing for variant-rich software systems and review two model-based software product line testing techniques. Sample-based testing generates a set of representative variants for testing, and variability-aware product line testing uses a family-based test model which contains the model-based specification of all considered product variants.},
booktitle = {Advanced Lectures of the 14th International School on Formal Methods for Executable Software Models - Volume 8483},
pages = {310–342},
numpages = {33}
}

@inproceedings{10.5555/2821357.2821367,
author = {Baresi, Luciano and Quinton, Cl\'{e}ment},
title = {Dynamically evolving the structural variability of dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {A Dynamic Software Product Line (dspl) is a widely used approach to handle variability at runtime, e.g., by activating or deactivating features to adapt the running configuration. With the emergence of highly configurable and evolvable systems, dspls have to cope with the evolution of their structural variability, i.e., the Feature Model (fm) used to derive the configuration. So far, little is known about the evolution of the fm while a configuration derived from this fm is running. In particular, such a dynamic evolution changes the dspl configuration space, which is thus unsynchronized with the running configuration and its adaptation capabilities. In this position paper, we propose and describe an initial architecture to manage the dynamic evolution of dspls and their synchronization. In particular, we explain how this architecture supports the evolution of dspls based on fms extended with cardinality and attributes, which, to the best of our knowledge, has never been addressed yet.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {57–63},
numpages = {7},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.5555/1885639.1885687,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire},
title = {MARTE mechanisms to model variability when analyzing embedded software product lines},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Nowadays, embedded systems development is increasing its complexity dealing with quality among others. Model Driven Development (MDD) and Software Product Line (SPL) can be adequate paradigms to traditional development and validation methods. MARTE (UML Profile for Modeling and Analysis of Real-Time and Embedded systems) profile facilitates model analysis thus ensuring quality achievement from models. SPL requires taking into account variability like functional, quality attributes, platform and allocation. Therefore, variability mechanisms of MARTE profile have been studied in order to perform embedded SPL model analysis.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {466–470},
numpages = {5},
keywords = {MARTE, embedded software, model analysis, software product lines, variability},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {featured transition systems, mutation analysis, variability},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/2492708.2492954,
author = {Benini, Luca and Flamand, Eric and Fuin, Didier and Melpignano, Diego},
title = {P2012: building an ecosystem for a scalable, modular and high-efficiency embedded computing accelerator},
year = {2012},
isbn = {9783981080186},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {P2012 is an area- and power-efficient many-core computing fabric based on multiple globally asynchronous, locally synchronous (GALS) clusters supporting aggressive fine-grained power, reliability and variability management. Clusters feature up to 16 processors and one control processor with independent instruction streams sharing a multi-banked L1 data memory, a multi-channel DMA engine, and specialized hardware for synchronization and scheduling. P2012 achieves extreme area and energy efficiency by supporting domain-specific acceleration at the processor and cluster level through the addition of dedicated HW IPs. P2012 can run standard OpenCL and OpenMP parallel codes well as proprietary Native Programming Model (NPM) SW components that provide the highest level of control on application-to-resource mapping. In Q3 2011 the P2012 SW Development Kit (SDK) has been made available to a community of R&amp;D users; it includes full OpenCL and NPM development environments. The first P2012 SoC prototype in 28nm CMOS will sample in Q4 2012, featuring four clusters and delivering 80GOPS (with single precision floating point support) in 15.2mm2 with 2W power consumption.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {983–987},
numpages = {5},
location = {Dresden, Germany},
series = {DATE '12}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1016/j.jss.2019.01.044,
author = {Th\"{u}m, Thomas and Kn\"{u}ppel, Alexander and Kr\"{u}ger, Stefan and Bolle, Stefanie and Schaefer, Ina},
title = {Feature-oriented contract composition},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.044},
doi = {10.1016/j.jss.2019.01.044},
journal = {J. Syst. Softw.},
month = jun,
pages = {83–107},
numpages = {25},
keywords = {Feature-oriented programming, Software product lines, Design by contract, Deductive verification, Formal methods}
}

@inproceedings{10.5555/1785246.1785323,
author = {Li, Yi-Yuan and Yin, Jian-wei and Li, Yin and Dong, Jin-Xiang},
title = {Configuration modeling based software product development},
year = {2007},
isbn = {354076836X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product line is an effective way to implement software production for mass customization. How to organize and configure the software artifacts in software product line to rapidly produce customized software product meeting individual demands is one of the key problems. Corresponding to the phases of feature selection and software artifact binding in the process of software production, the feature configuration model and software artifact configuration model are constructed to provide a uniform framework of constraint description for feature model and domain application requirement. The results of problem solving are the sets of feature and software artifact meeting feature constraints and application requirements. The proposed method of configuration modeling and problem solving provide a theoretical foundation to rapidly produce software product on the base of configuration of reusable domain assets.},
booktitle = {Proceedings of the 7th International Conference on Advanced Parallel Processing Technologies},
pages = {624–639},
numpages = {16},
keywords = {configuration rule, feature configuration, problem solving, software artifact configuration},
location = {Guangzhou, China},
series = {APPT'07}
}

@article{10.1145/2501654.2501664,
author = {Benoit, Anne and \c{C}ataly\"{u}rek, \"{U}mit V. and Robert, Yves and Saule, Erik},
title = {A survey of pipelined workflow scheduling: Models and algorithms},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2501654.2501664},
doi = {10.1145/2501654.2501664},
abstract = {A large class of applications need to execute the same workflow on different datasets of identical size. Efficient execution of such applications necessitates intelligent distribution of the application components and tasks on a parallel machine, and the execution can be orchestrated by utilizing task, data, pipelined, and/or replicated parallelism. The scheduling problem that encompasses all of these techniques is called pipelined workflow scheduling, and it has been widely studied in the last decade. Multiple models and algorithms have flourished to tackle various programming paradigms, constraints, machine behaviors, or optimization goals. This article surveys the field by summing up and structuring known results and approaches.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {50},
numpages = {36},
keywords = {Workflow programming, algorithms, distributed systems, filter-stream programming, latency, models, parallel systems, pipeline, scheduling, throughput}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@inproceedings{10.1007/978-3-662-45234-9_23,
author = {Lochau, Malte and Mennicke, Stephan and Baller, Hauke and Ribbeck, Lars},
title = {DeltaCCS: A Core Calculus for Behavioral Change},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_23},
doi = {10.1007/978-3-662-45234-9_23},
abstract = {Concepts for enriching formal languages with variability capabilities aim at comprehensive specifications and efficient development of families of similar software variants as propagated, e.g., by the software product line paradigm. However, recent approaches are usually limited to purely structural variability, e.g., by adapting choice operator semantics for variant selection. Those approaches lack 1 a modular separation of common and variable parts and/or 2 a rigorous formalization of semantical impacts of structural variations. To overcome those deficiencies, we propose a delta-oriented extension to Milner's process calculus CCS, called DeltaCCS, that allows for modular reasoning about behavioral variability. In DeltaCCS, modular change directives are applied to core processes by altering term rewriting semantics in a determined way. We define variability-aware CCS congruences for a modular reasoning on the preservation of behavioral properties defined by the Modal μ-Calculus after changing CCS specifications. We implemented a DeltaCCS model checker to efficiently verify the members of a family of process variants.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {320–335},
numpages = {16},
keywords = {Model Checking, Operational Semantics, Variability Modeling}
}

@inproceedings{10.1007/978-3-030-86130-8_44,
author = {Feng, Zhigang and Song, Xiaoqin and Lei, Lei},
title = {Efficient Concurrent Transmission Scheme for Wireless Ad Hoc Networks: A Joint Optimization Approach},
year = {2021},
isbn = {978-3-030-86129-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86130-8_44},
doi = {10.1007/978-3-030-86130-8_44},
abstract = {In this paper, we focus on the joint optimization of scheduling and power control to achieve effective concurrent transmission. Under constraints, we describe the optimization problem as a scheduling problem based on power control. In order to solve the complexity problem, we have determined the best power control strategy in the large network area. Using the dual-link communication architecture, we decompose the optimization problem into two sub-problems, namely power control and link scheduling. By solving two sub-problems, we have determined the best link scheduling and power control mechanism suitable for the actual network environment. We realize efficient concurrent transmission based on the optimal solution obtained by joint optimization to effectively utilize network resources and improve energy efficiency. The simulation results prove the effectiveness of the scheme. Compared with existing solutions, the joint optimization solution has obvious advantages in terms of network throughput and energy consumption.},
booktitle = {Wireless Algorithms, Systems, and Applications: 16th International Conference, WASA 2021, Nanjing, China, June 25–27, 2021, Proceedings, Part II},
pages = {563–574},
numpages = {12},
keywords = {Concurrent transmission, Power control, Link scheduling, Energy consumption, Dual-link structure},
location = {Nanjing, China}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {Program merging, control flow automata, model matching, quality assurance, variability encoding}
}

@article{10.1007/s10664-014-9336-6,
author = {Sobernig, Stefan and Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert},
title = {Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9336-6},
doi = {10.1007/s10664-014-9336-6},
abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition--it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features  in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1670–1705},
numpages = {36},
keywords = {Feature-oriented programming, Fuji, Software measurement, Software product lines, Structural cohesion, Structural coupling}
}

@article{10.1007/s11219-011-9170-7,
author = {Acher, Mathieu and Collet, Philippe and Gaignard, Alban and Lahire, Philippe and Montagnat, Johan and France, Robert B.},
title = {Composing multiple variability artifacts to assemble coherent workflows},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9170-7},
doi = {10.1007/s11219-011-9170-7},
abstract = {The development of scientific workflows is evolving toward the systematic use of service-oriented architectures, enabling the composition of dedicated and highly parameterized software services into processing pipelines. Building consistent workflows then becomes a cumbersome and error-prone activity as users cannot manage such large-scale variability. This paper presents a rigorous and tooled approach in which techniques from Software Product Line (SPL) engineering are reused and extended to manage variability in service and workflow descriptions. Composition can be facilitated while ensuring consistency. Services are organized in a rich catalog which is organized as a SPL and structured according to the common and variable concerns captured for all services. By relying on sound merging techniques on the feature models that make up the catalog, reasoning about the compatibility between connected services is made possible. Moreover, an entire workflow is then seen as a multiple SPL (i.e., a composition of several SPLs). When services are configured within, the propagation of variability choices is then automated with appropriate techniques and the user is assisted in obtaining a consistent workflow. The approach proposed is completely supported by a combination of dedicated tools and languages. Illustrations and experimental validations are provided using medical imaging pipelines, which are representative of current scientific workflows in many domains.},
journal = {Software Quality Journal},
month = sep,
pages = {689–734},
numpages = {46},
keywords = {Composition, Feature models, Scientific workflows, Software product lines}
}

@article{10.1016/j.infsof.2016.08.011,
author = {Tanhaei, Mohammad and Habibi, Jafar and Mirian-Hosseinabadi, Seyed-Hassan},
title = {Automating feature model refactoring},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.011},
doi = {10.1016/j.infsof.2016.08.011},
abstract = {Display Omitted Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the products' configurations in an SPL but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult.Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change.Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the ATL model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it.Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model.Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the ATL M2M language. Our framework assesses the correctness of the refactorings using the Alloy language.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {138–157},
numpages = {20},
keywords = {Feature model refactoring, Model transformation &amp; refactoring}
}

@inproceedings{10.1145/3053600.3053619,
author = {Mangels, Tatiana and Murarasu, Alin and Oden, Forest and Fishkin, Alexey and Becker, Daniel},
title = {Efficient Analysis at Edge},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053619},
doi = {10.1145/3053600.3053619},
abstract = {Digitalization changes traditional business models by using digital technologies to improve existing offerings and to create new offerings. Current technological trends such as artificial intelligence, autonomous systems, and predictive maintenance are ideal candidate technologies to enable digitalization use cases. Often, these technologies rely on the availability of large amounts of data and the capability to process these data efficiently. In contrast to consumer markets, industrial products must fulfill higher non-functional requirements such as fast response times, 24/7 availability and stability, real-time processing, safety, or security requirements. As a consequence, processing capabilities -- ranging from multicore and manycores to even high end parallel clusters -- have to be exploited to achieve necessary performance and stability needs. In this paper, we introduce a Distributed Multicore Monitoring Framework (MoMo) which is a reference monitoring solution developed at Siemens Corporate Technology. It can be used to easily build efficient and stable diagnostic solutions which can help to understand the correctness, availability, reliability, and performance of large-scale distributed systems based on live data. Due to its small footprint MoMo can be used to analyze data directly at the data source which, for instance, can significantly reduce the network load. While MoMo's efficiency comes from the usage of multicore processors (CPUs) for running analysis in parallel, its usability is guaranteed by its capability to easily integrate with other monitoring frameworks and its usage of SPL - a domain-specific language which allows user to easily define diagnostic algorithms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {85–90},
numpages = {6},
keywords = {data analysis, monitoring, parallel computing},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1007/978-3-319-27343-3_1,
author = {Braubach, Lars and Pokahr, Alexander and Kalinowski, Julian and Jander, Kai},
title = {Tailoring Agent Platforms with Software Product Lines},
year = {2015},
isbn = {9783319273426},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27343-3_1},
doi = {10.1007/978-3-319-27343-3_1},
abstract = {Agent platforms have been conceived traditionally as middleware, helping to deal with various application challenges like agent programming models, remote messaging, and coordination protocols. A\"{\i} \'{z}middleware is typically a bundle of functionalities necessary to execute multi-agent applications. In contrast to this traditional view, nowadays different use cases also for selected agent concepts have emerged requiring also different kinds of functionalities. Examples include a platform for conducting multi-agent simulations, intelligent agent behavior models for controlling non-player characters NPCs in games and a lightweight version suited for mobile devices. A one-size-fits-all software bundle often does not sufficiently match these requirements, because customers and developers want solutions specifically tailored to their needs, i.e. a small but focused solution is frequently preferred over bloated software with extraneous functionality. Software product lines are an approach suitable for creating a series of similar products from a common code base. In this paper we will show how software product line modeling and technology can help creating tailor-made products from multi-agent platforms. Concretely, the Jadex platform will be analyzed and a feature model as well as an implementation path will be presented.},
booktitle = {Revised Selected Papers of the 13th German Conference on Multiagent System Technologies - Volume 9433},
pages = {3–21},
numpages = {19},
location = {Cottbus, Germany},
series = {MATES 2015}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {automated testing, e-commerce, experience report, feature modelling, variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Automated production systems, Automation, Evolution, Software engineering}
}

@article{10.1007/s10664-016-9462-4,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9462-4},
doi = {10.1007/s10664-016-9462-4},
abstract = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1763–1794},
numpages = {32},
keywords = {Empirical evaluation, Feature models, Multi-objective evolutionary algorithms, Reverse engineering}
}

@inproceedings{10.1145/3485730.3485945,
author = {Liu, Tiantian and Gao, Ming and Lin, Feng and Wang, Chao and Ba, Zhongjie and Han, Jinsong and Xu, Wenyao and Ren, Kui},
title = {Wavoice: A Noise-resistant Multi-modal Speech Recognition System Fusing mmWave and Audio Signals},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3485945},
doi = {10.1145/3485730.3485945},
abstract = {With the advance in automatic speech recognition, voice user interface has gained popularity recently. Since the COVID-19 pandemic, VUI is increasingly preferred in online communication due to its non-contact. Additionally, various ambient noise impedes the public applications of voice user interfaces due to the requirement of audio-only speech recognition methods for a high signal-to-noise ratio. In this paper, we present Wavoice, the first noise-resistant multi-modal speech recognition system that fuses two distinct voice sensing modalities, i.e., millimeter-wave (mmWave) signals and audio signals from a microphone, together. One key contribution is that we model the inherent correlation between mmWave and audio signals. Based on it, Wavoice facilitates the real-time noise-resistant voice activity detection and user targeting from multiple speakers. Furthermore, we elaborate on two novel modules into the neural attention mechanism for multi-modal signals fusion, and result in accurate speech recognition. Extensive experiments verify Wavoice's effectiveness under various conditions with the character recognition error rate below 1% in a range of 7 meters. Wavoice outperforms existing audio-only speech recognition methods with lower character error rate and word error rate. The evaluation in complex scenes validates the robustness of Wavoice.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {97–110},
numpages = {14},
keywords = {Speech recognition, mmWave sensing, multimodal fusion, voice user interface},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@article{10.1023/A:1019791213967,
author = {Fornaciari, William and Sciuto, Donatella and Silvano, Cristina and Zaccaria, Vittorio},
title = {A Sensitivity-Based Design Space Exploration Methodology for Embedded Systems},
year = {2002},
issue_date = {September 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {7},
number = {1–2},
issn = {0929-5585},
url = {https://doi.org/10.1023/A:1019791213967},
doi = {10.1023/A:1019791213967},
abstract = {In this paper, we propose a system-level design methodology for the efficient exploration of the architectural parameters of the memory sub-systems, from the energy-delay joint perspective. The aim is to find the best configuration of the memory hierarchy without performing the exhaustive analysis of the parameters space. The target system architecture includes the processor, separated instruction and data caches, the main memory, and the system buses. To achieve a fast convergence toward the near-optimal configuration, the proposed methodology adopts an iterative local-search algorithm based on the sensitivity analysis of the cost function with respect to the tuning parameters of the memory sub-system architecture. The exploration strategy is based on the Energy-Delay Product (EDP) metric taking into consideration both performance and energy constraints. The effectiveness of the proposed methodology has been demonstrated through the design space exploration of a real-world case study: the optimization of the memory hierarchy of a MicroSPARC2-based system executing the set of Mediabench benchmarks for multimedia applications. Experimental results have shown an optimization speedup of 2 orders of magnitude with respect to the full search approach, while the near-optimal system-level configuration is characterized by a distance from the optimal full search configuration in the range of 2%.},
journal = {Des. Autom. Embedded Syst.},
month = sep,
pages = {7–33},
numpages = {27},
keywords = {Design space exploration, power and performance optimization}
}

@inproceedings{10.1109/ASE.2011.6100096,
author = {Oster, Zachary J. and Santhanam, Ganesh Ram and Basu, Samik},
title = {Automating analysis of qualitative preferences in goal-oriented requirements engineering},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100096},
doi = {10.1109/ASE.2011.6100096},
abstract = {In goal-oriented requirements engineering, a goal model graphically represents relationships between the required goals (functional requirements), tasks (realizations of goals), and optional goals (non-functional properties) involved in designing a system. It may, however, be impossible to find a design that fulfills all required goals and all optional goals. In such cases, it is useful to find designs that provide the required functionality while satisfying the most preferred set of optional goals under the goal model's constraints. We present an approach that considers expressive qualitative preferences over optional goals, as these can model interacting and/or mutually exclusive subgoals. Our framework employs a model checking-based method for reasoning with qualitative preferences to identify the most preferred alternative(s). We evaluate our approach using existing goal models from the literature.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {448–451},
numpages = {4},
series = {ASE '11}
}

@article{10.4018/IJCAC.2019100105,
author = {Aouzal, Khadija and Hafiddi, Hatim and Dahchour, Mohamed},
title = {Policy-Driven Middleware for Multi-Tenant SaaS Services Configuration},
year = {2019},
issue_date = {Oct 2019},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {4},
issn = {2156-1834},
url = {https://doi.org/10.4018/IJCAC.2019100105},
doi = {10.4018/IJCAC.2019100105},
abstract = {The multi-tenancy architecture allows software-as-a-service applications to serve multiple tenants with a single instance. This is beneficial as it leverages economies of scale. However, it does not cope with the specificities of each tenant and their variability; notably, the variability induced in the required quality levels that differ from a tenant to another. Hence, sharing one single instance hampers the fulfillment of these quality levels for all the tenants and leads to service level agreement violations. In this context, this article proposes a policy-driven middleware that configures the service according to the non-functional requirements of the tenants. The adopted approach combines software product lines engineering and model driven engineering principles. It spans the quality attributes lifecycle, from documenting them to annotating the service components with them as policies, and it enables dynamic configuration according to service level agreements terms of the tenants.},
journal = {Int. J. Cloud Appl. Comput.},
month = oct,
pages = {86–106},
numpages = {21},
keywords = {MDE, Multi-Tenancy, Non-Functional Variability, Policy, SaaS, SLA, SPLE}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1007/978-3-642-54804-8_1,
author = {Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Daum, Marcus and Klein, Joachim and M\"{a}rcker, Steffen and Wunderlich, Sascha},
title = {Probabilistic Model Checking and Non-standard Multi-objective Reasoning},
year = {2014},
isbn = {9783642548031},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-54804-8_1},
doi = {10.1007/978-3-642-54804-8_1},
abstract = {Probabilistic model checking is a well-established method for the automated quantitative system analysis. It has been used in various application areas such as coordination algorithms for distributed systems, communication and multimedia protocols, biological systems, resilient systems or security. In this paper, we report on the experiences we made in inter-disciplinary research projects where we contribute with formal methods for the analysis of hardware and software systems. Many performance measures that have been identified as highly relevant by the respective domain experts refer to multiple objectives and require a good balance between two or more cost or reward functions, such as energy and utility. The formalization of these performance measures requires several concepts like quantiles, conditional probabilities and expectations and ratios of cost or reward functions that are not supported by state-ofthe- art probabilistic model checkers. We report on our current work in this direction, including applications in the field of software product line verification.},
booktitle = {Proceedings of the 17th International Conference on Fundamental Approaches to Software Engineering - Volume 8411},
pages = {1–16},
numpages = {16}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {Dimensions of software configuration, configuration management and life cycle, developer study, variability},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {data management, digital engineering, security, software product lines},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.1145/1944892.1944894,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Th\"{u}m, Thomas and Saake, Gunter},
title = {Multi-dimensional variability modeling},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944894},
doi = {10.1145/1944892.1944894},
abstract = {The variability of a software product line (SPL)is often described with a feature model. To avoid highly complex models, stakeholders usually try to separate different variability dimensions, such as domain variability and implementation variability. This results in distinct variability models, which are easier to handle than one large model. On the other hand, it is sometimes required to analyze the variability dimensions of an SPL in combination using a single model only. To combine separate modeling and integrated analysis of variability, we present Velvet, a language for multi-dimensional variability modeling. Velvet allows stakeholders to model each variability dimension of an SPL separately and to compose the separated dimensions on demand. This improves reuse of feature models and supports independent modeling variability dimensions. Furthermore, Velvet integrates feature modeling and configuration in a single language. The combination of both concepts creates further reuse opportunities and allows stakeholders to independently configure variability dimensions.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {11–20},
numpages = {10},
keywords = {feature models, separation of concerns, variability modeling},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and L\"{o}we, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {MAPE-K, autonomic elements, context, goals, off-line training, on-line, variability, variants, variation-points},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.5555/2485288.2485437,
author = {Ayad, Gasser and Acquaviva, Andrea and Macii, Enrico and Sahbi, Brahim and Lemaire, Romain},
title = {HW-SW integration for energy-efficient/variability-aware computing},
year = {2013},
isbn = {9781450321532},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Recent trends in embedded system architectures brought a rapid shift towards multicore, heterogeneous and reconfigurable platforms. This imposes a large effort for programmers to develop their applications to efficiently exploit the underlying architecture. In addition, process variability issues lead to performance and power uncertainties, impacting expected quality of service and energy efficiency of the running software. In particular, variability may lead to sub-optimal runtime task allocation.In this paper we present a holistic approach to tackle these issues exploiting high level HW/SW modeling to customize the runtime library. The customization introduces variability awareness in task allocation decisions, with the final purpose of optimizing a given objective: Execution time, power consumption, or overall energy consumption.We present a complete walkthrough, from top-level modeling down to variability-aware execution using a parallelized computational kernel running on a next generation, NoC based, heterogeneous multicore simulation platform.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {607–611},
numpages = {5},
location = {Grenoble, France},
series = {DATE '13}
}

@inproceedings{10.1145/3251104,
author = {Langdon, William B. and Petke, Justyna and White, David R.},
title = {Session details: Genetic Improvement 2015 Workshop},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251104},
doi = {10.1145/3251104},
abstract = {It is our great pleasure to welcome you to the first international workshop on the Genetic Improvement of Software -- GI-2015, held at GECCO'15. Our goal was to bring together research from across the globe to exchange ideas on using optimisation techniques, particularly evolutionary computation such as genetic programming, to improve existing software. We invited short position papers to encourage the discussion of new ideas and recent work in addition to longer and more concrete submissions. The call for participation invited GI work on automatic bug-fixing; improving functionality; improving non-functional properties such as efficiency, memory and energy consumption; "plastic surgery" by transplanting functionality from other existing code to host software; and automatically specialising generic software for dedicated tasks. As you will see, we have accepted papers in most of these areas as well as papers on improving the nascent genetic improvement tools in use, improving parallel code, GI's relationship with software product lines (SPL), improving security and GI for embedded systems.We had submissions from Asia, Europe and both North and South America. They were exactly evenly split between full-length submissions (8) and two page position papers (8).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1016/j.jss.2014.12.041,
author = {Pascual, Gustavo G. and Lopez-Herrejon, Roberto E. and Pinto, M\'{o}nica and Fuentes, Lidia and Egyed, Alexander},
title = {Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.12.041},
doi = {10.1016/j.jss.2014.12.041},
abstract = {Mobile applications require to self-adapt their behavior to context changes.We propose a DSPL approach to manage variability at runtime.Configurations are generated using multiobjective evolutionary algorithms.We apply a fix operator to generate only valid configurations at runtime.We demonstrate that this approach is suitable for mobile environments. Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.},
journal = {J. Syst. Softw.},
month = may,
pages = {392–411},
numpages = {20},
keywords = {DSPL, Dynamic reconfiguration, Evolutionary algorithms}
}

@article{10.1016/j.eswa.2013.12.028,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Hierons, Robert M. and Benavides, David and Ruiz-Cort\'{e}s, Antonio},
title = {Automated generation of computationally hard feature models using evolutionary algorithms},
year = {2014},
issue_date = {June, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.12.028},
doi = {10.1016/j.eswa.2013.12.028},
abstract = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {3975–3992},
numpages = {18},
keywords = {Automated analysis, Evolutionary algorithms, Feature models, Performance testing, Search-based testing, Software product lines}
}

@article{10.1007/s11276-014-0838-3,
author = {Yang, Meng and Kim, Donghyun and Li, Deying and Chen, Wenping and Tokuta, Alade O.},
title = {Maximum lifetime suspect monitoring on the street with battery-powered camera sensors},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {4},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-014-0838-3},
doi = {10.1007/s11276-014-0838-3},
abstract = {A camera sensor network is a sensor network of a group of camera sensors and is being deployed for various surveillance and monitoring applications. In this paper, we propose a new surveillance model for camera sensor network, namely half-view model, which requires a camera sensor network to capture the face image of any object if it moves forward to pass over an area of interest. Based on this new surveillance model, we introduce a new sleep-wakeup scheduling problem in camera sensor network, namely the maximum lifetime half-view barrier-coverage (MaxL-HV-BC) problem, whose goal is to find an on-off schedule of battery-operated camera sensors such that the continuous time duration providing half-view barrier-coverage over an area of interest is maximized. We develop a strategy to check if a region is half-view covered by a given set of camera sensors, and use this strategy to design two new heuristic algorithms for MaxL-HV-BC. We also conduct simulations to compare the average performance of the proposed algorithms with a trivial solution as well as the theoretical upper bound.},
journal = {Wirel. Netw.},
month = may,
pages = {1093–1107},
numpages = {15},
keywords = {Barrier coverage, Camera sensor networks, Energy-efficiency, Half-view coverage, Maximum lifetime, Scheduling}
}

@inproceedings{10.1145/3384419.3430424,
author = {Xu, Weitao and Li, Zhenjiang and Xue, Wanli and Yu, Xiaotong and Wang, Jia and Luo, Chengwen and Li, Wei and Zomaya, Albert Y.},
title = {Inaudible acoustic signal based key agreement system for IoT devices: poster abstract},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430424},
doi = {10.1145/3384419.3430424},
abstract = {Secure Device-to-Device (D2D) communication is becoming increasingly important with the ever-growing number of Internet-of-Things (IoT) devices in our daily life. To achieve secure D2D communication, the key agreement between different IoT devices without any prior knowledge is becoming desirable. Although various approaches have been proposed in the literature, they suffer from a number of limitations, such as low key generation rate and short pairing distance. In this paper, we present an inaudible acoustic signal based key generation protocol for mobile devices. Based on acoustic channel reciprocity, our system exploits channel frequency response of two legitimate devices as a common secret to generate keys. Extensive experiments are conducted to evaluate the proposed system in different real environments. Evaluation results show that the proposed system can generate the same secret key for two mobile devices with high probability.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {689–690},
numpages = {2},
keywords = {IoT device, acoustic signal, device pairing, key generation},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {dynamic features, energy analysis, probabilistic model checking, software product lines},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/1562860.1562864,
author = {Siegmund, Norbert and Pukall, Mario and Soffner, Michael and K\"{o}ppen, Veit and Saake, Gunter},
title = {Using software product lines for runtime interoperability},
year = {2009},
isbn = {9781605585482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1562860.1562864},
doi = {10.1145/1562860.1562864},
abstract = {Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we outline why feature-oriented programming is appropriate in such scenarios. Second, we describe the runtime adaption process of services with feature-oriented programming.},
booktitle = {Proceedings of the Workshop on AOP and Meta-Data for Software Evolution},
articleno = {4},
numpages = {7},
keywords = {interoperability, runtime adaption, software product lines},
location = {Genova, Italy},
series = {RAM-SE '09}
}

@article{10.1109/TNET.2021.3056772,
author = {Ruby, Rukhsana and Zhong, Shuxin and ElHalawany, Basem M. and Luo, Hanjiang and Wu, Kaishun},
title = {SDN-Enabled Energy-Aware Routing in Underwater Multi-Modal Communication Networks},
year = {2021},
issue_date = {June 2021},
publisher = {IEEE Press},
volume = {29},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3056772},
doi = {10.1109/TNET.2021.3056772},
abstract = {Despite extensive research efforts, underwater sensor networks (UWSNs) still suffer from serious performance issues due to their inefficient and uncoordinated channel access and resource management. For example, due to the lack of holistic knowledge on the network resources, existing decentralized routing protocols fail to provide globally optimal performance. On the other hand, Software Defined Networking (SDN), as a promising paradigm to provide prominent centralized solutions, can be employed to address the aforementioned issues in UWSNs. Indeed, SDN brings unprecedented opportunities to improve the network performance through the development of advanced algorithms at controllers. In this paper, we study the routing problem in such a network with new features including centralized route decision, global network-state awareness, seamless route discovery while considering the optimization of several long-term global performance metrics. We formulate the entire routing problem of a multi-modal UWSN as an optimization problem while considering the interference phenomenon of ad hoc scenarios and some long-term global performance metrics of an ideal routing protocol. Our formulated problem nicely captures all possible flexibilities of a sensor node no matter it has the full-duplex or half-duplex functionality. Upon the formulation, we recognize the NP-hard nature of the problem for all possible scenarios. We adopt a rounding technique based on the convex programming relaxation concept to solve the formulated routing problem that considers full-duplex scenarios, whereas we solve the problem for half-duplex scenarios using a greedy method upon interpreting it as a submodular function maximization problem. Through extensive simulation via our Python-based in-house simulator, we verify that our proposed globally optimal routing scheme always outperforms three existing decentralized routing protocols (each of these protocols are selected from each of three prominent protocol types, i.e., flooding, cross-layer information and adaptive machine learning based, respectively) in terms of reliability, latency, energy efficiency, lifetime and fairness.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {965–978},
numpages = {14}
}

@inproceedings{10.1145/1944892.1944897,
author = {Gilson, Fabian and Englebert, Vincent},
title = {Towards handling architecture design, variability and evolution with model transformations},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944897},
doi = {10.1145/1944892.1944897},
abstract = {Software systems have to face evolving requirements from information system stakeholders, infrastructure modifications, and evolving rationales about the implementation. This increases the rate of migration and redeployment of systems. Recent approaches intend to abstract architectural element specifications from the implementing technology and manage software design through model transformations. Based on an Architecture Description Language integrating infrastructure modelling facilities and a requirement modelling language, the present work manages architecturally significant requirements and infrastructure evolutions by model transformations. Our approach offers support for evolution and variability management tasks as it makes explicit the rationales concerning requirements, infrastructure and implementation alternatives that guide both the software architecture and the infrastructure definition.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {39–48},
numpages = {10},
keywords = {architecturally significant requirement, architecture description language, architecture variability, infrastructure constraint, model transformation},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@phdthesis{10.5555/2049065,
author = {Sun, Hongyu},
advisor = {Lutz, Robyn R.},
title = {Quantifiable non-functional requirements modeling and static verification for web service compositions},
year = {2010},
isbn = {9781124430300},
publisher = {Iowa State University},
address = {USA},
abstract = {As service oriented architectures have become more widespread in industry, many complex web services are assembled using independently developed modular services from different vendors. Although the functionalities of the composite web services are ensured during the composition process, the non-functional requirements (NFRs) are often ignored in this process. Since quality of services plays a more and more important role in modern service-based systems, there is a growing need for effective approaches to verifying that a composite web service not only offers the required functionality but also satisfies the desired NFRs. Current approaches to verifying NFRs of composite services (as opposed to individual services) remain largely ad-hoc and informal in nature. This is especially problematic for high-assurance composite web services. High-assurance composite web services are those composite web services with special concern on critical NFRs such as security, safety and reliability. Examples of such applications include traffic control, medical decision support and the coordinated response systems for civil emergencies. The latter serves to motivate and illustrate the work described here.In this dissertation we develop techniques for ensuring that a composite service meets the user-specified NFRs expressible as hard constraints, e.g., “the messages of particular operations must be authenticated.” We introduce an automata-based framework for verifying that a composite service satisfies the desired NFRs based on the known guarantees regarding the non-functional properties of the component services. This automata-based model is able to represent NFRs that are hard, quantitative constraints on the composite web services. This model addresses two issues previously not handled in the modeling and verification of NFRs for composite web services: (1) the scope of the NFRs and (2) consistency checking of multiple NFRs.A scope of a NFR on a web service composition is the effective range of the NFR on the sub-workflows and modular services of the web service composition. It allows more precise description of a NFR constraint and more efficient verification. When multiple NFRs exist and overlap in their scopes, consistency checking is necessary to avoid wasted verification efforts on conflicting constraints. The approach presented here captures scope information in the model and uses it to check the consistency of multiple NFRs prior to the static verification of web service compositions. We illustrate how our approach can be used to verify security requirements for an Emergency Management System. We then focus on families of highly-customizable, composed web services where repeated verification of similar sets of NFRs can waste computation resources. We introduce a new approach to extend software product line engineering techniques to the web service composition domain. The resulting technique uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user makes customization decisions, the technique provides a more efficient way to verify customizable web services. A decision model, illustrated with examples from the emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space is shown to improve the quality of the composite services and reduce the cost of reverifying the same compositions. By distinguishing the commonalities and the variabilities of the web services, we divide the web composition into two stages: the preparation stage (to construct all commonalities) and the customization stage (to choose optional and alternative features). We thus draw most of the computation overhead into the first stage during the design in order to enable improved runtime efficiency during the second stage.A simulation platform was constructed to conduct experiments on the two verification approaches and three strategies introduced in this dissertation. The results of these experiments were analyzed to show the advantage of our automaton-based model in its verification efficiency with scoping information. We have shown how to choose the most efficient verification strategy from the three strategies of verifying multiple NFRs introduced in this dissertation under different circumstances. The results indicate that the software product line approach has significant efficiency improvement over traditional on-demand verification for highly customizable web service compositions.},
note = {AAI3438737}
}

@inproceedings{10.1007/978-3-662-45234-9_10,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hork\'{y}, Vojtundefinedch and Kit, Micha\l{} and Marek, Luk\'{a}\v{s} and T\r{u}ma, Petr},
title = {Towards Performance-Aware Engineering of Autonomic Component Ensembles},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_10},
doi = {10.1007/978-3-662-45234-9_10},
abstract = {Ensembles of autonomic components are a novel software engineering paradigm for development of open-ended distributed highly dynamic software systems e.g. smart cyber-physical systems. Recent research centered around the concept of ensemble-based systems resulted in design and development models that aim to systematize and simplify the engineering process of autonomic components and their ensembles. These methods highlight the importance of covering both the functional concepts and the non-functional properties, specifically performance-related aspects of the future systems. In this paper we propose an integration of the emerging techniques for performance assessment and awareness into different stages of the development process. Our goal is to aid both designers and developers of autonomic component ensembles with methods providing performance awareness throughout the entire development life cycle including runtime.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {131–146},
numpages = {16},
keywords = {component systems, ensemble-based systems, performance engineering}
}

@inproceedings{10.1145/3185768.3186406,
author = {Kub\'{a}t, Petr and Bulej, Lubom\'{\i}r and Bureundefined, Tom\'{a}undefined and Hork\'{y}, Vojtech and Tuma, Petr},
title = {Adaptive Dispatch: A Pattern for Performance-Aware Software Self-Adaptation},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186406},
doi = {10.1145/3185768.3186406},
abstract = {Modern software systems often employ dynamic adaptation to runtime conditions in some parts of their functionality -- well known examples range from autotuning of computing kernels through adaptive battery saving strategies of mobile applications to dynamic load balancing and failover functionality in computing clouds. Typically, the implementation of these features is problem-specific -- a particular autotuner, a particular load balancer, and so on -- and enjoys little support from the implementation environment beyond standard programming constructs. In this work, we propose Adaptive Dispatch as a generic coding pattern for implementing dynamic adaptation. We believe that such pattern can make the implementation of dynamic adaptation features better in multiple aspects -- an explicit adaptation construct makes the presence of adaptation easily visible to programmers, lends itself to manipulation with development tools, and facilitates coordination of adaptation behavior at runtime. We present an implementation of the Adaptive Dispatch pattern as an internal DSL in Scala.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {195–198},
numpages = {4},
keywords = {Scala, performance adaptation, performance awareness},
location = {Berlin, Germany},
series = {ICPE '18}
}

@article{10.1007/s10515-011-0080-5,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Apel, Sven and Saake, Gunter},
title = {Flexible feature binding in software product lines},
year = {2011},
issue_date = {June      2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0080-5},
doi = {10.1007/s10515-011-0080-5},
abstract = {A software product line (SPL) is a family of programs that share assets from a common code base. The programs of an SPL can be distinguished in terms of features, which represent units of program functionality that satisfy stakeholders' requirements. The features of an SPL can be bound either statically at program compile time or dynamically at run time. Both binding times are used in SPL development and have different advantages. For example, dynamic binding provides high flexibility whereas static binding supports fine-grained customizability without any impact on performance (e.g., for use on embedded systems). However, contemporary techniques for implementing SPLs force a programmer to choose the binding time already when designing an SPL and to mix different implementation techniques when multiple binding times are needed. We present an approach that integrates static and dynamic feature binding seamlessly. It allows a programmer to implement an SPL once and to decide per feature at deployment time whether it should be bound statically or dynamically. Dynamic binding usually introduces an overhead regarding resource consumption and performance. We reduce this overhead by statically merging features that are used together into dynamic binding units. A program can be configured at run time by composing binding units on demand. We use feature models to ensure that only valid feature combinations can be selected at compile and at run time. We provide a compiler and evaluate our approach on the basis of two non-trivial SPLs.},
journal = {Automated Software Engg.},
month = jun,
pages = {163–197},
numpages = {35},
keywords = {Dynamic binding, Feature binding time, Feature composition, Feature-oriented programming, Software product lines, Static binding}
}

@article{10.1016/j.jss.2016.09.045,
author = {Parejo, Jos\'{e} A. and S\'{a}nchez, Ana B. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio and Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Multi-objective test case prioritization in highly configurable systems},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.045},
doi = {10.1016/j.jss.2016.09.045},
abstract = {A multi-objective test case prioritization real-world case study is presented.Seven objective functions based on functional and non-functional data are proposed.Comparison of the effectiveness of 63 combinations of up to three objectives.NSGA-II evolutionary algorithm to solve the multi-objective prioritization problem.Multi-objective prioritization is more effective than mono-objective approaches. Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non-functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
journal = {J. Syst. Softw.},
month = dec,
pages = {287–310},
numpages = {24},
keywords = {Automated software testing, Highly-configurable systems, Test case prioritization, Variability}
}

@inproceedings{10.5555/2820518.2820528,
author = {Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando},
title = {Mining energy-aware commits},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {56–67},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1016/j.infsof.2016.08.005,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Requirements monitoring frameworks},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.005},
doi = {10.1016/j.infsof.2016.08.005},
abstract = {Analyze the characteristics and application areas of monitoring approaches proposed in different domains.Systematically identify frameworks supporting requirements monitoring.Analyze to what extent the monitoring frameworks support requirements monitoring in SoS. ContextSoftware systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. ObjectiveThe aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. MethodWe performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. ResultsWe identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. ConclusionsWe conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {89–109},
numpages = {21},
keywords = {Requirements monitoring, Systematic literature review, Systems of systems}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {java, javadoc, performance awareness, performance documentation, performance testing},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/3384419.3430727,
author = {Sun, Ke and Chen, Chen and Zhang, Xinyu},
title = {"Alexa, stop spying on me!": speech privacy protection against voice assistants},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430727},
doi = {10.1145/3384419.3430727},
abstract = {Voice assistants (VAs) are becoming highly popular recently as a general means of interacting with the Internet of Things. However, the use of always-on microphones on VAs imposes a looming threat on users' privacy. In this paper, we propose MicShield, the first system that serves as a companion device to enforce privacy preservation on VAs. MicShield introduces a novel selective jamming mechanism, which obfuscates the user's private speech while passing legitimate voice commands to the VAs. It achieves this by using a phoneme level jamming control pipeline. Our implementation and experiments demonstrate that MicShield can effectively protect a user's private speech, without affecting the VA's responsiveness.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {298–311},
numpages = {14},
keywords = {privacy protection, selective jamming, voice assistant},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/3384419.3430781,
author = {Sami, Sriram and Dai, Yimin and Tan, Sean Rui Xiang and Roy, Nirupam and Han, Jun},
title = {Spying with your robot vacuum cleaner: eavesdropping via lidar sensors},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430781},
doi = {10.1145/3384419.3430781},
abstract = {Eavesdropping on private conversations is one of the most common yet detrimental threats to privacy. A number of recent works have explored side-channels on smart devices for recording sounds without permission. This paper presents LidarPhone, a novel acoustic side-channel attack through the lidar sensors equipped in popular commodity robot vacuum cleaners. The core idea is to repurpose the lidar to a laser-based microphone that can sense sounds from subtle vibrations induced on nearby objects. LidarPhone carefully processes and extracts traces of sound signals from inherently noisy laser reflections to capture privacy sensitive information (such as speech emitted by a victim's computer speaker as the victim is engaged in a teleconferencing meeting; or known music clips from television shows emitted by a victim's TV set, potentially leaking the victim's political orientation or viewing preferences). We implement LidarPhone on a Xiaomi Roborock vacuum cleaning robot and evaluate the feasibility of the attack through comprehensive real-world experiments. We use the prototype to collect both spoken digits and music played by a computer speaker and a TV soundbar, of more than 30k utterances totaling over 19 hours of recorded audio. LidarPhone achieves approximately 91% and 90% average accuracies of digit and music classifications, respectively.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {354–367},
numpages = {14},
keywords = {acoustic side-channel, eavesdropping, lidar},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {auto-tuning, multi-tenancy, self-optimization, software-as-a-service, variability modeling},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.5555/1947545.1947579,
author = {Schlegel, Christian and Steck, Andreas and Brugali, Davide and Knoll, Alois},
title = {Design abstraction and processes in robotics: from code-driven to model-driven engineering},
year = {2010},
isbn = {3642173187},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Advanced software engineering is the key factor in the design of future complex cognitive robots. It will decide about their robustness, (run-time) adaptivity, cost-effectiveness and usability.We present a novel overall vision of a model-driven engineering approach for robotics that fuses strategies for robustness by design and robustness by adaptation. It enables rigid definitions of quality-of-service, re-configurability and physics-based simulation as well as for seamless system level integration of disparate technologies and resource awareness.We report on steps towards implementing this idea driven by a first robotics meta-model with first explications of non-functional properties. A model-driven toolchain provides the model transformation and code generation steps. It also provides design time analysis of resource parameters (e.g. schedulability analysis of realtime tasks) as step towards resource awareness in the development of integrated robotic systems.},
booktitle = {Proceedings of the Second International Conference on Simulation, Modeling, and Programming for Autonomous Robots},
pages = {324–335},
numpages = {12},
location = {Darmstadt, Germany},
series = {SIMPAR'10}
}

@inproceedings{10.1145/2377836.2377842,
author = {Gamez, Nadia and Romero, Daniel and Fuentes, Lidia and Rouvoy, Romain and Duchien, Laurence},
title = {Constraint-based self-adaptation of wireless sensor networks},
year = {2012},
isbn = {9781450315661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377836.2377842},
doi = {10.1145/2377836.2377842},
abstract = {In recent years, the Wireless Sensor Networks (WSNs) have become a useful mechanism to monitor physical phenomena in environments. The sensors that make part of these long-lived networks have to be reconfigured according to context changes in order to preserve the operation of the network. Such reconfigurations require to consider the distributed nature of the sensor nodes as well as their resource scarceness. Therefore, self-adaptations for WSNs have special requirements comparing with traditional information systems. In particular, the reconfiguration of the WSN requires a trade-off between critical dimensions for this kind of networks and devices, such as resource consumption or reconfiguration cost. Thus, in this paper, we propose to exploit Constraint-Satisfaction Problem (CSP) techniques in order to find a suitable configuration for self-adapting WSNs, modelled using a Dynamic Software Product Line (DSPL), when the context changes. We exploit CSP modeling to find a compromise between contradictory dimensions. To illustrate our approach, we use an Intelligent Transportation System scenario. This case study enables us to show the advantages of obtaining suitable and optimized configurations for self-adapting WSNs.},
booktitle = {Proceedings of the 2nd International Workshop on Adaptive Services for the Future Internet and 6th International Workshop on Web APIs and Service Mashups},
pages = {20–27},
numpages = {8},
keywords = {constraint-satisfaction problem, dynamic software product lines, self-adaptation, wireless sensor networks},
location = {Bertinoro, Italy},
series = {WAS4FI-Mashups '12}
}

@inproceedings{10.5555/1768904.1768909,
author = {Gallina, Barbara and Guelfi, Nicolas},
title = {A template for requirement elicitation of dependable product lines},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Engineering software quickly and at a low cost, while preserving quality, is a well-known objective that has not been reached. Reducing the development time can be achieved by reusing software components, as proposed in the software product line development approach. Dependability may be one of the most important attributes concerning quality, due to negative consequences (health, cost, time, etc.) induced by non-dependable software. Our proposal, presented in this article, is to offer a means to elicit the requirements of a product line, such that the dependability attribute would be explicitly considered, and such that reuse would be achieved by differentiating commonalities and variabilities between products. The proposed semi-formal template includes product commonality and variability elicitation, as well as elicitation of normal, misuse and recovery scenarios. Furthermore, we allow the elicitation of the advanced transactional nature of scenarios, since it provides us with a way to elicit fault tolerance requirements, which is our targeted means to achieving dependability.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {63–77},
numpages = {15},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00045,
author = {Tsuda, Naohiko and Washizaki, Hironori and Honda, Kiyoshi and Nakai, Hidenori and Fukazawa, Yoshiaki and Azuma, Motoei and Komiyama, Toshihiro and Nakano, Tadashi and Suzuki, Hirotsugu and Morita, Sumie and Kojima, Katsue and Hando, Akiyoshi},
title = {WSQF: comprehensive software quality evaluation framework and benchmark based on SQuaRE},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00045},
doi = {10.1109/ICSE-SEIP.2019.00045},
abstract = {Conventional quality evaluations of software concentrate on specific quality characteristics. Moreover, the measurement data are limited to specific products and organizations. Consequently, the present state of product quality and quality-in-use characteristics are not fully understood, preventing effective decision-making for software stakeholders. To alleviate this problem, ISO/IEC defined international standards called the SQuaRE (Systems and software Quality Requirements an Evaluation) series for comprehensive quality measurement and evaluation. However, these standards remain rather general and abstract, making them difficult to apply. In this paper, we establish a SQuaRE-based comprehensive software quality evaluation framework, Waseda Software Quality Framework (WSQF), which concretizes many product quality and quality-in-use measurement methods originally defined in the SQuaRE series. By applying the WSQF to 21 commercial ready-to-use software products, we revealed the status of software product quality. A resulted comprehensive benchmark includes trends of the quality measurement values, relationships among quality characteristics, relationship between quality-in-use and product quality, and relationship between the quality characteristics and product contexts within the limits of an application.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {312–321},
numpages = {10},
keywords = {quality assurance, software quality management, square series},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1145/3053600.3053617,
author = {Eichelberger, Holger and Qin, Cui and Schmid, Klaus},
title = {From Resource Monitoring to Requirements-based Adaptation: An Integrated Approach},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053617},
doi = {10.1145/3053600.3053617},
abstract = {In large and complex systems there is a need to monitor resources as it is critical for system operation to ensure sufficient availability of resources and to adapt the system as needed. While there are various (resource)-monitoring solutions, these typically do not include an analysis part that takes care of analyzing violations and responding to them. In this paper we report on experiences, challenges and lessons learned in creating a solution for performing requirements-monitoring for resource constraints and using this as a basis for adaptation to optimize the resource behavior. Our approach rests on reusing two previous solutions (one for resource monitoring and one for requirements-based adaptation) that were built in our group.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {91–96},
numpages = {6},
keywords = {adaptation, easy-produce, event processing, monitoring, quality requirements, resources, spass-meter},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1007/s00607-018-0646-1,
author = {Galindo, Jos\'{e} A. and Benavides, David and Trinidad, Pablo and Guti\'{e}rrez-Fern\'{a}ndez, Antonio-Manuel and Ruiz-Cort\'{e}s, Antonio},
title = {Automated analysis of feature models: Quo vadis?},
year = {2019},
issue_date = {May       2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {101},
number = {5},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-018-0646-1},
doi = {10.1007/s00607-018-0646-1},
abstract = {Feature models have been used since the 90s to describe software product lines as a way of reusing common parts in a family of software systems. In 2010, a systematic literature review was published summarizing the advances and settling the basis of the area of automated analysis of feature models (AAFM). From then on, different studies have applied the AAFM in different domains. In this paper, we provide an overview of the evolution of this field since 2010 by performing a systematic mapping study considering 423 primary sources. We found six different variability facets where the AAFM is being applied that define the tendencies: product configuration and derivation; testing and evolution; reverse engineering; multi-model variability-analysis; variability modelling and variability-intensive systems. We also confirmed that there is a lack of industrial evidence in most of the cases. Finally, we present where and when the papers have been published and who are the authors and institutions that are contributing to the field. We observed that the maturity is proven by the increment in the number of journals published along the years as well as the diversity of conferences and workshops where papers are published. We also suggest some synergies with other areas such as cloud or mobile computing among others that can motivate further research in the future.},
journal = {Computing},
month = may,
pages = {387–433},
numpages = {47},
keywords = {68T35, Automated analysis, Feature models, Software product lines, Variability-intensive systems}
}

@inproceedings{10.1145/2254756.2254772,
author = {Van Houdt, Benny and Bortolussi, Luca},
title = {Fluid limit of an asynchronous optical packet switch with shared per link full range wavelength conversion},
year = {2012},
isbn = {9781450310970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254756.2254772},
doi = {10.1145/2254756.2254772},
abstract = {We consider an asynchronous all optical packet switch (OPS) where each link consists of N wavelength channels and a pool of C ≤ N full range tunable wavelength converters. Under the assumption of Poisson arrivals with rate λ (per wavelength channel) and exponential packet lengths, we determine a simple closed-form expression for the limit of the loss probabilities Ploss(N) as N tends to infinity (while the load and conversion ratio σ=C/N remains fixed). More specifically, for σ ≤ λ2 the loss probability tends to (λ2-σ)/λ(1+λ), while for σ &gt; λ2 the loss tends to zero. We also prove an insensitivity result when the exponential packet lengths are replaced by certain classes of phase-type distributions. A key feature of the dynamical system (i.e., set of ODEs) that describes the limit behavior of this OPS switch, is that its right-hand side is discontinuous. To prove the convergence, we therefore had to generalize some existing result to the setting of piece-wise smooth dynamical systems.},
booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {113–124},
numpages = {12},
keywords = {fluid limit, optical packet switch, wavelength conversion},
location = {London, England, UK},
series = {SIGMETRICS '12}
}

@inproceedings{10.1109/WI-IAT.2014.170,
author = {Louati, Amine and Haddad, Joyce El and Pinson, Suzanne},
title = {A Multilevel Agent-Based Approach for Trustworthy Service Selection in Social Networks},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.170},
doi = {10.1109/WI-IAT.2014.170},
abstract = {The growing number of services available within social applications (viz. Social networks) raises a new and challenging search issue: selecting desired services from social networks. Traditional discovery and selection approaches, which are registry-based (e.g., UDDI, ebXML), have manifested their limitations as they often fall behind users' expectations. This is because registries fail to (i) take into consideration non functional properties such as QoS and trust and (ii) capitalize on the information resulting from the previous experiences between agents. To address these shortcomings, we use software agents as they support interactions and offer well-developed capabilities to formally express and interpret semantic information useful to evaluate trust. Trust in a service is a multi-aspect concept that includes a social-based aspect such as judging whether the provider is worthwhile pursuing before using his services (viz. Trust in sociability), expert-based aspect such as estimating whether the service behaves well and as expected (viz. Trust in expertise) and, recommender-based aspect such as assessing whether an agent is reliable and we can rely on its recommendations (viz. Trust in recommendation).},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {214–221},
numpages = {8},
keywords = {Multi-Agent Systems, Referral Systems, Service Selection, Social Networks, Trust},
series = {WI-IAT '14}
}

@inproceedings{10.1145/2851553.2851569,
author = {Hork\'{y}, Vojt\v{e}ch and Kotr\v{c}, Jaroslav and Libi\v{c}, Peter and T\r{u}ma, Petr},
title = {Analysis of Overhead in Dynamic Java Performance Monitoring},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2851569},
doi = {10.1145/2851553.2851569},
abstract = {In production environments, runtime performance monitoring is often limited to logging of high level events. More detailed measurements, such as method level tracing, tend to be avoided because their overhead can disrupt execution. This limits the information available to developers when solving performance issues at code level. One approach that reduces the measurement disruptions is dynamic performance monitoring, where the measurement instrumentation is inserted and removed as needed. Such selective monitoring naturally reduces the aggregate overhead, but also introduces transient overhead artefacts related to insertion and removal of instrumentation. We experimentally analyze this overhead in Java, focusing in particular on the measurement accuracy, the character of the transient overhead, and the longevity of the overhead artefacts.Among other results, we show that dynamic monitoring requires time from seconds to minutes to deliver stable measurements, that the instrumentation can both slow down and speed up the execution, and that the overhead artefacts can persist beyond the monitoring period.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {275–286},
numpages = {12},
keywords = {dynamic instrumentation, java, performance measurement overhead},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1109/ISPAW.2011.69,
author = {Kwon, Jagun and Hailes, Stephen},
title = {A Lightweight, Component-Based Approach to Engineering Reconfigurable Embedded Real-Time Control Software},
year = {2011},
isbn = {9780769544298},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISPAW.2011.69},
doi = {10.1109/ISPAW.2011.69},
abstract = {The cost of poor or repeat engineering in complex control systems is extremely high, and flexibility in software design and implementation is one of the key factors in staying competitive in the market. Complexity can be managed most effectively if the underlying software systems support structured, standardised, high-level abstraction layers that encapsulate unnecessary details behind well-defined interfaces. Moreover, since the costs of software maintenance are often as high as that of initial development, the ease with which it is possible flexibly to reconfigure, re-engineer, and replace software components in operational systems is also critical. In this paper, we present a lightweight, component-based approach to engineering embedded real-time control software, which is realized in the form of a middleware system named MIREA. The middleware supports dynamic reconfiguration of components written in C/C++, and addresses variability management in relation to non-functional properties, such as quality-of-service (QoS) and real-time scheduling. Users are allowed to componentize existing libraries easily, such as the standard NIST 4D/Real-time Control Systems (RCS) library, which has been successfully used in many U.S government-driven intelligent control projects, and to reuse them as dynamically reconfigurable components. A realistic illustration is provided showing how control systems are structured and reconfigured using our approach. In fact, we discuss our approach to control using a fusion of NIST RCS as a means of architecting a real time control system and MIREA as a means of realising that architecture. Our progress to date suggests that MIREA is indeed well suited as a middleware facilitating the construction of efficient, lightweight, and scalable real-time embedded control systems.},
booktitle = {Proceedings of the 2011 IEEE Ninth International Symposium on Parallel and Distributed Processing with Applications Workshops},
pages = {361–366},
numpages = {6},
series = {ISPAW '11}
}

@inproceedings{10.1145/2851553.2892038,
author = {Reichelt, David Georg and K\"{u}hne, Stefan},
title = {Empirical Analysis of Performance Problems on Code Level},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2892038},
doi = {10.1145/2851553.2892038},
abstract = {Performance problems are well known on architecture level. On code level their occurrences have not been systematically researched so far. Since a lot of everyday work of software developers is done on code level, methods and tools with focus on frequent performance problems are relevant. In the presented thesis, a method for systematically evaluating the occurrence and the frequency of performance problems on code level is presented and applied to repositories. The results of this empirical research will be a classification of performance problems and a quantification of their frequency. This will raise the awareness on certain problem classes for developers and will provide a basis for the development of new performance tools for preventing performance problems.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {117–120},
numpages = {4},
keywords = {change-based test selection, mining software repositories, performance analysis of software system versions, performance engineering},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1145/2897937.2897972,
author = {Choi, Young-kyu and Cong, Jason and Fang, Zhenman and Hao, Yuchen and Reinman, Glenn and Wei, Peng},
title = {A quantitative analysis on microarchitectures of modern CPU-FPGA platforms},
year = {2016},
isbn = {9781450342360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897937.2897972},
doi = {10.1145/2897937.2897972},
abstract = {CPU-FPGA heterogeneous acceleration platforms have shown great potential for continued performance and energy efficiency improvement for modern data centers, and have captured great attention from both academia and industry. However, it is nontrivial for users to choose the right platform among various PCIe and QPI based CPU-FPGA platforms from different vendors. This paper aims to find out what microarchitectural characteristics affect the performance, and how. We conduct our quantitative comparison and in-depth analysis on two representative platforms: QPI-based Intel-Altera HARP with coherent shared memory, and PCIe-based Alpha Data board with private device memory. We provide multiple insights for both application developers and platform designers.},
booktitle = {Proceedings of the 53rd Annual Design Automation Conference},
articleno = {109},
numpages = {6},
location = {Austin, Texas},
series = {DAC '16}
}

@inproceedings{10.1007/11751113_3,
author = {Caporuscio, Mauro and Muccini, Henry and Pelliccione, Patrizio and Di Nisio, Ezio},
title = {Rapid system development via product line architecture implementation},
year = {2005},
isbn = {3540340637},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11751113_3},
doi = {10.1007/11751113_3},
abstract = {Software Product Line (SPL) engineering allows designers to reason about an entire family of software applications, instead of a single product, with a strategic importance for the rapid development of new applications. While much effort has been spent so far in understanding and modeling SPLs and their architectures, very little attention has been given on how to systematically enforce SPL architectural decisions into the implementation step.In this paper we propose a methodological approach and an implementation framework, based on a plugin component-based development, which allows us to move from an architectural specification of the SPL to its implementation in a systematic way. We show the suitability of this framework through its application to the TOOL one case study SPL.},
booktitle = {Proceedings of the Second International Conference on Rapid Integration of Software Engineering Techniques},
pages = {18–33},
numpages = {16},
location = {Heraklion, Crete, Greece},
series = {RISE'05}
}

@article{10.1007/s11219-012-9185-8,
author = {Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan},
title = {Foreword to the special issue on quality engineering for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9185-8},
doi = {10.1007/s11219-012-9185-8},
journal = {Software Quality Journal},
month = sep,
pages = {421–424},
numpages = {4}
}

@inproceedings{10.1145/3277893.3277898,
author = {Bao, Yuanyuan and Ma, Liqiu and Chen, Wai},
title = {SSL: Synchronous Self-paced Learning for Internet-of-Things Devices},
year = {2018},
isbn = {9781450360517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277893.3277898},
doi = {10.1145/3277893.3277898},
abstract = {With the emergence of Internet-of-Things (IoT), we are witnessing rapid increases in the amount of IoT devices. However, due to IoT devices are often deployed in dynamic and unmanned environment, it is imperative that the IoT devices have automatic model construction capabilities. In this paper, we propose a novel two-step approach called synchronous self-paced learning (SSL) for the IoT devices to construct the model automatically. In the first step, we design a synchronous mechanism that makes correlated devices synchronized and obtain the pseudo labels to form the original training set. In the second step, we propose a sample selection method based on reliability and diversity to filter the original training set, based on which we automatically accomplish the model construction. We conduct empirical evaluation of our SSL method compared with three state-of-the-art methods; and the evaluation shows that by applying our SSL method, we can achieve the classifier with higher accuracy.},
booktitle = {Proceedings of the 1st ACM International Workshop on Smart Cities and Fog Computing},
pages = {13–18},
numpages = {6},
keywords = {Internet-of-Things (IoT), automatic learning, self-paced learning, wearable sensors},
location = {Shenzhen, China},
series = {CitiFog'18}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th international symposium on the maintenance and evolution of service-oriented and cloud-based systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {34–37},
numpages = {4},
keywords = {SOA, cloud computing, cloudbased systems, service-oriented systems, serviceoriented architecture, services, software evolution, software maintenance}
}

@inproceedings{10.1145/2479871.2479922,
author = {Bulej, Lubom\'{\i}r and Burea, Tom\'{a}\v{s} and Hork\'{y}, Vojt\v{c}ch and Keznikl, Jaroslav},
title = {Adaptive deployment in ad-hoc systems using emergent component ensembles: vision paper},
year = {2013},
isbn = {9781450316361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479871.2479922},
doi = {10.1145/2479871.2479922},
abstract = {Mobile cloud computing in the context of ad-hoc clouds brings new challenges when offloading computation from mobile devices. The management of application deployment needs to ensure that the offloading provides users with the expected benefits, but it suddenly needs to cope with a highly dynamic environment which lacks a central authority and in which computational nodes appear and disappear.We propose an approach to the management of ad-hoc systems in such dynamic environment using component ensembles that connect mobile devices with more powerful computation nodes. Our approach aims to address the challenges of scalability and robustness of such systems without the need for central authority, relying instead on simple patterns that lead to reasonable adaptation decisions based on limited and imprecise information.},
booktitle = {Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering},
pages = {343–346},
numpages = {4},
keywords = {ad-hoc cloud, adaptive deployment, ensembles},
location = {Prague, Czech Republic},
series = {ICPE '13}
}

@inproceedings{10.1145/2462326.2462332,
author = {Quinton, Cl\'{e}ment and Haderer, Nicolas and Rouvoy, Romain and Duchien, Laurence},
title = {Towards multi-cloud configurations using feature models and ontologies},
year = {2013},
isbn = {9781450320504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462326.2462332},
doi = {10.1145/2462326.2462332},
abstract = {Configuration and customization choices arise due to the heterogeneous and scalable aspect of the cloud computing paradigm. To avoid being restricted to a given cloud and ensure application requirements, using several clouds to deploy a multi-cloud configuration is recommended but introduces several challenges due to the amount of providers and their intrinsic variability. In this paper, we present a model-driven approach based on Feature Models (FMs) originating from Software Product Lines (SPL) to handle cloud variability and then manage and create cloud configurations. We combine it with ontologies, used to model the various semantics of cloud systems. The approach takes into consideration application technical requirements as well as non-functional ones to provide a set of valid cloud or multi-cloud configurations and is implemented in a framework named SALOON.},
booktitle = {Proceedings of the 2013 International Workshop on Multi-Cloud Applications and Federated Clouds},
pages = {21–26},
numpages = {6},
keywords = {cloud computing, feature model, multi-cloud, ontology, variability},
location = {Prague, Czech Republic},
series = {MultiCloud '13}
}

@inproceedings{10.1145/2188286.2188345,
author = {Bulej, Lubom\'{\i}r and Bure\v{s}, Tom\'{a}\v{s} and Keznikl, Jaroslav and Koubkov\'{a}, Alena and Podzimek, Andrej and T\r{u}ma, Petr},
title = {Capturing performance assumptions using stochastic performance logic},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188345},
doi = {10.1145/2188286.2188345},
abstract = {Compared to functional unit testing, automated performance testing is difficult, partially because correctness criteria are more difficult to express for performance than for functionality. Where existing approaches rely on absolute bounds on the execution time, we aim to express assertions on code performance in relative, hardware-independent terms. To this end, we introduce Stochastic Performance Logic (SPL), which allows making statements about relative method performance. Since SPL interpretation is based on statistical tests applied to performance measurements, it allows (for a special class of formulas) calculating the minimum probability at which a particular SPL formula holds. We prove basic properties of the logic and present an algorithm for SAT-solver-guided evaluation of SPL formulas, which allows optimizing the number of performance measurements that need to be made. Finally, we propose integration of SPL formulas with Java code using higher-level performance annotations, for performance testing and documentation purposes.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {311–322},
numpages = {12},
keywords = {performance testing, regression benchmarking},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.5555/1886301.1886321,
author = {K\"{a}stner, Daniel and Heckmann, Reinhold and Ferdinand, Christian},
title = {100% coverage for safety-critical software - efficient testing by static analysis},
year = {2010},
isbn = {3642156509},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Safety-critical embedded software is used more and more pervasively in the automotive, avionics and healthcare industries. Failures of such safety-critical embedded systems may cause high costs or even endanger human beings. Also for non-safety-critical applications, a software failure may necessitate expensive updates. Making sure that an application is working properly means addressing many different aspects. Development standards like DO-178B, IEC 61508 and the new revisions DO-178C, or ISO 26262 require to identify potential functional and non-functional hazards and to demonstrate that the software does not violate the relevant safety goals.For ensuring functional program properties automatic or model-based testing, and formal techniques like model checking become more and more widely used. For non-functional properties identifying a safe end-of-test criterion is a hard problem since failures usually occur in corner cases and full test coverage cannot be achieved. For some nonfunctional program properties this problem is solved by abstract interpretationbased static analysis techniques which provide full coverage and yield provably correct results. In this article we focus on static analyses of worst-case execution time, stack consumption, and runtime errors, which are increasingly adopted by industry in the validation and certification process for safety-critical software. We explain the underlying methodology and identify criteria for their successful application. The integration of static analyzers in the development process requires interfaces to other development tools, like code generators or scheduling tools. Using them for certification requires an appropriate tool qualification. We will address each of these topics and report on industrial experience.},
booktitle = {Proceedings of the 29th International Conference on Computer Safety, Reliability, and Security},
pages = {196–209},
numpages = {14},
location = {Vienna, Austria},
series = {SAFECOMP'10}
}

@article{10.1007/s11219-012-9193-8,
author = {Mets\"{a}, Jani and Maoz, Shahar and Katara, Mika and Mikkonen, Tommi},
title = {Using aspects for testing of embedded software: experiences from two industrial case studies},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9193-8},
doi = {10.1007/s11219-012-9193-8},
abstract = {Aspect-oriented software testing is emerging as an important alternative to conventional procedural and object-oriented testing techniques. This paper reports experiences from two case studies where aspects were used for the testing of embedded software in the context of an industrial application. In the first study, we used code-level aspects for testing non-functional properties. The methodology we used for deriving test aspect code was based on translating high-level requirements into test objectives, which were then implemented using test aspects in AspectC++. In the second study, we used high-level visual scenario-based models for the test specification, test generation, and aspect-based test execution. To specify scenario-based tests, we used a UML2-compliant variant of live sequence charts. To automatically generate test code from the models, a modified version of the S2A Compiler, outputting AspectC++ code, was used. Finally, to examine the results of the tests, we used the Tracer, a prototype tool for model-based trace visualization and exploration. The results of the two case studies show that aspects offer benefits over conventional techniques in the context of testing embedded software; these benefits are discussed in detail. Finally, towards the end of the paper, we also discuss the lessons learned, including the technological and other barriers to the future successful use of aspects in the testing of embedded software in industry.},
journal = {Software Quality Journal},
month = jun,
pages = {185–213},
numpages = {29},
keywords = {Aspect-oriented programming, Case studies, Embedded software, Software testing}
}

@article{10.1007/s11219-011-9146-7,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {A systematic review of quality attributes and measures for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9146-7},
doi = {10.1007/s11219-011-9146-7},
abstract = {It is widely accepted that software measures provide an appropriate mechanism for understanding, monitoring, controlling, and predicting the quality of software development projects. In software product lines (SPL), quality is even more important than in a single software product since, owing to systematic reuse, a fault or an inadequate design decision could be propagated to several products in the family. Over the last few years, a great number of quality attributes and measures for assessing the quality of SPL have been reported in literature. However, no studies summarizing the current knowledge about them exist. This paper presents a systematic literature review with the objective of identifying and interpreting all the available studies from 1996 to 2010 that present quality attributes and/or measures for SPL. These attributes and measures have been classified using a set of criteria that includes the life cycle phase in which the measures are applied; the corresponding quality characteristics; their support for specific SPL characteristics (e.g., variability, compositionality); the procedure used to validate the measures, etc. We found 165 measures related to 97 different quality attributes. The results of the review indicated that 92% of the measures evaluate attributes that are related to maintainability. In addition, 67% of the measures are used during the design phase of Domain Engineering, and 56% are applied to evaluate the product line architecture. However, only 25% of them have been empirically validated. In conclusion, the results provide a global vision of the state of the research within this area in order to help researchers in detecting weaknesses, directing research efforts, and identifying new research lines. In particular, there is a need for new measures with which to evaluate both the quality of the artifacts produced during the entire SPL life cycle and other quality characteristics. There is also a need for more validation (both theoretical and empirical) of existing measures. In addition, our results may be useful as a reference guide for practitioners to assist them in the selection or the adaptation of existing measures for evaluating their software product lines.},
journal = {Software Quality Journal},
month = sep,
pages = {425–486},
numpages = {62},
keywords = {Measures, Quality, Quality attributes, Software product lines, Systematic literature review}
}

@inproceedings{10.5555/648033.744227,
author = {Anastasopoulos, Michalis and Atkinson, Colin and Muthig, Dirk},
title = {A Concrete Method for Developing and Applying Product Line Architectures},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software development organizations are often deterred from introducing product line architectures by the lack of simple, ready-touse methods for developing and applying them. The well-known, published product-line-engineering methods tend to focus on the early stages of the software life cycle and address product line issues at a high-level of abstraction. Connecting product-line concepts with established implementation technologies is thus largely left to the user.This paper introduces a method, known as the KobrA method, which addresses this problem by enabling product line concerns to be coupled with regular (nonproduct line) architectural artifacts, and thus introduced incrementally. By explaining how the method can be understood as a concrete instantiation of the well-established PuLSE-DSSA product-line architecture approach, the paper clarifies the product line features of the KobrA method and illustrates how they can be used in tandem with established, general-purpose product line methods.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {294–312},
numpages = {19},
series = {NODe '02}
}

@inproceedings{10.1145/3205651.3208239,
author = {Escobar, Juan Jos\'{e} and Ortega, Julio and D\'{\i}az, Antonio Francisco and Gonz\'{a}lez, Jes\'{u}s and Damas, Miguel},
title = {Multi-objective feature selection for EEG classification with multi-level parallelism on heterogeneous CPU-GPU clusters},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208239},
doi = {10.1145/3205651.3208239},
abstract = {The present trend in the development of computer architectures that offer improvements in both performance and energy efficiency has provided clusters with interconnected nodes including multiple multi-core microprocessors and accelerators. In these so-called heterogeneous computers, the applications can take advantage of different parallelism levels according to the characteristics of the architectures in the platform. Thus, the applications should be properly programmed to reach good efficiencies, not only with respect to the achieved speedups but also taking into account the issues related to energy consumption. In this paper we provide a multi-objective evolutionary algorithm for feature selection in electroencephalogram (EEG) classification, which can take advantage of parallelism at multiple levels: among the CPU-GPU nodes interconnected in the cluster (through message-passing), and inside these nodes (through shared-memory thread-level parallelism in the CPU cores, and data-level parallelism and thread-level parallelism in the GPU). The procedure has been experimentally evaluated in performance and energy consumption and shows statistically significant benefits for feature selection: speedups of up to 73 requiring only a 6% of the energy consumed by the sequential code.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1862–1869},
numpages = {8},
keywords = {EEG multi-objective feature selection, distributed master-worker procedure, energy-aware computing, heterogeneous platform, parallel programming, subpopulation-based genetic algorithm},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1109/MiSE.2019.00018,
author = {Sch\"{o}ttle, Matthias and Kienzle, J\"{o}rg},
title = {On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: the case for signature extension},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00018},
doi = {10.1109/MiSE.2019.00018},
abstract = {Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {71–77},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}

@article{10.1016/j.jnca.2018.03.021,
title = {Resource management in cellular base stations powered by renewable energy sources},
year = {2018},
issue_date = {June 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {112},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2018.03.021},
doi = {10.1016/j.jnca.2018.03.021},
abstract = {This paper aims to consolidate the work carried out in making base station (BS) green and energy efficient by integrating renewable energy sources (RES). Clean and green technologies are mandatory for reduction of carbon footprint in future cellular networks. RES, especially solar and wind, are emerging as a viable alternate to fossil fuel based energy, which is the main cause of climate pollution. With advances in technologies, renewable energy is making inroads into all sectors including information and communication technologies (ICT). The main contributors of energy consumption in ICT sector are data centers' and cellular networks'. In cellular networks the BS is the main consumer of energy, mostly powered by the utility and a diesel generator. This energy comes at a significant operating cost as well as the environmental cost in terms of harmful greenhouse gas (GHG) emissions. Recent research shows that powering BSs with renewable energy is technically feasible. Although installation cost of energy from non-renewable fuel is still lower than RES, optimized use of the two sources can yield the best results. This paper presents a comprehensive overview of resource management in cellular BSs powered by RES and an in-depth analysis of power consumption optimization in order to reduce both cost and GHGs. Renewable energy sources are not only feasible for a stand-alone or off-grid BSs, but also feasible for on-grid BSs. This paper covers different aspects of optimization in cellular networks to provide reader with a holistic view of concepts, directions, and advancements in renewable energy based systems incorporated in cellular communications. Energy management strategies are studied in the realm of smart grids and other technologies, increasing the possibilities for energy efficiency further by employing schemes such as energy cooperation. Finally, the paper supports the move towards green communication in order to contribute positively towards climate change.},
journal = {J. Netw. Comput. Appl.},
month = jun,
pages = {1–17},
numpages = {17}
}

@inproceedings{10.1145/2809695.2809718,
author = {Stisen, Allan and Blunck, Henrik and Bhattacharya, Sourav and Prentow, Thor Siiger and Kj\ae{}rgaard, Mikkel Baun and Dey, Anind and Sonne, Tobias and Jensen, Mads M\o{}ller},
title = {Smart Devices are Different: Assessing and MitigatingMobile Sensing Heterogeneities for Activity Recognition},
year = {2015},
isbn = {9781450336314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809695.2809718},
doi = {10.1145/2809695.2809718},
abstract = {The widespread presence of motion sensors on users' personal mobile devices has spawned a growing research interest in human activity recognition (HAR). However, when deployed at a large-scale, e.g., on multiple devices, the performance of a HAR system is often significantly lower than in reported research results. This is due to variations in training and test device hardware and their operating system characteristics among others. In this paper, we systematically investigate sensor-, device- and workload-specific heterogeneities using 36 smartphones and smartwatches, consisting of 13 different device models from four manufacturers. Furthermore, we conduct experiments with nine users and investigate popular feature representation and classification techniques in HAR research. Our results indicate that on-device sensor and sensor handling heterogeneities impair HAR performances significantly. Moreover, the impairments vary significantly across devices and depends on the type of recognition technique used. We systematically evaluate the effect of mobile sensing heterogeneities on HAR and propose a novel clustering-based mitigation technique suitable for large-scale deployment of HAR, where heterogeneity of devices and their usage scenarios are intrinsic.},
booktitle = {Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems},
pages = {127–140},
numpages = {14},
keywords = {activity recognition, mobile sensing},
location = {Seoul, South Korea},
series = {SenSys '15}
}

@inproceedings{10.1145/2866614.2866616,
author = {Schulze, Sandro and Schulze, Michael and Ryssel, Uwe and Seidl, Christoph},
title = {Aligning Coevolving Artifacts Between Software Product Lines and Products},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866616},
doi = {10.1145/2866614.2866616},
abstract = {Software product lines (SPLs) play a pivotal role for developing a vast amount of related programs efficiently and with high quality. To this end, the SPL engineering process is separated into two levels: domain engineering (DE), which captures variability and development artifacts of the entire SPL, and application engineering (AE), which encompasses a variant-specific subset of the aforementioned artifacts. In the industrial practice of evolving an SPL, it is common that evolution is performed on both levels, which may affect the same artifacts (e.g., code, models) in different ways due to changes on the product line (DE) and the variant level (AE). As a result, conflicts may arise that have to be solved properly to guarantee correctness and validity of the affected artifacts. In this paper, we propose a methodology for resolving such conflicts to ensure correctness and consistency among artifacts while minimizing manual effort. Our method is comprehensive in two ways: First, we consider all kinds of artifacts (code and non-code) that may be subject to evolutionary changes in both DE and AE. Second, we also take into account that changing one particular artifact (e.g., a requirement) may require further changes to other artifacts of the same level. This way, our method reflects common industrial practices in SPL development and, thus, provides benefits for efficiently evolving real-world SPLs.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {9–16},
numpages = {8},
keywords = {SPL engineering, software evolution},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/974044.974052,
author = {Grassi, Vincenzo and Mirandola, Raffaela},
title = {Towards automatic compositional performance analysis of component-based systems},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974052},
doi = {10.1145/974044.974052},
abstract = {To make predictive analysis an effective tool for component-based software development (CBSD), it should be, as much as possible: compositional, to allow the re-use of known information about the properties of existing components, and automatic, to keep the pace with the timeliness and cost-effectiveness promises of CBSD. Towards this end, focusing on the predictive analysis of performance properties, we define a simple language, based on an abstract component model, to describe a component assembly, outlining which information should be included in it to support compositional performance analysis. Moreover, we outline a mapping of the constructs of the proposed language to elements of the RT-UML Profile, to give them a precisely defined "performance semantics", and to get a starting point for the exploitation of proposed UML-based methodologies and algorithms for performance analysis.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {59–63},
numpages = {5},
keywords = {component specification, performance, predictive analysis, software component},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@article{10.1147/sj.432.0384,
author = {Budinsky, F. and DeCandio, G. and Earle, R. and Francis, T. and Jones, J. and Li, J. and Nally, M. and Nelin, C. and Popescu, V. and Rich, S. and Ryman, A. and Wilson, T.},
title = {WebSphere Studio overview},
year = {2004},
issue_date = {April 2004},
publisher = {IBM Corp.},
address = {USA},
volume = {43},
number = {2},
issn = {0018-8670},
url = {https://doi.org/10.1147/sj.432.0384},
doi = {10.1147/sj.432.0384},
abstract = {In this paper we provide an overview of IBM WebSphere Studio, a family of tools for developing distributed applications for J2EE™ servers for state-of-the-art information technology systems. In today's business environment such systems are complex, comprise multiple platforms, and make use of a wide range of technologies and standards. Through a representative development scenario we illustrate the way WebSphere Studio satisfies the challenging requirements for a modern integrated development environment. The scenario covers a variety of technologies and standards, including database access, Web services standards, Enterprise JavaBeans™ implementation, integrated application testing, Web page design, and performance optimization. We also describe the Eclipse Modeling Framework, the open source technology base on which WebSphere Studio is built.},
journal = {IBM Syst. J.},
month = apr,
pages = {384–419},
numpages = {36}
}

@inproceedings{10.1145/1555349.1555351,
author = {Lelarge, Marc},
title = {Efficient control of epidemics over random networks},
year = {2009},
isbn = {9781605585116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1555349.1555351},
doi = {10.1145/1555349.1555351},
abstract = {Motivated by the modeling of the spread of viruses or epidemics with coordination among agents, we introduce a new model generalizing both the basic contact model and the bootstrap percolation. We analyze this percolated threshold model when the underlying network is a random graph with fixed degree distribution. Our main results unify many results in the random graphs literature. In particular, we provide a necessary and sufficient condition under which a single node can trigger a large cascade. Then we quantify the possible impact of an attacker against a degree based vaccination and an acquaintance vaccination. We define a security metric allowing to compare the different vaccinations. The acquaintance vaccination requires no knowledge of the node degrees or any other global information and is shown to be much more efficient than the uniform vaccination in all cases.},
booktitle = {Proceedings of the Eleventh International Joint Conference on Measurement and Modeling of Computer Systems},
pages = {1–12},
numpages = {12},
keywords = {epidemics, random graphs, vaccination},
location = {Seattle, WA, USA},
series = {SIGMETRICS '09}
}

@article{10.1147/JRD.2010.2050539,
author = {Molloy, C. and Iqbal, M.},
title = {Improving data-center efficiency for a smarter planet},
year = {2010},
issue_date = {July 2010},
publisher = {IBM Corp.},
address = {USA},
volume = {54},
number = {4},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2010.2050539},
doi = {10.1147/JRD.2010.2050539},
abstract = {In 2009, IBM launched its Smarter Planet™ initiative, which is based on the paradigm that virtually any physical object, process, or system can be instrumented, interconnected, and infused with intelligence. Because of the increased demand for information technology (IT) to facilitate Smarter Planet solutions, physical data centers have become interconnected, instrumented, and intelligent (i.e., adaptable, scalable, energy efficient, and cost effective). "Green data centers," which are discussed in this paper, are those that make use of facilities and IT integration, resulting in lower energy costs, reduced carbon footprint, and reduced demand for power, space, and cooling resources. This paper reviews energy-efficiency strategies that are being incorporated in eight million square feet of data-center space that IBM operates in support of its customers. These strategies include both a dynamic infrastructure IT initiative and an energy-efficiencies pillar of the IBM New Enterprise Data Center initiative. For example, compared to nonmodular designs, implementation of a modular design to optimize expandable data centers will better match the IT demand with the data-center energy supply. This high-level overview describes the integration of these strategies to create a class of leading-edge IBM data centers and a plan for the optimization of existing data centers.},
journal = {IBM J. Res. Dev.},
month = jul,
pages = {388–395},
numpages = {8}
}

@inproceedings{10.1145/2809695.2817877,
author = {Komatsu, Takanori and Akita, Jun-ichi},
title = {Poster: Power Spectrum Analysis of Reflected Waves with Ultrasonic Sensors Indicates "What the Target is"},
year = {2015},
isbn = {9781450336314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809695.2817877},
doi = {10.1145/2809695.2817877},
abstract = {We consider that ultrasonic waves can be used not only for measuring the distance to a target but also for comprehending what the target is simultaneously. We assumed that the physical characteristics of a target object (especially, surface information) are embedded in the power spectrum structure of the ultrasonic waves reflected from the target. The results of our investigation to analyze the characteristics of the power spectrum structure of waves reflected from various kinds of target materials showed whether target objects are soft and deformable or hard and undeformable.},
booktitle = {Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems},
pages = {387–388},
numpages = {2},
keywords = {power spectrum, reflected waves, total harmonic distortion, ultrasonic},
location = {Seoul, South Korea},
series = {SenSys '15}
}

@article{10.1016/j.ipm.2009.09.001,
author = {Johnson Lim, Soon Chong and Liu, Ying and Lee, Wing Bun},
title = {Multi-facet product information search and retrieval using semantically annotated product family ontology},
year = {2010},
issue_date = {July, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {46},
number = {4},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2009.09.001},
doi = {10.1016/j.ipm.2009.09.001},
abstract = {With the advent of various services and applications of Semantic Web, semantic annotation has emerged as an important research topic. The application of semantically annotated ontology had been evident in numerous information processing and retrieval tasks. One of such tasks is utilizing the semantically annotated ontology in product design which is able to suggest many important applications that are critical to aid various design related tasks. However, ontology development in design engineering remains a time consuming and tedious task that demands considerable human efforts. In the context of product family design, management of different product information that features efficient indexing, update, navigation, search and retrieval across product families is both desirable and challenging. For instance, an efficient way of retrieving timely information on product family can be useful for tasks such as product family redesign and new product variant derivation when requirements change. However, the current research and application of information search and navigation in product family is mostly limited to its structural aspect which is insufficient to handle advanced information search especially when the query targets at multiple aspects of a product. This paper attempts to address this problem by proposing an information search and retrieval framework based on the semantically annotated multi-facet product family ontology. Particularly, we propose a document profile (DP) model to suggest semantic tags for annotation purpose. Using a case study of digital camera families, we illustrate how the faceted search and retrieval of product information can be accomplished. We also exemplify how we can derive new product variants based on the designer's query of requirements via the faceted search and retrieval of product family information. Lastly, in order to highlight the value of our current work, we briefly discuss some further research and applications in design decision support, e.g. commonality analysis and variety comparison, based on the semantically annotated multi-facet product family ontology.},
journal = {Inf. Process. Manage.},
month = jul,
pages = {479–493},
numpages = {15},
keywords = {Information management and retrieval, Multi-facet, Ontology, Product family, Semantic annotation}
}

@inproceedings{10.5555/2820656.2820658,
author = {Lago, Patricia},
title = {Challenges and opportunities for sustainable software},
year = {2015},
publisher = {IEEE Press},
abstract = {With the increasing role played by software in supporting our society, its sustainability and environmental impact have become major factors in the development and operation of software-intensive systems. Myths and beliefs hide the real truth behind Green IT: IT is energy-inefficient because software is developed to make it so -- intentionally or not. But how far are we from being able to control software energy-efficiency? What makes software greener? How can we transform measuring software energy consumption in a general practice? What architectural design decisions will result in more sustainable systems? How can we ensure that new-generation software will be both cloud-ready and environmental-friendly? and How can we make evident the economic and social impact of developing software with 'energy in mind'? These are a few of the challenges ahead for a more sustainable digital society. This talk will discuss them, hence drawing directions for exciting challenges, promising opportunities, and ultimately inspiring research.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {1–2},
numpages = {2},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/1452520.1452554,
author = {Kanuparthy, Partha and Dovrolis, Constantine and Ammar, Mostafa},
title = {Spectral probing, crosstalk and frequency multiplexing in internet paths},
year = {2008},
isbn = {9781605583341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1452520.1452554},
doi = {10.1145/1452520.1452554},
abstract = {We present an end-to-end active probing methodology that creates frequency-domain signals in IP network paths. The signals are generated by periodic packet trains that cause short-lived queueing delay spikes. Different probers can be multiplexed in the frequency-domain on the same path. Further, a signal that is introduced by a "prober" in one path can cause a crosstalk effect, inducing a signal of the same frequency into another path (the "sampler") as long as the two paths share one or more bottleneck queues. Applications of the proposed methodology include the detection of shared store-and-forward devices among two or more paths, the creation of covert channels, and the modulation of voice or video periodic packet streams in less noisy frequencies. In this paper we focus on the first application. Our goal is to detect shared bottleneck(s) between a "sampler" and one or more "prober" paths. We present a spectral probing methodology as well as the corresponding signal processing/detection process. The accuracy of the method has been evaluated with controlled and repeatable simulation experiments, and it has also been tested on some Internet paths.},
booktitle = {Proceedings of the 8th ACM SIGCOMM Conference on Internet Measurement},
pages = {291–304},
numpages = {14},
keywords = {active probing, distributed agents, fourier transform, frequency multiplexing, network management, signal processing},
location = {Vouliagmeni, Greece},
series = {IMC '08}
}

@article{10.1016/j.jss.2019.02.026,
author = {Butting, Arvid and Eikermann, Robert and Kautz, Oliver and Rumpe, Bernhard and Wortmann, Andreas},
title = {Systematic composition of independent language features},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.026},
doi = {10.1016/j.jss.2019.02.026},
journal = {J. Syst. Softw.},
month = jun,
pages = {50–69},
numpages = {20}
}

@inproceedings{10.1145/2536714.2536722,
author = {Bouillet, Eric and Chen, Bei and Cooper, Chris and Dahlem, Dominik and Verscheure, Olivier},
title = {Fusing Traffic Sensor Data for Real-time Road Conditions},
year = {2013},
isbn = {9781450324304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536714.2536722},
doi = {10.1145/2536714.2536722},
abstract = {Transport authorities have been deploying and utilising sensor infrastructures in order to improve upon the level of transport-related services within cities. As existing resources are more and more constrained, novel means of utilising the data originating from these sensors are sought. Advanced IT infrastructures enable real-time processing use cases, such as travel time estimations along defined corridors. One major challenge is to deploy general purpose machine learning algorithms that are able to learn relationships between the covariates and a defined response variable. We introduce a data fusion approach using generalised additive models (GAM) to estimate journey times online in a real-time streaming platform. We experiments with bluetooth sensors and weather information to improve the estimation of journey times along a defined A3 corridor in south-central London. Our approach is able to continuously improve the journey time estimation as new (high-frequency) data becomes available. Our fusion platform also generalises to be able to process more data sources and it scales to city-wide deployments. This way, existing legacy sensor deployments can be utilised for novel value-added services and investments into infrastructural sensor deployments can be assessed in a data-driven way and on a needs basis.},
booktitle = {Proceedings of First International Workshop on Sensing and Big Data Mining},
pages = {1–6},
numpages = {6},
keywords = {Transport, big data, data fusion, sensor},
location = {Roma, Italy},
series = {SENSEMINE'13}
}

@inproceedings{10.1145/2749469.2750397,
author = {Akin, Berkin and Franchetti, Franz and Hoe, James C.},
title = {Data reorganization in memory using 3D-stacked DRAM},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750397},
doi = {10.1145/2749469.2750397},
abstract = {In this paper we focus on common data reorganization operations such as shuffle, pack/unpack, swap, transpose, and layout transformations. Although these operations simply relocate the data in the memory, they are costly on conventional systems mainly due to inefficient access patterns, limited data reuse and roundtrip data traversal throughout the memory hierarchy. This paper presents a two pronged approach for efficient data reorganization, which combines (i) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM, and (ii) a mathematical framework that is used to represent and optimize the reorganization operations.We evaluate our proposed system through two major use cases. First, we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads. Then, we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package. We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs. For the various test cases, in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {131–143},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@proceedings{10.1145/2304736,
title = {CBSE '12: Proceedings of the 15th ACM SIGSOFT symposium on Component Based Software Engineering},
year = {2012},
isbn = {9781450313452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 15th ACM SIGSOFT Symposium on Component Based Software Engineering (CBSE 2012) marks a milestone in the research on using components to build software systems in an efficient way. Over the years, this symposium has established a track record of bringing together researchers and practitioners from a variety of disciplines to promote a better understanding of CBSE from diverse perspectives, and to engage in active discussion and debate.Component-based software engineering continues to attract interest and evolve as a discipline for the rapid assembly of flexible software systems. CBSE combines elements of software requirements engineering, architecture, design, verification, testing, configuration and deployment. The role of and need for CBSE in industrial application remains critical.New trends in global services, distributed systems architectures, dynamically adaptable systems, and large-scale software systems often cross organizational boundaries and push the limits of established component-based methods, tools, and platforms. Innovative solutions from diverse paradigms (e.g., service-, aspect-, and agent-oriented) are needed to address these emerging trends. Topics of interest for CBSE 2012 therefore include, but are not limited to, the following:  Specification, architecture, and design of component models and component-based systems Software quality assurance for component-based engineering Verification, testing and certification of component-based systems Component composition, binding, and dynamic adaptation Component-based engineering with agents, aspects, or services Component-based product line engineering Non-functional properties (quality of service attributes) in component-based engineering Patterns and frameworks for component-based engineering Tools and methods for component-based engineering Industrial experience using component-based software development Empirical studies in component-based software engineering Teaching component-based software engineeringIn addition to the above, this year we have a special theme: Components for Achieving Long-Lived Systems. Many industrial systems have very strict requirements for uninterrupted operation. There are examples of systems that have aimed to provide continuous operation for more than 15 years (that is, since the first CBSE!). Such requirements place significant demands on the underlying architecture, mandating that the architecture be very well understood and carefully designed. In turn, the architecture, if implemented correctly, forms a foundation for achieving critical quality attributes such as dependability, robustness, usability, and flexibility. The principles of component-based software engineering offer a promise for achieving effective architectures for long-lived systems. This is especially so since this approach natively provides the ability to add, remove, replace, and/or modify components during operation. A related class of approaches deals with self-management in component-based systems in order to ensure continuous operation.CBSE 2012 received 50 submissions, each of which received at least three independent reviews by the CBSE Program Committee. The careful review process included an on-line discussion, after which 11 papers (22%) have been accepted for publication in this volume as full papers. Eleven more submissions have been selected as short papers. We have assembled an exciting program that shows that this is still a very vibrant and relevant community. We hope to see you at CBSE 2012 in Bertinoro!This year, CBSE is again part of the federated event CompArch, together with "QoSA 2012: 8th International ACM SIGSOFT Conference on the Quality of Software Architectures," "ISARCS 2012: 3rd International ACM SIGSOFT Symposium on Architecting Critical Systems," "WCOP 2012: 17th International Doctoral Symposium on Components and Architecture," and "ROSS 2012: Workshop on Reusing Open-Source Components."},
location = {Bertinoro, Italy}
}

@article{10.1145/2853073.2853095,
author = {Alebrahim, Azadeh and Fa\ss{}bender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta and Zdun, Uwe},
title = {Variability for Qualities in Software Architecture},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853095},
doi = {10.1145/2853073.2853095},
abstract = {Variability is a key factor of most systems. While there are many works covering variability in functionality, there is a research gap regarding variability in software qualities. There is an obvious imbalance between the importance of variability in the context of quality attributes, and the intensity of research in this area. To improve this situation, the First International Workshop on VAri- ability for QUalIties in SofTware Architecture (VAQUITA) was held jointly with ECSA 2015 in Cavtat/Dubrovnik, Croatia as a one-day workshop. The goal of VAQUITA was to investigate and stimulate the discourse about the matter of variability, qualities, and software architectures. The workshop featured three research paper presentations, one keynote talk, and two working group discussions. In this workshop report, we summarize the keynote talk and the presented papers. Additionally, we present the results of the working group discussions},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {32–35},
numpages = {4},
keywords = {Software architecture, quality attributes, variability}
}

@inproceedings{10.1145/974044.974089,
author = {Wu, Xiuping and Woodside, Murray},
title = {Performance modeling from software components},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974089},
doi = {10.1145/974044.974089},
abstract = {When software products are assembled from pre-defined components, performance prediction should be based on the components also. This supports rapid model-building, using previously calibrated sub-models or "performance components", in sync with the construction of the product. The specification of a performance component must be tied closely to the software component specification, but it also includes performance related parameters (describing workload characteristics and demands), and it abstracts the behaviour of the component in various ways (for reasons related to practical factors in performance analysis). A useful set of abstractions and parameters are already defined for layered performance modeling. This work extends them to accommodate software components, using a new XML-based language called Component-Based Modeling Language (CBML). With CBML, compatible components can be inserted into slots provided in a hierarchical component specification based on the UML component model.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {290–301},
numpages = {12},
keywords = {CBML, LQN, generative programming, layered queue model, performance prediction, software component, software performance, submodel},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@article{10.1016/j.infsof.2012.11.001,
author = {Schmid, Klaus and De Almeida, Eduardo Santana and Kishi, Tomoji},
title = {Editorial: Guest Editors' Introduction: Special Issue on Software Reuse and Product Lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.001},
doi = {10.1016/j.infsof.2012.11.001},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {489–490},
numpages = {2}
}

@inproceedings{10.1145/2228360.2228568,
author = {Melpignano, Diego and Benini, Luca and Flamand, Eric and Jego, Bruno and Lepley, Thierry and Haugou, Germain and Clermidy, Fabien and Dutoit, Denis},
title = {Platform 2012, a many-core computing accelerator for embedded SoCs: performance evaluation of visual analytics applications},
year = {2012},
isbn = {9781450311991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2228360.2228568},
doi = {10.1145/2228360.2228568},
abstract = {P2012 is an area- and power-efficient many-core computing accelerator based on multiple globally asynchronous, locally synchronous processor clusters. Each cluster features up to 16 processors with independent instruction streams sharing a multi-banked one-cycle access L1 data memory, a multi-channel DMA engine and specialized hardware for synchronization and aggressive power management. P2012 is 3D stacking ready and can be customized to achieve extreme area and energy efficiency by adding domain-specific HW IPs to the cluster. The first P2012 SoC prototype in 28nm CMOS will sample in Q3, featuring four 16-processor clusters, a 1MB L2 memory and delivering 80GOPS (with 32 bit single precision floating point support) in 18mm2 with 2W power consumption (worst-case). P2012 can run standard OpenCL™ and proprietary Native Programming Model SW components to achieve the highest level of control on application-to-resource mapping. A dedicated version of the OpenCV vision library is provided in the P2012 SW Development Kit to enable visual analytics acceleration. This paper will discuss preliminary performance measurements of common feature extraction and tracking algorithms, parallelized on P2012, versus sequential execution on ARM CPUs.},
booktitle = {Proceedings of the 49th Annual Design Automation Conference},
pages = {1137–1142},
numpages = {6},
keywords = {3D stacking, SoC, computer vision, feature extraction, low-power, many-core, process aware},
location = {San Francisco, California},
series = {DAC '12}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {evolutionary algorithms, genetic improvement, genetic programming, software product lines, variability},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1147/rd.492.0437,
author = {Lorenz, J. and Kral, S. and Franchetti, F. and Ueberhuber, C. W.},
title = {Vectorization techniques for the Blue Gene/L double FPU},
year = {2005},
issue_date = {March 2005},
publisher = {IBM Corp.},
address = {USA},
volume = {49},
number = {2},
issn = {0018-8646},
url = {https://doi.org/10.1147/rd.492.0437},
doi = {10.1147/rd.492.0437},
abstract = {This paper presents vectorization techniques tailored to meet the specifics of the two-way single-instruction multiple-data (SIMD) double-precision floating-point unit (FPU), which is a core element of the node application-specific integrated circuit (ASIC) chips of the IBM 360-teraflops Blue Gene®/L supercomputer. This paper focuses on the general-purpose basic-block vectorization and optimization methods as they are incorporated in the Vienna MAP vectorizer and optimizer. The innovative technologies presented here, which have consistently delivered superior performance and portability across a wide range of platforms, were carried over to prototypes of Blue Gene/L and joined with the automatic performance-tuning system known as Fastest Fourier Transform in the West (FFTW). FFTW performance-optimization facilities working with the compiler technologies presented in this paper are able to produce vectorized fast Fourier transform (FFT) codes that are tuned automatically to single Blue Gene/L processors and are up to 80% faster than the best-performing scalar FFT codes generated by FFTW.},
journal = {IBM J. Res. Dev.},
month = mar,
pages = {437–446},
numpages = {10}
}

@article{10.1007/s10664-019-09761-2,
author = {Xiang, Yi and Yang, Xiaowei and Zhou, Yuren and Zheng, Zibin and Li, Miqing and Huang, Han},
title = {Going deeper with optimal software products selection using many-objective optimization and satisfiability solvers},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09761-2},
doi = {10.1007/s10664-019-09761-2},
abstract = {In search-based software engineering, one actively studied problem is the optimal software product selection from a feature model using multiple (usually more than three) optimization objectives simultaneously. This can be represented as a many-objective optimization problem. The primary goal of solving this problem is to search for diverse and high-quality valid products as rapidly as possible. Previous studies have shown that combining search-based techniques with satisfiability (SAT) solvers was promising for achieving this goal, but it remained open that how different solvers affect the performance of a search algorithm, and that whether the ways to randomize solutions in the solvers make a difference. Moreover, we may need further investigation on the necessity of mixing different types of SAT solving techniques. In this paper, we address the above open research questions by performing a series of empirical studies on 21 features models, most of which are reverse-engineered from industrial software product lines. We examine four conflict-driven clause learning solvers, two stochastic local search solvers, and two different ways to randomize solutions. Experimental results suggest that the performance can be indeed affected by different SAT solvers, and by the ways to randomize solutions in the solvers. This study serves as a practical guideline for choosing and tuning SAT solvers for the many-objective optimal software product selection problem.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {591–626},
numpages = {36},
keywords = {Search-based software engineering, Feature model, Many-objective optimization, Optimal software product selection, Satisfiability solvers}
}

@article{10.1007/s00165-017-0441-3,
author = {Str\"{u}ber, D. and Rubin, J. and Arendt, T. and Chechik, M. and Taentzer, G. and Pl\"{o}ger, J.},
title = {Variability-based model transformation: formal foundation and application},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0441-3},
doi = {10.1007/s00165-017-0441-3},
abstract = {Model transformation systems often contain transformation rules that are substantially similar to each other, causing maintenance issues and performance bottlenecks. To address these issues, we introduce variability-based model transformation. The key idea is to encode a set of similar rules into a compact representation, called variability-based rule. We provide an algorithm for applying such rules in an efficient manner. In addition, we introduce rule merging, a three-component mechanism for enabling the automatic creation of variability-based rules. Our rule application and merging mechanisms are supported by a novel formal framework, using category theory to provide precise definitions and to prove correctness. In two realistic application scenarios, the created variability-based rules enabled considerable speedups, while also allowing the overall specifications to become more compact.},
journal = {Form. Asp. Comput.},
month = jan,
pages = {133–162},
numpages = {30},
keywords = {Model transformation, Graph transformation, Variability, Category theory}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {attack scenarios, docker-container, exploit, feature model, software architecture, variability model, vulnerability},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.sysarc.2019.02.012,
author = {Brings, Jennifer and Daun, Marian and Bandyszak, Torsten and Stricker, Vanessa and Weyer, Thorsten and Mirzaei, Elham and Neumann, Martin and Zernickel, Jan Stefan},
title = {Model-based documentation of dynamicity constraints for collaborative cyber-physical system architectures: Findings from an industrial case study},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {97},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2019.02.012},
doi = {10.1016/j.sysarc.2019.02.012},
journal = {J. Syst. Archit.},
month = aug,
pages = {153–167},
numpages = {15},
keywords = {Cardinality-based feature models, Cyber-physical systems, Exploratory case study, CPS network, Dynamic morphology, Dynamicity constraints}
}

@inproceedings{10.1145/3194760.3194761,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Integrating reuse into the rapid, continuous software engineering cycle through test-driven search},
year = {2018},
isbn = {9781450357456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194760.3194761},
doi = {10.1145/3194760.3194761},
abstract = {Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, "discontinuous" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.},
booktitle = {Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering},
pages = {8–11},
numpages = {4},
keywords = {rapid continuous code reuse, rapid continuous integration, test-driven development, test-driven reuse, test-driven search},
location = {Gothenburg, Sweden},
series = {RCoSE '18}
}

@inproceedings{10.1145/3302333.3302337,
author = {Al-Hajjaji, Mustafa and Ryssel, Uwe and Schulze, Michael},
title = {Validating Partial Configurations of Product Lines},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302337},
doi = {10.1145/3302333.3302337},
abstract = {Configuring a new variant of a product line is not always a one-time task. In some cases, many stakeholders are involved in the configuration process. This is needed for example, if different stakeholders are responsible for different parts of the product line and they are not allowed to see specific parts of the product line already configured in previous steps. Thus, a partial derivation can be performed, where a part of the configuration process can be done by some stakeholders, while finishing the rest of the configuration process can be achieved by others.Validating partial configurations is a challenging task, since the selection state of some features can still be open. In addition to these open selection states of features, values of attributes, calculations of expressions, as well as constraints are needed to be handled, as they can use information, which is not defined yet. Thus, a validator that ables to address the aforementioned challenges is required. In this paper, we discuss how the partial configurations can be validated considering different cases. While these discussed cases can be applied in general, we focus in this paper on applying them with respect to the industrial variant management tool pure::variants.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {4},
numpages = {6},
keywords = {Highly configurable systems, Multi-stage configuration, Partial configuration, Partial derivation, Product configuration, Software product lines},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1109/MICRO.2010.15,
author = {Watkins, Matthew A. and Albonesi, David H.},
title = {ReMAP: A Reconfigurable Heterogeneous Multicore Architecture},
year = {2010},
isbn = {9780769542997},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2010.15},
doi = {10.1109/MICRO.2010.15},
abstract = {This paper presents ReMAP, a reconfigurable architecture geared towards accelerating and parallelizing applications within a heterogeneous CMP. In ReMAP, threads share a common reconfigurable fabric that can be configured for individual thread computation or fine-grained communication with integrated computation. The architecture supports both fine-grained point-to-point communication for pipeline parallelization and fine-grained barrier synchronization. The combination of communication and configurable computation within ReMAP provides the unique ability to perform customized computation while data is transferred between cores, and to execute custom global functions after barrier synchronization. ReMAP demonstrates significantly higher performance and energy efficiency compared to hard-wired communication-only mechanisms, and over what can ideally be achieved by allocating the fabric area to additional or more powerful cores.},
booktitle = {Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {497–508},
numpages = {12},
series = {MICRO '43}
}

@inproceedings{10.1145/3401335.3401671,
author = {Ramautar, Vijanti},
title = {Model-Driven Ethical, Social and Environmental Accounting},
year = {2020},
isbn = {9781450375955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401335.3401671},
doi = {10.1145/3401335.3401671},
abstract = {Ethical, social and environmental accounting is the practice of assessing organisations' performances in sustainability and business ethics topics. The organisations typically publish the results in a sustainability or non-financial report. We aim at offering a novel perspective from which researchers investigate, practitioners apply and policy-makers regulate ethical, social and environmental accounting (ESEA). The large quantity of ESEA methods and tools causes managerial problems, affecting the identity of social enterprises and complicating policy making. We will develop a domain-specific modelling language to specify existing ESEA methods and capture the advantages of model-driven engineering. We will create a repository where method models can be stored. These models contain the data structure and configuration of the methods. We will also develop openESEA, a run-time model interpreter that automatically executes ESEA method models. We will offer features to allow organisations to tailor the methods to their needs, to support model management operations, and to compare existing methods to inform policy makers about their similarities and differences. This project combines expertise in information science and social entrepreneurship with the intention to pave the way to future research avenues in ESEA and, eventually, to profound changes towards a fair and sustainable economy.},
booktitle = {Proceedings of the 7th International Conference on ICT for Sustainability},
pages = {189–192},
numpages = {4},
keywords = {Model-driven engineering, domain-specific language, ethical, method engineering, social and environmental accounting, social entrepreneurship},
location = {Bristol, United Kingdom},
series = {ICT4S2020}
}

@inproceedings{10.1007/978-3-030-59762-7_3,
author = {Ali, Shaukat and Arcaini, Paolo and Yue, Tao},
title = {Do Quality Indicators Prefer Particular Multi-objective Search Algorithms in Search-Based Software Engineering?},
year = {2020},
isbn = {978-3-030-59761-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59762-7_3},
doi = {10.1007/978-3-030-59762-7_3},
abstract = {In Search-Based Software Engineering (SBSE), users typically select a set of Multi-Objective Search Algorithms (MOSAs) for their experiments without any justification, or they simply choose an MOSA because of its popularity (e.g., NSGA-II). On the other hand, users know certain characteristics of solutions they are interested in. Such characteristics are typically measured with Quality Indicators (QIs) that are commonly used to evaluate the quality of solutions produced by an MOSA. Consequently, these QIs are often employed to empirically evaluate a set of MOSAs for a particular search problem to find the best MOSA. Thus, to guide SBSE users in choosing an MOSA that represents the solutions measured by a specific QI they are interested in, we present an empirical evaluation with a set of SBSE problems to study the relationships among commonly used QIs and MOSAs in SBSE. Our aim, by studying such relationships, is to identify whether there are certain characteristics of a QI because of which it prefers a certain MOSA. Such preferences are then used to provide insights and suggestions to SBSE users in selecting an MOSA, given that they know which quality aspects of solutions they are looking for.},
booktitle = {Search-Based Software Engineering: 12th International Symposium, SSBSE 2020, Bari, Italy, October 7–8, 2020, Proceedings},
pages = {25–41},
numpages = {17},
keywords = {Search-based software engineering, Quality indicator, Multi-objective search algorithm},
location = {Bari, Italy}
}

@inproceedings{10.5555/2008503.2008518,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Bagheri, Ebrahim and Amyot, Daniel and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {Aspect-oriented feature models},
year = {2010},
isbn = {9783642212093},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Product Lines (SPLs) have emerged as a prominent approach for software reuse. SPLs are sets of software systems called families that are usually developed as a whole and share many common features. Feature models are most typically used as a means for capturing commonality and managing variability of the family. A particular product from the family is configured by selecting the desired features of that product. Typically, feature models are considered monolithic entities that do not support modularization well. As industrial feature models tend to be large, their modularization has become an important research topic lately. However, existing modularization approaches do not support modularization of crosscutting concerns. In this paper, we introduce Aspect-oriented Feature Models (AoFM) and argue that using aspect-oriented techniques improves the manageability and reduces the maintainability effort of feature models. Particularly, we advocate an asymmetric approach that allows for the modularization of basic and crosscutting concerns in feature models.},
booktitle = {Proceedings of the 2010 International Conference on Models in Software Engineering},
pages = {110–124},
numpages = {15},
keywords = {aspect-oriented modeling, feature models, software product lines},
location = {Oslo, Norway},
series = {MODELS'10}
}

@inproceedings{10.1007/978-3-030-10801-4_1,
author = {A\ss{}mann, Uwe and Grzelak, Dominik and Mey, Johannes and Pukhkaiev, Dmytro and Sch\"{o}ne, Ren\'{e} and Werner, Christopher and P\"{u}schel, Georg},
title = {Cross-Layer Adaptation in Multi-layer Autonomic Systems (Invited Talk)},
year = {2019},
isbn = {978-3-030-10800-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-10801-4_1},
doi = {10.1007/978-3-030-10801-4_1},
abstract = {This work presents a new reference architecture for multi-layer autonomic systems called context-controlled autonomic controllers (ConAC). Usually, the principle of multiple system layers contradicts the principle of a global adaptation strategy, because system layers are considered to be black boxes. The presented architecture relies on an explicit context model, so a simple change of contexts can consistently vary the adaptation strategies for all layers. This reveals that explicit context modeling enables consistent meta-adaptation in multi-layer autonomic systems. The paper presents two application areas for the ConAC architecture, robotic co-working and energy-adaptive servers, but many other multi-layered system designs should benefit from it.},
booktitle = {SOFSEM 2019: Theory and Practice of Computer Science: 45th International Conference on Current Trends in Theory and Practice of Computer Science, Nov\'{y} Smokovec, Slovakia, January 27-30, 2019, Proceedings},
pages = {1–20},
numpages = {20},
location = {Nov\'{y} Smokovec, Slovakia}
}

@inproceedings{10.1145/3001867.3001869,
author = {Schuster, Sven and Nieke, Michael and Schaefer, Ina},
title = {Name resolution strategies in variability realization languages for software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001869},
doi = {10.1145/3001867.3001869},
abstract = {Software Product Lines (SPLs) exploit reuse-in-the-large to enable customization by explicitly modeling commonalities and variabilities of closely related software systems. Different approaches exist to enable SPL development and product creation by using modular languages, such as Feature-Oriented Programming (FOP) (compositional) or Delta-Oriented Programming (DOP) (transformational). These languages incorporate, e.g., object-oriented languages, adding a layer of variability. Creating a variabilityaware Abstract Syntax Graph (ASG), i.e., an ASG that contains the complete variability of the SPL, facilitates family-based analyses and is essential for supporting developers during SPL development. To this end, name resolution has to be performed. However, name resolution for these languages is a challenge as multiple declarations for the same element may occur in different modules. In this paper, we propose four name resolution strategies for compositional and transformational SPL realization languages and discuss their benefits and drawbacks, categorized by relevant application scenarios of the ASG.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {11–17},
numpages = {7},
keywords = {Abstract Syntax Graph, Delta-Oriented Programming, Feature-Oriented Programming, Software Product Lines, name resolution},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Quality attributes, Service-based systems, Systematic literature review, Variability}
}

@article{10.1007/s10270-018-00704-x,
author = {Rodrigues, V\'{\i}tor and Donetti, Simone and Damiani, Ferruccio},
title = {Certifying delta-oriented programs},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00704-x},
doi = {10.1007/s10270-018-00704-x},
abstract = {A major design concern in modern software development frameworks is to ensure that mechanisms for updating code running on remote devices comply with given safety specifications. This paper presents a delta-oriented approach for implementing product lines where software reuse is achieved at the three levels of state-diagram modeling, C/$$text {C}^{_{_{_{++}}}} $$C++source code and binary code. A safety specification is expressed on the properties of reusable software libraries that can be dynamically loaded at run time after an over-the-air update. The compilation of delta-engineered code is certified using the framework of proof-carrying code in order to guarantee safety of software updates on remote devices. An empirical evaluation of the computational cost associated with formal safety checks is done by means of experimentation.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {2875–2906},
numpages = {32},
keywords = {Delta-oriented programming, Model-driven development, Proof-carrying code, Runtime systems, Safety properties}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2485922.2485944,
author = {Wu, Lisa and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Navigating big data with high-throughput, energy-efficient data partitioning},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485944},
doi = {10.1145/2485922.2485944},
abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {249–260},
numpages = {12},
keywords = {accelerator, data partitioning, microarchitecture, specialized functional unit, streaming data},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1016/j.datak.2014.07.003,
author = {Bre\ss{}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
title = {Load-aware inter-co-processor parallelism in database query processing},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2014.07.003},
doi = {10.1016/j.datak.2014.07.003},
abstract = {For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. Contribute heuristics to enhance performance by exploiting inter-device parallelismHeuristics consider load and speed on (co-)processors.Extensive evaluation on four use cases: aggregation, selection, sort, and joinAssess the performance of best heuristic for systems with multiple co-processorsDiscuss how operator-stream-based scheduling can be used in a query processor},
journal = {Data Knowl. Eng.},
month = sep,
pages = {60–79},
numpages = {20},
keywords = {Co-processing, Query optimization, Query processing}
}

@inproceedings{10.1145/2532443.2532448,
author = {Zhao, Tianqi and Zhao, Haiyan and Zhang, Wei},
title = {A preliminary study on requirements modeling methods for self-adaptive software systems},
year = {2013},
isbn = {9781450323697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532443.2532448},
doi = {10.1145/2532443.2532448},
abstract = {Internetware denotes a kind of complex distributed software system, which executes in an open, uncertain and dynamic environment, and adapts itself to changes in the environment. An important problem related to the development of Internetware applications is how to define their requirements. Traditional requirements modeling methods work well with software applications deployed in predictable environment, but cannot deal with Internetware applications, which have to identify and adapt themselves to the unpredictable situations of their environment. The self-adaptation characteristic of Internetware applications introduces challenges to the effective modeling of the requirements of Internetware applications. In this paper, we carry out a preliminary study on requirements modeling methods for self-adaptive software systems. In particular, we focus on how existing requirements modeling methods address the challenges caused by self-adaptation and what are the advantages and disadvantages of their solutions. By doing this study, we aim to identify the essential capabilities or properties that a requirements modeling method should possess so as to support the requirements modeling of self-adaptive software systems like Internetware.},
booktitle = {Proceedings of the 5th Asia-Pacific Symposium on Internetware},
articleno = {3},
numpages = {10},
keywords = {requirement evolution, requirement modeling method, requirement representation, requirement verification, self-adaptive system},
location = {Changsha, China},
series = {Internetware '13}
}

@inproceedings{10.1145/2420942.2420943,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Ga\v{s}evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {The Fourth International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML2012)},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420943},
doi = {10.1145/2420942.2420943},
abstract = {The International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML) series traditionally takes place as part of the Satellite Events of the ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS). Traditionally, NFPinDSML gathers researchers and practitioners interested in the estimation and evaluation of system quality and their integration in Domain Specific Modeling Languages and Model Driven Engineering in general. This paper is the summary of the fourth NFPinDSML workshop which was affiliated with MODELS 2012.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {1},
numpages = {2},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1016/j.asoc.2016.08.030,
author = {Saeed, Aneesa and Ab Hamid, Siti Hafizah and Mustafa, Mumtaz Begum},
title = {The experimental applications of search-based techniques for model-based testing},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.030},
doi = {10.1016/j.asoc.2016.08.030},
abstract = {Graphical abstractDisplay Omitted HighlightsA systematic review of applications of search-based techniques for model-based testing is provided.Four taxonomies are proposed to classify the applications based on the purpose, problems, solutions and evaluations.The applications are analyzed based on the proposed taxonomies.The development of search-based techniques for model-based testing is discussed.Limitations and potential research directions are summarized. ContextModel-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. ObjectiveThe objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research. MethodWe conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. ResultsThe results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT. ConclusionThis extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1094–1117},
numpages = {24},
keywords = {Model-based testing, Search-based techniques, Software testing, Systematic literature review, Taxonomy, Test case generation}
}

@inproceedings{10.1145/3384419.3430772,
author = {Wang, Ziqi and Chen, Zhe and Singh, Akash Deep and Garcia, Luis and Luo, Jun and Srivastava, Mani B.},
title = {UWHear: through-wall extraction and separation of audio vibrations using wireless signals},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430772},
doi = {10.1145/3384419.3430772},
abstract = {An ability to detect, classify, and locate complex acoustic events can be a powerful tool to help smart systems build context-awareness, e.g., to make rich inferences about human behaviors in physical spaces. Conventional methods to measure acoustic signals employ microphones as sensors. As signals from multiple acoustic sources are blended during propagation to a sensor, such methods impose a dual challenge of separating the signal for an acoustic event from background noise and from other acoustic events of interest. Recent research has proposed using radio-frequency (RF) signals, e.g., Wi-Fi and millimeter-wave (mmWave), to sense sound directly from source vibrations. Whereas these works allow separating an acoustic event from background noise, they cannot monitor multiple sound sources simultaneously. In this paper, we present UWHear, a system that simultaneously recovers and separates sounds from multiple sources. Unlike previous works using continuous-wave RF, UWHear employs Impulse Radio Ultra-Wideband (IR-UWB) technology, in order to construct an enhanced audio sensing system tackling the above challenges. Further, IR-UWB radios can penetrate light building materials, which enables UWHear to operate in some non-line-of-sight (NLOS) conditions. In addition to providing a theoretical guarantee for audio recovery using RF pulses, we also implement an audio sensing prototype exploiting a commercial-off-the-shelf IR-UWB radar. Our experiments show that UWHear can effectively separate the content of two speakers that are placed only 25cm apart. UWHear can also capture and separate multiple sounds and vibrations of household appliances while being immune to non-target noise coming from other directions.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {1–14},
numpages = {14},
keywords = {IR-UWB radar, RF sensing, audio sensing, wireless vibrometry},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Search-based Software Testing, Hyper-heuristics, Systematic Mapping, Evolutionary Algorithms, Genetic Algorithms, Meta-heuristics}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Dynamic Software Product Lines, Reconfiguration Decisions, Timed Automata},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1016/j.future.2019.04.032,
author = {Mousa, Afaf and Bentahar, Jamal and Alam, Omar},
title = {Context-aware composite SaaS using feature model},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {99},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.032},
doi = {10.1016/j.future.2019.04.032},
journal = {Future Gener. Comput. Syst.},
month = oct,
pages = {376–390},
numpages = {15}
}

@inbook{10.5555/1985668.1985673,
author = {Kazhamiakin, Raman and Benbernou, Salima and Baresi, Luciano and Plebani, Pierluigi and Uhlig, Maike and Barais, Olivier},
title = {Adaptation of service-based systems},
year = {2010},
isbn = {3642175988},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Service Research Challenges and Solutions for the Future Internet: S-Cube - towards Engineering, Managing and Adapting Service-Based Systems},
pages = {117–156},
numpages = {40}
}

@inproceedings{10.1007/978-3-642-41533-3_37,
author = {Alam, Omar and Kienzle, J\"{o}rg and Mussbacher, Gunter},
title = {Concern-Oriented Software Design},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_37},
doi = {10.1007/978-3-642-41533-3_37},
abstract = {There exist many solutions to solve a given design problem, and it is difficult to capture the essence of a solution and make it reusable for future designs. Furthermore, many variations of a given solution may exist, and choosing the best alternative depends on application-specific high-level goals and non-functional requirements. This paper proposes Concern-Oriented Software Design, a modelling technique that focuses on concerns as units of reuse. A concern groups related models serving the same purpose, and provides three interfaces to facilitate reuse. The variation interface presents the design alternatives and their impact on non-functional requirements. The customization interface of the selected alternative details how to adapt the generic solution to a specific context. Finally, the usage interface specifies the provided behaviour. We illustrate our approach by presenting the concern models of variations of the Observer design pattern, which internally depends on the Association concern to link observers and subjects.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {604–621},
numpages = {18}
}

@inproceedings{10.1145/2593735.2593739,
author = {Kugele, Stefan and Pucea, Gheorghe},
title = {Model-based optimization of automotive E/E-architectures},
year = {2014},
isbn = {9781450328470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593735.2593739},
doi = {10.1145/2593735.2593739},
abstract = {In this paper we present a generic framework to enable constraint-based automotive E/E-architecture optimization using a domain-specific language. The quality of today's automotive E/E-architectures is highly influenced by the mapping of software to executing hardware components: the so-called deployment problem. First, we introduce a holistic architectural model facilitating a seamless model-based development from requirements management to deployment, which is the focus of this work. Second, we introduce our domain-specific constraint and optimization language AAOL (Automotive Architecture Optimization Language) capable to express a wide range of deployment-relevant problems. Third, we present a generic, i.e., solver-independent framework currently supporting multi-objective evolutionary algorithms (MOEA). We investigate the feasibility of the approach by dint of a case study taken from the literature.},
booktitle = {Proceedings of the 6th International Workshop on Constraints in Software Testing, Verification, and Analysis},
pages = {18–29},
numpages = {12},
keywords = {Model-based optimization, automotive E/E-architecture, constraint satisfaction problem, domain-specific languages},
location = {Hyderabad, India},
series = {CSTVA 2014}
}

@inproceedings{10.1145/1028664.1028675,
author = {Spinczyk, Olaf and Beuche, Danilo},
title = {Modeling and building software product lines with eclipse},
year = {2004},
isbn = {1581138334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1028664.1028675},
doi = {10.1145/1028664.1028675},
booktitle = {Companion to the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
pages = {18–19},
numpages = {2},
keywords = {embedded systems, model-driven software development, software product lines, variant management},
location = {Vancouver, BC, CANADA},
series = {OOPSLA '04}
}

@article{10.1145/101320.1045580,
author = {Vance, R. E.},
title = {Article review: A message-based approach to discrete event simulation by R. L. Bagrodia, K. M. Chandy, and J. Misra. IEEE Trans. Softw. Eng. SE-13, 6 (June 1987)},
year = {1990},
issue_date = {Aug. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/101320.1045580},
doi = {10.1145/101320.1045580},
abstract = {As a service to our readers, PER has reached an agreement to reprint reviews of books and papers on simulation and modeling that originally appeared in ACM Computing Reviews. CR is a monthly journal that publishes critical reviews on a broad range of computing subjects including simulation and modeling. As an ACM member, you can subscribe to CR by writing to ACM Headquarters.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = sep,
pages = {27},
numpages = {1}
}

@inproceedings{10.1145/3023956.3023963,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Heymans, Patrick},
title = {Yo variability! JHipster: a playground for web-apps analyses},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023963},
doi = {10.1145/3023956.3023963},
abstract = {Though variability is everywhere, there has always been a shortage of publicly available cases for assessing variability-aware tools and techniques as well as supports for teaching variability-related concepts. Historical software product lines contains industrial secrets their owners do not want to disclose to a wide audience. The open source community contributed to large-scale cases such as Eclipse, Linux kernels, or web-based plugin systems (Drupal, WordPress). To assess accuracy of sampling and prediction approaches (bugs, performance), a case where all products can be enumerated is desirable. As configuration issues do not lie within only one place but are scattered across technologies and assets, a case exposing such diversity is an additional asset. To this end, we present in this paper our efforts in building an explicit product line on top of JHipster, an industrial open-source Web-app configurator that is both manageable in terms of configurations (≈ 163,000) and diverse in terms of technologies used. We present our efforts in building a variability-aware chain on top of JHipster's configurator and lessons learned using it as a teaching case at the University of Rennes. We also sketch the diversity of analyses that can be performed with our infrastructure as well as early issues found using it. Our long term goal is both to support students and researchers studying variability analysis and JHipster developers in the maintenance and evolution of their tools.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {44–51},
numpages = {8},
keywords = {case study, variability-related analyses, web-apps},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.4018/IJCSSA.2015070103,
author = {Kolagari, Ramin Tavakoli and Chen, DeJiu and Lanusse, Agnes and Librino, Renato and L\"{o}nn, Henrik and Mahmud, Nidhal and Mraidha, Chokri and Reiser, Mark-Oliver and Torchiaro, Sandra and Tucci-Piergiovanni, Sara and W\"{a}gemann, Tobias and Yakymets, Nataliya},
title = {Model-Based Analysis and Engineering of Automotive Architectures with EAST-ADL: Revisited},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {2166-7292},
url = {https://doi.org/10.4018/IJCSSA.2015070103},
doi = {10.4018/IJCSSA.2015070103},
abstract = {Modern cars have turned into complex high-technology products, subject to strict safety and timing requirements, in a short time span. This evolution has translated into development processes that are not as efficient, flexible and agile as they could or should be. This paper presents the main aspects and capabilities of a rich model-based design framework, founded on EAST-ADL. EAST-ADL is an architecture description language specific to the automotive domain and complemented by a methodology compliant with the functional safety standard for the automotive domain ISO26262. The language and the methodology are used to develop an information model in the sense of a conceptual model, providing the engineer the basis for specifying the various aspects of the system. Inconsistencies, redundancies, and partly even missing system description aspects can be found automaticlally by advanced analyses and optimization capabilities to effectively improve development processes of modern cars.},
journal = {Int. J. Concept. Struct. Smart Appl.},
month = jul,
pages = {25–70},
numpages = {46},
keywords = {AUTOSAR, Automotive Software Development, Dependability, EAST-ADL, Functional Safety, ISO 26262, Model-Based Software Development, Optimization, Software Product Lines, Timing Modelling}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Delaying of Decisions, Model Interfaces, Model Reuse, Model-Driven Engineering, Reuse Hierarchies},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1109/ICSE.2019.00090,
author = {Lillack, Max and St\u{a}nciulescu, \c{S}tefan and Hedman, Wilhelm and Berger, Thorsten and W\k{a}sowski, Andrzej},
title = {Intention-based integration of software variants},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00090},
doi = {10.1109/ICSE.2019.00090},
abstract = {Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional software merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants.In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions---domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the proposed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {831–842},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/2162049.2162071,
author = {Cardoso, Jo\~{a}o M.P. and Carvalho, Tiago and Coutinho, Jos\'{e} G.F. and Luk, Wayne and Nobre, Ricardo and Diniz, Pedro and Petrov, Zlatko},
title = {LARA: an aspect-oriented programming language for embedded systems},
year = {2012},
isbn = {9781450310925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2162049.2162071},
doi = {10.1145/2162049.2162071},
abstract = {The development of applications for high-performance embedded systems is typically a long and error-prone process. In addition to the required functions, developers must consider various and often conflicting non-functional application requirements such as performance and energy efficiency. The complexity of this process is exacerbated by the multitude of target architectures and the associated retargetable mapping tools. This paper introduces an As-pect-Oriented Programming (AOP) approach that conveys domain knowledge and non-functional requirements to optimizers and mapping tools. We describe a novel AOP language, LARA, which allows the specification of compi-lation strategies to enable efficient generation of software code and hardware cores for alternative target architectures. We illustrate the use of LARA for code instrumentation and analysis, and for guiding the application of compiler and hardware synthesis optimizations. An important LARA feature is its capability to deal with different join points, action models, and attributes, and to generate an aspect intermediate representation. We present examples of our aspect-oriented hardware/software design flow for mapping real-life application codes to embedded platforms based on Field Programmable Gate Array (FPGA) technology.},
booktitle = {Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development},
pages = {179–190},
numpages = {12},
keywords = {FPGAs, aspect-oriented programming, compilers, domain-specific languages, embedded systems, reconfigurable computing},
location = {Potsdam, Germany},
series = {AOSD '12}
}

@article{10.1016/j.jss.2018.07.014,
author = {Makki, Majid and Van Landuyt, Dimitri and Lagaisse, Bert and Joosen, Wouter},
title = {A comparative study of workflow customization strategies: Quality implications for multi-tenant SaaS},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.014},
doi = {10.1016/j.jss.2018.07.014},
journal = {J. Syst. Softw.},
month = oct,
pages = {423–438},
numpages = {16},
keywords = {Multi-tenancy, Software-as-a-Service, Workflow automation, Functional customization, Software quality}
}

@article{10.1017/S0890060404040053,
author = {Hofer, Adrian P. and Halman, Johannes I. M.},
title = {Complex products and systems: Potential from using layout platforms},
year = {2004},
issue_date = {January 2004},
publisher = {Cambridge University Press},
address = {USA},
volume = {18},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060404040053},
doi = {10.1017/S0890060404040053},
abstract = {In their quest to manage the complexity of offering greater product variety, firms in many industries are considering platform-based development of product families. Key in this approach is the sharing of components, modules, and other assets across a family of products. Current research indicates that companies are often choosing physical elements of the product architecture (i.e., components, modules, building blocks) for building platform-based product families. Other sources for platform potential are widely neglected. We argue that for complex products and systems with hierarchic product architectures and considerable freedom in design, a new platform type, the system layout, offers important commonality potential. This layout platform standardizes the arrangement of subsystems within the product family. This paper is based on three industry case studies, where a product family based on a common layout could be defined. In combination with segment-specific variety restrictions, this results in an effective, efficient, and flexible positioning of a company's products. The employment of layout platforms leads to substantial complexity reduction, and is the basis for competitive advantage, as it imposes a dominant design on a product family, improves its configurability, and supports effective market segmentation.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {55–69},
numpages = {15},
keywords = {Complex Products and Systems, Layout Platform, Platform Concept, Product Families}
}

@inproceedings{10.1145/2742647.2742658,
author = {Zhang, Li and Pathak, Parth H. and Wu, Muchen and Zhao, Yixin and Mohapatra, Prasant},
title = {AccelWord: Energy Efficient Hotword Detection through Accelerometer},
year = {2015},
isbn = {9781450334945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742647.2742658},
doi = {10.1145/2742647.2742658},
abstract = {Voice control has emerged as a popular method for interacting with smart-devices such as smartphones, smartwatches etc. Popular voice control applications like Siri and Google Now are already used by a large number of smartphone and tablet users. A major challenge in designing a voice control application is that it requires continuous monitoring of user?s voice input through the microphone. Such applications utilize hotwords such as "Okay Google" or "Hi Galaxy" allowing them to distinguish user?s voice command and her other conversations. A voice control application has to continuously listen for hotwords which significantly increases the energy consumption of the smart-devices.To address this energy efficiency problem of voice control, we present AccelWord in this paper. AccelWord is based on the empirical evidence that accelerometer sensors found in today?s mobile devices are sensitive to user?s voice. We also demonstrate that the effect of user?s voice on accelerometer data is rich enough so that it can be used to detect the hotwords spoken by the user. To achieve the goal of low energy cost but high detection accuracy, we combat multiple challenges, e.g. how to extract unique signatures of user?s speaking hotwords only from accelerometer data and how to reduce the interference caused by user?s mobility.We finally implement AccelWord as a standalone application running on Android devices. Comprehensive tests show AccelWord has hotword detection accuracy of 85% in static scenarios and 80% in mobile scenarios. Compared to the microphone based hotword detection applications such as Google Now and Samsung S Voice, AccelWord is 2 times more energy efficient while achieving the accuracy of 98% and 92% in static and mobile scenarios respectively.},
booktitle = {Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {301–315},
numpages = {15},
keywords = {accelerometer, accelword, energy, hotword detection, measurement},
location = {Florence, Italy},
series = {MobiSys '15}
}

@article{10.1016/j.scico.2019.102344,
author = {Basile, Davide and ter Beek, Maurice H. and Degano, Pierpaolo and Legay, Axel and Ferrari, Gian-Luigi and Gnesi, Stefania and Di Giandomenico, Felicita},
title = {Controller synthesis of service contracts with variability},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {187},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2019.102344},
doi = {10.1016/j.scico.2019.102344},
journal = {Sci. Comput. Program.},
month = feb,
numpages = {23},
keywords = {Supervisory control theory, Contract automata, Service orchestrations, Variability, Behavioural variability}
}

@inproceedings{10.1145/3426182.3426187,
author = {Kloibhofer, Sebastian and Pointhuber, Thomas and Heisinger, Maximilian and M\"{o}ssenb\"{o}ck, Hanspeter and Stadler, Lukas and Leopoldseder, David},
title = {SymJEx: symbolic execution on the GraalVM},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426182.3426187},
doi = {10.1145/3426182.3426187},
abstract = {Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.},
booktitle = {Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
pages = {63–72},
numpages = {10},
keywords = {Compiler optimizations, GraalVM, Java, Symbolic execution},
location = {Virtual, UK},
series = {MPLR '20}
}

@inproceedings{10.1145/3239235.3239240,
author = {Aljarallah, Sulaiman and Lock, Russell},
title = {An exploratory study of software sustainability dimensions and characteristics: end user perspectives in the kingdom of Saudi Arabia (KSA)},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239240},
doi = {10.1145/3239235.3239240},
abstract = {Background: Sustainability has become an important topic globally and the focus on ICT sustainability is increasing. However, issues exist, including vagueness and complexity of the concept itself, in addition to immaturity of the Software Engineering (SE) field. Aims: The study surveys respondents on software sustainability dimensions and characteristics from their perspectives, and seeks to derive rankings for their priority. Method: An exploratory study was conducted to quantitatively investigate Saudi Arabian (KSA) software user's perceptions with regard to the concept itself, the dimensions and characteristics of the software sustainability. Survey data was gathered from 906 respondents. Results: The results highlight key dimensions for sustainability and their priorities to users. The results also indicate that the characteristics perceived to be the most significant, were security, usability, reliability, maintainability, extensibility and portability, whereas respondents were relatively less concerned with computer ethics (e.g. privacy and trust), functionality, efficiency and reusability. A key finding was that females considered the environmental dimension to be more important than males. Conclusions: The dimensions and characteristics identified here can be used as a means of providing valuable feedback for the planning and implementation of future development of sustainable software.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {14},
numpages = {10},
keywords = {empirical study, software sustainability, sustainability dimensions},
location = {Oulu, Finland},
series = {ESEM '18}
}

@book{10.5555/1875377,
author = {Gai, Silvano and Salli, Tommi and Andersson, Roger},
title = {Cisco Unified Computing System (UCS) (Data Center): A Complete Reference Guide to the Cisco Data Center Virtualization Server Architecture},
year = {2010},
isbn = {1587141930},
publisher = {Cisco Press},
edition = {1st},
abstract = {The definitive guide to UCS and the Cisco Data Center Server: planning, architecture, components, deployment, and benefits With its new Unified Computing System (UCS) family of products, Cisco has introduced a fundamentally new vision for data center computing: one that reduces ownership cost, improves agility, and radically simplifies management. In this book, three Cisco insiders thoroughly explain UCS, and offer practical insights for IT professionals and decision-makers who are evaluating or implementing it. The authors establish the context for UCS by discussing the implications of virtualization, unified I/O, large memories and other key technologies, and showing how trends like cloud computing and green IT will drive the next-generation data center. Next, they take a closer look at the evolution of server CPU, memory, and I/O subsystems, covering advances such as the Intel XEON 5500, 5600, 7500, DDR3 memory, and unified I/O over 10 Gbps Ethernet. Building on these fundamentals, the authors then discuss UCS in detail, showing how it systematically overcomes key limitations of current data center environments. They review UCS features, components, and architecture, and demonstrate how it can improve data center performance, reliability, simplicity, flexibility, and energy efficiency. Along the way, they offer realistic planning, installation, and migration guidance: everything decision-makers and technical implementers need to gain maximum value from UCSnow, and for years to come. Silvano Gai has spent 11 years as Cisco Fellow, architecting Catalyst, MDS, and Nexus switches. He has written several books on networking, written multiple Internet Drafts and RFCs, and is responsible for 80 patents and applications. He teaches a course on this books topics at Stanford University. Tommi Salli, Cisco Technical Marketing Engineer, has nearly 20 years of experience with servers and applications at Cisco, Sun, VERITAS, and Nuova Systems. Roger Andersson, Cisco Manager, Technical Marketing, spent more than 12 years in the CLARiiON Engineering Division at EMC, and 5 years as Technical Product Manager at VERITAS/Symantec. He is now focused on Cisco UCS system management. Streamline data centers with UCS to systematically reduce cost of ownership Eliminate unnecessary server componentsand their setup, management, power, cooling, and cabling Use UCS to scale service delivery, simplify service movement, and improve agility Review the latest advances in processor, memory, I/O, and virtualization architectures for data center servers Understand the specific technical advantages of UCS Integrate UCS 6100 Fabric Interconnect, Cisco UCS 2100 Series Fabric Extenders, UCS 5100 Series Blade Server Enclosures, UCS B-Series Blade Servers, UCS C-Series Rack Servers, and UCS Adapters Use Cisco UCS Manager to manage all Cisco UCS components as a single, seamless entity Integrate third-party management tools from companies like BMC, CA, EMC, IBM, Microsoft, and VMware Practice all this with a copy of Cisco Unified Computing System Platform Emulator Lite (UCSPE Lite) on the DVD in the back of the book This book is part of the Networking Technology Series from Cisco Press, which offers networking professionals valuable information for constructing efficient networks, understanding new technologies, and building successful careers.}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Multi product line, deductive verification, method contracts},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@article{10.1007/s10270-017-0625-6,
author = {Str\"{u}ber, Daniel and Acre?Oaie, Vlad and Pl\"{o}ger, Jennifer},
title = {Model clone detection for rule-based model transformation languages},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0625-6},
doi = {10.1007/s10270-017-0625-6},
abstract = {Cloning is a convenient mechanism to enable reuse across and within software artifacts. On the downside, it is also a practice related to severe long-term maintainability impediments, thus generating a need to identify clones in affected artifacts. A large variety of clone detection techniques have been proposed for programming and modeling languages; yet no specific ones have emerged for model transformation languages. In this paper, we explore clone detection for rule-based model transformation languages, including graph-based ones, such as Henshin, and hybrid ones, such as ATL. We introduce use cases for such techniques in the context of constructive and analytical quality assurance, and a set of key requirements we derived from these use cases. To address these requirements, we describe our customization of existing model clone detection techniques: We consider eScan, an a-priori-based technique, ConQAT, a heuristic technique, and a hybrid technique based on a combination of eScan and ConQAT. We compare these techniques in a comprehensive experimental evaluation, based on three realistic Henshin rule sets, and a comprehensive body of examples from the ATL transformation zoo. Our results indicate that our customization of ConQAT enables the efficient detection of the considered clones, without sacrificing accuracy. With our contributions, we present the first evidence on the usefulness of model clone detection for the quality assurance of model transformations and pave the way for future research efforts at the intersection of model clone detection and model transformation.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {995–1016},
numpages = {22},
keywords = {ATL, Henshin, Model clone detection, Model transformation, Quality assurance}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Product lines, Feature model, Product-line configuration, Recommender systems, Personalized recommendations}
}

@inproceedings{10.1145/3299874.3319338,
author = {Adegbija, Tosiron and Lysecky, Roman and Kumar, Vinu Vijay},
title = {Right-Provisioned IoT Edge Computing: An Overview},
year = {2019},
isbn = {9781450362528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299874.3319338},
doi = {10.1145/3299874.3319338},
abstract = {Edge computing on the Internet of Things (IoT) is an increasingly popular paradigm in which computation is moved closer to the data source (i.e., edge devices). Edge computing mitigates the overheads of cloud-based computing arising from increased response time, communication bandwidth, data security and privacy, energy consumption, etc. However, given the potentially stringent resource constraints and functional requirements of emerging IoT devices, edge computing must neither be over- or under-provisioned for its stated purpose. In this paper, we present an overview of the problem of right-provisioned IoT edge computing, wherein IoT devices are equipped with resources that are 'just enough,' even when 'just enough' may not be clearly defined at design time. We highlight a few research directions and key challenges that must be addressed to enable right-provisioned IoT edge computing.},
booktitle = {Proceedings of the 2019 Great Lakes Symposium on VLSI},
pages = {531–536},
numpages = {6},
keywords = {adaptable computing, computation migration, edge computing, high-level synthesis, internet of things, iot security., low-power embedded systems, memory hierarchy, right-provisioned},
location = {Tysons Corner, VA, USA},
series = {GLSVLSI '19}
}

@inproceedings{10.1145/1182807.1182815,
author = {Girod, Lewis and Lukac, Martin and Trifa, Vlad and Estrin, Deborah},
title = {The design and implementation of a self-calibrating distributed acoustic sensing platform},
year = {2006},
isbn = {1595933433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1182807.1182815},
doi = {10.1145/1182807.1182815},
abstract = {We present the design, implementation, and evaluation of the Acoustic Embedded Networked Sensing Box (ENSBox), a platform for prototyping rapid-deployable distributed acoustic sensing systems, particularly distributed source localization. Each ENSBox integrates an ARM processor running Linux and supports key facilities required for source localization: a sensor array, wireless network services, time synchronization, and precise self-calibration of array position and orientation. The ENSBox's integrated, high precision self-calibration facility sets it apart from other platforms. This self-calibration is precise enough to support acoustic source localization applications in complex, realistic environments: e.g., 5 cm average 2D position error and 1.5 degree average orientation error over a partially obstructed 80x50 m outdoor area. Further, our integration of array orientation into the position estimation algorithm is a novel extension of traditional multilateration techniques. We present the result of several different test deployments, measuring the performance of the system in urban settings, as well as forested, hilly environments with obstructing foliage and 20-30 m distances between neighboring nodes.},
booktitle = {Proceedings of the 4th International Conference on Embedded Networked Sensor Systems},
pages = {71–84},
numpages = {14},
keywords = {distributed acoustic sensing, self-localization},
location = {Boulder, Colorado, USA},
series = {SenSys '06}
}

@inproceedings{10.1145/1028613.1028618,
author = {Gilani, Wasif and Naqvi, Nabeel Hasan and Spinczyk, Olaf},
title = {On adaptable middleware product lines},
year = {2004},
isbn = {1581139497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1028613.1028618},
doi = {10.1145/1028613.1028618},
abstract = {Middleware helps to manage the complexity and heterogeneity inherent in distributed systems. Traditional middleware has a monolithic architecture, which makes it difficult to adapt to special requirements such as those present in embedded applications. Middleware for small devices has to cope with a broad range of requirements as well as with the stringent resource constraints. In this paper we propose a family-based approach based on aspect-oriented programming (AOP) for the implementation of middleware product lines which are highly configurable and adaptable.Such an adaptable middleware is statically configured according to the requirements of the specific distributed application. Furthermore, the middleware is also capable of adapting to the dynamics of the distributed embedded system by dynamically reconfiguring itself during runtime. An efficient dynamic aspect weaver is needed for this kind of adaptability. We also discuss a family of dynamic weavers that complements our study of the family based middleware.},
booktitle = {Proceedings of the 3rd Workshop on Adaptive and Reflective Middleware},
pages = {207–213},
numpages = {7},
keywords = {adaptable middleware, aspect-oriented programming, dynamic reconfiguration, dynamic weaving, middleware product lines},
location = {Toronto, Ontario, Canada},
series = {ARM '04}
}

@inproceedings{10.1007/978-3-030-95391-1_37,
author = {Zheng, Xuda and Zhang, Chi and Duan, Keqiang and Wu, Weiguo and Yan, Jie},
title = {SLA: A Cache Algorithm for&nbsp;SSD-SMR Storage System with&nbsp;Minimum RMWs},
year = {2021},
isbn = {978-3-030-95390-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95391-1_37},
doi = {10.1007/978-3-030-95391-1_37},
abstract = {To satisfy the low-cost and massive data storage requirements imposed by data growth, Shingled Magnetic Recording (SMR) disks are extensively employed in the area of high-density storage. The primary drawback of SMR disks is the write amplification issue caused by sequential write constraints, which is becoming more prominent in the field of non-cold archives. Although a hybrid storage system comprised of Solid State Disks (SSDs) and SMRs may alleviate the aforementioned issue, current SSD cache replacement algorithms are still limited to the management method of Least Recently Used (LRU). The LRU queue, on the other hand, is ineffective in reducing the triggers of Read-Modify-Write (RMW), which is a critical factor for the performance of SMR disks. In this paper, we propose a new SMR Locality-Aware (SLA) algorithm based on a band-based management method. SLA adopts the Dual Locality Compare (DLC) strategy to solve the hit rate reduction problem caused by the traditional band-based management method, as well as the Relatively Clean Band First (RCBF) strategy to further minimize the number of RMW operations. Experiments indicate that, compared to the MSOT method, the SLA algorithm can maintain a similar hit rate as the LRU, while reducing the number of RMWs by 77.2% and the SMR disk write time by 95.1%.},
booktitle = {Algorithms and Architectures for Parallel Processing: 21st International Conference, ICA3PP 2021, Virtual Event, December 3–5, 2021, Proceedings, Part III},
pages = {587–601},
numpages = {15},
keywords = {Shingled Magnetic Recording, Replacement algorithm, Hybrid storage system, Spatial locality}
}

@inproceedings{10.5555/1987684.1987695,
author = {Juszczyk, Lukasz and Schall, Daniel and Mietzner, Ralph and Dustdar, Schahram and Leymann, Frank},
title = {CAGE: customizable large-scale SOA testbeds in the cloud},
year = {2010},
isbn = {9783642193934},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Large-scale and complex distributed systems are increasingly implemented as SOAs. These comprise diverse types of components, e.g., Web services, registries, workflow engines, and services buses, that interact with each others to establish composite functionality. The drawback of this trend is that testing of complex SOAs becomes a challenging task. During the development phase, testers must verify the system's correct functionality, but often do not have access to adequate testbeds. In this paper, we present an approach for solving this issue. We combine the Genesis2 testbed generator, that emulates SOA environments, with Cafe, a framework for provisioning of component-based applications in the cloud. Our approach allows to model large-scale service-based testbed infrastructures, to specify their behavior, and to deploy these automatically in the cloud. As a result, testers can emulate required environments on-demand for evaluating SOAs at runtime.},
booktitle = {Proceedings of the 2010 International Conference on Service-Oriented Computing},
pages = {76–87},
numpages = {12},
location = {San Francisco, CA},
series = {ICSOC'10}
}

@inproceedings{10.1145/223587.223611,
author = {Lebeck, Alvin R. and Wood, David A.},
title = {Active memory: a new abstraction for memory-system simulation},
year = {1995},
isbn = {0897916956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223587.223611},
doi = {10.1145/223587.223611},
abstract = {This paper describes the active memory abstraction for memory-system simulation. In this abstraction---designed specifically for on-the-fly simulation, memory references logically invoke a user-specified function depending upon the reference's type and accessed memory block state. Active memory allows simulator writers to specify the appropriate action on each reference, including "no action" for the common case of cache hits. Because the abstraction hides implementation details, implementations can be carefully tuned for particular platforms, permitting much more efficient on-the-fly simulation than the traditional trace-driven abstraction.Our SPARC implementation, Fast-Cache, executes simple data cache simulations two or three times faster than a highly-tuned trace-driven simulator and only 2 to 7 times slower than the original program. Fast-Cache implements active memory by performing a fast table look up of the memory block state, taking as few as 3 cycles on a SuperSPARC for the no-action case. Modeling the effects of Fast-Cache's additional lookup instructions qualitatively shows that Fast-Cache is likely to be the most efficient simulator for miss ratios between 3% and 40%.},
booktitle = {Proceedings of the 1995 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {220–230},
numpages = {11},
location = {Ottawa, Ontario, Canada},
series = {SIGMETRICS '95/PERFORMANCE '95}
}

@inproceedings{10.1145/3377024.3377036,
author = {Sprey, Joshua and Sundermann, Chico and Krieter, Sebastian and Nieke, Michael and Mauro, Jacopo and Th\"{u}m, Thomas and Schaefer, Ina},
title = {SMT-based variability analyses in FeatureIDE},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377036},
doi = {10.1145/3377024.3377036},
abstract = {Handling configurable systems with thousands of configuration options is a challenging problem in research and industry. One of the most common approaches to manage the configuration options of large systems is variability modelling. The verification and configuration process of large variability models is manually infeasible. Hence, they are usually assisted by automated analyses based on solving satisfiability problems (SAT). Recent advances in satisfiability modulo theories (SMT) could prove SMT solvers as a viable alternative to SAT solvers. However, SMT solvers are typically not utilized for variability analyses. A comparison for SAT and SMT could help to estimate SMT solvers potential for the automated analysis. We integrated two SMT solvers into FeatureIDE and compared them against a SAT solver on analyses for feature models, configurations, and realization artifacts. We give an overview of all variability analyses in FeatureIDE and present the results of our empirical evaluation for over 122 systems. We observed that SMT solvers are generally faster in generating explanations of unsatisfiable requests. However, the evaluated SAT solver outperformed SMT solvers for other analyses.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {9},
keywords = {SAT, SAT analysis, SAT vs SMT, SMT, SMT analysis, attribute optimization, configuration analysis, feature attributes, feature model analysis, feature models, preprocessor analysis, variability analysis},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.jss.2017.03.005,
author = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
title = {Automotive software engineering},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.005},
doi = {10.1016/j.jss.2017.03.005},
abstract = {A comprehensive survey of literature on Automotive Software Engineering (ASE).679 primary studies were identified, classified and analyzed with respect to five dimensions.Three most investigated areas include software architecture &amp; design, testing and reuse.ASE seems to have high industrial relevance but is relatively lower in its scientific rigor.Validation &amp; comparative studies are less represented and literature lacks practitioner-oriented guidelines. The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.},
journal = {J. Syst. Softw.},
month = jun,
pages = {25–55},
numpages = {31},
keywords = {Automotive software engineering, Automotive systems, Embedded systems, Literature survey, Software-intensive systems, Systematic mapping study}
}

@inproceedings{10.1145/1854273.1854284,
author = {Watkins, Matthew A. and Albonesi, David H.},
title = {Dynamically managed multithreaded reconfigurable architectures for chip multiprocessors},
year = {2010},
isbn = {9781450301787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1854273.1854284},
doi = {10.1145/1854273.1854284},
abstract = {Prior work has demonstrated that reconfigurable logic can significantly benefit certain applications. However, reconfigurable architectures have traditionally suffered from high area overhead and limited application coverage. We present a dynamically managed multithreaded reconfigurable architecture consisting of multiple clusters of shared reconfigurable fabrics that greatly reduces the area overhead of reconfigurability while still offering the same power efficiency and performance benefits. Like other shared SMT and CMP resources, the dynamic partitioning of the reconfigurable resource among sharing threads, along with the co-scheduling of threads among different reconfigurable clusters, must be intelligently managed for the full benefits of the shared fabrics to be realized.We propose a number of sophisticated dynamic management approaches, including the application of machine learning, multithreaded phase-based management, and stability detection. Overall, we show that, with our dynamic management policies, multithreaded reconfigurable fabrics can achieve better energy\texttimes{}delay2, at far less area and power, than providing each core with a much larger private fabric. Moreover, our approach achieves dramatically higher performance and energy-efficiency for particular workloads compared to what can be ideally achieved by allocating the fabric area to additional cores.},
booktitle = {Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
pages = {41–52},
numpages = {12},
keywords = {reconfigurable architecture, shared resource management},
location = {Vienna, Austria},
series = {PACT '10}
}

@article{10.1016/j.jss.2012.08.031,
author = {Kulesza, Uir\'{a} and Soares, S\'{e}Rgio and Chavez, Christina and Castor, Fernando and Borba, Paulo and Lucena, Carlos and Masiero, Paulo and Sant'Anna, Claudio and Ferrari, Fabiano and Alves, Vander and Coelho, Roberta and Figueiredo, Eduardo and Pires, Paulo F. and Delicato, Fl\'{a}Via and Piveta, Eduardo and Silva, Carla and Camargo, Valter and Braga, Rosana and Leite, Julio and Lemos, Ot\'{a}Vio and Mendon\c{c}A, Nabor and Batista, Thais and Bonif\'{a}Cio, Rodrigo and Cacho, N\'{e}Lio and Silva, Lyrene and Von Staa, Arndt and Silveira, F\'{a}Bio and Valente, Marco T\'{u}Lio and Alencar, Fernanda and Castro, Jaelson and Ramos, Ricardo and Penteado, Rosangela and Rubira, Cec\'{\i}Lia},
title = {The crosscutting impact of the AOSD Brazilian research community},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.08.031},
doi = {10.1016/j.jss.2012.08.031},
abstract = {Background: Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil. Aims: In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community. Method: Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations. Results: Our analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil.},
journal = {J. Syst. Softw.},
month = apr,
pages = {905–933},
numpages = {29},
keywords = {Aspect-Oriented Software Development, Modularity, Research impact}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Cloud computing, Cloud service selection, Decision-making}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {LATEX, constraint programming, generators, machine learning, technical writing, variability modelling},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1007/978-3-030-59003-1_10,
author = {G\'{o}mez, Paola and Casallas, Rubby and Roncancio, Claudia},
title = {Automatic Schema Generation for Document-Oriented Systems},
year = {2020},
isbn = {978-3-030-59002-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59003-1_10},
doi = {10.1007/978-3-030-59003-1_10},
abstract = {Popular document-oriented systems store JSON-like data (e.g. MongoDB). Such data formats combine the flexibility of semi-structured models and traditional data structures like records and arrays. This allows numerous structuring possibilities even for simple data. The data structure choice is important as it impacts many aspects such as memory footprint, data access performances and programming complexity. Our work aims at helping users in selecting data structuring from a set of automatically generated alternatives. These alternatives can be analyzed considering complexity metrics, query requirements and best practices using such “schemaless” databases. Our approach for “schema” generation has been inspired from Software Product Lines strategies based on feature models. From a UML class diagram that represents user’s data, we generate automatically a feature model that implicitly contains the structure alternatives with their variations and common points. This feature model satisfies document-oriented constraints so as user constraints reflecting good practices or particular needs. It leads to a set of data structuring alternatives to be considered by the user for his operational choices.},
booktitle = {Database and Expert Systems Applications: 31st International Conference, DEXA 2020, Bratislava, Slovakia, September 14–17, 2020, Proceedings, Part I},
pages = {152–163},
numpages = {12},
keywords = {NoSQL, Document-oriented systems, Variability, Feature models},
location = {Bratislava, Slovakia}
}

@inproceedings{10.1007/978-3-662-45234-9_25,
author = {Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Challenges in Modelling and Analyzing Quantitative Aspects of Bike-Sharing Systems},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_25},
doi = {10.1007/978-3-662-45234-9_25},
abstract = {Bike-sharing systems are becoming popular not only as a sustainable means of transportation in the urban environment, but also as a challenging case study that presents interesting run-time optimization problems. As a side-study within a research project aimed at quantitative analysis that used such a case study, we have observed how the deployed systems enjoy a wide variety of different features. We have therefore applied variability analysis to define a family of bike-sharing systems, and we have sought support in available tools. We have so established a tool chain that includes academic tools that provide different functionalities regarding the analysis of software product lines, from feature modelling to product derivation and from quantitative evaluation of the attributes of products to model checking value-passing modal specifications. The tool chain is currently experimented inside the mentioned project as a complement to more sophisticated product-based analysis techniques.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {351–367},
numpages = {17}
}

@inproceedings{10.1109/MEMCOD.2015.7340482,
title = {On the deployment problem of embedded systems},
year = {2015},
isbn = {9781509002375},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MEMCOD.2015.7340482},
doi = {10.1109/MEMCOD.2015.7340482},
abstract = {The quality of today's embedded systems e. g. in vehicles, airplanes, or automation plants is highly influenced by their architecture. In this context, we study the so-called deployment problem. The question is where (i. e., on which execution unit) to deploy which software application or which sensor/actuator shall be connected to which device in an automation plant. First, we introduce a domain-specific constraint and optimization language fitting the needs of our partners. Second, we investigate different approaches to tackle the deployment problem even for industrial size systems. Therefore, we present different solving strategies using (i) multi-objective evolutionary algorithms, (ii) SMT-based, and (iii) ILP-based solving approaches. Furthermore, a combination of the first two is used. We investigate the proposed methods and demonstrate their feasibility using two realistic systems: a civil flight control system (FCS), and a seawater desalination plant.},
booktitle = {Proceedings of the 2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign},
pages = {158–167},
numpages = {10},
series = {MEMOCODE '15}
}

@article{10.1007/s00607-013-0338-9,
author = {Bertolino, Antonia and Inverardi, Paola and Muccini, Henry},
title = {Software architecture-based analysis and testing: a look into achievements and future challenges},
year = {2013},
issue_date = {August    2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {95},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-013-0338-9},
doi = {10.1007/s00607-013-0338-9},
journal = {Computing},
month = aug,
pages = {633–648},
numpages = {16}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {combinatorial interaction testing, feature models, pairwise testing, software product lines},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@article{10.1007/s00450-014-0273-9,
author = {Goltz, Ursula and Reussner, Ralf H. and Goedicke, Michael and Hasselbring, Wilhelm and M\"{a}rtin, Lukas and Vogel-Heuser, Birgit},
title = {Design for future: managed software evolution},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3–4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-014-0273-9},
doi = {10.1007/s00450-014-0273-9},
abstract = {Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future--Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.},
journal = {Comput. Sci.},
month = aug,
pages = {321–331},
numpages = {11},
keywords = {Co-evolution, Design, Knowledge carrying software, Legacy systems, Software life cycle, maintenance and operation}
}

@inproceedings{10.5555/3297765.3297769,
author = {Trapp, Matthias and Pasewaldt, Sebastian and D\"{u}rschmid, Tobias and Semmo, Amir and D\"{o}llner, J\"{u}rgen},
title = {Teaching image-processing programming for mobile devices: a software development perspective},
year = {2018},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {In this paper we present a concept of a research course that teaches students in image processing as a building block of mobile applications. Our goal with this course is to teach theoretical foundations, practical skills in software development as well as scientific working principles to qualify graduates to start as fully-valued software developers or researchers. The course includes teaching and learning focused on the nature of small team research and development as encountered in the creative industries dealing with computer graphics, computer animation and game development. We discuss our curriculum design and issues in conducting undergraduate and graduate research that we have identified through four iterations of the course. Joint scientific demonstrations and publications of the students and their supervisors as well as quantitative and qualitative evaluation by students underline the success of the proposed concept. In particular, we observed that developing using a common software framework helps the students to jump start their course projects, while industry software processes such as branching coupled with a three-tier breakdown of project features helps them to structure and assess their progress.},
booktitle = {Proceedings of the 39th Annual European Association for Computer Graphics Conference: Education Papers},
pages = {17–24},
numpages = {8},
location = {Delft, The Netherlands},
series = {EG-EDU '18}
}

@article{10.1007/s10270-018-00712-x,
author = {Bencomo, Nelly and G\"{o}tz, Sebastian and Song, Hui},
title = {Models@run.time: a guided tour of the state of the art and research challenges},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00712-x},
doi = {10.1007/s10270-018-00712-x},
abstract = {More than a decade ago, the research topic models@run.time was coined. Since then, the research area has received increasing attention. Given the prolific results during these years, the current outcomes need to be sorted and classified. Furthermore, many gaps need to be categorized in order to further develop the research topic by experts of the research area but also newcomers. Accordingly, the paper discusses the principles and requirements of models@run.time and the state of the art of the research line. To make the discussion more concrete, a taxonomy is defined and used to compare the main approaches and research outcomes in the area during the last decade and including ancestor research initiatives. We identified and classified 275 papers on models@run.time, which allowed us to identify the underlying research gaps and to elaborate on the corresponding research challenges. Finally, we also facilitate sustainability of the survey over time by offering tool support to add, correct and visualize data.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {3049–3082},
numpages = {34},
keywords = {Causal connection, Models@run.time, Self-reflection, Systematic literature review}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {Architecture-implementation mapping, architectural evolution, architecture-centric development, architecture-centric feature traceability, variability conformance}
}

@inproceedings{10.1109/HICSS.2005.238,
author = {Phan, Vinh and Glisic, Savo and Luong, Dung},
title = {Energy-Efficient Smart Packet Access in WCDMA Networks: Performance with TCP/IP based Applications},
year = {2005},
isbn = {07695226889},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2005.238},
doi = {10.1109/HICSS.2005.238},
abstract = {This paper presents a performance analysis of adaptive packet access (PA) in WCDMA networks with TCP/IP based applications. The focus is on optimal tradeoffs between energy efficiency and TCP throughput over burst-error correlated fading channels. The channel model is adopted for low mobility PA applications. Both high-speed PA (HSPA) and low-speed PA (LSPA) with radio-link retransmission-and-error-control protocols are studied. By theoretical analysis and simulation, we show that in HSPA adapting the radio-packet retransmission period to the channel conditions significantly reduces the number of retransmissions while maintaining adequate TCP performance. This method is referred to as smart packet dispatching (SPD). In LSPA, adapting the length of radio packets to the channel conditions significantly increases the effective TCP throughput. This method is referred to as smart packet length (SPL). The SPD and SPL, commonly called smart packet access (SPA), are capable of avoiding burst-error states of correlated fading channels and therefore enhance the robustness of radio transmissions and allow the system to operate with much lower fade margin or transmit-power. Furthermore, the SPA schemes improve energy efficiency and system performance without need of excessive signal processing.},
booktitle = {Proceedings of the Proceedings of the 38th Annual Hawaii International Conference on System Sciences - Volume 09},
pages = {288.2},
series = {HICSS '05}
}

@inproceedings{10.1007/978-3-319-35122-3_9,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Alam, Omar and Sch\"{o}ttle, Matthias and Belloir, Nicolas and Collet, Philippe and Combemale, Benoit and Deantoni, Julien and Klein, Jacques and Rumpe, Bernhard},
title = {VCU: The Three Dimensions of Reuse},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_9},
doi = {10.1007/978-3-319-35122-3_9},
abstract = {Reuse, enabled by modularity and interfaces, is one of the most important concepts in software engineering. This is evidenced by an increasingly large number of reusable artifacts, ranging from small units such as classes to larger, more sophisticated units such as components, services, frameworks, software product lines, and concerns. This paper presents evidence that a canonical set of reuse interfaces has emerged over time: the variation, customization, and usage interfaces VCU. A reusable artifact that provides all three interfaces reaches the highest potential of reuse, as it explicitly exposes how the artifact can be manipulated during the reuse process along these three dimensions. We demonstrate the wide applicability of the VCU interfaces along two axes: across abstraction layers of a system specification and across existing reuse techniques. The former is shown with the help of a comprehensive case study including reusable requirements, software, and hardware models for the authorization domain. The latter is shown with a discussion on how the VCU interfaces relate to existing reuse techniques.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {122–137},
numpages = {16},
keywords = {Concern-oriented reuse, Configuration, Customization, Extension, Interfaces, Reuse, Usage, Variability},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@article{10.1016/j.future.2018.09.006,
author = {Munoz, Daniel-Jesus and Montenegro, Jos\'{e} A. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Energy-aware environments for the development of green applications for cyber–physical systems},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.006},
doi = {10.1016/j.future.2018.09.006},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {536–554},
numpages = {19},
keywords = {Energy consumption, Cyber–physical systems, Green plugin, HADAS eco-assistant}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {Self-adaptation, uncertainty, uncertainty models, uncertainty methods, unanticipated change, uncertainty challenges, survey}
}

@article{10.1016/j.jss.2010.02.017,
author = {White, J. and Benavides, D. and Schmidt, D. C. and Trinidad, P. and Dougherty, B. and Ruiz-Cortes, A.},
title = {Automated diagnosis of feature model configurations},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.02.017},
doi = {10.1016/j.jss.2010.02.017},
abstract = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1094–1107},
numpages = {14},
keywords = {Configuration, Constraint satisfaction, Diagnosis, Optimization, Software product-lines}
}

@inproceedings{10.1145/2025113.2025177,
author = {Mori, Marco},
title = {A software lifecycle process for context-aware adaptive systems},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025177},
doi = {10.1145/2025113.2025177},
abstract = {It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {412–415},
numpages = {4},
keywords = {consistent evolution, context-aware adaptive systems, feature engineering, software lifecycle process},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/3243734.3243739,
author = {Ispoglou, Kyriakos K. and AlBassam, Bader and Jaeger, Trent and Payer, Mathias},
title = {Block Oriented Programming: Automating Data-Only Attacks},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243739},
doi = {10.1145/3243734.3243739},
abstract = {With the widespread deployment of Control-Flow Integrity (CFI), control-flow hijacking attacks, and consequently code reuse attacks, are significantly more difficult. CFI limits control flow to well-known locations, severely restricting arbitrary code execution. Assessing the remaining attack surface of an application under advanced control-flow hijack defenses such as CFI and shadow stacks remains an open problem. We introduce BOPC, a mechanism to automatically assess whether an attacker can execute arbitrary code on a binary hardened with CFI/shadow stack defenses. BOPC computes exploits for a target program from payload specifications written in a Turing-complete, high-level language called SPL that abstracts away architecture and program-specific details. SPL payloads are compiled into a program trace that executes the desired behavior on top of the target binary. The input for BOPC is an SPL payload, a starting point (e.g., from a fuzzer crash) and an arbitrary memory write primitive that allows application state corruption. To map SPL payloads to a program trace, BOPC introduces Block Oriented Programming (BOP), a new code reuse technique that utilizes entire basic blocks as gadgets along valid execution paths in the program, i.e., without violating CFI or shadow stack policies. We find that the problem of mapping payloads to program traces is NP-hard, so BOPC first reduces the search space by pruning infeasible paths and then uses heuristics to guide the search to probable paths. BOPC encodes the BOP payload as a set of memory writes. We execute 13 SPL payloads applied to 10 popular applications. BOPC successfully finds payloads and complex execution traces -- which would likely not have been found through manual analysis -- while following the target's Control-Flow Graph under an ideal CFI policy in 81% of the cases.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1868–1882},
numpages = {15},
keywords = {binary analysis, block oriented programming, data only attacks, exploitation, program synthesis},
location = {Toronto, Canada},
series = {CCS '18}
}

@inproceedings{10.5555/1515915.1515947,
author = {Delicato, Fl\'{a}via Coimbra and Pires, Paulo F. and Pirmez, Luci and da Costa Carmo, Luiz Fernando Rust},
title = {A flexible middleware system for wireless sensor networks},
year = {2003},
isbn = {3540403175},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The current wireless sensor networks (WSN) are assumed to be designed for specific applications, having data communication protocols strongly coupled to applications. The future WSNs are envisioned as comprising of heterogeneous devices assisting to a large range of applications. To achieve this goal, a flexible middleware layer is needed, separating application specific features from the data communication protocol, while allowing applications to influence the WSN behavior for energy efficiency. We propose a service-based middleware system for WSNs. In our proposal, sensor nodes are service providers and applications are clients of such services. Our main goal is to enable an interoperability layer among applications and sensor networks, among different sensors in a WSN and eventually among different WSN spread all over the world.},
booktitle = {Proceedings of the ACM/IFIP/USENIX 2003 International Conference on Middleware},
pages = {474–492},
numpages = {19},
location = {Rio de Janeiro, Brazil},
series = {Middleware '03}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1016/j.future.2018.03.052,
author = {Sodhro, Ali Hassan and Pirbhulal, Sandeep and Sangaiah, Arun Kumar},
title = {Convergence of IoT and product lifecycle management in medical health care},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.03.052},
doi = {10.1016/j.future.2018.03.052},
journal = {Future Gener. Comput. Syst.},
month = sep,
pages = {380–391},
numpages = {12},
keywords = {Product lifecycle management, Internet of Medical Things, Battery lifecycle, Energy management, Battery recovery algorithm, Joint energy harvesting and duty-cycle optimization (JEHDO)}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {feature selection, performance analysis, software architectures, uncertainty},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1007/s10270-017-0587-8,
author = {Kluge, Roland and Stein, Michael and Varr\'{o}, Gergely and Sch\"{u}rr, Andy and Hollick, Matthias and M\"{u}hlh\"{a}user, Max},
title = {A systematic approach to constructing families of incremental topology control algorithms using graph transformation},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0587-8},
doi = {10.1007/s10270-017-0587-8},
abstract = {In the communication system domain, constructing and maintaining network topologies via topology control algorithms is an important crosscutting research area. Network topologies are usually modeled using attributed graphs whose nodes and edges represent the network nodes and their interconnecting links. A key requirement of topology control algorithms is to fulfill certain consistency and optimization properties to ensure a high quality of service. Still, few attempts have been made to constructively integrate these properties into the development process of topology control algorithms. Furthermore, even though many topology control algorithms share substantial parts (such as structural patterns or tie-breaking strategies), few works constructively leverage these commonalities and differences of topology control algorithms systematically. In previous work, we addressed the constructive integration of consistency properties into the development process. We outlined a constructive, model-driven methodology for designing individual topology control algorithms. Valid and high-quality topologies are characterized using declarative graph constraints; topology control algorithms are specified using programmed graph transformation. We applied a well-known static analysis technique to refine a given topology control algorithm in a way that the resulting algorithm preserves the specified graph constraints. In this paper, we extend our constructive methodology by generalizing it to support the specification of families of topology control algorithms. To show the feasibility of our approach, we reengineering six existing topology control algorithms and develop e-kTC, a novel energy-efficient variant of the topology control algorithm kTC. Finally, we evaluate a subset of the specified topology control algorithms using a new tool integration of the graph transformation tool eMoflon and the Simonstrator network simulation framework.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {279–319},
numpages = {41},
keywords = {Graph constraints, Graph transformation, Model-driven engineering, Network simulation, Static analysis, Wireless networks}
}

@article{10.1007/s10586-014-0397-5,
author = {Tan, Yujuan and Yan, Zhichao and Feng, Dan and He, Xubin and Zou, Qiang and Yang, Lei},
title = {De-Frag: an efficient scheme to improve deduplication performance via reducing data placement de-linearization},
year = {2015},
issue_date = {Mar 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-014-0397-5},
doi = {10.1007/s10586-014-0397-5},
abstract = {Data deduplication has become a commodity in large-scale storage systems, especially in data backup and archival systems. However, due to the removal of redundant data, data deduplication de-linearizes data placement and forces the data chunks of the same data object to be divided into multiple separate units. In our preliminary study, we found that the de-linearization of data placement compromises the data spatial locality that is used to improve data read performance, deduplication throughput and deduplication efficiency in some deduplication approaches, which significantly affects deduplication performance and makes some deduplication approaches become less effective. In this paper, we first analyze the negative effect of data placement de-linearization to deduplication performance, and then propose an effective approach called De-Frag to reduce the de-linearization of data placement. The key idea of De-Frag is to choose some redundant data to be written to the disks rather than be removed. It quantifies the spatial locality of each chunk group by spatial locality level (SPL for short) and writes the redundant chunks to disks when SPL value is smaller than a preset value, thus to reduce the de-linearization of data placement and enhance the spatial locality. As shown in our experimental results driven by real world datasets, De-Frag effectively enhances data spatial locality and improves deduplication throughput, deduplication efficiency, and data read performance, at the cost of slightly lower compression ratios.},
journal = {Cluster Computing},
month = mar,
pages = {79–92},
numpages = {14},
keywords = {Data deduplication, Data placement de-linearization, Spatial locality}
}

@inproceedings{10.1007/978-3-642-12261-3_7,
author = {Van Baelen, Stefan and Weigert, Thomas and Ober, Ileana and Espinoza, Huascar and Ober, Iulian},
title = {Model based architecting and construction of embedded systems (ACES-MB 2009)},
year = {2009},
isbn = {3642122604},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12261-3_7},
doi = {10.1007/978-3-642-12261-3_7},
abstract = {The second ACES-MB workshop brought together researchers and practitioners interested in model-based software engineering for real-time embedded systems, with a particular focus on the use of models for architecture description and domain-specific design, and for capturing non-functional constraints. Eleven presenters proposed contributions on domain-specific languages for embedded systems, the Architecture Analysis and Design Language (AADL), analysis and formalization, semantics preservation issues, and variability and reconfiguration. In addition, a lively group discussion tackled the issue of combining models from different Domain Specific Modeling Languages (DSMLs). This report presents an overview of the presentations and fruitful discussions that took place during the ACES-MB 2009 workshop.},
booktitle = {Proceedings of the 2009 International Conference on Models in Software Engineering},
pages = {63–67},
numpages = {5},
location = {Denver, CO},
series = {MODELS'09}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Control-flow feature interaction, Feature, Feature interaction, Feature-interaction prediction, Highly configurable software system, Performance feature interaction, Variability}
}

@article{10.1016/j.comnet.2013.02.025,
author = {Apel, Sven and Von Rhein, Alexander and Th\"{u}m, Thomas and K\"{a}stner, Christian},
title = {Feature-interaction detection based on feature-based specifications},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {57},
number = {12},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2013.02.025},
doi = {10.1016/j.comnet.2013.02.025},
abstract = {Formal specification and verification techniques have been used successfully to detect feature interactions. We investigate whether feature-based specifications can be used for this task. Feature-based specifications are a special class of specifications that aim at modularity in open-world, feature-oriented systems. The question we address is whether modularity of specifications impairs the ability to detect feature interactions, which cut across feature boundaries. In an exploratory study on 10 feature-oriented systems, we found that the majority of feature interactions could be detected based on feature-based specifications, but some specifications have not been modularized properly and require undesirable workarounds to modularization. Based on the study, we discuss the merits and limitations of feature-based specifications, as well as open issues and perspectives. A goal that underlies our work is to raise awareness of the importance and challenges of feature-based specification.},
journal = {Comput. Netw.},
month = aug,
pages = {2399–2409},
numpages = {11},
keywords = {Feature interaction, Feature orientation, Feature-based specification, Modularity, Software product lines}
}

@inproceedings{10.5555/2755753.2755812,
author = {Gomez, Andres and Pinto, Christian and Bartolini, Andrea and Rossi, Davide and Benini, Luca and Fatemi, Hamed and de Gyvez, Jose Pineda},
title = {Reducing energy consumption in microcontroller-based platforms with low design margin co-processors},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Advanced energy minimization techniques (i.e. DVFS, Thermal Management, etc) and their high-level HW/SW requirements are well established in high-throughput multicore systems. These techniques would have an intolerable overhead in low-cost, performance-constrained microcontroller units (MCU's). These devices can further reduce power by operating at a lower voltage, at the cost of increased sensitivity to PVT variation and increased design margins. In this paper, we propose an runtime environment for next-generation dual-core MCU platforms. These platforms complement a single-core with a low area overhead, reduced design margin shadow-processor. The runtime decreases the overall energy consumption by exploiting design corner heterogeneity between the two cores, rather than increasing the throughput. This allows the platform's power envelope to be dynamically adjusted to application-specific requirements. Our simulations show that, depending on the ratio of core to platform energy, total energy savings can be up to 20%.},
booktitle = {Proceedings of the 2015 Design, Automation &amp; Test in Europe Conference &amp; Exhibition},
pages = {269–272},
numpages = {4},
location = {Grenoble, France},
series = {DATE '15}
}

@inproceedings{10.1145/2593770.2593781,
author = {Ramaswamy, Arunkumar and Monsuez, Bruno and Tapus, Adriana},
title = {Model-driven software development approaches in robotics research},
year = {2014},
isbn = {9781450328494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593770.2593781},
doi = {10.1145/2593770.2593781},
abstract = {Recently, there is an encouraging trend in adopting model-driven engineering approaches for software development in robotics research. In this paper, currently available model-driven techniques in robotics are analyzed with respect to the domain-specific requirements. A conceptual overview of our software development approach called 'Self Adaptive Framework for Robotic Systems (SafeRobots)' is explained and we also try to position our approach within this model ecosystem.},
booktitle = {Proceedings of the 6th International Workshop on Modeling in Software Engineering},
pages = {43–48},
numpages = {6},
keywords = {Model-driven software development, Robotics},
location = {Hyderabad, India},
series = {MiSE 2014}
}

@inproceedings{10.1145/2491246.2491250,
author = {Zhang, Xi and Ansari, Junaid and Arya, Manish and M\"{a}h\"{o}nen, Petri},
title = {Exploring parallelization for medium access schemes on many-core software defined radio architecture},
year = {2013},
isbn = {9781450321815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491246.2491250},
doi = {10.1145/2491246.2491250},
abstract = {As multi-standard devices and high speed communication standards are emerging, timeliness requirements and flexibility for both baseband modem and medium access schemes are becoming essential. Software Defined Radios (SDRs), in this context, aim at offering the desired flexibility while satisfying the real-time constraints. An SDR architecture consisting of many-core homogeneous computing elements provides easy protocol implementation, a high level of portability and extension possibilities. It does not require architecture specific program code which is needed by the popular heterogeneous SDR architectures. Therefore, in this paper, we explore how a homogeneous SDR architecture is used for efficient realization and execution of Medium Access Control (MAC) protocols. In particular, we investigate the performance of two broad classes of MAC schemes on the Platform 2012 (P2012) many-core programmable computing fabric. We provide a toolchain which utilizes the characteristics of P2012 for MAC parallelization, runtime scheduling, and execution. Our results indicate that by using the supporting toolchain, reconfigurable MAC implementations are able to exploit the computational power offered by the platform and adhere to the timeliness constraints. Computationally intensive algorithms for MAC layer parameter optimization show an improvement of up to 85% in the convergence time as compared to using a single-core architecture.},
booktitle = {Proceedings of the Second Workshop on Software Radio Implementation Forum},
pages = {37–44},
numpages = {8},
keywords = {mac, many-core, parallelization, sdr platform},
location = {Hong Kong, China},
series = {SRIF '13}
}

@inbook{10.5555/1768283.1768287,
author = {Cuenot, Philippe and Chen, DeJiu and G\'{e}rard, S\'{e}bastien and L\"{o}nn, Henrik and Reiser, Mark-Oliver and Servat, David and Kolagari, Ramin Tavakoli and T\"{o}rngren, Martin and Weber, Matthias},
title = {Towards improving dependability of automotive systems by using the EAST-ADL architecture description language},
year = {2007},
isbn = {9783540740339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The complexity of embedded automotive systems calls for a more rigorous approach to system development compared to current state of practice. A critical issue is the management of the engineering information that defines the embedded system. Development time, cost efficiency, quality and most importantly, dependability, all benefit from appropriate information management. System modeling based on an architecture description language is a way to keep the engineering information in one information structure. The EAST-ADL was developed in the EAST-EEA project (www.east-eea.org) and is an architecture description language for automotive embedded systems. It is currently refined in the ATESST project (www.atesst.org). This chapter describes how dependability is addressed in the EAST-ADL. The engineering process defined in the EASIS project (www.easis-online.org) is used as an example to illustrate the support for engineering processes in EAST-ADL.},
booktitle = {Architecting Dependable Systems IV},
pages = {39–65},
numpages = {27}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00107,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-box performance-influence models: a profiling and learning approach (replication package)},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00107},
doi = {10.1109/ICSE-Companion52605.2021.00107},
abstract = {These artifacts refer to the study and implementation of the paper 'White-Box Performance-Influence Models: A Profiling and Learning Approach'. In this document, we describe the idea and process of how to build white-box performance models for configurable software systems. Specifically, we describe the general steps and tools that we have used to implement our approach, the data we have obtained, and the evaluation setup. We further list the available artifacts, such as raw measurements, configurations, and scripts at our software heritage repository.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {232–233},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1016/j.eswa.2012.08.026,
author = {Ognjanovi\'{c}, Ivana and Ga\v{s}Evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process},
year = {2013},
issue_date = {March, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.08.026},
doi = {10.1016/j.eswa.2012.08.026},
abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1094–1115},
numpages = {22},
keywords = {AHP method, Comparative preferences, Conditional preferences, Lexicographic order, S-AHP method, Well-formed preferences}
}

@inproceedings{10.1145/2047862.2047866,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Pukall, Mario and Apel, Sven},
title = {Tailoring dynamic software product lines},
year = {2011},
isbn = {9781450306898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047862.2047866},
doi = {10.1145/2047862.2047866},
abstract = {Software product lines (SPLs) and adaptive systems aim at variability to cope with changing requirements. Variability can be described in terms of features, which are central for development and configuration of SPLs. In traditional SPLs, features are bound statically before runtime. By contrast, adaptive systems support feature binding at runtime and are sometimes called dynamic SPLs (DSPLs). DSPLs are usually built from coarse-grained components, which reduces the number of possible application scenarios. To overcome this limitation, we closely integrate static binding of traditional SPLs and runtime adaptation of DSPLs. We achieve this integration by statically generating a tailor-made DSPL from a highly customizable SPL. The generated DSPL provides only the runtime variability required by a particular application scenario and the execution environment. The DSPL supports self-configuration based on coarse-grained modules. We provide a feature-based adaptation mechanism that reduces the effort of computing an optimal configuration at runtime. In a case study, we demonstrate the practicability of our approach and show that a seamless integration of static binding and runtime adaptation reduces the complexity of the adaptation process.},
booktitle = {Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering},
pages = {3–12},
numpages = {10},
keywords = {dynamic binding, feature-oriented programming, software product lines},
location = {Portland, Oregon, USA},
series = {GPCE '11}
}

@article{10.1016/j.cie.2016.09.023,
author = {Addo-Tenkorang, Richard and Helo, Petri T.},
title = {Big data applications in operations/supply-chain management},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.09.023},
doi = {10.1016/j.cie.2016.09.023},
abstract = {Harnessing optimum value from industrial data increased in the last two decades.A detailed review of "big data" application in operations/SC management processes.Proposed (Value-adding - V5) framework for operation/SC management. PurposeBig data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of "big data." Therefore, this paper attempts to thoroughly investigate "big data," its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of "big data" and its extension into "big data II"/IoT-value-adding perspectives by proposing a value-adding framework. Methodology/research approachThe research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of "big data" applications between 2010 and 2015. Findings/resultsThe four main attributes or factors identified with "big data" include - big data development sources (Variety - V1), big data acquisition (Velocity - V2), big data storage (Volume - V3), and finally big data analysis (Veracity - V4). However, the study of "big data" has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding - V5) - "Big Data cloud computing perspective/Internet of Things (IoT)". Hence, the four Vs of "big data" is now expanded into five Vs. Originality/value of researchThis paper presents original literature review research discussing "big data" issues, trends and perspectives in operations/supply-chain management in order to propose "Big data II" (IoT - Value-adding) framework. This proposed framework is supposed or assumed to be an extension of "big data" in a value-adding perspective, thus proposing that "big data" be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.},
journal = {Comput. Ind. Eng.},
month = nov,
pages = {528–543},
numpages = {16},
keywords = {Big data - applications and analysis, Cloud computing, Internet of Things (IoT), Master database management, Operations/supply-chain management}
}

@article{10.1147/JRD.2013.2243535,
author = {Hirzel, M. and Andrade, H. and Gedik, B. and Jacques-Silva, G. and Khandekar, R. and Kumar, V. and Mendell, M. and Nasgaard, H. and Schneider, S. and Soul\'{e}, R. and Wu, K.-L.},
title = {IBM streams processing language: analyzing big data in motion},
year = {2013},
issue_date = {May/July 2013},
publisher = {IBM Corp.},
address = {USA},
volume = {57},
number = {3–4},
issn = {0018-8646},
url = {https://doi.org/10.1147/JRD.2013.2243535},
doi = {10.1147/JRD.2013.2243535},
abstract = {The IBM Streams Processing Language (SPL) is the programming language for IBM InfoSphere® Streams, a platform for analyzing Big Data in motion. By "Big Data in motion," we mean continuous data streams at high data-transfer rates. InfoSphere Streams processes such data with both high throughput and short response times. To meet these performance demands, it deploys each application on a cluster of commodity servers. SPL abstracts away the complexity of the distributed system, instead exposing a simple graph-of-operators view to the user. SPL has several innovations relative to prior streaming languages. For performance and code reuse, SPL provides a code-generation interface to C++ and Java®. To facilitate writing well-structured and concise applications, SPL provides higher-order composite operators that modularize stream sub-graphs. Finally, to enable static checking while exposing optimization opportunities, SPL provides a strong type system and user-defined operator models. This paper provides a language overview, describes the implementation including optimizations such as fusion, and explains the rationale behind the language design.},
journal = {IBM J. Res. Dev.},
month = may,
articleno = {1},
numpages = {1}
}

@article{10.1016/j.jnca.2015.07.007,
author = {Yongsiriwit, Karn and Assy, Nour and Gaaloul, Walid},
title = {A semantic framework for configurable business process as a service in the cloud},
year = {2016},
issue_date = {January 2016},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {59},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2015.07.007},
doi = {10.1016/j.jnca.2015.07.007},
abstract = {With the advent of Cloud Computing, new opportunities for Business Process Outsourcing services have emerged. Business Process as a Service (BPaaS), a new cloud service model, has recently gained a great importance for outsourcing cloud-based business processes constructed for multi-tenancy. In such a multi-tenant environment, using configurable business process models enables the sharing of a reference process among different tenants that can be customized according to specific needs. With a large choice of configurable process modeling languages, different providers may deliver configurable processes with common functionalities but different representations which makes the process discovery and configuration a manual tedious task. This in turn creates cloud silos and vendors lock-in with non-reusable configurable BPaaS models. Therefore, with the aim of enabling the interoperability between multiple BPaaS providers, we propose in this paper a semantic framework for BPaaS configurable models. Taking advantage of Semantic Web technologies and data mining techniques, our framework allows for (1) an ontology-based high level abstract representation of BPaaS configurable models enriched with configuration guidelines and (2) an automated approach for extracting the configuration guidelines from existing process repositories. To show the feasibility and effectiveness of our approach, we extend Signavio with our semantic framework and conduct experiments on a dataset from SAP reference model.},
journal = {J. Netw. Comput. Appl.},
month = jan,
pages = {168–184},
numpages = {17},
keywords = {BPaaS, Business Process as a Service, Cloud Computing, Configurable process model, Green IT, Semantic technology}
}

@inproceedings{10.1145/3387940.3391474,
author = {Brings, Jennifer and Daun, Marian},
title = {Towards automated safety analysis for architectures of dynamically forming networks of cyber-physical systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391474},
doi = {10.1145/3387940.3391474},
abstract = {Dynamically forming networks of cyber-physical systems are becoming increasingly widespread in manufacturing, transportation, automotive, avionics and more domains. The emergence of future internet technology and the ambition for ever closer integration of different systems leads to highly collaborative cyber-physical systems. Such cyber-physical systems form networks to provide additional functions, behavior, and benefits the individual systems cannot provide on their own. As safety is a major concern of systems from these domains, there is a need to provide adequate support for safety analyses of these collaborative cyber-physical systems. This support must explicitly consider the dynamically formed networks of cyber-physical systems. This is a challenging task as the configurations of these cyber-physical system networks (i.e. the architecture of the super system the individual system joins) can differ enormously depending on the actual systems joining a cyber-physical system network. Furthermore, the configuration of the network heavily impacts the adaptations performed by the individual systems and thereby impacting the architecture not only of the system network but of all individual systems involved. As existing safety analysis techniques, however, are not meant for supporting such an array of potential system network configurations the individual system will have to be able to cope with at runtime, we propose automated support for safety analysis for these systems that considers the configuration of the system network. Initial evaluation results from the application to industrial case examples show that the proposed support can aid in the detection of safety defects.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {258–265},
numpages = {8},
keywords = {cyber-physical system, safety analysis, system architecture},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {configuration, literature review, performance, product line, product requirements, scalability, survey},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/1370175.1370249,
author = {Avgeriou, Paris and Lago, Patricia and Kruchten, Philippe},
title = {Third international workshop on sharing and reusing architectural knowledge (SHARK 2008)},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370175.1370249},
doi = {10.1145/1370175.1370249},
abstract = {The shift of the software architecture community towards architectural knowledge has brought along some promising research directions. In this workshop we discuss the issues that lead to the application of architectural knowledge in research and industrial practice; ongoing research and new ideas to advance the field. In its previous editions we examined the state of the art and practice, future challenges and trends. This third edition will discuss, among others, architectural knowledge as perceived by different research communities, including requirements engineering, service-oriented computing and international standardization.},
booktitle = {Companion of the 30th International Conference on Software Engineering},
pages = {1065–1066},
numpages = {2},
keywords = {architectural knowledge},
location = {Leipzig, Germany},
series = {ICSE Companion '08}
}

@article{10.1504/IJWET.2015.069359,
author = {Berkane, Mohamed Lamine and Seinturier, Lionel and Boufaida, Mahmoud},
title = {Using variability modelling and design patterns for self-adaptive system engineering: application to smart-home},
year = {2015},
issue_date = {May 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1},
issn = {1476-1289},
url = {https://doi.org/10.1504/IJWET.2015.069359},
doi = {10.1504/IJWET.2015.069359},
abstract = {Adaptability is an increasingly important requirement for many systems, in particular for those that are deployed in dynamically changing environments. The purpose is to let the systems react and adapt autonomously to changing executing conditions without human intervention. Due to the large number of variability decisions e.g., user needs, environment characteristics and the current lack of reusable adaptation expertise, it becomes increasingly difficult to build a system that satisfies all the requirements and constraints that might arise during its lifetime. In this paper, we propose an approach for developing policies for self-adaptive systems at multiple levels of abstraction. This approach is the first that allows the combination of variability with feature model and reusability with design pattern into a single solution for product derivation that gives strong support to develop self-adaptive systems in a modular way. We demonstrate the feasibility of the proposed approach with a use case based on a smart home scenario.},
journal = {Int. J. Web Eng. Technol.},
month = may,
pages = {65–93},
numpages = {29}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@article{10.4018/IJCSSA.2016010104,
author = {von Rosing, Mark and Fullington, Nathan and Walker, John},
title = {Using the Business Ontology and Enterprise Standards to Transform Three Leading Organizations},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {1},
issn = {2166-7292},
url = {https://doi.org/10.4018/IJCSSA.2016010104},
doi = {10.4018/IJCSSA.2016010104},
abstract = {This case story covers the exciting journey of three growth organizations and how they applied the Global University Alliance developed Business Ontology and various enterprise standards to innovate and transform their organization. The paper does so by firstly elaborating on the theory, then it introduces the three organizations, discussed the challenges and issues at hand. Followed by a discussion of their journey and the solution description. Various details about the journey and how enterprise standards where used will be shared, including how these standards assisted these organizations in rethinking their business model, the operating model which effected both the value, revenue and service model as well as the performance and cost model. The case concludes with detailed lessons learned and how the business ontology and standards helped the organizations changed.},
journal = {Int. J. Concept. Struct. Smart Appl.},
month = jan,
pages = {71–99},
numpages = {29},
keywords = {Apply Enterprise Standards, Back Office Consolidation, Business Governance, Business Model, Business Ontology, Business Process Management, Enterprise Optimization, Enterprise Semantics, Innovation &amp; Transformation, Knowledge Sharing, Mergers &amp; Acquisition, Meta Model, Operating Model, Organizational Capabilities, Professional Services Administration, Shared Service Operations, Strategic Choices, Strategic Position, Value Lifecycle}
}

@inproceedings{10.5555/2820518.2820523,
author = {Hashimoto, Masatomo and Terai, Masaaki and Maeda, Toshiyuki and Minami, Kazuo},
title = {Extracting facts from performance tuning history of scientific applications for predicting effective optimization patterns},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {13–23},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1007/s11257-015-9159-1,
author = {Dim, Eyal and Kuflik, Tsvi and Reinhartz-Berger, Iris},
title = {When user modeling intersects software engineering: the info-bead user modeling approach},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-015-9159-1},
doi = {10.1007/s11257-015-9159-1},
abstract = {User models (UMs) allow systems to provide personalized services to their users. Nowadays, UMs are developed ad-hoc, as part of specific applications, thus requiring repetitive development efforts. In this paper, we propose the info-bead user modeling approach, which is based on ideas taken from software engineering in general and component-based software development in particular. The basic standalone unit, the info-bead, represents a single user attribute within time-tagged information-items. An info-bead encapsulates an inference process that uses data received from sensors or other info-beads and yields an information-item value. Having standard interfaces, info-beads can be linked, thus creating info-pendants. Both info-beads and info-pendants can be assembled as needed into complex and abstract user models (UMs) and group models (GMs). The goal of the suggested approach is to ease the modeling process and to allow reuse of info beads developed for one UM in other UMs that need the same information. In order to assess the reusability and collaboration capabilities of the info-bead user modeling approach, we developed a prototype tool that enables UM designers, who are not necessarily software developers, to easily select and integrate info-beads for constructing UMs and GMs. We further demonstrated the use of the approach in a museum environment, for modeling of assistive technology ontology and for user modeling in various specific domains. Finally, we analyzed and assessed the characteristics of the approach with respect to existing generic user modeling criteria.},
journal = {User Modeling and User-Adapted Interaction},
month = aug,
pages = {189–229},
numpages = {41},
keywords = {Component-based user model, Group model, Info-bead, Info-pendant, User model, User model reusability, User modeling software engineering, User modeling tool}
}

@article{10.1016/j.datak.2021.101929,
author = {Ali, Mughees and Khan, Saif Ur Rehman and Hussain, Shahid},
title = {Self-adaptation in smartphone applications: Current state-of-the-art techniques, challenges, and future directions},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {136},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101929},
doi = {10.1016/j.datak.2021.101929},
journal = {Data Knowl. Eng.},
month = nov,
numpages = {19},
keywords = {Self-adaptation, Mobile applications, Smartphone applications, Self-adaptive mobile apps}
}

@article{10.1007/s10664-019-09763-0,
author = {Kr\"{u}ger, Jacob and Lausberger, Christian and von Nostitz-Wallwitz, Ivonne and Saake, Gunter and Leich, Thomas},
title = {Search. Review. Repeat? An empirical study of threats to replicating SLR searches},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09763-0},
doi = {10.1007/s10664-019-09763-0},
abstract = {A systematic literature review (SLR) is an empirical method used to provide an overview of existing knowledge and to aggregate evidence within a domain. For computer science, several threats to the completeness of such reviews have been identified, leading to recommendations and guidelines on how to improve their quality. However, few studies address to what extent researchers can replicate an SLR. To conduct a replication, researchers have to first understand how the set of primary studies has been identified in the original study, and can ideally retrieve the same set when following the reported protocol. In this article, we focus on this initial step of a replication and report a two-fold empirical study: Initially, we performed a tertiary study using a sample of SLRs in computer science and identified what information that is needed to replicate the searches is reported. Based on the results, we conducted a descriptive, multi-case study on digital libraries to investigate to what extent these allow replications. The results reveal two threats to replications of SLRs: First, while researchers have improved the quality of their reports, relevant details are still missing—we refer to a reporting threat. Second, we found that some digital libraries are inconsistent in their query results—we refer to a searching threat. While researchers conducting a review can only overcome the first threat and the second may not be an issue for all kinds of replications, researchers should be aware of both threats when conducting, reviewing, and building on SLRs.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {627–677},
numpages = {51},
keywords = {Tertiary study, Systematic literature review, Software engineering, Threats to validity, Replication, Digital library}
}

@article{10.1016/j.future.2018.05.023,
author = {Guerrera, Danilo and Maffia, Antonio and Burkhart, Helmar},
title = {Reproducible stencil compiler benchmarks using prova!      },
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.05.023},
doi = {10.1016/j.future.2018.05.023},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {933–946},
numpages = {14},
keywords = {Reproducibility, Reproducible research, HPC, Stencil, Performance engineering, Roofline}
}

@inproceedings{10.1145/1596495.1596500,
author = {Peper, Christian and Schneider, Daniel},
title = {On runtime service quality models in adaptive ad-hoc systems},
year = {2009},
isbn = {9781605586816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596495.1596500},
doi = {10.1145/1596495.1596500},
abstract = {Ad-hoc computer systems can automatically realize higher services when at least two distributed and communicating (embedded) devices come together. For this purpose, they must able to manage appearance and loss of devices and resources, and they have to adapt to changes in requirements and environment. Based on a component-oriented approach for adaptive ad-hoc systems, this paper suggests a high-level service quality reference model to advocate further research on the quality matching problem between service provider and client components.},
booktitle = {Proceedings of the 2009 ESEC/FSE Workshop on Software Integration and Evolution @ Runtime},
pages = {11–18},
numpages = {8},
keywords = {ad-hoc systems, adaptivity, ambient intelligence, component-orientation, distributed systems, quality-of-service, ubiquitous computing},
location = {Amsterdam, The Netherlands},
series = {SINTER '09}
}

@article{10.1145/3170432,
author = {Dayarathna, Miyuru and Perera, Srinath},
title = {Recent Advancements in Event Processing},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3170432},
doi = {10.1145/3170432},
abstract = {Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {33},
numpages = {36},
keywords = {Event processing, complex event processing, data stream processing}
}

@article{10.1007/s10270-014-0405-5,
author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
title = {Applying UML/MARTE on industrial projects: challenges, experiences, and guidelines},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-014-0405-5},
doi = {10.1007/s10270-014-0405-5},
abstract = {Modeling and Analysis of Real-Time and Embedded Systems (MARTE) is a Unified Modeling Language (UML) profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In the last 5 years, we have applied UML/MARTE to three distinct industrial problems in three industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experience of solving these problems by applying UML/MARTE on four industrial case studies. We highlight the challenges we faced with respect to the industrial adoption of MARTE. Based on our combined experience, we derive a framework to guide practitioners for future applications of UML/MARTE in an industrial context. The framework provides a set of detailed guidelines that help reduce the gap between the modeling notations and real-world industrial application needs.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1367–1385},
numpages = {19},
keywords = {Architecture Modeling, Industrial Case Studies, MARTE, Model-based Testing, Real-Time Embedded Systems, UML}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software paradigms, assessment types and non-functional requirements in model-based integration testing: a systematic literature review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {assessment types, model-based integration testing, non-functional requirements, systematic literature review},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1007/978-3-642-36949-0_15,
author = {Membarth, Richard and Hannig, Frank and Teich, J\"{u}rgen and K\"{o}rner, Mario and Eckert, Wieland},
title = {Mastering software variant explosion for GPU accelerators},
year = {2012},
isbn = {9783642369483},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36949-0_15},
doi = {10.1007/978-3-642-36949-0_15},
abstract = {Mapping algorithms in an efficient way to the target hardware poses a challenge for algorithm designers. This is particular true for heterogeneous systems hosting accelerators like graphics cards. While algorithm developers have profound knowledge of the application domain, they often lack detailed insight into the underlying hardware of accelerators in order to exploit the provided processing power. Therefore, this paper introduces a rule-based, domain-specific optimization engine for generating the most appropriate code variant for different Graphics Processing Unit (GPU) accelerators. The optimization engine relies on knowledge fused from the application domain and the target architecture. The optimization engine is embedded into a framework that allows to design imaging algorithms in a Domain-Specific Language (DSL). We show that this allows to have one common description of an algorithm in the DSL and select the optimal target code variant for different GPU accelerators and target languages like CUDA and OpenCL.},
booktitle = {Proceedings of the 18th International Conference on Parallel Processing Workshops},
pages = {123–132},
numpages = {10},
location = {Rhodes Island, Greece},
series = {Euro-Par'12}
}

@inproceedings{10.1145/2721956.2721977,
author = {Kajtazovic, Nermin and Preschern, Christopher and H\"{o}ller, Andrea and Kreiner, Christian},
title = {Towards pattern-based reuse in safety-critical systems},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721977},
doi = {10.1145/2721956.2721977},
abstract = {Challenges such as time-to-market, reduced costs for change and maintenance have radically influenced development of today's safety-critical systems. Many domains have already adopted their system's engineering to support modular and component-based architectures. With the component-based design paradigm, the system engineering is utilized allowing to distribute development among different development teams, however, with the price that there is no full trust in independently developed parts, which makes their reuse challenging. Until now, many approaches that address reuse, on conceptual or detailed level, have been proposed. A very important aspect addressed here is to document the information flow between system parts in detail, i.e. from higher abstraction levels down to the implementation details, in order to put more trust into independently developed parts of the system.In this paper, we describe a compact pattern system with the aim to establish a link between high level concepts for reuse and detailed description of the behavior of system parts. The main goal is to document these details up to the higher levels of abstraction in more systematic way.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {33},
numpages = {15},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@article{10.1007/s10270-020-00823-4,
author = {Kretschmer, Roland and Khelladi, Djamel Eddine and Lopez-Herrejon, Roberto Erick and Egyed, Alexander},
title = {Consistent change propagation within models},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00823-4},
doi = {10.1007/s10270-020-00823-4},
abstract = {Developers change models with clear intentions—e.g., for refactoring, defects removal, or evolution. However, in doing so, developers are often unaware of the consequences of their changes. Changes to one part of a model may affect other parts of the same model and/or even other models, possibly created and maintained by other developers. The consequences are incomplete changes and with it inconsistencies within or across models. Extensive works exist on detecting and repairing inconsistencies. However, the literature tends to focus on inconsistencies as errors in need of repairs rather than on incomplete changes in need of further propagation. Many changes are non-trivial and require a series of coordinated model changes. As developers start changing the model, intermittent inconsistencies arise with other parts of the model that developers have not yet changed. These inconsistencies are cues for incomplete change propagation. Resolving these inconsistencies should be done in a manner that is consistent with the original changes. We speak of consistent change propagation. This paper leverages classical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only suggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair. In doing so, our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier changes. We argue that consistent change propagation is essential for effective model-driven engineering. Our approach and its tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with two versioned models shows that our approach identifies actual repair sequences that developers had chosen. Furthermore, an experiment involving 22 participants shows that our change propagation approach meets the workflow of how developers handle changes by always computing the sequence of repairs resulting from the change propagation.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {539–555},
numpages = {17},
keywords = {Model-driven engineering, Inconsistency repair, Change propagation, Consistency detection}
}

@inproceedings{10.1007/978-3-030-37734-2_62,
author = {Choi, Jung-Woo},
title = {Real-Time Demonstration of Personal Audio and 3D Audio Rendering Using Line Array Systems},
year = {2020},
isbn = {978-3-030-37733-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-37734-2_62},
doi = {10.1007/978-3-030-37734-2_62},
abstract = {Control of sound fields using array loudspeakers has been attempted in many practical areas, such as 3D audio, active noise control, and personal audio. In this work, we demonstrate two real-time sound field control systems involving a line array of loudspeakers. The first one, a personal audio system, aims to reproduce two independent sound zones with different audio programs at the same time. By suppressing acoustic interference between two sound zones, the personal audio system allows users at different locations to enjoy independent sounds. In the second demonstration, active control of spatial audio scene is presented. It has been found that the interaction between the radiation from a sound source and surrounding environment is linked with many perceptual cues of spaciousness. Especially, the perceived stage width and distance are strongly related to the interaural cross-correlation and direct-to-reverberation ratio, which can be easily manipulated by changing the directivity of a loudspeaker array. The smooth transition of spaciousness is demonstrated by changing the shapes of multiple beam patterns radiated from the line array.},
booktitle = {MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5–8, 2020, Proceedings, Part II},
pages = {734–738},
numpages = {5},
keywords = {Sound field control, Personal audio, Spatial audio},
location = {Daejeon, Korea (Republic of)}
}

@inproceedings{10.1145/2661136.2661159,
author = {Acher, Mathieu and Combemale, Benoit and Collet, Philippe},
title = {Metamorphic Domain-Specific Languages: A Journey into the Shapes of a Language},
year = {2014},
isbn = {9781450332101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661136.2661159},
doi = {10.1145/2661136.2661159},
abstract = {External or internal domain-specific languages (DSLs) or (fluent) APIs' Whoever you are - a developer or a user of a DSL - you usually have to choose side; you should not! What about metamorphic DSLs that change their shape according to your needs? Our 4-years journey of providing the "right" support (in the domain of feature modeling), led us to develop an external DSL, different shapes of an internal API, and maintain all these languages. A key insight is that there is no one-size-fits-all solution or no clear superiority of a solution compared to another. On the contrary, we found that it does make sense to continue the maintenance of an external and internal DSL. Based on our experience and on an analysis of the DSL engineering field, the vision that we foresee for the future of software languages is their ability to be self-adaptable to the most appropriate shape (including the corresponding integrated development environment) according to a particular usage or task. We call metamorphic DSL such a language, able to change from one shape to another shape.},
booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
pages = {243–253},
numpages = {11},
keywords = {domain-specific languages, metamorphic, programming},
location = {Portland, Oregon, USA},
series = {Onward! 2014}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Component-based software development, Component-based software engineering, Software component, Systematic mapping study}
}

@article{10.1007/s00450-011-0202-0,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {A quality driven extension to the QVT-relations transformation language},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-011-0202-0},
doi = {10.1007/s00450-011-0202-0},
abstract = {An emerging approach to software development is Model Driven Software Development 				(MDSD). It shifts the focus from source code to models, aims at cost reduction, risk 				mitigation, and eases the engineering of complex applications. System models can be 				used in the early development stages to verify certain relevant properties, such as 				performance, before source code is available and problems become hard and costly to 				solve. The present status of Model Driven Engineering (MDE) is still far from this 				ideal situation. A well-known problem is feedback provisioning, which arises when 				different solutions for the same design problem exist. An approach for feedback 				provisioning automation leverages model transformations, which glue together models 				in an MDSD setting, encapsulate the design rationale, and promote knowledge reuse 				and solutions otherwise available only to experienced engineers. In this article we 				present QVTR2, our solution to the feedback problem. 					QVTR2 is an extension of the QVT-Relations language 				with constructs to express design alternatives, their impact on non-functional 				metrics, and how to evaluate them and guide the engineers in the selection of the 				most appropriate solution. We demonstrate the effectiveness of our solution by using 				the QVTR2 engine to perform a modified version of the 				standard UML-to-RDBMS transformation in the 				context of a real e-commerce application, and by showing how we can guide a 				non-expert engineer in the selection of a solution that satisfies given performance 				requirements.},
journal = {Comput. Sci.},
month = feb,
pages = {1–20},
numpages = {20},
keywords = {Feedback provisioning, Model driven quality prediction, Model driven software development, Model transformations}
}

@inproceedings{10.1145/3319008.3319015,
author = {Fu, Changlan and Zhang, He and Huang, Xin and Zhou, Xin and Li, Zhi},
title = {A Review of Meta-ethnographies in Software Engineering},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319015},
doi = {10.1145/3319008.3319015},
abstract = {Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR). Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines. One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs. Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE. Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs. We carried out an empirical inquiry by integrating SLR and confirmatory email survey. Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it. Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs. Our study reveals that meta-ethnography is a suitable method to SE research. We discuss challenges and propose recommendations of adopting meta-ethnography in SE. Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {68–77},
numpages = {10},
keywords = {meta-ethnography, qualitative research synthesis, systematic (literature) review},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1145/2517208.2517213,
author = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
title = {A comparison of product-based, feature-based, and family-based type checking},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517213},
doi = {10.1145/2517208.2517213},
abstract = {Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {115–124},
numpages = {10},
keywords = {feature-oriented programming, fuji, product-line analysis, type checking},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1016/S1877-0509(20)30921-2,
title = {Contents},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(20)30921-2},
doi = {10.1016/S1877-0509(20)30921-2},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–xvi},
numpages = {14}
}

@inproceedings{10.1145/2556624.2556627,
author = {B\"{u}rdek, Johannes and Lity, Sascha and Lochau, Malte and Berens, Markus and Goltz, Ursula and Sch\"{u}rr, Andy},
title = {Staged configuration of dynamic software product lines with complex binding time constraints},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556627},
doi = {10.1145/2556624.2556627},
abstract = {Dynamic software product lines (DSPL) constitute a promising approach for developing highly-configurable, runtime-adaptive systems in a feature-oriented way. A DSPL integrates both variability in time and space in a unified conceptual framework. For this, domain features are equipped with additional binding time information to distinguish between static configuration parameters and dynamically (re-) configurable features. Until now, little support exists to specify and validate staged (re-)configuration semantics for DSPLs in a concise way. In this paper, we propose conservative extensions to domain feature models comprising variable feature binding times together with different kinds of binding time constraints. Those extensions are motivated by a real-world industrial case study from the automation engineering domain. Our implementation performs a model transformation into plain feature models treatable by corresponding state-of-the-art analysis tools. We conducted an evaluation of our approach concerning the case study.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {8},
keywords = {dynamic software product lines, extended feature models, industrial case study, staged configuration},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/800002.800507,
author = {Gaffney, John E.},
title = {Machine Instruction Count Program},
year = {1982},
isbn = {089791077X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800002.800507},
doi = {10.1145/800002.800507},
abstract = {The Machine Instruction Count Program (MIC Program) was originally developed in 1978 to produce 'operator' and 'operand' counts of object programs written for the AN/UYK-7 military computer. In 1981, its capability was expanded so that it could apply to the AN/UYS-1 (or “Advanced Signal Processor”) military computer. The former machine, made by UNIVAC, hosts the IBM-developed software for the sonar and defensive weapons system/command system for the TRIDENT missile launching submarine and the software for the sonar for the new Los Angeles-class attack submarines. The second machine, made by IBM, is incorporated into several military systems including the LAMPS anti-submarine warfare system.The MIC program has been applied to collect a large amount of data about programs written for the AN/UYK-7 and AN/UYS-1 computers. From these data, various of the well-known software 'metrics'(1) such as 'volume', 'language level', and 'difficulty' have been calculated. Some of the results obtained have been reported in the literature (3,4). Probably, the most significant practical use of these data, so far, has been the development of formulas for use in the estimation of the amount of code to be written(2,5) as a function of measures of the requirements that they are to implement or the (top-level) design that they are to implement.},
booktitle = {Selected Papers of the 1982 ACM SIGMETRICS Workshop on Software Metrics: Part 1},
pages = {72–79},
numpages = {8},
location = {Seattle, Washington, USA},
series = {SCORE '82}
}

@article{10.1016/j.scico.2021.102694,
author = {Liebrenz, Timm and Herber, Paula and Glesner, Sabine},
title = {Service-oriented decomposition and verification of hybrid system models using feature models and contracts},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {211},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102694},
doi = {10.1016/j.scico.2021.102694},
journal = {Sci. Comput. Program.},
month = nov,
numpages = {25},
keywords = {Hybrid systems, Compositional verification, Theorem proving, Model-driven development}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and K\"{a}stner, Christian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {PLA model, product-line analysis, software product lines},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{10.1007/s11277-015-2986-x,
author = {Alansi, Mohammed and Elshafiey, Ibrahim and Al-Sanie, Abdulhameed and Mabrouk, Ahmed},
title = {FPGA Implementation of Multi-User Detection Genetic Algorithm Tool for SDMA-OFDM Systems},
year = {2016},
issue_date = {Feb 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {86},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-015-2986-x},
doi = {10.1007/s11277-015-2986-x},
abstract = {Robust multi-user detection (MUD) methods based on space division multiple access (SDMA) techniques are essential to efficiently exploit the electromagnetic spectrum. In this paper, an adaptive Genetic Algorithm-based tool for SDMA-OFDM Systems (GASOS) is developed to improve the performance and computational complexity in cases of fully-loaded and overloaded multi-user scenarios. The data flow in GASOS is appropriate in pipelining and parallelization to reduce operational time. A new GASOS-based MUD hardware design for SDMA-OFDM systems is proposed using FPGA architecture. The design details are presented together with their planned operational modules. Resource utilization is optimized, and the total number of clock cycles required is found to be 15 initially, in addition to one clock cycle per member of algorithm population. A clock frequency of 100 MHz is used and implementation is carried out on Xilinx® Virtex-6 FPGA, built in the development platform ML605 edition with JTAG Hardware Co-simulation. According to the results obtained from the developed algorithm and implementation tools, a high number of users can be physically possible and provided with support. Real-time based implementation of MUD systems has the potential to play a major role in next-generation communication systems.},
journal = {Wirel. Pers. Commun.},
month = feb,
pages = {1241–1263},
numpages = {23},
keywords = {FPGA, Genetic Algorithms, MUD, SDMA and OFDM systems, Xilinx}
}

@inproceedings{10.1145/3361525.3361544,
author = {Ni, Xiang and Schneider, Scott and Pavuluri, Raju and Kaus, Jonathan and Wu, Kun-Lung},
title = {Automating Multi-level Performance Elastic Components for IBM Streams},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361544},
doi = {10.1145/3361525.3361544},
abstract = {Streaming applications exhibit abundant opportunities for pipeline parallelism, data parallelism and task parallelism. Prior work in IBM Streams introduced an elastic threading model that sought the best performance by automatically tuning the number of threads. In this paper, we introduce the ability to automatically discover where that threading model is profitable. However this introduces a new challenge: we have separate performance elastic mechanisms that are designed with different objectives, leading to potential negative interactions and unintended performance degradation. We present our experiences in overcoming these challenges by showing how to coordinate separate but interfering elasticity mechanisms to maxmize performance gains with stable and fast parallelism exploitation. We first describe an elastic performance mechanism that automatically adapts different threading models to different regions of an application. We then show a coherent ecosystem for coordinating this threading model elasticty with thread count elasticity. This system is an online, stable multi-level elastic coordination scheme that adapts different regions of a streaming application to different threading models and number of threads. We implemented this multi-level coordination scheme in IBM Streams and demonstrated that it (a) scales to over a hundred threads; (b) can improve performance by an order of magnitude on two different processor architectures when an application can benefit from multiple threading models; and (c) achieves performance comparable to hand-optimized applications but with much fewer threads.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {163–175},
numpages = {13},
keywords = {Stream processing, elastic scheduling, runtime},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1007/978-3-030-27455-9_4,
author = {Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Farah, Paulo Roberto and Vergilio, Silvia Regina and Guizzo, Giovani},
title = {A Review of Ten Years of the Symposium on Search-Based Software Engineering},
year = {2019},
isbn = {978-3-030-27454-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27455-9_4},
doi = {10.1007/978-3-030-27455-9_4},
abstract = {The year 2018 marked the tenth anniversary of the Symposium on Search Based Software Engineering (SSBSE). In order to better understand the characteristics and evolution of papers published in SSBSE, this work reports results from a mapping study targeting the ten proceedings of SSBSE. Our goal is to identify and to analyze authorship collaborations, the impact and relevance of SSBSE in terms of citations, the software engineering areas commonly studied as well as the new problems recently solved, the computational intelligence techniques preferred by authors and the rigour of experiments conducted in the papers. Besides this analysis, we list some recommendations to new authors who envisage to publish their work in SSBSE. Despite of existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups.},
booktitle = {Search-Based Software Engineering: 11th International Symposium, SSBSE 2019, Tallinn, Estonia, August 31 – September 1, 2019, Proceedings},
pages = {42–57},
numpages = {16},
keywords = {Systematic mapping, SBSE, Bibliometric analysis},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/133057.133106,
author = {Altman, Eitan and Nain, Philippe},
title = {Closed-loop control with delayed information},
year = {1992},
isbn = {0897915070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/133057.133106},
doi = {10.1145/133057.133106},
abstract = {The theory of Markov Control Model with Perfect State Information (MCM-PSI) requires that the current state of the system is known to the decision maker at decision instants. Otherwise, one speaks of Markov Control Model with Imperfect State Information (MCM-ISI). In this article, we introduce a new class of MCM-ISI, where the information on the state of the system is delayed. Such an information structure is encountered, for instance, in high-speed data networks.In the first part of this article, we show that by enlarging the state space so as to include the last known state as well as all the decisions made during the travel time of the information, we may reduce a MCM-ISI to a MCM-PSI. In the second part of this paper, this result is applied to a flow control problem.  Considered is a discrete time queueing model with Bernoulli arrivals and geometric services, where the intensity of the arrival stream is controlled. At the beginning of slot t+1, t=0,1,2,…, the decision maker has to select the probability of having one arrival in the current time slot from the set {p1, p2}, 0 ≤ p2 &lt; p1 ≤ 1, only on the basis of the queue-length and action histories in [0, t]. The aim is to optimize a discounted throughput/delay criterion. We show that there exists an optimal policy of a threshold type, where the threshold is seen to depend on the last  action.},
booktitle = {Proceedings of the 1992 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {193–204},
numpages = {12},
location = {Newport, Rhode Island, USA},
series = {SIGMETRICS '92/PERFORMANCE '92}
}

@inproceedings{10.1145/133057.133099,
author = {Lui, John C. S. and Muntz, Richard R.},
title = {Algorithmic approach to bounding the mean response time of a minimum expected delay routing system},
year = {1992},
isbn = {0897915070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/133057.133099},
doi = {10.1145/133057.133099},
abstract = {In this paper we present an algorithmic approach to bounding the mean response time of a multi-server system in which the minimum expected delay routing policy issued, i.e., an arriving job will join the queue which has the minimal expected value of unfinished work. We assume the queueing system to have K servers, each with an infinite capacity queue. The arrival process is Poisson with parameter λ, and the service time of server i is exponentially distributed with mean 1/μi, 1 ≤ i ≤ K. The computation algorithm we present allows one to tradeoff accuracy and computational cost. Upper and lower bounds on the expected response time and expected number of customers  are computed; the spread between the bounds can be reduced with additional space and time complexity. Examples are presented which illustrate the excellent relative accuracy attainable with relatively little computation.},
booktitle = {Proceedings of the 1992 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {140–151},
numpages = {12},
location = {Newport, Rhode Island, USA},
series = {SIGMETRICS '92/PERFORMANCE '92}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3001913.3001917,
author = {Ravi, Prashant and Syam, Uma and Kapre, Nachiket},
title = {Preventive Detection of Mosquito Populations using Embedded Machine Learning on Low Power IoT Platforms},
year = {2016},
isbn = {9781450346498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001913.3001917},
doi = {10.1145/3001913.3001917},
abstract = {We can accurately detect mosquito species with 80% accuracy using frequency spectrum analysis of insect wing-beat patterns when mapped to low-power embedded/IoT hardware. We combine energy-efficient hardware acceleration optimizations with algorithmic tuning of signal processing and machine-learning routines to deliver a platform for insect classification. The use of low power accelerator blocks in cheap embedded boards such as the Raspberry Pi 3 and Intel Edison, along with performance tuning of the software implementations enable a competitive implementation of mosquito classification task on standard datasets. Our approach demonstrates a concrete application of embedding intelligence in edge devices for reducing system-level energy needs instead of simply uploading sensory data directly to the cloud for post-processing. For the mosquito classification task, we are able to deliver classification accuracies as high as 80% with Intel Edison processing times as low as 5 ms per set of 8K audio samples and an energy use of 5 mJ per sample (2 months of continuous non-stop use on an AA battery with 2000 mAh capacity or longer depending on insect activity). We envision a network of connected sensors and embedded/IoT platforms deployed in vulnerable such as construction sites, mines, areas of known mosquito activity, ponds, riverfronts, or other areas with standing water bodies. In our experiments, targeting a 20% packet loss rate, we observed the ad-hoc WiFi range for mesh networks using the Raspberry Pi 3 boards to be 14 m while the Photon board connecting to infrastructure WiFi router nodes can stretch this to 35 m.},
booktitle = {Proceedings of the 7th Annual Symposium on Computing for Development},
articleno = {3},
numpages = {10},
location = {Nairobi, Kenya},
series = {ACM DEV '16}
}

@inproceedings{10.1007/978-3-319-26844-6_32,
author = {Brink, Christopher and Heisig, Philipp and Sachweh, Sabine},
title = {Using Cross-Dependencies During Configuration of System Families},
year = {2015},
isbn = {9783319268439},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26844-6_32},
doi = {10.1007/978-3-319-26844-6_32},
abstract = {Nowadays, the automotive industry uses software product lines to support the management and maintenance of software variants. However, the development of mechatronic systems includes not merely software, but also other system parts like operating system, hardware or even mechanical parts. We call a combination of these system parts a system family SF. This combination raises the question how different variable system parts can be modeled and used for a combined configuration in a flexible way. We argue that a modeling process should combine all of these system parts, while the product configuration has to consider dependencies between them. Based on our previous work, we address this question and discuss dependencies between different system parts.},
booktitle = {Proceedings of the 16th International Conference on Product-Focused Software Process Improvement - Volume 9459},
pages = {439–452},
numpages = {14},
keywords = {Dependencies, Feature models, Hardware/software, Product lines, System families, Systems},
location = {Bolzano, Italy},
series = {PROFES 2015}
}

@inproceedings{10.1007/978-3-642-30982-3_7,
author = {Petriu, Dorina C. and Alhaj, Mohammad and Tawhid, Rasha},
title = {Software performance modeling},
year = {2012},
isbn = {9783642309816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30982-3_7},
doi = {10.1007/978-3-642-30982-3_7},
abstract = {Ideally, a software development methodology should include both the ability to specify non-functional requirements and to analyze them starting early in the lifecycle; the goal is to verify whether the system under development would be able to meet such requirements. This chapter considers quantitative performance analysis of UML software models annotated with performance attributes according to the standard "UML Profile for Modeling and Analysis of Real-Time and Embedded Systems" (MARTE). The chapter describes a model transformation chain named PUMA (Performance by Unified Model Analysis) that enables the integration of performance analysis in a UML-based software development process, by automating the derivation of performance models from UML+MARTE software models, and by facilitating the interoperability of UML tools and performance tools. PUMA uses an intermediate model called "Core Scenario Model" (CSM) to bridge the gap between different kinds of software models accepted as input and different kinds of performance models generated as output. Transformation principles are described for transforming two kinds of UML behaviour representation (sequence and activity diagrams) into two kinds of performance models (Layered Queueing Networks and stochastic Petri nets). Next, PUMA extensions are described for two classes of software systems: service-oriented architecture (SOA) and software product lines (SPL).},
booktitle = {Proceedings of the 12th International Conference on Formal Methods for the Design of Computer, Communication, and Software Systems: Formal Methods for Model-Driven Engineering},
pages = {219–262},
numpages = {44},
location = {Bertinoro, Italy},
series = {SFM'12}
}

@inproceedings{10.1145/2307636.2307676,
author = {Borchert, Christoph and Lohmann, Daniel and Spinczyk, Olaf},
title = {CiAO/IP: a highly configurable aspect-oriented IP stack},
year = {2012},
isbn = {9781450313018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307636.2307676},
doi = {10.1145/2307636.2307676},
abstract = {Internet protocols are constantly gaining relevance for the domain of mobile and embedded systems. However, building complex network protocol stacks for small resource-constrained devices is more than just porting a reference implementation. Due to the cost pressure in this area especially the memory footprint has to be minimized. Therefore, embedded TCP/IP implementations tend to be statically configurable with respect to the concrete application scenario. This paper describes our software engineering approach for building CiAO/IP - a tailorable TCP/IP stack for small embedded systems, which pushes the limits of static configurability while retaining source code maintainability. Our evaluation results show that CiAO/IP thereby outperforms both lwIP and uIP in terms of code size (up to 90% less than uIP), throughput (up to 20% higher than lwIP), energy consumption (at least 40% lower than uIP) and, most importantly, tailorability.},
booktitle = {Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services},
pages = {435–448},
numpages = {14},
keywords = {aop, aspect-oriented programming, aspectc++, embedded systems, internet protocol, network protocol stacks, operating systems, tcp/ip},
location = {Low Wood Bay, Lake District, UK},
series = {MobiSys '12}
}

@inproceedings{10.1145/2110147.2110167,
author = {Czarnecki, Krzysztof and Gr\"{u}nbacher, Paul and Rabiser, Rick and Schmid, Klaus and W\k{a}sowski, Andrzej},
title = {Cool features and tough decisions: a comparison of variability modeling approaches},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110167},
doi = {10.1145/2110147.2110167},
abstract = {Variability modeling is essential for defining and managing the commonalities and variabilities in software product lines. Numerous variability modeling approaches exist today to support domain and application engineering activities. Most are based on feature modeling (FM) or decision modeling (DM), but so far no systematic comparison exists between these two classes of approaches. Over the last two decades many new features have been added to both FM and DM and it is tough to decide which approach to use for what purpose. This paper clarifies the relation between FM and DM. We aim to systematize the research field of variability modeling and to explore potential synergies. We compare multiple aspects of FM and DM ranging from historical origins and rationale, through syntactic and semantic richness, to tool support, identifying commonalities and differences. We hope that this effort will improve the understanding of the range of approaches to variability modeling by discussing the possible variations. This will provide insights to users considering adopting variability modeling in practice and to designers of new languages, such as the new OMG Common Variability Language.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {173–182},
numpages = {10},
keywords = {decision modeling, feature modeling, product lines, variability modeling},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.5555/2018027.2018039,
author = {Johnsen, Andreas and Lundqvist, Kristina},
title = {Developing dependable software-intensive systems: AADL vs. EAST-ADL},
year = {2011},
isbn = {9783642213373},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Dependable software-intensive systems, such as embedded systems for avionics and vehicles are often developed under severe quality, schedule and budget constraints. As the size and complexity of these systems dramatically increases, the architecture design phase becomes more and more significant in order to meet these constraints. The use of Architecture Description Languages (ADLs) provides an important basis for mutual communication, analysis and evaluation activities. Hence, selecting an ADL suitable for such activities is of great importance. In this paper we compare and investigate the two ADLs - AADL and EASTADL. The level of support provided to developers of dependable software-intensive systems is compared, and several critical areas of the ADLs are highlighted. Results of using an extended comparison framework showed many similarities, but also one clear distinction between the languages regarding the perspectives and the levels of abstraction in which systems are modeled.},
booktitle = {Proceedings of the 16th Ada-Europe International Conference on Reliable Software Technologies},
pages = {103–117},
numpages = {15},
keywords = {AADL, EAST-ADL, architecture description languages, dependable systems, software-intensive systems},
location = {Edinburgh, UK},
series = {Ada-Europe'11}
}

@article{10.1016/j.rcim.2019.101892,
author = {Zhang, Kai and Qu, Ting and Zhou, Dajian and Jiang, Hongfei and Lin, Yuanxin and Li, Peize and Guo, Hongfei and Liu, Yang and Li, Congdong and Huang, George Q},
title = {Digital twin-based opti-state control method for a synchronized production operation system},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2019.101892},
doi = {10.1016/j.rcim.2019.101892},
journal = {Robot. Comput.-Integr. Manuf.},
month = jun,
numpages = {15},
keywords = {Optimal State, Digital Twin, Resilient Control, Synchronized Control, Uncertainties}
}

@inproceedings{10.5555/1251990.1253400,
author = {Ramanathan, N. and Yarvis, M. and Chhabra, J. and Kushalnagar, N. and Krishnamurthy, L. and Estrin, D.},
title = {A stream-oriented power management protocol for low duty cycle sensor network applications},
year = {2005},
isbn = {0780392469},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Most power management protocols are packet-based and optimized for applications with mostly asynchronous (i.e. unexpected) traffic. We present AppSleep, a stream-oriented power management protocol for latency tolerant sensor network applications. For this class of applications, AppSleep demonstrates an over 3/spl times/ lifetime gain over B-MAC and SMAC. AppSleep leverages application characteristics in order to take advantage of periods of high latency tolerance to put the network to sleep for extended periods of time, while still facilitating low latency responses when required. AppSleep also gives applications the flexibility to efficiently and effectively trade latency for energy when desired, and enables energy efficient multi-fragment unicast communication by only keeping the active route awake. We also present Adaptive AppSleep, an application driven addition to AppSleep which supports varying latency requirements while still maximizing energy efficiency. Our evaluation demonstrates that for an overlooked class of applications, stream-oriented power management protocols such as AppSleep outperform packet-based protocols such as B-MAC and S-MAC.},
booktitle = {Proceedings of the 2nd IEEE Workshop on Embedded Networked Sensors},
pages = {53–61},
numpages = {9},
series = {EmNets '05}
}

@article{10.1016/j.future.2014.12.002,
author = {Weinreich, Rainer and Groher, Iris and Miesbauer, Cornelia},
title = {An expert survey on kinds, influence factors and documentation of design decisions in practice},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2014.12.002},
doi = {10.1016/j.future.2014.12.002},
abstract = {Support for capturing architectural knowledge has been identified as an important research challenge. As the basis for an approach to recovering design decisions and capturing their rationale, we performed an expert survey in practice to gain insights into the different kinds, influence factors, and sources for design decisions and also into how they are currently captured in practice. The survey was conducted with 25 software architects, software team leads, and senior developers from 22 different companies in 10 different countries with more than 13 years of experience in software development on average. The survey confirms earlier work by other authors on design decision classification and influence factors, and also identifies additional kinds of decisions and influence factors not mentioned in previous work. In addition, we gained insight into the practice of capturing, the relative importance of different decisions and influence factors, and into potential sources for recovering decisions. We present results of a qualitative expert survey on design decisions in practice.We examine design decision classification, documentation, and influence factors.We collect architects' experiences in decision making and documentation.We provide recommendations for potential improvements and research directions based on the results of our study.Results are compared to literature and similar studies.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {145–160},
numpages = {16},
keywords = {Design decision classification, Design decision documentation, Design decision influence factors, Design decisions, Software architecture knowledge management}
}

@inproceedings{10.1007/978-3-642-29645-1_22,
author = {Mussbacher, Gunter and Al Abed, Wisam and Alam, Omar and Ali, Shaukat and Beugnard, Antoine and Bonnet, Valentin and Br\ae{}k, Rolv and Capozucca, Alfredo and Cheng, Betty H. C. and Fatima, Urooj and France, Robert and Georg, Geri and Guelfi, Nicolas and Istoan, Paul and J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Klein, Jacques and L\'{e}zoray, Jean-Baptiste and Malakuti, Somayeh and Moreira, Ana and Phung-Khac, An and Troup, Lucy},
title = {Comparing six modeling approaches},
year = {2011},
isbn = {9783642296444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29645-1_22},
doi = {10.1007/978-3-642-29645-1_22},
abstract = {While there are many aspect-oriented modeling (AOM) approaches, from requirements to low-level design, it is still difficult to compare them and know under which conditions different approaches are most applicable. This comparison, however, is crucially important to unify existing AOM and more traditional object-oriented modeling (OOM) approaches and to generalize individual approaches into a comprehensive end-to-end method. Such a method does not yet exist. This paper reports on work done at the inaugural Comparing Modeling Approaches (CMA) workshop towards the goal of identifying potential comprehensive methodologies: (i) a common, focused case study for six modeling approaches, (ii) a set of criteria applied to each of the six approaches, and (iii) the assessment results.},
booktitle = {Proceedings of the 2011th International Conference on Models in Software Engineering},
pages = {217–243},
numpages = {27},
keywords = {aspect-oriented modeling, case study, comparison criteria, object-oriented modeling},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Feature interactions, Highly configurable software systems, Machine learning, Performance prediction, Performance-influence models, Software product lines, Variability}
}

@inproceedings{10.1007/978-3-642-54804-8_7,
author = {Kowal, Matthias and Schaefer, Ina and Tribastone, Mirco},
title = {Family-Based Performance Analysis of Variant-Rich Software Systems},
year = {2014},
isbn = {9783642548031},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-54804-8_7},
doi = {10.1007/978-3-642-54804-8_7},
abstract = {We study models of software systems with variants that stem from a specific choice of configuration parameters with a direct impact on performance properties. Using UML activity diagrams with quantitative annotations, we model such systems as a product line. The efficiency of a product-based evaluation is typically low because each product must be analyzed in isolation, making difficult the re-use of computations across variants. Here, we propose a family-based approach based on symbolic computation. A numerical assessment on large activity diagrams shows that this approach can be up to three orders of magnitude faster than product-based analysis in large models, thus enabling computationally efficient explorations of large parameter spaces.},
booktitle = {Proceedings of the 17th International Conference on Fundamental Approaches to Software Engineering - Volume 8411},
pages = {94–108},
numpages = {15}
}

@inproceedings{10.1145/1134650.1134678,
author = {Pandey, Raju and Wu, Jeffrey},
title = {BOTS: a constraint-based component system for synthesizing scalable software systems},
year = {2006},
isbn = {159593362X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134650.1134678},
doi = {10.1145/1134650.1134678},
abstract = {Embedded application developers create applications for a wide range of devices with different resource constraints. Developers want to maximize the use of the limited resources available on the device while still not exceeding the capabilities of the device. To do this, the developer must be able to scale his software for different platforms. In this paper, we present a software engineering methodology that automatically scales software to different platforms. We intend to have the application developer write high level functional specifications of his software and have tools that automatically scale the underlying runtime. These tools will use the functional and non-functional constraints of both the hardware and client application to produce an appropriate runtime. Our initial results show that the proposed approach can scale operating systems and virtual machines that satisfy the constraints of varying hardware/application combinations.},
booktitle = {Proceedings of the 2006 ACM SIGPLAN/SIGBED Conference on Language, Compilers, and Tool Support for Embedded Systems},
pages = {189–198},
numpages = {10},
keywords = {components, constraints, embedded systems, generative programming, runtime systems, wireless sensor networks},
location = {Ottawa, Ontario, Canada},
series = {LCTES '06}
}

@article{10.1109/92.920819,
author = {Kim, Suhwan and Papaefthymiou, Marios C.},
title = {True single-phase adiabatic circuitry},
year = {2001},
issue_date = {Feb. 2001},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {9},
number = {1},
issn = {1063-8210},
url = {https://doi.org/10.1109/92.920819},
doi = {10.1109/92.920819},
abstract = {Dynamic logic families that rely on energy recovery to achieve low energy dissipation control the flow of data through gate cascades using multiphase clocks. Consequently, they typically use multiple clock generators and can exhibit increased energy consumption on their clock distribution networks. Moreover, they are not attractive for high-speed design due to their high complexity and clock skew management problems. In this paper, we present TSEL, the first energy-recovering (a.k.a. adiabatic) logic family that operates with a single-phase sinusoidal clocking scheme. We also present SCAL, a source-coupled variant of TSEL with improved supply voltage scalability and energy efficiency. Optimal performance under any operating conditions is achieved in SCAL using a tunable current source in each gate. TSEL and SCAL outperform previous adiabatic logic families in terms of energy efficiency and operating speed. In layout-based simulations with 0.5 /spl mu/m standard CMOS process parameters, 8-bit carry-lookahead adders (CLAs) in TSEL and SCAL function correctly for operating frequencies exceeding 200 MHz. In comparison with corresponding CLAs in alternative logic styles that operate at minimum supply voltages, CLAs designed in our single-phase adiabatic logic families are more energy efficient across a broad range of operating frequencies. Specifically, for clock rates ranging from 10 to 200 MHz, our andbit SCAL CLAs are 1.5 to 2.5 times more energy efficient than corresponding adders developed in PAL and 2N2P and 2.0 to 5.0 times less dissipative than their purely combinational or pipelined CMOS counterparts.},
journal = {IEEE Trans. Very Large Scale Integr. Syst.},
month = feb,
pages = {52–64},
numpages = {13},
keywords = {adiabatic circuits, carry-lookahead adder, dynamic circuits, energy recovery logic, low-energy computing, true single-phase clocking}
}

@inproceedings{10.1145/55595.55613,
author = {Irgon, A. E. and Dragoni, A. H. and Huleatt, T. O.},
title = {FAST: A large scale expert system for application and system software performance tuning},
year = {1988},
isbn = {0897912543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/55595.55613},
doi = {10.1145/55595.55613},
booktitle = {Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems},
pages = {151–156},
numpages = {6},
location = {Santa Fe, New Mexico, USA},
series = {SIGMETRICS '88}
}

@article{10.1155/2010/158602,
author = {Liu, Hanyu and Akoglu, Ali},
title = {Timing-driven nonuniform depopulation-based clustering},
year = {2010},
issue_date = {January 2010},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2010},
issn = {1687-7195},
url = {https://doi.org/10.1155/2010/158602},
doi = {10.1155/2010/158602},
abstract = {Low-cost FPGAs have comparable number of Configurable Logic Blocks (CLBs) with respect to resource-rich FPGAs but have much less routing tracks. For CAD tools, this situation increases the difficulty of successfully mapping a circuit into the low-cost FPGAs. Instead of switching to resource-rich FPGAs, the designers could employ depopulation-based clustering techniques which underuse CLBs, hence improve routability by spreading the logic over the architecture. However, all depopulation-based clustering algorithms to this date increase critical path delay. In this paper, we present a timing-driven nonuniform depopulation-based clustering technique, T-NDPack, that targets critical path delay and channel width constraints simultaneously. T-NDPack adjusts the CLB capacity based on the criticality of the Basic Logic Element (BLE). Results show that T-NDPack reduces minimum channel width by 11.07% while increasing the number of CLBs by 13.28% compared to T-VPack. More importantly, T-NDPack decreases critical path delay by 2.89%.},
journal = {Int. J. Reconfig. Comput.},
month = jan,
articleno = {3},
numpages = {1}
}

@article{10.1515/cait-2015-0015,
author = {Stoilov, Todor and Stoilova, Krasimira and Papageorgiou, Markos and Papamichail, Ioannis},
title = {Bi-Level Optimization in a Transport Network},
year = {2015},
issue_date = {4 2015},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {15},
number = {5},
issn = {1314-4081},
url = {https://doi.org/10.1515/cait-2015-0015},
doi = {10.1515/cait-2015-0015},
abstract = {AbstractThis paper applies a bi-level formalism for the optimal control of an urban transportation network. The well known store-and-forward model in traffic control is utilized in order to increase the control space of the optimization problem. Mainly, the store-and-forward models apply the split as a control argument, assuming the traffic light cycle as a constant parameter. The paper shows that by using a bi-level formalism the control problem can be defined within increased control space comprising both the split and the cycle. Both are found as optimal solutions of a bi-level optimization problem.},
journal = {Cybern. Inf. Technol.},
month = apr,
pages = {37–49},
numpages = {13},
keywords = {Traffic control, autonomic computing, bi-level optimization, computer systems, multilevel systems}
}

@inproceedings{10.5555/3320516.3320904,
author = {Wenzel, Sigrid and Peter, Tim and Stoldt, Johannes and Schlegel, Andreas and Uhlig, Tobias and J\'{o}svai, J\'{a}nos},
title = {Considering energy in the simulation of manufacturing systems},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {In recent years, environmental aspects became one of the key interests in manufacturing. Accordingly, simulation studies had to include factors like energy or emissions. This paper aims to provide a comprehensive introduction to the state of the art in modeling of energy and emissions in simulation of manufacturing systems. We review existing literature to develop a landscape of common approaches and best practices. Typical goals and objectives of the reviewed simulation projects are summarized. Furthermore, we will evaluate the structure and life cycle phases of the examined manufacturing systems and look into the requirements and implementation of respective simulation studies. Finally, we will discuss open questions and future trends in this field of research.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3275–3286},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/1868688.1868690,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Apel, Sven},
title = {Automating energy optimization with features},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868690},
doi = {10.1145/1868688.1868690},
abstract = {Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (e.g., feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {2–9},
numpages = {8},
keywords = {energy consumption, feature-oriented programming, software product lines},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {AADL, ADL, MARTE, MDA, MDD, MDE, TLA+, UML, embedded, real-time, xUML}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Personalized Recommendations, Product-Line Configuration, Recommenders, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.5555/2045753.2045804,
author = {Zhou, Jingang and Zhao, Dazhe and Liu, Jiren},
title = {A domain specific language for interactive enterprise application development},
year = {2011},
isbn = {9783642239816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Web-based enterprise applications (EAs) have become the mainstream for business systems; however, there are enormous challenges for EAs development to meet the software quality and delivery deadline. In this paper, we propose a domain specific language, called WL4EA, which combines components with generative reuse and targets for popular application frameworks (or platform) and supports high interactivity. With WL4EA, an EA can be declaratively specified as some sets of entities, views, business objects, and data access objects. Such language elements will be composed according to known EA architecture and patterns. Such a DSL and code generation can lower the development complexity and error proneness and improve efficiency.},
booktitle = {Proceedings of the 2011 International Conference on Web Information Systems and Mining - Volume Part II},
pages = {351–360},
numpages = {10},
keywords = {domain specific language, enterprise application, generative programming, web Application},
location = {Taiyuan, China},
series = {WISM'11}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2024436.2024438,
author = {Calinescu, Radu},
title = {When the requirements for adaptation and high integrity meet},
year = {2011},
isbn = {9781450308533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024436.2024438},
doi = {10.1145/2024436.2024438},
abstract = {Two classes of software that are notoriously difficult to develop on their own are rapidly merging into one. This will affect every key service that we rely upon in modern society, yet a successful merge is unlikely to be achievable using software development techniques specific to either class.This paper explains the growing demand for software capable of both self-adaptation and high integrity, and advocates the use of a collection of "@runtime" techniques for its development, operation and management. We summarise early research into the development of such techniques, and discuss the remaining work required to overcome the great challenge of self-adaptive high-integrity software.},
booktitle = {Proceedings of the 8th Workshop on Assurances for Self-Adaptive Systems},
pages = {1–4},
numpages = {4},
keywords = {high-integrity software, model checking, self-adaptive software},
location = {Szeged, Hungary},
series = {ASAS '11}
}

@article{10.1016/j.rcim.2016.03.002,
author = {Valente, A.},
title = {Reconfigurable industrial robots},
year = {2016},
issue_date = {October 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.03.002},
doi = {10.1016/j.rcim.2016.03.002},
abstract = {Industrial robots undergo design and re-configuration processes to target extremely challenging precision and reliability performance with agile and efficient architectures. The need for such features currently prevent the exploitation of reconfigurable robotics in manufacturing. The current work presents an approach to design and configure reconfigurable robots for the high precision manufacturing industry. The work proposes a configuration algorithm that enables the identification of the robot architectures and the related reconfigurability features by selecting the type and number of robot modules to be implemented over time in order to better accomplish a number of production requirements. Particularly, assuming the robot will work by utilising a finite set of robotic modules, the algorithm determines the set of modules to form the arm and the ones to be allocated in the robot storage for possible usage over time. Results show a number of benefits such as a robotic chain with customised reaching and degrees of freedom with a reduced cost by performing an accurate module selection and configuration; this should lead the robot users to prefer reconfigurable robots to commercial rigid catalogue solutions proposed by robot manufacturers. An approach to build and configure robotic chains for Reconfigurable Industrial Robots.The Problem Formulation is based on Stochastic Programming Technique to match future evolutions of production requirements.A real industrial case studies proofs the actual benefits coming from reconfigurable robotics in manufacturing.},
journal = {Robot. Comput.-Integr. Manuf.},
month = oct,
pages = {115–126},
numpages = {12},
keywords = {Reconfigurable Industrial Robotics, Stochastic programming}
}

@article{10.1007/s10664-015-9364-x,
author = {Passos, Leonardo and Teixeira, Leopoldo and Dintzner, Nicolas and Apel, Sven and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof and Borba, Paulo and Guo, Jianmei},
title = {Coevolution of variability models and related software artifacts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9364-x},
doi = {10.1007/s10664-015-9364-x},
abstract = {Variant-rich software systems offer a large degree of customization, allowing users to configure the target system according to their preferences and needs. Facing high degrees of variability, these systems often employ variability models to explicitly capture user-configurable features (e.g., systems options) and the constraints they impose. The explicit representation of features allows them to be referenced in different variation points across different artifacts, enabling the latter to vary according to specific feature selections. In such settings, the evolution of variability models interplays with the evolution of related artifacts, requiring the two to evolve together, or coevolve. Interestingly, little is known about how such coevolution occurs in real-world systems, as existing research has focused mostly on variability evolution as it happens in variability models only. Furthermore, existing techniques supporting variability evolution are usually validated with randomly-generated variability models or evolution scenarios that do not stem from practice. As the community lacks a deep understanding of how variability evolution occurs in real-world systems and how it relates to the evolution of different kinds of software artifacts, it is not surprising that industry reports existing tools and solutions ineffective, as they do not handle the complexity found in practice. Attempting to mitigate this overall lack of knowledge and to support tool builders with insights on how variability models coevolve with other artifact types, we study a large and complex real-world variant-rich software system: the Linux kernel. Specifically, we extract variability-coevolution patterns capturing changes in the variability model of the Linux kernel with subsequent changes in Makefiles and C source code. From the analysis of the patterns, we report on findings concerning evolution principles found in the kernel, and we reveal deficiencies in existing tools and theory when handling changes captured by our patterns.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1744–1793},
numpages = {50},
keywords = {Variability, Software product lines, Patterns, Linux, Evolution}
}

@article{10.1016/j.comnet.2019.07.013,
author = {Pimpinella, Andrea and Redondi, Alessandro E.C. and Cesana, Matteo},
title = {Walk this way! An IoT-based urban routing system for smart cities},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.07.013},
doi = {10.1016/j.comnet.2019.07.013},
journal = {Comput. Netw.},
month = oct,
numpages = {12},
keywords = {Temporal forecasting, Spatial interpolation, Urban routing, Smart cities, Internet of things}
}

@article{10.1016/j.jss.2012.10.013,
author = {Nakagawa, Elisa Y. and Antonino, Pablo O. and Becker, Martin and Maldonado, Jos\'{e} C. and Storf, Holger and Villela, Karina B. and Rombach, Dieter},
title = {Relevance and perspectives of AAL in Brazil},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.10.013},
doi = {10.1016/j.jss.2012.10.013},
abstract = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and AAL software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing AAL systems. However, in spite of its relevance, AAL has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area.},
journal = {J. Syst. Softw.},
month = apr,
pages = {985–996},
numpages = {12},
keywords = {Reference architecture, Population aging, Ambient Assisted Living (AAL), AAL platform}
}

@inproceedings{10.1145/2088876.2088879,
author = {Merle, Philippe and Rouvoy, Romain and Seinturier, Lionel},
title = {A reflective platform for highly adaptive multi-cloud systems},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088879},
doi = {10.1145/2088876.2088879},
abstract = {Cloud platforms are increasingly used for hosting a broad diversity of services from traditional e-commerce applications to interactive web-based IDEs. However, we observe that the proliferation of offers by Cloud vendors raises several challenges. Developers will not only have to deploy applications for a specific Cloud, but will also have to consider migrating services from one cloud to another, and to manage applications spanning multiple Clouds. In this paper, we therefore report on a first experiment we conducted to build a multi-Cloud system on top of thirteen existing IaaS/PaaS. From this experiment, we advocate for two dimensions of adaptability---design and execution time---that applications for such systems require to exhibit. Finally, we propose a roadmap for future multi-Cloud systems.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.5555/1768904.1768924,
author = {Regnell, Bj\"{o}rn and H\"{o}st, Martin and Svensson, Richard Berntsson},
title = {A quality performance model for cost-benefit analysis of non-functional requirements applied to the mobile handset domain},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In market-driven requirements engineering for platform-based development of embedded systems such as mobile phones, it is crucial to market success to find the right balance among competing quality aspects (aka nonfunctional requirements). This paper presents a conceptual model that incorporates quality as a dimension in addition to the cost and value dimensions used in prioritisation approaches for functional requirements. The model aims at supporting discussion and decision-making in early requirements engineering related to activities such as roadmapping, release planning and platform scoping. The feasibility and relevance of the model is initially validated through interviews with requirements experts in six cases that represent important areas in the mobile handset domain. The validation suggests that the model is relevant and feasible for this particular domain.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {277–291},
numpages = {15},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@article{10.1504/IJWGS.2012.051527,
author = {Charfi, Anis and Schmeling, Benjamin and Mezini, Mira},
title = {An aspect-oriented framework for specification and enforcement of non-functional concerns in WS-BPEL},
year = {2012},
issue_date = {January 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {4},
issn = {1741-1106},
url = {https://doi.org/10.1504/IJWGS.2012.051527},
doi = {10.1504/IJWGS.2012.051527},
abstract = {Web Service processes in WS-BPEL have several non-functional requirements such as security and reliable messaging. Although there are many WS-* specifications that address these concerns, their integration with WS-BPEL is still open. In this paper, we discuss these non-functional requirements and present a survey on the current support for their specification and enforcement in WS-BPEL engines. Moreover, we introduce an aspect-oriented container framework that uses a declarative deployment descriptor to specify the non-functional requirements. For the enforcement, aspects in AO4BPEL 2.0 are generated, which intercept the process execution and call dedicated middleware Web Services.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {386–424},
numpages = {39}
}

@inproceedings{10.1145/1973009.1973101,
author = {Pacull, Francois and Bertels, Koen and Danek, Martin and Urlini, Giulio},
title = {SMECY: smart multi-core embedded systems},
year = {2011},
isbn = {9781450306676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1973009.1973101},
doi = {10.1145/1973009.1973101},
abstract = {SMECY project is an ambitious European initiative involving 29 partners across 9 countries to enable Europe to have a leader role in multi-core domain by developing new programming technologies enabling the exploitation of architectures offering hundreds of cores. Multi-core technologies will rapidly provide to the parallel computing field improved performance, energy saving and cost reduction and will become of strategic value in winning market share in all areas of embedded systems. Given the need, SMECY lays the focus on targeting programming multi-core architecture for consumer electronics with efficient resources management. The first presentation describes the overall project while the two others are respectively dedicated to the multi-core platforms targeted in the project and the description of the tools constituting the bricks of the tool chains.},
booktitle = {Proceedings of the 21st Edition of the Great Lakes Symposium on Great Lakes Symposium on VLSI},
pages = {427–428},
numpages = {2},
keywords = {tool chain, compilation},
location = {Lausanne, Switzerland},
series = {GLSVLSI '11}
}

@article{10.1109/92.386232,
author = {Ko, Uming and Balsara, Poras T. and Lee, Wai},
title = {Low-power design techniques for high-performance CMOS adders},
year = {1995},
issue_date = {June 1995},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {3},
number = {2},
issn = {1063-8210},
url = {https://doi.org/10.1109/92.386232},
doi = {10.1109/92.386232},
abstract = {A high-performance adder is one of the most critical components of a processor which determines its throughput, as it is used in the ALU, the floating-point unit, and for address generation in case of cache or memory access. In this paper, low-power design techniques for various digital circuit families are studied for implementing high-performance adders, with the objective to optimize performance per watt or energy efficiency as well as silicon area efficiency. While the investigation is done using 100 MHz, 32 b carry lookahead (CLA) adders in a 0.6 /spl mu/m CMOS technology, most techniques presented here can also be applied to other parallel adder algorithms such as carry-select adders (CSA) and other energy efficient CMOS circuits. Among the techniques presented here, the double pass-transistor logic (DPL) is found to be the most energy efficient while the single-rail domino and complementary pass-transistor logic (CPL) result in the best performance and the most area efficient adders, respectively. The impact of transistor threshold voltage scaling on energy efficiency is also examined when the supply voltage is scaled from 3.5 V down to 1.0 V.&lt; &gt;},
journal = {IEEE Trans. Very Large Scale Integr. Syst.},
month = jun,
pages = {327–333},
numpages = {7},
keywords = {low power, high performance, digital CMOS, adder}
}

@article{10.1007/s11227-014-1192-z,
author = {Santos, Andr\'{e} C. and Cardoso, Jo\~{a}o M. and Diniz, Pedro C. and Ferreira, Diogo R. and Petrov, Zlatko},
title = {A DSL for specifying run-time adaptations for embedded systems: an application to vehicle stereo navigation},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {70},
number = {3},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-014-1192-z},
doi = {10.1007/s11227-014-1192-z},
abstract = {The traditional approach for specifying adaptive behavior in embedded applications requires developers to engage in error-prone programming tasks. This results in long design cycles and in the inherent inability to explore and evaluate a wide variety of alternative adaptation behaviors, critical for systems exposed to dynamic operational and situational environments. In this paper, we introduce a domain-specific language (DSL) for specifying and implementing run-time adaptable application behavior. We illustrate our approach using a real-life stereo navigation application as a case study, highlighting the impact and benefits of dynamically adapting algorithm parameters. The experiments reveal our approach effective, as such run-time adaptations are easily specified in a higher level by the DSL, and thus at a lower programming effort than when using a general-purpose language such as C.},
journal = {J. Supercomput.},
month = dec,
pages = {1218–1248},
numpages = {31},
keywords = {Stereo navigation, Run-time adaptations, Embedded systems, Domain-specific languages, Adaptable behavior}
}

@article{10.1016/j.scico.2020.102414,
author = {Pelliccione, Patrizio and Knauss, Eric and \r{A}gren, S. Magnus and Heldal, Rogardt and Bergenhem, Carl and Vinel, Alexey and Brunneg\r{a}rd, Oliver},
title = {Beyond connected cars: A systems of systems perspective},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {191},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2020.102414},
doi = {10.1016/j.scico.2020.102414},
journal = {Sci. Comput. Program.},
month = jun,
numpages = {21},
keywords = {Software architecture, Architecture framework, Automotive, Systems of systems, Software engineering}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@inproceedings{10.1145/1169086.1169088,
author = {Bencomo, N. and Blair, G. and Grace, P.},
title = {Models, reflective mechanisms and family-based systems to support dynamic configuration},
year = {2006},
isbn = {1595934235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1169086.1169088},
doi = {10.1145/1169086.1169088},
abstract = {Middleware platforms must satisfy an increasingly broad and variable set of requirements arising from the needs of both applications and underlying systems deployed in dynamically changing environments such as environment monitoring and disaster management. To meet these requirements, middleware platforms must offer a high degree of configurability at deployment time and runtime. At Lancaster we use reflection, components and component frameworks, and middleware families as the basis of our approach to develop dynamically configurable middleware platforms. In our approach, components and component frameworks provide structure, and reflection provides support for dynamic configuration and extensibility for run-time evolution and adaptation. This approach however has contributed to make the development and operation of middleware platforms even more complex. Middleware developers deal with a large number of variability decisions when planning (re)configurations and adaptations. This paper examines how Model-Driven Engineering (MDE), Domain Specific Languages (DSLs) and System Family Engineering can be used to improve the development of middleware families, systematically generating middleware configurations from high level descriptions. We present Genie, a DSL-based prototype development-tool that supports the specification, validation and generation of artefacts for component-based reflective middleware. In particular, this paper describes how the Genie toolkit improves the development of the Gridkit middleware through the modelling and automated generation of middleware policies; that remove the complexity of handling large number of runtime adaptation policies.},
booktitle = {Proceedings of the 1st Workshop on MOdel Driven Development for Middleware (MODDM '06)},
pages = {1–6},
numpages = {6},
keywords = {system families, reflective middleware, grid computing, MDE, DSL},
location = {Melbourne, Australia},
series = {MODDM '06}
}

@inproceedings{10.1145/1134285.1134293,
author = {Nehmer, J\"{u}rgen and Becker, Martin and Karshmer, Arthur and Lamm, Rosemarie},
title = {Living assistance systems: an ambient intelligence approach},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134293},
doi = {10.1145/1134285.1134293},
abstract = {In this paper, we present an integrated system concept for the living assistance domain based on ambient intelligence technology and discuss the resulting challenges for the software engineering discipline. Automated living assistance systems represent a promising approach for the prolongation of an independent and self-conducted life of handicapped and elderly people thereby, enhancing their quality of life and minimizing the need for manual social/medical care. It is demonstrated that living assistance systems must realize flexibility and adaptability at the algorithmic, architectural and human interface level to an extent unknown in present systems. The construction of robust, trustworthy living assistance systems is an extremely challenging task and requires novel approaches for dependable self-adapting software architectures, resource efficiency, and self-adapting multi-modal human-computer interfaces. The resulting consequences and challenges for the discipline of software engineering are outlined in this paper.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {43–50},
numpages = {8},
keywords = {home care systems, elderly care, assisted living, ambient intelligence, adaptivity},
location = {Shanghai, China},
series = {ICSE '06}
}

@inproceedings{10.5555/2927628.2927785,
author = {Dey, S. and Chandra, S.},
title = {Addressing computational and networking constraints to enable video streaming from wireless appliances},
year = {2005},
isbn = {0780393473},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Enabling real-time video streaming from a wireless appliance requires compute intensive video compression to be performed in real-time on the appliance before transmitting the data upstream. However, the tasks of real-time video encoding and streaming from the wireless appliances are challenging due to a) limited computational and battery resources, and b) limited and time-varying network bandwidth availability. In this paper, we present a technique for enabling real-time video compression and transmission from wireless appliances based on run-time video adaptation. We present an adaptation engine for dynamic selection of video compression parameters such that both the computational and the network bandwidth constraints are satisfied, while maximizing the end user's viewing quality. The algorithm is based on the analysis of the effect of different video compression parameters on computational and network resource usage, and the video quality. Since our approach is based on judicious selection of video compression parameters and does not require changes to the compression algorithm itself, it is applicable to a wide range of video compression standards. We have also developed an iPAQ-based end-to-end video streaming system to evaluate our approach. Experiments conducted on this test-bed indicate that our proposed technique achieves significant improvements in overall video quality under computation (up to 4/spl times/) and network bandwidth (/spl sim/3dB) constraints. We also show significant improvements in the energy efficiency as a result of adaptation.},
booktitle = {Proceedings of the Proceedings of the 2005 3rd Workshop on Embedded Systems for Real-Time Multimedia},
series = {ESTMED '05}
}

@inproceedings{10.5555/978-3-030-45234-6_fm,
title = {Front Matter},
year = {2020},
isbn = {978-3-030-45233-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Fundamental Approaches to Software Engineering: 23rd International Conference, FASE 2020, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25–30, 2020, Proceedings},
pages = {i–xiii},
location = {Dublin, Ireland}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.4108/eai.25-10-2016.2266615,
author = {Distefano, Salvatore and Scarpa, Marco},
title = {Quantitative assessment of workflow performance through PH reduction},
year = {2017},
isbn = {9781631901416},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/eai.25-10-2016.2266615},
doi = {10.4108/eai.25-10-2016.2266615},
abstract = {Workflows are logical abstraction of processes widely adopted in several contexts such as economy and management sciences (business processes), service engineering (service oriented architecture, Web services, BPEL), software engineering (component based systems, UML, flowcharts) distributed computing (Grid, Cloud, Mapreduce). Design and operation of workflows are critical stages in which problems and issues not manifested by the single block arise from compositions. To deal with such issues, proper techniques and tools should be implemented as support for workflow designers and operators. This paper proposes a solution for the evaluation of workflow performance starting from the components’ ones. Based on the stochastic characterization of the workflow tasks, phase type distributions and stochastic workflow reduction rules, the proposed approach allows to overcome the limits of existing solutions, considering general response time distributions while providing parametric analysis on customer usage profiles and design alternatives. To demonstrate the effectiveness of the proposed solution an example taken from literature is evaluated.},
booktitle = {Proceedings of the 10th EAI International Conference on Performance Evaluation Methodologies and Tools on 10th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {117–124},
numpages = {8},
keywords = {workflow, usage profile, phase type, performance, non-markovian behaviors, design alternatives},
location = {Taormina, Italy},
series = {VALUETOOLS'16}
}

@inproceedings{10.1145/2212736.2212739,
author = {Takeuchi, Mikio and Makino, Yuki and Kawachiya, Kiyokuni and Horii, Hiroshi and Suzumura, Toyotaro and Suganuma, Toshio and Onodera, Tamiya},
title = {Compiling X10 to Java},
year = {2011},
isbn = {9781450307703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212736.2212739},
doi = {10.1145/2212736.2212739},
abstract = {X10 is a new programming language for improving the software productivity in the multicore era by making parallel/distributed programming easier. X10 programs are compiled into C++ or Java source code, but X10 supports various features not supported directly in Java. To implement them efficiently in Java, new compilation techniques are needed.This paper discusses problems in translating X10-specific functions to Java and provides our solutions. By using appropriate implementations, sequential execution performance has been improved by about 5 times making it comparable to native Java. The parallel execution performance has also been improved and the gap from Java Fork/Join performance is about 3 times when run at a single place. Initial evaluation of distributed execution shows good scalability. Most of the results in this paper have already been incorporated in X10 release 2.1.2.Many of the compilation techniques described in this paper can be useful for implementing other programming languages targeted for Java or other managed environments.},
booktitle = {Proceedings of the 2011 ACM SIGPLAN X10 Workshop},
articleno = {3},
numpages = {10},
keywords = {optimization, evaluation, code generation, X10, Java},
location = {San Jose, California},
series = {X10 '11}
}

@inproceedings{10.1109/ICSE.2019.00092,
author = {Lazreg, Sami and Cordy, Maxime and Collet, Philippe and Heymans, Patrick and Mosser, S\'{e}bastien},
title = {Multifaceted automated analyses for variability-intensive embedded systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00092},
doi = {10.1109/ICSE.2019.00092},
abstract = {Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {854–865},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/2487336.2487363,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, CA, USA},
series = {SEAMS '13}
}

@inproceedings{10.1145/1370062.1370078,
author = {Espinoza, Huascar and Servat, David and G\'{e}rard, S\'{e}bastien},
title = {Leveraging analysis-aided design decision knowledge in UML-based development of embedded systems},
year = {2008},
isbn = {9781605580388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370062.1370078},
doi = {10.1145/1370062.1370078},
abstract = {Many important works have been carried out to provide modeling languages (e.g., UML, SDL) with expressiveness to support embedded system design, validation and verification. A fundamental shortcoming in current model-driven approaches is the inability to explicitly capture design decisions and trade-offs between different non-functional parameters, among which timeliness, memory usage, and power consumption are of primary interest. This paper highlights technical limitations in UML to specify complex non-functional evaluation scenarios of candidate architectures, and outlines our current work to provide straightforward solutions.},
booktitle = {Proceedings of the 3rd International Workshop on Sharing and Reusing Architectural Knowledge},
pages = {55–62},
numpages = {8},
keywords = {trade-off analysis, model-driven engineering, embedded systems, design space exploration, UML},
location = {Leipzig, Germany},
series = {SHARK '08}
}

@article{10.1016/j.jnca.2011.09.002,
author = {Beaubrun, Ronald and Llano-Ruiz, Jhon-Fredy and Poirier, Benoit and Quintero, Alejandro},
title = {A middleware architecture for disseminating delay-constrained information in wireless sensor networks},
year = {2012},
issue_date = {January, 2012},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {35},
number = {1},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2011.09.002},
doi = {10.1016/j.jnca.2011.09.002},
abstract = {Data dissemination is a process where information is transmitted towards different destinations. In order to prevent accidents, coordinate rescue operations and warn people about critical events, this process imposes end-to-end delay constraints. Accordingly, the delay from each source to each destination should be monitored. For this purpose, a middleware is required between the network and the applications, offering the tracking capabilities of disseminated information. In this paper, we propose a middleware architecture for disseminating delay-constrained information in Wireless Sensor Networks (WSNs). In order to evaluate the feasibility of such architecture, a proof of concept of a real scenario is implemented. For performance evaluation, the end-to-end delay and the percentage of success related to the disseminated information are analyzed. Such analysis reveals that the middleware offers a percentage of success close to 98%, which is highly superior to the success of individual resources, such as Short Message Services (SMS), emailing and twitter.},
journal = {J. Netw. Comput. Appl.},
month = jan,
pages = {403–411},
numpages = {9},
keywords = {Wireless sensor network, Proof of concept, Middleware architecture, Delay-constrained application, Data dissemination}
}

@inproceedings{10.1145/2517208.2517228,
author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and P\"{u}schel, Markus},
title = {Spiral in scala: towards the systematic construction of generators for performance libraries},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517228},
doi = {10.1145/2517208.2517228},
abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {125–134},
numpages = {10},
keywords = {synthesis, selective precomputation, scalar replacement, data representation, abstraction over staging},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1007/s10470-017-1082-4,
author = {Huang, Jun and Wu, Jianhui and Wu, Aidong},
title = {Two-step Vcm-based MS switching method with dual-capacitive arrays for SAR ADCs},
year = {2018},
issue_date = {January   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {94},
number = {1},
issn = {0925-1030},
url = {https://doi.org/10.1007/s10470-017-1082-4},
doi = {10.1007/s10470-017-1082-4},
abstract = {An ultra-low-power two-step merge and split (MS) switching method for a dual-capacitive arrays (DCAs) successive approximation register analogue-to-digital converter is presented. This method only requires two reference levels, i.e. Gnd and Vcm (Vcm = 1/2Vref). Compared with the conventional method, the proposed method achieves 99.89 and 80.96% reduction in average switching energy and capacitors, respectively, meanwhile maintaining good linearity. In addition, it barely consumes reset energy and keeps common-mode voltage of DCAs almost constant.},
journal = {Analog Integr. Circuits Signal Process.},
month = jan,
pages = {155–160},
numpages = {6},
keywords = {Vcm-based, Ultra-low-power, Two-step, Switching method, SAR ADC, Linearity}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Software Tailoring, Software Product Lines, Linux, Feature Selection},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@article{10.5555/1139246.1672835,
author = {Jiao, Jianxin (Roger) and Helander, Martin G.},
title = {Development of an electronic configure-to-order platform for customized product development},
year = {2006},
issue_date = {April 2006},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {3},
issn = {0166-3615},
abstract = {Next generation manufacturing calls for mass customization of product fulfillment encompassing global and networked enterprises. This paper reports the development of an electronic configure-to-order platform for customized product development over the Internet. It aims to integrate different Web-based services across companies, as well as various lifecycle issues of product fulfillment, into a collaborative web of interactive commerce. A generic product family (GPF) master model is proposed to support product customization over the Internet while achieving a synergy of sales force automation, product design, manufacturing planning, and supply chain management within a coherent framework. Implementation of the configure-to-order platform is discussed in detail with a particular application to injection-molded parts.},
journal = {Comput. Ind.},
month = apr,
pages = {231–244},
numpages = {14},
keywords = {Supply chain management, Product development, Mass customization, Global manufacturing, E-commerce}
}

@inproceedings{10.1145/215530.215575,
author = {Short, Joel and Bagrodia, Rajive and Kleinrock, Leonard},
title = {Mobile wireless network system simulation},
year = {1995},
isbn = {0897918142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/215530.215575},
doi = {10.1145/215530.215575},
booktitle = {Proceedings of the 1st Annual International Conference on Mobile Computing and Networking},
pages = {195–209},
numpages = {15},
location = {Berkeley, California, USA},
series = {MobiCom '95}
}

@inproceedings{10.1145/3235830.3235834,
author = {Escobar, Juan Jos\'{e} and Ortega, Julio and D\'{\i}az, Antonio Francisco and Gonz\'{a}lez, Jes\'{u}s and Damas, Miguel},
title = {Speedup and Energy Analysis of EEG Classification for BCI Tasks on CPU-GPU Clusters},
year = {2018},
isbn = {9781450365314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3235830.3235834},
doi = {10.1145/3235830.3235834},
abstract = {Many data mining applications on bioinformatics and bioengineering require solving problems with different profiles from the point of view of their implicit parallelism. In this context, heterogeneous architectures comprised by interconnected nodes with multiple multi-core microprocessors and accelerators, such as vector processors, Graphics Processing Units (GPUs), or Field-Programmable Gate Arrays would constitute suitable platforms that offer the possibility of not only to accelerate the running time of the applications, but also to optimize the energy consumption. In this paper, we analyze the speedups and energy consumption of a parallel multiobjective approach for feature selection and classification of electroencephalograms in Brain Computing Interface tasks, by considering different implementation alternatives in a heterogeneous CPU-GPU cluster. The procedure is able to take advantage of parallelism through message-passing among the CPU-GPU nodes of the cluster (through shared-memory and thread-level parallelism in the CPU cores, and data-level parallelism and thread-level parallelism in the GPU). The experimental results show high code accelerations and high energy-savings: running times between 1.4 and 5.3% of the sequential time and energy consumptions between 5.9 and 11.6% of the energy consumed by the sequential execution.},
booktitle = {Proceedings of the 6th International Workshop on Parallelism in Bioinformatics},
pages = {33–43},
numpages = {11},
keywords = {Parallelism, Heterogeneous Cluster, Energy-aware Computing, EEG Classification, Distributed Programming, BCI Tasks},
location = {Barcelona, Spain},
series = {PBio 2018}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.5555/2337223.2337416,
author = {Perrouin, Gilles and Morin, Brice and Chauvel, Franck and Fleurey, Franck and Klein, Jacques and Le Traon, Yves and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards flexible evolution of dynamically adaptive systems},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Modern software systems need to be continuously available under varying conditions. Their ability to dynamically adapt to their execution context is thus increasingly seen as a key to their success. Recently, many approaches were proposed to design and support the execution of Dynamically Adaptive Systems (DAS). However, the ability of a DAS to evolve is limited to the addition, update or removal of adaptation rules or reconfiguration scripts. These artifacts are very specific to the control loop managing such a DAS and runtime evolution of the DAS requirements may affect other parts of the DAS. In this paper, we argue to evolve all parts of the loop. We suggest leveraging recent advances in model-driven techniques to offer an approach that supports the evolution of both systems and their adaptation capabilities. The basic idea is to consider the control loop itself as an adaptive system.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1353–1356},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1007/11527800_24,
author = {Cointe, Pierre},
title = {Towards generative programming},
year = {2004},
isbn = {3540278842},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527800_24},
doi = {10.1007/11527800_24},
abstract = {Generative Programming (GP) is an attempt to manufacture software components in an automated way by developing programs that synthesize other programs. Our purpose is to introduce the what and the how of the GP approach from a programming language point of view. For the what we discuss the lessons learned from object-oriented languages seen as general purpose languages to develop software factories. For the how we compare a variety of approaches and techniques based on program transformation and generation. On the one hand, we present the evolution of open-ended languages from metalevel programming to aspect-oriented programming. On the other hand, we introduce domain-specific languages as a way to bridge the gap between conceptual models and programming languages.},
booktitle = {Proceedings of the 2004 International Conference on Unconventional Programming Paradigms},
pages = {315–325},
numpages = {11},
location = {Le Mont Saint Michel, France},
series = {UPP'04}
}

@inproceedings{10.5555/1939864.1939916,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Dynamic resource reallocation between deployment components},
year = {2010},
isbn = {3642169007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Today's software systems are becoming increasingly configurable and designed for deployment on a plethora of architectures, ranging from sequential machines via multicore and distributed architectures to the cloud. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. To model and analyze systems without a fixed architecture, the models need to naturally capture and range over relevant deployment scenarios. For this purpose, it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. In this paper, the object-oriented modeling language Creol is extended with a notion of dynamic deployment components with parametric processing resources, such that processor resources may be explicitly reallocated. The approach is compositional in the sense that functional models and reallocation strategies are both expressed in Creol, and functional models can be run alone or in combination with different reallocation strategies. The formal semantics of deployment components is given in rewriting logic, extending the semantics of Creol, and executes on Maude, which allows simulations and test suites to be applied to models which vary in their available resources as well as in their resource reallocation strategies.},
booktitle = {Proceedings of the 12th International Conference on Formal Engineering Methods and Software Engineering},
pages = {646–661},
numpages = {16},
location = {Shanghai, China},
series = {ICFEM'10}
}

@inproceedings{10.1145/3386263.3406898,
author = {Fu, Rongliang and Zhang, Zhi-Min and Tang, Guang-Ming and Huang, Junying and Ye, Xiao-Chun and Fan, Dong-Rui and Sun, Ning-Hui},
title = {Design Automation Methodology from RTL to Gate-level Netlist and Schematic for RSFQ Logic Circuits},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3406898},
doi = {10.1145/3386263.3406898},
abstract = {The superconducting rapid single flux quantum (RSFQ) logic circuit has the characteristics of high speed and low power consumption, making it an attractive candidate for future supercomputers. However, computer-aided design (CAD) tools for CMOS cannot be directly applied to RSFQ logic due to their distinct properties. For instance, the RSFQ logic gate can work properly when all its fan-ins have the same logic level. This paper presents the design flow from RTL to RSFQ logic netlist and schematic. First, we implement logic synthesis for RSFQ logic circuits. It achieves path balancing while minimizing the number of DFFs. In addition, we propose an automatic schematic generator for the RSFQ logic circuits. It converts the synthesized netlist into its equivalent schematic. A layer assignment algorithm is proposed, which makes all gates layered in the order of the clock arrival time. Experimental results with ISCAS85 and EPFL benchmarks along with some Kogge-Stone adders have shown a 29.2% reduction in the number of DFFs over the breadth-first first search; moreover, 59.57% and 5.3% decrease in the number of layers of the schematic and number of edge crossings over the ELK tool.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {145–150},
numpages = {6},
keywords = {superconducting logic circuits, schematic generation, logic synthesis, layer assignment, design automation, RSFQ},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@inproceedings{10.5555/2486788.2487053,
author = {Heymans, Patrick and Legay, Axel and Cordy, Maxime},
title = {Efficient quality assurance of variability-intensive systems},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Variability is becoming an increasingly important concern in software development but techniques to cost-effectively verify and validate software in the presence of variability have yet to become widespread. This half-day tutorial offers an overview of the state of the art in an emerging discipline at the crossroads of formal methods and software engineering: quality assurance of variability-intensive systems. We will present the most significant results obtained during the last four years or so, ranging from conceptual foundations to readily usable tools. Among the various quality assurance techniques, we focus on model checking, but also extend the discussion to other techniques. With its lightweight usage of mathematics and balance between theory and practice, this tutorial is designed to be accessible to a broad audience. Researchers working in the area, willing to join it, or simply curious, will get a comprehensive picture of the recent developments. Practitioners developing variability-intensive systems are invited to discover the capabilities of our techniques and tools, and to consider integrating them in their processes.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1496–1498},
numpages = {3},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1145/1970353.1970369,
author = {Abouzeid, Fady and Clerc, Sylvain and Firmin, Fabian and Renaudin, Marc and Sas, Tiempo and Sicard, Gilles},
title = {40nm CMOS 0.35V-Optimized Standard Cell Libraries for Ultra-Low Power Applications},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/1970353.1970369},
doi = {10.1145/1970353.1970369},
abstract = {Ultra-low voltage is now a well-known solution for energy constrained applications designed using nanometric process technologies. This work is focused on setting up an automated methodology to enable the design of ultra-low voltage digital circuits exclusively using standard EDA tools. To achieve this goal, a 0.35V energy-delay optimized library was developed. This library, fully compliant with standard library design flow and characterization, was verified through the design and fabrication of a BCH decoder circuit, following a standard front-end to back-end flow. At 0.33V, it performs at 600 kHz with a dynamic energy consumption reduced by a factor 14x from nominal 1.1V. Based on this design, experiments, and preliminary silicon results, two additional libraries were developed in order to enhance future ultra-low voltage circuit performance.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {35},
numpages = {17},
keywords = {ultra low voltage, subthreshold, methodology, low power, logic, library, energy, design, circuit, CMOS, Bose Choudhury Hocquenghem}
}

@inproceedings{10.1145/2884781.2884861,
author = {Tan, Tian Huat and Chen, Manman and Sun, Jun and Liu, Yang and Andr\'{e}, \'{E}tienne and Xue, Yinxing and Dong, Jin Song},
title = {Optimizing selection of competing services with probabilistic hierarchical refinement},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884861},
doi = {10.1145/2884781.2884861},
abstract = {Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (ProHR). ProHR effectively reduces the search space by removing competing services that cannot be part of the selection. ProHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, ProHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. ProHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {85–95},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2744769.2744915,
author = {Rahimi, Abbas and Cesarini, Daniele and Marongiu, Andrea and Gupta, Rajesh K. and Benini, Luca},
title = {Task scheduling strategies to mitigate hardware variability in embedded shared memory clusters},
year = {2015},
isbn = {9781450335201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744769.2744915},
doi = {10.1145/2744769.2744915},
abstract = {Manufacturing and environmental variations cause timing errors that are typically avoided by conservative design guardbands or corrected by circuit level error detection and correction. These measures incur energy and performance penalties. This paper considers methods to reduce this cost by expanding the scope of variability mitigation through the software stack. In particular, we propose workload deployment methods that reduce the likelihood of timing errors in shared memory clusters of processor cores. This and other methods are incorporated in a runtime layer in the OpenMP framework that enables parsimonious countermeasures against timing errors induced by hardware variability. The runtime system "introspectively" monitors the costs of tasks execution on various cores and transparently associates descriptive metadata with the tasks. By utilizing the characterized metadata, we propose several policies that enhance the cluster choices for scheduling tasks to cores according to measured hardware variability and system workload. We devise efficient task scheduling strategies for simultaneous management of variability and workload by exploiting centralized and distributed approaches to workload distribution. Both schedulers surpass current state-of-the-art approaches; the distributed (or the centralized) achieves on average 30% (or 17%) energy, and 17% (4%) performance improvement.},
booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
articleno = {152},
numpages = {6},
location = {San Francisco, California},
series = {DAC '15}
}

@inproceedings{10.5555/1949303.1949307,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Validating timed models of deployment components with parametric concurrency},
year = {2010},
isbn = {3642180698},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many software systems today are designed without assuming a fixed underlying architecture, and may be adapted for sequential, multicore, or distributed deployment. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. Models of such systems need to capture and range over relevant deployment scenarios, so it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. This paper proposes an abstract model of deployment components for concurrent objects, extending the Creol modeling language. The deployment components are parametric in the amount of concurrency they provide; i.e., they vary in processing resources. We give a formal semantics of deployment components and characterize equivalence between deployment components which differ in concurrent resources in terms of test suites. Our semantics is executable on Maude, which allows simulations and test suites to be applied to a deployment component with different concurrent resources.},
booktitle = {Proceedings of the 2010 International Conference on Formal Verification of Object-Oriented Software},
pages = {46–60},
numpages = {15},
location = {Paris, France},
series = {FoVeOOS'10}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@article{10.1016/j.jss.2017.09.033,
author = {Badampudi, Deepika and Wnuk, Krzysztof and Wohlin, Claes and Franke, Ulrik and Smite, Darja and Cicchetti, Antonio},
title = {A decision-making process-line for selection of software asset origins and components},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.033},
doi = {10.1016/j.jss.2017.09.033},
abstract = {Presents a process-line for selecting software asset origins and components.Process-line helps decision-makers to build their decisions-making process.The process-line is evaluated through five case studies in three companies.The practitioners did not perceive any activity to be missing in the process-line.A sub-set of activities were followed by the companies without any specific order. Selecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were not executed in a specific order. Therefore, the refinement of the solution into a process-line approach increases the flexibility and hence it is better in capturing the differences in the decision-making processes observed in the case studies. The applicability of the process-line was then validated in three case studies in a third company.},
journal = {J. Syst. Softw.},
month = jan,
pages = {88–104},
numpages = {17},
keywords = {Decision-making, Component-based software engineering, Case study}
}

@article{10.5555/965697.965701,
author = {Du, Xuehong and Jiao, Jianxin and Tseng, Mitchell M.},
title = {Product family modeling and design support: An approach based on graph rewriting systems},
year = {2002},
issue_date = {April 2002},
publisher = {Cambridge University Press},
address = {USA},
volume = {16},
number = {2},
issn = {0890-0604},
abstract = {Earlier research on product family design (PFD) often highlights isolated and successful empirical studies with a limited attempt to explore the modeling and design support issues surrounding this economically important class of engineering design problems. This paper proposes a graph rewriting system to organize product family data according to the underpinning logic and to model product derivation mechanisms for PFD. It represents the structural and behavioral aspects of product families as family graphs and related graph operations, respectively. The derivation of product variants becomes a graph rewriting process, in which family graphs are transformed to variant graphs by applying appropriate graph rewriting rules. The system is developed in the language of programmed graph rewriting systems or PROGRES, which supports the specification of hierarchical graph schema and parametric rewriting rules. A meta model is defined for family graphs to factor out those entities common to all product families. A generic model is defined to describe all specific entities relevant to particular families. An instance model describes all product variants for individual customer orders. A prototype of a graph-based PFD system for office chairs is also developed. The system can provide an interactive environment for customers to make choices among product offerings. It also facilitates design automation of product families and enhances interactions and negotiations among sales, design, and manufacturing.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = apr,
pages = {103–120},
numpages = {18},
keywords = {Product Family, PROGRES, Mass Customization Systems, Graph Grammar, Design Automation}
}

@article{10.1016/j.jss.2012.04.079,
author = {Peng, Xin and Chen, Bihuan and Yu, Yijun and Zhao, Wenyun},
title = {Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop},
year = {2012},
issue_date = {December, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.04.079},
doi = {10.1016/j.jss.2012.04.079},
abstract = {Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2707–2719},
numpages = {13},
keywords = {Self-tuning, Preference, Goal-oriented reasoning, Feedback control theory, Earned business value}
}

@inproceedings{10.1145/1233901.1233907,
author = {Lohmann, Daniel and Streicher, Jochen and Spinczyk, Olaf and Schr\"{o}der-Preikschat, Wolfgang},
title = {Interrupt synchronization in the CiAO operating system: experiences from implementing low-level system policies by AOP},
year = {2007},
isbn = {9781595936578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233901.1233907},
doi = {10.1145/1233901.1233907},
abstract = {Configurability is a major issue in the domain of embedded system software. Existing systems specifically lack good techniques to implement configurability of architectural OS concerns, such as the choice of isolation or synchronization policies to use. As such policies have a very cross-cutting character, aspects should provide good means to implement them in a configurable way. While our results show that this is in fact the case, 1) things could have been easier if additional language features were available, and, 2) additional means to influence the back-end code generation turned out to be very important. This paper presents our experiences in using AspectC++ to design and implement interrupt synchronization as a configurable property in the CiAO operating system.},
booktitle = {Proceedings of the 6th Workshop on Aspects, Components, and Patterns for Infrastructure Software},
pages = {6–es},
keywords = {configurability, aspect-oriented programming (AOP), aspect-aware operating system, CiAO, AspectC++},
location = {Vancouver, British Columbia, Canada},
series = {ACP4IS '07}
}

@inproceedings{10.1145/3453688.3461486,
author = {Fu, Rongliang and Huang, Junying and Zhang, Zhi-Min},
title = {Equivalence Checking for Superconducting RSFQ Logic Circuits},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461486},
doi = {10.1145/3453688.3461486},
abstract = {Equivalence checking is a key component of the verification methodology for digital circuit designs. In this paper, we propose an equivalence checking framework for superconducting rapid single-flux-quantum (RSFQ) logic circuits which include acyclic circuits and bit-slice-based cyclic circuits. It consists of a structure checker and a logic checker. The structure checker is used to check whether the circuit meets the design rules of superconducting RSFQ logic circuits. The logic checker can be used to check whether two RSFQ gate-level circuits have the same logic function. For the logic checker, we propose a logic equivalence checking method based on logic cone partition. The circuit network is simplified layer by layer and iteratively partitioned into logic cones, each of which is verified by the SMT solver. The experimental results show the feasibility of our approach on superconducting RSFQ logic circuits.},
booktitle = {Proceedings of the 2021 Great Lakes Symposium on VLSI},
pages = {51–56},
numpages = {6},
keywords = {equivalence checking, logic cone, rapid single-flux-quantum (rsfq), smt, superconducting circuits},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@article{10.1016/j.jss.2016.06.068,
author = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
title = {Cloud migration process-A survey, evaluation framework, and open challenges},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.068},
doi = {10.1016/j.jss.2016.06.068},
abstract = {The relevant approaches for migrating legacy applications to the cloud are surveyed.An extensive analysis of existing approaches on the basis of a set of important criteria/features.Important cloud migration activities, techniques, and concerns that need to be properly addressed in a typical cloud migration process are delineated.Existing open issues and future research opportunities on the cloud migration research area are discussed. Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.},
journal = {J. Syst. Softw.},
month = oct,
pages = {31–69},
numpages = {39},
keywords = {Process model, Migration methodology, Legacy application, Evaluation framework, Cloud migration, Cloud computing}
}

@article{10.1145/3303849,
author = {R\"{o}ger, Henriette and Mayer, Ruben},
title = {A Comprehensive Survey on Parallelization and Elasticity in Stream Processing},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3303849},
doi = {10.1145/3303849},
abstract = {Stream Processing (SP) has evolved as the leading paradigm to process and gain value from the high volume of streaming data produced, e.g., in the domain of the Internet of Things. An SP system is a middleware that deploys a network of operators between data sources, such as sensors, and the consuming applications. SP systems typically face intense and highly dynamic data streams. Parallelization and elasticity enable SP systems to process these streams with continuous high quality of service. The current research landscape provides a broad spectrum of methods for parallelization and elasticity in SP. Each method makes specific assumptions and focuses on particular aspects. However, the literature lacks a comprehensive overview and categorization of the state of the art in SP parallelization and elasticity, which is necessary to consolidate the state of the research and to plan future research directions on this basis. Therefore, in this survey, we study the literature and develop a classification of current methods for both parallelization and elasticity in SP systems.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {36},
numpages = {37},
keywords = {parallelization, elasticity, data stream management system, complex event processing, Stream processing}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {variability, non-functional requirements, monitoring, SPLs},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1007/978-3-540-30554-5_19,
author = {Spinczyk, Olaf and Schoettner, Michael and Gal, Andreas},
title = {Programming languages and operating systems},
year = {2004},
isbn = {354023988X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30554-5_19},
doi = {10.1007/978-3-540-30554-5_19},
abstract = {This report gives an overview over the First ECOOP Workshop on Programming Languages and Operating Systems (PLOS 2004). It explains the motivation for the workshop and gives a summary of the workshop contributions and discussions during the workshop.},
booktitle = {Proceedings of the 2004 International Conference on Object-Oriented Technology},
pages = {202–213},
numpages = {12},
location = {Oslo, Norway},
series = {ECOOP'04}
}

@inproceedings{10.1145/2988336.2988353,
author = {Issarny, Valerie and Mallet, Vivien and Nguyen, Kinh and Raverdy, Pierre-Guillaume and Rebhi, Fadwa and Ventura, Raphael},
title = {Dos and Don'ts in Mobile Phone Sensing Middleware: Learning from a Large-Scale Experiment},
year = {2016},
isbn = {9781450343008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988336.2988353},
doi = {10.1145/2988336.2988353},
abstract = {Mobile phone sensing contributes to changing the way we approach science: massive amount of data is being contributed across places and time, and paves the way for advanced analyses of numerous phenomena at an unprecedented scale. Still, despite the extensive research work on enabling resource-efficient mobile phone sensing with a very-large crowd, key challenges remain. One challenge is facing the introduction of a new heterogeneity dimension in the traditional middleware research landscape. The middleware must deal with the heterogeneity of the contributing crowd in addition to the system's technical heterogeneities. In order to tackle these two heterogeneity dimensions together, we have been conducting a large-scale empirical study in cooperation with the city of Paris. Our experiment revolves around the public release of a mobile app for urban pollution monitoring that builds upon a dedicated mobile crowd-sensing middleware. In this paper, we report on the empirical analysis of the resulting mobile phone sensing efficiency from both technical and social perspectives, in face of a large and highly heterogeneous population of participants. We concentrate on the data originating from the 20 most popular phone models of our user base, which represent contributions from over 2,000 users with 23 million observations collected over 10 months. Following our analysis, we introduce a few recommendations to overcome -technical and crowd- heterogeneities in the implementation of mobile phone sensing applications and supporting middleware.},
booktitle = {Proceedings of the 17th International Middleware Conference},
articleno = {17},
numpages = {13},
keywords = {Urban sensing, Sensors heterogeneity, Sensing accuracy, Mobile phone sensing, Crowd-sensing},
location = {Trento, Italy},
series = {Middleware '16}
}

@article{10.1109/TASLP.2018.2889927,
author = {Zhu, Qiaoxi and Coleman, Philip and Qiu, Xiaojun and Wu, Ming and Yang, Jun and Burnett, Ian},
title = {Robust Personal Audio Geometry Optimization in the SVD-Based Modal Domain},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2889927},
doi = {10.1109/TASLP.2018.2889927},
abstract = {Personal audio generates sound zones in a shared space to provide private and personalized listening experiences with minimized interference between consumers. Regularization has been commonly used to increase the robustness of such systems against potential perturbations in the sound reproduction. However, the performance is limited by the system geometry such as the number and location of the loudspeakers and controlled zones. This paper proposes a geometry optimization method to find the most geometrically robust approach for personal audio amongst all available candidate system placements. The proposed method aims to approach the most “natural” sound reproduction so that the solo control of the listening zone coincidently accompanies the preferred quiet zone. Being formulated in the SVD-based modal domain, the method is demonstrated by applications in three typical personal audio optimizations, i.e., the acoustic contrast control, the pressure matching, and the planarity control. Simulation results show that the proposed method can obtain the system geometry with better avoidance of “occlusion,” improved robustness to regularization, and improved broadband equalization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {610–620},
numpages = {11}
}

@inproceedings{10.1109/MISE.2009.5069896,
author = {Acher, Mathieu and Lahire, Philippe and Moisan, Sabine and Rigault, Jean-Paul},
title = {Tackling high variability in video surveillance systems through a model transformation approach},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069896},
doi = {10.1109/MISE.2009.5069896},
abstract = {This work explores how model-driven engineering techniques can support the configuration of systems in domains presenting multiple variability factors. Video surveillance is a good candidate for which we have an extensive experience. Ultimately, we wish to automatically generate a software component assembly from an application specification, using model to model transformations. The challenge is to cope with variability both at the specification and at the implementation levels. Our approach advocates a clear separation of concerns. More precisely, we propose two feature models, one for task specification and the other for software components. The first model can be transformed into one or several valid component configurations through step-wise specialization. This paper outlines our approach, focusing on the two feature models and their relations. We particularly insist on variability and constraint modeling in order to achieve the mapping from domain variability to software variability through model transformations.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {44–49},
numpages = {6},
series = {MISE '09}
}

@article{10.1016/j.chb.2016.09.030,
author = {Zhuhadar, Leyla and Thrasher, Evelyn and Marklin, Scarlett and de Pablos, Patricia Ordez},
title = {The next wave of innovationReview of smart cities intelligent operation systems},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2016.09.030},
doi = {10.1016/j.chb.2016.09.030},
abstract = {The use of new technologies in business models and infrastructure has been driven in part by the Internet and globalization. The next trend of innovations is likely to come from humans' ability to connect to machines and the data that comes from these connections. The IBM Intelligent Operation Center (IOC) is a system of systems that is not intended to replace an existing physical infrastructure that gathers raw data. Instead, it is intended to extract only the data necessary to optimize the operations of the organization. The types of data and integration into the IOC make efficient problem solving solutions readily available to city authorities. The user interface and standard operating procedure and the resource processing capabilities of the IOC indicate that this system is optimal for smart cities of the future with regard to improvement of quality of life and ease of navigation. The need for smart cities, universities, campuses, citizens, and students to drive growth of urban and regional economies is evident. In this article, a thorough analysis of the architectural design of an intelligent operational system is completed to present a smart solution for cities to unify departments and agencies under one umbrella. IOC is a system of systems that is intended to optimize an organization.IOC makes efficient problem solving solutions available to city authorities.Smart cities, universities, and citizens to drive growth of economies.An intelligent operational system to drive a smart solution for cities.},
journal = {Comput. Hum. Behav.},
month = jan,
pages = {273–281},
numpages = {9},
keywords = {Smart citizen, Smart cities, Simulation, Intelligent Operation Center, Complex and intelligent systems}
}

@inproceedings{10.1145/566726.566758,
author = {Beuche, Danilo and Fr\"{o}hlich, Ant\^{o}nio Augusto and Meyer, Reinhard and Papajewski, Holger and Sch\"{o}n, Friedrich and Schr\"{o}der-Preikschat, Wolfgang and Spinczyk, Olaf and Spinczyk, Ute},
title = {On architecture transparency in operating systems},
year = {2000},
isbn = {9781450373562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/566726.566758},
doi = {10.1145/566726.566758},
booktitle = {Proceedings of the 9th Workshop on ACM SIGOPS European Workshop: Beyond the PC: New Challenges for the Operating System},
pages = {147–152},
numpages = {6},
location = {Kolding, Denmark},
series = {EW 9}
}

@article{10.1007/s10270-017-0622-9,
author = {Bruneliere, Hugo and Burger, Erik and Cabot, Jordi and Wimmer, Manuel},
title = {A feature-based survey of model view approaches},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0622-9},
doi = {10.1007/s10270-017-0622-9},
abstract = {When dealing with complex systems, information is very often fragmented across many different models expressed within a variety of (modeling) languages. To provide the relevant information in an appropriate way to different kinds of stakeholders, (parts of) such models have to be combined and potentially revamped by focusing on concerns of particular interest for them. Thus, mechanisms to define and compute views over models are highly needed. Several approaches have already been proposed to provide (semi)automated support for dealing with such model views. This paper provides a detailed overview of the current state of the art in this area. To achieve this, we relied on our own experiences of designing and applying such solutions in order to conduct a literature review on this topic. As a result, we discuss the main capabilities of existing approaches and propose a corresponding research agenda. We notably contribute a feature model describing what we believe to be the most important characteristics of the support for views on models. We expect this work to be helpful to both current and potential future users and developers of model view techniques, as well as to any person generally interested in model-based software and systems engineering.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {1931–1952},
numpages = {22},
keywords = {Viewpoint, View, Survey, Modeling, Model}
}

@article{10.1016/j.scico.2012.07.019,
author = {Chabridon, Sophie and Conan, Denis and Abid, Zied and Taconet, Chantal},
title = {Building ubiquitous QoC-aware applications through model-driven software engineering},
year = {2013},
issue_date = {October, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {10},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.07.019},
doi = {10.1016/j.scico.2012.07.019},
abstract = {As every-day mobile devices can easily be equipped with multiple sensing capabilities, ubiquitous applications are expected to exploit the richness of the context information that can be collected by these devices in order to provide the service that is the most appropriate to the situation of the user. However, the design and implementation of such context-aware ubiquitous appplications remain challenging as there exist very few models and tools to guide application designers and developers in mastering the complexity of context information. This becomes even more crucial as context is by nature imperfect. One way to address this issue is to associate to context information meta-data representing its quality. We propose a generic and extensible design process for context-aware applications taking into account the quality of context (QoC). We demonstrate its use on a prototype application for sending flash sale offers to mobile users. We present extensive performance results in terms of memory and processing time of both elementary context management operations and the whole context policy implementing the Flash sale application. The cost of adding QoC management is also measured and appears to be limited to a few milliseconds. We show that a context policy with 120 QoC-aware nodes can be processed in less than 100 ms on a mobile phone. Moreover, a policy of almost 3000 nodes can be instantiated before exhausting the resources of the phone. This enables very rich application scenarios enhancing the user experience and will favor the development of new ubiquitous applications.},
journal = {Sci. Comput. Program.},
month = oct,
pages = {1912–1929},
numpages = {18},
keywords = {Ubiquitous computing, Quality of context, Pervasive computing, Model-driven software engineering, Domain specific language, Context}
}

@inproceedings{10.1007/11805816_32,
author = {Althoff, Klaus-Dieter and Hanft, Alexandre and Schaaf, Martin},
title = {Case factory: maintaining experience to learn},
year = {2006},
isbn = {3540368434},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11805816_32},
doi = {10.1007/11805816_32},
abstract = {In this paper, we outline our vision of a case factory that deals with developing (future) knowledge-based systems. The functionality of such a system is provided by different kinds of agents. We focus especially on case-based-reasoning agents, which play an important part within our vision and the corresponding architecture. Our method of constructing a case-based reasoning system using agents is based on integration with the experience factory approach. We define a single architecture adopting ideas from the concept of software product-lines with a focus on combining technical and organizational knowledge. Finally, the paper closes with a brief overview of the current state of our work and a conceptual evaluation of its components with respect to related work.},
booktitle = {Proceedings of the 8th European Conference on Advances in Case-Based Reasoning},
pages = {429–442},
numpages = {14},
location = {Fethiye, Turkey},
series = {ECCBR'06}
}

@inproceedings{10.1007/978-3-319-04891-8_10,
author = {Meier, Matthias and Breddemann, Mark and Spinczyk, Olaf},
title = {Hardware APIs: A Software-Centric Approach for Automated Derivation of MPSoC Hardware Structures Based on Static Code Analysis},
year = {2014},
isbn = {9783319048901},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-04891-8_10},
doi = {10.1007/978-3-319-04891-8_10},
abstract = {Multiprocessor systems on a chip (MPSoCs) are a popular class of course-grained parallel computer architectures, which are very useful, because they support re-use of legacy software components and application-specific tailoring of hardware structures at the same time. Furthermore, model-driven design frameworks for MPSoCs such as Xilinx' EDK or our own LavA-framework facilitate very fast system development. However, in this paper we argue that these design frameworks are not ideal from the development process perspective. Instead, we propose a software-centric approach that is based on the hardware API concept. The API is a representation of hardware components on the software level, which is generated from a hardware meta-model. It allows us to automatically derive a hardware structure based on access patterns in software, revealed by a static code analysis. This trick reduces the number of hardware details the developer needs to deal with and avoids configuration inconsistencies between the hardware and software levels by design.},
booktitle = {Proceedings of the 27th International Conference on Architecture of Computing Systems  ARCS 2014 - Volume 8350},
pages = {111–122},
numpages = {12}
}

@article{10.1016/j.infsof.2012.04.001,
author = {Abramov, Jenny and Sturm, Arnon and Shoval, Peretz},
title = {Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.04.001},
doi = {10.1016/j.infsof.2012.04.001},
abstract = {Context: Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process. Objective: We present an empirical study conducted to evaluate a Pattern-based method for Secure Development - PbSD - that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design. Method: The PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in SQL and Oracle's VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods. Results: We found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use. Conclusion: The pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {1029–1043},
numpages = {15},
keywords = {Security patterns, Secure software development, Model driven development, Database design, Controlled experiment, Authorization}
}

@inproceedings{10.1007/11790853_42,
author = {Althoff, Klaus-Dieter and Decker, Bj\"{o}rn and Hanft, Alexandre and M\"{a}nz, Jens and Newo, R\'{e}gis and Nick, Markus and Rech, J\"{o}rg and Schaaf, Martin},
title = {Intelligent information systems for knowledge work(ers)},
year = {2006},
isbn = {3540360360},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11790853_42},
doi = {10.1007/11790853_42},
abstract = {Our society needs and expects more high-value services. Such “knowledge-intensive” services can only be delivered if the necessary organizational and technical requirements are fulfilled. In addition, the cost-benefit analysis from the service provider point of view needs to be positive. Continuous improvement and goal-directed (partial) automation of such services is therefore of crucial importance. As a contribution to this we describe our current research vision for (partially) automated support of knowledge work(ers) based on intelligent information systems focusing on the use of experience. For the implementation of such a vision we base on the integration of approaches from artificial intelligence and software engineering. A “deep” integration of case-based reasoning and experience factory is a first successful step in this direction [33, 28]. We envision the further integration of software product-lines and multi-agent systems as the next one.},
booktitle = {Proceedings of the 6th Industrial Conference on Data Mining Conference on Advances in Data Mining: Applications in Medicine, Web Mining, Marketing, Image and Signal Mining},
pages = {539–547},
numpages = {9},
location = {Leipzig, Germany},
series = {ICDM'06}
}

@article{10.1016/j.sysarc.2015.07.010,
author = {Meier, Matthias and Breddemann, Mark and Spinczyk, Olaf},
title = {Interfacing the hardware API with a feature-based operating system family},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {61},
number = {10},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2015.07.010},
doi = {10.1016/j.sysarc.2015.07.010},
abstract = {Multiprocessor systems on a chip (MPSoCs) are a popular class of course-grained parallel computer architectures, which are very useful, because they support re-use of legacy software components and application-specific tailoring of hardware structures at the same time. Furthermore, model-driven design frameworks for MPSoCs such as Xilinx' EDK or our own LavA-framework facilitate very fast system development. However, in this paper we argue that these design frameworks are not ideal from the development process perspective. Instead, we propose a software-centric approach that is based on the hardware API concept. The API is a representation of hardware components on the software level, which is generated from a hardware meta-model. It allows us to automatically derive a hardware structure based on access patterns in software, revealed by a static code analysis. This trick reduces the number of hardware details the developer needs to deal with and avoids configuration inconsistencies between the hardware and software levels by design. Furthermore, we present how the development process can benefit from the hardware API, when the API is interfaced with a configurable operating system.},
journal = {J. Syst. Archit.},
month = nov,
pages = {531–538},
numpages = {8},
keywords = {Software-centric configuration, Operating system, MPSoC, Hardware representation, HW/SW co-design, FPGA}
}

@inproceedings{10.1145/2088876.2088882,
author = {Huynh, Ngoc-Tho and Phung-Khac, An and Segarra, Maria-Teresa},
title = {Towards reliable distributed reconfiguration},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088882},
doi = {10.1145/2088876.2088882},
abstract = {In component-based software engineering, reconfiguration often refers to the activity of changing a running software system at the component level. Reconfiguration is widely used for evolving and adapting software systems that can not be shut down for update. However, in distributed systems, supporting reconfiguration is a challenging task since a reconfiguration consists of distributed reconfiguration actions that need to be coordinated. Particularly, this task becomes much more challenging in the context of unstable networks where nodes may disconnect frequently, even during reconfiguration. To address this challenge, we propose a platform supporting distributed reconfiguration that embodies a solution for managing system states at reconfiguration time. We define (1) different system states regarding reconfiguration and (2) ways that the system will act accordingly. When a disconnection is detected during a reconfiguration, the system may correct reconfiguration plans to continue the reconfiguration if possible, or recover if the reconfiguration fails.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {36–41},
numpages = {6},
keywords = {reliability, distributed reconfiguration, component-based software engineering},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@article{10.1155/2021/5798741,
author = {Kim, Sungwook and Bazzi, Alessandro},
title = {A New Two-Stage Bargaining Game Approach for Intra- and Inter-WBAN Management},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/5798741},
doi = {10.1155/2021/5798741},
abstract = {The Internet of Medical Things (IoMT) is an amalgamation of smart devices to operate the wireless body area network (WBAN) by using networking technologies. To reduce the burden on WBANs, they link to the mobile edge computing (MEC), on which captured medical data can be stored and analyzed. In this paper, we design a new control scheme to effectively share the limited computation and communication resources in the MEC-assisted WBAN (M-W) platform. Based on the bargaining game theory, our proposed scheme explores the mutual benefits of intra- and inter-WBAN interactions. To dynamically adapt the current system conditions, we shape each WBAN’s aspirations to reach a reciprocal consensus for different application services. Utilizing two control factors, we provide a unifying framework for the study of intra- and inter-WBAN bargaining problems to share the limited system resource. Based on the feasibility and real-time effectiveness, the main novelty of the proposed scheme is the ability to achieve a relevant tradeoff between efficiency and fairness through the interactive bargaining process. At last, the experimental results show that the proposed scheme achieves substantial performance improvements to the comparison schemes.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {10}
}

@article{10.1145/3294054,
author = {Choi, Young-Kyu and Cong, Jason and Fang, Zhenman and Hao, Yuchen and Reinman, Glenn and Wei, Peng},
title = {In-Depth Analysis on Microarchitectures of Modern Heterogeneous CPU-FPGA Platforms},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3294054},
doi = {10.1145/3294054},
abstract = {Conventional homogeneous multicore processors are not able to provide the continued performance and energy improvement that we have expected from past endeavors. Heterogeneous architectures that feature specialized hardware accelerators are widely considered a promising paradigm for resolving this issue. Among different heterogeneous devices, FPGAs that can be reconfigured to accelerate a broad class of applications with orders-of-magnitude performance/watt gains, are attracting increased attention from both academia and industry. As a consequence, a variety of CPU-FPGA acceleration platforms with diversified microarchitectural features have been supplied by industry vendors. Such diversity, however, poses a serious challenge to application developers in selecting the appropriate platform for a specific application or application domain.This article aims to address this challenge by determining which microarchitectural characteristics affect performance, and in what ways. Specifically, we conduct a quantitative comparison and an in-depth analysis on five state-of-the-art CPU-FPGA acceleration platforms: (1) the Alpha Data board and (2) the Amazon F1 instance that represent the traditional PCIe-based platform with private device memory; (3) the IBM CAPI that represents the PCIe-based system with coherent shared memory; (4) the first generation of the Intel Xeon+FPGA Accelerator Platform that represents the QPI-based system with coherent shared memory; and (5) the second generation of the Intel Xeon+FPGA Accelerator Platform that represents a hybrid PCIe-based (non-coherent) and QPI-based (coherent) system with shared memory. Based on the analysis of their CPU-FPGA communication latency and bandwidth characteristics, we provide a series of insights for both application developers and platform designers. Furthermore, we conduct two case studies to demonstrate how these insights can be leveraged to optimize accelerator designs. The microbenchmarks used for evaluation have been released for public use.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = feb,
articleno = {4},
numpages = {20},
keywords = {Xeon+FPGA, Heterogeneous computing, CPU-FPGA platform, CAPI, AWS F1}
}

@inproceedings{10.1145/2783258.2783270,
author = {Yan, Feng and Ruwase, Olatunji and He, Yuxiong and Chilimbi, Trishul},
title = {Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783270},
doi = {10.1145/2783258.2783270},
abstract = {Big deep neural network (DNN) models trained on large amounts of data have recently achieved the best accuracy on hard tasks, such as image and speech recognition. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. To enable training of extremely large DNNs, models are partitioned across machines. To expedite training on very large data sets, multiple model replicas are trained in parallel on different subsets of the training examples with a global parameter server maintaining shared weights across these replicas. The correct choice for model and data partitioning and overall system provisioning is highly dependent on the DNN and distributed system hardware characteristics. These decisions currently require significant domain expertise and time consuming empirical state space exploration.This paper develops performance models that quantify the impact of these partitioning and provisioning decisions on overall distributed system performance and scalability. Also, we use these performance models to build a scalability optimizer that efficiently determines the optimal system configuration that minimizes DNN training time. We evaluate our performance models and scalability optimizer using a state-of-the-art distributed DNN training framework on two benchmark applications. The results show our performance models estimate DNN training time with high estimation accuracy and our scalability optimizer correctly chooses the best configurations, minimizing the training time of distributed DNNs.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1355–1364},
numpages = {10},
keywords = {scalability, performance modeling, optimization, distributed system, deep learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@article{10.1016/j.jpdc.2019.03.002,
author = {Zarei Zefreh, Ebrahim and Lotfi, Shahriar and Mohammad Khanli, Leyli and Karimpour, Jaber},
title = {Topology and computational-power aware tile mapping of perfectly nested loops with dependencies on distributed systems},
year = {2019},
issue_date = {Jul 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.03.002},
doi = {10.1016/j.jpdc.2019.03.002},
journal = {J. Parallel Distrib. Comput.},
month = jul,
pages = {14–35},
numpages = {22},
keywords = {Distributed systems, Topology aware tile mapping, Computational-power aware tile mapping, Parallelization, Nested loop}
}

@article{10.1016/j.infsof.2019.03.015,
author = {Borg, Markus and Chatzipetrou, Panagiota and Wnuk, Krzysztof and Al\'{e}groth, Emil and Gorschek, Tony and Papatheocharous, Efi and Shah, Syed Muhammad Ali and Axelsson, Jakob},
title = {Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.015},
doi = {10.1016/j.infsof.2019.03.015},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {18–34},
numpages = {17},
keywords = {Survey, Decision making, Software architecture, Sourcing, Component-based software engineering}
}

@inproceedings{10.1145/1081706.1081757,
author = {Estublier, Jacky and Vega, German},
title = {Reuse and variability in large software applications},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081757},
doi = {10.1145/1081706.1081757},
abstract = {Reuse has always been a major goal in software engineering, since it promises large gains in productivity, quality and time to market reduction. Practical experience has shown that substantial reuse has only successfully happened in two cases: libraries, where many generic and small components can be found; and product lines, where domains-specific components can be assembled in different ways to produce variations of a given product.In this paper we examine how product lines have successfully achieved reuse of coarse-grained components, and the underlying factors limiting this approach to narrowly scoped domains. We then build on this insight to present an approach, called software federation, which proposes a mechanism to overcome the identified limitations, and therefore makes reuse of coarse-grained components possible over a larger range of applications. Our approach extends and generalizes the product line approach, extending the concepts and mechanisms available to manage variability. The system is in use in different companies, validating the claims made in this paper.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {316–325},
numpages = {10},
keywords = {workflow, variability, reuse, product line, product families, process driven application, model driven software engineering, interoperability, MDA, EAI, COTS, AOP},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@inproceedings{10.1145/2509136.2509522,
author = {Bhattacharya, Suparna and Gopinath, Kanchi and Nanda, Mangala Gowri},
title = {Combining concern input with program analysis for bloat detection},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509522},
doi = {10.1145/2509136.2509522},
abstract = {Framework based software tends to get bloated by accumulating optional features (or concerns) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead.We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {745–764},
numpages = {20},
keywords = {software bloat, program concerns, feature oriented programming},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@inproceedings{10.1109/WI.2006.173,
author = {Zhou, Jiehan and Niemela, Eila},
title = {Toward Semantic QoS Aware Web Services: Issues, Related Studies and Experience},
year = {2006},
isbn = {0769527477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2006.173},
doi = {10.1109/WI.2006.173},
abstract = {Semantic QoSaware Web services incorporating the emerging Web services in the QoSaware system development are promoting ServiceOriented Software Engineering (SOSE). To identify the steps toward semantic quality of service (QoS)aware Web services, this paper examines previous studies related to semantic QoSaware Web services, including QoSaware Web service architectures, QoS classification, QoS ontology, QoS specification languages, and Web service creation tools. Moreover, a case study is presented to discuss the gaps between our current quality driven software development approach and the semantic QoSaware Web services.},
booktitle = {Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {553–557},
numpages = {5},
series = {WI '06}
}

@article{10.1007/s10270-013-0347-3,
author = {Ebert, Christof},
title = {Improving engineering efficiency with PLM/ALM},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0347-3},
doi = {10.1007/s10270-013-0347-3},
abstract = {Rising cost pressure is forcing manufacturers and their suppliers to jointly and consistently master product development. Our industry case study shows how a leading automotive OEM over time has achieved effective interaction of engineering processes, tools, and people on the basis of product and application life-cycle management (PLM/ALM). Its scope is first an introduction to PLM/ALM on the basis of a model-driven engineering (MDE) for one or several products or product families. Second, PLM and ALM need tool support to the degree necessary to ease handling and drive reuse and consistency. Third, introducing MDE needs profound change management. Starting from establishing the relevant engineering processes, we show how they can be effectively automated for best possible usage across the enterprise and even for suppliers. We practically describe how such a profound change process is successfully managed together with impacted engineers and how the concepts can be transferred to other companies. Concrete results for efficiency improvement, shorter lead time, and better quality in product development combined with better global engineering underline the business value.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {443–449},
numpages = {7},
keywords = {Product lifecycle management (PLM), Industry voice, Efficiency, Application lifecycle management (ALM)}
}

@inproceedings{10.1145/2000259.2000263,
author = {Koziolek, Heiko},
title = {Sustainability evaluation of software architectures: a systematic review},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000263},
doi = {10.1145/2000259.2000263},
abstract = {Long-living software systems are sustainable if they can be cost-efficiently maintained and evolved over their entire life-cycle. The quality of software architectures determines sustainability to a large extent. Scenario-based software architecture evaluation methods can support sustainability analysis, but they are still reluctantly used in practice. They are also not integrated with architecture-level metrics when evaluating implemented systems, which limits their capabilities. Existing literature reviews for architecture evaluation focus on scenario-based methods, but do not provide a critical reflection of the applicability of such methods for sustainability evaluation. Our goal is to measure the sustainability of a software architecture both during early design using scenarios and during evolution using scenarios and metrics, which is highly relevant in practice. We thus provide a systematic literature review assessing scenario-based methods for sustainability support and categorize more than 40 architecture-level metrics according to several design principles. Our review identifies a need for further empirical research, for the integration of existing methods, and for the more efficient use of formal architectural models.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {3–12},
numpages = {10},
keywords = {sustainability, survey, software architecture, evolution scenario, architectural metric},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.5555/3408352.3408415,
author = {Shahsavani, Soheil Nazar and Zhang, Bo and Pedram, Massoud},
title = {A timing uncertainty-aware clock tree topology generation algorithm for single flux quantum circuits},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {This paper presents a low-cost, timing uncertainty-aware synchronous clock tree topology generation algorithm for single flux quantum (SFQ) logic circuits. The proposed method considers the criticality of the data paths in terms of timing slacks as well as the total wirelength of the clock tree and generates a (height-) balanced binary clock tree using a bottom-up approach and an integer linear programming (ILP) formulation. The statistical timing analysis results for ten benchmark circuits show that the proposed method improves the total wirelength and the total negative hold slack by 4.2% and 64.6%, respectively, on average, compared with a wirelength-driven state-of-the-art balanced topology generation approach.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {278–281},
numpages = {4},
keywords = {clock topology generation, clock tree synthesis, mathematical programming, single flux quantum, timing uncertainty},
location = {Grenoble, France},
series = {DATE '20}
}

@article{10.1504/IJIPT.2016.079546,
author = {Balakrishnan, Senthil Murugan and Sangaiah, Arun Kumar},
title = {Aspect-oriented middleware framework for resolving service discovery issues in Internet of Things},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {2/3},
issn = {1743-8209},
url = {https://doi.org/10.1504/IJIPT.2016.079546},
doi = {10.1504/IJIPT.2016.079546},
abstract = {Internet of Things IoT is a model of future internet and pervasive computing which have its own challenges derived from the internet in terms of scalability, undefined topology and so on. The proposed work aims to resolve the challenges posed by IoT in service discovery functionality. Considering the pervasive and context dependent nature of IoT the planned work bases its development strategy using an aspect oriented software development methodology. The novelty lies in achieving high degree configuration and customisability by selecting subset of middleware functionality depending on the need. The performance is compared with MUSIC pervasive computing middleware and Android built-in configuration. The result reveals 6.5% decrease on average boot up and reconfiguration time for smart phones and 11.6% percentage decrease in Android tablets. In the context of boot up and reconfiguration time the middleware brings out 5% decrease for smart phones and 3% for Android tablets when compared with MUSIC middleware. The middleware shows 6.3% and 4% reduction in execution time of applications on smart phones and tablets when assessed with MUSIC middleware.},
journal = {Int. J. Internet Protoc. Technol.},
month = jan,
pages = {62–78},
numpages = {17},
keywords = {smartphones, service discovery, reconfiguration time, pervasive computing, internet of things, boot up time, aspect-oriented middleware, Spring AOP, IoT, Android tablets}
}

@article{10.1016/j.knosys.2006.04.004,
author = {Zha, Xuan F. and Sriram, Ram D.},
title = {Platform-based product design and development: A knowledge-intensive support approach},
year = {2006},
issue_date = {November, 2006},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {19},
number = {7},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2006.04.004},
doi = {10.1016/j.knosys.2006.04.004},
abstract = {This paper presents a knowledge-intensive support paradigm for platform-based product family design and development. The fundamental issues underlying the product family design and development, including product platform and product family modeling, product family generation and evolution, and product family evaluation for customization, are discussed. A module-based integrated design scheme is proposed with knowledge support for product family architecture modeling, product platform establishment, product family generation, and product variant assessment. A systematic methodology and the relevant technologies are investigated and developed for knowledge supported product family design process. The developed information and knowledge-modeling framework and prototype system can be used for platform product design knowledge capture, representation and management and offer on-line support for designers in the design process. The issues and requirements related to developing a knowledge-intensive support system for modular platform-based product family design are also addressed.},
journal = {Know.-Based Syst.},
month = nov,
pages = {524–543},
numpages = {20},
keywords = {Product platform, Product family, Product architecture, Modular design, Knowledge support}
}

@article{10.1007/s10664-016-9466-0,
author = {Behnamghader, Pooyan and Le, Duc Minh and Garcia, Joshua and Link, Daniel and Shahbazian, Arman and Medvidovic, Nenad},
title = {A large-scale study of architectural evolution in open-source software systems},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9466-0},
doi = {10.1007/s10664-016-9466-0},
abstract = {From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated, sometimes careless changes to a system during its lifespan. Despite decay's prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers' understanding of those changes. In this paper, we take a step toward addressing that scarcity by introducing an architecture recovery framework, ARCADE, for conducting large-scale replicable empirical studies of architectural change across different versions of a software system. ARCADE includes two novel architectural change metrics, which are the key to enabling large-scale empirical studies of architectural change. We utilize ARCADE to conduct an empirical study of changes found in software architectures spanning several hundred versions of 23 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of departure in a system's architecture during the system's maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system's implementation-level structure as a proxy for its architecture.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1146–1193},
numpages = {48},
keywords = {Software evolution, Software architecture, Open-source software, Architecture recovery, Architectural change}
}

@article{10.1007/s10664-013-9263-y,
author = {Bjarnason, Elizabeth and Runeson, Per and Borg, Markus and Unterkalmsteiner, Michael and Engstr\"{o}m, Emelie and Regnell, Bj\"{o}rn and Sabaliauskaite, Giedre and Loconsole, Annabella and Gorschek, Tony and Feldt, Robert},
title = {Challenges and practices in aligning requirements with verification and validation: a case study of six companies},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9263-y},
doi = {10.1007/s10664-013-9263-y},
abstract = {Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two. We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VV activities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1809–1855},
numpages = {47},
keywords = {Verification, Validation, Testing, Requirements engineering, Case study, Alignment}
}

@inproceedings{10.5555/1045658.1045692,
author = {Billig, Andreas and Busse, Susanne and Leicher, Andreas and S\"{u}\ss{}, J\"{o}rn Guy},
title = {Platform independent model transformation based on triple},
year = {2004},
isbn = {3540234284},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Reuse is an important topic in software engineering as it promises advantages like faster time-to-market and cost reduction. Reuse of models on an abstract level is more beneficial than on the code level, because these models can be mapped into several technologies and can be adapted according to different requirements. Unfortunately, development tools only provide fixed mappings between abstract models described in a language such as UML and source code for a particular technology. These mappings are based on one-to-one relationships between elements of both levels. As a consequence, it is rarely possible to customize mappings according to specific user requirements.We aim to improve model reuse by providing a framework that generates customized mappings according to specified requirements. The framework is able to handle mappings aimed for several component technologies as it is based on an ADL. It is realized in Triple to represent components on different levels of abstraction and to perform the actual transformation. It uses feature models to describe mapping alternatives.},
booktitle = {Proceedings of the 5th ACM/IFIP/USENIX International Conference on Middleware},
pages = {493–511},
numpages = {19},
location = {Toronto, Canada},
series = {Middleware '04}
}

@inproceedings{10.1109/WI-IAT.2009.363,
author = {Li, Mu and Huai, JinPeng and Guo, HuiPeng},
title = {An Adaptive Web Services Selection Method Based on the QoS Prediction Mechanism},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.363},
doi = {10.1109/WI-IAT.2009.363},
abstract = {In recent years, many QoS-based web service selection methods have been proposed. However, as QoS changes dynamically, the atomic services of a composite web service could be replaced with other ones that have better quality. The performance of a composite web service will be decreased if this replacement happens frequently in runtime. Predicting the change of QoS accurately in select phase can effectively reduce this web services “thrash”. In this paper, we propose a web service selection algorithm GFS (Goodness-Fit Selection algorithm) based on QoS prediction mechanism in dynamic environments. We use structural equation to model the QoS measurement of web services. By taking the advantage of the prediction mechanism of structural equation model, we can quantitatively predict the change of quality of service dynamically. Optimal web service is selected based on the predicted results. Simulation results show that in dynamic environments, GFS provides higher selection accuracy than previous selection methods.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {395–402},
numpages = {8},
keywords = {web service selection, prediction, Structural Equation Modeling, QoS},
series = {WI-IAT '09}
}

@article{10.1145/1007775.811112,
author = {Josephs, William H.},
title = {A mini-computer based library control system},
year = {1978},
issue_date = {November 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3–4},
issn = {0163-5999},
url = {https://doi.org/10.1145/1007775.811112},
doi = {10.1145/1007775.811112},
abstract = {One of the major problems encountered in any large scale programming project is the control of the software. Invariably, such large programs are divided into many smaller elements since these are easier to code, test and document. However, such a division adds new complexity to the task of Configuration Management since the many source modules, data base elements, JCL (Job Control Language) and DATA files must be controlled with the goal of maximizing program integrity and minimizing the chances of procedural errors. Furthermore, whenever any program is released either for field test or for final production, an entire change control procedure must be implemented in order to trace, install, debug and verify fixes or extensions to the original program. These maintenance activities can account for up to 80 percent of the entire programming cost in a large, multi-year project. The library control program (SYSM) presented here was developed to aid in these processes. It has facilities for capturing all elements of a program (commonly called baselining), editing any element or group of elements that have been baselined to build an updated version of the program, adding and/or deleting elements of a program, and listing the current contents of a given element or elements. SYSM is written mainly in FORTRAN, and runs on a Hewlett-Packard HP-21MX computer with two tape drives, the vendor supplied RTE-II or RTE-III operating system, and at least 16K of user available core. It can be used to control code targeted for either the HP21MX itself, or, using the optional HP/LSI-11 link program, code targeted for a Digital Equipment Corp. LSI-11 system.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jan,
pages = {126–132},
numpages = {7}
}

@article{10.1016/j.future.2015.03.006,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated configuration support for infrastructure migration to the cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.03.006},
doi = {10.1016/j.future.2015.03.006},
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration-in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. We support the decision making in migration planning to the cloud.We use Feature Models to describe the configuration space of an IaaS.We automate the search of the most suitable IaaS configuration.Our approach improves the results of commercial applications on Amazon EC2.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {200–212},
numpages = {13},
keywords = {IaaS, Feature model, EC2, Cloud migration, Automated analysis}
}

@inproceedings{10.5555/2485288.2485422,
author = {Rahimi, Abbas and Marongiu, Andrea and Burgio, Paolo and Gupta, Rajesh K. and Benini, Luca},
title = {Variation-tolerant OpenMP tasking on tightly-coupled processor clusters},
year = {2013},
isbn = {9781450321532},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {We present a variation-tolerant tasking technique for tightly-coupled shared memory processor clusters that relies upon modeling advance across the hardware/software interface. This is implemented as an extension to the OpenMP 3.0 tasking programming model. Using the notion of Task-Level Vulnerability (TLV) proposed here, we capture dynamic variations caused by circuit-level variability as a high-level software knowledge. This is accomplished through a variation-aware hardware/software codesign where: (i) Hardware features variability monitors in conjunction with online per-core characterization of TLV metadata; (ii) Software supports a Task-level Errant Instruction Management (TEIM) technique to utilize TLV metadata in the runtime OpenMP task scheduler. This method greatly reduces the number of recovery cycles compared to the baseline scheduler of OpenMP [22], consequently instruction per cycle (IPC) of a 16-core processor cluster is increased up to 1.51\texttimes{} (1.17\texttimes{} on average). We evaluate the effectiveness of our approach with various number of cores (4,8,12,16), and across a wide temperature range(ΔT=90°C).},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {541–546},
numpages = {6},
location = {Grenoble, France},
series = {DATE '13}
}

@inproceedings{10.1145/2480362.2480567,
author = {Durelli, Rafael S. and Santib\'{a}\~{n}ez, Daniel S. M. and Anquetil, Nicolas and Delamaro, M\'{a}rcio E. and de Camargo, Valter Vieira},
title = {A systematic review on mining techniques for crosscutting concerns},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480567},
doi = {10.1145/2480362.2480567},
abstract = {&lt;u&gt;Background:&lt;/u&gt; The several maintenance tasks a system is submitted during its life usually cause its architecture deviates from the original conceivable design, ending up with scattered and tangled concerns across the software. The research area named concern mining attempts to identify such scattered and tangled concerns to support maintenance and reverse-engineering. &lt;u&gt;Objectives:&lt;/u&gt; The aim of this paper is threefold: (i) identifying techniques employed in this research area, (ii) extending a taxonomy available on the literature and (iii) recommending an initial combination of some techniques. &lt;u&gt;Results:&lt;/u&gt; We selected 62 papers by their mining technique. Among these papers, we identified 18 mining techniques for crosscutting concern. Based on these techniques, we have extended a taxonomy available in the literature, which can be used to position each new technique, and to compare it with the existing ones along relevant dimensions. As consequence, we present some combinations of these techniques taking into account high values of precision and recall that could improve the identification of both Persistence and Observer concerns. The combination that we recommend may serve as a roadmap to potential users of mining techniques for crosscutting concerns.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1080–1087},
numpages = {8},
keywords = {systematic review, cross-cutting concerns, concern mining, aspect mining},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@article{10.1016/j.adhoc.2017.03.010,
author = {Kishk, S. and Almofari, N.H. and Zaki, F.W.},
title = {Distributed resource allocation in D2D communication networks with energy harvesting relays using stable matching},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2017.03.010},
doi = {10.1016/j.adhoc.2017.03.010},
abstract = {Fifth Generation (5G) cellular networks are expected to provide high data rates by using emerging technologies such as multi-tier heterogeneous networks, Device to Device (D2D) communication and densification of small base stations. D2D uses direct transmission between two cellular devices to increase the system throughput. Relays are used to reduce the loss in user data rate when the D2D users are far from each other and the relay nodes are used to serve the cellular users when the channel is not good enough. To get good performance from D2D relay assisted network, suitable, robust and low complexity resource allocation algorithm must be used. In this paper, an algorithm for user association, resource blocks allocation and power control when considering the energy harvesting relays in heterogeneous multi-tier network is presented. This paper introduces a centralized solution using time sharing strategy and a distributed low complexity solution using stable matching theory.},
journal = {Ad Hoc Netw.},
month = jun,
pages = {114–123},
numpages = {10},
keywords = {Stable matching, Resource allocation, D2D relay assisted}
}

@inproceedings{10.5555/2028067.2028076,
author = {Snow, Kevin Z. and Krishnan, Srinivas and Monrose, Fabian and Provos, Niels},
title = {SHELLOS: enabling fast detection and forensic analysis of code injection attacks},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {The availability of off-the-shelf exploitation toolkits for compromising hosts, coupled with the rapid rate of exploit discovery and disclosure, has made exploit or vulnerability-based detection far less effective than it once was. For instance, the increasing use of metamorphic and polymorphic techniques to deploy code injection attacks continues to confound signature-based detection techniques. The key to detecting these attacks lies in the ability to discover the presence of the injected code (or, shellcode). One promising technique for doing so is to examine data (be that from network streams or buffers of a process) and efficiently execute its content to find what lurks within. Unfortunately, current approaches for achieving this goal are not robust to evasion or scalable, primarily because of their reliance on software-based CPU emulators. In this paper, we argue that the use of software-based emulation techniques are not necessary, and instead propose a new framework that leverages hardware virtualization to better enable the detection of code injection attacks. We also report on our experience using this framework to analyze a corpus of malicious Portable Document Format (PDF) files and network-based attacks.},
booktitle = {Proceedings of the 20th USENIX Conference on Security},
pages = {9},
numpages = {1},
location = {San Francisco, CA},
series = {SEC'11}
}

@article{10.1016/j.jss.2006.08.039,
author = {Kuz, Ihor and Liu, Yan and Gorton, Ian and Heiser, Gernot},
title = {CAmkES: A component model for secure microkernel-based embedded systems},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.08.039},
doi = {10.1016/j.jss.2006.08.039},
abstract = {Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services.},
journal = {J. Syst. Softw.},
month = may,
pages = {687–699},
numpages = {13},
keywords = {Microkernel, Embedded system, Component architecture}
}

@article{10.1007/s00034-018-1002-6,
author = {Chen, Chengying and Chen, Liming},
title = {A 79-dB SNR 1.1-mW Fully Integrated Hearing Aid SoC},
year = {2019},
issue_date = {July      2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {7},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-018-1002-6},
doi = {10.1007/s00034-018-1002-6},
abstract = {For low-power hearing aid device application, a fully integrated optimized hearing aid SoC structure is proposed in this paper. The SoC consists of high-resolution, low-power analog front-end (AFE), time-division-multiplexed power-on-reset circuit, charge pump, digital signal processing (DSP) platform, and Class-D amplifier. A novel peak-statistical algorithm is proposed to track signal amplitude and adjust automatic gain control loop gain precisely. A comparative DWA is applied to break the correlation of in-band tone and sequential selection scheme, which realizes second-order noise shaping and suppresses harmonic effectively. The SoC has been implemented with 0.13 µm CMOS process. By measurement, it shows that the peak signal-to-noise ratio (SNR) of AFE is 82.6 dB and peak SNR of Class-D amplifier is 79.8 dB. Also, three main algorithms of wide dynamic range compression, noise reduction, and feedback cancelation are executed through DSP platform. With 1 V supply voltage, total SoC power is 1.1 mW and core area is 9.3 mm2. Based on our SoC, a hearing aid device prototype is produced that shows its great potential for mass manufacture in the future.},
journal = {Circuits Syst. Signal Process.},
month = jul,
pages = {2893–2909},
numpages = {17},
keywords = {Low power, Hearing aid, DSP, Analog front-end}
}

@article{10.1155/2021/8822786,
author = {Zhao, Yiqing and Prabhu, M. and Ahmed, Ramyar Rzgar and Sahu, Anoop Kumar and Bruneo, Dario},
title = {Research Trends and Performance of IIoT Communication Network-Architectural Layers of Petrochemical Industry 4.0 for Coping with Circular Economy},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/8822786},
doi = {10.1155/2021/8822786},
abstract = {In the present era, many Petrochemical Industries (PIs) are driven energetically due to IIoT (Industrial Internet of Things) Communication Networks/Architectural Layers (CNs/ALs), abbreviated as PI4.0-CNs/ALs. PI4.0 fruitfully participated to achieve the Circular Economy (CE) by speeding the reutilization, recovery, and recycling of scrap materials by minimizing cost, unproductive operations, energy consumption, emission of flue gases, etc. Recently, it has been ascertained that the identification and measurement of Research Trends (RTs) of CNs-ALs help the PI4.0 to build the future CE. In addressing the said research challenge, the objective of this research dossier is turned towards inculcating into future PI4.0 researchers the RTs of CNs/ALs of PI4.0, so that the researches can be organized over the very weak and moderately performing CNs-ALs to hike the future CE. To materialize the RTs of PI4-CNs/ALs, the authors conducted the Systematic Literature Survey (SLS) focusing on PI4.0-CNs/ALs, i.e., Internet of Things (IoTs), Cyber Physical System (CPS), Virtual Reality (VR), Integration (I), Data Optimization (DO), Enterprise Resource Planning (ERP), Plant Control (PC), Data and Analytics (DA), Network (N), and Information and Data Management (IDM). The authors searched three hundred two research documents, wherein two hundred seventy-five research manuscripts qualified as RQ2. Next, the authors collected the DOIs/URLs corresponding to each CN-AL and explored the Sum of Digit Scoring System (SDSS) to summarize the DOIs/URLs of PI4.0-CNs/ALs. The RTs of DO have been determined as excellent and stronger over 2007-2017 than residue CNs/ALs. Eventually, the authors advised scholars to focus on the research areas of very weak and moderately weak performing CNs/ALs in order to attain future CE.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {32}
}

@inproceedings{10.5555/978-3-030-29983-5_fm,
title = {Front Matter},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {i–xxii},
location = {Paris, France}
}

@inproceedings{10.1145/2593882.2593895,
author = {Hatcliff, John and Wassyng, Alan and Kelly, Tim and Comar, Cyrille and Jones, Paul},
title = {Certifiably safe software-dependent systems: challenges and directions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593895},
doi = {10.1145/2593882.2593895},
abstract = {The amount and impact of software-dependence in critical systems impinging on daily life is increasing rapidly. In many of these systems, inadequate software and systems engineering can lead to economic disaster, injuries or death. Society generally does not recognize the potential of losses from deficiencies of systems due to software until after some mishap occurs. Then there is an outcry, reflecting societal expectations; however, few know what it takes to achieve the expected safety and, in general, loss-prevention.  On the one hand there are unprecedented, exponential increases in size, inter-dependencies, intricacies, numbers and variety in the systems and distribution of development processes across organizations and cultures. On the other hand, industry's capability to verify and validate these systems has not kept up. Mere compliance with existing standards, techniques, and regulations cannot guarantee the safety properties of these systems. The gap between practice and capability is increasing rapidly.  This paper considers the future of software engineering as needed to support development and certification of safety-critical software-dependent systems. We identify a collection of challenges and document their current state, the desired state, gaps and barriers to reaching the desired state, and potential directions in software engineering research and education that could address the gaps and barriers.},
booktitle = {Future of Software Engineering Proceedings},
pages = {182–200},
numpages = {19},
keywords = {verification, validation, standards, safety, requirements, hazard analysis, assurance, Certification},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@article{10.1186/s13638-020-01795-1,
author = {Cox, Bert and Van der Perre, Liesbet and Wielandt, Stijn and Ottoy, Geoffrey and De Strycker, Lieven},
title = {High precision hybrid RF and ultrasonic chirp-based ranging for low-power IoT nodes},
year = {2020},
issue_date = {Nov 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
number = {1},
issn = {1687-1472},
url = {https://doi.org/10.1186/s13638-020-01795-1},
doi = {10.1186/s13638-020-01795-1},
abstract = {Hybrid acoustic-RF systems offer excellent ranging accuracy, yet they typically come at a power consumption that is too high to meet the energy constraints of mobile IoT nodes. We combine pulse compression and synchronized wake-ups to achieve a ranging solution that limits the active time of the nodes to 1 ms. Hence, an ultra low-power consumption of 9.015 µW for a single measurement is achieved. The operation time is estimated on 8.5 years on a CR2032 coin cell battery at a 1 Hz update rate, which is over 250 times larger than state-of-the-art RF-based positioning systems. Measurements based on a proof-of-concept hardware platform show median distance error values below 10 cm. Both simulations and measurements demonstrate that the accuracy is reduced at low signal-to-noise ratios and when reflections occur. We introduce three methods that enhance the distance measurements at a low extra processing power cost. Hence, we validate in realistic environments that the centimeter accuracy can be obtained within the energy budget of mobile devices and IoT nodes. The proposed hybrid signal ranging system can be extended to perform accurate, low-power indoor positioning.},
journal = {EURASIP J. Wirel. Commun. Netw.},
month = sep,
numpages = {24},
keywords = {Acoustic signal processing, Pulse compression, Ultra low-power electronics, Hybrid signaling, Ranging}
}

@article{10.1016/j.jss.2016.03.038,
author = {Deb, Novarun and Chaki, Nabendu and Ghose, Aditya},
title = {Extracting finite state models from i* models},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.038},
doi = {10.1016/j.jss.2016.03.038},
abstract = {The Naive Algorithm (NA) extracts all possible finite state models from i* models.We observe an explosion in the finite state model space.The Semantic Implosion Algorithm (SIA) is a solution to this explosion problem.NA and SIA are extensively simulated on different categories of i* models.SIA drastically reduces the model space growth from O(1020) (for NA) to O(103). i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.},
journal = {J. Syst. Softw.},
month = nov,
pages = {265–280},
numpages = {16},
keywords = {i* model, Model transformation, Model checking}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Spanish community, Research trends, Systematic review, Search-based software engineering}
}

@inproceedings{10.1145/949344.949346,
author = {Thomas, Dave and Barry, Brian M.},
title = {Model driven development: the case for domain oriented programming},
year = {2003},
isbn = {1581137516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/949344.949346},
doi = {10.1145/949344.949346},
abstract = {In this paper, we offer an alternative vision for domain driven development (3D). Our approach is model driven and emphasizes the use of generic and specific domain oriented programming (DOP) languages. DOP uses strong specific languages, which directly incorporate domain abstractions, to allow knowledgeable end users to succinctly express their needs in the form of an application computation. Most domain driven development (3D) approaches and techniques are targeted at professional software engineers and computer scientists. We argue that DOP offers a promising alternative. Specifically we are focused on empowering application developers who have extensive domain knowledge as well as sound foundations in their professions, but may not be formally trained in computer science.We provide a brief survey of DOP experiences, which show that many of the best practices such as patterns, refactoring, and pair programming are naturally and ideally practiced in a Model Driven Development (MDD) setting. We compare and contrast our DOP with other popular approaches, most of which are deeply rooted in the OO community.Finally we highlight challenges and opportunities in the design and implementation of such languages.},
booktitle = {Companion of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {2–7},
numpages = {6},
keywords = {programming by professional end users, model driven development, end user programming, domain specific languages, domain driven development},
location = {Anaheim, CA, USA},
series = {OOPSLA '03}
}

@inproceedings{10.5555/3320516.3320905,
author = {Stoldt, Johannes and Prell, Bastian and Schlegel, Andreas and Putz, Matthias},
title = {Applications for models of renewable energy sources and energy storages in material flow simulation},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {The increasing reliance on volatile renewable energy sources in the European Union raises questions regarding the future mechanisms of the energy markets. Energy-intensive production industries are particularly expected to take a more active role by shaping their energy demand according to the availability of wind and sun. Hence, they will need to align their production processes with external energy market signals. This paper presents an application example for a Siemens Plant Simulation extension that makes holistic material flow and energy flow studies of factories possible. The so-called eniBRIC class library provides functionalities for investigating the flow of energy between infrastructure and production equipment. Since its latest update, it can also be used to model renewable energy sources as well as energy storages. A case study of the E3-Research Factory showcases the features of eniBRIC and provides an outlook on future research in the field of energy-flexible production.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3287–3298},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1007/11527800_25,
author = {Czarnecki, Krzysztof},
title = {Overview of generative software development},
year = {2004},
isbn = {3540278842},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527800_25},
doi = {10.1007/11527800_25},
abstract = {System family engineering seeks to exploit the commonalities among systems from a given problem domain while managing the variabilities among them in a systematic way. In system family engineering, new system variants can be rapidly created based on a set of reusable assets (such as a common architecture, components, models, etc.). Generative software development aims at modeling and implementing system families in such a way that a given system can be automatically generated from a specification written in one or more textual or graphical domain-specific languages. This paper gives an overview of the basic concepts and ideas of generative software development including DSLs, domain and application engineering, generative domain models, networks of domains, and technology projections. The paper also discusses the relationship of generative software development to other emerging areas such as Model Driven Development and Aspect-Oriented Software Development.},
booktitle = {Proceedings of the 2004 International Conference on Unconventional Programming Paradigms},
pages = {326–341},
numpages = {16},
location = {Le Mont Saint Michel, France},
series = {UPP'04}
}

@inproceedings{10.1007/978-3-642-35623-0_2,
author = {Toffetti, Giovanni},
title = {Web engineering for cloud computing (web engineering forecast: cloudy with a chance of opportunities)},
year = {2012},
isbn = {9783642356223},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35623-0_2},
doi = {10.1007/978-3-642-35623-0_2},
abstract = {Web Engineering has always been concerned with modelling the functional aspects of Web applications. Non-functional (e.g., performance, availability) properties of Web applications have traditionally been a minor concern in the Web engineering community and have been seen as technology- or system-related issues. The advent of Cloud computing, with substantial delegation of "system concerns" to infrastructure or platform providers, seems at a first sight to confirm the validity of this choice. But is this really true?We will argue that, in order to be able to actually profit from the Cloud computing paradigm, Web Engineering methodologies need several interventions transcending the platform-specific concerns of adapting to Cloud technologies.In this position paper, we call for a long-due revamp of Web engineering methodologies to become more sound engineering practices with respect to both functional and non-functional aspects of Web applications. To this end, we propose a methodological framework that preserves the advantages of model-driven development, but also takes into account performance and cost considerations for Cloud-based applications.},
booktitle = {Proceedings of the 12th International Conference on Current Trends in Web Engineering},
pages = {5–19},
numpages = {15},
location = {Berlin, Germany},
series = {ICWE'12}
}

@article{10.1007/s10270-015-0498-5,
author = {Rodrigues, Taniro and Delicato, Fl\'{a}via C. and Batista, Thais and Pires, Paulo F. and Pirmez, Luci},
title = {An approach based on the domain perspective to develop WSAN applications},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0498-5},
doi = {10.1007/s10270-015-0498-5},
abstract = {As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {949–977},
numpages = {29},
keywords = {WSAN applications, UML profile, Model-driven architecture, Domain-specific language, Code generation, Architecture, Abstraction}
}

@inproceedings{10.1145/2602928.2603080,
author = {Lytra, Ioanna and Sobernig, Stefan and Tran, Huy and Zdun, Uwe},
title = {A pattern language for service-based platform integration and adaptation},
year = {2012},
isbn = {9781450329439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602928.2603080},
doi = {10.1145/2602928.2603080},
abstract = {Often software systems accommodate one or more software platforms on top of which various applications are developed and executed. Different application areas, such as enterprise resource planning, mobile devices, telecommunications, and so on, require different and specialized platforms. Many of them offer their services using standardized interface technologies to support integration with the applications built on top of them and with other platforms. The diversity of platform technologies and interfaces, however, renders the integration of multiple platforms challenging. In this paper, we discuss design alternatives for tailoring heterogeneous service platforms by studying high-level and low-level architectural design decisions for integrating and for adapting platforms. We survey and organize existing patterns and design decisions in the literature as a pattern language. With this pattern language, we address the various decision categories and interconnections for the service-based integration and the adaptation of applications developed based on software platforms. We apply this pattern language in an industry case study.},
booktitle = {Proceedings of the 17th European Conference on Pattern Languages of Programs},
articleno = {4},
numpages = {27},
keywords = {service-based platform integration, pattern language, design patterns},
location = {Irsee, Germany},
series = {EuroPLoP '12}
}

@inproceedings{10.1145/3458864.3467880,
author = {Garg, Nakul and Bai, Yang and Roy, Nirupam},
title = {Owlet: enabling spatial information in ubiquitous acoustic devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467880},
doi = {10.1145/3458864.3467880},
abstract = {This paper presents a low-power and miniaturized design for acoustic direction-of-arrival (DoA) estimation and source localization, called Owlet. The required aperture, power consumption, and hardware complexity of the traditional array-based spatial sensing techniques make them unsuitable for small and power-constrained IoT devices. Aiming to overcome these fundamental limitations, Owlet explores acoustic microstructures for extracting spatial information. It uses a carefully designed 3D-printed metamaterial structure that covers the microphone. The structure embeds a direction-specific signature in the recorded sounds. Owlet system learns the directional signatures through a one-time in-lab calibration. The system uses an additional microphone as a reference channel and develops techniques that eliminate environmental variation, making the design robust to noises and multipaths in arbitrary locations of operations. Owlet prototype shows 3.6° median error in DoA estimation and 10cm median error in source localization while using a 1.5cm \texttimes{} 1.3cm acoustic structure for sensing. The prototype consumes less than 100th of the energy required by a traditional microphone array to achieve similar DoA estimation accuracy. Owlet opens up possibilities of low-power sensing through 3D-printed passive structures.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {255–268},
numpages = {14},
keywords = {spatial sensing, low-power sensing, acoustic metamaterial, IoT},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Software reuse, Software library, Search-based software engineering, Multi-objective optimization}
}

@inproceedings{10.1007/978-3-319-29339-4_21,
author = {Elibol, Ercan and Calderon, Juan and Llofriu, Martin and Quintero, Carlos and Moreno, Wilfrido and Weitzenfeld, Alfredo},
title = {Power Usage Reduction of Humanoid Standing Process Using Q-Learning},
year = {2015},
isbn = {978-3-319-29338-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29339-4_21},
doi = {10.1007/978-3-319-29339-4_21},
abstract = {An important area of research in humanoid robots is energy consumption, as it limits autonomy, and can harm task performance. This work focuses on power aware motion planning. Its principal aim is to find joint trajectories to allow for a humanoid robot to go from crouch to stand position while minimizing power consumption. Q-Learning (QL) is used to search for optimal joint paths subject to angular position and torque restrictions. A planar model of the humanoid is used, which interacts with QL during a simulated offline learning phase. The best joint trajectories found during learning are then executed by a physical humanoid robot, the Aldebaran NAO. Position, velocity, acceleration, and current of the humanoid system are measured to evaluate energy, mechanical power, and Center of Mass (CoM) in order to estimate the performance of the new trajectory which yield a considerable reduction in power consumption.},
booktitle = {RoboCup 2015: Robot World Cup XIX},
pages = {251–263},
numpages = {13},
keywords = {Humanoid, Dynamic modeling, Energy analysis, Optimization, Q-learning},
location = {Hefei, China}
}

@article{10.1177/0037549715603480,
author = {Peters, Brady},
title = {Integrating acoustic simulation in architectural design workflows},
year = {2015},
issue_date = {9 2015},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
volume = {91},
number = {9},
issn = {0037-5497},
url = {https://doi.org/10.1177/0037549715603480},
doi = {10.1177/0037549715603480},
abstract = {Sound is an important part of our experience of buildings. However, architects design largely using visually based techniques and largely for visual phenomena. Aiming to address this problem, the research presented in this paper proposes four digital design workflows that integrate acoustic computer simulation into architectural design. These techniques enable architects to design for both visual and acoustic criteria. The goal is to develop rapid and accessible workflows for architects that allow acoustic performance to be tuned as geometry and materials are modified at the scale of the room, and also at the scale of the surface. The discovery and testing of these techniques takes place within the design of the FabPod, a semi-enclosed meeting room situated within an open-plan working environment. The project builds on previous research investigating the design principles, the acoustic performance, and the fabrication methods of hyperboloid surface geometry. Four design workflows were developed: two of these investigate the acoustic performance of the room and use existing acoustic simulation software, and the other two workflows investigate the acoustic performance of the surface and use custom-written scripts to calculate and visualize sound scattering. This paper presents the background to the study, outlines the digital workflows, describes how they integrate acoustic simulation, and shows some of the data produced by these simulations.},
journal = {Simulation},
month = sep,
pages = {787–808},
numpages = {22},
keywords = {simulation visualization, performance-based design, design workflows, computer-aided design, architectural design, architectural acoustics, acoustic simulation}
}

@inbook{10.5555/2167873.2167886,
author = {Kalyanakrishnan, Shivaram and Hester, Todd and Quinlan, Michael and Bentor, Yinon and Stone, Peter},
title = {Three humanoid soccer platforms: comparison and synthesis},
year = {2010},
isbn = {3642118755},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this article, we provide an overview of three humanoid soccer platforms currently in use at RoboCup: 3D simulation, the humanoid Standard Platform League (SPL), and the Webots-based simulator released with the SPL. Although these platforms trace different historical roots, today they share the same robot model, the Aldebaran Nao. Consequently, they face a similar set of challenges, primary among which is the need to develop reliable and robust bipedal locomotion. In this paper, we compare and contrast these platforms, drawing on the experiences of our team, UT Austin Villa, in developing agents for each of them. We identify specific roles for these three platforms in advancing the overarching goals of RoboCup.},
booktitle = {RoboCup 2009: Robot Soccer World Cup XIII},
pages = {140–152},
numpages = {13}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {software measures, feature-oriented software development, feature interactions},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@article{10.1155/2011/726014,
author = {Sun, Guanyi and Xu, Shengnan and Wang, Xu and Wang, Dawei and Tang, Eugene and Deng, Yangdong and Chan, Sun},
title = {A high-throughput, high-accuracy system-level simulation framework for system on chips},
year = {2011},
issue_date = {January 2011},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2011},
issn = {1065-514X},
url = {https://doi.org/10.1155/2011/726014},
doi = {10.1155/2011/726014},
abstract = {Today's System-on-Chips (SoCs) design is extremely challenging because it involves complicated design tradeoffs and heterogeneous design expertise. To explore the large solution space, system architects have to rely on system-level simulators to identify an optimized SoC architecture. In this paper, we propose a system-level simulation framework, System Performance Simulation Implementation Mechanism, or SPSIM. Based on SystemC TLM2.0, the framework consists of an executable SoC model, a simulation tool chain, and a modeling methodology. Compared with the large body of existing research in this area, this work is aimed at delivering a high simulation throughput and, at the same time, guaranteeing a high accuracy on real industrial applications. Integrating the leading TLM techniques, our simulator can attain a simulation speed that is not slower than that of the hardware execution by a factor of 35 on a set of real-world applications. SPSIM incorporates effective timing models, which can achieve a high accuracy after hardware-based calibration. Experimental results on a set of mobile applications proved that the difference between the simulated and measured results of timing performance is within 10%, which in the past can only be attained by cycle-accurate models.},
journal = {VLSI Des.},
month = jan,
articleno = {6},
numpages = {17}
}

@inproceedings{10.1145/2541940.2541962,
author = {Zahedi, Seyed Majid and Lee, Benjamin C.},
title = {REF: resource elasticity fairness with sharing incentives for multiprocessors},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541962},
doi = {10.1145/2541940.2541962},
abstract = {With the democratization of cloud and datacenter computing, users increasingly share large hardware platforms. In this setting, architects encounter two challenges: sharing fairly and sharing multiple resources. Drawing on economic game-theory, we rethink fairness in computer architecture. A fair allocation must provide sharing incentives (SI), envy-freeness (EF), and Pareto efficiency (PE).We show that Cobb-Douglas utility functions are well suited to modeling user preferences for cache capacity and memory bandwidth. And we present an allocation mechanism that uses Cobb-Douglas preferences to determine each user's fair share of the hardware. This mechanism provably guarantees SI, EF, and PE, as well as strategy-proofness in the large (SPL). And it does so with modest performance penalties, less than 10% throughput loss, relative to an unfair mechanism.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {145–160},
numpages = {16},
keywords = {multiprocessor architectures, game theory, fair sharing, economic mechanisms},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1016/j.asoc.2017.02.016,
author = {Oladimeji, Muyiwa Olakanmi and Turkey, Mikdam and Dudley, Sandra},
title = {HACH},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.02.016},
doi = {10.1016/j.asoc.2017.02.016},
abstract = {Graphical abstractDisplay Omitted Wireless sensor networks (WSNs) require energy management protocols to efficiently use the energy supply constraints of battery-powered sensors to prolong its network lifetime. This paper proposes a novel Heuristic Algorithm for Clustering Hierarchy (HACH), which sequentially performs selection of inactive nodes and cluster head nodes at every round. Inactive node selection employs a stochastic sleep scheduling mechanism to determine the selection of nodes that can be put into sleep mode without adversely affecting network coverage. Also, the clustering algorithm uses a novel heuristic crossover operator to combine two different solutions to achieve an improved solution that enhances the distribution of cluster head nodes and coordinates energy consumption in WSNs. The proposed algorithm is evaluated via simulation experiments and compared with some existing algorithms. Our protocol shows improved performance in terms of extended lifetime and maintains favourable performances even under different energy heterogeneity settings.},
journal = {Appl. Soft Comput.},
month = jun,
pages = {452–461},
numpages = {10},
keywords = {Wireless sensor networks, Sleep scheduling, Heuristic crossover, Energy heterogeneity, Coverage, Clustering}
}

@inproceedings{10.5555/1862739.1862748,
author = {Thalheim, Bernhard and Schewe, Klaus-Dieter and Ma, Hui},
title = {Conceptual application domain modelling},
year = {2009},
isbn = {9781920682774},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Application domain description precedes requirements engineering, and is the basis for the development of a software or information system that satisfies all expectations of its users. The greatest challenge in this area is the evolution of the application domain itself. In this paper we address this problem by explicit consideration of application cases that are defined by user profiles and intentions and the system environment, i.e. scope and context. User profiles and intentions are captured through the concept of persona. We show how the application domain description can be mapped to requirements and discuss engineering of application domain descriptions.},
booktitle = {Proceedings of the Sixth Asia-Pacific Conference on Conceptual Modeling - Volume 96},
pages = {49–58},
numpages = {10},
location = {Wellington, New Zealand},
series = {APCCM '09}
}

@inproceedings{10.1007/978-3-642-33666-9_41,
author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
title = {Experiences of applying UML/MARTE on three industrial projects},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_41},
doi = {10.1007/978-3-642-33666-9_41},
abstract = {MARTE (Modeling and Analysis of Real-Time and Embedded Systems) is a UML profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In previous years, we have applied UML/MARTE to three distinct industrial problems in various industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experiences of solving these problems by applying UML/MARTE on four industrial case studies. Based on our common experiences, we derive a framework to help practitioners for future applications of UML/MARTE. The framework provides a set of detailed guidelines on how to apply MARTE in industrial contexts and will help reduce the gap between the modeling standards and industrial needs.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {642–658},
numpages = {17},
keywords = {real-time embedded systems, model-based testing, architecture modeling, UML, MARTE},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Video analysis, Variability, Combinatorial testing},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@article{10.1016/j.ijinfomgt.2015.09.008,
author = {Chang, Victor and Ramachandran, Muthu and Yao, Yulin and Kuo, Yen-Hung and Li, Chung-Sheng},
title = {A resiliency framework for an enterprise cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {36},
number = {1},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2015.09.008},
doi = {10.1016/j.ijinfomgt.2015.09.008},
abstract = {We have presented a resilient framework for an enterprise cloud.We have developed an architecture with four major services to demonstrate resiliency, where the cloud computing adoption framework (CCAF) takes the center role to blend other services.We explain how our work is relevant to business resiliency.We have the support from a large scale survey to ensure that our design and service can meet the large number of user requirements. This paper presents a systematic approach to develop a resilient software system which can be developed as emerging services and analytics for resiliency. While using the resiliency as a good example for enterprise cloud security, all resilient characteristics should be blended together to produce greater impacts. A framework, cloud computing adoption framework (CCAF), is presented in details. CCAF has four major types of emerging services and each one has been explained in details with regard to the individual function and how each one can be integrated. CCAF is an architectural framework that blends software resilience, service components and guidelines together and provides real case studies to produce greater impacts to the organizations adopting cloud computing and security. CCAF provides business alignments and provides agility, efficiency and integration for business competitive edge. In order to validate user requirements and system designs, a large scale survey has been conducted with detailed analysis provided for each major question. We present our discussion and conclude that the use of CCAF framework can illustrate software resilience and security improvement for enterprise security. CCAF framework itself is validated as an emerging service for enterprise cloud computing with analytics showing survey analysis.},
journal = {Int. J. Inf. Manag.},
month = feb,
pages = {155–166},
numpages = {12},
keywords = {Resilient software for Enterprise Cloud, Cloud security and software engineering best practice, Cloud computing Adoption Framework (CCAF), - Software resiliency}
}

@inproceedings{10.1007/978-3-642-12179-1_40,
author = {Salim, Flora Dilys and Burry, Jane},
title = {Software openness: evaluating parameters of parametric modeling tools to support creativity and multidisciplinary design integration},
year = {2010},
isbn = {3642121780},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12179-1_40},
doi = {10.1007/978-3-642-12179-1_40},
abstract = {The ubiquitous computing era has pushed the Architecture, Engineering, and Construction (AEC) industry towards new frontiers of digitally enabled practice. Are these the frontiers originally identified by the pioneers in the field? Architectural design has progressively shifted from two-dimensional paper based pencil sketched models to digital models drawn in various Computer-Aided Design (CAD) tools. The recent adoption of parametric modeling tools from the aerospace industry has been driven by the need for tools that can assist in rapid flexible modeling. The adaptation of parametric modeling has reformed both pedagogy and practice of architectural design. The question remains if parametric design has answered all the requirements specified by Steven Anson Coons in his 1963 proposal for a Computer-Aided Design (CAD) system. Given the growth of computational power and ubiquitous computing, how has CAD met the visions of its pioneers with respect to the flexibility and ease of communication with the computer and support of simultaneous design conversations with many designers working on the same project? This paper will revisit ideas conceived by the early inventors of CAD, explore the opportunities for advancing parametric modeling with the existing ubiquitous computing infrastructure, and introduces the notion of software openness to support creativity and multidisciplinary design integration.},
booktitle = {Proceedings of the 2010 International Conference on Computational Science and Its Applications - Volume Part III},
pages = {483–497},
numpages = {15},
keywords = {software openness, parametric modeling, parametric design, CAE, CAD},
location = {Fukuoka, Japan},
series = {ICCSA'10}
}

@article{10.1016/j.future.2019.01.042,
author = {Dawaliby, Samir and Bradai, Abbas and Pousset, Yannis},
title = {Adaptive dynamic network slicing in LoRa networks},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {98},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.01.042},
doi = {10.1016/j.future.2019.01.042},
journal = {Future Gener. Comput. Syst.},
month = sep,
pages = {697–707},
numpages = {11},
keywords = {Quality of service (QoS), Resource allocation, Network slicing, LoRa, Wireless networks, Internet of Things (IoT)}
}

@inproceedings{10.1007/11678779_7,
author = {Pesonen, Jani and Katara, Mika and Mikkonen, Tommi},
title = {Production-Testing of embedded systems with aspects},
year = {2005},
isbn = {3540326049},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11678779_7},
doi = {10.1007/11678779_7},
abstract = {A test harness plays an important role in the development of any embedded system. Although the harness can be excluded from final products, its architecture should support maintenance and reuse, especially in the context of testing product families. Aspect-orientation is a new technique for software architecture that should enable scattered and tangled code to be addressed in a modular fashion, thus facilitating maintenance and reuse. However, the design of interworking between object-oriented baseline architecture and aspects attached on top of it is an issue, which has not been solved conclusively. For industrial-scale use, guidelines on what to implement with objects and what with aspects should be derived. In this paper, we introduce a way to reflect the use of aspect-orientation to production testing software of embedded systems. Such piece of a test harness is used to smoke test the proper functionality of a manufactured device. The selection of suitable implementation technique is based on variance of devices to be tested, with aspects used as means for increased flexibility. Towards the end of the paper, we also present the results of our experiments in the Symbian OS context that show some obstacles in the current tool support that should be addressed before further case studies can be conducted.},
booktitle = {Proceedings of the First Haifa International Conference on Hardware and Software Verification and Testing},
pages = {90–102},
numpages = {13},
location = {Haifa, Israel},
series = {HVC'05}
}

@article{10.1016/j.jss.2013.06.064,
author = {Tahir, Abbas and Tosi, Davide and Morasca, Sandro},
title = {A systematic review on the functional testing of semantic web services},
year = {2013},
issue_date = {November, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.064},
doi = {10.1016/j.jss.2013.06.064},
abstract = {Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services.},
journal = {J. Syst. Softw.},
month = nov,
pages = {2877–2889},
numpages = {13},
keywords = {Testing approach, Systematic literature review, Semantic web services, Functional testing}
}

@article{10.1016/j.advengsoft.2015.08.009,
author = {Abanda, F.H. and Vidalakis, C. and Oti, A.H. and Tah, J.H.M.},
title = {A critical analysis of Building Information Modelling systems used in construction projects},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {90},
number = {C},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2015.08.009},
doi = {10.1016/j.advengsoft.2015.08.009},
abstract = {Construction life cycle project needs to be understood for BIM to be easily integrated.Interoperability is crucial for the uptake of BIM in the construction industry.Supply chain members need to collaborate on projects to deliver BIM compliant projects.There is a need for vendors/manufacturers to adopt a common language in publishing information about BIM software. Building Information Modelling (BIM) is now a global digital technology which is widely believed to have the potential to revolutionise the construction industry. This has been mainly a result of worldwide government initiatives promoting BIM uptake to improve efficiency and quality in delivering construction projects. This push has been accompanied by the release of a tremendous amount of BIM software systems which are now available in the market. Although this can be seen as a positive development, one cannot ignore how it has overwhelmed many professionals who cannot easily distinguish between the uses of these software systems. Previous studies about different BIM systems have generally been limited in scope focusing predominantly on operational issues. This study aims to conduct a comprehensive and critical appraisal of a wide range of BIM software systems currently being used in managing construction project information. To achieve this, five main methods are adopted. These include a systematic review of the literature, a structured questionnaire survey, action learning, focus group discussions and email surveys. It has to be noted that, although it is impossible to examine the totality of BIM systems, the study adopts a holistic approach looking at most of the major BIM system categories and 122 application examples which are common in the architecture, engineering and construction (AEC) industry.},
journal = {Adv. Eng. Softw.},
month = dec,
pages = {183–201},
numpages = {19},
keywords = {Plug-ins, Interoperability, Decision-support, Construction, Classification, BIM}
}

@article{10.1287/msom.2018.0710,
author = {Fu, Wayne and Kalkanci, Basak and Subramanian, Ravi},
title = {Are Hazardous Substance Rankings Effective? An Empirical Investigation of Information Dissemination About the Relative Hazards of Chemicals and Emissions Reductions},
year = {2019},
issue_date = {Summer 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {21},
number = {3},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2018.0710},
doi = {10.1287/msom.2018.0710},
abstract = {Problem definition: Whether information dissemination about chemical hazards drives managers at facilities to undertake corresponding environmental actions, remains an open question that has not been adequately examined in the literature. Academic/practical relevance: We fill this gap in the literature by empirically investigating reductions in chemical emissions by facilities in relation to changes in the assessed hazard levels of chemicals evidenced in periodically-updated public information. We also examine the moderating effects of operational leanness—an attribute that prior studies have shown to be associated with better environmental performance—in our setting wherein the assessed hazard levels of chemicals change over time. Methodology: We draw data from four U.S. sources—the Substance Priority List from the Agency for Toxic Substances and Disease Registry, the Toxics Release Inventory from the EPA, the National Establishment Time-Series, and Compustat. We employ a panel model with facility-chemical- and time-fixed effects. Results: We find that public information dissemination on chemical hazards is effective, as indicated by the significant association between increases in the assessed hazard levels of chemicals and greater subsequent emissions reductions. Specifically, we find that facilities reduce emissions by an additional 4.28% on average, and their use of source reduction increases by 3.07% on average when the relative assessed hazard level of a chemical increases compared to when it decreases. We find that, overall, leaner facilities outperform less lean facilities with respect to emissions reductions. However, when the assessed hazard level increases, less lean facilities increase their emissions reductions more than leaner facilities. Managerial implications: Our findings provide insights for managers prioritizing environmental actions, including the extent of emissions reductions achievable by practicing lean. Our results can also be leveraged by governmental/nongovernmental organizations to anticipate responses to informational updates on chemical hazards, depending on characteristics of the affected facilities.},
journal = {Manufacturing &amp; Service Operations Management},
month = jul,
pages = {602–619},
numpages = {18},
keywords = {Toxics Release Inventory, information dissemination, lean operations, hazardous chemicals, environmental operations}
}

@inproceedings{10.5555/1787553.1787572,
author = {Canal, Carlos and Murillo, Juan Manuel and Poizat, Pascal},
title = {Practical approaches for software adaptation: report on the 4th workshop WCAT at ECOOP 2007},
year = {2007},
isbn = {3540781943},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Coordination and Adaptation are two key issues when developing complex distributed systems. Coordination focuses on the interaction among software entities. Adaptation focuses on solving the problems that arise when the interacting entities do not match properly. This is the report of the fourth edition of the WCAT workshop, that took place in Berlin jointly with ECOOP 2007. Previous editions the workshop dealt with general issues which mainly served for a better characterization of Software Adaptation as an emerging discipline within the field of Software Engineering. For this edition, we wanted to put the focus on practical approaches for software adaptation, in order to show how this discipline helps in the construction of current software systems.},
booktitle = {Proceedings of the 2007 Conference on Object-Oriented Technology},
pages = {154–165},
numpages = {12},
location = {Berlin, Germany},
series = {ECOOP'07}
}

@article{10.1155/2013/683615,
author = {Chattopadhyay, Anupam},
title = {Ingredients of adaptability: a survey of reconfigurable processors},
year = {2013},
issue_date = {January 2013},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2013},
issn = {1065-514X},
url = {https://doi.org/10.1155/2013/683615},
doi = {10.1155/2013/683615},
abstract = {For a design to survive unforeseen physical effects like aging, temperature variation, and/or emergence of new application standards, adaptability needs to be supported. Adaptability, in its complete strength, is present in reconfigurable processors, which makes it an important IP in modern System-on-Chips (SoCs). Reconfigurable processors have risen to prominence as a dominant computing platform across embedded, general-purpose, and high-performance application domains during the last decade. Significant advances have been made in many areas such as, identifying the advantages of reconfigurable platforms, their modeling, implementation flow and finally towards early commercial acceptance. This paper reviews these progresses from various perspectives with particular emphasis on fundamental challenges and their solutions. Empowered with the analysis of past, the future research roadmap is proposed.},
journal = {VLSI Des.},
month = jan,
articleno = {10},
numpages = {1}
}

@inproceedings{10.1007/978-3-642-31762-0_13,
author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tapia Tarifa, S. Lizeth},
title = {A formal model of user-defined resources in resource-restricted deployment scenarios},
year = {2011},
isbn = {9783642317613},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31762-0_13},
doi = {10.1007/978-3-642-31762-0_13},
abstract = {Software today is often developed for deployment on varying architectures. In order to model and analyze the consequences of such deployment choices at an early stage in software development, it seems desirable to capture aspects of low-level deployment concerns in high-level models. In this paper, we propose an integration of a generic cost model for resource consumption with deployment components in Timed ABS, an abstract behavioral specification language for executable object-oriented models. The actual cost model may be user-defined and specified by means of annotations in the executable Timed ABS model, and can be used to capture specific resource requirements such as processing capacity or memory usage. Architectural variations are specified by resource-restricted deployment scenarios with different capacities. For this purpose, the models have deployment components which are parametric in their assigned resources. The approach is demonstrated on an example of multimedia processing servers with a user-defined cost model for memory usage. We use our simulation tool to analyze deadline misses for given usage and deployment scenarios.},
booktitle = {Proceedings of the 2011 International Conference on Formal Verification of Object-Oriented Software},
pages = {196–213},
numpages = {18},
location = {Turin, Italy},
series = {FoVeOOS'11}
}

@inproceedings{10.5555/1129601.1129626,
author = {Chen, Tung-Chieh and Chang, Yao-Wen and Lin, Shyh-Chang},
title = {IMF: interconnect-driven multilevel floorplanning for large-scale building-module designs},
year = {2005},
isbn = {078039254X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present in this paper, a new interconnect-driven multilevel floorplanning, called IMF, to handle large-scale building-module designs. Unlike the traditional multilevel framework that adopts the "V-cycle" framework: bottom-up coarsening followed by top-down uncoarsening, in contrast, IMF works in the "/spl Lambda/-cycle" manner: top-down uncoarsening (partitioning) followed by bottom-up coarsening (merging). The top-down partitioning stage iteratively partitions the floorplan region based on mm-cut bipartitioning with exact net-weight modeling to reduce the number of global interconnections and thus the total wirelength. Then, the bottom-up merging stage iteratively applies fixed-outline floorplanning using simulated annealing for all regions and merges two neighboring regions recursively. We also propose an accelerative fixed-outline floorplanning (AFF) to speed up wirelength minimization under the outline constraint. Experimental results show that IMF consistently obtains the best floorplanning results with the smallest wirelength for large-scale building-module designs, compared with all publicly available floorplanners. In particular, IMF scales very well as the circuit size increases. The /spl Lambda/-cycle multilevel framework outperforms the V-cycle one in the optimization of global circuit effects, such as interconnection and crosstalk optimization, since the /spl Lambda/-cycle framework considers the global configuration first and then processes down to local ones level by level and thus the global effects can be handled at earlier stages. The /spl Lambda/-cycle multilevel framework is general and thus can be readily applied to other problems.},
booktitle = {Proceedings of the 2005 IEEE/ACM International Conference on Computer-Aided Design},
pages = {159–164},
numpages = {6},
location = {San Jose, CA},
series = {ICCAD '05}
}

@article{10.1016/j.infsof.2009.11.008,
author = {Ovaska, Eila and Evesti, Antti and Henttonen, Katja and Palviainen, Marko and Aho, Pekka},
title = {Knowledge based quality-driven architecture design and evaluation},
year = {2010},
issue_date = {June, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.008},
doi = {10.1016/j.infsof.2009.11.008},
abstract = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {577–601},
numpages = {25},
keywords = {Tool, Software architecture, Quality attribute, Ontology, Model-driven development, Evaluation}
}

@article{10.1007/s10270-019-00735-y,
author = {Wolny, Sabine and Mazak, Alexandra and Carpella, Christine and Geist, Verena and Wimmer, Manuel},
title = {Thirteen years of SysML: a systematic mapping study},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00735-y},
doi = {10.1007/s10270-019-00735-y},
abstract = {The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML.},
journal = {Softw. Syst. Model.},
month = jan,
pages = {111–169},
numpages = {59},
keywords = {Systems engineering, Systematic mapping study, SysML}
}

@article{10.1016/j.specom.2017.02.002,
author = {Wu, Liang and Xiao, Ke and Wang, Supin and Wan, Mingxi},
title = {Vocal efficiency of electrolaryngeal speech production},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.02.002},
doi = {10.1016/j.specom.2017.02.002},
abstract = {Vocal efficiency of EL speech production was defined and measured.The actual utilization efficiency of the battery energy was no more than 0.1%.The energy transfer process was divided into three successive stages.The non-linear transducer and the neck tissue features were main reasons for power losses. From the perspective of efficiency, this article studied the energy transfer and conversion in the process of electrolaryngeal (EL) speech production. An overall vocal efficiency of EL speech production was defined as the ratio of the acoustic power of the EL speech to the electric power supplied by the battery. The measurements of a commercial EL showed that the actual utilization efficiency of the battery energy was no more than 0.1%. The energy transfer process was divided into three successive stages. The corresponding efficiencies of these stages were defined and estimated to analyze potential power losses and possible impact of two factors (EL cap and vowel) on the vocal efficiency. It was concluded that the non-linear transducer of the EL device and the physiological features of the neck tissue were the main reasons for the high power losses and low vocal efficiency. Furthermore, both EL cap and phonation vowel showed significant effects on the EL vocal efficiency. Thus, improvement of EL linear vibrator and compatible cap will be beneficial to raising the vocal efficiency and improving the EL speech quality.},
journal = {Speech Commun.},
month = may,
pages = {17–24},
numpages = {8},
keywords = {Vocal efficiency, Energy transfer, Electrolaryngeal speech}
}

@inproceedings{10.1109/ASEW.2008.4686323,
author = {Brcina, Robert and Riebisch, Matthias},
title = {Architecting for evolvability by means of traceability and features},
year = {2008},
isbn = {9781424427765},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASEW.2008.4686323},
doi = {10.1109/ASEW.2008.4686323},
abstract = {The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systems' evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {III–72–III–81},
location = {L'Aquila, Italy},
series = {ASE'08}
}

@article{10.1145/383845.383863,
author = {Coady, Yvonne and Kiczales, Gregor and Feeley, Mike and Hutchinson, Norm and Ong, Joon Suan},
title = {Structuring operating system aspects: using AOP to improve OS structure modularity},
year = {2001},
issue_date = {Oct. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/383845.383863},
doi = {10.1145/383845.383863},
journal = {Commun. ACM},
month = oct,
pages = {79–82},
numpages = {4}
}

@article{10.1145/2638550,
author = {Wu, Lisa and Polychroniou, Orestis and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Energy Analysis of Hardware and Software Range Partitioning},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/2638550},
doi = {10.1145/2638550},
abstract = {Data partitioning is a critical operation for manipulating large datasets because it subdivides tasks into pieces that are more amenable to efficient processing. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries. This article measures the performance and energy of state-of-the-art software partitioners, and describes and evaluates a hardware range partitioner that further improves efficiency.The software implementation is broken into two phases, allowing separate analysis of the partition function computation and data shuffling costs. Although range partitioning is commonly thought to be more expensive than simpler strategies such as hash partitioning, our measurements indicate that careful data movement and optimization of the partition function can allow it to approach the throughput and energy consumption of hash or radix partitioning.For further acceleration, we describe a hardware range partitioner, or HARP, a streaming framework that offers a seamless execution environment for this and other streaming accelerators, and a detailed analysis of a 32nm physical design that matches the throughput of four to eight software threads while consuming just 6.9% of the area and 4.3% of the power of a Xeon core in the same technology generation.},
journal = {ACM Trans. Comput. Syst.},
month = aug,
articleno = {8},
numpages = {24},
keywords = {streaming data, specialized functional unit, microarchitecture, data partitioning, Accelerator}
}

@article{10.1145/3351239,
author = {Gao, Yang and Wang, Wei and Phoha, Vir V. and Sun, Wei and Jin, Zhanpeng},
title = {EarEcho: Using Ear Canal Echo for Wearable Authentication},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351239},
doi = {10.1145/3351239},
abstract = {Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19% and precision of 95.16% for one-time authentication, while a recall of 97.55% and precision of 97.57% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {81},
numpages = {24},
keywords = {wearable devices, echo, ear canal, biometric, authentication, Acoustic}
}

@inproceedings{10.5555/2362793.2362812,
author = {Borders, Kevin and Springer, Jonathan and Burnside, Matthew},
title = {Chimera: a declarative language for streaming network traffic analysis},
year = {2012},
publisher = {USENIX Association},
address = {USA},
abstract = {Intrusion detection systems play a vital role in network security. Central to these systems is the language used to express policies. Ideally, this language should be powerful, implementation-agnostic, and cross-platform. Unfortunately, today's popular intrusion detection systems fall short of this goal. Each has their own policy language in which expressing complicated logic requires implementation-specific code. Database systems have adapted SQL to handle streaming data, but have yet to achieve the efficiency and flexibility required for complex intrusion detection tasks.In this paper, we introduce Chimera, a declarative query language for network traffic processing that bridges the gap between powerful intrusion detection systems and a simple, platform-independent SQL syntax. Chimera extends streaming SQL languages to better handle network traffic by adding structured data types, first-class functions, and dynamic window boundaries. We show how these constructs can be applied to real-world scenarios, such as side-jacking detection and DNS feature extraction. Finally, we describe the implementation and evaluation of a compiler that translates Chimera queries into low-level code for the Bro event language.},
booktitle = {Proceedings of the 21st USENIX Conference on Security Symposium},
pages = {19},
numpages = {1},
location = {Bellevue, WA},
series = {Security'12}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@inbook{10.5555/1986548.1986549,
author = {Kienzle, J\"{o}rg and Guelfi, Nicolas and Mustafiz, Sadaf},
title = {Crisis management systems: a case study for aspect-oriented modeling},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The intent of this document is to define a common case study for the aspect-oriented modeling research community. The domain of the case study is crisis management systems, i.e., systems that help in identifying, assessing, and handling a crisis situation by orchestrating the communication between all parties involved in handling the crisis, by allocating and managing resources, and by providing access to relevant crisis-related information to authorized users. This document contains informal requirements of crisis management systems (CMSs) in general, a feature model for a CMS product line, use case models for a car crash CMS (CCCMS), a domain model for the CCCMS, an informal physical architecture description of the CCCMS, as well as some design models of a possible object-oriented implementation of parts of the CCCMS backend. AOM researchers who want to demonstrate the power of their AOM approach or technique can hence apply the approach at the most appropriate level of abstraction.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {1–22},
numpages = {22}
}

@inbook{10.5555/1980562.1980563,
author = {Kienzle, J\"{o}rg and Guelfi, Nicolas and Mustafiz, Sadaf},
title = {Crisis management systems: a case study for aspect-oriented modeling},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The intent of this document is to define a common case study for the aspect-oriented modeling research community. The domain of the case study is crisis management systems, i.e., systems that help in identifying, assessing, and handling a crisis situation by orchestrating the communication between all parties involved in handling the crisis, by allocating and managing resources, and by providing access to relevant crisis-related information to authorized users. This document contains informal requirements of crisis management systems (CMSs) in general, a feature model for a CMS product line, use case models for a car crash CMS (CCCMS), a domain model for the CCCMS, an informal physical architecture description of the CCCMS, as well as some design models of a possible object-oriented implementation of parts of the CCCMS backend. AOM researchers who want to demonstrate the power of their AOM approach or technique can hence apply the approach at the most appropriate level of abstraction.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {1–22},
numpages = {22}
}

@article{10.1007/s11235-014-9886-3,
author = {Wan, Liangtian and Han, Guangjie and Rodrigues, Joel J. and Si, Weijian and Feng, Naixing},
title = {An energy efficient DOA estimation algorithm for uncorrelated and coherent signals in virtual MIMO systems},
year = {2015},
issue_date = {May       2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {59},
number = {1},
issn = {1018-4864},
url = {https://doi.org/10.1007/s11235-014-9886-3},
doi = {10.1007/s11235-014-9886-3},
abstract = {The multiple input and multiple output (MIMO) and smart antenna (SA) technique have been widely accepted as promising schemes to improve the spectrum efficiency and coverage of mobile communication systems. The definition of direction-of-arrival (DOA) estimation is that multiple directions of incident signals can be estimated simultaneously by some algorithms using the received data. The conventional DOA estimation of user equipments (UEs) is one by one, which is named as sequential scheme. The Virtual MIMO (VMIMO) scheme is that the base station (BS) estimates the DOAs of UEs in a parallel way, which adopts the SA simultaneously. Obviously, when the power is fixed, the VMIMO scheme is much more energy efficient than the sequential scheme. In VMIMO scheme, a set of UEs are grouped together to simultaneously communicate with the BS on a given resource block. Then the BS using multiple antennas can estimate the 2D-DOA of the UEs in the group simultaneously. Based on VMIMO system, the 2D-DOA estimation algorithm for uncorrelated and coherent signals is proposed in this paper. The special structure of mutual coupling matrix (MCM) of uniform linear array (ULA) is applied to eliminate the effect of mutual coupling. The 2D-DOA of uncorrelated signals can be estimated by DOA-matrix method. The parameter pairing between azimuth and elevation is accomplished. Then these estimations are utilized to get the mutual coupling coefficients. Based on spatial smoothing and DOA matrix method, the 2D-DOA of coherent signals can be estimated. The Cramer---Rao lower bound is derived at last. Simulation results demonstrate the effectiveness and performance of the proposed algorithm.},
journal = {Telecommun. Syst.},
month = may,
pages = {93–110},
numpages = {18},
keywords = {VMIMO system, Mutual coupling, Multipath, DOA estimation}
}

@inproceedings{10.5555/1762146.1762166,
author = {D'Alberto, Paolo and P\"{u}schel, Markus and Franchetti, Franz},
title = {performance/energy optimization of dsp transforms on the XScale processor},
year = {2007},
isbn = {9783540693376},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The XScale processor family provides user-controllable independent configuration of CPU, bus, and memory frequencies. This feature introduces another handle for the code optimization with respect to energy consumption or runtime performance. We quantify the effect of frequency configurations on both performance and energy for three signal processing transforms: the discrete Fourier transform (DFT), finite impulse response (FIR) filters, and the Walsh-Hadamard Transform (WHT).To do this, we use SPIRAL, a program generation and optimization system for signal processing transforms. For a given transform to be implemented, SPIRAL searches over different algorithms to find the best match to the given platform with respect to the chosen performance metric (usually runtime). In this paper we use SPIRAL to generate implementations for different frequency configurations and optimize for runtime and physically measured energy consumption. In doing so we show that first, each transform achieves best performance/energy consumption for a different system configuration; second, the best code depends on the chosen configuration, problem size and algorithm; third, the fastest implementation is not always the most energy efficient; fourth, we introduce dynamic (i.e., during execution) reconfiguration in order to further improve performance/energy. Finally, we benchmark SPIRAL generated code against Intel's vendor library routines. We show competitive results as well as 20% performance improvements or energy reduction for selected transforms and problem sizes.},
booktitle = {Proceedings of the 2nd International Conference on High Performance Embedded Architectures and Compilers},
pages = {201–214},
numpages = {14},
location = {Ghent, Belgium},
series = {HiPEAC'07}
}

@article{10.1145/2339118.2339120,
author = {Abd-El-Malek, Michael and Wachs, Matthew and Cipar, James and Sanghi, Karan and Ganger, Gregory R. and Gibson, Garth A. and Reiter, Michael K.},
title = {File system virtual appliances: Portable file system implementations},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/2339118.2339120},
doi = {10.1145/2339118.2339120},
abstract = {File system virtual appliances (FSVAs) address the portability headaches that plague file system (FS) developers. By packaging their FS implementation in a virtual machine (VM), separate from the VM that runs user applications, they can avoid the need to port the file system to each operating system (OS) and OS version. A small FS-agnostic proxy, maintained by the core OS developers, connects the FSVA to whatever OS the user chooses. This article describes an FSVA design that maintains FS semantics for unmodified FS implementations and provides desired OS and virtualization features, such as a unified buffer cache and VM migration. Evaluation of prototype FSVA implementations in Linux and NetBSD, using Xen as the virtual machine manager (VMM), demonstrates that the FSVA architecture is efficient, FS-agnostic, and able to insulate file system implementations from OS differences that would otherwise require explicit porting.},
journal = {ACM Trans. Storage},
month = sep,
articleno = {9},
numpages = {26},
keywords = {virtual machines, file systems, Operating systems}
}

@inproceedings{10.5555/1807906.1807954,
author = {Grigoriu, Mircea and Popescu, Marius-Constantin and Popescu, Luminita-Georgeta and Popescu, Cristinel and Dinu, Doina Cornelia},
title = {Environmental air conditioned requirements new approach},
year = {2010},
isbn = {9789604741595},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {There are different ways in air conditioned characteristics selection and approach, following the specific application (domestic, office, industrial or technological. For each practical situation, there are designed special parameters, defined on the application necessities, climate conditions, field dimensions, temperature required, thermal losses etc. In domestic application, field studies demonstrate that there are substantial numbers of not-satisfied people in many buildings, among them those suffering from Sick Building Syndrome symptoms, even there are established special standards and guidelines to comply. Based on existing information and on new research results, five principles are suggested as elements behind a new philosophy of excellence: better indoor air quality increases productivity and decreases Sick Building Syndrome (SBS) symptoms; unnecessary indoor pollution sources should be avoided; the air should be served cool and dry to the occupants; "personalized air", i.e. a small amount of clean air, should be served gently, close to the breathing zone of each individual.},
booktitle = {Proceedings of the 5th IASME/WSEAS International Conference on Energy &amp; Environment},
pages = {257–260},
numpages = {4},
keywords = {sick building, productivity, health, comfort, air quality, air conditioning},
location = {UK},
series = {EE'10}
}

@article{10.1007/s10270-017-0592-y,
author = {Ross, Jordan A. and Murashkin, Alexandr and Liang, Jia Hui and Antkiewicz, Micha\l{} and Czarnecki, Krzysztof},
title = {Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0592-y},
doi = {10.1007/s10270-017-0592-y},
abstract = {In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today's engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multi-level, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of question templates, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive sub-systems.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {739–767},
numpages = {29},
keywords = {Multi-perspective architectures, Multi-level architectures, Early design, E/E architecture, Candidate architectures, Architecture synthesis, Architecture optimization}
}

@inproceedings{10.1145/3412382.3458260,
author = {Xu, Weitao and Li, Zhenjiang and Xue, Wanli and Yu, Xiaotong and Wei, Bo and Wang, Jia and Luo, Chengwen and Li, Wei and Zomaya, Albert Y.},
title = {InaudibleKey: Generic Inaudible Acoustic Signal based Key Agreement Protocol for Mobile Devices},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3458260},
doi = {10.1145/3412382.3458260},
abstract = {Secure Device-to-Device (D2D) communication is becoming increasingly important with the ever-growing number of Internet-of-Things (IoT) devices in our daily life. To achieve secure D2D communication, the key agreement between different IoT devices without any prior knowledge is becoming desirable. Although various approaches have been proposed in the literature, they suffer from a number of limitations, such as low key generation rate and short pairing distance. In this paper, we present InaudibleKey, an inaudible acoustic signal based key generation protocol for mobile devices. Based on acoustic channel reciprocity, InaudibleKey exploits the acoustic channel frequency response of two legitimate devices as a common secret to generating keys. InaudibleKey employs several novel technologies to significantly improve its performance. We conduct extensive experiments to evaluate the proposed system in different real environments. Compared to state-of-the-art works, InaudibleKey improves key generation rate by 3 times, extends pairing distance by 3.2 times, and reduces information reconciliation counts by 2.5 times. Security analysis demonstrates that InaudibleKey is resilient to a number of malicious attacks. We also implement InaudibleKey on modern smartphones and resource-limited IoT devices. Results show that it is energy-efficient and can run on both powerful and resource-limited IoT devices without incurring excessive resource consumption.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {106–118},
numpages = {13},
keywords = {Mobile devices, Key generation, Device pairing, Acoustic signal},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@inproceedings{10.5555/244522.244552,
author = {Cong, Jason and He, Lei},
title = {An efficient approach to simultaneous transistor and interconnect sizing},
year = {1997},
isbn = {0818675977},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we study the simultaneous transistor and interconnect sizing (STIS) problem. We define a class of optimization problems as CH-posynomial programs and reveal a general dominance property for all CH-posynomial programs. We show that the STIS problems under a number of transistor delay models are CH-posynomial programs and propose an efficient and near-optimal STIS algorithm based on the dominance property. When used to solve the simultaneous driver/buffer and wire sizing problem for real designs, it reduces the maximum delay by up to 16.1%, and more significantly, reduces the power consumption by a factor of 1.63X, when compared with the original designs. When used to solve the transistor sizing problem, it achieves a smooth area-delay trade-off. Moreover, the algorithm optimizes a clock net of 367 drivers/buffers and 59304 /spl mu/m-long wire in 120 seconds, and a 32-bit adder with 1026 transistors in 66 seconds on a SPARC-5 workstation.},
booktitle = {Proceedings of the 1996 IEEE/ACM International Conference on Computer-Aided Design},
pages = {181–186},
numpages = {6},
keywords = {wire sizing problem, transistor sizing, transistor and interconnect sizing, driver/buffer, circuit CAD, STIS, CH-posynomial programs},
location = {San Jose, California, USA},
series = {ICCAD '96}
}

@inproceedings{10.5555/1781974.1782003,
author = {On, JeongSeok and Kim, JaeHyun and Lee, Jaiyong and Kim, Yeonsoo and Chong, Hakjin},
title = {A MAC protocol with adaptive preloads considering low duty-cycle in WSNs},
year = {2007},
isbn = {3540770232},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Leading MAC protocols developed for duty-cycled WSNs such as BMAC employ a long preamble and channel sampling. The long preamble introduces excess latency at each hop and results in excess energy consumption at non-target receivers in ultra low-duty cycled WSNs. In this paper we propose AS-MAC (Asynchronous Sensor MAC), a low power MAC protocol for wireless sensor networks (WSNs). AS-MAC solves these problems by employing a series of preload approach that retains the advantages of low power listening and independent channel sampling schedule. The preload massage includes a destination address and a remaining time until data transmission. Moreover ASMAC offers an additional advantage such as flexible duty cycle as data rate varies. We demonstrate that AS-MAC is better performance than B-MAC through analysis and evaluation.},
booktitle = {Proceedings of the 3rd International Conference on Mobile Ad-Hoc and Sensor Networks},
pages = {269–280},
numpages = {12},
location = {Beijing, China},
series = {MSN'07}
}

@article{10.1007/s10515-017-0215-4,
author = {Boussa\"{\i}d, Ilhem and Siarry, Patrick and Ahmed-Nacer, Mohamed},
title = {A survey on search-based model-driven engineering},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0215-4},
doi = {10.1007/s10515-017-0215-4},
abstract = {Model-driven engineering (MDE) and search-based software engineering (SBSE) are both relevant approaches to software engineering. MDE aims to raise the level of abstraction in order to cope with the complexity of software systems, while SBSE involves the application of metaheuristic search techniques to complex software engineering problems, reformulating engineering tasks as optimization problems. The purpose of this paper is to survey the relatively recent research activity lying at the interface between these two fields, an area that has come to be known as search-based model-driven engineering. We begin with an introduction to MDE, the concepts of models, of metamodels and of model transformations. We also give a brief introduction to SBSE and metaheuristics. Then, we survey the current research work centered around the combination of search-based techniques and MDE. The literature survey is accompanied by the presentation of references for further details.},
journal = {Automated Software Engg.},
month = jun,
pages = {233–294},
numpages = {62},
keywords = {Search-based software engineering (SBSE), Model-driven engineering (MDE), Metaheuristics, Metaheuristic}
}

@inproceedings{10.1145/3079856.3080244,
author = {Venkataramani, Swagath and Ranjan, Ashish and Banerjee, Subarno and Das, Dipankar and Avancha, Sasikanth and Jagannathan, Ashok and Durg, Ajaya and Nagaraj, Dheemanth and Kaul, Bharat and Dubey, Pradeep and Raghunathan, Anand},
title = {ScaleDeep: A Scalable Compute Architecture for Learning and Evaluating Deep Networks},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080244},
doi = {10.1145/3079856.3080244},
abstract = {Deep Neural Networks (DNNs) have demonstrated state-of-the-art performance on a broad range of tasks involving natural language, speech, image, and video processing, and are deployed in many real world applications. However, DNNs impose significant computational challenges owing to the complexity of the networks and the amount of data they process, both of which are projected to grow in the future. To improve the efficiency of DNNs, we propose ScaleDeep, a dense, scalable server architecture, whose processing, memory and interconnect subsystems are specialized to leverage the compute and communication characteristics of DNNs. While several DNN accelerator designs have been proposed in recent years, the key difference is that ScaleDeep primarily targets DNN training, as opposed to only inference or evaluation. The key architectural features from which ScaleDeep derives its efficiency are: (i) heterogeneous processing tiles and chips to match the wide diversity in computational characteristics (FLOPs and Bytes/FLOP ratio) that manifest at different levels of granularity in DNNs, (ii) a memory hierarchy and 3-tiered interconnect topology that is suited to the memory access and communication patterns in DNNs, (iii) a low-overhead synchronization mechanism based on hardware data-flow trackers, and (iv) methods to map DNNs to the proposed architecture that minimize data movement and improve core utilization through nested pipelining. We have developed a compiler to allow any DNN topology to be programmed onto ScaleDeep, and a detailed architectural simulator to estimate performance and energy. The simulator incorporates timing and power models of ScaleDeep's components based on synthesis to Intel's 14nm technology. We evaluate an embodiment of ScaleDeep with 7032 processing tiles that operates at 600 MHz and has a peak performance of 680 TFLOPs (single precision) and 1.35 PFLOPs (half-precision) at 1.4KW. Across 11 state-of-the-art DNNs containing 0.65M-14.9M neurons and 6.8M-145.9M weights, including winners from 5 years of the ImageNet competition, ScaleDeep demonstrates 6x-28x speedup at iso-power over the state-of-the-art performance on GPUs.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {13–26},
numpages = {14},
keywords = {Deep Neural Networks, Hardware Accelerators, System Architecture},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1007/s10515-015-0188-0,
author = {Bulej, Lubom\'{\i}r and Bure\v{s}, Tom\'{a}\v{s} and Hork\'{y}, Vojtundefinedch and Kotr\u{a}\'{z}, Jaroslav and Marek, Luk\'{a}\v{s} and Troj\'{a}nek, Tom\'{a}\v{s} and T\'{z}Ma, Petr},
title = {Unit testing performance with Stochastic Performance Logic},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-015-0188-0},
doi = {10.1007/s10515-015-0188-0},
abstract = {Unit testing is an attractive quality management tool in the software development process, however, practical obstacles make it difficult to use unit tests for performance testing. We present Stochastic Performance Logic, a formalism for expressing performance requirements, together with interpretations that facilitate performance evaluation in the unit test context. The formalism and the interpretations are implemented in a performance testing framework and evaluated in multiple experiments, demonstrating the ability to identify performance differences in realistic unit test scenarios.},
journal = {Automated Software Engg.},
month = mar,
pages = {139–187},
numpages = {49},
keywords = {Unit testing, Performance evaluation, Java}
}

@inproceedings{10.5555/2874916.2874975,
author = {Mittal, Saurabh and Ruth, Mark and Pratt, Annabelle and Lunacek, Monte and Krishnamurthy, Dheepak and Jones, Wesley},
title = {A system-of-systems approach for integrated energy systems modeling and simulation},
year = {2015},
isbn = {9781510810594},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Energy systems integration combines energy carriers, including electricity, with infrastructures, to maximize efficiency and minimize waste. In order to study systems at a variety of physical scales---from individual buildings to distribution systems---interconnected through these energy infrastructures, NREL is developing an Integrated Energy System Model (IESM), with an initial focus on the electricity system. Today's electricity grid is the most complex system ever built---and the future grid is likely to be even more complex because it will incorporate distributed energy resources (DERs) such as wind, solar, and various other sources of generation and energy storage. The complexity is further augmented by the possible evolution to new retail market structures that would provide incentives to owners of DERs to support the grid. The IESM can be used to understand and test the impact of new retail market structures and technologies such as DERs, demand-response equipment, and energy management systems on the system's ability to provide reliable electricity to all customers. The IESM is composed of a power flow simulator (GridLAB-D), building and appliance models including home energy management system implemented using either GAMS or Pyomo, a market layer, and is able to include hardware-in-the-loop simulation (testing appliances such as air conditioners, dishwashers, etc.). The IESM is a system-of-systems (SoS) simulator wherein the constituent systems are brought together in a virtual testbed. We will describe an SoS approach for developing a distributed simulation environment. We will elaborate on the methodology and the control mechanisms used in the co-simulation illustrated by a case study.},
booktitle = {Proceedings of the Conference on Summer Computer Simulation},
pages = {1–10},
numpages = {10},
keywords = {system-of-systems, smart grid, optimization, integrated energy systems, discrete-event simulation, co-simulation, GridLAB-D, DEVS},
location = {Chicago, Illinois},
series = {SummerSim '15}
}

@inproceedings{10.5555/3290281.3290302,
author = {Kalra, Sumit and T, Prabhakar},
title = {Implementation patterns for multi-tenancy},
year = {2017},
isbn = {9781941652060},
publisher = {The Hillside Group},
address = {USA},
abstract = {Recently multi-tenant applications for SaaS in cloud computing are on rise. These applications increase the degree of resource sharing among tenants with various functional and non-functional requirements. However, often it results in higher design complexity. In this work, we discuss various design patterns to build these applications with efficient tenant management. We divided these patterns in the three categories. The categorization is based on their applicability to design, development, and runtime phases during software development life cycle. These patterns make an application tenant aware and enable multi-tenancy without adding much overhead and complexity in its design.},
booktitle = {Proceedings of the 24th Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {16},
keywords = {tenant operation and management, multi-tenant},
location = {Vancouver, British Columbia, Canada},
series = {PLoP '17}
}

@article{10.1007/s00034-018-0859-8,
author = {Xu, Jin and Qiao, Yuansong and Fu, Zhizhong and Wen, Quan},
title = {Image Block Compressive Sensing Reconstruction via Group-Based Sparse Representation and Nonlocal Total Variation},
year = {2019},
issue_date = {January   2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {1},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-018-0859-8},
doi = {10.1007/s00034-018-0859-8},
abstract = {Compressive sensing (CS) has recently drawn considerable attentions in signal and image processing communities as a joint sampling and compression approach. Generally, the image CS reconstruction can be formulated as an optimization problem with a properly chosen regularization function based on image priors. In this paper, we propose an efficient image block compressive sensing (BCS) reconstruction method, which combine the best of group-based sparse representation (GSR) model and nonlocal total variation (NLTV) model to regularize the solution space of the image CS recovery optimization problem. Specifically, the GSR model is utilized to simultaneously enforce the intrinsic local sparsity and the nonlocal self-similarity of natural images, while the NLTV model is explored to characterize the smoothness of natural images on a larger scale than the classical total variation (TV) model. To efficiently solve the proposed joint regularized optimization problem, an algorithm based on the split Bregman iteration is developed. The experimental results demonstrate that the proposed method outperforms current state-of-the-art image BCS reconstruction methods in both objective quality and visual perception.},
journal = {Circuits Syst. Signal Process.},
month = jan,
pages = {304–328},
numpages = {25},
keywords = {Split Bregman iteration, Nonlocal total variation, Joint regularization, Group-based sparse representation, Block compressive sensing}
}

@book{10.5555/2809012,
author = {Leito, Paulo and Karnouskos, Stamatis},
title = {Industrial Agents: Emerging Applications of Software Agents in Industry},
year = {2015},
isbn = {0128003413},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
edition = {1st},
abstract = {Industrial Agents explains how multi-agent systems improve collaborative networks to offer dynamic service changes, customization, improved quality and reliability, and flexible infrastructure. Learn how these platforms can offer distributed intelligent management and control functions with communication, cooperation and synchronization capabilities, and also provide for the behavior specifications of the smart components of the system. The book offers not only an introduction to industrial agents, but also clarifies and positions the vision, on-going efforts, example applications, assessment and roadmap applicable to multiple industries. This edited work is guided and co-authored by leaders of the IEEE Technical Committee on Industrial Agents who represent both academic and industry perspectives and share the latest research along with their hands-on experiences prototyping and deploying industrial agents in industrial scenarios.Learn how new scientific approaches and technologies aggregate resources such next generation intelligent systems, manual workplaces and information and material flow systemGain insight from experts presenting the latest academic and industry research on multi-agent systemsExplore multiple case studies and example applications showing industrial agents in a variety of scenariosUnderstand implementations across the enterprise, from low-level control systems to autonomous and collaborative management units}
}

@article{10.1007/s10270-016-0575-4,
author = {Voelter, Markus and Kolb, Bernd and Szab\'{o}, Tam\'{a}s and Ratiu, Daniel and Deursen, Arie},
title = {Lessons learned from developing mbeddr: a case study in language engineering with MPS},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0575-4},
doi = {10.1007/s10270-016-0575-4},
abstract = {Language workbenches are touted as a promising technology to engineer languages for use in a wide range of domains, from programming to science to business. However, not many real-world case studies exist that evaluate the suitability of language workbench technology for this task. This paper contains such a case study. In particular, we evaluate the development of mbeddr, a collection of integrated languages and language extensions built with the Jetbrains MPS language workbench. mbeddr consists of 81 languages, with their IDE support, 34 of them C extensions. The mbeddr languages use a wide variety of notations--textual, tabular, symbolic and graphical--and the C extensions are modular; new extensions can be added without changing the existing implementation of C. mbeddr's development has spanned 10 person-years so far, and the tool is used in practice and continues to be developed. This makes mbeddr a meaningful case study of non-trivial size and complexity. The evaluation is centered around five research questions: language modularity, notational freedom and projectional editing, mechanisms for managing complexity, performance and scalability issues and the consequences for the development process. We draw generally positive conclusions; language engineering with MPS is ready for real-world use. However, we also identify a number of areas for improvement in the state of the art in language engineering in general, and in MPS in particular.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {585–630},
numpages = {46},
keywords = {Languages, Language workbenches, Language extension, Language engineering, Experimentation, Domain-specific language, Case study}
}

@article{10.1007/s10270-012-0293-5,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel C.},
title = {Does aspect-oriented modeling help improve the readability of UML state machines?},
year = {2014},
issue_date = {July      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0293-5},
doi = {10.1007/s10270-012-0293-5},
abstract = {Aspect-oriented modeling (AOM) is a relatively recent and very active field of research, whose application has, however, been limited in practice. AOM is assumed to yield several potential benefits such as enhanced modularization, easier evolution, increased reusability, and improved readability of models, as well as reduced modeling effort. However, credible, solid empirical evidence of such benefits is lacking. We evaluate the "readability" of state machines when modeling crosscutting behavior using AOM and more specifically AspectSM, a recently published UML profile. This profile extends the UML state machine notation with mechanisms to define aspects using state machines. Readability is indirectly measured through defect identification and fixing rates in state machines, and the scores obtained when answering a comprehension questionnaire about the system behavior. With AspectSM, crosscutting behavior is modeled using so-called "aspect state machines". Their readability is compared with that of system state machines directly modeling crosscutting and standard behavior together. An initial controlled experiment and a much larger replication were conducted with trained graduate students, in two different institutions and countries, to achieve the above objective. We use two baselines of comparisons--standard UML state machines without hierarchical features (flat state machines) and standard state machines with hierarchical/concurrent features (hierarchical state machines). The results showed that defect identification and fixing rates are significantly better with AspectSM than with both flat and hierarchical state machines. However, in terms of comprehension scores and inspection effort, no significant difference was observed between any of the approaches. Results of the experiments suggest that one should use, when possible, aspect state machines along with hierarchical and/or concurrent features of UML state machines to model crosscutting behaviors.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {1189–1221},
numpages = {33},
keywords = {UML state machines, Defect identification and fixing, Controlled experiment, Comprehension, Aspect-oriented modeling}
}

@book{10.5555/2669162,
author = {Felfernig, Alexander and Hotz, Lothar and Bagley, Claire and Tiihonen, Juha},
title = {Knowledge-based Configuration: From Research to Business Cases},
year = {2014},
isbn = {012415817X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1},
abstract = {Knowledge-based Configuration incorporates knowledge representation formalisms to capture complex product models and reasoning methods to provide intelligent interactive behavior with the user. This book represents the first time that corporate and academic worlds collaborate integrating research and commercial benefits of knowledge-based configuration. Foundational interdisciplinary material is provided for composing models from increasingly complex products and services. Case studies, the latest research, and graphical knowledge representations that increase understanding of knowledge-based configuration provide a toolkit to continue to push the boundaries of what configurators can do and how they enable companies and customers to thrive.Includes detailed discussion of state-of-the art configuration knowledge engineering approaches such as automated testing and debugging, redundancy detection, and conflict management Provides an overview of the application of knowledge-based configuration technologies in the form of real-world case studies from SAP, Siemens, Kapsch, and more Explores the commercial benefits of knowledge-based configuration technologies to business sectors from services to industrial equipment Uses concepts that are based on an example personal computer configuration knowledge base that is represented in an UML-based graphical language}
}

@inproceedings{10.1145/1173706.1173740,
author = {Leavens, Gary T. and Abrial, Jean-Raymond and Batory, Don and Butler, Michael and Coglio, Alessandro and Fisler, Kathi and Hehner, Eric and Jones, Cliff and Miller, Dale and Peyton-Jones, Simon and Sitaraman, Murali and Smith, Douglas R. and Stump, Aaron},
title = {Roadmap for enhanced languages and methods to aid verification},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173740},
doi = {10.1145/1173706.1173740},
abstract = {This roadmap describes ways that researchers in four areas---specification languages, program generation, correctness by construction, and programming languages---might help further the goal of verified software. It also describes what advances the "verified software" grand challenge might anticipate or demand from work in these areas. That is, the roadmap is intended to help foster collaboration between the grand challenge and these research areas.A common goal for research in these areas is to establish language designs and tool architectures that would allow multiple annotations and tools to be used on a single program. In the long term, researchers could try to unify these annotations and integrate such tools.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {221–236},
numpages = {16},
keywords = {verified software grand challenge, verification, tools, specification languages, programming languages, program generation, correctness by construction, annotations},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@inproceedings{10.1145/3452021.3458317,
author = {Alur, Rajeev and Hilliard, Phillip and Ives, Zachary G. and Kallas, Konstantinos and Mamouras, Konstantinos and Niksic, Filip and Stanford, Caleb and Tannen, Val and Xue, Anton},
title = {Synchronization Schemas},
year = {2021},
isbn = {9781450383813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452021.3458317},
doi = {10.1145/3452021.3458317},
abstract = {We present a type-theoretic framework for data stream processing for real-time decision making, where the desired computation involves a mix of sequential computation, such as smoothing and detection of peaks and surges, and naturally parallel computation, such as relational operations, key-based partitioning, and map-reduce. Our framework unifies sequential (ordered) and relational (unordered) data models. In particular, we define synchronization schemas as types, and series-parallel streams (SPS) as objects of these types. A synchronization schema imposes a hierarchical structure over relational types that succinctly captures ordering and synchronization requirements among different kinds of data items. Series-parallel streams naturally model objects such as relations, sequences, sequences of relations, sets of streams indexed by key values, time-based and event-based windows, and more complex structures obtained by nesting of these. We introduce series-parallel stream transformers (SPST) as a domain-specific language for modular specification of deterministic transformations over such streams. SPSTs provably specify only monotonic transformations allowing streamability, have a modular structure that can be exploited for correct parallel implementation, and are composable allowing specification of complex queries as a pipeline of transformations.},
booktitle = {Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {1–18},
numpages = {18},
keywords = {synchronization schemas, stream processing, series-parallel partial orders, relations, parallelism, streams, database query languages},
location = {Virtual Event, China},
series = {PODS'21}
}

@article{10.1016/j.jnca.2017.12.001,
author = {Dias de Assuno, Marcos and da Silva Veith, Alexandre and Buyya, Rajkumar},
title = {Distributed data stream processing and edge computing},
year = {2018},
issue_date = {February 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {103},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2017.12.001},
doi = {10.1016/j.jnca.2017.12.001},
abstract = {Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructure, wearable assistance, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and efficient manner. More recently, architecture has been proposed to use edge computing for data stream processing. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to fluctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make efficient resource management decisions based on current load. Elasticity becomes even more challenging in highly distributed environments comprising edge and cloud computing resources. This work examines some of these challenges and discusses solutions proposed in the literature to address them. HighlightsThe paper surveys state of the art on stream processing engines and mechanisms.The work describes how existing solutions exploit resource elasticity features of cloud computing in stream processing.It presents a gap analysis and future directions on stream processing on heterogeneous environments.},
journal = {J. Netw. Comput. Appl.},
month = feb,
pages = {1–17},
numpages = {17},
keywords = {Stream processing, Resource elasticity, Cloud computing, Big Data}
}

@inproceedings{10.5555/787260.787727,
author = {Turgis, S. and Daga, J. M. and Portal, J. M. and Auvergne, D.},
title = {Internal power modelling and minimization in CMOS inverters},
year = {1997},
isbn = {0818677864},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present in this paper an alternative for the internal (short-circuit and overshoot) power dissipation estimation of CMOS structures. Using a first order macro-modelling, we consider submicronic additional effects such as: input slow dependency of short-circuit currents and input-to-output coupling. Considering an equivalent capacitance concept we directly compare the different power components. Validations are presented by comparing simulated values (HSPICE level 6, foundry model 0.7 /spl mu/m) to calculated ones. Application to buffer design enlightens the importance of the internal power component and clearly shows that common sizing alternatives for power and delay minimization can be considered.},
booktitle = {Proceedings of the 1997 European Conference on Design and Test},
pages = {603},
keywords = {short-circuit current, overshoot, minimization, macromodel, internal power modelling, input-to-output coupling, foundry model, equivalent capacitance, buffer design, HSPICE simulation, CMOS logic circuits, CMOS inverter},
series = {EDTC '97}
}

@article{10.1007/s11276-016-1404-y,
author = {Doosti-Aref, Abdollah and Ebrahimzadeh, Ataollah},
title = {Efficient cooperative multicarrier underwater acoustic communication over the Persian Gulf channel},
year = {2018},
issue_date = {May       2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-016-1404-y},
doi = {10.1007/s11276-016-1404-y},
abstract = {Real-time and reliable transmission with high data rate is one of the most crucial design issues in underwater acoustic (UWA) communications. This paper is part of an operational project in the Persian Gulf which is known as a shallow water channel worldwide. Building on the promising combination of cooperative and multicarrier techniques, we first address the design of an efficient UWA communication system over the Persian Gulf channel. We assume sparse and frequency-selective Rician fading, non-white correlated Gaussian ambient noise, and non-uniform Doppler distortion among subcarriers. Based on the extensive field measurements, we adopt a comprehensive channel model including modified sound speed profile, modified absorption coefficient, reflections from the surface and bottom of the sea, and modified ambient noise model. In our work, carrier frequency is efficiently determined based on the system and environmental parameters. In addition, a simple criterion for Doppler scale calculation is proposed. Moreover, based on the approximate auto-correlation function of the ambient noise, whitening filter is utilized at the receiver. In many applications of underwater acoustic sensors network (UW-ASN), nodes may not be big enough to have more than one antenna. Therefore, to achieve spatial diversity, cooperation between nodes can be a proper alternate method. In another part of this research, toward a proper approach that each node operates under full-duplex mode, we address a cooperative virtually-aided transmission scenario which is called cooperative multiple input single output. Simulation results demonstrate that our proposed models can significantly improve the bit error rate performance of UW-ASN in the Persian Gulf channel.},
journal = {Wirel. Netw.},
month = may,
pages = {1265–1278},
numpages = {14},
keywords = {Underwater acoustic channel, Persian Gulf, OFDM, Cooperative multicarrier transmission, CMISO}
}

@article{10.1007/s10515-018-0247-4,
author = {Gonzalez-Fernandez, Yasser and Hamidi, Saeideh and Chen, Stephen and Liaskos, Sotirios},
title = {Efficient elicitation of software configurations using crowd preferences and domain knowledge},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-018-0247-4},
doi = {10.1007/s10515-018-0247-4},
abstract = {As software systems grow in size and complexity, the process of configuring them to meet individual needs becomes more and more challenging. Users, especially those that are new to a system, are faced with an ever increasing number of configuration possibilities, making the task of choosing the right one more and more daunting. However, users are rarely alone in using a software system. Crowds of other users or the designers themselves can provide with examples and rules as to what constitutes a meaningful configuration. We introduce a technique for designing optimal interactive configuration elicitation dialogs, aimed at utilizing crowd and expert information to reduce the amount of manual configuration effort. A repository of existing user configurations supplies us with information about popular ways to complete an existing partial configuration. Designers augment this information with their own constraints. A Markov decision process (MDP) model is then created to encode configuration elicitation dialogs that maximize the automatic configuration decisions based on the crowd and the designers' information. A genetic algorithm is employed to solve the MDP when problem sizes prevent use of common exact techniques. In our evaluation with various configuration models we show that the technique is feasible, saves configuration effort and scales for real problem sizes of a few hundreds of features.},
journal = {Automated Software Engg.},
month = mar,
pages = {87–123},
numpages = {37},
keywords = {Software customization, Software configuration, Markov decision processes, Genetic algorithms}
}

@article{10.1007/s10515-012-0117-4,
author = {N\"{o}hrer, Alexander and Egyed, Alexander},
title = {C2O configurator: a tool for guided decision-making},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-012-0117-4},
doi = {10.1007/s10515-012-0117-4},
abstract = {Decision models are widely used in software engineering to describe and restrict decision-making (e.g., deriving a product from a product-line). Since decisions are typically interdependent, it is often neither obvious which decisions have the most significant impact nor which decisions might ultimately conflict. Unfortunately, the current state-of-the-art provides little support for dealing with such situations. On the one hand, some conflicts can be avoided by providing more freedom in which order decisions are made (i.e., most important decisions first). On the other hand, conflicts are unavoidable at times, and living with conflicts may be preferable over forcing the user to fix them right away--particularly because fixing conflicts becomes easier as more is known about a user's intentions. This paper introduces the C2O (Configurator 2.0) tool for guided decision-making. The tool allows the user to answer questions in an arbitrary order--with and without the presence of inconsistencies. While giving users those freedoms, it still supports and guides them by (i) rearranging the order of questions according to their potential to minimize user input, (ii) providing guidance to avoid follow-on conflicts, and (iii) supporting users in fixing conflicts at a later time.},
journal = {Automated Software Engg.},
month = jun,
pages = {265–296},
numpages = {32}
}

@inbook{10.5555/2340883.2340909,
author = {Minsky, Naftaly H.},
title = {Decentralized governance of distributed systems via interaction control},
year = {2012},
isbn = {9783642294136},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper introduces an abstract reference model, called &lt;em&gt;interaction control&lt;/em&gt; (IC), for the governance of large and heterogeneous distributed systems. This model goes well beyond conventional access control, along a number of dimensions. In particular, the IC model has the following characteristics: (1) it is inherently decentralized, and thus scalable even for a wide range of stateful policies; (2) it is very general, and not biased toward any particular type of policies; thus providing a significant realization of the age-old principle of &lt;em&gt;separation of policy from mechanism&lt;/em&gt; ; and (3) it enables flexible, composition-free, interoperability between different policies.The IC model, which is an abstraction of a mechanism called law-governed interaction (LGI), has been designed as a minimalist reference model that can be reified into a whole family of potential control mechanisms that may support different types of communication, with different performance requirements and for different application domains.},
booktitle = {Logic Programs, Norms and Action: Essays in Honor of Marek J. Sergot on the Occasion of His 60th Birthday},
pages = {374–400},
numpages = {27}
}

@article{10.1016/j.jss.2013.02.061,
author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and Mcminn, Phil},
title = {An orchestrated survey of methodologies for automated software test case generation},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.02.061},
doi = {10.1016/j.jss.2013.02.061},
abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1978–2001},
numpages = {24},
keywords = {Test case generation, Test automation, Symbolic execution, Software testing, Search-based software testing, Orchestrated survey, Model-based testing, Combinatorial testing, Adaptive random testing}
}

@article{10.1007/s00466-015-1257-8,
author = {Hospital-Bravo, Ra\'{u}l and Sarrate, Josep and D\'{\i}ez, Pedro},
title = {Numerical modeling of undersea acoustics using a partition of unity method with plane waves enrichment},
year = {2016},
issue_date = {May       2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {57},
number = {5},
issn = {0178-7675},
url = {https://doi.org/10.1007/s00466-015-1257-8},
doi = {10.1007/s00466-015-1257-8},
abstract = {A new 2D numerical model to predict the underwater acoustic propagation is obtained by exploring the potential of the Partition of Unity Method (PUM) enriched with plane waves. The aim of the work is to obtain sound pressure level distributions when multiple operational noise sources are present, in order to assess the acoustic impact over the marine fauna. The model takes advantage of the suitability of the PUM for solving the Helmholtz equation, especially for the practical case of large domains and medium frequencies. The seawater acoustic absorption and the acoustic reflectance of the sea surface and sea bottom are explicitly considered, and perfectly matched layers (PML) are placed at the lateral artificial boundaries to avoid spurious reflexions. The model includes semi-analytical integration rules which are adapted to highly oscillatory integrands with the aim of reducing the computational cost of the integration step. In addition, we develop a novel strategy to mitigate the ill-conditioning of the elemental and global system matrices. Specifically, we compute a low-rank approximation of the local space of solutions, which in turn reduces the number of degrees of freedom, the CPU time and the memory footprint. Numerical examples are presented to illustrate the capabilities of the model and to assess its accuracy.},
journal = {Comput. Mech.},
month = may,
pages = {717–732},
numpages = {16},
keywords = {Underwater acoustic propagation, Singular value decomposition, Plane waves, Partition of unity method, Low-rank approximation, Helmholtz equation, Complex wavenumber}
}

@article{10.1016/j.jss.2016.09.027,
author = {Hamid, Brahim and Perez, Jon},
title = {Supporting pattern-based dependability engineering via model-driven development},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.027},
doi = {10.1016/j.jss.2016.09.027},
abstract = {A pattern-based approach as a new method for system dependability engineering based on the reuse of patterns.The design a set of DSMLs to specify the dependability patterns.The development of a set of tools to support the proposed approach.The application of the approach in the context of railway systems.These results suggest our work has wider applicability and usefulness. Safety-critical systems require a high level of safety and integrity. Therefore, generating such systems involves specific software building processes. Many domains are not traditionally involved in these types of software problems and must adapt their current processes accordingly. Typically, such requirements are developed ad hoc for each system, preventing further reuse beyond the domain-specific boundaries. This paper proposes a solution for software system development based on the reuse of dedicated subsystems, i.e., so-called dependability patterns that have been pre-engineered to adapt to a specific domain. We use Model-Driven Engineering (MDE) to describe dependability patterns and a methodology for developing dependable software systems using these patterns. Moreover, we describe an operational architecture for development tools to support the approach. An empirical evaluation of the proposed approach is presented through its practical application to a case study in the railway domain, which has strong dependability requirements, to support a pattern-based development approach. This case study is followed by a survey to better understand the perceptions of practitioners regarding our approach.},
journal = {J. Syst. Softw.},
month = dec,
pages = {239–273},
numpages = {35},
keywords = {System engineering, Safety, Patterns, Model driven engineering, Meta-modeling, Dependability}
}

@article{10.1007/s00466-014-1080-7,
author = {Schneider, Daniel and Schmid, Stefan and Selzer, Michael and B\"{o}hlke, Thomas and Nestler, Britta},
title = {Small strain elasto-plastic multiphase-field model},
year = {2015},
issue_date = {January   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {55},
number = {1},
issn = {0178-7675},
url = {https://doi.org/10.1007/s00466-014-1080-7},
doi = {10.1007/s00466-014-1080-7},
abstract = {A small strain plasticity model, based on the principles of continuum mechanics, is incorporated into a phase-field model for heterogeneous microstructures in polycrystalline and multiphase material systems (Nestler et al., Phys Rev 71:1---6, 2005). Thereby, the displacement field is computed by solving the local momentum balance dynamically (Spatschek et al., Phys Rev 75:1---14, 2007) using the finite difference method on a staggered grid. The elastic contribution is expressed as the linear approximation according to the Cauchy stress tensor. In order to calculate the plastic strain, the Prandtl---Reuss model is implemented consisting of an associated flow rule in combination with the von Mises yield criterion and a linear isotropic hardening approximation. Simulations are performed illustrating the evolution of the stress and plastic strain using a radial return mapping algorithm for single phase system and two phase microstructures. As an example for interface evolution coupling with elasto-plastic effects, we present crack propagation simulations in ductile material.},
journal = {Comput. Mech.},
month = jan,
pages = {27–35},
numpages = {9},
keywords = {Phase-field, Multiphase-field, Elasto-plasticity, Crack propagation}
}

@article{10.4018/jkm.2010040103,
author = {Kamthan, Pankaj},
title = {A Viewpoint-Based Approach for Understanding the Morphogenesis of Patterns},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {2},
issn = {1548-0666},
url = {https://doi.org/10.4018/jkm.2010040103},
doi = {10.4018/jkm.2010040103},
abstract = {An understanding of knowledge artifacts such as patterns is a necessary prerequisite for any subsequent action. In this article, as an initial step for formulating a theoretical basis for patterns, a conceptual model of primitive viewpoints is proposed and, by exploring one of the viewpoints, a conceptual model for stakeholders of a pattern is presented. This is followed by the description of a conceptual model of a process, namely P3, for the production of patterns. The workflows of P3 highlight, as appropriate, the interface of patterns to humans and/or machines. The implications of the Semantic Web and the Social Web towards P3 are briefly discussed.},
journal = {Int. J. Knowl. Manag.},
month = apr,
pages = {40–65},
numpages = {26},
keywords = {Web, Process, Knowledge Representation, Experiential Knowledge, Conceptual Reuse, Conceptual Model}
}

@inproceedings{10.5555/1927661.1927724,
author = {Si, Xiaojie and Zhang, Xuyun and Dou, Wanchun},
title = {A novel local optimization method for QoS-aware web service composition},
year = {2010},
isbn = {3642165141},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {QoS-aware web service selection has become a hot-spot research topic in the domain of web service composition. In previous works, the multiple tasks recruited in a composite schema are usually considered of equal importance. However, it is unreasonable for each task to have the absolutely same weight in certain circumstances. Hence, it is a great challenge to mine the weights among different tasks to reflect customers' partial preferences. In view of this challenge, a novel local optimization method is presented in this paper, which is based on a two-hierarchy weight, i.e., weight of task's criteria and weight of tasks. Finally, a case study is demonstrated to validate the feasibility of our proposal.},
booktitle = {Proceedings of the 2010 International Conference on Web Information Systems and Mining},
pages = {402–409},
numpages = {8},
keywords = {weight, service composition, local optimization, QoS},
location = {Sanya, China},
series = {WISM'10}
}

@article{10.1023/A:1016589208824,
author = {Atkinson, Colin and Bunse, Christian and Gro\ss{}, Hans-Gerhard and K\"{u}hne, Thomas},
title = {Towards a General Component Model for Web-Based Applications},
year = {2002},
issue_date = {June 2002},
publisher = {J. C. Baltzer AG, Science Publishers},
address = {USA},
volume = {13},
number = {1–4},
issn = {1022-7091},
url = {https://doi.org/10.1023/A:1016589208824},
doi = {10.1023/A:1016589208824},
abstract = {The cost effective development of web applications is perhaps one of the most challenging areas of software engineering today. Not only are the problems to be solved, and the solution technologies to be used, in web application development among the most rapidly changing in the software industry, but the business pressures of cost, quality and time-to-market are among the most extreme. Web application development therefore has potentially the most to gain from software reuse approaches that can offer a greater return on development time than traditional approaches. However, simply combining ideas from these reuse paradigms and traditional web development technologies in ad-hoc ways will not result in sustainable improvements. In this paper we describe a systematic way of combining the benefits of component-based development and model driven architectures, two important reuse approaches, to support the cost effective development and maintenance of web applications. After first defining a suitably abstract component-model, the paper explains how component architectures can be systematically and rigorously modeled using UML. It then describes a powerful technique, known as stratification, for separating the various cross cutting aspects of a web application such that a suitable platform specific architecture can be traceably generated. Finally, the paper introduces a technique for increasing the trustworthiness of components by giving them the capability to check their deployment environment at run-time.},
journal = {Ann. Softw. Eng.},
month = jun,
pages = {35–69},
numpages = {35}
}

@inproceedings{10.1145/170657.170749,
author = {Payton, Teri},
title = {STARS (panel): impact of megaprogramming on systems engineering, process and future competitiveness},
year = {1993},
isbn = {0897916212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170657.170749},
doi = {10.1145/170657.170749},
booktitle = {Proceedings of the Conference on TRI-Ada '93},
pages = {334–337},
numpages = {4},
location = {Seattle, Washington, USA},
series = {TRI-Ada '93}
}

@article{10.1504/IJCSE.2015.067054,
author = {Modica, Giuseppe Di and Tomarchio, Orazio},
title = {A semantic framework to support resource discovery in future cloud markets},
year = {2015},
issue_date = {January 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1/2},
issn = {1742-7185},
url = {https://doi.org/10.1504/IJCSE.2015.067054},
doi = {10.1504/IJCSE.2015.067054},
abstract = {The market of cloud resources is currently dominated by proprietary solutions for what concerns resource delivering, pricing models and service level agreements. In the future cloud markets, when cloud standards will get mature and full interoperability among cloud systems will be a reality, the competition challenge among providers will be played on the capability of supplying high and differentiated QoS levels. In this new scenario advanced and flexible mechanisms to support the matchmaking between what providers offer and what customers demand must be devised. Along with an analysis of the current cloud offering in terms of pricing model, SLA negotiation capabilities, service performance levels and cloud application requirements, this work proposes the definition of a semantic model to support the supply-demand matchmaking process in future cloud markets. Leveraging on a semantic description of the cloud resources' features, customers will be able to discover cloud offers that best suit their own business needs. Tests conducted on an implementation prototype proved the viability of the approach.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {1–14},
numpages = {14}
}

@article{10.1016/j.mejo.2013.03.016,
author = {Russo, Patrice and Yengui, Firas and Pillonnet, Gael and Taupin, Sophie and Abouchi, Nacer},
title = {Dynamic voltage scaling for series hybrid amplifiers},
year = {2013},
issue_date = {September, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {44},
number = {9},
issn = {0026-2692},
url = {https://doi.org/10.1016/j.mejo.2013.03.016},
doi = {10.1016/j.mejo.2013.03.016},
abstract = {We present an optimization of the voltage scaling algorithm in low power audio class-G amplifier for headphones application to allow longer playback time. The optimization approach minimizes the voltage difference between the internal audio amplifier power supply and its output signal over a large range of operating conditions. The modeling is based on a behavioral model enabling accurate and rapid evaluation of efficiency and audio quality with realistic input stimuli. The model validated in practice is used to optimize the voltage scaling using only few power supply levels. Thanks to a global search algorithm followed by a local one, the optimization gives the better parameters for voltage scaling algorithm while keeping a good audio quality. The proposed configuration increases the efficiency up to 48% at nominal operation.},
journal = {Microelectron. J.},
month = sep,
pages = {753–763},
numpages = {11},
keywords = {Optimization, Hybrid amplifier, Dynamic voltage scaling, Class-G/H, Audio amplifier}
}

@inproceedings{10.1145/2345396.2345586,
author = {Natarajan, Jaya Sudha and Sevukamoorthy, Lakshmi},
title = {Auto-clever fuzzy (ACF) based intelligent system for monitoring and controlling the hydrocarbons -air toxics emitted by the vehicle motors},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345586},
doi = {10.1145/2345396.2345586},
abstract = {Numerous methods have been designed for choosing the operating point of an IC Engine in a parallel hybrid electric vehicle. This paper is based on choosing a service point about an optimal operating point, based on the road load, the Battery State of Charge (SOC), and the optimal operating point of an IC Engine. An optimal service point (ideal case) is calculated based on the minimization of a criterion, of which fuel and emissions are challenging parameters. Based on a set of weights, the relative importance of fuel economy and emissions is dynamically chosen, and an optimal torque that can be requested from the IC Engine is calculated. Based on the battery SOC constraints and the road load (driver's request), the actual output torque of the IC Engine is computed. The remaining torque (at that speed) required to meet the road load is provided by a buffer to the Electric Motor (EM). The buffer may produce either positive or negative torque. This process would highly be effective for saving the environment from pollutions caused by motor vehicles.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {1187–1192},
numpages = {6},
keywords = {machine intelligence, fuzzy c- means, SOC},
location = {Chennai, India},
series = {ICACCI '12}
}

@inproceedings{10.1145/2095536.2095554,
author = {Siala, Fatma and Lajmi, Soufiene and Ghedira, Khaled},
title = {Multi-agent selection of multiple composite web services based on CBR method and driven by QoS},
year = {2011},
isbn = {9781450307840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095536.2095554},
doi = {10.1145/2095536.2095554},
abstract = {Many companies aim to use Web services to integrate heterogeneous or remote applications in SOA (Service Oriented Architecture) contexts. Indeed, one of the main assets of service-orientation is a composition to develop higher level services, so-called composite services, by re-using existing services. Since many available Web services provide overlapping or identical functionality, with different Quality of Service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. However, for a composition, we can have different combinations and execution paths. Particularly, a composite service can generate different schemes that give various QoS scores.This paper presents a framework which deals with the selection of composite Web services on the base of Multi-Agents negotiation. The objective of these agents is to find out the best Composite QoS (CQoS) based on Web services availability. This scalable framework supports different combinations and execution paths using CBR technique. The proposed Multi-Agents framework is compared to an existing approach in terms of execution time. Experiments have demonstrated that our framework provide reliable results in comparison with the existing approach.},
booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-Based Applications and Services},
pages = {90–97},
numpages = {8},
keywords = {web service, multi-agent system, execution paths, contract-net protocol, composition, QoS, CBR technique},
location = {Ho Chi Minh City, Vietnam},
series = {iiWAS '11}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inbook{10.5555/1793854.1793863,
author = {Liu, Chunjian Robin and Gibbs, Celina and Coady, Yvonne},
title = {Safe and sound evolution with SONAR: sustainable optimization and navigation with aspects for system-wide reconciliation},
year = {2007},
isbn = {3540770410},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Traditional diagnostic and optimization techniques typically rely on static instrumentation of a small portion of an overall system. Unfortunately, solely static and localized approaches are simply no longer sustainable in the evolution of today's complex and dynamic systems. Sustainable Optimization and Navigation with Aspects for system-wide Reconciliation is a fluid and unified framework that enables stakeholders to explore and adapt meaningful entities that are otherwise spread across predefined abstraction boundaries. Through a combination of Aspect-Oriented Programming, Extensible Markup Language, and management tools such as Java Management Extensions, SONAR can comprehensively coalesce scattered artifacts--enabling evolution to be more inclusive of system-wide considerations by supporting both iterative and interactive practices. We believe this system-wide approach promotes the application of safe and sound principles in system evolution. This paper presents SONAR's model, examples of its concrete manifestation, and an overview of its associated costs and benefits. Case studies demonstrate how SONAR can be used to accurately identify performance bottlenecks and effectively evolve systems by optimizing behaviour, even at runtime.},
booktitle = {Transactions on Aspect-Oriented Software Development IV},
pages = {163–190},
numpages = {28}
}

@inproceedings{10.5555/1765571.1765583,
author = {Liu, Shih-Hsi and Bryant, Barrett R. and Auguston, Mikhail and Gray, Jeff and Raje, Rajeev and Tuceryan, Mihran},
title = {A component-based approach for constructing high-confidence distributed real-time and embedded systems},
year = {2005},
isbn = {9783540711551},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In applying Component-Based Software Engineering (CBSE) techniques to the domain of Distributed Real-time and Embedded (DRE) Systems, there are five critical challenges: 1) discovery of relevant components and resources, 2) specification and modeling of components, 3) exploration and elimination of design assembly options, 4) automated generation of heterogeneous component bridges, and 5) validation of context-related embedded systems. To address these challenges, this paper introduces four core techniques to facilitate high-confidence DRE system construction from components: 1) A component and resource discovery technique promotes component searching based on rich and precise descriptions of components and context; 2) A timed colored Petri Net-based modeling toolkit enables design and analysis on DRE systems, as well as reduces unnecessary later work by eliminating infeasible design options; 3) A formal specification language describes all specifications consistently and automatically generates component bridges for seamless system integration; and 4) A grammar-based formalism specifies context behaviors and validates integrated systems using sufficient context-related test cases. The success of these ongoing techniques may not only accelerate the software development pace and reduce unnecessary development cost, but also facilitate high-confidence DRE system construction using different formalisms over the entire software life-cycle.},
booktitle = {Proceedings of the 12th Monterey Conference on Reliable Systems on Unreliable Networked Platforms},
pages = {225–247},
numpages = {23},
location = {Laguna Beach, CA, 2005}
}

@inproceedings{10.1145/2491956.2462163,
author = {Nowatzki, Tony and Sartin-Tarm, Michael and De Carli, Lorenzo and Sankaralingam, Karthikeyan and Estan, Cristian and Robatmili, Behnam},
title = {A general constraint-centric scheduling framework for spatial architectures},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462163},
doi = {10.1145/2491956.2462163},
abstract = {Specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecture-specific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures.Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {495–506},
numpages = {12},
keywords = {spatial architectures, spatial architecture scheduling, integer linear programming},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@article{10.1016/j.jss.2017.01.026,
author = {Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Galster, Matthias and Avgeriou, Paris},
title = {A mapping study on design-time quality attributes and metrics},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.026},
doi = {10.1016/j.jss.2017.01.026},
abstract = {Support to the quality attribute (QA) &amp; metric selection process.Maintainability is the most studied QA for most domains and development phases.Quality attributes are usually assessed through a correlation to a single metric.Metrics are validated in empirical settings and may lack theoretical validity. Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.},
journal = {J. Syst. Softw.},
month = may,
pages = {52–77},
numpages = {26},
keywords = {Software quality, Measurement, Mapping study, Design-time quality attributes}
}

@article{10.1007/s10270-010-0147-y,
author = {Choi, Yunja and Bunse, Christian},
title = {Design verification in model-based μ-controller development using an abstract component},
year = {2011},
issue_date = {February  2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-010-0147-y},
doi = {10.1007/s10270-010-0147-y},
abstract = {Component-based software development is a promising approach for controlling the complexity and quality of software systems. Nevertheless, recent advances in quality control techniques do not seem to keep up with the growing complexity of embedded software; embedded systems often consist of dozens to hundreds of software/hardware components that exhibit complex interaction behavior. Unanticipated quality defects in a component can be a major source of system failure. To address this issue, this paper suggests a design verification approach integrated into the model-driven, component-based development methodology Marmot. The notion of abstract components--the basic building blocks of Marmot--helps to lift the level of abstraction, facilitates high-level reuse, and reduces verification complexity by localizing verification problems between abstract components before refinement and after refinement. This enables the identification of unanticipated design errors in the early stages of development. This work introduces the Marmot methodology, presents a design verification approach in Marmot, and demonstrates its application on the development of a μ-controller-based abstraction of a car mirror control system. An application on TinyOS shows that the approach helps to reuse models as well as their verification results in the development process.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {91–115},
numpages = {25},
keywords = {Model-driven development, Embedded systems, Design verification, Abstract component}
}

@inproceedings{10.1145/1950413.1950448,
author = {Kenter, Tobias and Plessl, Christian and Platzner, Marco and Kauschke, Michael},
title = {Performance estimation framework for automated exploration of CPU-accelerator architectures},
year = {2011},
isbn = {9781450305549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950413.1950448},
doi = {10.1145/1950413.1950448},
abstract = {In this paper we present a fast and fully automated approach for studying the design space when interfacing reconfigurable accelerators with a CPU. Our challenge is, that a reasonable evaluation of architecture parameters requires a hardware/software partitioning that makes best use of each given architecture configuration. Therefore we developed a framework based on the LLVM infrastructure that performs this partitioning with high-level estimation of the runtime on the target architecture utilizing profiling information and code analysis. By making use of program characteristics also during the partitioning process, we improve previous results for various benchmarks and especially for growing interface latencies between CPU and accelerator.},
booktitle = {Proceedings of the 19th ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {177–180},
numpages = {4},
keywords = {performance estimation, llvm, hardware/software partitioning, design space exploration},
location = {Monterey, CA, USA},
series = {FPGA '11}
}

@article{10.1504/IJWMC.2014.063054,
author = {Siala, Fatma and Ghedira, Khaled},
title = {How to select dynamically a QoS-driven composite web service by a multi-agent system using CBR method},
year = {2014},
issue_date = {July 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {4},
issn = {1741-1084},
url = {https://doi.org/10.1504/IJWMC.2014.063054},
doi = {10.1504/IJWMC.2014.063054},
abstract = {Service-oriented architecture permits the composition of web services provided with different Quality of Service QoS levels. In a given composition, finding the set of services that optimises some QoS attributes under its constraints is a problem that needs to be solved. Our aim is to propose an intelligent approach to the selection of a Composite Web Service CWS based on QoS. This paper reports the authors' recent research on addressing the issue. An overview on the previously proposed approaches is presented. These approaches correspond to several improvements of an existing multi-agent one, which is well-cited in the specialised literature. Each framework, implemented on JADE Java Agent Development framework, improves another one in terms of CPU time and/or QoS score, to reach a new agent-based and scalable framework. The last framework utilises the agents' ability of negotiation, interaction and cooperation in order to facilitate the selection of composite web services. By using CBR method, the agents can memorise QoS scores and availability. The improvements are related not only to the CPU time but also to the Composite QoS CQoS value, while operating in a dynamic environment and taking into account user preferences.},
journal = {Int. J. Wire. Mob. Comput.},
month = jul,
pages = {327–347},
numpages = {21}
}

@article{10.1145/2658993,
author = {Nowatzki, Tony and Sartin-Tarm, Michael and De Carli, Lorenzo and Sankaralingam, Karthikeyan and Estan, Cristian and Robatmili, Behnam},
title = {A Scheduling Framework for Spatial Architectures Across Multiple Constraint-Solving Theories},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/2658993},
doi = {10.1145/2658993},
abstract = {Spatial architectures provide energy-efficient computation but require effective scheduling algorithms. Existing heuristic-based approaches offer low compiler/architect productivity, little optimality insight, and low architectural portability.We seek to develop a spatial-scheduling framework by utilizing constraint-solving theories and find that architecture primitives and scheduler responsibilities can be related through five abstractions: computation placement, data routing, event timing, resource utilization, and the optimization objective. We encode these responsibilities as 20 mathematical constraints, using SMT and ILP, and create schedulers for the TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using constraint solving is implementable, is practical, and can outperform specialized schedulers.},
journal = {ACM Trans. Program. Lang. Syst.},
month = nov,
articleno = {2},
numpages = {30},
keywords = {Satisfiability Modulo Theories, Spatial architectures, integer linear programming, spatial architecture scheduling}
}

@inproceedings{10.5555/2429759.2430133,
author = {Rabe, Markus and Horvath, Adrienn and Spieckermann, Sven and Fechteler, Till},
title = {An approach for increasing flexibility in green supply chains driven by simulation},
year = {2012},
publisher = {Winter Simulation Conference},
abstract = {Flexibility is a relevant topic in the field of green supply chains (GSC), as disturbances lead to additional transport and storage, frequently aggravated by energy-consuming air conditioning requirements. This paper discusses how simulation can support to establish flexible GSCs with specific focus on decreasing CO2 and energy consumption. For this purpose, the term flexibility is structured into categories, and methodological approaches driven by simulation in supply chains are studied. Flexibility requirements in the context of a GSC are analyzed and potential support derived for increasing this flexibility, gained by a join of simulation techniques, data models and morphological characteristics of flexibility. An approach for systematic flexibility analysis is presented on the grounds of a data mart that represents both internal and external factors influencing GSC scenarios.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {277},
numpages = {12},
location = {Berlin, Germany},
series = {WSC '12}
}

@article{10.1007/s00779-010-0356-y,
author = {L\'{e}zoray, Jean-Baptiste and Segarra, Maria-Teresa and Phung-Khac, An and Th\'{e}paut, Andr\'{e} and Gilliot, Jean-Marie and Beugnard, Antoine},
title = {A design process enabling adaptation in pervasive heterogeneous contexts},
year = {2011},
issue_date = {April     2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-010-0356-y},
doi = {10.1007/s00779-010-0356-y},
abstract = {In the next decades, the growth in population aging will cause important problems to most industrialized countries. To tackle this issue, Ambient Assistive Living (AAL) systems can reinforce the well-being of elderly people, by providing emergency, autonomy enhancement, and comfort services. These services will postpone the need of a medicalized environment and will allow the elderly to stay longer at home. However, each elderly has specific needs and a deployment environment of such services is likely unique. Furthermore, the needs evolve over time, and so does the deployment environment of the system. In this paper, we propose the use of a model-based development method, the adaptive medium approach, to enable dynamic adaptation of AAL systems. We also propose improvements to make it more suited to the AAL domain, such as considering heterogeneity and a composition model. The paper includes an evaluation of the prototype implementing the approach, and a comparison with related work.},
journal = {Personal Ubiquitous Comput.},
month = apr,
pages = {353–363},
numpages = {11},
keywords = {Model-driven engineering, Heterogeneity, Dynamic adaptation, Adaptive medium approach, AAL}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.5555/1983222.1983245,
author = {Israsena, Pasin and Lertamonsin, Sumate},
title = {Future directions in hearing aid testing and evaluation},
year = {2008},
publisher = {Singapore Therapeutic, Assistive &amp; Rehabilitative Technologies (START) Centre},
address = {Midview City, SGP},
abstract = {This paper reviews present industrial-standard practice in hearing aid electroacoustic measurements, especially for quality assurance purposes, and discusses the future directions in advanced hearing aid measurements. Modern hearing aids possess non-linear digital signal processing (DSP) functions such as noise reduction or acoustic feedback cancellation, for which the present standard developed for the earlier analog-based generation may not be entirely appropriated. It is envisaged that the updated procedure could include features such as qualitative measurements on directionality, SNR and noise suppression.},
booktitle = {Proceedings of the 2nd International Convention on Rehabilitation Engineering &amp; Assistive Technology},
pages = {78–82},
numpages = {5},
keywords = {testing, hearing aid, assistive technology, DSP},
location = {Bangkok, Thailand},
series = {iCREATe '08}
}

@inproceedings{10.5555/2675983.2676315,
author = {Gutenschwager, Kai and Rabe, Markus and Sari, Mehmet Umut and Fechteler, Till},
title = {A data model for carbon footprint simulation in consumer goods supply chains},
year = {2013},
isbn = {9781479920778},
publisher = {IEEE Press},
abstract = {CO2 efficiency is currently a popular topic in supply chain management. Most approaches are based on the Life Cycle Assessment (LCA) which usually exploits data from a static database. This approach is effective when estimating the carbon footprint of products or groups of products in general. Simulation has been a proper method for metering the effectiveness of logistics systems, and could thus be expected to also support the analysis of CO2 efficiency in supply chains (SC) when combined with an LCA database. However, research shows that this combination does not deliver reliable results when the target of the study is improvement of the logistics in the SC. The paper demonstrates the shortcomings of the LCA-analogous approach and proposes a data model that enables discrete event simulation of SC logistics including its impact on the carbon footprint that is under development in the e-SAVE joint project funded by the European Commission.},
booktitle = {Proceedings of the 2013 Winter Simulation Conference: Simulation: Making Decisions in a Complex World},
pages = {2677–2688},
numpages = {12},
location = {Washington, D.C.},
series = {WSC '13}
}

@article{10.1145/209891.209901,
author = {Mohan, C. and Agrawal, D. and Alonso, G. and El Abbadi, A. and Guenthoer, R. and Kamath, M.},
title = {Exotica: a project on advanced transaction management and workflow systems},
year = {1995},
issue_date = {Aug. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {0894-0819},
url = {https://doi.org/10.1145/209891.209901},
doi = {10.1145/209891.209901},
abstract = {This paper is an overview of the Exotica project, currently in progress at the IBM Almaden Research Center. The project aims at exploring several research areas from advanced transaction management concepts to client/server architectures and mobile computing within the context of business processes and workflow management. The ultimate goal is to incorporate these ideas into IBM's products and prototypes. The project involves IBM groups in Almaden (U.S.A.), Hursley (U.K.), Boeblingen (Germany), and Vienna (Austria). In this paper we briefly describe two IBM products, FlowMark, a workflow management system, and MQSeries, a messaging system, as the environments in which we are focusing our research. We also discuss some of our results in the areas of availability, replication, distribution, and advanced transaction models, as well as describe our future research directions. 10},
journal = {SIGOIS Bull.},
month = aug,
pages = {45–50},
numpages = {6}
}

@article{10.1016/j.jpdc.2019.09.005,
author = {Amah, Tekenate E. and Kamat, Maznah and Abu Bakar, Kamalrulnizam and Moreira, Waldir and Oliveira, Antonio and Batista, Marcos A.},
title = {Preparing opportunistic networks for smart cities: Collecting sensed data with minimal knowledge},
year = {2020},
issue_date = {Jan 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.09.005},
doi = {10.1016/j.jpdc.2019.09.005},
journal = {J. Parallel Distrib. Comput.},
month = jan,
pages = {21–55},
numpages = {35},
keywords = {Smart City, Wireless sensors, Sensed Data Collection, Contact information overhead, Opportunistic networks}
}

@article{10.1016/j.cor.2012.06.014,
author = {Garroppo, Rosario G. and Giordano, Stefano and Nencioni, Gianfranco and Scutell\`{a}, Maria Grazia},
title = {Mixed Integer Non-Linear Programming models for Green Network Design},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {40},
number = {1},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2012.06.014},
doi = {10.1016/j.cor.2012.06.014},
abstract = {The paper focuses on Network Power Management in telecommunication infrastructures. Specifically, the paper describes four energy aware network design problems, with the related mathematical models, for reducing the power consumption of the current and future Internet. Each problem is based on a different characterization and power awareness of the network devices, leading to either Mixed Integer Linear Programming or Mixed Integer Non-Linear Programming models. We have assessed the effectiveness of the proposed approaches under different real core network topology scenarios by evaluating the impact of several network parameters. To the best of our knowledge, this is the first work that deeply investigates the behavior of a pool of diverse Network Power Management approaches, including the first Mixed Integer Non-Linear Programming model for the Power Aware Network Design with Bundled Links.},
journal = {Comput. Oper. Res.},
month = jan,
pages = {273–281},
numpages = {9},
keywords = {Routing and design, Power aware problems, MINLP models, MILP models, Computational analysis}
}

@article{10.1504/IJAIP.2009.026765,
author = {Koshizen, Takamasa and Kon, Motohri and Raynolds, Carson and Aihara, Kazuyuki},
title = {Habituation detection with Allen-Cahn boundary generation},
year = {2009},
issue_date = {June 2009},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {1},
number = {4},
issn = {1755-0386},
url = {https://doi.org/10.1504/IJAIP.2009.026765},
doi = {10.1504/IJAIP.2009.026765},
abstract = {We describe a new habituation system that makes use of Partial Differential Equations (PDEs) to deal with time-inconsistent patterns. With respect to our detection system, the Allen-Cahn (AC) equation is used to model a boundary which evolves over time. As a result, the AC could outperform the traditional techniques even real-world dataset. Thus, it leads to a boundary computation for robust detection system.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jun,
pages = {463–487},
numpages = {25},
keywords = {user modelling, time inconsistency, robust detection systems, pattern categorisation, partial differential equations, habituation, boundary growth, PDEs, Allen-Cahn equation}
}

@inproceedings{10.5555/1411620.1411666,
author = {Seiciu, Petre Lucian and Filipoiu, Ioan Dan and Laurian, Tiberiu},
title = {Theoretical aspects related to the design of a mechatronic system for recovery of the disabled persons},
year = {2008},
isbn = {9789606766770},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Human walking is theoretically explained by two prevailing, but contradictory theories. The first theory called "The Six Determinants Of Gait" (SDG) aims to minimize the energetic cost of locomotion by reducing the vertical displacement of the body center of mass (COM). The second theory called "The Inverted Pendulum Analogy" (IPA) suggests that walking is a movement combination of two pendulums: the inverted pendulum for the stance leg and a direct pendulum for the swing leg. Therefore, results that COM describes a quasi-sinusoid curve. The main evidence against SDG theory is that a flattened COM trajectory increases muscle work and force requirements. The IPA theory analysis shows that it predicts no work or force requirements. These shortcomings may be resolved through a new theory: "The Dynamic Walking Theory" DWT. These theories describe more or less appropriate walking of healthy persons. The paper presents some theoretical aspects to be considered at the design of a mechatronic system for the locomotory disabled persons.},
booktitle = {Proceedings of the 9th WSEAS International Conference on International Conference on Automation and Information},
pages = {230–235},
numpages = {6},
keywords = {rehabilitation, mechatronic system, locomotory disabled persons},
location = {Bucharest, Romania},
series = {ICAI'08}
}

@article{10.1109/90.251895,
author = {Partridge, Craig and Pink, Stephen},
title = {A faster UDP},
year = {1993},
issue_date = {Aug. 1993},
publisher = {IEEE Press},
volume = {1},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/90.251895},
doi = {10.1109/90.251895},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {429–440},
numpages = {12}
}

@article{10.1177/1094342004041291,
author = {P\"{u}schel, Markus and Moura, Jos\'{e} M. F. and Singer, Bryan and Xiong, Jianxin and Johnson, Jeremy and Padua, David and Veloso, Manuela and Johnson, Robert W.},
title = {Spiral: A Generator for Platform-Adapted Libraries of Signal Processing Algorithms},
year = {2004},
issue_date = {February  2004},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {18},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342004041291},
doi = {10.1177/1094342004041291},
abstract = {SPIRAL is a generator for libraries of fast software implementations of linear signal processing transforms. These libraries are adapted to the computing platform and can be re-optimized as the hardware is upgraded or replaced. This paper describes the main components of SPIRAL: the mathematical framework that concisely describes signal transforms and their fast algorithms; the formula generator that captures at the algorithmic level the degrees of freedom in expressing a particular signal processing transform; the formula translator that encapsulates the compilation degrees of freedom when translating a specific algorithm into an actual code implementation; and, finally, an intelligent search engine that finds within the large space of alternative formulas and implementations the "best" match to the given computing platform. We present empirical data that demonstrate the high performance of SPIRAL generated code.},
journal = {Int. J. High Perform. Comput. Appl.},
month = feb,
pages = {21–45},
numpages = {25},
keywords = {signal transform, signal processing, search, program generation, optimization, domain-specific language, automatic performance tuning, Fourier transform, FFT, DFT}
}

@article{10.1016/j.comnet.2012.03.009,
author = {Cuomo, Francesca and Cianfrani, Antonio and Polverini, Marco and Mangione, Daniele},
title = {Network pruning for energy saving in the Internet},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {56},
number = {10},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2012.03.009},
doi = {10.1016/j.comnet.2012.03.009},
abstract = {Many scientific works propose methods of reducing the amount of energy consumed by the Internet. Although the structure of the Internet was not developed with specific attention to energy consumption, there are various components on which it is possible to act. In our work, we analyze the possibility of affecting the topology of the network. We propose a heuristic called Energy Saving based on TOPology control (ESTOP), which identifies poorly used router line cards by leveraging certain topological properties of the graph modeling an Internet Service Provider (ISP). By acting on these line cards - for example, by putting them into sleep mode - we prune the Internet topology and achieve significant energy savings while preserving the primary topological characteristics of the pruned network. Although ESTOP is traffic-unaware, we assess its behavior under real traffic loads, demonstrating that its performance is comparable to the more complex traffic-aware solutions proposed in the literature.},
journal = {Comput. Netw.},
month = jul,
pages = {2355–2367},
numpages = {13},
keywords = {Green Internet, Graph theory, Energy consumption}
}

@article{10.1007/s007780050074,
author = {Kabra, Navin and DeWitt, David J.},
title = {OPT++ : an object-oriented implementation for extensible database query optimization},
year = {1999},
issue_date = {April 1999},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s007780050074},
doi = {10.1007/s007780050074},
abstract = {In this paper we describe the design and implementation of OPT++, a tool for extensible database query optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space, and also to change the search strategy. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an optimizer-implementor. OPT++ considerably simplifies both, the task of implementing an optimizer for a new database system, and the task of evaluating alternative optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of a series of performance studies. These results validate our design and show that, in spite of its flexibility, OPT++ can be used to build efficient optimizers.},
journal = {The VLDB Journal},
month = apr,
pages = {55–78},
numpages = {24},
keywords = {Software architecture, Query optimization, Object-relational databases, Extensibility}
}

@article{10.1145/2159542.2159547,
author = {Milder, Peter and Franchetti, Franz and Hoe, James C. and P\"{u}schel, Markus},
title = {Computer Generation of Hardware for Linear Digital Signal Processing Transforms},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2159542.2159547},
doi = {10.1145/2159542.2159547},
abstract = {Linear signal transforms such as the discrete Fourier transform (DFT) are very widely used in digital signal processing and other domains. Due to high performance or efficiency requirements, these transforms are often implemented in hardware. This implementation is challenging due to the large number of algorithmic options (e.g., fast Fourier transform algorithms or FFTs), the variety of ways that a fixed algorithm can be mapped to a sequential datapath, and the design of the components of this datapath. The best choices depend heavily on the resource budget and the performance goals of the target application. Thus, it is difficult for a designer to determine which set of options will best meet a given set of requirements.In this article we introduce the Spiral hardware generation framework and system for linear transforms. The system takes a problem specification as input as well as directives that define characteristics of the desired datapath. Using a mathematical language to represent and explore transform algorithms and datapath characteristics, the system automatically generates an algorithm, maps it to a datapath, and outputs a synthesizable register transfer level Verilog description suitable for FPGA or ASIC implementation. The quality of the generated designs rivals the best available handwritten IP cores.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = apr,
articleno = {15},
numpages = {33},
keywords = {linear transform, high-level synthesis, hardware generation, fast Fourier transform, discrete Fourier transform, Digital signal processing transform}
}

@book{10.5555/2086747,
author = {Laszewski, Tom and Nauduri, Prakash},
title = {Migrating to the Cloud: Oracle Client/Server Modernization},
year = {2011},
isbn = {9781597496476},
publisher = {Syngress Publishing},
edition = {1st},
abstract = {Whether your company is planning on database migration, desktop application migration, or has IT infrastructure consolidation projects, this book gives you all the resources youll need. It gives you recommendations on tools, strategy and best practices and serves as a guide as you plan, determine effort and budget, design, execute and roll your modern Oracle system out to production. Focusing on Oracle grid relational database technology and Oracle Fusion Middleware as the target cloud-based architecture, your company can gain organizational efficiency, agility, increase innovation and reduce IT Total Cost of Ownership (TCO) by moving to service-oriented, Web-based cloud architectures. Focuses on Oracle architecture, Middleware and COTS business applications Explains the tools and technologies necessary for your legacy migration Gives useful information about various strategies, migration methodologies and efficient plans for executing migration projects Table of Contents Introduction Chapter 1: Migrating to the Cloud: ClientServer Migrations to the Oracle Cloud Chapter 2: Identifying the Level of Effort and Cost Chapter 3: Methodology and Design Chapter 4: Relational Migration Tools Chapter 5: Database Schema and Data Migration Chapter 6: Database Stored Object Migration Chapter 7: Application MigrationPorting Due to Database Migration Chapter 8: Migrating Applications to the Cloud Chapter 9: Service Enablement of ClientServer Applications Chapter 10: Oracle Database Cloud Infrastructure Planning and Implementation Chapter 11: Sybase Migrations from a Systems Integrator Perspective, and Case Study Chapter 12: Application Migration: Oracle Forms to Oracle Application Development Framework 11g Chapter 13: Application Migration: PowerBuilder to Oracle APEX Chapter 14: Challenges and Emerging Trends}
}

@inproceedings{10.1145/319151.319161,
author = {Chiueh, Tzi-cker and Venkitachalam, Ganesh and Pradhan, Prashant},
title = {Integrating segmentation and paging protection for safe, efficient and transparent software extensions},
year = {1999},
isbn = {1581131402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319151.319161},
doi = {10.1145/319151.319161},
abstract = {The trend towards extensible software architectures and component-based software development demands safe, efficient, and easy-to-use extension mechanisms to enforce protection boundaries among software modules residing in the same address space. This paper describes the design, implementation, and evaluation of a novel intra-address space protection mechanism called Palladium, which exploits the segmentation and paging hardware in the Intel X86 architecture and efficiently supports safe kernel-level and user-level extensions in a way that is largely transparent to programmers and existing programming tools. Based on the considerations on ease of extension programming and systems implementation complexity, Palladium uses different approaches to support user-level and kernel-level extension mechanisms. To demonstrate the effectiveness of the Palladium architecture, we built a Web server that exploits the user-level extension mechanism to invoke CGI scripts as local function calls in a safe way, and we constructed a compiled network packet filter that exploits the kernel-level extension mechanism to run packet-filtering binaries safely inside the kernel at native speed. The current Palladium prototype implementation demonstrates that a protected procedure call and return costs 142 CPU cycles on a Pentium 200MHz machine running Linux.},
booktitle = {Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles},
pages = {140–153},
numpages = {14},
location = {Charleston, South Carolina, USA},
series = {SOSP '99}
}

@article{10.1016/j.automatica.2012.06.027,
author = {Zhang, Wen-An and Feng, Gang and Yu, Li},
title = {Multi-rate distributed fusion estimation for sensor networks with packet losses},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {48},
number = {9},
issn = {0005-1098},
url = {https://doi.org/10.1016/j.automatica.2012.06.027},
doi = {10.1016/j.automatica.2012.06.027},
abstract = {This paper presents a distributed fusion estimation method for estimating states of a dynamical process observed by wireless sensor networks (WSNs) with random packet losses. It is assumed that the dynamical process is not changing too rapidly, and a multi-rate scheme by which the sensors estimate states at a faster time scale and exchange information with neighbors at a slower time scale is proposed to reduce communication costs. The estimation is performed by taking into account the random packet losses in two stages. At the first stage, every sensor in the WSN collects measurements from its neighbors to generate a local estimate, then local estimates in the neighbors are further collected at the second stage to form a fused estimate to improve estimation performance and reduce disagreements among local estimates at different sensors. Local optimal linear estimators are designed by using the orthogonal projection principle, and the fusion estimators are designed by using a fusion rule weighted by matrices in the linear minimum variance sense. Simulations of a target tracking system are given to show that the time scale of information exchange among sensors can be slower while still maintaining satisfactory estimation performance by using the developed estimation method.},
journal = {Automatica},
month = sep,
pages = {2016–2028},
numpages = {13},
keywords = {Wireless sensor networks, Packet losses, Kalman filtering, Information fusion, Distributed estimation}
}

@inproceedings{10.1145/1287731.1287738,
author = {Ono, Yasuhiro and Lifton, Joshua and Feldmeier, Mark and Paradiso, Joseph A.},
title = {Distributed acoustic conversation shielding: an application of a smart transducer network},
year = {2007},
isbn = {9781595937353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287731.1287738},
doi = {10.1145/1287731.1287738},
abstract = {In this paper, we introduce distributed acoustic conversation shielding, a novel application of a transducer (sensors and speakers) network. This application protects the privacy of spontaneous conversations in a workplace by masking the participants' voices with sound from distributed loudspeakers that adapt to the dynamic location of the conversation vs. that of potential eavesdroppers. We demonstrate how the speakers collaborate with various sensors to produce masking sounds that satisfy the requirements of this application. An index of intelligibility, SNR (Signal-to-Noise Ratio) was used to evaluate the performance of our system. We suggest how the measured SNR can be used to adaptively servo the volume of the masking sounds.},
booktitle = {Proceedings of the First ACM Workshop on Sensor and Actor Networks},
pages = {27–34},
numpages = {8},
keywords = {sound masking, sensor network, location awareness, distributed control, conversation shielding},
location = {Montreal, Quebec, Canada},
series = {SANET '07}
}

@article{10.1561/2500000045,
author = {Ringer, Talia and Palmskog, Karl and Sergey, Ilya and Gligoric, Milos and Tatlock, Zachary},
title = {QED at Large: A Survey of Engineering of Formally Verified Software},
year = {2019},
issue_date = {Sep 2019},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {5},
number = {2–3},
issn = {2325-1107},
url = {https://doi.org/10.1561/2500000045},
doi = {10.1561/2500000045},
abstract = {Development of formal proofs of correctness of programs can
increase actual and perceived reliability and facilitate better understanding
of program specifications and their underlying assumptions.
Tools supporting such development have been available
for over 40 years, but have only recently seen wide practical use.
Projects based on construction of machine-checked formal proofs
are now reaching an unprecedented scale, comparable to large software
projects, which leads to new challenges in proof development
and maintenance. Despite its increasing importance, the field of
proof engineering is seldom considered in its own right; related
theories, techniques, and tools span many fields and venues. This
survey of the literature presents a holistic understanding of proof
engineering for program correctness, covering impact in practice,
foundations, proof automation, proof organization, and practical
proof development.},
journal = {Found. Trends Program. Lang.},
month = sep,
pages = {102–281},
numpages = {183}
}

@article{10.1504/IJHPCN.2006.010204,
author = {Sridhar, K. N. and Jacob, Lillykutty},
title = {Performance evaluation and enhancement of a link stability based routing protocol for MANETs},
year = {2006},
issue_date = {July 2006},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {1/2},
issn = {1740-0562},
url = {https://doi.org/10.1504/IJHPCN.2006.010204},
doi = {10.1504/IJHPCN.2006.010204},
abstract = {In this paper, first we present a performance comparison of Associativity Based Routing (ABR) and Ad hoc On-demand Distance Vector (AODV) routing protocols. Delay, throughput and energy consumption are the performance metrics that are used for the comparison. We also carry out scenario based performance evaluation of ABR. We propose a technique to reduce the packet size by changing the mechanics of the protocol, and evaluate the proposed technique considering delay, throughput and energy consumption. We also propose and study a scheduling mechanism (neighbour-state dependent scheduling) that uses the overhead information in ABR routing packets to improve the performance.},
journal = {Int. J. High Perform. Comput. Netw.},
month = jul,
pages = {66–77},
numpages = {12},
keywords = {wireless communications, throughput, scheduling, routing protocols, performance evaluation, performance enhancement, mobility models, mobile networks, link stability, energy consumption, delay, associativity based routing, ad hoc on-demand distance vector, ad hoc networks, MANET, AODV, ABR routing packets}
}

@article{10.5555/1046430.1046440,
author = {Yu, Yang and Prasanna, Viktor K.},
title = {Energy-balanced task allocation for collaborative processing in wireless sensor networks},
year = {2005},
issue_date = {February 2005},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {1–2},
issn = {1383-469X},
abstract = {We propose an energy-balanced allocation of a real-time application onto a single-hop cluster of homogeneous sensor nodes connected with multiple wireless channels. An epoch-based application consisting of a set of communicating tasks is considered. Each sensor node is equipped with discrete dynamic voltage scaling (DVS). The time and energy costs of both computation and communication activities are considered. We propose both an Integer Linear Programming (ILP) formulation and a polynomial time 3-phase heuristic. Our simulation results show that for small scale problems (with ≤ 10 tasks), up to 5x lifetime improvement is achieved by the ILP-based approach, compared with the baseline where no DVS is used. Also, the 3-phase heuristic achieves up to 63% of the system lifetime obtained by the ILP-based approach. For large scale problems (with 60-100 tasks), up to 3.5x lifetime improvement can be achieved by the 3-phase heuristic. We also incorporate techniques for exploring the energy-latency tradeoffs of communication activities (such as modulation scaling), which leads to 10x lifetime improvement in our simulations. Simulations were further conducted for two real world problems - LU factorization and Fast Fourier Transformation (FFT). Compared with the baseline where neither DVS nor modulation scaling is used, we observed up to 8x lifetime improvement for the LU factorization algorithm and up to 9x improvement for FFT.},
journal = {Mob. Netw. Appl.},
month = feb,
pages = {115–131},
numpages = {17},
keywords = {single-hop wireless networks, sensor networks, energy saving, ILP}
}

@inproceedings{10.1145/224538.224547,
author = {Chiueh, Tzi-cker and Verma, Manish},
title = {A compiler-directed distributed shared memory system},
year = {1995},
isbn = {0897917286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/224538.224547},
doi = {10.1145/224538.224547},
booktitle = {Proceedings of the 9th International Conference on Supercomputing},
pages = {77–86},
numpages = {10},
location = {Barcelona, Spain},
series = {ICS '95}
}

@article{10.1016/j.comnet.2009.10.004,
author = {Kant, Krishna},
title = {Data center evolution},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {53},
number = {17},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2009.10.004},
doi = {10.1016/j.comnet.2009.10.004},
abstract = {Data centers form a key part of the infrastructure upon which a variety of information technology services are built. As data centers continue to grow in size and complexity, it is desirable to understand aspects of their design that are worthy of carrying forward, as well as existing or upcoming shortcomings and challenges that would have to be addressed. We envision the data center evolving from owned physical entities to potentially outsourced, virtualized and geographically distributed infrastructures that still attempt to provide the same level of control and isolation that owned infrastructures do. We define a layered model for such data centers and provide a detailed treatment of state of the art and emerging challenges in storage, networking, management and power/thermal aspects.},
journal = {Comput. Netw.},
month = dec,
pages = {2939–2965},
numpages = {27},
keywords = {Virtualization, Solid state storage, Power management, InfiniBand, Ethernet, Data center}
}

@article{10.5555/2773570.2773724,
author = {Lluch-Lafuente, Alberto and Montanari, Ugo},
title = {Quantitative μ-calculus and CTL Based on Constraint Semirings},
year = {2005},
issue_date = {January 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1571-0661},
abstract = {Model checking and temporal logics are boolean. The answer to the model checking question does a system satisfy a property? is either true or false, and properties expressed in temporal logics are defined over boolean propositions. While this classic approach is enough to specify and verify boolean temporal properties, it does not allow to reason about quantitative aspects of systems. Some quantitative extensions of temporal logics has been already proposed, especially in the context of probabilistic systems. They allow to answer questions like with which probability does a system satisfy a property?We present a generalization of two well-known temporal logics: CTL and the μ-calculus. Both extensions are defined over c-semirings, an algebraic structure that captures many problems and that has been proposed as a general framework for soft constraint satisfaction problems (CSP). Basically, a c-semiring consists of a domain, an additive operation and a multiplicative operation, which satisfy some properties. We present the semantics of the extended logics over transition systems, where a formula is interpreted as a mapping from the set of states to the domain of the c-semiring, and show that the usual connection between CTL and μ-calculus does not hold in general. In addition, we reason about the feasibility of computing the logics and illustrate some applications of our framework, including boolean model checking.},
journal = {Electron. Notes Theor. Comput. Sci.},
month = jan,
pages = {37–59},
numpages = {23},
keywords = {Temporal Logics, Quantitative Model Checking, Constraints, Constraint Semirings}
}

@article{10.1145/270849.270854,
author = {Edwards, Stephen H. and Weide, Bruce W.},
title = {WISR8: 8th annual workshop on software reuse: summary and working group reports},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270854},
doi = {10.1145/270849.270854},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {17–32},
numpages = {16}
}

@proceedings{10.1145/2818427,
title = {SA '15: SIGGRAPH Asia 2015 Mobile Graphics and Interactive Applications},
year = {2015},
isbn = {9781450339285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kobe, Japan}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@book{10.5555/2815511,
author = {Wu, Caesar and Buyya, Rajkumar},
title = {Cloud Data Centers and Cost Modeling: A Complete Guide To Planning, Designing and Building a Cloud Data Center},
year = {2015},
isbn = {012801413X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development}
}

@inproceedings{10.5555/644108.644115,
author = {Irani, Sandy and Shukla, Sandeep and Gupta, Rajesh},
title = {Algorithms for power savings},
year = {2003},
isbn = {0898715385},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {This paper examines two different mechanisms for saving power in battery-operated embedded systems. The first is that the system can be placed in a sleep state if it is idle. However, a fixed amount of energy is required to bring the system back into an active state in which it can resume work. The second way in which power savings can be achieved is by varying the speed at which jobs are run. We utilize a power consumption curve P(s). which indicates the power consumption level given a particular speed. We assume that P(s) and P(s)/s are convex. The problem is to schedule arriving jobs in a way that minimizes total energy use and so that each job is completed after its arrival time and before its deadline. Although each problem has been considered separately, this is the first theoretical analysis of systems which can use both mechanisms. We give an off line algorithm which is within a factor of three of the optimal algorithm. We also give an online algorithm with a constant competitive ratio.},
booktitle = {Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {37–46},
numpages = {10},
location = {Baltimore, Maryland},
series = {SODA '03}
}

@inproceedings{10.1145/336512.336546,
author = {Lamsweerde, Axel van},
title = {Formal specification: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336546},
doi = {10.1145/336512.336546},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {147–159},
numpages = {13},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1145/800008.808038,
author = {Slamecka, Vladimir},
title = {Conference abstracts},
year = {1977},
isbn = {9781450373739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800008.808038},
doi = {10.1145/800008.808038},
abstract = {One problem in computer program testing arises when errors are found and corrected after a portion of the tests have run properly. How can it be shown that a fix to one area of the code does not adversely affect the execution of another area? What is needed is a quantitative method for assuring that new program modifications do not introduce new errors into the code. This model considers the retest philosophy that every program instruction that could possibly be reached and tested from the modified code be retested at least once. The problem is how to determine the minimum number of test cases to be rerun. The process first involves generating the test case dependency matrix and the reachability matrix. Using the test case dependency matrix and the appropriate rows of the reachability matrix, a 0-1 integer program can be specified. The solution of the integer program yields the minimum number of test cases to be rerun, and the coefficients of the objective function identify which specific test cases to rerun.},
booktitle = {Proceedings of the 5th Annual ACM Computer Science Conference},
pages = {1–36},
numpages = {36},
series = {CSC '77}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1145/279437.279460,
author = {Thompson, Craig},
title = {Workshop on compositional software architectures: workshop report},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/279437.279460},
doi = {10.1145/279437.279460},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {44–63},
numpages = {20}
}

@book{10.5555/1611279,
author = {Druin, Allison},
title = {Mobile Technology for Children: Designing for Interaction and Learning},
year = {2009},
isbn = {9780080954097},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {With the goal of improving the design of mobile technology for children, On the Move brings together contributions from HCI leaders in research, and industry, and technology and education based policy experts to analyze and evaluate and present solutions. To show readers how they can apply each design problem and case study to their HCI professional or academic work, each chapter will contain best practice advice. In HCI, social implications, in addition to interface design, usability, and performance, are all part of an informed design solution. Chauncey Wilson, Senior User Researcher, Autodesk, Inc., and forthcoming MK author, states, The design of mobile devices is not an algorithmic process, it is must be considered in a social context that examines culture, changing trends, and other factors. This proposal provides a solid foundation of the social and cultural factors that are critical in the design of mobile products for children. There are many technological solutions to consider, many contexts to explore user scenarios and many goals for supporting learning. It contains the work of 43 authors from 9 countries, each deeply invested in improving and analyzing design of childrens mobile products. These authors have diverse points of view, but it is a subject that deserves debate. The trends, design and use of these products has been both lauded and criticized, and the debate is far from over. The need has never been greater for an evaluation of the design and the affects of the design mobile technology as it pertains to children's products and learning - the good and the bad - especially for and by the people who conduct research, develop and design the products. *First book for HCI practitioners and researchers to present a multitude of voices on the design, technology, and impact of mobile devices for children from global perspective *Features contributions from leading HCI academics, professionals, and childrens technology policy leaders from nine countries *Each contribution and case study is followed by a best practice overview to help readers improve future research and design and for a quick reference at a later date}
}

@book{10.5555/996538,
author = {Amon, Cherie and Shinder, Thomas W. and Carasik-Henmi, Anne},
title = {The Best Damn Firewall Book Period},
year = {2007},
isbn = {1931836906},
publisher = {Syngress Publishing},
edition = {2},
abstract = {The Second Edition of the Best Damn Firewall Book Period is completely revised and updated to include all of the most recent releases from Microsoft, Cisco, Juniper Network, SonicWALL, and Check Point. Compiled from the best of the Syngress firewall library and authored by product experts such as Dr. Tom Shinder on ISA Server, this volume is an indispensable addition to a serious networking professionals toolkit. Coverage includes migrating to ISA Server 2006, integrating Windows Firewall and Vista security into your enterprise, successfully integrating Voice over IP applications around firewalls, and analyzing security log files. Sections are organized by major vendor, and include hardware, software and VPN configurations for each product line. New to this Edition: * Microsoft firewall protection, from Windows Firewall to ISA Server 2006 * Cisco PIX Version 7, including VPN configuration and IDS * Analyzing Firewall Logs and Reports * VoIP and Firewall Bypassing}
}

@book{10.5555/1537084,
author = {Shinder, Thomas W. and Shinder, Debra Littlejohn and Dimcev, Adrian F. and Eaton-Lee, James and Jones, Jason and Moffat, Steve},
title = {Dr. Tom Shinder's ISA Server 2006 Migration Guide},
year = {2007},
isbn = {9780080555515},
publisher = {Syngress Publishing},
abstract = {Dr. Tom Shinder's ISA Server 2006 Migration Guide provides a clear, concise, and thorough path to migrate from previous versions of ISA Server to ISA Server 2006. ISA Server 2006 is an incremental upgrade from ISA Server 2004, this book provides all of the tips and tricks to perform a successful migration, rather than rehash all of the features which were rolled out in ISA Server 2004. Also, learn to publish Exchange Server 2007 with ISA 2006 and to build a DMZ. * Highlights key issues for migrating from previous versions of ISA Server to ISA Server 2006. * Learn to Publish Exchange Server 2007 Using ISA Server 2006. * Create a DMZ using ISA Server 2006. * Dr. Tom Shinder's previous two books on configuring ISA Server have sold more than 50,000 units worldwide. * Dr. Tom Shinder is a Microsoft Most Valuable Professional (MVP) for ISA Server and a member of the ISA Server beta testing team. * This book will be the "Featured Product" on the Internet's most popular ISA Server site www.isaserver.org.}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@book{10.5555/2742301,
author = {Preim, Bernhard and Botha, Charl P.},
title = {Visual Computing for Medicine: Theory, Algorithms, and Applications},
year = {2013},
isbn = {9780124159792},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includes algorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques from research, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field.}
}

@book{10.5555/1564784,
author = {Wang},
title = {System-on-Chip Test Architectures: Nanometer  Design for Testability},
year = {2007},
isbn = {9780080556802},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Modern electronics testing has a legacy of more than 40 years. The introduction of new technologies, especially nanometer technologies with 90nm or smaller geometry, has allowed the semiconductor industry to keep pace with the increased performance-capacity demands from consumers. As a result, semiconductor test costs have been growing steadily and typically amount to 40% of today's overall product cost. This book is a comprehensive guide to new VLSI Testing and Design-for-Testability techniques that will allow students, researchers, DFT practitioners, and VLSI designers to master quickly System-on-Chip Test architectures, for test debug and diagnosis of digital, memory, and analog/mixed-signal designs. KEY FEATURES * Emphasizes VLSI Test principles and Design for Testability architectures, with numerous illustrations/examples. * Most up-to-date coverage available, including Fault Tolerance, Low-Power Testing, Defect and Error Tolerance, Network-on-Chip (NOC) Testing, Software-Based Self-Testing, FPGA Testing, MEMS Testing, and System-In-Package (SIP) Testing, which are not yet available in any testing book. * Covers the entire spectrum of VLSI testing and DFT architectures, from digital and analog, to memory circuits, and fault diagnosis and self-repair from digital to memory circuits. * Discusses future nanotechnology test trends and challenges facing the nanometer design era; promising nanotechnology test techniques, including Quantum-Dots, Cellular Automata, Carbon-Nanotubes, and Hybrid Semiconductor/Nanowire/Molecular Computing. * Practical problems at the end of each chapter for students.}
}

@proceedings{10.1145/2951913,
title = {ICFP 2016: Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nara, Japan}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@book{10.5555/2505467,
author = {Vacca, John R. and Vacca, John R.},
title = {Computer and Information Security Handbook, Second Edition},
year = {2013},
isbn = {0123943973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

